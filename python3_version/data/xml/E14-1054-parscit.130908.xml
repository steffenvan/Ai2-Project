<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003314">
<title confidence="0.985854">
Distributional Lexical Entailment by Topic Coherence
</title>
<author confidence="0.996782">
Laura Rimell
</author>
<affiliation confidence="0.97994">
University of Cambridge
Computer Laboratory
</affiliation>
<email confidence="0.990295">
laura.rimell@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993722" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997911875">
Automatic detection of lexical entailment,
or hypernym detection, is an important
NLP task. Recent hypernym detection
measures have been based on the Distri-
butional Inclusion Hypothesis (DIH). This
paper assumes that the DIH sometimes
fails, and investigates other ways of quan-
tifying the relationship between the co-
occurrence contexts of two terms. We con-
sider the top features in a context vector
as a topic, and introduce a new entailment
detection measure based on Topic Coher-
ence (TC). Our measure successfully de-
tects hypernyms, and a TC-based family
of measures contributes to multi-way rela-
tion classification.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966225806452">
Automatically detecting lexical entailment – for
example, that lion entails animal or guitar entails
instrument, also known as hypernym detection –
is an important linguistic task in its own right, and
is also a prerequisite for recognizing entailments
between longer text segments such as phrases or
sentences (Bos and Markert, 2005; Garrette et al.,
2011; Baroni et al., 2012; Beltagy et al., 2013).
Several recent techniques for hypernym de-
tection have made use of distributional seman-
tics (Weeds and Weir, 2003; Weeds et al., 2004;
Clarke, 2009; Kotlerman et al., 2010; Lenci and
Benotto, 2012). These techniques are based on the
Distributional Inclusion Hypothesis (Geffet and
Dagan, 2005), hereafter DIH, which proposes that
if term A entails term B (B is a hypernym of A),
then the contexts in which A occurs are a subset of
those in which B occurs. For example, all the con-
texts (co-occurrences) of lion – which might in-
clude zoo, hunt, wild, food, etc. – are also contexts
of animal. Existing measures look at the amount
of overlap between the co-occurrences of A and B,
in order to judge whether B is a hypernym of A.
The motivation for the present paper is the well-
known fact that the DIH is not fully correct. There
are many reasons why a hyponym might occur
in contexts where its hypernym does not. Some
contexts are collocational, e.g. lion king. Other
contexts are highly specific, e.g. mane applies
uniquely to lions, horses, and zebras; it would be
unusual to see text about animals with manes. The
need to be informative is also relevant: lion cub
will occur much more frequently than animal cub,
since animal is of the wrong level of generality to
pair with cub.
Moreover, the more general a hypernym be-
comes – up to the level of WordNet root elements,
such as entity – its predominant sense ceases to
correspond to the sense intended in hyponym-
hypernym chains. Thus we never hear about going
to visit an entity at the zoo.
This paper starts from the assumption that the
DIH sometimes fails, and investigates not the
amount of containment of A’s features in B’s fea-
tures, but rather the nature of the non-contained
features. We consider the top features of a dis-
tributional vector as a topic, and use recent mea-
sures for automatically measuring Topic Coher-
ence (Newman et al., 2010; Mimno et al., 2011)
to evaluate how the topics change under various
conditions. Using a notion of vector negation, we
investigate whether the distributional topic of e.g.
lion becomes more or less coherent when we sub-
tract the contexts of animal.
We introduce a new measure, Ratio of Change
in Topic Coherence (RCTC), for detecting lexical
entailment. The measure detects hypernyms with
reasonable accuracy, and a family of Topic Coher-
ence measures is used to perform a multi-way clas-
sification of tuples by relation class. Finally, we
investigate how the level of generality of a hyper-
nym affects entailment measures.
</bodyText>
<page confidence="0.963788">
511
</page>
<note confidence="0.9973245">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997382" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999981951219512">
Historically, manually developed resources such
as WordNet (Miller, 1995) have been used to sup-
ply lexical entailment information to NLP appli-
cations (Bos and Markert, 2005). More recently,
a number of techniques for detecting lexical en-
tailment have been developed using distributional
semantics (Weeds and Weir, 2003; Weeds et al.,
2004; Geffet and Dagan, 2005; Clarke, 2009;
Kotlerman et al., 2010; Lenci and Benotto, 2012).
These measures quantify to what extent the co-
occurrence features of a term A are included in
those of another term B, by a direct comparison
of the distributional vectors A� and A Kotlerman
et al. (2010) use the notion of Average Precision
from Information Retrieval to weight the relative
importance of the overlapping features. Lenci and
Benotto (2012) also check the extent to which B’s
features are not a subset of A’s, as a proxy for the
more general character of B. The success of these
feature inclusion measures has provided general
support for the DIH. Following Szpektor and Da-
gan (2008), inclusion measures are also sometimes
balanced with similarity measures such as LIN
similarity (Lin, 1998), to ensure that A and B are
semantically related, since unrelated pairs that dif-
fer in frequency can mimic feature inclusion.
Previous distributional approaches to hypernym
detection have generally involved a single mea-
sure, designed to rank hypernyms above other re-
lation classes. Evaluation has largely involved
either ranking or binary classification tasks, and
there has been little work on using a variety of
measures to distinguish multiple relation classes.
Lenci and Benotto (2012) perform a ranking task
using the multi-class BLESS dataset (Baroni and
Lenci, 2011), but not a classification. We perform
a multi-way classification using a variety of Topic
Coherence measures. Recent Semantic Relation
Classification shared tasks (SemEval-2010 Task 8,
SemEval-2012 Task 2) are also relevant, though
the relation classes and approaches have differed.
</bodyText>
<sectionHeader confidence="0.8059445" genericHeader="method">
3 Topic Coherence for Distributional
Lexical Entailment
</sectionHeader>
<bodyText confidence="0.99971424">
The intuition behind our approach is to investigate
whether term A, the candidate hyponym, has a co-
herent topic reflected in its distributional features,
which apply only to A and not to its hypernym B.
Consider A=beer, B=beverage. They may share
features such as drink, cold, and party. But if we
minimize or exclude B’s features and examine the
remaining features of A (we discuss how to do this
in Section 3.3), we might be left with more specific
features such as pint, lager, and brew.
If A and B share almost all contexts, we would
be left with a set of uninformative features, merely
corpus noise. If A and B share few contexts, there
would be little change to A’s topic when excluding
B’s features. Between the extremes, a range of
change in A’s topic is possible; we seek to quantify
this change and relate it to entailment.
To do this we need a way of treating a distri-
butional context vector as a topic. We treat the
N highest-weighted context features in A� as the
topic of A (topicA). If we represent the vector
A� ≡ {fci,A}i, where fci,A is the weighted co-
occurrence value of context feature ci, then topicA
is a set {cj}, j E 1...N, of the N highest-weighted
context features cj in A.
</bodyText>
<subsectionHeader confidence="0.988937">
3.1 Hypotheses
</subsectionHeader>
<bodyText confidence="0.98062352">
We consider two opposing hypotheses.
Hypothesis 1: Removing hypernym B’s fea-
tures from topicA will decrease the coherence of
topicA. If being a B is very important to being an
A, then the collection of remaining features may
become more random. Hypothesis 1 is consistent
with the DIH, since it implies that the important
features of A are also features of B.
As a corollary, removing A’s features from B
may not change the coherence of topicB very
much. Since A is just an instance of B, topicB
retains coherence (i.e. there’s a lot to being an an-
imal besides what’s involved in being a lion).
Hypothesis 2: Removing hypernym B’s fea-
tures from topicA will increase the coherence of
topicA. Perhaps A, by virtue of being more spe-
cific, occurs in a highly coherent set of contexts
where B does not. Hypothesis 2 is inconsistent
with the DIH, since it imples that a hyponym al-
ways has specific features which the hypernym
does not share.
As a corollary, removing hyponym A’s features
from hypernym B might decrease the coherence of
topicB, if removing specific features leaves only
more general, less informative features behind.
</bodyText>
<subsectionHeader confidence="0.998835">
3.2 Topic Coherence Measure
</subsectionHeader>
<bodyText confidence="0.9991865">
We use a Topic Coherence (TC) measure from re-
cent work on automatic evalution of topics gener-
ated from corpora by latent variable models (New-
man et al., 2010; Mimno et al., 2011; Stevens et
</bodyText>
<page confidence="0.99487">
512
</page>
<bodyText confidence="0.999540222222222">
al., 2012). TC measures are applied to the top N
words from a generated topic. They assign pair-
wise relatedness scores to the words, and return
the mean or median from the word-pair scores.
We adopt the best method from Newman et al.
(2010), equal to the median pairwise Pointwise
Mutual Information (PMI) of the top N words, us-
ing Wikipedia as a background corpus for PMI.1
The measure is given in Equation (1):
</bodyText>
<equation confidence="0.99578075">
TC({cj}) = median(PMI(ci, ck), i, k E 1...N, i &lt; k)
where {cj} is the topic, and PMI is defined as: (1)
PMI(ci, ck) = log p(ci, ck)
p(ci)p(ck) (2)
</equation>
<bodyText confidence="0.999715692307692">
We use intra-sentence co-occurrence in Wikipedia
for calculating PMI.
Note that our definition of a topic, namely the
top N features from a distributional vector, does
not correspond to a topic generated by a latent
variable model, because it does not have a prob-
ability distribution over words. However, the TC
measures we adopt do not make use of such a
probability distribution except for choosing the top
N words from a topic, which are then treated as an
unordered set for the pairwise operations. New-
man et al. (2010) uses N=10, and Mimno et al.
(2011) uses N=5...20; we investigate a range of N.
</bodyText>
<subsectionHeader confidence="0.993707">
3.3 Vector Negation
</subsectionHeader>
<bodyText confidence="0.9950525">
For removing one topic from another, we draw on
the concept of vector negation (Widdows and Pe-
ters, 2003; Widdows, 2003). Vector negation has
proved useful for modeling word senses in Infor-
mation Retrieval. For example, one might want to
formulate a query for suit NOT lawsuit, which will
retrieve terms such as shirt and jacket and exclude
plaintiff and damages.
We test two versions of vector negation. The
first, Widdows (Widdows, 2003), represents A
NOT B as the projection of A onto B1, the sub-
space orthogonal to B in the vector space V .
Specifically, 111 ≡ {v E V : v · B = 0}. The
formula for Widdows A NOT B is:
</bodyText>
<equation confidence="0.938865">
B� (3)
</equation>
<bodyText confidence="0.9723855">
The second, Strict negation, simply zeros out
any context features of A that are non-zero in B:
</bodyText>
<equation confidence="0.999190333333333">
r0 if fci,B # 0
fci,AnotB ≡Sl (4)
fi,A if fci,B = 0
</equation>
<footnote confidence="0.7748415">
1In our case, Wikipedia is also the source corpus for our
context vectors.
</footnote>
<bodyText confidence="0.999484666666667">
This measure is harsher than Widdows negation,
which decreases the value of common features but
does not remove them completely.
</bodyText>
<subsectionHeader confidence="0.980239">
3.4 Generality Measure
</subsectionHeader>
<bodyText confidence="0.999987333333333">
Herbelot and Ganesalingam (2013) experiment
with hypernym detection using a generality mea-
sure. They measure the Kullback-Leibler (KL) di-
vergence (Eq. 5) between the probability distribu-
tion over context words for a term A, and the back-
ground probability distribution. The idea is that
the greater the KL divergence, the more informa-
tive and therefore specific the term is, while hyper-
nyms are likely to be more general.
</bodyText>
<equation confidence="0.993584">
DKL(p(fi|A)||p(fi)) = Eiln(p(fi|A))p(fi) (5)
p(fi)
</equation>
<bodyText confidence="0.99823775">
Herbelot and Ganesalingam (2013) found that
KL divergence on its own was not sufficient for
successful hypernym detection. We experiment
with it in combination with TC measures.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="method">
4 Methods
</sectionHeader>
<subsectionHeader confidence="0.999715">
4.1 Context Vectors
</subsectionHeader>
<bodyText confidence="0.999902">
We produced context vectors from a 2010
Wikipedia download, lemmatized using morpha
(Minnen et al., 2001). The 10K most frequent lem-
mas in the corpus, minus common stop words and
the 25 most frequent lemmas, served as the context
features. Feature co-occurrences were counted in
a 7-word window around the target lemma (three
words each side of the target lemma), and limited
to intra-sentence co-occurrences.
Co-occurrence counts were weighted using T-
test. We chose T-test because it does not over-
emphasize infrequent features; however, early ex-
periments with Positive PMI weighting showed
the overall performance of our measures to be sim-
ilar with both weighting schemes.
We benchmarked our context vectors on the
WS353 word similarity task (Finkelstein et al.,
2002) and found them to be of comparable accu-
racy with previous literature.
</bodyText>
<table confidence="0.9948912">
Rel Class Target Related Word Total
HYPER alligator animal 638
COORD alligator lizard 1,760
MERO alligator mouth 1,402
RAND-N alligator message 3,253
</table>
<tableCaption confidence="0.961103">
Table 1: Examples from the BLESS subset; num-
ber of tuples per relation in the development set.
</tableCaption>
<figure confidence="0.985619">
A�· B�
|B|2
A NOT B ≡ A�
</figure>
<page confidence="0.991136">
513
</page>
<table confidence="0.999477666666667">
Macroaverage Microaverage
Relation Class Relation Class
Coherence of HYPER MERO COORD RAND-N HYPER MERO COORD RAND-N
TopicA 5.14 ±1.63 5.16 ±1.66 5.13 ±1.63 5.16 ±1.66 5.14 ±1.59 5.37 ±1.56 5.22 ±1.63 5.28 ±1.62
TopicAnotB 3.82 ±1.27 3.86 ±1.02 3.49 ±0.94 5.07 ±1.50 3.88 ±1.73 4.07 ±1.42 3.58 ±1.51 5.17 ±1.64
TopicA-TopicAnotB 1.32 ±1.54 1.30 ±1.28 1.64 ±1.58 0.09 ±0.43 1.26 ±1.86 1.30 ±1.49 1.64 ±1.92 0.11 ±0.83
TopicB 4.97 ±0.58 4.51 ±0.52 5.02 ±0.73 4.49 ±0.24 5.01 ±1.15 4.53 ±1.44 5.07 ±1.63 4.50 ±1.30
TopicBnotA 4.36 ±0.55 3.92 ±0.53 3.33 ±0.67 4.45 ±0.27 4.37 ±1.15 3.89 ±1.32 3.35 ±1.61 4.46 ±1.41
TopicB-TopicBnotA 0.61 ±0.69 0.59 ±0.48 1.68 ±0.88 0.04 ±0.14 0.64 ±1.34 0.64 ±1.33 1.72 ±2.07 0.04 ±0.77
</table>
<tableCaption confidence="0.998292">
Table 2: Average Topic Coherence measures on the development set, using N=10, Strict negation.
</tableCaption>
<subsectionHeader confidence="0.994661">
4.2 Evaluation Dataset
</subsectionHeader>
<bodyText confidence="0.999995954545454">
We used a subset of the BLESS dataset (Baroni
and Lenci, 2011) as defined by Lenci and Benotto
(2012). The entire dataset consists of 200 con-
crete nouns in 17 broad noun classes (e.g. cloth-
ing, amphibian/reptile, vegetable, container), par-
ticipating in a variety of relations. The subset con-
tains the relation classes hypernym (HYPER), co-
ordinate (COORD, i.e. co-hyponym), meronym
(MERO, i.e. part-of), and random-noun (RAND-
N, an unrelated noun). It consists of 14,547 tuples
in total. Table 1 gives an example of each rela-
tion class, along with the total number of tuples
per class in the development data.
Since there was no pre-defined development-
test split for the BLESS subset, we randomly se-
lected half of the data for development. For each
of the 17 broad noun classes, we randomly chose
half of the target nouns, and included all their HY-
PER, COORD, MERO, and RAND-N tuples. This
resulted in a development set consisting of 96 tar-
get nouns and 7,053 tuples; and a test set consist-
ing of 104 nouns and 7,494 tuples.
</bodyText>
<sectionHeader confidence="0.985592" genericHeader="method">
5 Topic Coherence Behavior
</sectionHeader>
<bodyText confidence="0.995603473684211">
We first investigate how topic coherence behaves
across the four relation classes. Table 2 shows
the average values and standard deviation of TC-
related measures on the development data. The
left-hand side gives macro-averages, where values
are first averaged per-class for each target word,
then averaged across the 96 target words in the de-
velopment set. The right-hand side gives micro-
averages across all tuples in the development set.
The micro- and macro-averages are similar, and
we report macro-averages from now on.2
Row 1 of Table 2 shows the original coherence
of topicA, and row 2 the coherence of topicAnotB.
2Lenci and Benotto (2012) also report macro-averages,
but our figures are not comparable to theirs, which are based
on a nearest-neighbor analysis.
Row 3 is simply the difference between the two,
showing the absolute change in coherence. Rows
4-6 are analogous. In general, coherence values
for A and B ranged from the 3’s to the 6’s, with
very high coherence of 7 or 8 and very low coher-
ence of 1 or 2. We did not normalize TC values.
Comparing rows 1 and 4, we see that the B top-
ics are slightly less coherent than the A topics,
probably due to the makeup of the dataset (B terms
include hypernyms and random words, while A
terms are concrete nouns).
Column 1 shows that removing hypernym B
from A results in a decrease in coherence, from
5.14 to 3.82. The difference in coherence, 1.32
in this case, is shown in row 3. Removing A
from B also results in a coherence decrease, but
a much smaller one: only a 0.61 average absolute
decrease. Because the starting coherence values of
A and B may be different, we focus on the amount
of change in coherence when we perform the nega-
tion (rows 3 and 6), rather than the absolute coher-
ence of the negated vectors (rows 2 and 5).
Interestingly, column 2 shows that the be-
haviour of meronyms is almost identical to hyper-
nyms. This is surprising for two reasons: first,
meronyms are intuitively more specific than their
holonyms; and second, previous studies tended to
conflate hypernyms with coordinates rather than
meronyms (Lenci and Benotto, 2012).
Column 3, rows 3 and 6, show that coor-
dinates behave differently from hypernyms and
meronyms. Vector negation in both directions
results in a similar loss of coherence (1.64 and
1.68), reflecting the fact that coordinates have a
symmetrical relationship. The average change is
also greater, although there is a wide variance. In
column 4, the coherence differences for random
nouns are again symmetrical, but in this case very
small, since a randomly selected noun will not
share many contexts with the target word.
We can also define a TC-based similarity mea-
</bodyText>
<page confidence="0.982473">
514
</page>
<table confidence="0.999888">
Relation Class
Measure HYPER MERO COORD RAND-N
TC Meet 5.36 5.12 5.98 3.62
LIN 0.41 0.41 0.48 0.22
GenKLA 4.89 4.89 4.89 4.89
GenKLB 4.60 4.49 5.01 4.95
DiffGenKL 0.29 0.40 -0.12 -0.05
</table>
<tableCaption confidence="0.97545">
Table 3: Average similarity and generality mea-
sures on the dev. set, using N=10, Strict negation.
</tableCaption>
<bodyText confidence="0.999787604651163">
sure. We define A� MEET B� as the intersec-
tion of two vectors, where each feature value
fci,A MEET B ≡ min(fci,A, fci,B). Table 3 shows
TC(A MEET B), with LIN similarity (Lin, 1998)
between A and B for comparison. We expect that
if A and B are similar, their common features
will form a coherent topic. Indeed hypernyms
and meronyms have high values, with coordinates
slightly higher and random nouns much lower.
Table 3 also shows the KL divergence-based
generality measure from Section 3.4. Term B is
slightly more general (lower score) than term A for
hypernyms and meronyms. This may suggest that
meronyms are more general distributionally than
their holonyms, e.g. leg is a holonym of alligator,
but also associated with many other animals.
Table 4 shows the topics for owl and its hyper-
nym creature. Using Strict negation to create owl
NOT creature causes a number of contexts to be
removed from owl: sized, owl, burrow, hawk, typ-
ical, medium, eagle, large, nest. Instead, more
idiosyncratic contexts rise to the top, including
northern, mexican, grouping, and bar (as in an
owl’s markings). These idiosyncratic contexts are
not mutually informative and cause a sizeable de-
crease in TC.
On the other hand, removing owl from crea-
ture does not decrease the coherence nearly as
much. The contexts that are promoted – fantas-
tic, bizarre, fairy – are mutually consistent with
the other creature contexts.
So far our results support Hypothesis 1: remov-
ing B from A decreases its coherence. However,
we hypothesize that this may not be the case for
hypernyms at all levels of generality. Consider-
ing the pair owl-chordate, there is no change from
topicA to topicAnotB. But chordate loses a size-
able amount of coherence when owl is removed;
the topic changes from primitive, ancestral, ances-
tor, evolution, lineage, basal, earliest, fossil, non-,
neural (TC 6.62), to earliest, non-, neural, affinity,
probable, genome, suspected, universally, group,
approximation (TC 3.60).
</bodyText>
<sectionHeader confidence="0.990165" genericHeader="method">
6 Hypernym Detection Measures
</sectionHeader>
<bodyText confidence="0.999977375">
Since we use the same dataset as Lenci and
Benotto (2012), we report the invCL measure in-
troduced in that paper, which outperformed the
other measures reported there, including those of
Weeds and Weir (2003), Weeds et al. (2004), and
Clarke (2009). Let fA be the weight of feature f in
A, and let FA be the set of features with non-zero
weights in A. Then we have:
</bodyText>
<equation confidence="0.999209">
CL(A, B) = EfEFAnFbmin(fA, fB) (6)
EfEFAfA
,I
invCL(A, B) = CL(A, B) * (1 − CL(B, A)) (7)
</equation>
<bodyText confidence="0.9999436">
We also report the balAPinc measure of Kotler-
man et al. (2010), which is not included in the
Lenci and Benotto (2012) evaluation. This mea-
sure begins with APinc, in which the features of A
are ranked by weight, highest to lowest:
</bodyText>
<equation confidence="0.939461333333333">
ErE1...|FA|P(r) * rel(fr)
APinc(A, B) = (8)
|FA|
</equation>
<bodyText confidence="0.998425">
where P(r) is the “precision” at rank r, that is,
how many of B’s features are included at rank r
in the features of A; and rel(fr) is a relevance
feature reflecting how important fr is in B (see
Kotlerman et al. (2010) for details). The balanced
version balAPinc is:
</bodyText>
<equation confidence="0.994442">
,I
balAPinc(A, B) = LIN(A, B) * APinc(A, B) (9)
</equation>
<bodyText confidence="0.998751090909091">
owl (5.19) owl not creature (3.25) creature (5.91) creature not owl (5.09) owl meet creature (4.14)
barn barn mythical mythical small
sized grey -like supernatural large
owl northern strange alien burrow
burrow mexican supernatural legendary night
hawk falcon magical fantastic elf
typical creek alien bizarre little
medium mountains evil aquatic giant
eagle grouping legendary dangerous prey
large bar giant vicious hunt
nest california resemble fairy purple
</bodyText>
<tableCaption confidence="0.992567">
Table 4: Topics from the development data with Topic Coherence values.
</tableCaption>
<page confidence="0.960236">
515
</page>
<table confidence="0.9996205">
N = 5 Widdows 20
10 15
HYPER 1.00 1.00 1.00 1.00
MERO 0.99 1.00 1.00 1.00
COORD 1.02 1.00 1.00 1.01
RAND-N 1.00 1.00 1.00 1.00
Strict
N = 5 10 15 20
HYPER 1.64 1.42 1.23 1.19
MERO 1.91 1.23 1.24 1.20
COORD 1.36 1.15 1.10 1.16
RAND-N 1.08 1.03 1.03 1.02
</table>
<tableCaption confidence="0.999888">
Table 5: RCTC with varying N and neg type.
</tableCaption>
<bodyText confidence="0.999979416666667">
We introduce a new measure, Ratio of Change
in Topic Coherence (RCTC). Based on Section 5,
we expect that for hypernyms the change in coher-
ence from A to AnotB is greater than the change
from B to BnotA. However, we cannot simply use
the ratio (A-AnotB)/(B-BnotA), because the very
small changes in the RAND-N class result in very
small denominators and unstable values. Instead,
we consider two ratios: the magnitude of TC(A)
compared to TC(AnotB), and the magnitude of
TC(B) compared to TC(BnotA). We take the ra-
tio of these figures:
</bodyText>
<sectionHeader confidence="0.745597" genericHeader="method">
RCTC(A, B) =
</sectionHeader>
<bodyText confidence="0.999847222222222">
If topicA is much more coherent than AnotB,
the numerator will be relatively large. If topicB
is not much more coherent than topicBnotA, the
denominator will be relatively small. Both of these
factors encourage RCTC to be larger.3
We also balanced RCTC with three different
factors: LIN similarity, a generality ratio, and
TC(MeetAB). In each case we calculated the bal-
anced value as √RCTC ∗ factor.
</bodyText>
<sectionHeader confidence="0.993953" genericHeader="method">
7 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.99596625">
We first look at the effect of N (topic size) and
negation type on RCTC on the development data
(Table 5). It is clear that RCTC distinguishes rela-
tion types using Strict but not Widdows negation.
We believe this is because, as the “harsher” ver-
sion of negation, it allows less-related features to
rise to the top of the topic and reveal greater dif-
ferences in topic coherence. N=10 was the only
3Although TC values are PMI values, which can be neg-
ative, in practice the median pairwise PMI is almost never
negative, because there tend to be more positive than nega-
tive values among the pairwise comparisons. Therefore, we
have not accounted for sign in the ratio. We have handled
as special cases the few instances where TC(topicAnotB) or
TC(topicBnotA) takes the value of −infinity due to zero co-
occurrences between many of the features.
</bodyText>
<table confidence="0.840089142857143">
invCL bal RCTC RCTC RCTC RCTC
APinc bal bal bal
LIN GEN MEET
HYPER 0.41 0.23 1.37 0.72 1.09 2.62
MERO 0.39 0.22 1.28 0.70 1.06 2.51
COORD 0.38 0.22 1.44 0.71 1.05 2.50
RAND-N 0.25 0.10 1.03 0.46 1.01 1.92
</table>
<tableCaption confidence="0.829874">
Table 6: Hypernym identification on full dataset:
average value by relation.
</tableCaption>
<bodyText confidence="0.9845115">
value that ranked hypernyms the highest; we use
N=10 for the remaining experiments.
We then proceed to hypernym identification on
the full dataset (Table 6). All measures we tested
assigned the highest average value to hypernyms
(in bold) compared to the other relations.
</bodyText>
<subsectionHeader confidence="0.996618">
7.1 Ranking Task
</subsectionHeader>
<bodyText confidence="0.99999278125">
Lenci and Benotto (2012) introduced a ranking
task for hypernym detection on the BLESS data,
which we replicate here. In this task a measure is
used to rank all tuples from the data. The accuracy
of the ranking is assessed from the point of view
of each relation class. The goal is for hypernyms
to have the highest accuracy of all the classes.
We report the Information Retrieval (IR) mea-
sure Mean Average Precision (MAP) for each
class, following Lenci and Benotto (2012). We
also report Mean R-Precision (RPrec), equal to the
precision at rank R where R is the number of ele-
ments in the class. None of the measures we eval-
uated achieves the highest result for hypernyms4,
though invCL consistently performs better for hy-
pernyms than do the other measures (Table 7).
Both MAP and RPrec give more weight to cor-
rect rankings near the top of the list, as is suit-
able for IR applications. In the context of hyper-
nym detection, they could test a system’s ability to
find one or two good-quality hypernyms quickly
from a set of candidates. However, these measures
are less appropriate for testing whether a system
can, in general, rank hypernyms over other rela-
tions. Therefore, we also report Mean Area Un-
der the ROC Curve, or Wilcoxon-Mann-Whitney
statistic (AUC), which gives equal weight to cor-
rect rankings at the top and bottom of the list, and
also compensates for unbalanced data. Table 7
shows that RCTCbalMEET performs identically
to invCL on the AUC measure. This comparison
suggests that invCL is better at placing hypernyms
</bodyText>
<footnote confidence="0.6830215">
4Lenci and Benotto (2012) report a different result, possi-
bly due to the use of different context vectors.
</footnote>
<equation confidence="0.8672968">
TC(topicA)
(10)
TC(topicAnotB)
TC(topicB)
TC(topicBnotA)
</equation>
<page confidence="0.996062">
516
</page>
<table confidence="0.9993225">
invCL balAPinc RCTC RCTC RCTC RCTC
balLIN balGEN balMEET
RPrec Hyper 0.30 0.25 0.17 0.20 0.12 0.19
Mero 0.32 0.29 0.30 0.31 0.21 0.32
Coord 0.39 0.43 0.27 0.42 0.27 0.40
Rand-N 0.18 0.19 0.38 0.16 0.42 0.18
AUC Hyper 0.18 0.17 0.16 0.17 0.14 0.18
Mero 0.31 0.31 0.27 0.31 0.24 0.31
Coord 0.38 0.39 0.25 0.39 0.28 0.37
Rand-N 0.13 0.13 0.32 0.12 0.34 0.15
MAP Hyper 0.35 0.30 0.22 0.24 0.17 0.24
Mero 0.37 0.35 0.35 0.36 0.27 0.37
Coord 0.41 0.46 0.30 0.45 0.32 0.43
Rand-N 0.32 0.32 0.43 0.31 0.46 0.33
</table>
<tableCaption confidence="0.999855">
Table 7: Ranking results. Bold indicates best result for hypernyms by evaluation measure.
</tableCaption>
<bodyText confidence="0.989243666666667">
at the top of the ranking, but over the whole dataset
the two measures rank hypernyms above other tu-
ples equally.
</bodyText>
<subsectionHeader confidence="0.997843">
7.2 Classification Task
</subsectionHeader>
<bodyText confidence="0.99993706060606">
We performed a four-way classification of tuples
by relation class. We used LIBSVM (Chang and
Lin, 2011). As described in Section 4.2, the
BLESS data is unbalanced, with hypernyms – our
target class – making up only about 9% of the
data. To address this imbalance, we used LIB-
SVM’s option to increase the cost associated with
the smaller classes during parameter tuning and
training. We based the weights on the develop-
ment data only (HYPER: 9% of the data, weight
factor 10; MERO: 20% of the data, weight factor
5; COORD: 25% of the data, weight factor 4).
We used LIBSVM’s default Radial Basis Func-
tion kernel. On the development data we per-
formed 10-fold cross-validation. We used LIB-
SVM’s grid.py utility for tuning the parameters C
and γ separately for each fold. We also tuned and
trained models on the development data and tested
them on the test data.
We used four sets of features (Table 8): (1)
invCL on its own; (2) TC features; (3) all features
(invCL, TC, plus additional similarity and gener-
ality measures); and (4) all except TC features.
The results of classification on the development
data are shown in Table 9, and on the test data in
Table 10. Although we report overall accuracy,
this is a poor measure of classificaton quality for
unbalanced data. The tables therefore provide the
Precision, Recall, and F-score by relation class.
The overall accuracy is respectable, although
it can be seen that the hypernym class was the
most difficult to predict, despite weighting the cost
function. Hypernyms may be particularly difficult
</bodyText>
<subsectionHeader confidence="0.836745">
Feature Description
</subsectionHeader>
<equation confidence="0.9473853">
invCL Lenci’s invCL(A, B) (Eq. 7)
topicA TC(A)
topicAnotB TC(B)
diffTopicA TC(A) − TC(A NOT B)
ratioTopicsA TC(A NOT B)/TC(A)
topicB TC(B)
topicBnotA TC(B NOT A)
diffTopicB TC(B) − TC(B NOT A)
ratioTopicsB TC(B NOT A)/TC(B)
topicMeetAB TC(A MEET B)
</equation>
<table confidence="0.9769979375">
ratioTopics1 TC(A NOT B)/TC(B NOT A)
ratioTopics2 diffTopicA / diffTopicB
DiffTopics1 diffTopicA - diffTopicB
DiffTopics2 diffTopicA + diffTopicB
RCTC RCTC(A, B) (Eq. 10)
RCTCbalMEET RCTCbalMEET(A, B)
APinc Kotlerman’s APinc(A, B) (Eq. 8)
balAPinc Kotlerman’s balAPinc(A, B) (Eq. 9)
LIN LIN similarity
genKLA DKL(p(fi|A)||p(fi)) (Eq. 5)
genKLB DKL(p(fi|B)||p(fi)) (Eq. 5)
diffGenKL genKLA - genKLB
ratioGenKL genKLA / genKLB
RCTCbalLIN RCTCbalLIN(A, B)
RCTCbalGEN RCTCbalGEN(A, B)
RCTCbalInvCL RCTC(A, B) bal. with invCL(A, B)
</table>
<tableCaption confidence="0.9729205">
Table 8: Features used in classification experi-
ment. InvCL; TC features; additional features.
</tableCaption>
<bodyText confidence="0.9999345">
to isolate given their similarity to meronyms and
intermediate status between coordinates and ran-
dom nouns on some of the features.
Importantly, while previous work has focused
on single measures such as invCL, the classifica-
tion task highlights a key aspect of the TC ap-
proach. Because we can measure the TC of sev-
eral different vectors for any given tuple (original
terms, negated topics, intersection, etc.) we can
perform multi-way classification much more accu-
rately than with the invCL measure alone. More-
over, the TC features make an important contribu-
tion to the multi-way classification over and above
invCL and other previous similarity and generality
</bodyText>
<page confidence="0.988224">
517
</page>
<table confidence="0.999801411764706">
Feature Set Acc Class P R F
invCL 39.2 Hyper 29.2 19.6 22.5
Mero 25.5 51.7 34.0
Coord 19.3 26.4 21.3
Rand-N 73.5 44.9 55.6
TC Feats 56.7 Hyper 20.3 41.4 27.1
Mero 36.5 48.4 41.4
Coord 66.5 54.5 59.5
Rand-N 87.1 64.7 74.2
All except TC 59.2 Hyper 28.7 19.7 22.9
Mero 35.1 56.2 43.2
Coord 58.2 54.5 56.2
Rand-N 85.5 71.0 77.5
All 64.0 Hyper 30.5 24.4 26.7
Mero 44.9 44.6 44.6
Coord 60.3 65.6 62.8
Rand-N 80.0 79.6 79.7
</table>
<tableCaption confidence="0.926824">
Table 9: Classification results on development
data using 10-fold cross-validation.
</tableCaption>
<table confidence="0.999950117647059">
Feature Set Acc Class P R F
invCL 42.2 Hyper 31.1 19.3 23.8
Mero 32.6 54.3 40.7
Coord 23.1 29.3 25.8
Rand-N 75.8 48.2 59.0
TC Feats 56.2 Hyper 20.0 45.1 27.7
Mero 36.7 42.9 40.0
Coord 64.2 56.5 60.1
Rand-N 88.6 64.5 74.6
All except TC 60.6 Hyper 23.9 17.9 20.5
Mero 38.1 56.4 45.5
Coord 58.2 56.1 57.1
Rand-N 86.5 73.8 79.6
All 63.1 Hyper 33.9 28.6 31.0
Mero 44.1 36.9 40.2
Coord 57.2 64.3 60.6
Rand-N 78.2 81.5 79.8
</table>
<tableCaption confidence="0.987247">
Table 10: Classification results on test data using
development data as training.
</tableCaption>
<bodyText confidence="0.994535125">
measures, with the set of all features yielding the
highest overall accuracy.
Another interesting result is that classification
with the TC features alone results in much higher
recall (though lower precision) for hypernyms
than any of the other feature sets, and on the de-
velopment data (Table 9) results in the highest F-
score for hypernyms.
</bodyText>
<sectionHeader confidence="0.992308" genericHeader="method">
8 Hypernym Depth
</sectionHeader>
<bodyText confidence="0.9999465">
We performed a simple preliminary experiment to
test the speculation that the interaction between
topics depends on the level of generality of the hy-
pernym. Using the WordNet::Similarity package
(Pedersen et al., 2004), we divided the develop-
ment data into bins according to the depth of the
hypernym from the WordNet root node. Table 11
shows average values by hypernym depth.
</bodyText>
<table confidence="0.999853692307692">
D Qty diffA diffB RCTC invCL balAPinc
1 1 0.66 0.27 1.08 0.15 0.01
3 35 0.33 0.16 1.12 0.44 0.23
5 108 0.32 -0.65 1.32 0.33 0.16
6 41 1.21 0.24 1.50 0.44 0.21
7 160 1.45 0.64 1.34 0.44 0.27
8 136 1.30 0.90 1.25 0.35 0.19
9 71 1.37 1.09 1.26 0.41 0.23
10 51 1.90 2.10 2.08 0.41 0.24
11 15 1.85 1.50 1.23 0.48 0.31
12 13 2.08 1.45 1.24 0.28 0.17
13 3 2.49 0.97 1.67 0.27 0.12
14 4 2.02 1.97 1.05 0.27 0.09
</table>
<tableCaption confidence="0.999684">
Table 11: Average value by depth D of hypernym.
</tableCaption>
<bodyText confidence="0.999934">
There is a striking result for diffA, i.e.
TC(topicA) - TC(topicAnotB): the deeper the hy-
pernym in the WordNet hierarchy, the greater the
value. This suggests that more abstract hypernyms
have less interaction with their hyponyms’ topics.
A similar, though less pronounced, effect is seen
for diffB. However, the three measures RCTC,
invCL, and balAPinc remain relatively stable as
the hypernym depth changes. While this is some-
what reassuring, these averages clearly have not
yet captured the difficulty which the DIH encoun-
ters in individual cases such as owl–chordate.
</bodyText>
<sectionHeader confidence="0.997617" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999990222222222">
We have introduced a set of Topic Coherence mea-
sures, particularly the Ratio of Change in Topic
Coherence, to identify hypernyms. These mea-
sures perform comparably to previous hypernym
detection measures on many tasks, while provid-
ing a different view of the relationship between the
distributional vectors of two terms, and contribut-
ing to a more accurate multi-way relation classifi-
cation, especially higher recall for hypernyms.
The approach presented here provides a start-
ing point for entailment measures that do not rely
solely on the Distributional Inclusion Hypothesis.
One issue with the current proposal is that it tests
for a single coherent distributional topic, whereas
multiple senses may be represented in a word’s top
context features. Future work will integrate Word
Sense Disambiguation methods into the Topic Co-
herence based lexical entailment approach.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9786488">
This work is supported by EPSRC grant
EP/I037512/1. We gratefully acknowledge
helpful discussion from Stephen Clark, Tamara
Polajnar, Julie Weeds, Jeremy Reffin, David Weir,
and the anonymous reviewers.
</bodyText>
<page confidence="0.995808">
518
</page>
<sectionHeader confidence="0.995844" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939591397849">
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP workshop on GEMS:
GEometrical Models of natural language Semantics,
pages 1–10, Edinburgh.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of EACL, pages 23–32.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. In Proceedings of *SEM,
pages 11–21, Atlanta, Georgia.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of HLT-EMNLP, pages 628–635, Vancouver.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/∼cjlin/libsvm.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the EACL workshop on GEMS: GEometrical Mod-
els of natural language Semantics, pages 112–119,
Athens.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20:116–131.
Dan Garrette, Katrin Erk, and Raymond Mooney.
2011. Integrating logical representations with prob-
abilistic information using Markov Logic. In Pro-
ceedings of IWCS, Oxford, UK.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL, Michigan.
Aur´elie Herbelot and Mohan Ganesalingam. 2013.
Measuring semantic content in distributional vec-
tors. In Proceedings of ACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16:359–389.
Alessandro Lenci and Giuli Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of *SEM, pages 75–79, Montreal.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of ICML, Madi-
son, Wisconson.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of EMNLP, pages 262–272, Edinburgh.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207–223.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In Proceedings of NAACL, pages 100–
108, Los Angeles, California.
Ted Pedersen, Siddarth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - measuring the
relatedness of concepts. In Proceedings of NAACL
(Demonstration System), pages 38–41, Boston, MA.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Butler. 2012. Exploring topic coher-
ence over many models and many topics. In Pro-
ceedings of EMNLP, pages 952–961, Jeju Island,
Korea.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COL-
ING, Manchester, UK.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP, pages 81–88, Sapporo, Japan.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015–
1021, Geneva.
Dominic Widdows and Stanley Peters. 2003. Word
vectors and quantum logic. In Proceedings of
the Eight Mathematics of Language Conference,
Bloomington, Indiana.
Dominic Widdows. 2003. Orthogonal negation in vec-
tor spaces for modelling word-meanings and docu-
ment retrieval. In Proceedings of ACL, pages 136–
143, Sapporo, Japan.
</reference>
<page confidence="0.998488">
519
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.773378">
<title confidence="0.976864">Distributional Lexical Entailment by Topic Coherence</title>
<author confidence="0.9848">Laura</author>
<affiliation confidence="0.94407">University of Computer</affiliation>
<email confidence="0.995095">laura.rimell@cl.cam.ac.uk</email>
<abstract confidence="0.993999705882353">Automatic detection of lexical entailment, or hypernym detection, is an important NLP task. Recent hypernym detection measures have been based on the Distributional Inclusion Hypothesis (DIH). This paper assumes that the DIH sometimes fails, and investigates other ways of quantifying the relationship between the cooccurrence contexts of two terms. We consider the top features in a context vector as a topic, and introduce a new entailment detection measure based on Topic Coherence (TC). Our measure successfully detects hypernyms, and a TC-based family of measures contributes to multi-way relation classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP workshop on GEMS: GEometrical Models of natural language Semantics,</booktitle>
<pages>1--10</pages>
<location>Edinburgh.</location>
<contexts>
<context position="5637" citStr="Baroni and Lenci, 2011" startWordPosition="908" endWordPosition="911">arity measures such as LIN similarity (Lin, 1998), to ensure that A and B are semantically related, since unrelated pairs that differ in frequency can mimic feature inclusion. Previous distributional approaches to hypernym detection have generally involved a single measure, designed to rank hypernyms above other relation classes. Evaluation has largely involved either ranking or binary classification tasks, and there has been little work on using a variety of measures to distinguish multiple relation classes. Lenci and Benotto (2012) perform a ranking task using the multi-class BLESS dataset (Baroni and Lenci, 2011), but not a classification. We perform a multi-way classification using a variety of Topic Coherence measures. Recent Semantic Relation Classification shared tasks (SemEval-2010 Task 8, SemEval-2012 Task 2) are also relevant, though the relation classes and approaches have differed. 3 Topic Coherence for Distributional Lexical Entailment The intuition behind our approach is to investigate whether term A, the candidate hyponym, has a coherent topic reflected in its distributional features, which apply only to A and not to its hypernym B. Consider A=beer, B=beverage. They may share features such</context>
<context position="13420" citStr="Baroni and Lenci, 2011" startWordPosition="2238" endWordPosition="2241">.73 4.07 ±1.42 3.58 ±1.51 5.17 ±1.64 TopicA-TopicAnotB 1.32 ±1.54 1.30 ±1.28 1.64 ±1.58 0.09 ±0.43 1.26 ±1.86 1.30 ±1.49 1.64 ±1.92 0.11 ±0.83 TopicB 4.97 ±0.58 4.51 ±0.52 5.02 ±0.73 4.49 ±0.24 5.01 ±1.15 4.53 ±1.44 5.07 ±1.63 4.50 ±1.30 TopicBnotA 4.36 ±0.55 3.92 ±0.53 3.33 ±0.67 4.45 ±0.27 4.37 ±1.15 3.89 ±1.32 3.35 ±1.61 4.46 ±1.41 TopicB-TopicBnotA 0.61 ±0.69 0.59 ±0.48 1.68 ±0.88 0.04 ±0.14 0.64 ±1.34 0.64 ±1.33 1.72 ±2.07 0.04 ±0.77 Table 2: Average Topic Coherence measures on the development set, using N=10, Strict negation. 4.2 Evaluation Dataset We used a subset of the BLESS dataset (Baroni and Lenci, 2011) as defined by Lenci and Benotto (2012). The entire dataset consists of 200 concrete nouns in 17 broad noun classes (e.g. clothing, amphibian/reptile, vegetable, container), participating in a variety of relations. The subset contains the relation classes hypernym (HYPER), coordinate (COORD, i.e. co-hyponym), meronym (MERO, i.e. part-of), and random-noun (RANDN, an unrelated noun). It consists of 14,547 tuples in total. Table 1 gives an example of each relation class, along with the total number of tuples per class in the development data. Since there was no pre-defined developmenttest split f</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the EMNLP workshop on GEMS: GEometrical Models of natural language Semantics, pages 1–10, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>23--32</pages>
<contexts>
<context position="1156" citStr="Baroni et al., 2012" startWordPosition="168" endWordPosition="171">vector as a topic, and introduce a new entailment detection measure based on Topic Coherence (TC). Our measure successfully detects hypernyms, and a TC-based family of measures contributes to multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also cont</context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of EACL, pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Montague meets markov: Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>11--21</pages>
<location>Atlanta,</location>
<contexts>
<context position="1179" citStr="Beltagy et al., 2013" startWordPosition="172" endWordPosition="175">d introduce a new entailment detection measure based on Topic Coherence (TC). Our measure successfully detects hypernyms, and a TC-based family of measures contributes to multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also contexts of animal. Existin</context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. Montague meets markov: Deep semantics with probabilistic logical form. In Proceedings of *SEM, pages 11–21, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>628--635</pages>
<location>Vancouver.</location>
<contexts>
<context position="1112" citStr="Bos and Markert, 2005" startWordPosition="160" endWordPosition="163">ms. We consider the top features in a context vector as a topic, and introduce a new entailment detection measure based on Topic Coherence (TC). Our measure successfully detects hypernyms, and a TC-based family of measures contributes to multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include</context>
<context position="4103" citStr="Bos and Markert, 2005" startWordPosition="664" endWordPosition="667">curacy, and a family of Topic Coherence measures is used to perform a multi-way classification of tuples by relation class. Finally, we investigate how the level of generality of a hypernym affects entailment measures. 511 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benot</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of HLT-EMNLP, pages 628–635, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/∼cjlin/libsvm.</note>
<contexts>
<context position="26179" citStr="Chang and Lin, 2011" startWordPosition="4429" endWordPosition="4432">r 0.18 0.17 0.16 0.17 0.14 0.18 Mero 0.31 0.31 0.27 0.31 0.24 0.31 Coord 0.38 0.39 0.25 0.39 0.28 0.37 Rand-N 0.13 0.13 0.32 0.12 0.34 0.15 MAP Hyper 0.35 0.30 0.22 0.24 0.17 0.24 Mero 0.37 0.35 0.35 0.36 0.27 0.37 Coord 0.41 0.46 0.30 0.45 0.32 0.43 Rand-N 0.32 0.32 0.43 0.31 0.46 0.33 Table 7: Ranking results. Bold indicates best result for hypernyms by evaluation measure. at the top of the ranking, but over the whole dataset the two measures rank hypernyms above other tuples equally. 7.2 Classification Task We performed a four-way classification of tuples by relation class. We used LIBSVM (Chang and Lin, 2011). As described in Section 4.2, the BLESS data is unbalanced, with hypernyms – our target class – making up only about 9% of the data. To address this imbalance, we used LIBSVM’s option to increase the cost associated with the smaller classes during parameter tuning and training. We based the weights on the development data only (HYPER: 9% of the data, weight factor 10; MERO: 20% of the data, weight factor 5; COORD: 25% of the data, weight factor 4). We used LIBSVM’s default Radial Basis Function kernel. On the development data we performed 10-fold cross-validation. We used LIBSVM’s grid.py uti</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/∼cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: an overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL workshop on GEMS: GEometrical Models of natural language Semantics,</booktitle>
<pages>112--119</pages>
<location>Athens.</location>
<contexts>
<context position="1327" citStr="Clarke, 2009" startWordPosition="198" endWordPosition="199">ontributes to multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also contexts of animal. Existing measures look at the amount of overlap between the co-occurrences of A and B, in order to judge whether B is a hypernym of A. The motivation for t</context>
<context position="4306" citStr="Clarke, 2009" startWordPosition="697" endWordPosition="698">ures. 511 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of these feature inclusion measures has provided general support</context>
<context position="19620" citStr="Clarke (2009)" startWordPosition="3287" endWordPosition="3288">ge from topicA to topicAnotB. But chordate loses a sizeable amount of coherence when owl is removed; the topic changes from primitive, ancestral, ancestor, evolution, lineage, basal, earliest, fossil, non-, neural (TC 6.62), to earliest, non-, neural, affinity, probable, genome, suspected, universally, group, approximation (TC 3.60). 6 Hypernym Detection Measures Since we use the same dataset as Lenci and Benotto (2012), we report the invCL measure introduced in that paper, which outperformed the other measures reported there, including those of Weeds and Weir (2003), Weeds et al. (2004), and Clarke (2009). Let fA be the weight of feature f in A, and let FA be the set of features with non-zero weights in A. Then we have: CL(A, B) = EfEFAnFbmin(fA, fB) (6) EfEFAfA ,I invCL(A, B) = CL(A, B) * (1 − CL(B, A)) (7) We also report the balAPinc measure of Kotlerman et al. (2010), which is not included in the Lenci and Benotto (2012) evaluation. This measure begins with APinc, in which the features of A are ranked by weight, highest to lowest: ErE1...|FA|P(r) * rel(fr) APinc(A, B) = (8) |FA| where P(r) is the “precision” at rank r, that is, how many of B’s features are included at rank r in the features</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the EACL workshop on GEMS: GEometrical Models of natural language Semantics, pages 112–119, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>20--116</pages>
<contexts>
<context position="12178" citStr="Finkelstein et al., 2002" startWordPosition="2031" endWordPosition="2034">orpus, minus common stop words and the 25 most frequent lemmas, served as the context features. Feature co-occurrences were counted in a 7-word window around the target lemma (three words each side of the target lemma), and limited to intra-sentence co-occurrences. Co-occurrence counts were weighted using Ttest. We chose T-test because it does not overemphasize infrequent features; however, early experiments with Positive PMI weighting showed the overall performance of our measures to be similar with both weighting schemes. We benchmarked our context vectors on the WS353 word similarity task (Finkelstein et al., 2002) and found them to be of comparable accuracy with previous literature. Rel Class Target Related Word Total HYPER alligator animal 638 COORD alligator lizard 1,760 MERO alligator mouth 1,402 RAND-N alligator message 3,253 Table 1: Examples from the BLESS subset; number of tuples per relation in the development set. A�· B� |B|2 A NOT B ≡ A� 513 Macroaverage Microaverage Relation Class Relation Class Coherence of HYPER MERO COORD RAND-N HYPER MERO COORD RAND-N TopicA 5.14 ±1.63 5.16 ±1.66 5.13 ±1.63 5.16 ±1.66 5.14 ±1.59 5.37 ±1.56 5.22 ±1.63 5.28 ±1.62 TopicAnotB 3.82 ±1.27 3.86 ±1.02 3.49 ±0.94</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20:116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Integrating logical representations with probabilistic information using Markov Logic.</title>
<date>2011</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<location>Oxford, UK.</location>
<contexts>
<context position="1135" citStr="Garrette et al., 2011" startWordPosition="164" endWordPosition="167"> features in a context vector as a topic, and introduce a new entailment detection measure based on Topic Coherence (TC). Our measure successfully detects hypernyms, and a TC-based family of measures contributes to multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food,</context>
</contexts>
<marker>Garrette, Erk, Mooney, 2011</marker>
<rawString>Dan Garrette, Katrin Erk, and Raymond Mooney. 2011. Integrating logical representations with probabilistic information using Markov Logic. In Proceedings of IWCS, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Michigan.</location>
<contexts>
<context position="1473" citStr="Geffet and Dagan, 2005" startWordPosition="217" endWordPosition="220">ails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also contexts of animal. Existing measures look at the amount of overlap between the co-occurrences of A and B, in order to judge whether B is a hypernym of A. The motivation for the present paper is the wellknown fact that the DIH is not fully correct. There are many reasons why a hyponym might occur in contexts where its h</context>
<context position="4292" citStr="Geffet and Dagan, 2005" startWordPosition="693" endWordPosition="696"> affects entailment measures. 511 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of these feature inclusion measures has provided g</context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>M. Geffet and I. Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of ACL, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aur´elie Herbelot</author>
<author>Mohan Ganesalingam</author>
</authors>
<title>Measuring semantic content in distributional vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10761" citStr="Herbelot and Ganesalingam (2013)" startWordPosition="1809" endWordPosition="1812"> The first, Widdows (Widdows, 2003), represents A NOT B as the projection of A onto B1, the subspace orthogonal to B in the vector space V . Specifically, 111 ≡ {v E V : v · B = 0}. The formula for Widdows A NOT B is: B� (3) The second, Strict negation, simply zeros out any context features of A that are non-zero in B: r0 if fci,B # 0 fci,AnotB ≡Sl (4) fi,A if fci,B = 0 1In our case, Wikipedia is also the source corpus for our context vectors. This measure is harsher than Widdows negation, which decreases the value of common features but does not remove them completely. 3.4 Generality Measure Herbelot and Ganesalingam (2013) experiment with hypernym detection using a generality measure. They measure the Kullback-Leibler (KL) divergence (Eq. 5) between the probability distribution over context words for a term A, and the background probability distribution. The idea is that the greater the KL divergence, the more informative and therefore specific the term is, while hypernyms are likely to be more general. DKL(p(fi|A)||p(fi)) = Eiln(p(fi|A))p(fi) (5) p(fi) Herbelot and Ganesalingam (2013) found that KL divergence on its own was not sufficient for successful hypernym detection. We experiment with it in combination </context>
</contexts>
<marker>Herbelot, Ganesalingam, 2013</marker>
<rawString>Aur´elie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<pages>16--359</pages>
<contexts>
<context position="1351" citStr="Kotlerman et al., 2010" startWordPosition="200" endWordPosition="203">multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also contexts of animal. Existing measures look at the amount of overlap between the co-occurrences of A and B, in order to judge whether B is a hypernym of A. The motivation for the present paper is the </context>
<context position="4330" citStr="Kotlerman et al., 2010" startWordPosition="699" endWordPosition="702">eedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of these feature inclusion measures has provided general support for the DIH. Following </context>
<context position="19890" citStr="Kotlerman et al. (2010)" startWordPosition="3341" endWordPosition="3345">, probable, genome, suspected, universally, group, approximation (TC 3.60). 6 Hypernym Detection Measures Since we use the same dataset as Lenci and Benotto (2012), we report the invCL measure introduced in that paper, which outperformed the other measures reported there, including those of Weeds and Weir (2003), Weeds et al. (2004), and Clarke (2009). Let fA be the weight of feature f in A, and let FA be the set of features with non-zero weights in A. Then we have: CL(A, B) = EfEFAnFbmin(fA, fB) (6) EfEFAfA ,I invCL(A, B) = CL(A, B) * (1 − CL(B, A)) (7) We also report the balAPinc measure of Kotlerman et al. (2010), which is not included in the Lenci and Benotto (2012) evaluation. This measure begins with APinc, in which the features of A are ranked by weight, highest to lowest: ErE1...|FA|P(r) * rel(fr) APinc(A, B) = (8) |FA| where P(r) is the “precision” at rank r, that is, how many of B’s features are included at rank r in the features of A; and rel(fr) is a relevance feature reflecting how important fr is in B (see Kotlerman et al. (2010) for details). The balanced version balAPinc is: ,I balAPinc(A, B) = LIN(A, B) * APinc(A, B) (9) owl (5.19) owl not creature (3.25) creature (5.91) creature not owl</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16:359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giuli Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>75--79</pages>
<location>Montreal.</location>
<contexts>
<context position="1377" citStr="Lenci and Benotto, 2012" startWordPosition="204" endWordPosition="207">ification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also contexts of animal. Existing measures look at the amount of overlap between the co-occurrences of A and B, in order to judge whether B is a hypernym of A. The motivation for the present paper is the wellknown fact that the DI</context>
<context position="4356" citStr="Lenci and Benotto, 2012" startWordPosition="703" endWordPosition="706">erence of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of these feature inclusion measures has provided general support for the DIH. Following Szpektor and Dagan (2008),</context>
<context position="13459" citStr="Lenci and Benotto (2012)" startWordPosition="2245" endWordPosition="2248">opicA-TopicAnotB 1.32 ±1.54 1.30 ±1.28 1.64 ±1.58 0.09 ±0.43 1.26 ±1.86 1.30 ±1.49 1.64 ±1.92 0.11 ±0.83 TopicB 4.97 ±0.58 4.51 ±0.52 5.02 ±0.73 4.49 ±0.24 5.01 ±1.15 4.53 ±1.44 5.07 ±1.63 4.50 ±1.30 TopicBnotA 4.36 ±0.55 3.92 ±0.53 3.33 ±0.67 4.45 ±0.27 4.37 ±1.15 3.89 ±1.32 3.35 ±1.61 4.46 ±1.41 TopicB-TopicBnotA 0.61 ±0.69 0.59 ±0.48 1.68 ±0.88 0.04 ±0.14 0.64 ±1.34 0.64 ±1.33 1.72 ±2.07 0.04 ±0.77 Table 2: Average Topic Coherence measures on the development set, using N=10, Strict negation. 4.2 Evaluation Dataset We used a subset of the BLESS dataset (Baroni and Lenci, 2011) as defined by Lenci and Benotto (2012). The entire dataset consists of 200 concrete nouns in 17 broad noun classes (e.g. clothing, amphibian/reptile, vegetable, container), participating in a variety of relations. The subset contains the relation classes hypernym (HYPER), coordinate (COORD, i.e. co-hyponym), meronym (MERO, i.e. part-of), and random-noun (RANDN, an unrelated noun). It consists of 14,547 tuples in total. Table 1 gives an example of each relation class, along with the total number of tuples per class in the development data. Since there was no pre-defined developmenttest split for the BLESS subset, we randomly select</context>
<context position="15053" citStr="Lenci and Benotto (2012)" startWordPosition="2511" endWordPosition="2514">how topic coherence behaves across the four relation classes. Table 2 shows the average values and standard deviation of TCrelated measures on the development data. The left-hand side gives macro-averages, where values are first averaged per-class for each target word, then averaged across the 96 target words in the development set. The right-hand side gives microaverages across all tuples in the development set. The micro- and macro-averages are similar, and we report macro-averages from now on.2 Row 1 of Table 2 shows the original coherence of topicA, and row 2 the coherence of topicAnotB. 2Lenci and Benotto (2012) also report macro-averages, but our figures are not comparable to theirs, which are based on a nearest-neighbor analysis. Row 3 is simply the difference between the two, showing the absolute change in coherence. Rows 4-6 are analogous. In general, coherence values for A and B ranged from the 3’s to the 6’s, with very high coherence of 7 or 8 and very low coherence of 1 or 2. We did not normalize TC values. Comparing rows 1 and 4, we see that the B topics are slightly less coherent than the A topics, probably due to the makeup of the dataset (B terms include hypernyms and random words, while A</context>
<context position="16520" citStr="Lenci and Benotto, 2012" startWordPosition="2773" endWordPosition="2776">decrease, but a much smaller one: only a 0.61 average absolute decrease. Because the starting coherence values of A and B may be different, we focus on the amount of change in coherence when we perform the negation (rows 3 and 6), rather than the absolute coherence of the negated vectors (rows 2 and 5). Interestingly, column 2 shows that the behaviour of meronyms is almost identical to hypernyms. This is surprising for two reasons: first, meronyms are intuitively more specific than their holonyms; and second, previous studies tended to conflate hypernyms with coordinates rather than meronyms (Lenci and Benotto, 2012). Column 3, rows 3 and 6, show that coordinates behave differently from hypernyms and meronyms. Vector negation in both directions results in a similar loss of coherence (1.64 and 1.68), reflecting the fact that coordinates have a symmetrical relationship. The average change is also greater, although there is a wide variance. In column 4, the coherence differences for random nouns are again symmetrical, but in this case very small, since a randomly selected noun will not share many contexts with the target word. We can also define a TC-based similarity mea514 Relation Class Measure HYPER MERO </context>
<context position="19430" citStr="Lenci and Benotto (2012)" startWordPosition="3254" endWordPosition="3257">thesis 1: removing B from A decreases its coherence. However, we hypothesize that this may not be the case for hypernyms at all levels of generality. Considering the pair owl-chordate, there is no change from topicA to topicAnotB. But chordate loses a sizeable amount of coherence when owl is removed; the topic changes from primitive, ancestral, ancestor, evolution, lineage, basal, earliest, fossil, non-, neural (TC 6.62), to earliest, non-, neural, affinity, probable, genome, suspected, universally, group, approximation (TC 3.60). 6 Hypernym Detection Measures Since we use the same dataset as Lenci and Benotto (2012), we report the invCL measure introduced in that paper, which outperformed the other measures reported there, including those of Weeds and Weir (2003), Weeds et al. (2004), and Clarke (2009). Let fA be the weight of feature f in A, and let FA be the set of features with non-zero weights in A. Then we have: CL(A, B) = EfEFAnFbmin(fA, fB) (6) EfEFAfA ,I invCL(A, B) = CL(A, B) * (1 − CL(B, A)) (7) We also report the balAPinc measure of Kotlerman et al. (2010), which is not included in the Lenci and Benotto (2012) evaluation. This measure begins with APinc, in which the features of A are ranked by</context>
<context position="23667" citStr="Lenci and Benotto (2012)" startWordPosition="3994" endWordPosition="3997">etween many of the features. invCL bal RCTC RCTC RCTC RCTC APinc bal bal bal LIN GEN MEET HYPER 0.41 0.23 1.37 0.72 1.09 2.62 MERO 0.39 0.22 1.28 0.70 1.06 2.51 COORD 0.38 0.22 1.44 0.71 1.05 2.50 RAND-N 0.25 0.10 1.03 0.46 1.01 1.92 Table 6: Hypernym identification on full dataset: average value by relation. value that ranked hypernyms the highest; we use N=10 for the remaining experiments. We then proceed to hypernym identification on the full dataset (Table 6). All measures we tested assigned the highest average value to hypernyms (in bold) compared to the other relations. 7.1 Ranking Task Lenci and Benotto (2012) introduced a ranking task for hypernym detection on the BLESS data, which we replicate here. In this task a measure is used to rank all tuples from the data. The accuracy of the ranking is assessed from the point of view of each relation class. The goal is for hypernyms to have the highest accuracy of all the classes. We report the Information Retrieval (IR) measure Mean Average Precision (MAP) for each class, following Lenci and Benotto (2012). We also report Mean R-Precision (RPrec), equal to the precision at rank R where R is the number of elements in the class. None of the measures we eva</context>
<context position="25201" citStr="Lenci and Benotto (2012)" startWordPosition="4259" endWordPosition="4262">d test a system’s ability to find one or two good-quality hypernyms quickly from a set of candidates. However, these measures are less appropriate for testing whether a system can, in general, rank hypernyms over other relations. Therefore, we also report Mean Area Under the ROC Curve, or Wilcoxon-Mann-Whitney statistic (AUC), which gives equal weight to correct rankings at the top and bottom of the list, and also compensates for unbalanced data. Table 7 shows that RCTCbalMEET performs identically to invCL on the AUC measure. This comparison suggests that invCL is better at placing hypernyms 4Lenci and Benotto (2012) report a different result, possibly due to the use of different context vectors. TC(topicA) (10) TC(topicAnotB) TC(topicB) TC(topicBnotA) 516 invCL balAPinc RCTC RCTC RCTC RCTC balLIN balGEN balMEET RPrec Hyper 0.30 0.25 0.17 0.20 0.12 0.19 Mero 0.32 0.29 0.30 0.31 0.21 0.32 Coord 0.39 0.43 0.27 0.42 0.27 0.40 Rand-N 0.18 0.19 0.38 0.16 0.42 0.18 AUC Hyper 0.18 0.17 0.16 0.17 0.14 0.18 Mero 0.31 0.31 0.27 0.31 0.24 0.31 Coord 0.38 0.39 0.25 0.39 0.28 0.37 Rand-N 0.13 0.13 0.32 0.12 0.34 0.15 MAP Hyper 0.35 0.30 0.22 0.24 0.17 0.24 Mero 0.37 0.35 0.35 0.36 0.27 0.37 Coord 0.41 0.46 0.30 0.45 0</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giuli Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of *SEM, pages 75–79, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of ICML,</booktitle>
<location>Madison, Wisconson.</location>
<contexts>
<context position="5063" citStr="Lin, 1998" startWordPosition="823" endWordPosition="824">ose of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of these feature inclusion measures has provided general support for the DIH. Following Szpektor and Dagan (2008), inclusion measures are also sometimes balanced with similarity measures such as LIN similarity (Lin, 1998), to ensure that A and B are semantically related, since unrelated pairs that differ in frequency can mimic feature inclusion. Previous distributional approaches to hypernym detection have generally involved a single measure, designed to rank hypernyms above other relation classes. Evaluation has largely involved either ranking or binary classification tasks, and there has been little work on using a variety of measures to distinguish multiple relation classes. Lenci and Benotto (2012) perform a ranking task using the multi-class BLESS dataset (Baroni and Lenci, 2011), but not a classification</context>
<context position="17550" citStr="Lin, 1998" startWordPosition="2951" endWordPosition="2952">ase very small, since a randomly selected noun will not share many contexts with the target word. We can also define a TC-based similarity mea514 Relation Class Measure HYPER MERO COORD RAND-N TC Meet 5.36 5.12 5.98 3.62 LIN 0.41 0.41 0.48 0.22 GenKLA 4.89 4.89 4.89 4.89 GenKLB 4.60 4.49 5.01 4.95 DiffGenKL 0.29 0.40 -0.12 -0.05 Table 3: Average similarity and generality measures on the dev. set, using N=10, Strict negation. sure. We define A� MEET B� as the intersection of two vectors, where each feature value fci,A MEET B ≡ min(fci,A, fci,B). Table 3 shows TC(A MEET B), with LIN similarity (Lin, 1998) between A and B for comparison. We expect that if A and B are similar, their common features will form a coherent topic. Indeed hypernyms and meronyms have high values, with coordinates slightly higher and random nouns much lower. Table 3 also shows the KL divergence-based generality measure from Section 3.4. Term B is slightly more general (lower score) than term A for hypernyms and meronyms. This may suggest that meronyms are more general distributionally than their holonyms, e.g. leg is a holonym of alligator, but also associated with many other animals. Table 4 shows the topics for owl an</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of ICML, Madison, Wisconson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="4003" citStr="Miller, 1995" startWordPosition="649" endWordPosition="650"> (RCTC), for detecting lexical entailment. The measure detects hypernyms with reasonable accuracy, and a family of Topic Coherence measures is used to perform a multi-way classification of tuples by relation class. Finally, we investigate how the level of generality of a hypernym affects entailment measures. 511 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>262--272</pages>
<location>Edinburgh.</location>
<contexts>
<context position="3100" citStr="Mimno et al., 2011" startWordPosition="511" endWordPosition="514">eneral a hypernym becomes – up to the level of WordNet root elements, such as entity – its predominant sense ceases to correspond to the sense intended in hyponymhypernym chains. Thus we never hear about going to visit an entity at the zoo. This paper starts from the assumption that the DIH sometimes fails, and investigates not the amount of containment of A’s features in B’s features, but rather the nature of the non-contained features. We consider the top features of a distributional vector as a topic, and use recent measures for automatically measuring Topic Coherence (Newman et al., 2010; Mimno et al., 2011) to evaluate how the topics change under various conditions. Using a notion of vector negation, we investigate whether the distributional topic of e.g. lion becomes more or less coherent when we subtract the contexts of animal. We introduce a new measure, Ratio of Change in Topic Coherence (RCTC), for detecting lexical entailment. The measure detects hypernyms with reasonable accuracy, and a family of Topic Coherence measures is used to perform a multi-way classification of tuples by relation class. Finally, we investigate how the level of generality of a hypernym affects entailment measures. </context>
<context position="8525" citStr="Mimno et al., 2011" startWordPosition="1406" endWordPosition="1409">being more specific, occurs in a highly coherent set of contexts where B does not. Hypothesis 2 is inconsistent with the DIH, since it imples that a hyponym always has specific features which the hypernym does not share. As a corollary, removing hyponym A’s features from hypernym B might decrease the coherence of topicB, if removing specific features leaves only more general, less informative features behind. 3.2 Topic Coherence Measure We use a Topic Coherence (TC) measure from recent work on automatic evalution of topics generated from corpora by latent variable models (Newman et al., 2010; Mimno et al., 2011; Stevens et 512 al., 2012). TC measures are applied to the top N words from a generated topic. They assign pairwise relatedness scores to the words, and return the mean or median from the word-pair scores. We adopt the best method from Newman et al. (2010), equal to the median pairwise Pointwise Mutual Information (PMI) of the top N words, using Wikipedia as a background corpus for PMI.1 The measure is given in Equation (1): TC({cj}) = median(PMI(ci, ck), i, k E 1...N, i &lt; k) where {cj} is the topic, and PMI is defined as: (1) PMI(ci, ck) = log p(ci, ck) p(ci)p(ck) (2) We use intra-sentence c</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In Proceedings of EMNLP, pages 262–272, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="11514" citStr="Minnen et al., 2001" startWordPosition="1926" endWordPosition="1929"> probability distribution over context words for a term A, and the background probability distribution. The idea is that the greater the KL divergence, the more informative and therefore specific the term is, while hypernyms are likely to be more general. DKL(p(fi|A)||p(fi)) = Eiln(p(fi|A))p(fi) (5) p(fi) Herbelot and Ganesalingam (2013) found that KL divergence on its own was not sufficient for successful hypernym detection. We experiment with it in combination with TC measures. 4 Methods 4.1 Context Vectors We produced context vectors from a 2010 Wikipedia download, lemmatized using morpha (Minnen et al., 2001). The 10K most frequent lemmas in the corpus, minus common stop words and the 25 most frequent lemmas, served as the context features. Feature co-occurrences were counted in a 7-word window around the target lemma (three words each side of the target lemma), and limited to intra-sentence co-occurrences. Co-occurrence counts were weighted using Ttest. We chose T-test because it does not overemphasize infrequent features; however, early experiments with Positive PMI weighting showed the overall performance of our measures to be similar with both weighting schemes. We benchmarked our context vect</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>100--108</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="3079" citStr="Newman et al., 2010" startWordPosition="507" endWordPosition="510"> Moreover, the more general a hypernym becomes – up to the level of WordNet root elements, such as entity – its predominant sense ceases to correspond to the sense intended in hyponymhypernym chains. Thus we never hear about going to visit an entity at the zoo. This paper starts from the assumption that the DIH sometimes fails, and investigates not the amount of containment of A’s features in B’s features, but rather the nature of the non-contained features. We consider the top features of a distributional vector as a topic, and use recent measures for automatically measuring Topic Coherence (Newman et al., 2010; Mimno et al., 2011) to evaluate how the topics change under various conditions. Using a notion of vector negation, we investigate whether the distributional topic of e.g. lion becomes more or less coherent when we subtract the contexts of animal. We introduce a new measure, Ratio of Change in Topic Coherence (RCTC), for detecting lexical entailment. The measure detects hypernyms with reasonable accuracy, and a family of Topic Coherence measures is used to perform a multi-way classification of tuples by relation class. Finally, we investigate how the level of generality of a hypernym affects </context>
<context position="8505" citStr="Newman et al., 2010" startWordPosition="1401" endWordPosition="1405">haps A, by virtue of being more specific, occurs in a highly coherent set of contexts where B does not. Hypothesis 2 is inconsistent with the DIH, since it imples that a hyponym always has specific features which the hypernym does not share. As a corollary, removing hyponym A’s features from hypernym B might decrease the coherence of topicB, if removing specific features leaves only more general, less informative features behind. 3.2 Topic Coherence Measure We use a Topic Coherence (TC) measure from recent work on automatic evalution of topics generated from corpora by latent variable models (Newman et al., 2010; Mimno et al., 2011; Stevens et 512 al., 2012). TC measures are applied to the top N words from a generated topic. They assign pairwise relatedness scores to the words, and return the mean or median from the word-pair scores. We adopt the best method from Newman et al. (2010), equal to the median pairwise Pointwise Mutual Information (PMI) of the top N words, using Wikipedia as a background corpus for PMI.1 The measure is given in Equation (1): TC({cj}) = median(PMI(ci, ck), i, k E 1...N, i &lt; k) where {cj} is the topic, and PMI is defined as: (1) PMI(ci, ck) = log p(ci, ck) p(ci)p(ck) (2) We </context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Proceedings of NAACL, pages 100– 108, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddarth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL (Demonstration System),</booktitle>
<pages>38--41</pages>
<location>Boston, MA.</location>
<contexts>
<context position="30759" citStr="Pedersen et al., 2004" startWordPosition="5186" endWordPosition="5189">st data using development data as training. measures, with the set of all features yielding the highest overall accuracy. Another interesting result is that classification with the TC features alone results in much higher recall (though lower precision) for hypernyms than any of the other feature sets, and on the development data (Table 9) results in the highest Fscore for hypernyms. 8 Hypernym Depth We performed a simple preliminary experiment to test the speculation that the interaction between topics depends on the level of generality of the hypernym. Using the WordNet::Similarity package (Pedersen et al., 2004), we divided the development data into bins according to the depth of the hypernym from the WordNet root node. Table 11 shows average values by hypernym depth. D Qty diffA diffB RCTC invCL balAPinc 1 1 0.66 0.27 1.08 0.15 0.01 3 35 0.33 0.16 1.12 0.44 0.23 5 108 0.32 -0.65 1.32 0.33 0.16 6 41 1.21 0.24 1.50 0.44 0.21 7 160 1.45 0.64 1.34 0.44 0.27 8 136 1.30 0.90 1.25 0.35 0.19 9 71 1.37 1.09 1.26 0.41 0.23 10 51 1.90 2.10 2.08 0.41 0.24 11 15 1.85 1.50 1.23 0.48 0.31 12 13 2.08 1.45 1.24 0.28 0.17 13 3 2.49 0.97 1.67 0.27 0.12 14 4 2.02 1.97 1.05 0.27 0.09 Table 11: Average value by depth D o</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddarth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - measuring the relatedness of concepts. In Proceedings of NAACL (Demonstration System), pages 38–41, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Stevens</author>
<author>Philip Kegelmeyer</author>
<author>David Andrzejewski</author>
<author>David Butler</author>
</authors>
<title>Exploring topic coherence over many models and many topics.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>952--961</pages>
<location>Jeju Island,</location>
<marker>Stevens, Kegelmeyer, Andrzejewski, Butler, 2012</marker>
<rawString>Keith Stevens, Philip Kegelmeyer, David Andrzejewski, and David Butler. 2012. Exploring topic coherence over many models and many topics. In Proceedings of EMNLP, pages 952–961, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="4955" citStr="Szpektor and Dagan (2008)" startWordPosition="805" endWordPosition="809">; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of these feature inclusion measures has provided general support for the DIH. Following Szpektor and Dagan (2008), inclusion measures are also sometimes balanced with similarity measures such as LIN similarity (Lin, 1998), to ensure that A and B are semantically related, since unrelated pairs that differ in frequency can mimic feature inclusion. Previous distributional approaches to hypernym detection have generally involved a single measure, designed to rank hypernyms above other relation classes. Evaluation has largely involved either ranking or binary classification tasks, and there has been little work on using a variety of measures to distinguish multiple relation classes. Lenci and Benotto (2012) p</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>I. Szpektor and I. Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>81--88</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1293" citStr="Weeds and Weir, 2003" startWordPosition="190" endWordPosition="193">rnyms, and a TC-based family of measures contributes to multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also contexts of animal. Existing measures look at the amount of overlap between the co-occurrences of A and B, in order to judge whether B is a h</context>
<context position="4248" citStr="Weeds and Weir, 2003" startWordPosition="685" endWordPosition="688"> how the level of generality of a hypernym affects entailment measures. 511 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of the</context>
<context position="19580" citStr="Weeds and Weir (2003)" startWordPosition="3278" endWordPosition="3281">sidering the pair owl-chordate, there is no change from topicA to topicAnotB. But chordate loses a sizeable amount of coherence when owl is removed; the topic changes from primitive, ancestral, ancestor, evolution, lineage, basal, earliest, fossil, non-, neural (TC 6.62), to earliest, non-, neural, affinity, probable, genome, suspected, universally, group, approximation (TC 3.60). 6 Hypernym Detection Measures Since we use the same dataset as Lenci and Benotto (2012), we report the invCL measure introduced in that paper, which outperformed the other measures reported there, including those of Weeds and Weir (2003), Weeds et al. (2004), and Clarke (2009). Let fA be the weight of feature f in A, and let FA be the set of features with non-zero weights in A. Then we have: CL(A, B) = EfEFAnFbmin(fA, fB) (6) EfEFAfA ,I invCL(A, B) = CL(A, B) * (1 − CL(B, A)) (7) We also report the balAPinc measure of Kotlerman et al. (2010), which is not included in the Lenci and Benotto (2012) evaluation. This measure begins with APinc, in which the features of A are ranked by weight, highest to lowest: ErE1...|FA|P(r) * rel(fr) APinc(A, B) = (8) |FA| where P(r) is the “precision” at rank r, that is, how many of B’s feature</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of EMNLP, pages 81–88, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1015--1021</pages>
<location>Geneva.</location>
<contexts>
<context position="1313" citStr="Weeds et al., 2004" startWordPosition="194" endWordPosition="197">family of measures contributes to multi-way relation classification. 1 Introduction Automatically detecting lexical entailment – for example, that lion entails animal or guitar entails instrument, also known as hypernym detection – is an important linguistic task in its own right, and is also a prerequisite for recognizing entailments between longer text segments such as phrases or sentences (Bos and Markert, 2005; Garrette et al., 2011; Baroni et al., 2012; Beltagy et al., 2013). Several recent techniques for hypernym detection have made use of distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These techniques are based on the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005), hereafter DIH, which proposes that if term A entails term B (B is a hypernym of A), then the contexts in which A occurs are a subset of those in which B occurs. For example, all the contexts (co-occurrences) of lion – which might include zoo, hunt, wild, food, etc. – are also contexts of animal. Existing measures look at the amount of overlap between the co-occurrences of A and B, in order to judge whether B is a hypernym of A. The mo</context>
<context position="4268" citStr="Weeds et al., 2004" startWordPosition="689" endWordPosition="692">rality of a hypernym affects entailment measures. 511 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 511–519, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics 2 Related Work Historically, manually developed resources such as WordNet (Miller, 1995) have been used to supply lexical entailment information to NLP applications (Bos and Markert, 2005). More recently, a number of techniques for detecting lexical entailment have been developed using distributional semantics (Weeds and Weir, 2003; Weeds et al., 2004; Geffet and Dagan, 2005; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012). These measures quantify to what extent the cooccurrence features of a term A are included in those of another term B, by a direct comparison of the distributional vectors A� and A Kotlerman et al. (2010) use the notion of Average Precision from Information Retrieval to weight the relative importance of the overlapping features. Lenci and Benotto (2012) also check the extent to which B’s features are not a subset of A’s, as a proxy for the more general character of B. The success of these feature inclusion</context>
<context position="19601" citStr="Weeds et al. (2004)" startWordPosition="3282" endWordPosition="3285">hordate, there is no change from topicA to topicAnotB. But chordate loses a sizeable amount of coherence when owl is removed; the topic changes from primitive, ancestral, ancestor, evolution, lineage, basal, earliest, fossil, non-, neural (TC 6.62), to earliest, non-, neural, affinity, probable, genome, suspected, universally, group, approximation (TC 3.60). 6 Hypernym Detection Measures Since we use the same dataset as Lenci and Benotto (2012), we report the invCL measure introduced in that paper, which outperformed the other measures reported there, including those of Weeds and Weir (2003), Weeds et al. (2004), and Clarke (2009). Let fA be the weight of feature f in A, and let FA be the set of features with non-zero weights in A. Then we have: CL(A, B) = EfEFAnFbmin(fA, fB) (6) EfEFAfA ,I invCL(A, B) = CL(A, B) * (1 − CL(B, A)) (7) We also report the balAPinc measure of Kotlerman et al. (2010), which is not included in the Lenci and Benotto (2012) evaluation. This measure begins with APinc, in which the features of A are ranked by weight, highest to lowest: ErE1...|FA|P(r) * rel(fr) APinc(A, B) = (8) |FA| where P(r) is the “precision” at rank r, that is, how many of B’s features are included at ran</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of COLING, pages 1015– 1021, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Stanley Peters</author>
</authors>
<title>Word vectors and quantum logic.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eight Mathematics of Language Conference,</booktitle>
<location>Bloomington, Indiana.</location>
<contexts>
<context position="9829" citStr="Widdows and Peters, 2003" startWordPosition="1639" endWordPosition="1643">ic, namely the top N features from a distributional vector, does not correspond to a topic generated by a latent variable model, because it does not have a probability distribution over words. However, the TC measures we adopt do not make use of such a probability distribution except for choosing the top N words from a topic, which are then treated as an unordered set for the pairwise operations. Newman et al. (2010) uses N=10, and Mimno et al. (2011) uses N=5...20; we investigate a range of N. 3.3 Vector Negation For removing one topic from another, we draw on the concept of vector negation (Widdows and Peters, 2003; Widdows, 2003). Vector negation has proved useful for modeling word senses in Information Retrieval. For example, one might want to formulate a query for suit NOT lawsuit, which will retrieve terms such as shirt and jacket and exclude plaintiff and damages. We test two versions of vector negation. The first, Widdows (Widdows, 2003), represents A NOT B as the projection of A onto B1, the subspace orthogonal to B in the vector space V . Specifically, 111 ≡ {v E V : v · B = 0}. The formula for Widdows A NOT B is: B� (3) The second, Strict negation, simply zeros out any context features of A tha</context>
</contexts>
<marker>Widdows, Peters, 2003</marker>
<rawString>Dominic Widdows and Stanley Peters. 2003. Word vectors and quantum logic. In Proceedings of the Eight Mathematics of Language Conference, Bloomington, Indiana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Orthogonal negation in vector spaces for modelling word-meanings and document retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>136--143</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9845" citStr="Widdows, 2003" startWordPosition="1644" endWordPosition="1645">res from a distributional vector, does not correspond to a topic generated by a latent variable model, because it does not have a probability distribution over words. However, the TC measures we adopt do not make use of such a probability distribution except for choosing the top N words from a topic, which are then treated as an unordered set for the pairwise operations. Newman et al. (2010) uses N=10, and Mimno et al. (2011) uses N=5...20; we investigate a range of N. 3.3 Vector Negation For removing one topic from another, we draw on the concept of vector negation (Widdows and Peters, 2003; Widdows, 2003). Vector negation has proved useful for modeling word senses in Information Retrieval. For example, one might want to formulate a query for suit NOT lawsuit, which will retrieve terms such as shirt and jacket and exclude plaintiff and damages. We test two versions of vector negation. The first, Widdows (Widdows, 2003), represents A NOT B as the projection of A onto B1, the subspace orthogonal to B in the vector space V . Specifically, 111 ≡ {v E V : v · B = 0}. The formula for Widdows A NOT B is: B� (3) The second, Strict negation, simply zeros out any context features of A that are non-zero i</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Orthogonal negation in vector spaces for modelling word-meanings and document retrieval. In Proceedings of ACL, pages 136– 143, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>