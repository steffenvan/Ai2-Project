<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000248">
<title confidence="0.996968">
Inversion Transduction Grammar for Joint Phrasal Translation Modeling
</title>
<author confidence="0.998481">
Colin Cherry
</author>
<affiliation confidence="0.9990995">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.54875">
Edmonton, AB, Canada, T6G 2E8
</address>
<email confidence="0.994064">
colinc@cs.ualberta.ca
</email>
<author confidence="0.897669">
Dekang Lin
</author>
<affiliation confidence="0.879766">
Google Inc.
</affiliation>
<address confidence="0.6469925">
1600 Amphitheatre Parkway
Mountain View, CA, USA, 9403
</address>
<email confidence="0.997465">
lindek@google.com
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989055555556">
We present a phrasal inversion trans-
duction grammar as an alternative to
joint phrasal translation models. This
syntactic model is similar to its flat-
string phrasal predecessors, but admits
polynomial-time algorithms for Viterbi
alignment and EM training. We demon-
strate that the consistency constraints that
allow flat phrasal models to scale also help
ITG algorithms, producing an 80-times
faster inside-outside algorithm. We also
show that the phrasal translation tables
produced by the ITG are superior to those
of the flat joint phrasal model, producing
up to a 2.5 point improvement in BLEU
score. Finally, we explore, for the first
time, the utility of a joint phrasal transla-
tion model as a word alignment method.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996111627907">
Statistical machine translation benefits greatly from
considering more than one word at a time. One
can put forward any number of non-compositional
translations to support this point, such as the col-
loquial Canadian French-English pair, (Wo les mo-
teurs, Hold your horses), where no clear word-to-
word connection can be drawn. Nearly all cur-
rent decoding methods have shifted to phrasal rep-
resentations, gaining the ability to handle non-
compositional translations, but also allowing the de-
coder to memorize phenomena such as monolingual
agreement and short-range movement, taking pres-
sure off of language and distortion models.
Despite the success of phrasal decoders, knowl-
edge acquisition for translation generally begins
with a word-level analysis of the training text, tak-
ing the form of a word alignment. Attempts to apply
the same statistical analysis used at the word level
in a phrasal setting have met with limited success,
held back by the sheer size of phrasal alignment
space. Hybrid methods that combine well-founded
statistical analysis with high-confidence word-level
alignments have made some headway (Birch et al.,
2006), but suffer from the daunting task of heuris-
tically exploring a still very large alignment space.
In the meantime, synchronous parsing methods effi-
ciently process the same bitext phrases while build-
ing their bilingual constituents, but continue to be
employed primarily for word-to-word analysis (Wu,
1997). In this paper we unify the probability models
for phrasal translation with the algorithms for syn-
chronous parsing, harnessing the benefits of both
to create a statistically and algorithmically well-
founded method for phrasal analysis of bitext.
Section 2 begins by outlining the phrase extrac-
tion system we intend to replace and the two meth-
ods we combine to do so: the joint phrasal transla-
tion model (JPTM) and inversion transduction gram-
mar (ITG). Section 3 describes our proposed solu-
tion, a phrasal ITG. Section 4 describes how to ap-
ply our phrasal ITG, both as a translation model and
as a phrasal word-aligner. Section 5 tests our system
in both these capacities, while Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.999313" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.996341">
2.1 Phrase Table Extraction
</subsectionHeader>
<bodyText confidence="0.9961375">
Phrasal decoders require a phrase table (Koehn et
al., 2003), which contains bilingual phrase pairs and
</bodyText>
<page confidence="0.992787">
17
</page>
<note confidence="0.541422">
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17–24,
Rochester, New York, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99989825">
scores indicating their utility. The surface heuris-
tic is the most popular method for phrase-table con-
struction. It extracts all consistent phrase pairs from
word-aligned bitext (Koehn et al., 2003). The word
alignment provides bilingual links, indicating trans-
lation relationships between words. Consistency is
defined so that alignment links are never broken by
phrase boundaries. For each token w in a consistent
phrase pair p, all tokens linked to w by the alignment
must also be included in p. Each consistent phrase
pair is counted as occurring once per sentence pair.
The scores for the extracted phrase pairs are pro-
vided by normalizing these flat counts according to
common English or Foreign components, producing
the conditional distributions p( f|e) and p(e |f).
The surface heuristic can define consistency ac-
cording to any word alignment; but most often, the
alignment is provided by GIZA++ (Och and Ney,
2003). This alignment system is powered by the
IBM translation models (Brown et al., 1993), in
which one sentence generates the other. These mod-
els produce only one-to-many alignments: each gen-
erated token can participate in at most one link.
Many-to-many alignments can be created by com-
bining two GIZA++ alignments, one where English
generates Foreign and another with those roles re-
versed (Och and Ney, 2003). Combination ap-
proaches begin with the intersection of the two
alignments, and add links from the union heuris-
tically. The grow-diag-final (GDF) combination
heuristic (Koehn et al., 2003) adds links so that each
new link connects a previously unlinked token.
</bodyText>
<subsectionHeader confidence="0.999856">
2.2 Joint phrasal translation model
</subsectionHeader>
<bodyText confidence="0.999685">
The IBM models that power GIZA++ are trained
with Expectation Maximization (Dempster et al.,
1977), or EM, on sentence-aligned bitext. A transla-
tion model assigns probabilities to alignments; these
alignment distributions are used to count translation
events, which are then used to estimate new parame-
ters for the translation model. Sampling is employed
when the alignment distributions cannot be calcu-
lated efficiently. This statistically-motivated process
is much more appealing than the flat counting de-
scribed in Section 2.1, but it does not directly in-
clude phrases.
The joint phrasal translation model (Marcu and
Wong, 2002), or JPTM, applies the same statistical
techniques from the IBM models in a phrasal setting.
The JPTM is designed according to a generative pro-
cess where both languages are generated simultane-
ously. First, a bag of concepts, or cepts, C is gener-
ated. Each ci E C corresponds to a bilingual phrase
pair, ci = (ei, �fi). These contiguous phrases are
permuted in each language to create two sequences
of phrases. Initially, Marcu and Wong assume that
the number of cepts, as well as the phrase orderings,
are drawn from uniform distributions. That leaves
a joint translation distribution p(ei, �fi) to determine
which phrase pairs are selected. Given a lexicon of
possible cepts and a predicate L(E, F, C) that de-
termines if a bag of cepts C can be bilingually per-
muted to create the sentence pair (E, F), the proba-
bility of a sentence pair is:
</bodyText>
<equation confidence="0.996269666666667">
p(E, F) a {C|L(E,F,C)} � �
E �H p(ei, �fi) �(1)
ciEC
</equation>
<bodyText confidence="0.973291666666667">
If left unconstrained, (1) will consider every phrasal
segmentation of E and F, and every alignment be-
tween those phrases. Later, a distortion model based
on absolute token positions is added to (1).
The JPTM faces several problems when scaling
up to large training sets:
</bodyText>
<listItem confidence="0.796683">
1. The alignment space enumerated by the sum
in (1) is huge, far larger than the one-to-many
space explored by GIZA++.
2. The translation distribution p(e,
</listItem>
<bodyText confidence="0.998305666666667">
all co-occurring phrases observed in the bitext.
This is far too large to fit in main memory, and
can be unwieldly for storage on disk.
</bodyText>
<listItem confidence="0.70882325">
3. Given a non-uniform p(e, 1), there is no effi-
cient algorithm to compute the expectation of
phrase pair counts required for EM, or to find
the most likely phrasal alignment.
</listItem>
<bodyText confidence="0.932231181818182">
Marcu and Wong (2002) address point 2 with a lexi-
con constraint; monolingual phrases that are above
a length threshold or below a frequency threshold
are excluded from the lexicon. Point 3 is handled
by hill-climbing to a likely phrasal alignment and
sampling around it. However, point 1 remains unad-
dressed, which prevents the model from scaling to
large data sets.
Birch et al. (2006) handle point 1 directly by re-
ducing the size of the alignment space. This is
f) will cover
</bodyText>
<page confidence="0.995457">
18
</page>
<bodyText confidence="0.9997449375">
accomplished by constraining the JPTM to only
use phrase pairs that are consistent with a high-
confidence word alignment, which is provided by
GIZA++ intersection. We refer to this constrained
JPTM as a C-JPTM. This strikes an interesting
middle ground between the surface heuristic de-
scribed in Section 2.1 and the JPTM. Like the sur-
face heuristic, a word alignment is used to limit the
phrase pairs considered, but the C-JPTM reasons
about distributions over phrasal alignments, instead
of taking flat counts. The consistency constraint al-
lows them to scale their C-JPTM up to 700,000 sen-
tence pairs. With this constraint in place, the use of
hill-climbing and sampling during EM training be-
comes one of the largest remaining weaknesses of
the C-JPTM.
</bodyText>
<subsectionHeader confidence="0.984569">
2.3 Inversion Transduction Grammar
</subsectionHeader>
<bodyText confidence="0.999989058823529">
Like the JPTM, stochastic synchronous grammars
provide a generative process to produce a sentence
and its translation simultaneously. Inversion trans-
duction grammar (Wu, 1997), or ITG, is a well-
studied synchronous grammar formalism. Terminal
productions of the form A —* e/f produce a to-
ken in each stream, or a token in one stream with
the null symbol 0 in the other. To allow for move-
ment during translation, non-terminal productions
can be either straight or inverted. Straight produc-
tions, with their non-terminals inside square brack-
ets [...], produce their symbols in the given order in
both streams. Inverted productions, indicated by an-
gled brackets (...), are output in reverse order in the
Foreign stream only.
The work described here uses the binary bracket-
ing ITG, which has a single non-terminal:
</bodyText>
<equation confidence="0.634779">
A —* [AA]  |(AA)  |e/f (2)
</equation>
<bodyText confidence="0.9990958">
This grammar admits an efficient bitext parsing al-
gorithm, and holds no language-specific biases.
(2) cannot represent all possible permutations of
concepts that may occur during translation, because
some permutations will require discontinuous con-
stituents (Melamed, 2003). This ITG constraint is
characterized by the two forbidden structures shown
in Figure 1 (Wu, 1997). Empirical studies suggest
that only a small percentage of human translations
violate these constraints (Cherry and Lin, 2006).
</bodyText>
<figureCaption confidence="0.999547">
Figure 1: The two ITG forbidden structures.
</figureCaption>
<figure confidence="0.976133">
a) A-[AA] b) A-&lt;AA&gt; c) A-e/f
</figure>
<figureCaption confidence="0.9923215">
Figure 2: Three ways in which a phrasal ITG can
analyze a multi-word span or phrase.
</figureCaption>
<bodyText confidence="0.999835888888889">
Stochastic ITGs are parameterized like their
PCFG counterparts (Wu, 1997); productions
A —* X are assigned probability Pr(X |A). These
parameters can be learned from sentence-aligned bi-
text using the EM algorithm. The expectation task
of counting productions weighted by their probabil-
ity is handled with dynamic programming, using the
inside-outside algorithm extended to bitext (Zhang
and Gildea, 2004).
</bodyText>
<sectionHeader confidence="0.932396" genericHeader="method">
3 ITG as a Phrasal Translation Model
</sectionHeader>
<bodyText confidence="0.999443736842105">
This paper introduces a phrasal ITG; in doing so,
we combine ITG with the JPTM. ITG parsing al-
gorithms consider every possible two-dimensional
span of bitext, each corresponding to a bilingual
phrase pair. Each multi-token span is analyzed in
terms of how it could be built from smaller spans us-
ing a straight or inverted production, as is illustrated
in Figures 2 (a) and (b). To extend ITG to a phrasal
setting, we add a third option for span analysis: that
the span under consideration might have been drawn
directly from the lexicon. This option can be added
to our grammar by altering the definition of a termi-
nal production to include phrases: A —* 6/ 1. This
third option is shown in Figure 2 (c). The model
implied by this extended grammar is trained using
inside-outside and EM.
Our approach differs from previous attempts to
use ITGs for phrasal bitext analysis. Wu (1997)
used a binary bracketing ITG to segment a sen-
</bodyText>
<figure confidence="0.982445266666667">
f7 f2 f3 fa f7 f2 f3 fa
e3
ea
e3
e2
e7
ea
e2
e7
calmez vous
calm down
calmez vous
calm down
calmez vous
calm down
</figure>
<page confidence="0.990557">
19
</page>
<bodyText confidence="0.996736785714286">
tence while simultaneously word-aligning it to its
translation, but the model was trained heuristically
with a fixed segmentation. Vilar and Vidal (2005)
used ITG-like dynamic programming to drive both
training and alignment for their recursive translation
model, but they employed a conditional model that
did not maintain a phrasal lexicon. Instead, they
scored phrase pairs using IBM Model 1.
Our phrasal ITG is quite similar to the JPTM.
Both models are trained with EM, and both em-
ploy generative stories that create a sentence and its
translation simultaneously. The similarities become
more apparent when we consider the canonical-form
binary-bracketing ITG (Wu, 1997) shown here:
</bodyText>
<figure confidence="0.920320833333333">
5 A  |B  |C (3)
A [AB]  |[BB]  |[CB] |
[AC]  |[BC]  |[CC]
B (AA)  |(BA)  |(CA) |
(AC)  |(BC)  |(CC)
C E/ f�
</figure>
<bodyText confidence="0.980668625">
(3) is employed in place of (2) to reduce redundant
alignments and clean up EM expectations.1 More
importantly for our purposes, it introduces a preter-
minal C, which generates all phrase pairs or cepts.
When (3) is parameterized as a stochastic ITG, the
conditional distribution p(6/ 1|C) is equivalent to
the JPTM’s p(e, 1); both are joint distributions over
all possible phrase pairs. The distributions condi-
tioned on the remaining three non-terminals assign
probability to concept movement by tracking inver-
sions. Like the JPTM’s distortion model, these pa-
rameters grade each movement decision indepen-
dently. With terminal productions producing cepts,
and inversions measuring distortion, our phrasal ITG
is essentially a variation on the JPTM with an alter-
nate distortion model.
Our phrasal ITG has two main advantages over
the JPTM. Most significantly, we gain polynomial-
time algorithms for both Viterbi alignment and EM
expectation, through the use of ITG parsing and
inside-outside algorithms. These phrasal ITG algo-
rithms are no more expensive asymptotically than
their word-to-word counterparts, since each poten-
tial phrase needs to be analyzed anyway during
</bodyText>
<footnote confidence="0.8048455">
1If the null symbol 0 is included among the terminals, then
redundant parses will still occur, but far less frequently.
</footnote>
<bodyText confidence="0.999692444444444">
constituent construction. We hypothesize that us-
ing these methods in place of heuristic search and
sampling will improve the phrasal translation model
learned by EM. Also, we can easily incorporate links
to 0 by including the symbol among our terminals.
To minimize redundancy, we allow only single to-
kens, not phrases, to align to 0. The JPTM does not
allow links to 0.
The phrasal ITG also introduces two new compli-
cations. ITG Viterbi and inside-outside algorithms
have polynomial complexity, but that polynomial is
O(n6), where n is the length of the longer sentence
in the pair. This is too slow to train on large data
sets without massive parallelization. Also, ITG al-
gorithms explore their alignment space perfectly, but
that space has been reduced by the ITG constraint
described in Section 2.3. We will address each of
these issues in the following two subsections.
</bodyText>
<subsectionHeader confidence="0.999184">
3.1 Pruning Spans
</subsectionHeader>
<bodyText confidence="0.99996375">
First, we address the problem of scaling ITG to large
data. ITG dynamic programming algorithms work
by analyzing each bitext span only once, storing its
value in a table for future use. There are O(n4) of
these spans, and each analysis takes O(n2) time. An
effective approach to speeding up ITG algorithms
is to eliminate unlikely spans as a preprocessing
step, assigning them 0 probability and saving the
time spent processing them. Past approaches have
pruned spans using IBM Model 1 probability esti-
mates (Zhang and Gildea, 2005) or using agreement
with an existing parse tree (Cherry and Lin, 2006).
The former is referred to as tic-tac-toe pruning be-
cause it uses both inside and outside estimates.
We propose a new ITG pruning method that lever-
ages high-confidence links by pruning all spans that
are inconsistent with a provided alignment. This
is similar to the constraint used in the C-JPTM,
but we do not just eliminate those spans as poten-
tial phrase-to-phrase links: we never consider any
ITG parse that builds a non-terminal over a pruned
span.2 This fixed-link pruning will speed up both
Viterbi alignment and EM training by reducing the
number of analyzed spans, and so long as we trust
</bodyText>
<footnote confidence="0.9812225">
2Birch et al. (2006) re-introduce inconsistent phrase-pairs in
cases where the sentence pair could not be aligned otherwise.
We allow links to 0 to handle these situations, completely elim-
inating the pruned spans from our alignment space.
</footnote>
<page confidence="0.991732">
20
</page>
<bodyText confidence="0.999798666666667">
our high-confidence links, it will do so harmlessly.
We demonstrate the effectiveness of this pruning
method experimentally in Section 5.1.
</bodyText>
<subsectionHeader confidence="0.999552">
3.2 Handling the ITG Constraint
</subsectionHeader>
<bodyText confidence="0.999990695652174">
Our remaining concern is the ITG constraint. There
are some alignments that we just cannot build, and
sentence pairs requiring those alignments will occur.
These could potentially pollute our training data; if
the system is unable to build the right alignment, the
counts it will collect from that pair must be wrong.
Furthermore, if our high-confidence links are not
ITG-compatible, our fixed-link pruning will prevent
the aligner from forming any alignments at all.
However, these two potential problems cancel
each other out. Sentence pairs containing non-ITG
translations will tend to have high-confidence links
that are also not ITG-compatible. Our EM learner
will simply skip these sentence pairs during train-
ing, avoiding pollution of our training data. We can
use a linear-time algorithm (Zhang et al., 2006) to
detect non-ITG movement in our high-confidence
links, and remove the offending sentence pairs from
our training corpus. This results in only a minor re-
duction in training data; in our French-English train-
ing set, we lose less than 1%. In the experiments de-
scribed in Section 5, all systems that do not use ITG
will take advantage of the complete training set.
</bodyText>
<sectionHeader confidence="0.99503" genericHeader="method">
4 Applying the model
</sectionHeader>
<bodyText confidence="0.999748375">
Any phrasal translation model can be used for two
tasks: translation modeling and phrasal word align-
ment. Previous work on JPTM has focused on only
the first task. We are interested in phrasal alignment
because it may be better suited to heuristic phrase-
extraction than word-based models. This section de-
scribes how to use our phrasal ITG first as a transla-
tion model, and then as a phrasal aligner.
</bodyText>
<subsectionHeader confidence="0.988478">
4.1 Translation Modeling
</subsectionHeader>
<bodyText confidence="0.999733117647059">
We can test our model’s utility for translation by
transforming its parameters into a phrase table for
the phrasal decoder Pharaoh (Koehn et al., 2003).
Any joint model can produce the necessary condi-
tional probabilities by conditionalizing the joint ta-
ble in both directions. We use our p(¯e/ ¯f|C) dis-
tribution from our stochastic grammar to produce
p(¯e |¯f) and p(¯f|¯e) values for its phrasal lexicon.
Pharaoh also includes lexical weighting param-
eters that are derived from the alignments used to
induce its phrase pairs (Koehn et al., 2003). Us-
ing the phrasal ITG as a direct translation model,
we do not produce alignments for individual sen-
tence pairs. Instead, we provide a lexical preference
with an IBM Model 1 feature pM1 that penalizes un-
matched words (Vogel et al., 2003). We include both
pM1(¯e |f) and pM1( f|¯e).
</bodyText>
<subsectionHeader confidence="0.969106">
4.2 Phrasal Word Alignment
</subsectionHeader>
<bodyText confidence="0.99998572972973">
We can produce a translation model using inside-
outside, without ever creating a Viterbi parse. How-
ever, we can also examine the maximum likelihood
phrasal alignments predicted by the trained model.
Despite its strengths derived from using phrases
throughout training, the alignments predicted by our
phrasal ITG are usually unsatisfying. For exam-
ple, the fragment pair (order of business, ordre des
travaux) is aligned as a phrase pair by our system,
linking every English word to every French word.
This is frustrating, since there is a clear compo-
sitional relationship between the fragment’s com-
ponent words. This happens because the system
seeks only to maximize the likelihood of its train-
ing corpus, and phrases are far more efficient than
word-to-word connections. When aligning text, an-
notators are told to resort to many-to-many links
only when no clear compositional relationship ex-
ists (Melamed, 1998). If we could tell our phrasal
aligner the same thing, we could greatly improve the
intuitive appeal of our alignments. Again, we can
leverage high-confidence links for help.
In the high-confidence alignments provided by
GIZA++ intersection, each token participates in at
most one link. Links only appear when two word-
based IBM translation models can agree. Therefore,
they occur at points of high compositionality: the
two words clearly account for one another. We adopt
an alignment-driven definition of compositional-
ity: any phrase pair containing two or more high-
confidence links is compositional, and can be sep-
arated into at least two non-compositional phrases.
By removing any phrase pairs that are compositional
by this definition from our terminal productions,
we can ensure that our aligner never creates such
phrases during training or alignment. Doing so pro-
duces far more intuitive alignments. Aligned with
</bodyText>
<page confidence="0.997933">
21
</page>
<bodyText confidence="0.99925525">
a model trained using this non-compositional con-
straint (NCC), our example now forms three word-
to-word connections, rather than a single phrasal
one. The phrases produced with this constraint are
very small, and include only non-compositional con-
text. Therefore, we use the constraint only to train
models intended for Viterbi alignment, and not when
generating phrase tables directly as in Section 4.1.
</bodyText>
<sectionHeader confidence="0.994292" genericHeader="conclusions">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99998125">
In this section, we first verify the effectiveness of
fixed-link pruning, and then test our phrasal ITG,
both as an aligner and as a translation model. We
train all translation models with a French-English
Europarl corpus obtained by applying a 25 to-
ken sentence-length limit to the training set pro-
vided for the HLT-NAACL SMT Workshop Shared
Task (Koehn and Monz, 2006). The resulting cor-
pus has 393,132 sentence pairs. 3,376 of these
are omitted for ITG methods because their high-
confidence alignments have ITG-incompatible con-
structions. Like our predecessors (Marcu and Wong,
2002; Birch et al., 2006), we apply a lexicon con-
straint: no monolingual phrase can be used by any
phrasal model unless it occurs at least five times.
High-confidence alignments are provided by inter-
secting GIZA++ alignments trained in each direc-
tion with 5 iterations each of Model 1, HMM, and
Model 4. All GIZA++ alignments are trained with
no sentence-length limit, using the full 688K corpus.
</bodyText>
<subsectionHeader confidence="0.999685">
5.1 Pruning Speed Experiments
</subsectionHeader>
<bodyText confidence="0.999983777777778">
To measure the speed-up provided by fixed-link
pruning, we timed our phrasal inside-outside algo-
rithm on the first 100 sentence pairs in our training
set, with and without pruning. The results are shown
in Table 1. Tic-tac-toe pruning is included for com-
parison. With fixed-link pruning, on average 95%
of the possible spans are pruned, reducing running
time by two orders of magnitude. This improvement
makes ITG training feasible, even with large bitexts.
</bodyText>
<subsectionHeader confidence="0.999834">
5.2 Alignment Experiments
</subsectionHeader>
<bodyText confidence="0.9998692">
The goal of this experiment is to compare the Viterbi
alignments from the phrasal ITG to gold standard
human alignments. We do this to validate our non-
compositional constraint and to select good align-
ments for use with the surface heuristic.
</bodyText>
<tableCaption confidence="0.99623">
Table 1: Inside-outside run-time comparison.
</tableCaption>
<table confidence="0.99975125">
Method Seconds Avg. Spans Pruned
No Prune 415 -
Tic-tac-toe 37 68%
Fixed link 5 95%
</table>
<tableCaption confidence="0.917395">
Table 2: Alignment Comparison.
</tableCaption>
<table confidence="0.999608666666667">
Method Prec Rec F-measure
GIZA++ Intersect 96.7 53.0 68.5
GIZA++ Union 82.5 69.0 75.1
GIZA++ GDF 84.0 68.2 75.2
Phrasal ITG 50.7 80.3 62.2
Phrasal ITG + NCC 75.4 78.0 76.7
</table>
<bodyText confidence="0.994638233333333">
Following the lead of (Fraser and Marcu, 2006),
we hand-aligned the first 100 sentence pairs of
our training set according to the Blinker annota-
tion guidelines (Melamed, 1998). We did not dif-
ferentiate between sure and possible links. We re-
port precision, recall and balanced F-measure (Och
and Ney, 2003). For comparison purposes, we in-
clude the results of three types of GIZA++ combina-
tion, including the grow-diag-final heuristic (GDF).
We tested our phrasal ITG with fixed link prun-
ing, and then added the non-compositional con-
straint (NCC). During development we determined
that performance levels off for both of the ITG mod-
els after 3 EM iterations. The results are shown in
Table 2.
The first thing to note is that GIZA++ Intersection
is indeed very high precision. Our confidence in it
as a constraint is not misplaced. We also see that
both phrasal models have significantly higher recall
than any of the GIZA++ alignments, even higher
than the permissive GIZA++ union. One factor con-
tributing to this is the phrasal model’s use of cepts:
it completely interconnects any phrase pair, while
GIZA++ union and GDF may not. Its global view
of phrases also helps in this regard: evidence for a
phrase can be built up over multiple sentences. Fi-
nally, we note that in terms of alignment quality,
the non-compositional constraint is an unqualified
success for the phrasal ITG. It produces a 25 point
improvement in precision, at the cost of 2 points
</bodyText>
<page confidence="0.99133">
22
</page>
<bodyText confidence="0.99993625">
of recall. This produces the highest balanced F-
measure observed on our test set, but the utility of
its alignments will depend largely on one’s desired
precision-recall trade-off.
</bodyText>
<subsectionHeader confidence="0.993813">
5.3 Translation Experiments
</subsectionHeader>
<bodyText confidence="0.99940175">
In this section, we compare a number of different
methods for phrase table generation in a French to
English translation task. We are interested in an-
swering three questions:
</bodyText>
<listItem confidence="0.9559792">
1. Does the phrasal ITG improve on the C-JPTM?
2. Can phrasal translation models outperform the
surface heuristic?
3. Do Viterbi phrasal alignments provide better
input for the surface heuristic?
</listItem>
<bodyText confidence="0.999007333333333">
With this in mind, we test five phrase tables. Two
are conditionalized phrasal models, each EM trained
until performance degrades:
</bodyText>
<listItem confidence="0.895872833333333">
• C-JPTM3 as described in (Birch et al., 2006)
• Phrasal ITG as described in Section 4.1
Three provide alignments for the surface heuristic:
• GIZA++ with grow-diag-final (GDF)
• Viterbi Phrasal ITG with and without the non-
compositional constraint
</listItem>
<bodyText confidence="0.999972411764706">
We use the Pharaoh decoder (Koehn et al., 2003)
with the SMT Shared Task baseline system (Koehn
and Monz, 2006). Weights for the log-linear model
are set using the 500-sentence tuning set provided
for the shared task with minimum error rate train-
ing (Och, 2003) as implemented by Venugopal
and Vogel (2005). Results on the provided 2000-
sentence development set are reported using the
BLEU metric (Papineni et al., 2002). For all meth-
ods, we report performance with and without IBM
Model 1 features (M1), along with the size of the re-
sulting tables in millions of phrase pairs. The results
of all experiments are shown in Table 3.
We see that the Phrasal ITG surpasses the C-
JPTM by more than 2.5 BLEU points. A large com-
ponent of this improvement is due to the ITG’s use
of inside-outside for expectation calculation, though
</bodyText>
<footnote confidence="0.513274">
3Supplied by personal communication. Run with default pa-
rameters, but with maximum phrase length increased to 5.
</footnote>
<tableCaption confidence="0.99804">
Table 3: Translation Comparison.
</tableCaption>
<table confidence="0.99973175">
Method BLEU +M1 Size
Conditionalized Phrasal Model
C-JPTM 26.27 28.98 1.3M
Phrasal ITG 28.85 30.24 2.2M
Alignment with Surface Heuristic
GIZA++ GDF 30.46 30.61 9.8M
Phrasal ITG 30.31 30.39 5.8M
Phrasal ITG + NCC 30.66 30.80 9.0M
</table>
<bodyText confidence="0.9904105">
there are other differences between the two sys-
tems.4 This improvement over search and sampling
is demonstrated by the ITG’s larger table size; by ex-
ploring more thoroughly, it is extracting more phrase
pairs from the same amount of data. Both systems
improve drastically with the addition of IBM Model
1 features for lexical preference. These features also
narrow the gap between the two systems. To help
calibrate the contribution of these features, we pa-
rameterized the ITG’s phrase table using only Model
1 features, which scores 27.17.
Although ITG+M1 comes close, neither phrasal
model matches the performance of the surface
heuristic. Whatever the surface heuristic lacks in
sophistication, it makes up for in sheer coverage,
as demonstrated by its huge table sizes. Even the
Phrasal ITG Viterbi alignments, which over-commit
wildly and have horrible precision, score slightly
higher than the best phrasal model. The surface
heuristic benefits from capturing as much context
as possible, while still covering smaller translation
events with its flat counts. It is not held back by
any lexicon constraints. When GIZA++ GDF+M1
is forced to conform to a lexicon constraint by drop-
ping any phrase with a frequency lower than 5 from
its table, it scores only 29.26, for a reduction of 1.35
BLEU points.
Phrases extracted from our non-compositional
Viterbi alignments receive the highest BLEU score,
but they are not significantly better than GIZA++
GDF. The two methods also produce similarly-sized
tables, despite the ITG’s higher recall.
4Unlike our system, the Birch implementation does table
smoothing and internal lexical weighting, both of which should
help improve their results. The systems also differ in distortion
modeling and 0 handling, as described in Section 3.
</bodyText>
<page confidence="0.98966">
23
</page>
<figure confidence="0.8213415">
6 Conclusion lation: Parameter estimation. Computational Linguistics,
19(2):263–312.
</figure>
<bodyText confidence="0.999866315789474">
We have presented a phrasal ITG as an alternative
to the joint phrasal translation model. This syntactic
solution to phrase modeling admits polynomial-time
training and alignment algorithms. We demonstrate
that the same consistency constraints that allow joint
phrasal models to scale also dramatically speed up
ITGs, producing an 80-times faster inside-outside
algorithm. We show that when used to learn phrase
tables for the Pharaoh decoder, the phrasal ITG is
superior to the constrained joint phrasal model, pro-
ducing tables that result in a 2.5 point improve-
ment in BLEU when used alone, and a 1 point im-
provement when used with IBM Model 1 features.
This suggests that ITG’s perfect expectation count-
ing does matter; other phrasal models could benefit
from either adopting the ITG formalism, or improv-
ing their sampling heuristics.
We have explored, for the first time, the utility of a
joint phrasal model as a word alignment method. We
present a non-compositional constraint that turns the
phrasal ITG into a high-recall phrasal aligner with
an F-measure that is comparable to GIZA++.
With search and sampling no longer a concern,
the remaining weaknesses of the system seem to lie
with the model itself. Phrases are just too efficient
probabilistically: were we to remove all lexicon con-
straints, EM would always align entire sentences to
entire sentences. This pressure to always build the
longest phrase possible may be overwhelming oth-
erwise strong correlations in our training data. A
promising next step would be to develop a prior over
lexicon size or phrase size, allowing EM to intro-
duce large phrases at a penalty, and removing the
need for artificial constraints on the lexicon.
Acknowledgments Special thanks to Alexandra
Birch for the use of her code, and to our reviewers
for their comments. The first author is funded by
Alberta Ingenuity and iCORE studentships.
</bodyText>
<sectionHeader confidence="0.999645" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999875370370371">
A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Constraining the phrase-based, joint probability statistical
translation model. In HLT-NAACL Workshop on Statistical
Machine Translation, pages 154–157.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
C. Cherry and D. Lin. 2006. A comparison of syntactically
motivated word alignment spaces. In EACL, pages 145–152.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(1):1–38.
A. Fraser and D. Marcu. 2006. Semi-supervised training for
statistical word alignment. In ACL, pages 769–776.
P. Koehn and C. Monz. 2006. Manual and automatic evalu-
ation of machine translation. In HLT-NACCL Workshop on
Statistical Machine Translation, pages 102–121.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 127–133.
D. Marcu and W. Wong. 2002. A phrase-based, joint probabil-
ity model for statistic machine translation. In EMNLP, pages
133–139.
I. D. Melamed. 1998. Manual annotation of translational
equivalence: The blinker project. Technical Report 98-07,
Institute for Research in Cognitive Science.
I. D. Melamed. 2003. Multitext grammars and synchronous
parsers. In HLT-NAACL, pages 158–165.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19–52.
F. J. Och. 2003. Minimum error rate training for statistical
machine translation. In ACL, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU:
a method for automatic evaluation of machine translation. In
ACL, pages 311–318.
A. Venugopal and S. Vogel. 2005. Considerations in maximum
mutual information and minimum classification error train-
ing for statistical machine translation. In EAMT.
J. M. Vilar and E. Vidal. 2005. A recursive statistical transla-
tion model. In Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts, pages 199–207.
S. Vogel, Y. Zhang, F. Huang, A. Tribble, A. Venugopal,
B. Zhang, and A. Waibel. 2003. The CMU statistical ma-
chine translation system. In MT Summmit.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377–403.
H. Zhang and D. Gildea. 2004. Syntax-based alignment: Su-
pervised or unsupervised? In COLING, pages 418–424.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized inversion
transduction grammar for alignment. In ACL, pages 475–
482.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Syn-
chronous binarization for machine translation. In HLT-
NAACL, pages 256–263.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.166183">
<title confidence="0.999911">Inversion Transduction Grammar for Joint Phrasal Translation Modeling</title>
<author confidence="0.98105">Colin</author>
<affiliation confidence="0.999546">Department of Computing University of</affiliation>
<address confidence="0.983784">Edmonton, AB, Canada, T6G</address>
<email confidence="0.977838">colinc@cs.ualberta.ca</email>
<affiliation confidence="0.352932">Dekang Google</affiliation>
<address confidence="0.958083">1600 Amphitheatre Mountain View, CA, USA,</address>
<email confidence="0.999938">lindek@google.com</email>
<abstract confidence="0.998362368421053">We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models. This syntactic model is similar to its flatstring phrasal predecessors, but admits polynomial-time algorithms for Viterbi alignment and EM training. We demonstrate that the consistency constraints that allow flat phrasal models to scale also help ITG algorithms, producing an 80-times faster inside-outside algorithm. We also show that the phrasal translation tables produced by the ITG are superior to those of the flat joint phrasal model, producing up to a 2.5 point improvement in BLEU score. Finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Constraining the phrase-based, joint probability statistical translation model.</title>
<date>2006</date>
<booktitle>In HLT-NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>154--157</pages>
<contexts>
<context position="2166" citStr="Birch et al., 2006" startWordPosition="324" endWordPosition="327">uch as monolingual agreement and short-range movement, taking pressure off of language and distortion models. Despite the success of phrasal decoders, knowledge acquisition for translation generally begins with a word-level analysis of the training text, taking the form of a word alignment. Attempts to apply the same statistical analysis used at the word level in a phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space. Hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (Birch et al., 2006), but suffer from the daunting task of heuristically exploring a still very large alignment space. In the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (Wu, 1997). In this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of both to create a statistically and algorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extrac</context>
<context position="7797" citStr="Birch et al. (2006)" startWordPosition="1242" endWordPosition="1245">n main memory, and can be unwieldly for storage on disk. 3. Given a non-uniform p(e, 1), there is no efficient algorithm to compute the expectation of phrase pair counts required for EM, or to find the most likely phrasal alignment. Marcu and Wong (2002) address point 2 with a lexicon constraint; monolingual phrases that are above a length threshold or below a frequency threshold are excluded from the lexicon. Point 3 is handled by hill-climbing to a likely phrasal alignment and sampling around it. However, point 1 remains unaddressed, which prevents the model from scaling to large data sets. Birch et al. (2006) handle point 1 directly by reducing the size of the alignment space. This is f) will cover 18 accomplished by constraining the JPTM to only use phrase pairs that are consistent with a highconfidence word alignment, which is provided by GIZA++ intersection. We refer to this constrained JPTM as a C-JPTM. This strikes an interesting middle ground between the surface heuristic described in Section 2.1 and the JPTM. Like the surface heuristic, a word alignment is used to limit the phrase pairs considered, but the C-JPTM reasons about distributions over phrasal alignments, instead of taking flat co</context>
<context position="15851" citStr="Birch et al. (2006)" startWordPosition="2563" endWordPosition="2566"> The former is referred to as tic-tac-toe pruning because it uses both inside and outside estimates. We propose a new ITG pruning method that leverages high-confidence links by pruning all spans that are inconsistent with a provided alignment. This is similar to the constraint used in the C-JPTM, but we do not just eliminate those spans as potential phrase-to-phrase links: we never consider any ITG parse that builds a non-terminal over a pruned span.2 This fixed-link pruning will speed up both Viterbi alignment and EM training by reducing the number of analyzed spans, and so long as we trust 2Birch et al. (2006) re-introduce inconsistent phrase-pairs in cases where the sentence pair could not be aligned otherwise. We allow links to 0 to handle these situations, completely eliminating the pruned spans from our alignment space. 20 our high-confidence links, it will do so harmlessly. We demonstrate the effectiveness of this pruning method experimentally in Section 5.1. 3.2 Handling the ITG Constraint Our remaining concern is the ITG constraint. There are some alignments that we just cannot build, and sentence pairs requiring those alignments will occur. These could potentially pollute our training data;</context>
<context position="21598" citStr="Birch et al., 2006" startWordPosition="3480" endWordPosition="3483">s and Results In this section, we first verify the effectiveness of fixed-link pruning, and then test our phrasal ITG, both as an aligner and as a translation model. We train all translation models with a French-English Europarl corpus obtained by applying a 25 token sentence-length limit to the training set provided for the HLT-NAACL SMT Workshop Shared Task (Koehn and Monz, 2006). The resulting corpus has 393,132 sentence pairs. 3,376 of these are omitted for ITG methods because their highconfidence alignments have ITG-incompatible constructions. Like our predecessors (Marcu and Wong, 2002; Birch et al., 2006), we apply a lexicon constraint: no monolingual phrase can be used by any phrasal model unless it occurs at least five times. High-confidence alignments are provided by intersecting GIZA++ alignments trained in each direction with 5 iterations each of Model 1, HMM, and Model 4. All GIZA++ alignments are trained with no sentence-length limit, using the full 688K corpus. 5.1 Pruning Speed Experiments To measure the speed-up provided by fixed-link pruning, we timed our phrasal inside-outside algorithm on the first 100 sentence pairs in our training set, with and without pruning. The results are s</context>
<context position="25270" citStr="Birch et al., 2006" startWordPosition="4083" endWordPosition="4086"> one’s desired precision-recall trade-off. 5.3 Translation Experiments In this section, we compare a number of different methods for phrase table generation in a French to English translation task. We are interested in answering three questions: 1. Does the phrasal ITG improve on the C-JPTM? 2. Can phrasal translation models outperform the surface heuristic? 3. Do Viterbi phrasal alignments provide better input for the surface heuristic? With this in mind, we test five phrase tables. Two are conditionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000- sentence development set are reported using the BLEU metric </context>
</contexts>
<marker>Birch, Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Constraining the phrase-based, joint probability statistical translation model. In HLT-NAACL Workshop on Statistical Machine Translation, pages 154–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine transC.</title>
<date>1993</date>
<journal>Cherry</journal>
<booktitle>In EACL,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="4528" citStr="Brown et al., 1993" startWordPosition="698" endWordPosition="701">h token w in a consistent phrase pair p, all tokens linked to w by the alignment must also be included in p. Each consistent phrase pair is counted as occurring once per sentence pair. The scores for the extracted phrase pairs are provided by normalizing these flat counts according to common English or Foreign components, producing the conditional distributions p( f|e) and p(e |f). The surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by GIZA++ (Och and Ney, 2003). This alignment system is powered by the IBM translation models (Brown et al., 1993), in which one sentence generates the other. These models produce only one-to-many alignments: each generated token can participate in at most one link. Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed (Och and Ney, 2003). Combination approaches begin with the intersection of the two alignments, and add links from the union heuristically. The grow-diag-final (GDF) combination heuristic (Koehn et al., 2003) adds links so that each new link connects a previously unlinked token. 2.2 Joint phrasal t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine transC. Cherry and D. Lin. 2006. A comparison of syntactically motivated word alignment spaces. In EACL, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="5243" citStr="Dempster et al., 1977" startWordPosition="810" endWordPosition="813">: each generated token can participate in at most one link. Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed (Och and Ney, 2003). Combination approaches begin with the intersection of the two alignments, and add links from the union heuristically. The grow-diag-final (GDF) combination heuristic (Koehn et al., 2003) adds links so that each new link connects a previously unlinked token. 2.2 Joint phrasal translation model The IBM models that power GIZA++ are trained with Expectation Maximization (Dempster et al., 1977), or EM, on sentence-aligned bitext. A translation model assigns probabilities to alignments; these alignment distributions are used to count translation events, which are then used to estimate new parameters for the translation model. Sampling is employed when the alignment distributions cannot be calculated efficiently. This statistically-motivated process is much more appealing than the flat counting described in Section 2.1, but it does not directly include phrases. The joint phrasal translation model (Marcu and Wong, 2002), or JPTM, applies the same statistical techniques from the IBM mod</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Semi-supervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>769--776</pages>
<contexts>
<context position="23104" citStr="Fraser and Marcu, 2006" startWordPosition="3727" endWordPosition="3730">ts The goal of this experiment is to compare the Viterbi alignments from the phrasal ITG to gold standard human alignments. We do this to validate our noncompositional constraint and to select good alignments for use with the surface heuristic. Table 1: Inside-outside run-time comparison. Method Seconds Avg. Spans Pruned No Prune 415 - Tic-tac-toe 37 68% Fixed link 5 95% Table 2: Alignment Comparison. Method Prec Rec F-measure GIZA++ Intersect 96.7 53.0 68.5 GIZA++ Union 82.5 69.0 75.1 GIZA++ GDF 84.0 68.2 75.2 Phrasal ITG 50.7 80.3 62.2 Phrasal ITG + NCC 75.4 78.0 76.7 Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). We did not differentiate between sure and possible links. We report precision, recall and balanced F-measure (Och and Ney, 2003). For comparison purposes, we include the results of three types of GIZA++ combination, including the grow-diag-final heuristic (GDF). We tested our phrasal ITG with fixed link pruning, and then added the non-compositional constraint (NCC). During development we determined that performance levels off for both of the ITG models after 3 EM i</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>A. Fraser and D. Marcu. 2006. Semi-supervised training for statistical word alignment. In ACL, pages 769–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation.</title>
<date>2006</date>
<booktitle>In HLT-NACCL Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="21363" citStr="Koehn and Monz, 2006" startWordPosition="3444" endWordPosition="3447">s constraint are very small, and include only non-compositional context. Therefore, we use the constraint only to train models intended for Viterbi alignment, and not when generating phrase tables directly as in Section 4.1. 5 Experiments and Results In this section, we first verify the effectiveness of fixed-link pruning, and then test our phrasal ITG, both as an aligner and as a translation model. We train all translation models with a French-English Europarl corpus obtained by applying a 25 token sentence-length limit to the training set provided for the HLT-NAACL SMT Workshop Shared Task (Koehn and Monz, 2006). The resulting corpus has 393,132 sentence pairs. 3,376 of these are omitted for ITG methods because their highconfidence alignments have ITG-incompatible constructions. Like our predecessors (Marcu and Wong, 2002; Birch et al., 2006), we apply a lexicon constraint: no monolingual phrase can be used by any phrasal model unless it occurs at least five times. High-confidence alignments are provided by intersecting GIZA++ alignments trained in each direction with 5 iterations each of Model 1, HMM, and Model 4. All GIZA++ alignments are trained with no sentence-length limit, using the full 688K c</context>
<context position="25583" citStr="Koehn and Monz, 2006" startWordPosition="4134" endWordPosition="4137">nslation models outperform the surface heuristic? 3. Do Viterbi phrasal alignments provide better input for the surface heuristic? With this in mind, we test five phrase tables. Two are conditionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000- sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>P. Koehn and C. Monz. 2006. Manual and automatic evaluation of machine translation. In HLT-NACCL Workshop on Statistical Machine Translation, pages 102–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="3273" citStr="Koehn et al., 2003" startWordPosition="503" endWordPosition="506">gorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (JPTM) and inversion transduction grammar (ITG). Section 3 describes our proposed solution, a phrasal ITG. Section 4 describes how to apply our phrasal ITG, both as a translation model and as a phrasal word-aligner. Section 5 tests our system in both these capacities, while Section 6 concludes. 2 Background 2.1 Phrase Table Extraction Phrasal decoders require a phrase table (Koehn et al., 2003), which contains bilingual phrase pairs and 17 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17–24, Rochester, New York, April 2007. c�2007 Association for Computational Linguistics scores indicating their utility. The surface heuristic is the most popular method for phrase-table construction. It extracts all consistent phrase pairs from word-aligned bitext (Koehn et al., 2003). The word alignment provides bilingual links, indicating translation relationships between words. Consistency is defined so that alignment links are never </context>
<context position="5037" citStr="Koehn et al., 2003" startWordPosition="778" endWordPosition="781">IZA++ (Och and Ney, 2003). This alignment system is powered by the IBM translation models (Brown et al., 1993), in which one sentence generates the other. These models produce only one-to-many alignments: each generated token can participate in at most one link. Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed (Och and Ney, 2003). Combination approaches begin with the intersection of the two alignments, and add links from the union heuristically. The grow-diag-final (GDF) combination heuristic (Koehn et al., 2003) adds links so that each new link connects a previously unlinked token. 2.2 Joint phrasal translation model The IBM models that power GIZA++ are trained with Expectation Maximization (Dempster et al., 1977), or EM, on sentence-aligned bitext. A translation model assigns probabilities to alignments; these alignment distributions are used to count translation events, which are then used to estimate new parameters for the translation model. Sampling is employed when the alignment distributions cannot be calculated efficiently. This statistically-motivated process is much more appealing than the f</context>
<context position="18022" citStr="Koehn et al., 2003" startWordPosition="2912" endWordPosition="2915">omplete training set. 4 Applying the model Any phrasal translation model can be used for two tasks: translation modeling and phrasal word alignment. Previous work on JPTM has focused on only the first task. We are interested in phrasal alignment because it may be better suited to heuristic phraseextraction than word-based models. This section describes how to use our phrasal ITG first as a translation model, and then as a phrasal aligner. 4.1 Translation Modeling We can test our model’s utility for translation by transforming its parameters into a phrase table for the phrasal decoder Pharaoh (Koehn et al., 2003). Any joint model can produce the necessary conditional probabilities by conditionalizing the joint table in both directions. We use our p(¯e/ ¯f|C) distribution from our stochastic grammar to produce p(¯e |¯f) and p(¯f|¯e) values for its phrasal lexicon. Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs (Koehn et al., 2003). Using the phrasal ITG as a direct translation model, we do not produce alignments for individual sentence pairs. Instead, we provide a lexical preference with an IBM Model 1 feature pM1 that penalizes u</context>
<context position="25519" citStr="Koehn et al., 2003" startWordPosition="4123" endWordPosition="4126">Does the phrasal ITG improve on the C-JPTM? 2. Can phrasal translation models outperform the surface heuristic? 3. Do Viterbi phrasal alignments provide better input for the surface heuristic? With this in mind, we test five phrase tables. Two are conditionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000- sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasebased translation. In HLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistic machine translation.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="5776" citStr="Marcu and Wong, 2002" startWordPosition="890" endWordPosition="893">dels that power GIZA++ are trained with Expectation Maximization (Dempster et al., 1977), or EM, on sentence-aligned bitext. A translation model assigns probabilities to alignments; these alignment distributions are used to count translation events, which are then used to estimate new parameters for the translation model. Sampling is employed when the alignment distributions cannot be calculated efficiently. This statistically-motivated process is much more appealing than the flat counting described in Section 2.1, but it does not directly include phrases. The joint phrasal translation model (Marcu and Wong, 2002), or JPTM, applies the same statistical techniques from the IBM models in a phrasal setting. The JPTM is designed according to a generative process where both languages are generated simultaneously. First, a bag of concepts, or cepts, C is generated. Each ci E C corresponds to a bilingual phrase pair, ci = (ei, �fi). These contiguous phrases are permuted in each language to create two sequences of phrases. Initially, Marcu and Wong assume that the number of cepts, as well as the phrase orderings, are drawn from uniform distributions. That leaves a joint translation distribution p(ei, �fi) to d</context>
<context position="7432" citStr="Marcu and Wong (2002)" startWordPosition="1181" endWordPosition="1184">on model based on absolute token positions is added to (1). The JPTM faces several problems when scaling up to large training sets: 1. The alignment space enumerated by the sum in (1) is huge, far larger than the one-to-many space explored by GIZA++. 2. The translation distribution p(e, all co-occurring phrases observed in the bitext. This is far too large to fit in main memory, and can be unwieldly for storage on disk. 3. Given a non-uniform p(e, 1), there is no efficient algorithm to compute the expectation of phrase pair counts required for EM, or to find the most likely phrasal alignment. Marcu and Wong (2002) address point 2 with a lexicon constraint; monolingual phrases that are above a length threshold or below a frequency threshold are excluded from the lexicon. Point 3 is handled by hill-climbing to a likely phrasal alignment and sampling around it. However, point 1 remains unaddressed, which prevents the model from scaling to large data sets. Birch et al. (2006) handle point 1 directly by reducing the size of the alignment space. This is f) will cover 18 accomplished by constraining the JPTM to only use phrase pairs that are consistent with a highconfidence word alignment, which is provided b</context>
<context position="21577" citStr="Marcu and Wong, 2002" startWordPosition="3476" endWordPosition="3479">tion 4.1. 5 Experiments and Results In this section, we first verify the effectiveness of fixed-link pruning, and then test our phrasal ITG, both as an aligner and as a translation model. We train all translation models with a French-English Europarl corpus obtained by applying a 25 token sentence-length limit to the training set provided for the HLT-NAACL SMT Workshop Shared Task (Koehn and Monz, 2006). The resulting corpus has 393,132 sentence pairs. 3,376 of these are omitted for ITG methods because their highconfidence alignments have ITG-incompatible constructions. Like our predecessors (Marcu and Wong, 2002; Birch et al., 2006), we apply a lexicon constraint: no monolingual phrase can be used by any phrasal model unless it occurs at least five times. High-confidence alignments are provided by intersecting GIZA++ alignments trained in each direction with 5 iterations each of Model 1, HMM, and Model 4. All GIZA++ alignments are trained with no sentence-length limit, using the full 688K corpus. 5.1 Pruning Speed Experiments To measure the speed-up provided by fixed-link pruning, we timed our phrasal inside-outside algorithm on the first 100 sentence pairs in our training set, with and without pruni</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A phrase-based, joint probability model for statistic machine translation. In EMNLP, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Manual annotation of translational equivalence: The blinker project.</title>
<date>1998</date>
<tech>Technical Report 98-07,</tech>
<institution>Institute for Research in Cognitive Science.</institution>
<contexts>
<context position="19640" citStr="Melamed, 1998" startWordPosition="3175" endWordPosition="3176">ur phrasal ITG are usually unsatisfying. For example, the fragment pair (order of business, ordre des travaux) is aligned as a phrase pair by our system, linking every English word to every French word. This is frustrating, since there is a clear compositional relationship between the fragment’s component words. This happens because the system seeks only to maximize the likelihood of its training corpus, and phrases are far more efficient than word-to-word connections. When aligning text, annotators are told to resort to many-to-many links only when no clear compositional relationship exists (Melamed, 1998). If we could tell our phrasal aligner the same thing, we could greatly improve the intuitive appeal of our alignments. Again, we can leverage high-confidence links for help. In the high-confidence alignments provided by GIZA++ intersection, each token participates in at most one link. Links only appear when two wordbased IBM translation models can agree. Therefore, they occur at points of high compositionality: the two words clearly account for one another. We adopt an alignment-driven definition of compositionality: any phrase pair containing two or more highconfidence links is compositional</context>
<context position="23233" citStr="Melamed, 1998" startWordPosition="3749" endWordPosition="3750">validate our noncompositional constraint and to select good alignments for use with the surface heuristic. Table 1: Inside-outside run-time comparison. Method Seconds Avg. Spans Pruned No Prune 415 - Tic-tac-toe 37 68% Fixed link 5 95% Table 2: Alignment Comparison. Method Prec Rec F-measure GIZA++ Intersect 96.7 53.0 68.5 GIZA++ Union 82.5 69.0 75.1 GIZA++ GDF 84.0 68.2 75.2 Phrasal ITG 50.7 80.3 62.2 Phrasal ITG + NCC 75.4 78.0 76.7 Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). We did not differentiate between sure and possible links. We report precision, recall and balanced F-measure (Och and Ney, 2003). For comparison purposes, we include the results of three types of GIZA++ combination, including the grow-diag-final heuristic (GDF). We tested our phrasal ITG with fixed link pruning, and then added the non-compositional constraint (NCC). During development we determined that performance levels off for both of the ITG models after 3 EM iterations. The results are shown in Table 2. The first thing to note is that GIZA++ Intersection is indeed very high precision. O</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. D. Melamed. 1998. Manual annotation of translational equivalence: The blinker project. Technical Report 98-07, Institute for Research in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Multitext grammars and synchronous parsers.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>158--165</pages>
<contexts>
<context position="9788" citStr="Melamed, 2003" startWordPosition="1564" endWordPosition="1565">with their non-terminals inside square brackets [...], produce their symbols in the given order in both streams. Inverted productions, indicated by angled brackets (...), are output in reverse order in the Foreign stream only. The work described here uses the binary bracketing ITG, which has a single non-terminal: A —* [AA] |(AA) |e/f (2) This grammar admits an efficient bitext parsing algorithm, and holds no language-specific biases. (2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003). This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). Figure 1: The two ITG forbidden structures. a) A-[AA] b) A-&lt;AA&gt; c) A-e/f Figure 2: Three ways in which a phrasal ITG can analyze a multi-word span or phrase. Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A —* X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. </context>
</contexts>
<marker>Melamed, 2003</marker>
<rawString>I. D. Melamed. 2003. Multitext grammars and synchronous parsers. In HLT-NAACL, pages 158–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4443" citStr="Och and Ney, 2003" startWordPosition="684" endWordPosition="687">cy is defined so that alignment links are never broken by phrase boundaries. For each token w in a consistent phrase pair p, all tokens linked to w by the alignment must also be included in p. Each consistent phrase pair is counted as occurring once per sentence pair. The scores for the extracted phrase pairs are provided by normalizing these flat counts according to common English or Foreign components, producing the conditional distributions p( f|e) and p(e |f). The surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by GIZA++ (Och and Ney, 2003). This alignment system is powered by the IBM translation models (Brown et al., 1993), in which one sentence generates the other. These models produce only one-to-many alignments: each generated token can participate in at most one link. Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed (Och and Ney, 2003). Combination approaches begin with the intersection of the two alignments, and add links from the union heuristically. The grow-diag-final (GDF) combination heuristic (Koehn et al., 2003) adds </context>
<context position="23363" citStr="Och and Ney, 2003" startWordPosition="3769" endWordPosition="3772">side run-time comparison. Method Seconds Avg. Spans Pruned No Prune 415 - Tic-tac-toe 37 68% Fixed link 5 95% Table 2: Alignment Comparison. Method Prec Rec F-measure GIZA++ Intersect 96.7 53.0 68.5 GIZA++ Union 82.5 69.0 75.1 GIZA++ GDF 84.0 68.2 75.2 Phrasal ITG 50.7 80.3 62.2 Phrasal ITG + NCC 75.4 78.0 76.7 Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). We did not differentiate between sure and possible links. We report precision, recall and balanced F-measure (Och and Ney, 2003). For comparison purposes, we include the results of three types of GIZA++ combination, including the grow-diag-final heuristic (GDF). We tested our phrasal ITG with fixed link pruning, and then added the non-compositional constraint (NCC). During development we determined that performance levels off for both of the ITG models after 3 EM iterations. The results are shown in Table 2. The first thing to note is that GIZA++ Intersection is indeed very high precision. Our confidence in it as a constraint is not misplaced. We also see that both phrasal models have significantly higher recall than a</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="25733" citStr="Och, 2003" startWordPosition="4161" endWordPosition="4162">e phrase tables. Two are conditionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000- sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3Supplied by personal communication</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25893" citStr="Papineni et al., 2002" startWordPosition="4184" endWordPosition="4187">• Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000- sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3Supplied by personal communication. Run with default parameters, but with maximum phrase length increased to 5. Table 3: Translation Comparison. Method BLEU +M1 Size Conditionalized Phrasal Mode</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>S Vogel</author>
</authors>
<title>Considerations in maximum mutual information and minimum classification error training for statistical machine translation.</title>
<date>2005</date>
<booktitle>In EAMT.</booktitle>
<contexts>
<context position="25778" citStr="Venugopal and Vogel (2005)" startWordPosition="4166" endWordPosition="4169">itionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000- sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3Supplied by personal communication. Run with default parameters, but with maxim</context>
</contexts>
<marker>Venugopal, Vogel, 2005</marker>
<rawString>A. Venugopal and S. Vogel. 2005. Considerations in maximum mutual information and minimum classification error training for statistical machine translation. In EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Vilar</author>
<author>E Vidal</author>
</authors>
<title>A recursive statistical translation model.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>199--207</pages>
<contexts>
<context position="11815" citStr="Vilar and Vidal (2005)" startWordPosition="1905" endWordPosition="1908">r by altering the definition of a terminal production to include phrases: A —* 6/ 1. This third option is shown in Figure 2 (c). The model implied by this extended grammar is trained using inside-outside and EM. Our approach differs from previous attempts to use ITGs for phrasal bitext analysis. Wu (1997) used a binary bracketing ITG to segment a senf7 f2 f3 fa f7 f2 f3 fa e3 ea e3 e2 e7 ea e2 e7 calmez vous calm down calmez vous calm down calmez vous calm down 19 tence while simultaneously word-aligning it to its translation, but the model was trained heuristically with a fixed segmentation. Vilar and Vidal (2005) used ITG-like dynamic programming to drive both training and alignment for their recursive translation model, but they employed a conditional model that did not maintain a phrasal lexicon. Instead, they scored phrase pairs using IBM Model 1. Our phrasal ITG is quite similar to the JPTM. Both models are trained with EM, and both employ generative stories that create a sentence and its translation simultaneously. The similarities become more apparent when we consider the canonical-form binary-bracketing ITG (Wu, 1997) shown here: 5 A |B |C (3) A [AB] |[BB] |[CB] | [AC] |[BC] |[CC] B (AA) |(BA) </context>
</contexts>
<marker>Vilar, Vidal, 2005</marker>
<rawString>J. M. Vilar and E. Vidal. 2005. A recursive statistical translation model. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>Y Zhang</author>
<author>F Huang</author>
<author>A Tribble</author>
<author>A Venugopal</author>
<author>B Zhang</author>
<author>A Waibel</author>
</authors>
<title>The CMU statistical machine translation system.</title>
<date>2003</date>
<booktitle>In MT Summmit.</booktitle>
<contexts>
<context position="18657" citStr="Vogel et al., 2003" startWordPosition="3018" endWordPosition="3021">l can produce the necessary conditional probabilities by conditionalizing the joint table in both directions. We use our p(¯e/ ¯f|C) distribution from our stochastic grammar to produce p(¯e |¯f) and p(¯f|¯e) values for its phrasal lexicon. Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs (Koehn et al., 2003). Using the phrasal ITG as a direct translation model, we do not produce alignments for individual sentence pairs. Instead, we provide a lexical preference with an IBM Model 1 feature pM1 that penalizes unmatched words (Vogel et al., 2003). We include both pM1(¯e |f) and pM1( f|¯e). 4.2 Phrasal Word Alignment We can produce a translation model using insideoutside, without ever creating a Viterbi parse. However, we can also examine the maximum likelihood phrasal alignments predicted by the trained model. Despite its strengths derived from using phrases throughout training, the alignments predicted by our phrasal ITG are usually unsatisfying. For example, the fragment pair (order of business, ordre des travaux) is aligned as a phrase pair by our system, linking every English word to every French word. This is frustrating, since t</context>
</contexts>
<marker>Vogel, Zhang, Huang, Tribble, Venugopal, Zhang, Waibel, 2003</marker>
<rawString>S. Vogel, Y. Zhang, F. Huang, A. Tribble, A. Venugopal, B. Zhang, and A. Waibel. 2003. The CMU statistical machine translation system. In MT Summmit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2473" citStr="Wu, 1997" startWordPosition="372" endWordPosition="373">e statistical analysis used at the word level in a phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space. Hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (Birch et al., 2006), but suffer from the daunting task of heuristically exploring a still very large alignment space. In the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (Wu, 1997). In this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of both to create a statistically and algorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (JPTM) and inversion transduction grammar (ITG). Section 3 describes our proposed solution, a phrasal ITG. Section 4 describes how to apply our phrasal ITG, both as a translation model and as a phr</context>
<context position="8855" citStr="Wu, 1997" startWordPosition="1414" endWordPosition="1415">d alignment is used to limit the phrase pairs considered, but the C-JPTM reasons about distributions over phrasal alignments, instead of taking flat counts. The consistency constraint allows them to scale their C-JPTM up to 700,000 sentence pairs. With this constraint in place, the use of hill-climbing and sampling during EM training becomes one of the largest remaining weaknesses of the C-JPTM. 2.3 Inversion Transduction Grammar Like the JPTM, stochastic synchronous grammars provide a generative process to produce a sentence and its translation simultaneously. Inversion transduction grammar (Wu, 1997), or ITG, is a wellstudied synchronous grammar formalism. Terminal productions of the form A —* e/f produce a token in each stream, or a token in one stream with the null symbol 0 in the other. To allow for movement during translation, non-terminal productions can be either straight or inverted. Straight productions, with their non-terminals inside square brackets [...], produce their symbols in the given order in both streams. Inverted productions, indicated by angled brackets (...), are output in reverse order in the Foreign stream only. The work described here uses the binary bracketing ITG</context>
<context position="10247" citStr="Wu, 1997" startWordPosition="1637" endWordPosition="1638">possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003). This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). Figure 1: The two ITG forbidden structures. a) A-[AA] b) A-&lt;AA&gt; c) A-e/f Figure 2: Three ways in which a phrasal ITG can analyze a multi-word span or phrase. Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A —* X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004). 3 ITG as a Phrasal Translation Model This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM. ITG parsing algorithms consider every possible two-dimensional span of bitext, each corresponding to a bilingual phrase pair. Each multi-token span is</context>
<context position="11499" citStr="Wu (1997)" startWordPosition="1847" endWordPosition="1848"> from smaller spans using a straight or inverted production, as is illustrated in Figures 2 (a) and (b). To extend ITG to a phrasal setting, we add a third option for span analysis: that the span under consideration might have been drawn directly from the lexicon. This option can be added to our grammar by altering the definition of a terminal production to include phrases: A —* 6/ 1. This third option is shown in Figure 2 (c). The model implied by this extended grammar is trained using inside-outside and EM. Our approach differs from previous attempts to use ITGs for phrasal bitext analysis. Wu (1997) used a binary bracketing ITG to segment a senf7 f2 f3 fa f7 f2 f3 fa e3 ea e3 e2 e7 ea e2 e7 calmez vous calm down calmez vous calm down calmez vous calm down 19 tence while simultaneously word-aligning it to its translation, but the model was trained heuristically with a fixed segmentation. Vilar and Vidal (2005) used ITG-like dynamic programming to drive both training and alignment for their recursive translation model, but they employed a conditional model that did not maintain a phrasal lexicon. Instead, they scored phrase pairs using IBM Model 1. Our phrasal ITG is quite similar to the J</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Syntax-based alignment: Supervised or unsupervised? In</title>
<date>2004</date>
<booktitle>COLING,</booktitle>
<pages>418--424</pages>
<contexts>
<context position="10578" citStr="Zhang and Gildea, 2004" startWordPosition="1684" endWordPosition="1687">anslations violate these constraints (Cherry and Lin, 2006). Figure 1: The two ITG forbidden structures. a) A-[AA] b) A-&lt;AA&gt; c) A-e/f Figure 2: Three ways in which a phrasal ITG can analyze a multi-word span or phrase. Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A —* X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004). 3 ITG as a Phrasal Translation Model This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM. ITG parsing algorithms consider every possible two-dimensional span of bitext, each corresponding to a bilingual phrase pair. Each multi-token span is analyzed in terms of how it could be built from smaller spans using a straight or inverted production, as is illustrated in Figures 2 (a) and (b). To extend ITG to a phrasal setting, we add a third option for span analysis: that the span under consideration might have been drawn directly from the lexicon. This option can be adde</context>
</contexts>
<marker>Zhang, Gildea, 2004</marker>
<rawString>H. Zhang and D. Gildea. 2004. Syntax-based alignment: Supervised or unsupervised? In COLING, pages 418–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>475--482</pages>
<contexts>
<context position="15161" citStr="Zhang and Gildea, 2005" startWordPosition="2445" endWordPosition="2448">. We will address each of these issues in the following two subsections. 3.1 Pruning Spans First, we address the problem of scaling ITG to large data. ITG dynamic programming algorithms work by analyzing each bitext span only once, storing its value in a table for future use. There are O(n4) of these spans, and each analysis takes O(n2) time. An effective approach to speeding up ITG algorithms is to eliminate unlikely spans as a preprocessing step, assigning them 0 probability and saving the time spent processing them. Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). The former is referred to as tic-tac-toe pruning because it uses both inside and outside estimates. We propose a new ITG pruning method that leverages high-confidence links by pruning all spans that are inconsistent with a provided alignment. This is similar to the constraint used in the C-JPTM, but we do not just eliminate those spans as potential phrase-to-phrase links: we never consider any ITG parse that builds a non-terminal over a pruned span.2 This fixed-link pruning will speed up both Viterbi alignment and EM train</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>H. Zhang and D. Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In ACL, pages 475– 482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>L Huang</author>
<author>D Gildea</author>
<author>K Knight</author>
</authors>
<title>Synchronous binarization for machine translation. In</title>
<date>2006</date>
<booktitle>HLTNAACL,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="17061" citStr="Zhang et al., 2006" startWordPosition="2748" endWordPosition="2751">ing data; if the system is unable to build the right alignment, the counts it will collect from that pair must be wrong. Furthermore, if our high-confidence links are not ITG-compatible, our fixed-link pruning will prevent the aligner from forming any alignments at all. However, these two potential problems cancel each other out. Sentence pairs containing non-ITG translations will tend to have high-confidence links that are also not ITG-compatible. Our EM learner will simply skip these sentence pairs during training, avoiding pollution of our training data. We can use a linear-time algorithm (Zhang et al., 2006) to detect non-ITG movement in our high-confidence links, and remove the offending sentence pairs from our training corpus. This results in only a minor reduction in training data; in our French-English training set, we lose less than 1%. In the experiments described in Section 5, all systems that do not use ITG will take advantage of the complete training set. 4 Applying the model Any phrasal translation model can be used for two tasks: translation modeling and phrasal word alignment. Previous work on JPTM has focused on only the first task. We are interested in phrasal alignment because it m</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006. Synchronous binarization for machine translation. In HLTNAACL, pages 256–263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>