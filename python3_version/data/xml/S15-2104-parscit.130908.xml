<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023849">
<title confidence="0.938146">
SWASH: A Naive Bayes Classifier for Tweet Sentiment Identification
</title>
<author confidence="0.956751">
Ruth Talbot&apos;, Chloe Acheampong2 and Richard Wicentowski&apos;
</author>
<affiliation confidence="0.9014205">
&apos;Swarthmore College, Swarthmore, PA USA
2Ashesi University, Accra, Ghana
</affiliation>
<email confidence="0.9869425">
{rtalbot1, richardw}@cs.swarthmore.edu
chloeacheampong@gmail.com
</email>
<sectionHeader confidence="0.995481" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.545166277777778">
This paper describes a sentiment classifica-
tion system designed for SemEval-2015, Task
10, Subtask B. The system employs a con-
strained, supervised text categorization ap-
proach. Firstly, since thorough preprocess-
ing of tweet data was shown to be effective
in previous SemEval sentiment classification
tasks, various preprocessessing steps were in-
troduced to enhance the quality of lexical in-
formation. Secondly, a Naive Bayes classi-
fier is used to detect tweet sentiment. The
classifier is trained only on the training data
provided by the task organizers. The system
makes use of external human-generated lists
of positive and negative words at several steps
throughout classification. The system pro-
duced an overall F-score of 59.26 on the of-
ficial test set.
</bodyText>
<sectionHeader confidence="0.998336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983892857143">
Over the past few years, an increasing number of
people have begun to express their opinion through
social networks and microblogging services. Twit-
ter, as one of the most popular of these social net-
works, has become a major platform for social com-
munication, allowing its users to send and read short
messages called ‘tweets’. Tweets have become im-
portant in a variety of tasks, including the predic-
tion of election results (O’Connor et al., 2010). The
emergence of online expressions of opinion has at-
tracted interest in sentiment analysis of tweets in
both academia and industry. Sentiment analysis,
also known as opinion mining, focuses on computa-
tional treatments of sentiments (emotions, attitudes,
opinions) in natural language text. In this paper we
describe our submission to Task 10, subtask B: Mes-
sage Polarity Classification. The task is defined as:
‘Given a message, classify whether the message is of
positive, negative, or neutral sentiment. For a mes-
sage conveying both a positive and negative senti-
ment, whichever is the stronger sentiment should be
chosen’ (Rosenthal et al., 2015).
This paper describes a system which utilizes a
Naive Bayes classifier to determine the sentiment of
tweets. This paper describes the resources used, the
system details, including preprocessing steps taken,
feature extraction and classifier implemented, and
the test runs and end results.
</bodyText>
<sectionHeader confidence="0.999171" genericHeader="introduction">
2 Resources
</sectionHeader>
<subsectionHeader confidence="0.999454">
2.1 Labeled Tweets
</subsectionHeader>
<bodyText confidence="0.9999664">
This system is constrained, and the only training
data used is the sentiment labeled training data pro-
vided by the task organizers. The training data we
used includes 8142 tweets, each labeled as positive,
negative or neutral.
</bodyText>
<subsectionHeader confidence="0.999861">
2.2 Sentiment Lexicon
</subsectionHeader>
<bodyText confidence="0.9999793">
Our system relies on an external lexicon of approx-
imately 6800 tokens labeled as either positive or
negative (Liu et al., 2005). The lexicon consists
of words that humans have tagged as having either
strongly negative or strongly positive sentiment. If a
word in a tweet is preidentified as highly positive or
negative, we add a special feature to the tweet’s fea-
tures to indicate that the tweet included a highly pos-
itive word or a highly negative word (Kiritchenko
et al., 2014). Although multiple lexicons exist, e.g.
</bodyText>
<page confidence="0.985421">
626
</page>
<note confidence="0.688055">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 626–630,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.9998618">
Preprocessing Step F1 score change
jazzy −5.67
stopwords −.56
negation 1.74
username normalization 0.34
url normalization 0.40
overriding 1.74
lowercasing 2.21
tokenization 4.00
sentiment lexicon 5.81
</table>
<tableCaption confidence="0.997083666666667">
Table 1: Changes in F1-score obtained by each prepro-
cessing step (taken individually, not cumulatively) using
5-fold cross validation on the provided training set.
</tableCaption>
<bodyText confidence="0.978706">
(Wilson et al., 2005) and (Mohammad et al., 2013),
we were unable to include them due to time con-
straints.
</bodyText>
<sectionHeader confidence="0.993206" genericHeader="method">
3 System Details
</sectionHeader>
<bodyText confidence="0.999978714285714">
The system consists of several preprocessing steps,
feature extraction, a Naive Bayes classifier and a
secondary classifier that makes use of tokens that are
strongly correlating with either a positive or nega-
tive sentiment. Improvements that were attempted
but were unsuccessful in improving the system are
also described.
</bodyText>
<subsectionHeader confidence="0.998987">
3.1 Preprocessing Steps
3.1.1 Tokenization
</subsectionHeader>
<bodyText confidence="0.9999934">
All tweets are tokenized using Twokenizer, a
Twitter-specific tokenizer (Owoputi et al., 2013).
The tokenizer can detect and handle conditions un-
likely to occur in more formal writing, such as men-
tions, hashtags and retweet tokens.
</bodyText>
<subsectionHeader confidence="0.862538">
3.1.2 Normalization
</subsectionHeader>
<bodyText confidence="0.998942">
During preprocessing, all tweets are normalized.
This included several steps:
</bodyText>
<listItem confidence="0.999279">
• Lowercasing all words (e.g. ‘Hello’ to ‘hello’
or ‘heRe’ to ‘here’)
• Converting all URLs (identified as strings con-
taining ‘.com’, ‘http’, ‘www’ and ‘.co’) to the
string ‘URL’
• Converting all mentions (identified as strings
beginning with ‘@’) to ‘username’
</listItem>
<subsectionHeader confidence="0.87284">
3.1.3 Negation
</subsectionHeader>
<bodyText confidence="0.999995842105263">
The system implements a basic version of nega-
tion to improve the accuracy of the classifier. When
processing the data, any words in between a neg-
ative adverb or verb, a ‘negation key’ (e.g. never,
not, can’t) and the next end of sentence indicator,
in this case, any punctuation symbol, are negated.
Negation was implemented to avoid misclassifica-
tion of tweets due to a word of one sentiment follow-
ing a negation key and therefore being of the oppo-
site sentiment. For instance, a sentence could state:
“That movie was not the best thing I’ve ever seen.”
Clearly, this sentence is negative, but without nega-
tion, the presence of the word ‘best,’ a typically pos-
itive word, might lead this tweet to be classified as
positive, not negative. If however, a tag is added (in
this case ‘NOT ’) to any words following a negation
key, those words will be more likely to be classified
appropriately, as ‘NOT best’ will more often be seen
in negative contexts (Kiritchenko et al., 2014).
</bodyText>
<subsectionHeader confidence="0.845777">
3.1.4 Other Preprocessing Considered
</subsectionHeader>
<bodyText confidence="0.999982291666667">
Several other preprocessing steps were consid-
ered. In particular, a spell corrector, Jazzy1, was
used as it had previously been shown to be effective
(Miura et al., 2014). This step was taken to reduce
dimensionality and provide better matches with the
sentiment lexicon, e.g. converting ‘luve’ into ‘love’,
so instead of seeing ‘love’ once in a positive con-
text and ‘luve’ once in a positive context, we would
see ‘love’ twice in a positive context, giving it more
weight as a positive feature and finding a match in
the sentiment lexicon. However, Jazzy actually re-
duced accuracy and F-score of our system. One po-
tential explanation for this finding is that tweets may
contain significant amounts of abbreviations, slang
and misspellings that are too far removed from the
original spelling for a spell checker to identify and
adjust to its correct spelling.
Additionally, removing stopwords was attempted.
A list of the 25 most common words in the En-
glish language was acquired using the Brown Cor-
pus. This list provided the system with common
words unlikely to be strongly associated with any
sense. These words were then removed before fea-
ture selection. In our final implementation of the
</bodyText>
<footnote confidence="0.992952">
1http://jazzy.sourceforge.net/
</footnote>
<page confidence="0.971819">
627
</page>
<table confidence="0.9997564">
Sentiment Precision Recall F-score
Negative 61.72 50.45 55.52
Positive 73.98 66.17 69.86
Neutral 64.09 76.21 69.63
F1 score: 62.69 Accuracy: 67.42
</table>
<tableCaption confidence="0.910572">
Table 2: F-scores for individual sentiments and over-
all score, produced using 5-fold cross validation on
SemEval-2015 training data.
</tableCaption>
<bodyText confidence="0.9184795">
classifier, removing stopwords has a small negative
effect on performance.
</bodyText>
<subsectionHeader confidence="0.997743">
3.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999968538461539">
The features used in our classifier are unigrams,
negated unigrams, and two special tags indicating
the presence or absence of words in the tweet be-
ing found in the sentiment lexicon. During prepro-
cessing, negated unigrams are created by prepending
‘NOT ’ to a unigram if it follows a negation key, de-
scribed above. If the unigram follows a negation key,
only the negated unigram, not its original form, is in-
cluded as a feature. In addition, a ‘positive’ or ‘neg-
ative’ feature (represented by ‘POSW’ or ‘NEGW’)
is added for each positive or negative word a tweet
contained, as identified by inclusion in the sentiment
lexicon.
</bodyText>
<subsectionHeader confidence="0.961474">
3.3 Classifiers
3.3.1 Naive Bayes Classifier
</subsectionHeader>
<bodyText confidence="0.999990466666667">
We used a Naive Bayes classifier to classify the
tweets. Naive Bayes relies on the assumption of
conditional independence among the features, some-
thing that is clearly not true here. While Naive
Bayes classifiers manage to perform well despite
this assumption, a classifier not reliant on this as-
sumption might outperform a Naive Bayes classifier
(Gamallo and Garcia, 2014).
The Naive Bayes classifier employed Laplace
smoothing. More advanced smoothing techniques
were attempted, but actually reduced both the ac-
curacy and F1 score of the system. The additive
smoothing constant was empirically chosen to be
0.4. The Naive Bayes classifier was trained solely
on the training data from SemEval-2015.
</bodyText>
<subsectionHeader confidence="0.85253">
3.3.2 Other Classifiers Attempted
</subsectionHeader>
<bodyText confidence="0.999959705882353">
In addition to Naive Bayes, several other classi-
fiers were tried, and an attempt was made to em-
ploy a combination of multiple classifiers to predict
sentiment. These classifiers included a typical de-
cision list (which defaults to most frequent sense
classification), and a number of classifiers included
in scikit-learn (Pedregosa et al., 2011): LinearSVC,
GaussianNB, NearestCentroid, MultinomialNB, and
BernoulliNB. Each classifier used the same prepro-
cessing and feature selection employed by the Naive
Bayes classifier. However, after implementing all of
these classifiers and attempting to use a combina-
tion of their sense decisions to make a more accu-
rate prediction, none of the classifiers, nor any com-
bination of their decisions, outperformed the Naive
Bayes classifier, and therefore none were used in our
submission.
</bodyText>
<subsectionHeader confidence="0.994694">
3.3.3 Post-Processing
</subsectionHeader>
<bodyText confidence="0.999858416666667">
Several features were identified that, when
present, were strongly indicative of a positive or
negative sense (e.g. ‘:)’, ‘:(’, ‘awful’, ‘love’). If one
of those features was present in a tweet, a rule-based
system overrode the decision of the Naive Bayes
classifier, labeling the tweet as either positive or neg-
ative. This step was conducted after negation so
that no unnegated words would be used to classify a
tweet incorrectly. Suprisingly, this ‘overriding’ step
improved our F1 score by several points, indicating
that there are several features that when present are
strongly indicative of a tweet’s sense.
These strongly positive or negative overriding fea-
tures were determined by inspection of training data
and using our own knowledge to come up with sym-
bols and words which were highly polar in senti-
ment. The positive word list contained 4 emoticons2
and 7 overly positive words: ‘love’, ‘great’, ‘happy’,
‘wonderful’, ‘good’, ‘perfect’, and ‘beautiful’. The
negative list contained 6 emoticons3, 4 curse words
and 5 negative words: ‘fuck’, ‘shit’, ‘ass’, ‘crap’,
‘hate’, ‘awful’, ‘stupid’, ‘horrible’, and ‘ugh’. Fu-
ture work could include automatically inducing such
a list from training data.
</bodyText>
<footnote confidence="0.9908375">
2Positive :) :D :-) ;)
3Negative :( :-( :/ :’( :.( &gt;:(
</footnote>
<page confidence="0.988111">
628
</page>
<table confidence="0.9998328">
Laplace F-score sentiment F-score
λ weight
.3 62.40 1 59.25
.4 62.69 5 62.69
.5 62.39 6 62.39
</table>
<tableCaption confidence="0.9549696">
Table 3: Two parameters empirically determined using
crossvalidation. In Laplace smoothing, λ is the additive
constant for unknown words. The ‘positive’ and ‘nega-
tive’ features introduced by the sentiment lexicon were
given five times the weight of the token unigrams.
</tableCaption>
<table confidence="0.999956285714286">
Dataset Rank F1
Twitter 2015 21 59.26
Live Journal 2014 24 69.43
SMS 2013 34 56.49
Twitter 2013 27 63.07
Twitter 2014 31 62.93
Twitter 2014 Sarcasm 15 48.42
</table>
<tableCaption confidence="0.787241">
Table 4: Performance on the official 2015 test data as well
as on the progress data sets.
</tableCaption>
<sectionHeader confidence="0.991883" genericHeader="method">
4 Test Runs
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999932111111111">
This paper describes the implementation of a senti-
ment classification system that uses extensive pre-
processing and a Naive Bayes sentiment classifier.
Using only a Naive Bayes classifier the system
achieved a 59.26 F1 score, placing 21st out of 40
overall in Task 10, subtask B. Interestingly, our sys-
tem overperformed in the sarcasm progress data set,
requiring some further investigation.
While our attempt at weighting the decision of
multiple classifiers was unsuccessful, we believe
this was due to using the same features for each clas-
sifier, and that these features may have been overfit-
ted to those found effective in a Naive Bayes classi-
fier.
Additionally, our human-generated list of posi-
tive and negative words and symbols, whose pres-
ence automatically overrode the classifier’s decision,
should be further explored. It is highly likely that
more words and symbols exist whose presence is
highly indicative of a negative or positive tweet sen-
timent. Automatic creation of these lists would
likely improve performance and be more experimen-
tally justified.
In addition to attempting additional classifiers, sev-
eral parameter values were experimented with using
5-fold cross validation to determine which produced
the best F-scores.
</bodyText>
<subsectionHeader confidence="0.984829">
4.1 Parameter Selection
</subsectionHeader>
<bodyText confidence="0.999996071428572">
As mentioned earlier, the constant used for addi-
tive Laplace smoothing was determined empirically.
Values between 0 and 1 were tested, and it was de-
termined that the ideal value was 0.4. Table 3 shows
the change in score for 3 different values close to
0.4.
The second parameter tuned empirically was the
weight given to the ‘positive’ and ‘negative’ fea-
tures added if a tweet contained a positive or neg-
ative word listed in the sentiment lexicon. After ex-
perimenting with various ways of oversampling this
feature, we determined that giving these words five
times the weight of other unigrams was the optimal
number under crossvalidation. (see Table 3).
</bodyText>
<sectionHeader confidence="0.998491" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99909752173913">
Pablo Gamallo and Marcos Garcia. 2014. Citius: A
naive-bayes strategy for sentiment analysis on english
tweets. In Proceedings of SemEval-2014, pages 171–
175.
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. NRC-Canada-2014: Detect-
ing aspects and sentiment in customer reviews. In Pro-
ceedings of SemEval-2014, pages 437–442.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the Web. In Proceedings of WWW’05, pages 342–
351.
Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and
Tomoko Ohkuma. 2014. TeamX: A sentiment an-
alyzer with enhanced lexicon mapping and weight-
ing scheme for unbalanced data. In Proceedings of
SemEval-2014, pages 628–632.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings of
SemEval-2013, pages 321–327.
Brendan O’Connor, Ramnath Balasubramanyan, Bryan
Routledge, and Noah Smith. 2010. From tweets to
</reference>
<page confidence="0.987181">
629
</page>
<reference confidence="0.999426958333333">
polls: Linking test sentiment to public opinion time
series. In Proceedings of the International AAAI Con-
ference on Weblogs and Social Media, pages 122–129.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL-2013, pages 380–390.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and Edouard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learn-
ing Research, 12:2825–2830.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment analy-
sis in Twitter. In Proceedings of SemEval-2015.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings HLT ’05, pages
347–354.
</reference>
<page confidence="0.997627">
630
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262867">
<title confidence="0.997466">SWASH: A Naive Bayes Classifier for Tweet Sentiment Identification</title>
<author confidence="0.667756">Chloe</author>
<author confidence="0.667756">Richard</author>
<address confidence="0.704501">College, Swarthmore, PA</address>
<affiliation confidence="0.763813">University, Accra,</affiliation>
<email confidence="0.999826">chloeacheampong@gmail.com</email>
<abstract confidence="0.997100157894737">This paper describes a sentiment classification system designed for SemEval-2015, Task 10, Subtask B. The system employs a constrained, supervised text categorization approach. Firstly, since thorough preprocessing of tweet data was shown to be effective in previous SemEval sentiment classification tasks, various preprocessessing steps were introduced to enhance the quality of lexical information. Secondly, a Naive Bayes classifier is used to detect tweet sentiment. The classifier is trained only on the training data provided by the task organizers. The system makes use of external human-generated lists of positive and negative words at several steps throughout classification. The system produced an overall F-score of 59.26 on the official test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pablo Gamallo</author>
<author>Marcos Garcia</author>
</authors>
<title>Citius: A naive-bayes strategy for sentiment analysis on english tweets.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval-2014,</booktitle>
<pages>171--175</pages>
<contexts>
<context position="8554" citStr="Gamallo and Garcia, 2014" startWordPosition="1339" endWordPosition="1342">feature. In addition, a ‘positive’ or ‘negative’ feature (represented by ‘POSW’ or ‘NEGW’) is added for each positive or negative word a tweet contained, as identified by inclusion in the sentiment lexicon. 3.3 Classifiers 3.3.1 Naive Bayes Classifier We used a Naive Bayes classifier to classify the tweets. Naive Bayes relies on the assumption of conditional independence among the features, something that is clearly not true here. While Naive Bayes classifiers manage to perform well despite this assumption, a classifier not reliant on this assumption might outperform a Naive Bayes classifier (Gamallo and Garcia, 2014). The Naive Bayes classifier employed Laplace smoothing. More advanced smoothing techniques were attempted, but actually reduced both the accuracy and F1 score of the system. The additive smoothing constant was empirically chosen to be 0.4. The Naive Bayes classifier was trained solely on the training data from SemEval-2015. 3.3.2 Other Classifiers Attempted In addition to Naive Bayes, several other classifiers were tried, and an attempt was made to employ a combination of multiple classifiers to predict sentiment. These classifiers included a typical decision list (which defaults to most freq</context>
</contexts>
<marker>Gamallo, Garcia, 2014</marker>
<rawString>Pablo Gamallo and Marcos Garcia. 2014. Citius: A naive-bayes strategy for sentiment analysis on english tweets. In Proceedings of SemEval-2014, pages 171– 175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Colin Cherry</author>
<author>Saif Mohammad</author>
</authors>
<title>NRC-Canada-2014: Detecting aspects and sentiment in customer reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval-2014,</booktitle>
<pages>437--442</pages>
<contexts>
<context position="3195" citStr="Kiritchenko et al., 2014" startWordPosition="492" endWordPosition="495">ded by the task organizers. The training data we used includes 8142 tweets, each labeled as positive, negative or neutral. 2.2 Sentiment Lexicon Our system relies on an external lexicon of approximately 6800 tokens labeled as either positive or negative (Liu et al., 2005). The lexicon consists of words that humans have tagged as having either strongly negative or strongly positive sentiment. If a word in a tweet is preidentified as highly positive or negative, we add a special feature to the tweet’s features to indicate that the tweet included a highly positive word or a highly negative word (Kiritchenko et al., 2014). Although multiple lexicons exist, e.g. 626 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 626–630, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Preprocessing Step F1 score change jazzy −5.67 stopwords −.56 negation 1.74 username normalization 0.34 url normalization 0.40 overriding 1.74 lowercasing 2.21 tokenization 4.00 sentiment lexicon 5.81 Table 1: Changes in F1-score obtained by each preprocessing step (taken individually, not cumulatively) using 5-fold cross validation on the provided training set. (Wilson</context>
<context position="5862" citStr="Kiritchenko et al., 2014" startWordPosition="910" endWordPosition="913"> of tweets due to a word of one sentiment following a negation key and therefore being of the opposite sentiment. For instance, a sentence could state: “That movie was not the best thing I’ve ever seen.” Clearly, this sentence is negative, but without negation, the presence of the word ‘best,’ a typically positive word, might lead this tweet to be classified as positive, not negative. If however, a tag is added (in this case ‘NOT ’) to any words following a negation key, those words will be more likely to be classified appropriately, as ‘NOT best’ will more often be seen in negative contexts (Kiritchenko et al., 2014). 3.1.4 Other Preprocessing Considered Several other preprocessing steps were considered. In particular, a spell corrector, Jazzy1, was used as it had previously been shown to be effective (Miura et al., 2014). This step was taken to reduce dimensionality and provide better matches with the sentiment lexicon, e.g. converting ‘luve’ into ‘love’, so instead of seeing ‘love’ once in a positive context and ‘luve’ once in a positive context, we would see ‘love’ twice in a positive context, giving it more weight as a positive feature and finding a match in the sentiment lexicon. However, Jazzy actua</context>
</contexts>
<marker>Kiritchenko, Zhu, Cherry, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. NRC-Canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of SemEval-2014, pages 437–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the Web. In</title>
<date>2005</date>
<booktitle>Proceedings of WWW’05,</booktitle>
<pages>342--351</pages>
<contexts>
<context position="2842" citStr="Liu et al., 2005" startWordPosition="431" endWordPosition="434">mine the sentiment of tweets. This paper describes the resources used, the system details, including preprocessing steps taken, feature extraction and classifier implemented, and the test runs and end results. 2 Resources 2.1 Labeled Tweets This system is constrained, and the only training data used is the sentiment labeled training data provided by the task organizers. The training data we used includes 8142 tweets, each labeled as positive, negative or neutral. 2.2 Sentiment Lexicon Our system relies on an external lexicon of approximately 6800 tokens labeled as either positive or negative (Liu et al., 2005). The lexicon consists of words that humans have tagged as having either strongly negative or strongly positive sentiment. If a word in a tweet is preidentified as highly positive or negative, we add a special feature to the tweet’s features to indicate that the tweet included a highly positive word or a highly negative word (Kiritchenko et al., 2014). Although multiple lexicons exist, e.g. 626 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 626–630, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Preprocessing Step </context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the Web. In Proceedings of WWW’05, pages 342– 351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhide Miura</author>
<author>Shigeyuki Sakaki</author>
<author>Keigo Hattori</author>
<author>Tomoko Ohkuma</author>
</authors>
<title>TeamX: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval-2014,</booktitle>
<pages>628--632</pages>
<contexts>
<context position="6071" citStr="Miura et al., 2014" startWordPosition="942" endWordPosition="945">is sentence is negative, but without negation, the presence of the word ‘best,’ a typically positive word, might lead this tweet to be classified as positive, not negative. If however, a tag is added (in this case ‘NOT ’) to any words following a negation key, those words will be more likely to be classified appropriately, as ‘NOT best’ will more often be seen in negative contexts (Kiritchenko et al., 2014). 3.1.4 Other Preprocessing Considered Several other preprocessing steps were considered. In particular, a spell corrector, Jazzy1, was used as it had previously been shown to be effective (Miura et al., 2014). This step was taken to reduce dimensionality and provide better matches with the sentiment lexicon, e.g. converting ‘luve’ into ‘love’, so instead of seeing ‘love’ once in a positive context and ‘luve’ once in a positive context, we would see ‘love’ twice in a positive context, giving it more weight as a positive feature and finding a match in the sentiment lexicon. However, Jazzy actually reduced accuracy and F-score of our system. One potential explanation for this finding is that tweets may contain significant amounts of abbreviations, slang and misspellings that are too far removed from </context>
</contexts>
<marker>Miura, Sakaki, Hattori, Ohkuma, 2014</marker>
<rawString>Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and Tomoko Ohkuma. 2014. TeamX: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. In Proceedings of SemEval-2014, pages 628–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of SemEval-2013,</booktitle>
<pages>321--327</pages>
<contexts>
<context position="3837" citStr="Mohammad et al., 2013" startWordPosition="580" endWordPosition="583">lexicons exist, e.g. 626 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 626–630, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Preprocessing Step F1 score change jazzy −5.67 stopwords −.56 negation 1.74 username normalization 0.34 url normalization 0.40 overriding 1.74 lowercasing 2.21 tokenization 4.00 sentiment lexicon 5.81 Table 1: Changes in F1-score obtained by each preprocessing step (taken individually, not cumulatively) using 5-fold cross validation on the provided training set. (Wilson et al., 2005) and (Mohammad et al., 2013), we were unable to include them due to time constraints. 3 System Details The system consists of several preprocessing steps, feature extraction, a Naive Bayes classifier and a secondary classifier that makes use of tokens that are strongly correlating with either a positive or negative sentiment. Improvements that were attempted but were unsuccessful in improving the system are also described. 3.1 Preprocessing Steps 3.1.1 Tokenization All tweets are tokenized using Twokenizer, a Twitter-specific tokenizer (Owoputi et al., 2013). The tokenizer can detect and handle conditions unlikely to occ</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Proceedings of SemEval-2013, pages 321–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan Routledge</author>
<author>Noah Smith</author>
</authors>
<title>From tweets to polls: Linking test sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan Routledge, and Noah Smith. 2010. From tweets to polls: Linking test sentiment to public opinion time series. In Proceedings of the International AAAI Conference on Weblogs and Social Media, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-2013,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-2013, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gael Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<contexts>
<context position="9259" citStr="Pedregosa et al., 2011" startWordPosition="1446" endWordPosition="1449">echniques were attempted, but actually reduced both the accuracy and F1 score of the system. The additive smoothing constant was empirically chosen to be 0.4. The Naive Bayes classifier was trained solely on the training data from SemEval-2015. 3.3.2 Other Classifiers Attempted In addition to Naive Bayes, several other classifiers were tried, and an attempt was made to employ a combination of multiple classifiers to predict sentiment. These classifiers included a typical decision list (which defaults to most frequent sense classification), and a number of classifiers included in scikit-learn (Pedregosa et al., 2011): LinearSVC, GaussianNB, NearestCentroid, MultinomialNB, and BernoulliNB. Each classifier used the same preprocessing and feature selection employed by the Naive Bayes classifier. However, after implementing all of these classifiers and attempting to use a combination of their sense decisions to make a more accurate prediction, none of the classifiers, nor any combination of their decisions, outperformed the Naive Bayes classifier, and therefore none were used in our submission. 3.3.3 Post-Processing Several features were identified that, when present, were strongly indicative of a positive or</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval-2015.</booktitle>
<contexts>
<context position="2145" citStr="Rosenthal et al., 2015" startWordPosition="322" endWordPosition="325">pressions of opinion has attracted interest in sentiment analysis of tweets in both academia and industry. Sentiment analysis, also known as opinion mining, focuses on computational treatments of sentiments (emotions, attitudes, opinions) in natural language text. In this paper we describe our submission to Task 10, subtask B: Message Polarity Classification. The task is defined as: ‘Given a message, classify whether the message is of positive, negative, or neutral sentiment. For a message conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen’ (Rosenthal et al., 2015). This paper describes a system which utilizes a Naive Bayes classifier to determine the sentiment of tweets. This paper describes the resources used, the system details, including preprocessing steps taken, feature extraction and classifier implemented, and the test runs and end results. 2 Resources 2.1 Labeled Tweets This system is constrained, and the only training data used is the sentiment labeled training data provided by the task organizers. The training data we used includes 8142 tweets, each labeled as positive, negative or neutral. 2.2 Sentiment Lexicon Our system relies on an extern</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in Twitter. In Proceedings of SemEval-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings HLT ’05,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="3809" citStr="Wilson et al., 2005" startWordPosition="575" endWordPosition="578"> 2014). Although multiple lexicons exist, e.g. 626 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 626–630, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Preprocessing Step F1 score change jazzy −5.67 stopwords −.56 negation 1.74 username normalization 0.34 url normalization 0.40 overriding 1.74 lowercasing 2.21 tokenization 4.00 sentiment lexicon 5.81 Table 1: Changes in F1-score obtained by each preprocessing step (taken individually, not cumulatively) using 5-fold cross validation on the provided training set. (Wilson et al., 2005) and (Mohammad et al., 2013), we were unable to include them due to time constraints. 3 System Details The system consists of several preprocessing steps, feature extraction, a Naive Bayes classifier and a secondary classifier that makes use of tokens that are strongly correlating with either a positive or negative sentiment. Improvements that were attempted but were unsuccessful in improving the system are also described. 3.1 Preprocessing Steps 3.1.1 Tokenization All tweets are tokenized using Twokenizer, a Twitter-specific tokenizer (Owoputi et al., 2013). The tokenizer can detect and handl</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings HLT ’05, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>