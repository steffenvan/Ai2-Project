<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.997790333333333">
Detecting Dependencies between Semantic Verb
Subclasses and Subcategorization Frames in Text
Corpora
</title>
<author confidence="0.995132">
Victor Poznanski, Antonio Sanfilippo
</author>
<affiliation confidence="0.954116">
SHARP Laboratories of Europe Ltd.
Oxford Science Park, Oxford 0X4 4GA
</affiliation>
<email confidence="0.63717">
{vp, aps}ftrg.oxford.ac.uk
</email>
<sectionHeader confidence="0.909585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998929666666667">
We present a method for individuating dependencies between the semantic class of
predicates and their associated subcategorization frames, and describe an implemen-
tation which allows the acquisition of such dependencies from bracketed texts.
</bodyText>
<sectionHeader confidence="0.999344" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986763">
There is a widespread belief among linguists that a predicate&apos;s subcategorization frames
are largely determined by its lexical-semantic properties [23, 11, 121. Consider the do-
main of movement verbs. Following Talmy [23], these can be semantically classified with
reference to the meaning components: MOTION, MANNER, CAUSATION, THEME (MOVING
ENTITY), PATH AND REFERENCE LOCATIONS (GOAL, SOURCE). Lexicalization patterns
which arise from identifying clusters of such meaning components in verb senses can be
systematically related to distinct subcategorization frames.1 For example, the arguments
of a verb expressing directed caused motion (e.g. bring, put, give) are normally a causative
subject (agent), a theme direct object (moving entity) and a directional argument express-
ing path and reference location (goal), e.g.
</bodyText>
<listItem confidence="0.868619">
(1) Jackie will bring a bottle of retsina to the party
</listItem>
<sectionHeader confidence="0.90084" genericHeader="introduction">
CAUSER THEME PATH GOAL
</sectionHeader>
<bodyText confidence="0.987921454545455">
However, a motion verb which is not amenable to direct external causation [13], will
typically take a theme subject, with the possible addition of a directional argument, e.g.
(2) The baby crawled (across the room)
Co-occurrence restrictions between meaning components may also preempt subcategoriza-
tion options; for example, manner of motion verbs in Italian cannot integrate a completed
path component and therefore never subcategorize for a directional argument, e.g.
(3)*Carlo ha camminato a casa
Carlo walked home
&apos;Following Levin [12] and Sanfilippo [18], we maintain that valency reduction processes (e.g. the
causative-inchoative alternation) are semantically governed and thus do not weaken the correlation be-
tween verb semantics and subcategorization properties.
</bodyText>
<page confidence="0.99759">
82
</page>
<bodyText confidence="0.998865">
These generalizations are important for NLP since they frequently cover large sub-
classes of lexical items and can be used both to reduce redundancy and elucidate sig-
nificant aspects of lexical structure. Moreover, a precise characterization of the relation
between semantic subclasses and subcategorization properties of verbs can aid lexical dis-
ambiguation. For example, the verb accord can be used in either one of two senses: agree
or give, e.g.
</bodyText>
<listItem confidence="0.594114">
(4) a The two alibis do not accord
</listItem>
<bodyText confidence="0.9955595">
Your alibi does not accord with his
b They accorded him a warm welcome
Accord is intransitive in the agree senses shown in (4a), and ditransitive in the give sense
shown in (4b).
The manual encoding of subcategorization options for each choice of verb subclass in
the language is very costly to develop and maintain. This problem can be alleviated by
automatically extracting collocational information, e.g. grammar codes, from Machine
Readable Dictionaries (MRDs). However, most of these dictionaries are not intended for
such processing; their readership rarely require or desire such exhaustive and exacting
precision. More specifically, the information available is in most cases compiled manually
according to the lexicographer&apos;s intuitions rather than (semi-)automatically derived from
texts recording actual language use. As a source of lexical information for NLP, MRDs
are therefore liable to suffer from omissions, inconsistencies and occasional errors as well
as being unable to cope with evolving usage [1, 4, 2, 61. Ultimately, the maintenance
costs involved in redressing such inadequacies are likely to reduce the initial appeal of
generating subcategorization lists from MRDs.
In keeping with these observations, we implemented a suite of programs which provide
an integrated approach to lexical knowledge acquisition. The programs elicit dependen-
cies between semantic verb classes and their admissible subcategorization frames using
machine readable thesauri to assist in semantic tagging of texts.
</bodyText>
<sectionHeader confidence="0.977208" genericHeader="method">
2 Background
</sectionHeader>
<bodyText confidence="0.9885830625">
Currently available dictionaries do not provide a sufficiently reliable source of lexical
knowlege for NLP systems. This has led an increasing number of researchers to look
at text corpora as a source of information [8, 22, 9, 6, 3]. For example, Brent [6] de-
scribes a program which retrieves subcategorization frames from untagged text. Brent&apos;s
approach relies on detecting nominal, clausal and infinitive complements after identifi-
cation of proper nouns and pronouns using predictions based on GB&apos;s Case Filter [16)
— e.g. in English, a noun phrase occurs to the immediate left of a tensed verb, or the
immediate right of a main verb or preposition. Brent&apos;s results are impressive considering
that no text preprocessing (e.g. tagging or bracketing) is assumed. However, the number
of subcategorization options recognized is minima1,2 and it is hard to imagine how the
approach could be extended to cover the full range of subcategorization possibilities with-
out introducing some form of text preprocessing. Also, the phrasal patterns extracted are
too impoverished to infer selectional restrictions as they only contain proper nouns and
pronouns.
2Brent&apos;s program recognizes five subcategorization frames built out of three kinds of constituents:
noun phrase, clause, infinitive.
</bodyText>
<page confidence="0.998123">
83
</page>
<bodyText confidence="0.999653117647059">
Lexical acquisition of collocational information from preprocessed text is now becom-
ing more popular as tools for analyzing corpora are getting to be more reliable [9]. For
example, Basili et al. [3] present a method for acquiring sublanguage-specific selectional
restrictions from corpora which uses text processing techniques such as morphological
tagging and shallow syntactic analysis. Their approach relies on extracting word pairs
and triples which represent crucial environments for the acquisition of selectional re-
strictions (e.g. V_prep_N(go,to,Boston)). They then replace words with semantic tags
(V_prep_N(PHYSICAL_ACT-to-PLACE)) and compute co-occurrence preferences among
them. Semantic tags are crucial for making generalizations about the types of words which
can appear in a given context (e.g. as the argument of a verb or preposition). However,
Basili et al. rely on manual encoding in the assignment of semantic tags; such a practice
is bound to become more costly as the text under consideration grows in size and may
prove prohibitively expensive with very large corpora. Furthermore, the semantic tags
are allowed to vary from domain to domain (e.g. commercial and legal corpora) and are
not hierarchically structured. With no consequent notion of subsumption, it might be
impossible to identify &amp;quot;families&amp;quot; of tags relating to germane concepts across sublanguages
(e.g. PHYSICAL_ACT, ACT; BUILDING, REAL_ESTATES).
</bodyText>
<sectionHeader confidence="0.783990666666667" genericHeader="method">
3 CorPSE: a Body of Programs for Acquiring Se-
mantically Tagged Subcategorization Frames from
Bracketed Texts
</sectionHeader>
<bodyText confidence="0.99831025">
In developing CorPSE (Corpus-based Predicate Structure Extractor) we followed Basili
et al.&apos;s idea of extracting semantically tagged phrasal frames from preprocessed text,
but we used the Longman Lexicon of Contemporary English (LLOCE [151) to automate
semantic tagging. LLOCE entries are similar to those of learner&apos;s dictionaries, but are
arranged in a thesaurus-like fashion using semantic codes which provide a linguistically-
motivated classification of words. For example, [19] show that the semantic codes of
LLOCE are instrumental in identifying members of the six subclasses of psychological
predicates described in (5) [12, 11].
</bodyText>
<figure confidence="0.99442925">
Affect type
Experiencer Subject Stimulus Subject
experience
admire
fear
interest
fascinate
scare
Neutral
Positive
Negative
(5)
</figure>
<bodyText confidence="0.978962">
As shown in (6), each verb representing a subclass has a code which often provides a
uniform characterization of the subclass.
</bodyText>
<listItem confidence="0.81624225">
Code Group Header Entries
Fl Relating to feeling feel, sense, experience ...
F140 Admiring and honouring admire, respect, look up to ...
F121 Fear and Dread fear, fear for, be frightened ...
F25 Attracting and interesting attract, interest, concern ...
F26 Attracting and interesting very much fascinate, enthrall, enchant ...
F122 Frighten and panic frighten, scare, terrify...
(6)
</listItem>
<page confidence="0.971663">
84
</page>
<bodyText confidence="0.941389">
Moreover, LLOCE codes are conveniently arranged into a 3-tier hierarchy according
to specificity, e.g.
</bodyText>
<listItem confidence="0.772444666666667">
F Feelings, Emotions, Attitudes and Sensations
F20-F40 Liking and not Liking
F26 Attracting and Interesting very much
</listItem>
<bodyText confidence="0.993558538461539">
fascinate, enthrall, enchant, charm, captivate
The bottom layer of the hierarchy contains over 1500 domain-specific tags, the middle
layer has 129 tags and the top (most general) layer has 14. Domain-specific tags are
always linked to intermediate tags which are, in turn, linked to general tags. Thus we
can tag sublanguages using domain-specific semantic codes (as do Basili et a/.) without
generating unrelated sets of such codes.
We assigned semantic tags to Subcategorization Frame tokens (SF tokens) extracted
from the Penn Treebank [14, 20, 21] to produce Subcategorization Frame types (SF types).
Each SF type consists of a verb stem associated with one or more semantic tags, and a
list of its (non-subject) complements, if any. The head of noun phrase complements were
also semantically tagged. We used LLOCE collocational information — grammar codes
— to reduce or remove semantic ambiguity arising from multiple assignment of tags to
verb and noun stems. The structures below exemplify these three stages.
</bodyText>
<table confidence="0.597374666666667">
SF token: ((DENY VB)
(NP (ALIENS NNS))
(NP (*COMPOUND-NOUN* (STATE NN) (BENEFITS NNS))))
SF type: ((&amp;quot;deny&amp;quot; (&amp;quot;C193&amp;quot;-refuse &amp;quot;G127&amp;quot;-reject))
((*NP* (&amp;quot;C&amp;quot;-people_and_family))
(*NP* (&amp;quot;N&amp;quot;-general_and_abstract_terms))))
Disambiguated SF type: ((&amp;quot;deny&amp;quot; (&amp;quot;C193&amp;quot;))
((*NP* (&amp;quot;C&amp;quot;))
(*NP* (&amp;quot;N&amp;quot;))))
</table>
<subsectionHeader confidence="0.979269">
3.1 CorPSE&apos;s General Functionality
</subsectionHeader>
<bodyText confidence="0.9504335">
CorPSE is conceptually segmented into 2 parts: a predicate structure extractor, and a
semantic processor. The predicate structure extractor takes bracketed text as input,
and outputs SF tokens. The semantic processor converts SF tokens into SF types and
disambiguates them.
</bodyText>
<subsectionHeader confidence="0.994588">
3.1.1 Extracting SF Tokens
</subsectionHeader>
<bodyText confidence="0.999891">
The predicate structure extractor elicits SF tokens from a bracketed input corpus. These
tokens are formed from phrasal fragments which correspond to a subcategorization frame,
factoring out the most relevant information. In the case of verbs, such fragments corre-
spond to verb phrases where the following simplificatory changes have been applied:
</bodyText>
<listItem confidence="0.98467">
• NP complements have been reduced to the head noun (or head nouns in the case of
coordinated NP&apos;s or nominal compounds), e.g. ( (FACES VBZ) (NP (CHARGES NNS)))
</listItem>
<page confidence="0.919637">
85
</page>
<listItem confidence="0.998452333333333">
• PP complements have been reduced to the head preposition plus the head of the
complement noun phrase, e.g. ((RIDES VBZ) (PP IN ((VAN NN) )) )
• VP complements are reduced to a mention of the VFORM of the head verb, e.g.
((TRY VB) (VP TO))
• clausal complements are reduced to a mention of the complementizer which intro-
duces them, e.g. ((ARGUED VBD) (SBAR THAT))
</listItem>
<bodyText confidence="0.718677333333333">
An important step in the extraction of SF tokens is to distinguish passive and active
verb phrases. Passives are discriminated by locating a past participle following an auxiliary
be.
</bodyText>
<subsectionHeader confidence="0.982987">
3.1.2 Converting SF Tokens into SF Types
</subsectionHeader>
<bodyText confidence="0.998590583333333">
The semantic processor operates on the output of the predicate structure extractor. In-
flected words in input SF tokens are first passed through a general purpose morphological
analyser [17] and reduced to bare stems suitable for automated dictionary and lexicon
searches. The next phase is to supplement SF tokens with semantic tags from LLOCE us-
ing the facilities of the ACQUILEX LDB [5, 7] and DCK [17]; LLOCE tags are associated
with verb stems and simply replace noun stems.
The resulting SF structures are finally converted into SF types according to the rep-
resentation system whose syntax is sketched in (7) where: stem is the verb stem, parts a
possibly empty sequence of particles associated with the verb stem, {A N } is the set of
ILOCE semantic codes, pform the head of a prepositional phrase, compform the possibly
empty complementizer of a clausal complement, and cat any category not covered by np-,
pp-, sbar- and vp- frames.
</bodyText>
<equation confidence="0.947575777777778">
(7) SF-type ::= ( stem parts sem comps)
sem ::= ( {A N }* )
comps ::= comp*
comp ::= ( np-frame I pp-frame I sbar-frame I vp-frame I cat-frame) )
np-frame ::= ( *NP* sem )
pp-frame ::= ( *PP* pform comp )
sbar-frame ::= ( *SDAR* compform )
vp-frame ::= ( *VP* vform )
cat-frame ::= ( *CAT* cat )
</equation>
<subsectionHeader confidence="0.936077">
3.1.3 Disambiguating SF Types
</subsectionHeader>
<bodyText confidence="0.9989645">
The disambiguation module of the semantic processor coalesces SF types, and reduces
semantic tags when verb stems have several codes.
Coalescing merges SF types with isomorphic structure and identical verb stem, com-
bining the semantic codes of NP-frames, e.g.
</bodyText>
<page confidence="0.978278">
86
</page>
<table confidence="0.998975666666667">
((&amp;quot;accord&amp;quot; (&amp;quot;D101&amp;quot; &amp;quot;N226&amp;quot;) ) ( (&amp;quot;accord&amp;quot; (&amp;quot;D101.&amp;quot; &amp;quot;N226&amp;quot;))
((*PP* TO (*NP* (&amp;quot;C&amp;quot;))))) ((*PP* TO (*NP* (&amp;quot;C&amp;quot; &amp;quot;G&amp;quot;)))))
((&amp;quot;accord&amp;quot; (&amp;quot;D101&amp;quot; &amp;quot;N226&amp;quot;) )
((*PP* TO (*NP* (&amp;quot;G&amp;quot;)))))
((&amp;quot;accord&amp;quot; (&amp;quot;D101&amp;quot; &amp;quot;N226&amp;quot;) )
((*PP* TO (*NP* (HC.. &amp;quot;G&amp;quot;)))))
</table>
<bodyText confidence="0.994787666666667">
This process can be performed in linear time when the input is lexicographically sorted.
We employ two tag reduction methods. The first eliminates equivalent tags, the second
applies syntactico-semantic restrictions using LLOCE grammar codes.
More than one LLOCE code can apply to a particular entry. Under these circum-
stances, it may be possible to ignore one or more of them. For example, the verb function
is assigned two distinct codes in LLOCE: 128 functioning and serving, and N123 func-
tioning and performing. Although I- and N-codes may in principle differ considerably, in
this case they are very similar; indeed, the entries for the two codes are identical. This
identity can be automatically inferred from the descriptor associated with semantic codes
in the LLOCE index. For example, for a verb such as accord where each semantic code is
related to a distinct entry, the index gives two separate descriptors:
accord ...
give v D101
agree v N226
By contrast, different codes related to the same entry are associated with the same de-
scriptor, as shown for the entry function below.
function ...
work v 128, N123
We exploit the correlation between descriptors and semantic codes in the 1,1,0(110, index,
reducing multiple codes indexed by the same descriptor to just one. More precisely, the
reduction involves substitution of all codes having equal descriptors with a new code which
represents the logical conjunction of the substituted codes. This is shown in (8) where
&amp;quot;128-1-N123&amp;quot; is defined as the intersection of &amp;quot;128&amp;quot; and &amp;quot;N123&amp;quot; in the LLOCE hierarchy
of semantics codes as indicated in (9).
</bodyText>
<figure confidence="0.928505166666667">
((&amp;quot;function&amp;quot; (&amp;quot;128&amp;quot; &amp;quot;N123&amp;quot;)) ((&amp;quot;function&amp;quot; (&amp;quot;128+N123&amp;quot;))
((*PP* LIKE (*NP* (&amp;quot;C&amp;quot;)))))
(9)
(8) ((*PP* LIKE (*NP* (&amp;quot;C&amp;quot;)))))
128 N123
128-I-N123
</figure>
<page confidence="0.993088">
87
</page>
<bodyText confidence="0.8779378">
The second means for disambiguating SF types consists of filtering out the codes of
verb stems which are incompatible with the type of subcategorization frame in which they
occur. This is done by using collocational information provided in LLOCE. For example,
the verb deny is assigned two distinct semantic codes which cannot be reduced to one as
they have different descriptors:
deny ...
refuse v C193
reject v G127
The difference in semantic code entails distinct subcategorization options: deny can have
a ditransitive subcategorization frame only in the refuse sense, e.g.
</bodyText>
<listItem confidence="0.561699666666667">
d (refuse)
(10) Republican senator David Lock&apos;s bill would permanently {*enydeny (reject) illegal
aliens all State benefits
</listItem>
<bodyText confidence="0.85845475">
The codependency between semantic verb class and subcategorization can often be in-
ferred by the grammar code of LLOCE entries. For example, only the entry for the
refuse sense of deny in LLOCE includes the grammar code D1 which signals a ditransitive
subcategorization frame:
</bodyText>
<listItem confidence="0.953445333333333">
(11) C193 verbs: not letting or allowing
deny [D1;T1]
G127 verbs: rejecting...
</listItem>
<bodyText confidence="0.98658325">
deny 1 [T1,4,5;V3] ...2 [Ti] ...
Semantic codes which are incompatible with the SF types in which they occur, such as
G127 in (12), can thus be filtered out by enforcing constraints between SF type comple-
ment structures and LLOCE grammar codes.
</bodyText>
<equation confidence="0.992501666666667">
(12) ((&amp;quot;deny&amp;quot;(&amp;quot;C193&amp;quot; &amp;quot;G127&amp;quot;))
((*NP* (&amp;quot;C&amp;quot; ) )
(*NP* (&amp;quot;N&amp;quot; ) ) ) )
</equation>
<bodyText confidence="0.99547425">
To automate this process, we first form a set GC of compatible grammar codes for each
choice of complement structure in SF types. For example, the set of compatible grammar
codes GC for any SF type with two noun phrase complements is restricted to the singleton
set {DO, e.g.
</bodyText>
<listItem confidence="0.657813">
(13) ((siem sem) GC = {D1}
( (*NP* sem )
(*NP* sem )))
</listItem>
<bodyText confidence="0.999919">
A set of 2-tuples of the form (verb-stem-semantic-code, grammar-codes) is formed by
noting the LLOCE grammar codes for each semantic code that could apply to the verb
stem. If the grammar codes of any 2-tuple have no intersection with the grammatical
restrictions GC, we conclude that the associated verb-stem-semantic code is not possible.&apos;
For example, C193 in the SF type for deny in (13) is paired up with the grammar codes
(D1;T1} and G127 with {T1,4,5;V3} according to the LLOCE entries for deny shown in
</bodyText>
<footnote confidence="0.562391">
3 This procedure is only effective if the corpus subcategorization information is equally or more precise
than the dictionary information. For our corpus, it proved to be the case.
</footnote>
<page confidence="0.99867">
88
</page>
<bodyText confidence="0.9945135">
(12). The constraints in (14) would thus license automatic removal of semantic code G127
from the SF type for ditransitive deny as shown in (15).
</bodyText>
<equation confidence="0.959076571428572">
(14) ((&amp;quot;deny&amp;quot; ((C193, {D1, T1}) GC = {D1}
(G127, {T1, T4, T5, V3})) )
( (*NP* (&amp;quot;C&amp;quot;))
(*NP* (&amp;quot;N&amp;quot;))))
(15) ((&amp;quot;deny&amp;quot; (&amp;quot;C193&amp;quot;-refuse &amp;quot;G127&amp;quot;-reject)) ((&amp;quot;deny&amp;quot; (&amp;quot;C193&amp;quot;))
((*NP* (&amp;quot;C&amp;quot;)) ((*NP* (&amp;quot;C&amp;quot;))
(*NP* (&amp;quot;N&amp;quot;)))) (*NP* (&amp;quot;R&amp;quot;))))
</equation>
<bodyText confidence="0.983227857142857">
It may appear that there is a certain circularity in our work. We use grammar codes
to help disambiguate SF types, but it might be argued that the corpus could not have
been bracketed without some prior grammatical information: subcategorisation frames.
This picture is inaccurate because our SF types provide collocational information which is
not in LLOCE. For example, the SF type shown in (16a) captures the use of link in (16b);
this subcategorization cannot be inferred from the LLOCE entry where no PP headed by
to is mentioned.
</bodyText>
<listItem confidence="0.555959666666667">
(16) a ((&amp;quot;link&amp;quot; NIL (&amp;quot;N&amp;quot;))
((*NP* (&amp;quot;C&amp;quot;))
(*PP* TO (*NP* (&amp;quot;B&amp;quot; &amp;quot;N&amp;quot;)))))
</listItem>
<bodyText confidence="0.833371857142857">
b The arrest warrant issued in Florida links the attorney to a government probe
of the Medhyin drug cartel ...
Indeed, another possible use for our system would be to provide feedback to an on-line
dictionary. We also provide a partial indication of selectional restrictions, i.e. the se-
mantic tags of NP complements. Furthermore, text can be bracketed using techniques
such as stochastic and semi-automatic parsing which need not rely on exhaustive lists of
subcategorisations.
</bodyText>
<sectionHeader confidence="0.978844" genericHeader="method">
4 Using CorPSE: Emerging Trends and Current Lim-
itations
</sectionHeader>
<bodyText confidence="0.998221">
In testing CorPSE, our main objectives were:
</bodyText>
<listItem confidence="0.98469075">
• to assess the functionality of text pre-processing techniques involving automated
semantic tagging and lexical disambiguation, and
• to show that such techniques may yield profitable results in capturing regularities
in the syntax-semantics interface
</listItem>
<bodyText confidence="0.9999005">
In order to do this, we ran CorPSE on a section of the Penn Treebank comprising 576
bracketed sentences from radio transcripts. LFrom these sentences, CorPSE extracted 1335
SF tokens comprising 1245 active VPs and 90 passives. The SF tokens were converted
into 817 SF types. The coalescence process reduced the 817 SF types to 583, which are
representative of 346 distinct verb stems. The verb stern of 308 of these 583 SF types was
semantically ambiguous as it was associated with more than one semantic tag. In some
</bodyText>
<page confidence="0.999462">
89
</page>
<bodyText confidence="0.93521175">
cases, this ambiguity was appropriate because the semantic codes assigned to the stem
were all compatible with the complement structure of their SF type. For example, the
verb call can occur in either one of two senses, summon and phone, with no change in
subcategorization structure:
</bodyText>
<listItem confidence="0.918511">
(17) a Supper is ready, call the kids
b Call me when you land in Paris
</listItem>
<bodyText confidence="0.679966">
In this case, CorPSE correctly maintains the ambiguity as shown in (18).
</bodyText>
<listItem confidence="0.292034">
(18) ( (&amp;quot;call&amp;quot; (&amp;quot;G&amp;quot;-summon &amp;quot;V-phone))
((*NP* (&amp;quot;C&amp;quot; &amp;quot;J&amp;quot; &amp;quot;N&amp;quot;))))
</listItem>
<bodyText confidence="0.999921027027027">
In other cases, the ambiguity was in need of resolution as some of the verb-stem&apos;s semantic
codes referred to the same LLOCE entry or were incompatible with the complement
structure in the SF type (see §3.1.3). Disambiguation using semantic tag equivalence
reduced the ambiguity of 206 types, totally disambiguating 31 stems. Applying collocation
restrictions further reduced 38 stems, totally disambiguating 24 of them.
Taking into account that the amount of data processed was too small to use statistical
techniques for disambiguation, the results achieved are very promising: we managed to
reduce ambiguity in over half the SF types and totally disambiguated 16 percent, thus
providing a unique correspondence between semantic verb class and subcategorization
frame in 346 cases. Of the remaining 179 SF frames, 106 had verb stems with two
semantic codes, 72 had verb stems with 3-5 semantic codes and the verb stem of one SF
type had 6. Needless to say, the number of ambiguous SF types is bound to increase
as more texts are processed. However, as we accumulate more data, we will be able to
apply statistical techniques to reduce lexical ambiguity, e.g. by computing co-occurrence
restrictions between the semantic codes of the verb stem and complement heads in SF
types.
The table below summarizes some of the results concerning the correlation of semantic
codes and subcategorization options obtained by running CorPse on the Penn Treebank
fragment. The first column lists the LLOCE semantic codes which are explained in (20).
The second column indicates the number of unique subcategorization occurrences for
each code. A major difficulty in computing this relation was the presence of certain con-
stituents as arguments that are usually thought of as adjuncts. For example, purpose
clauses and time adverbials such as yesterday, all day, in March, on Friday had often
been bracketed as arguments (i.e. sisters to a V node). Our solution was to filter out
inadequately parsed arguments semi-automatically. Certain constituents were automati-
cally filtered from SF types as their status as adjuncts was manifest, e.g. complements
introduced by prepositions and complementizers such as without, as, since and because.
Other suspect constituents, such as infinitive VPs which could represent purpose clauses,
were processed by direct query. A second problem was the residual ambiguities in SF
types mentioned above. These biased the significance of occurrences since one or more
codes in an ambiguous SF type could be inconsistent with the subcategorization of the
SF type. A measure of the &amp;quot;noise&amp;quot; factor introduced by ambiguous SF types is given in
the third column of (19), where ambiguity rate is computed by dividing the number of
codes associated with the same complement structure by the number of occurrences of
that code with any complement structure. This ambiguity measure allows the significance
of the figures in the second column to be assessed. For example, since the occurrences of
&amp;quot;E&amp;quot; instances were invariably ambiguous, it is difficult to draw reliable conclusions about
</bodyText>
<page confidence="0.995151">
90
</page>
<bodyText confidence="0.993801625">
them. Indeed, on referring most of these SF types (e.g. beat, bolt and have) back to their
source texts, the &amp;quot;Food St Drink&amp;quot; connotation proved incorrect. The figures in column 1
were normalised as percentages of the total number of occurrences in order to provide a
measure of the statistical significance of the results in the remaining columns. We thus
conclude that the results for B, E, H, and I are unlikely to be significant as they occur
with low relative frequency and are highly ambiguous. The final three columns quantify
the relative frequency of occurrence for VP, SBAR and PP complements in SF types for
each semantic code.
</bodyText>
<table confidence="0.8186006">
Code # Occ. % Ambig Rel. Freq. % VP &lt;4,C C C C C C c&gt; % PP
A 4 0 1 0 0
B 9 44 1 0 3
C 72 67 9 15 39
D 57 65 7 16 44
E 23 83 3 22 57
F 42 40 5 10 21
G 132 33 17 7 28
H 11 82 1 0 27
I 27 74 3 4 63
J 68 57 9 12 35
K 29 69 4 0 48
L 33 36 4 21 27
M 130 50 16 2 52
N 161 44 20 14 35
Code Explanation
A Life Si Living Things
The Body, its Functions St Welfare
People St the Family
Building, Houses, the Home, Clothes
Food, Drink &amp; Farming
Feelings, Emotions, Attitudes Si Sensations
Thought Si Communication, Language St Grammar
Substances, Materials, Objects St Equipment
Arts Si Crafts, Science St Technology, Industry St Education
Numbers, Measurement, Money St Commerce
Entertainment, Sports St Games
Space and Time
Movement, Location, Travel St Transport
General St Abstract Terms
</table>
<bodyText confidence="0.9988606">
Although the results are not clear-cut, there are some emerging trends worth considering.
For example, the low frequency of VP and SBAR complements with code &amp;quot;M&amp;quot; reflects
the relatively rare incidence of clausal arguments in the semantics of motion and location
verbs. By contrast, the relatively high frequency of PP complements with this code can be
related to the semantic propensity of motion and location verbs to take spatial arguments.
</bodyText>
<page confidence="0.996032">
91
</page>
<bodyText confidence="0.999982705882353">
The &amp;quot;A&amp;quot; verbs (eg. create, live and murder) appear to be strongly biased towards taking
a direct object complement only. This might be due to the fact that these verbs involve
creating, destroying or manipulating life rather than events. Finally, the overwhelmingly
high frequency of SBAR complements with &amp;quot;G&amp;quot; verbs is related to the fact that thought
and communication verbs typically involve individuals and states of affairs.
We also found interesting results concerning the distribution of subcategorization op-
tions among specializations of the same general code. For example, 23 out of 130 occur-
rences of &amp;quot;M&amp;quot; verbs exhibited an &amp;quot;NP PP&amp;quot; complement structure; 17 of these were found
in SF types with codes &amp;quot;M50-M65&amp;quot; which largely characterize verbs of caused directed
motion: Putting and Taking, Pulling Pushing. This trend confirms some of the obser-
vations discussed in the introduction. It is now premature to report results of this kind
more fully since the corpus data used was too small and genre-specific to make more re-
liable and detailed inferences about the relation between subcategorization and semantic
verb subclass. We hope that further work with larger corpora will uncover new patterns
and corroborate current correlations which at present can only be regarded as providing
suggestive evidence. Other than using substantially larger texts, improvements could also
be obtained by enriching SF types, e.g. by adding information about subject constituents.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999945571428571">
We have provided the building blocks for a system that combines the advantages of free-
text processing of corpora with the more organised information found in MRDs, such as
semantic tags and collocational information. We have shown how such a system can be
used to acquire lexical knowledge in the form of semantically tagged subcategorization
frames. These results can assist the automatic construction of lexicons for NLP, semantic
tagging for data retrieval from textual databases as well as to help maintain, refine and
augment MRDs.
</bodyText>
<sectionHeader confidence="0.99488" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999813125">
Some of the work discussed in this paper was carried out at the Computer Laboratory in
Cambridge within the context of the ACQUILEX project. The Penn-Treebank-data used
were provided in CD-ROM format by the University of Pennsylvania through the ACL
Data Collection Initiative (ACL/DCI CD-ROM I, September 1991). We are indebted
to Ian Johnson for helpful comments and encouragement, and to John Beaven and Pete
Whitelock for providing feedback on previous versions of this paper. Many thanks also
to Ann Copestake and Victor Lesk for their invaluable contribution towards mounting
LLOCE on the LDB.
</bodyText>
<sectionHeader confidence="0.999214" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985276333333333">
[1] Atkins, B., Kegl, J. 8.6 Levin, B. (1986) Explicit and Implicit Information in Dictionar-
ies. In Advances in Lexicology, Proceedings of the Second Annual Conference of the
Centre for the New OED, University of Waterloo, Waterloo, Ontario.
</reference>
<page confidence="0.933475">
92
</page>
<reference confidence="0.999969810810811">
[2] Atkins, B. &amp; Levin, B. (1991) Admitting Impediments. In Zernik, U. (ed.) Lexical
Acquisition: Using On-Line Resources to Build a Lexicon., Lawrence Erlbaum Asso-
ciates, Hillsdale, New Jersey.
[3] Basili, R. and Pazienza, M. T. and Velardi, P. (1992). Computational Lexicography:
the Neat Examples and the Odd Exemplars. In Proc. 3rd Conference on Applied NLP,
Trento, Italy.
[4] Boguraev, B. &amp; Briscoe, T. (1989) Utilising the LDOCE Grammar Codes. In Boguraev,
B. &amp; Briscoe, T. (eds.) Computational Lexicography for Natural Language Processing.
Longman, London.
[5] Boguraev, B., Briscoe, T., Carroll, J. and Copestake, A. (1990) Database Models for
Computational Lexicography. In Proceedings of EURALEX IV, Malaga, Spain.
[6] Brent, M R. (1991) Automatic Semantic Classification of Verbs from their Syntactic
Contexts: An Implemented Classifier for Stativity. In Proc 29th ACL, University of
California, Berkeley, California.
[7] Carroll, J. (1992). The ACQUILEX Lexical Database System: System Description
and User Manual. In The (Other) Cambridge ACQUILEX Papers, TR 253, University
of Cambridge, Cambridge, UK.
[8] Church, Kenneth and Hanks, Patrick (1989) Word Association Norms, Mutual Infor-
mation and Lexicography. Proc 23rd ACL. pp. 76 — 83.
[9] Church, K and Gale, W. and flanks, P. and Hindle, D (1991) Using Statistics in
Lexical Analysis. In Lexical Acquisition, Zernik, Uri, Ed. Erlbaum, Hillsdale, NJ.
[10] Hindle, D. (1990). Noun Classification from Predicate Argument Structures. In Proc
28th ACL. pp. 268 — 275.
[11] Jackendoff, R. (1990) Semantic Structures. MIT Press, Cambridge, Mass.
[12] Levin, B. (1989) Towards a Lexical Organization of English Verbs. Ms., Dept. of
Linguistics, Northwestern University
[13] Levin, B. and Rappaport, M. (1991) The Lexical Semantics of Verbs in Motion: The
Perspective from Unaccusativity. To appear in Roca, I. (ed.) Thematic Structure: Its
Role in Grammar, Foris, Dordrecht.
[14] Liberman, M. and Marcus, M (1992) Very Large Text Corpora: What You Can Do
with Them, and How to Do It. Tutorial notes, 30th ACL, University of Delaware,
Newark, Delaware.
[15] McArthur, T. (1981) Longman Lexicon of Contemporary English. Longman, London.
[16] Rouvret, A. and Vergnaud, J Ft. (1980) Specifying Reference to the Subject. In Lin-
guistic Enquiry, 11(1).
[17] Sanfilippo, A (1992) A Morphological Analyser for English and Italian. In The (Other)
Cambridge ACQUILEX Papers, TR 253, University of Cambridge, Cambridge, UK.
</reference>
<page confidence="0.982817">
93
</page>
<reference confidence="0.9998938">
[18] Sanfilippo, A (1993) Verbal Diathesis: Knowledge Acquisition, Lexicon Construction
and Dictionary Compilation. TR SLE/IT/93-11, Sharp Laboratories of Europe, Ox-
ford, UK.
[19] Sanfilippo, A and Poznaiiski, V. (1992) The Acquisition of Lexical Knowledge from
Combined Machine-Readable Dictionary Sources. In Proceedings of the 3rd Conference
on Applied Natural Language Processing, Trento.
[20] Santorini, B. (1991) Bracketing Guidelines for the Penn Treebank Project. Ms. Uni-
versity of Pennsylvania.
[21] Santorini, B. (1991) Part-of-Spech Tagging Guidelines for the Penn Treebank Project.
Ms. University of Pennsylvania.
[22] Smajda, F A. and McKeown, K. R. (1990) Automatically Extracting and Represent-
ing Collocations for Language Generation. In Proc 28th ACL. pp. 252 — 259.
[23] Talmy, L. Lexicalization Patterns: Semantic Structure in Lexical Form. In Shopen,
T. (ed) Language Typology and Syntactic Description 3. Grammatical Categories and
the Lexicon, CUP, 1985.
</reference>
<page confidence="0.999552">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847551">
<title confidence="0.993196">Detecting Dependencies between Semantic Subclasses and Subcategorization Frames in Text Corpora</title>
<author confidence="0.999139">Victor Poznanski</author>
<author confidence="0.999139">Antonio Sanfilippo</author>
<affiliation confidence="0.947877">SHARP Laboratories of Europe Oxford Science Park, Oxford 0X4</affiliation>
<email confidence="0.994264">vpftrg.oxford.ac.uk</email>
<email confidence="0.994264">apsftrg.oxford.ac.uk</email>
<abstract confidence="0.99019425">We present a method for individuating dependencies between the semantic class of predicates and their associated subcategorization frames, and describe an implementation which allows the acquisition of such dependencies from bracketed texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Atkins</author>
<author>J Kegl</author>
</authors>
<title>Explicit and Implicit Information in Dictionaries.</title>
<date>1986</date>
<booktitle>In Advances in Lexicology, Proceedings of the Second Annual Conference of the Centre for the</booktitle>
<institution>New OED, University of Waterloo,</institution>
<location>Waterloo, Ontario.</location>
<marker>[1]</marker>
<rawString>Atkins, B., Kegl, J. 8.6 Levin, B. (1986) Explicit and Implicit Information in Dictionaries. In Advances in Lexicology, Proceedings of the Second Annual Conference of the Centre for the New OED, University of Waterloo, Waterloo, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Atkins</author>
<author>B Levin</author>
</authors>
<title>Admitting Impediments.</title>
<date>1991</date>
<editor>In Zernik, U. (ed.)</editor>
<location>Hillsdale, New Jersey.</location>
<marker>[2]</marker>
<rawString>Atkins, B. &amp; Levin, B. (1991) Admitting Impediments. In Zernik, U. (ed.) Lexical Acquisition: Using On-Line Resources to Build a Lexicon., Lawrence Erlbaum Associates, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>Computational Lexicography: the Neat Examples and the Odd Exemplars.</title>
<date>1992</date>
<booktitle>In Proc. 3rd Conference on Applied NLP,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="4423" citStr="[8, 22, 9, 6, 3]" startWordPosition="643" endWordPosition="647">f generating subcategorization lists from MRDs. In keeping with these observations, we implemented a suite of programs which provide an integrated approach to lexical knowledge acquisition. The programs elicit dependencies between semantic verb classes and their admissible subcategorization frames using machine readable thesauri to assist in semantic tagging of texts. 2 Background Currently available dictionaries do not provide a sufficiently reliable source of lexical knowlege for NLP systems. This has led an increasing number of researchers to look at text corpora as a source of information [8, 22, 9, 6, 3]. For example, Brent [6] describes a program which retrieves subcategorization frames from untagged text. Brent&apos;s approach relies on detecting nominal, clausal and infinitive complements after identification of proper nouns and pronouns using predictions based on GB&apos;s Case Filter [16) — e.g. in English, a noun phrase occurs to the immediate left of a tensed verb, or the immediate right of a main verb or preposition. Brent&apos;s results are impressive considering that no text preprocessing (e.g. tagging or bracketing) is assumed. However, the number of subcategorization options recognized is minima</context>
<context position="5675" citStr="[3]" startWordPosition="836" endWordPosition="836">uld be extended to cover the full range of subcategorization possibilities without introducing some form of text preprocessing. Also, the phrasal patterns extracted are too impoverished to infer selectional restrictions as they only contain proper nouns and pronouns. 2Brent&apos;s program recognizes five subcategorization frames built out of three kinds of constituents: noun phrase, clause, infinitive. 83 Lexical acquisition of collocational information from preprocessed text is now becoming more popular as tools for analyzing corpora are getting to be more reliable [9]. For example, Basili et al. [3] present a method for acquiring sublanguage-specific selectional restrictions from corpora which uses text processing techniques such as morphological tagging and shallow syntactic analysis. Their approach relies on extracting word pairs and triples which represent crucial environments for the acquisition of selectional restrictions (e.g. V_prep_N(go,to,Boston)). They then replace words with semantic tags (V_prep_N(PHYSICAL_ACT-to-PLACE)) and compute co-occurrence preferences among them. Semantic tags are crucial for making generalizations about the types of words which can appear in a given c</context>
</contexts>
<marker>[3]</marker>
<rawString>Basili, R. and Pazienza, M. T. and Velardi, P. (1992). Computational Lexicography: the Neat Examples and the Odd Exemplars. In Proc. 3rd Conference on Applied NLP, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>T Briscoe</author>
</authors>
<title>Utilising the LDOCE Grammar Codes.</title>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing.</booktitle>
<editor>In Boguraev, B. &amp; Briscoe, T. (eds.)</editor>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="15844" citStr="[T1,4,5;V3]" startWordPosition="2412" endWordPosition="2412">ct subcategorization options: deny can have a ditransitive subcategorization frame only in the refuse sense, e.g. d (refuse) (10) Republican senator David Lock&apos;s bill would permanently {*enydeny (reject) illegal aliens all State benefits The codependency between semantic verb class and subcategorization can often be inferred by the grammar code of LLOCE entries. For example, only the entry for the refuse sense of deny in LLOCE includes the grammar code D1 which signals a ditransitive subcategorization frame: (11) C193 verbs: not letting or allowing deny [D1;T1] G127 verbs: rejecting... deny 1 [T1,4,5;V3] ...2 [Ti] ... Semantic codes which are incompatible with the SF types in which they occur, such as G127 in (12), can thus be filtered out by enforcing constraints between SF type complement structures and LLOCE grammar codes. (12) ((&amp;quot;deny&amp;quot;(&amp;quot;C193&amp;quot; &amp;quot;G127&amp;quot;)) ((*NP* (&amp;quot;C&amp;quot; ) ) (*NP* (&amp;quot;N&amp;quot; ) ) ) ) To automate this process, we first form a set GC of compatible grammar codes for each choice of complement structure in SF types. For example, the set of compatible grammar codes GC for any SF type with two noun phrase complements is restricted to the singleton set {DO, e.g. (13) ((siem sem) GC = {D1} ( (*N</context>
</contexts>
<marker>[4]</marker>
<rawString>Boguraev, B. &amp; Briscoe, T. (1989) Utilising the LDOCE Grammar Codes. In Boguraev, B. &amp; Briscoe, T. (eds.) Computational Lexicography for Natural Language Processing. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>T Briscoe</author>
<author>J Carroll</author>
<author>A Copestake</author>
</authors>
<title>Database Models for Computational Lexicography.</title>
<date>1990</date>
<booktitle>In Proceedings of EURALEX IV, Malaga,</booktitle>
<contexts>
<context position="11650" citStr="[5, 7]" startWordPosition="1730" endWordPosition="1731">nt step in the extraction of SF tokens is to distinguish passive and active verb phrases. Passives are discriminated by locating a past participle following an auxiliary be. 3.1.2 Converting SF Tokens into SF Types The semantic processor operates on the output of the predicate structure extractor. Inflected words in input SF tokens are first passed through a general purpose morphological analyser [17] and reduced to bare stems suitable for automated dictionary and lexicon searches. The next phase is to supplement SF tokens with semantic tags from LLOCE using the facilities of the ACQUILEX LDB [5, 7] and DCK [17]; LLOCE tags are associated with verb stems and simply replace noun stems. The resulting SF structures are finally converted into SF types according to the representation system whose syntax is sketched in (7) where: stem is the verb stem, parts a possibly empty sequence of particles associated with the verb stem, {A N } is the set of ILOCE semantic codes, pform the head of a prepositional phrase, compform the possibly empty complementizer of a clausal complement, and cat any category not covered by np-, pp-, sbar- and vp- frames. (7) SF-type ::= ( stem parts sem comps) sem ::= ( </context>
<context position="15844" citStr="[T1,4,5;V3]" startWordPosition="2412" endWordPosition="2412">ct subcategorization options: deny can have a ditransitive subcategorization frame only in the refuse sense, e.g. d (refuse) (10) Republican senator David Lock&apos;s bill would permanently {*enydeny (reject) illegal aliens all State benefits The codependency between semantic verb class and subcategorization can often be inferred by the grammar code of LLOCE entries. For example, only the entry for the refuse sense of deny in LLOCE includes the grammar code D1 which signals a ditransitive subcategorization frame: (11) C193 verbs: not letting or allowing deny [D1;T1] G127 verbs: rejecting... deny 1 [T1,4,5;V3] ...2 [Ti] ... Semantic codes which are incompatible with the SF types in which they occur, such as G127 in (12), can thus be filtered out by enforcing constraints between SF type complement structures and LLOCE grammar codes. (12) ((&amp;quot;deny&amp;quot;(&amp;quot;C193&amp;quot; &amp;quot;G127&amp;quot;)) ((*NP* (&amp;quot;C&amp;quot; ) ) (*NP* (&amp;quot;N&amp;quot; ) ) ) ) To automate this process, we first form a set GC of compatible grammar codes for each choice of complement structure in SF types. For example, the set of compatible grammar codes GC for any SF type with two noun phrase complements is restricted to the singleton set {DO, e.g. (13) ((siem sem) GC = {D1} ( (*N</context>
</contexts>
<marker>[5]</marker>
<rawString>Boguraev, B., Briscoe, T., Carroll, J. and Copestake, A. (1990) Database Models for Computational Lexicography. In Proceedings of EURALEX IV, Malaga, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>Automatic Semantic Classification of Verbs from their Syntactic Contexts: An Implemented Classifier for Stativity.</title>
<date>1991</date>
<booktitle>In Proc 29th ACL,</booktitle>
<institution>University of California, Berkeley,</institution>
<location>California.</location>
<contexts>
<context position="4423" citStr="[8, 22, 9, 6, 3]" startWordPosition="643" endWordPosition="647">f generating subcategorization lists from MRDs. In keeping with these observations, we implemented a suite of programs which provide an integrated approach to lexical knowledge acquisition. The programs elicit dependencies between semantic verb classes and their admissible subcategorization frames using machine readable thesauri to assist in semantic tagging of texts. 2 Background Currently available dictionaries do not provide a sufficiently reliable source of lexical knowlege for NLP systems. This has led an increasing number of researchers to look at text corpora as a source of information [8, 22, 9, 6, 3]. For example, Brent [6] describes a program which retrieves subcategorization frames from untagged text. Brent&apos;s approach relies on detecting nominal, clausal and infinitive complements after identification of proper nouns and pronouns using predictions based on GB&apos;s Case Filter [16) — e.g. in English, a noun phrase occurs to the immediate left of a tensed verb, or the immediate right of a main verb or preposition. Brent&apos;s results are impressive considering that no text preprocessing (e.g. tagging or bracketing) is assumed. However, the number of subcategorization options recognized is minima</context>
</contexts>
<marker>[6]</marker>
<rawString>Brent, M R. (1991) Automatic Semantic Classification of Verbs from their Syntactic Contexts: An Implemented Classifier for Stativity. In Proc 29th ACL, University of California, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
</authors>
<title>The ACQUILEX Lexical Database System: System Description and User Manual. In The (Other) Cambridge ACQUILEX Papers,</title>
<date>1992</date>
<tech>TR 253,</tech>
<institution>University of Cambridge,</institution>
<location>Cambridge, UK.</location>
<contexts>
<context position="11650" citStr="[5, 7]" startWordPosition="1730" endWordPosition="1731">nt step in the extraction of SF tokens is to distinguish passive and active verb phrases. Passives are discriminated by locating a past participle following an auxiliary be. 3.1.2 Converting SF Tokens into SF Types The semantic processor operates on the output of the predicate structure extractor. Inflected words in input SF tokens are first passed through a general purpose morphological analyser [17] and reduced to bare stems suitable for automated dictionary and lexicon searches. The next phase is to supplement SF tokens with semantic tags from LLOCE using the facilities of the ACQUILEX LDB [5, 7] and DCK [17]; LLOCE tags are associated with verb stems and simply replace noun stems. The resulting SF structures are finally converted into SF types according to the representation system whose syntax is sketched in (7) where: stem is the verb stem, parts a possibly empty sequence of particles associated with the verb stem, {A N } is the set of ILOCE semantic codes, pform the head of a prepositional phrase, compform the possibly empty complementizer of a clausal complement, and cat any category not covered by np-, pp-, sbar- and vp- frames. (7) SF-type ::= ( stem parts sem comps) sem ::= ( </context>
</contexts>
<marker>[7]</marker>
<rawString>Carroll, J. (1992). The ACQUILEX Lexical Database System: System Description and User Manual. In The (Other) Cambridge ACQUILEX Papers, TR 253, University of Cambridge, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography.</title>
<date>1989</date>
<booktitle>Proc 23rd ACL.</booktitle>
<pages>76--83</pages>
<contexts>
<context position="4423" citStr="[8, 22, 9, 6, 3]" startWordPosition="643" endWordPosition="647">f generating subcategorization lists from MRDs. In keeping with these observations, we implemented a suite of programs which provide an integrated approach to lexical knowledge acquisition. The programs elicit dependencies between semantic verb classes and their admissible subcategorization frames using machine readable thesauri to assist in semantic tagging of texts. 2 Background Currently available dictionaries do not provide a sufficiently reliable source of lexical knowlege for NLP systems. This has led an increasing number of researchers to look at text corpora as a source of information [8, 22, 9, 6, 3]. For example, Brent [6] describes a program which retrieves subcategorization frames from untagged text. Brent&apos;s approach relies on detecting nominal, clausal and infinitive complements after identification of proper nouns and pronouns using predictions based on GB&apos;s Case Filter [16) — e.g. in English, a noun phrase occurs to the immediate left of a tensed verb, or the immediate right of a main verb or preposition. Brent&apos;s results are impressive considering that no text preprocessing (e.g. tagging or bracketing) is assumed. However, the number of subcategorization options recognized is minima</context>
</contexts>
<marker>[8]</marker>
<rawString>Church, Kenneth and Hanks, Patrick (1989) Word Association Norms, Mutual Information and Lexicography. Proc 23rd ACL. pp. 76 — 83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
<author>P flanks</author>
<author>D Hindle</author>
</authors>
<title>Using Statistics in Lexical Analysis.</title>
<date>1991</date>
<booktitle>In Lexical Acquisition,</booktitle>
<location>Zernik, Uri, Ed. Erlbaum, Hillsdale, NJ.</location>
<contexts>
<context position="4423" citStr="[8, 22, 9, 6, 3]" startWordPosition="643" endWordPosition="647">f generating subcategorization lists from MRDs. In keeping with these observations, we implemented a suite of programs which provide an integrated approach to lexical knowledge acquisition. The programs elicit dependencies between semantic verb classes and their admissible subcategorization frames using machine readable thesauri to assist in semantic tagging of texts. 2 Background Currently available dictionaries do not provide a sufficiently reliable source of lexical knowlege for NLP systems. This has led an increasing number of researchers to look at text corpora as a source of information [8, 22, 9, 6, 3]. For example, Brent [6] describes a program which retrieves subcategorization frames from untagged text. Brent&apos;s approach relies on detecting nominal, clausal and infinitive complements after identification of proper nouns and pronouns using predictions based on GB&apos;s Case Filter [16) — e.g. in English, a noun phrase occurs to the immediate left of a tensed verb, or the immediate right of a main verb or preposition. Brent&apos;s results are impressive considering that no text preprocessing (e.g. tagging or bracketing) is assumed. However, the number of subcategorization options recognized is minima</context>
<context position="5643" citStr="[9]" startWordPosition="830" endWordPosition="830">d to imagine how the approach could be extended to cover the full range of subcategorization possibilities without introducing some form of text preprocessing. Also, the phrasal patterns extracted are too impoverished to infer selectional restrictions as they only contain proper nouns and pronouns. 2Brent&apos;s program recognizes five subcategorization frames built out of three kinds of constituents: noun phrase, clause, infinitive. 83 Lexical acquisition of collocational information from preprocessed text is now becoming more popular as tools for analyzing corpora are getting to be more reliable [9]. For example, Basili et al. [3] present a method for acquiring sublanguage-specific selectional restrictions from corpora which uses text processing techniques such as morphological tagging and shallow syntactic analysis. Their approach relies on extracting word pairs and triples which represent crucial environments for the acquisition of selectional restrictions (e.g. V_prep_N(go,to,Boston)). They then replace words with semantic tags (V_prep_N(PHYSICAL_ACT-to-PLACE)) and compute co-occurrence preferences among them. Semantic tags are crucial for making generalizations about the types of wor</context>
</contexts>
<marker>[9]</marker>
<rawString>Church, K and Gale, W. and flanks, P. and Hindle, D (1991) Using Statistics in Lexical Analysis. In Lexical Acquisition, Zernik, Uri, Ed. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun Classification from Predicate Argument Structures.</title>
<date>1990</date>
<booktitle>In Proc 28th ACL.</booktitle>
<pages>268--275</pages>
<marker>[10]</marker>
<rawString>Hindle, D. (1990). Noun Classification from Predicate Argument Structures. In Proc 28th ACL. pp. 268 — 275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="7657" citStr="[12, 11]" startWordPosition="1117" endWordPosition="1118">s-based Predicate Structure Extractor) we followed Basili et al.&apos;s idea of extracting semantically tagged phrasal frames from preprocessed text, but we used the Longman Lexicon of Contemporary English (LLOCE [151) to automate semantic tagging. LLOCE entries are similar to those of learner&apos;s dictionaries, but are arranged in a thesaurus-like fashion using semantic codes which provide a linguisticallymotivated classification of words. For example, [19] show that the semantic codes of LLOCE are instrumental in identifying members of the six subclasses of psychological predicates described in (5) [12, 11]. Affect type Experiencer Subject Stimulus Subject experience admire fear interest fascinate scare Neutral Positive Negative (5) As shown in (6), each verb representing a subclass has a code which often provides a uniform characterization of the subclass. Code Group Header Entries Fl Relating to feeling feel, sense, experience ... F140 Admiring and honouring admire, respect, look up to ... F121 Fear and Dread fear, fear for, be frightened ... F25 Attracting and interesting attract, interest, concern ... F26 Attracting and interesting very much fascinate, enthrall, enchant ... F122 Frighten and</context>
</contexts>
<marker>[11]</marker>
<rawString>Jackendoff, R. (1990) Semantic Structures. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>Towards a Lexical Organization of English Verbs.</title>
<date>1989</date>
<tech>Ms.,</tech>
<institution>Dept. of Linguistics, Northwestern University</institution>
<contexts>
<context position="1942" citStr="[12]" startWordPosition="275" endWordPosition="275">l bring a bottle of retsina to the party CAUSER THEME PATH GOAL However, a motion verb which is not amenable to direct external causation [13], will typically take a theme subject, with the possible addition of a directional argument, e.g. (2) The baby crawled (across the room) Co-occurrence restrictions between meaning components may also preempt subcategorization options; for example, manner of motion verbs in Italian cannot integrate a completed path component and therefore never subcategorize for a directional argument, e.g. (3)*Carlo ha camminato a casa Carlo walked home &apos;Following Levin [12] and Sanfilippo [18], we maintain that valency reduction processes (e.g. the causative-inchoative alternation) are semantically governed and thus do not weaken the correlation between verb semantics and subcategorization properties. 82 These generalizations are important for NLP since they frequently cover large subclasses of lexical items and can be used both to reduce redundancy and elucidate significant aspects of lexical structure. Moreover, a precise characterization of the relation between semantic subclasses and subcategorization properties of verbs can aid lexical disambiguation. For e</context>
<context position="7657" citStr="[12, 11]" startWordPosition="1117" endWordPosition="1118">s-based Predicate Structure Extractor) we followed Basili et al.&apos;s idea of extracting semantically tagged phrasal frames from preprocessed text, but we used the Longman Lexicon of Contemporary English (LLOCE [151) to automate semantic tagging. LLOCE entries are similar to those of learner&apos;s dictionaries, but are arranged in a thesaurus-like fashion using semantic codes which provide a linguisticallymotivated classification of words. For example, [19] show that the semantic codes of LLOCE are instrumental in identifying members of the six subclasses of psychological predicates described in (5) [12, 11]. Affect type Experiencer Subject Stimulus Subject experience admire fear interest fascinate scare Neutral Positive Negative (5) As shown in (6), each verb representing a subclass has a code which often provides a uniform characterization of the subclass. Code Group Header Entries Fl Relating to feeling feel, sense, experience ... F140 Admiring and honouring admire, respect, look up to ... F121 Fear and Dread fear, fear for, be frightened ... F25 Attracting and interesting attract, interest, concern ... F26 Attracting and interesting very much fascinate, enthrall, enchant ... F122 Frighten and</context>
</contexts>
<marker>[12]</marker>
<rawString>Levin, B. (1989) Towards a Lexical Organization of English Verbs. Ms., Dept. of Linguistics, Northwestern University</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
<author>M Rappaport</author>
</authors>
<title>The Lexical Semantics of Verbs in Motion: The Perspective from Unaccusativity.</title>
<date>1991</date>
<editor>Roca, I. (ed.)</editor>
<location>Dordrecht.</location>
<note>To appear in</note>
<contexts>
<context position="1480" citStr="[13]" startWordPosition="208" endWordPosition="208">IONS (GOAL, SOURCE). Lexicalization patterns which arise from identifying clusters of such meaning components in verb senses can be systematically related to distinct subcategorization frames.1 For example, the arguments of a verb expressing directed caused motion (e.g. bring, put, give) are normally a causative subject (agent), a theme direct object (moving entity) and a directional argument expressing path and reference location (goal), e.g. (1) Jackie will bring a bottle of retsina to the party CAUSER THEME PATH GOAL However, a motion verb which is not amenable to direct external causation [13], will typically take a theme subject, with the possible addition of a directional argument, e.g. (2) The baby crawled (across the room) Co-occurrence restrictions between meaning components may also preempt subcategorization options; for example, manner of motion verbs in Italian cannot integrate a completed path component and therefore never subcategorize for a directional argument, e.g. (3)*Carlo ha camminato a casa Carlo walked home &apos;Following Levin [12] and Sanfilippo [18], we maintain that valency reduction processes (e.g. the causative-inchoative alternation) are semantically governed a</context>
</contexts>
<marker>[13]</marker>
<rawString>Levin, B. and Rappaport, M. (1991) The Lexical Semantics of Verbs in Motion: The Perspective from Unaccusativity. To appear in Roca, I. (ed.) Thematic Structure: Its Role in Grammar, Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liberman</author>
<author>M Marcus</author>
</authors>
<title>Very Large Text Corpora: What You Can Do with Them, and How to Do It. Tutorial notes, 30th ACL,</title>
<date>1992</date>
<institution>University of Delaware,</institution>
<location>Newark, Delaware.</location>
<contexts>
<context position="9071" citStr="[14, 20, 21]" startWordPosition="1328" endWordPosition="1330">king and not Liking F26 Attracting and Interesting very much fascinate, enthrall, enchant, charm, captivate The bottom layer of the hierarchy contains over 1500 domain-specific tags, the middle layer has 129 tags and the top (most general) layer has 14. Domain-specific tags are always linked to intermediate tags which are, in turn, linked to general tags. Thus we can tag sublanguages using domain-specific semantic codes (as do Basili et a/.) without generating unrelated sets of such codes. We assigned semantic tags to Subcategorization Frame tokens (SF tokens) extracted from the Penn Treebank [14, 20, 21] to produce Subcategorization Frame types (SF types). Each SF type consists of a verb stem associated with one or more semantic tags, and a list of its (non-subject) complements, if any. The head of noun phrase complements were also semantically tagged. We used LLOCE collocational information — grammar codes — to reduce or remove semantic ambiguity arising from multiple assignment of tags to verb and noun stems. The structures below exemplify these three stages. SF token: ((DENY VB) (NP (ALIENS NNS)) (NP (*COMPOUND-NOUN* (STATE NN) (BENEFITS NNS)))) SF type: ((&amp;quot;deny&amp;quot; (&amp;quot;C193&amp;quot;-refuse &amp;quot;G127&amp;quot;-reje</context>
</contexts>
<marker>[14]</marker>
<rawString>Liberman, M. and Marcus, M (1992) Very Large Text Corpora: What You Can Do with Them, and How to Do It. Tutorial notes, 30th ACL, University of Delaware, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McArthur</author>
</authors>
<title>Longman Lexicon of Contemporary English.</title>
<date>1981</date>
<publisher>Longman,</publisher>
<location>London.</location>
<marker>[15]</marker>
<rawString>McArthur, T. (1981) Longman Lexicon of Contemporary English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rouvret</author>
<author>J Ft Vergnaud</author>
</authors>
<title>Specifying Reference to the Subject.</title>
<date>1980</date>
<booktitle>In Linguistic Enquiry,</booktitle>
<volume>11</volume>
<issue>1</issue>
<marker>[16]</marker>
<rawString>Rouvret, A. and Vergnaud, J Ft. (1980) Specifying Reference to the Subject. In Linguistic Enquiry, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sanfilippo</author>
</authors>
<title>A Morphological Analyser for English and Italian. In The (Other) Cambridge ACQUILEX Papers,</title>
<date>1992</date>
<tech>TR 253,</tech>
<institution>University of Cambridge,</institution>
<location>Cambridge, UK.</location>
<contexts>
<context position="11448" citStr="[17]" startWordPosition="1696" endWordPosition="1696"> a mention of the VFORM of the head verb, e.g. ((TRY VB) (VP TO)) • clausal complements are reduced to a mention of the complementizer which introduces them, e.g. ((ARGUED VBD) (SBAR THAT)) An important step in the extraction of SF tokens is to distinguish passive and active verb phrases. Passives are discriminated by locating a past participle following an auxiliary be. 3.1.2 Converting SF Tokens into SF Types The semantic processor operates on the output of the predicate structure extractor. Inflected words in input SF tokens are first passed through a general purpose morphological analyser [17] and reduced to bare stems suitable for automated dictionary and lexicon searches. The next phase is to supplement SF tokens with semantic tags from LLOCE using the facilities of the ACQUILEX LDB [5, 7] and DCK [17]; LLOCE tags are associated with verb stems and simply replace noun stems. The resulting SF structures are finally converted into SF types according to the representation system whose syntax is sketched in (7) where: stem is the verb stem, parts a possibly empty sequence of particles associated with the verb stem, {A N } is the set of ILOCE semantic codes, pform the head of a prepos</context>
</contexts>
<marker>[17]</marker>
<rawString>Sanfilippo, A (1992) A Morphological Analyser for English and Italian. In The (Other) Cambridge ACQUILEX Papers, TR 253, University of Cambridge, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sanfilippo</author>
</authors>
<title>Verbal Diathesis: Knowledge Acquisition, Lexicon Construction and Dictionary Compilation.</title>
<date>1993</date>
<tech>TR SLE/IT/93-11,</tech>
<institution>Sharp Laboratories of Europe,</institution>
<location>Oxford, UK.</location>
<contexts>
<context position="1962" citStr="[18]" startWordPosition="278" endWordPosition="278">retsina to the party CAUSER THEME PATH GOAL However, a motion verb which is not amenable to direct external causation [13], will typically take a theme subject, with the possible addition of a directional argument, e.g. (2) The baby crawled (across the room) Co-occurrence restrictions between meaning components may also preempt subcategorization options; for example, manner of motion verbs in Italian cannot integrate a completed path component and therefore never subcategorize for a directional argument, e.g. (3)*Carlo ha camminato a casa Carlo walked home &apos;Following Levin [12] and Sanfilippo [18], we maintain that valency reduction processes (e.g. the causative-inchoative alternation) are semantically governed and thus do not weaken the correlation between verb semantics and subcategorization properties. 82 These generalizations are important for NLP since they frequently cover large subclasses of lexical items and can be used both to reduce redundancy and elucidate significant aspects of lexical structure. Moreover, a precise characterization of the relation between semantic subclasses and subcategorization properties of verbs can aid lexical disambiguation. For example, the verb acc</context>
</contexts>
<marker>[18]</marker>
<rawString>Sanfilippo, A (1993) Verbal Diathesis: Knowledge Acquisition, Lexicon Construction and Dictionary Compilation. TR SLE/IT/93-11, Sharp Laboratories of Europe, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sanfilippo</author>
<author>V Poznaiiski</author>
</authors>
<title>The Acquisition of Lexical Knowledge from Combined Machine-Readable Dictionary Sources.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd Conference on Applied Natural Language Processing,</booktitle>
<location>Trento.</location>
<contexts>
<context position="7503" citStr="[19]" startWordPosition="1094" endWordPosition="1094">EAL_ESTATES). 3 CorPSE: a Body of Programs for Acquiring Semantically Tagged Subcategorization Frames from Bracketed Texts In developing CorPSE (Corpus-based Predicate Structure Extractor) we followed Basili et al.&apos;s idea of extracting semantically tagged phrasal frames from preprocessed text, but we used the Longman Lexicon of Contemporary English (LLOCE [151) to automate semantic tagging. LLOCE entries are similar to those of learner&apos;s dictionaries, but are arranged in a thesaurus-like fashion using semantic codes which provide a linguisticallymotivated classification of words. For example, [19] show that the semantic codes of LLOCE are instrumental in identifying members of the six subclasses of psychological predicates described in (5) [12, 11]. Affect type Experiencer Subject Stimulus Subject experience admire fear interest fascinate scare Neutral Positive Negative (5) As shown in (6), each verb representing a subclass has a code which often provides a uniform characterization of the subclass. Code Group Header Entries Fl Relating to feeling feel, sense, experience ... F140 Admiring and honouring admire, respect, look up to ... F121 Fear and Dread fear, fear for, be frightened ...</context>
</contexts>
<marker>[19]</marker>
<rawString>Sanfilippo, A and Poznaiiski, V. (1992) The Acquisition of Lexical Knowledge from Combined Machine-Readable Dictionary Sources. In Proceedings of the 3rd Conference on Applied Natural Language Processing, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Santorini</author>
</authors>
<title>Bracketing Guidelines for the Penn Treebank Project.</title>
<date>1991</date>
<institution>Ms. University of Pennsylvania.</institution>
<contexts>
<context position="9071" citStr="[14, 20, 21]" startWordPosition="1328" endWordPosition="1330">king and not Liking F26 Attracting and Interesting very much fascinate, enthrall, enchant, charm, captivate The bottom layer of the hierarchy contains over 1500 domain-specific tags, the middle layer has 129 tags and the top (most general) layer has 14. Domain-specific tags are always linked to intermediate tags which are, in turn, linked to general tags. Thus we can tag sublanguages using domain-specific semantic codes (as do Basili et a/.) without generating unrelated sets of such codes. We assigned semantic tags to Subcategorization Frame tokens (SF tokens) extracted from the Penn Treebank [14, 20, 21] to produce Subcategorization Frame types (SF types). Each SF type consists of a verb stem associated with one or more semantic tags, and a list of its (non-subject) complements, if any. The head of noun phrase complements were also semantically tagged. We used LLOCE collocational information — grammar codes — to reduce or remove semantic ambiguity arising from multiple assignment of tags to verb and noun stems. The structures below exemplify these three stages. SF token: ((DENY VB) (NP (ALIENS NNS)) (NP (*COMPOUND-NOUN* (STATE NN) (BENEFITS NNS)))) SF type: ((&amp;quot;deny&amp;quot; (&amp;quot;C193&amp;quot;-refuse &amp;quot;G127&amp;quot;-reje</context>
</contexts>
<marker>[20]</marker>
<rawString>Santorini, B. (1991) Bracketing Guidelines for the Penn Treebank Project. Ms. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Santorini</author>
</authors>
<title>Part-of-Spech Tagging Guidelines for the Penn Treebank Project.</title>
<date>1991</date>
<institution>Ms. University of Pennsylvania.</institution>
<contexts>
<context position="9071" citStr="[14, 20, 21]" startWordPosition="1328" endWordPosition="1330">king and not Liking F26 Attracting and Interesting very much fascinate, enthrall, enchant, charm, captivate The bottom layer of the hierarchy contains over 1500 domain-specific tags, the middle layer has 129 tags and the top (most general) layer has 14. Domain-specific tags are always linked to intermediate tags which are, in turn, linked to general tags. Thus we can tag sublanguages using domain-specific semantic codes (as do Basili et a/.) without generating unrelated sets of such codes. We assigned semantic tags to Subcategorization Frame tokens (SF tokens) extracted from the Penn Treebank [14, 20, 21] to produce Subcategorization Frame types (SF types). Each SF type consists of a verb stem associated with one or more semantic tags, and a list of its (non-subject) complements, if any. The head of noun phrase complements were also semantically tagged. We used LLOCE collocational information — grammar codes — to reduce or remove semantic ambiguity arising from multiple assignment of tags to verb and noun stems. The structures below exemplify these three stages. SF token: ((DENY VB) (NP (ALIENS NNS)) (NP (*COMPOUND-NOUN* (STATE NN) (BENEFITS NNS)))) SF type: ((&amp;quot;deny&amp;quot; (&amp;quot;C193&amp;quot;-refuse &amp;quot;G127&amp;quot;-reje</context>
</contexts>
<marker>[21]</marker>
<rawString>Santorini, B. (1991) Part-of-Spech Tagging Guidelines for the Penn Treebank Project. Ms. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F A Smajda</author>
<author>K R McKeown</author>
</authors>
<title>Automatically Extracting and Representing Collocations for Language Generation. In</title>
<date>1990</date>
<booktitle>Proc 28th ACL.</booktitle>
<pages>252--259</pages>
<contexts>
<context position="4423" citStr="[8, 22, 9, 6, 3]" startWordPosition="643" endWordPosition="647">f generating subcategorization lists from MRDs. In keeping with these observations, we implemented a suite of programs which provide an integrated approach to lexical knowledge acquisition. The programs elicit dependencies between semantic verb classes and their admissible subcategorization frames using machine readable thesauri to assist in semantic tagging of texts. 2 Background Currently available dictionaries do not provide a sufficiently reliable source of lexical knowlege for NLP systems. This has led an increasing number of researchers to look at text corpora as a source of information [8, 22, 9, 6, 3]. For example, Brent [6] describes a program which retrieves subcategorization frames from untagged text. Brent&apos;s approach relies on detecting nominal, clausal and infinitive complements after identification of proper nouns and pronouns using predictions based on GB&apos;s Case Filter [16) — e.g. in English, a noun phrase occurs to the immediate left of a tensed verb, or the immediate right of a main verb or preposition. Brent&apos;s results are impressive considering that no text preprocessing (e.g. tagging or bracketing) is assumed. However, the number of subcategorization options recognized is minima</context>
</contexts>
<marker>[22]</marker>
<rawString>Smajda, F A. and McKeown, K. R. (1990) Automatically Extracting and Representing Collocations for Language Generation. In Proc 28th ACL. pp. 252 — 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Talmy</author>
</authors>
<title>Lexicalization Patterns: Semantic Structure in Lexical Form.</title>
<date>1985</date>
<booktitle>In Shopen, T. (ed) Language Typology and Syntactic Description 3. Grammatical Categories and the Lexicon, CUP,</booktitle>
<contexts>
<context position="721" citStr="[23]" startWordPosition="96" endWordPosition="96">, Antonio Sanfilippo SHARP Laboratories of Europe Ltd. Oxford Science Park, Oxford 0X4 4GA {vp, aps}ftrg.oxford.ac.uk Abstract We present a method for individuating dependencies between the semantic class of predicates and their associated subcategorization frames, and describe an implementation which allows the acquisition of such dependencies from bracketed texts. 1 Introduction There is a widespread belief among linguists that a predicate&apos;s subcategorization frames are largely determined by its lexical-semantic properties [23, 11, 121. Consider the domain of movement verbs. Following Talmy [23], these can be semantically classified with reference to the meaning components: MOTION, MANNER, CAUSATION, THEME (MOVING ENTITY), PATH AND REFERENCE LOCATIONS (GOAL, SOURCE). Lexicalization patterns which arise from identifying clusters of such meaning components in verb senses can be systematically related to distinct subcategorization frames.1 For example, the arguments of a verb expressing directed caused motion (e.g. bring, put, give) are normally a causative subject (agent), a theme direct object (moving entity) and a directional argument expressing path and reference location (goal), e.</context>
</contexts>
<marker>[23]</marker>
<rawString>Talmy, L. Lexicalization Patterns: Semantic Structure in Lexical Form. In Shopen, T. (ed) Language Typology and Syntactic Description 3. Grammatical Categories and the Lexicon, CUP, 1985.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>