<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013466">
<title confidence="0.997668">
Discovering Implicit Discourse Relations Through Brown Cluster Pair
Representation and Coreference Patterns
</title>
<author confidence="0.991928">
Attapol T. Rutherford
</author>
<affiliation confidence="0.919829333333333">
Department of Computer Science
Brandeis University
Waltham, MA 02453, USA
</affiliation>
<email confidence="0.996924">
tet@brandeis.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999215">
Sentences form coherent relations in a
discourse without discourse connectives
more frequently than with connectives.
Senses of these implicit discourse rela-
tions that hold between a sentence pair,
however, are challenging to infer. Here,
we employ Brown cluster pairs to rep-
resent discourse relation and incorporate
coreference patterns to identify senses of
implicit discourse relations in naturally
occurring text. Our system improves the
baseline performance by as much as 25%.
Feature analyses suggest that Brown clus-
ter pairs and coreference patterns can re-
veal many key linguistic characteristics of
each type of discourse relation.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999737045454545">
Sentences must be pieced together logically in a
discourse to form coherent text. Many discourse
relations in the text are signaled explicitly through
a closed set of discourse connectives. Simply
disambiguating the meaning of discourse connec-
tives can determine whether adjacent clauses are
temporally or causally related (Pitler et al., 2008;
Wellner et al., 2009). Discourse relations and their
senses, however, can also be inferred by the reader
even without discourse connectives. These im-
plicit discourse relations in fact outnumber explicit
discourse relations in naturally occurring text. In-
ferring types or senses of implicit discourse re-
lations remains a key challenge in automatic dis-
course analysis.
A discourse parser requires many subcompo-
nents which form a long pipeline. The implicit
discourse relation discovery has been shown to be
the main performance bottleneck of an end-to-end
parser (Lin et al., 2010). It is also central to many
applications such as automatic summarization and
question-answering systems.
</bodyText>
<author confidence="0.780498">
Nianwen Xue
</author>
<affiliation confidence="0.958260666666667">
Department of Computer Science
Brandeis University
Waltham, MA 02453, USA
</affiliation>
<email confidence="0.983882">
xuen@brandeis.edu
</email>
<bodyText confidence="0.999975634146341">
Existing systems, which make heavy use of
word pairs, suffer from data sparsity problem as
a word pair in the training data may not appear
in the test data. A better representation of two
adjacent sentences beyond word pairs could have
a significant impact on predicting the sense of
the discourse relation that holds between them.
Data-driven theory-independent word classifica-
tion such as Brown clustering should be able
to provide a more compact word representation
(Brown et al., 1992). Brown clustering algorithm
induces a hierarchy of words in a large unanno-
tated corpus based on word co-occurrences within
the window. The induced hierarchy might give
rise to features that we would otherwise miss. In
this paper, we propose to use the cartesian product
of Brown cluster assignment of the sentence pair
as an alternative abstract word representation for
building an implicit discourse relation classifier.
Through word-level semantic commonalities
revealed by Brown clusters and entity-level rela-
tions revealed by coreference resolution, we might
be able to paint a more complete picture of the
discourse relation in question. Coreference resolu-
tion unveils the patterns of entity realization within
the discourse, which might provide clues for the
types of the discourse relations. The information
about certain entities or mentions in one sentence
should be carried over to the next sentence to form
a coherent relation. It is possible that coreference
chains and semantically-related predicates in the
local context might show some patterns that char-
acterize types of discourse relations. We hypoth-
esize that coreferential rates and coreference pat-
terns created by Brown clusters should help char-
acterize different types of discourse relations.
Here, we introduce two novel sets of features
for implicit discourse relation classification. Fur-
ther, we investigate the effects of using Brown
clusters as an alternative word representation and
analyze the impactful features that arise from
</bodyText>
<page confidence="0.985029">
645
</page>
<note confidence="0.746706555555556">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 645–654,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
Number of instances
Implicit Explicit
COMPARISON 2503 (15.11%) 5589 (33.73%)
CONTINGENCY 4255 (25.68%) 3741 (22.58%)
EXPANSION 8861 (53.48%) 72 (0.43%)
TEMPORAL 950 (5.73%) 3684 (33.73%)
Total 16569 (100%) 13086 (100%)
</note>
<tableCaption confidence="0.978684">
Table 1: The distribution of senses of implicit dis-
course relations is imbalanced.
</tableCaption>
<bodyText confidence="0.9892965">
Brown cluster pairs. We also study coreferential
patterns in different types of discourse relations in
addition to using them to boost the performance
of our classifier. These two sets of features along
with previously used features outperform the base-
line systems by approximately 5% absolute across
all categories and reveal many important charac-
teristics of implicit discourse relations.
</bodyText>
<sectionHeader confidence="0.988798" genericHeader="introduction">
2 Sense annotation in Penn Discourse
Treebank
</sectionHeader>
<bodyText confidence="0.999823513513513">
The Penn Discourse Treebank (PDTB) is the
largest corpus richly annotated with explicit
and implicit discourse relations and their senses
(Prasad et al., 2008). PDTB is drawn from
Wall Street Journal articles with overlapping an-
notations with the Penn Treebank (Marcus et al.,
1993). Each discourse relation contains the infor-
mation about the extent of the arguments, which
can be a sentence, a constituent, or an incontigu-
ous span of text. Each discourse relation is also
annotated with the sense of the relation that holds
between the two arguments. In the case of implicit
discourse relations, where the discourse connec-
tives are absent, the most appropriate connective
is annotated.
The senses are organized hierarchically. Our fo-
cus is on the top level senses because they are the
four fundamental discourse relations that various
discourse analytic theories seem to converge on
(Mann and Thompson, 1988). The top level senses
are COMPARISON, CONTINGENCY, EXPANSION,
and TEMPORAL.
The explicit and implicit discourse relations al-
most orthogonally differ in their distributions of
senses (Table 1). This difference has a few im-
plications for studying implicit discourse relations
and uses of discourse connectives (Patterson and
Kehler, 2013). For example, TEMPORAL relations
constitute only 5% of the implicit relations but
33% of the explicit relations because they might
not be as natural to create without discourse con-
nectives. On the other hand, EXPANSION rela-
tions might be more cleanly achieved without ones
as indicated by its dominance in the implicit dis-
course relations. This imbalance in class distri-
bution requires greater care in building statistical
classifiers (Wang et al., 2012).
</bodyText>
<sectionHeader confidence="0.989718" genericHeader="method">
3 Experiment setup
</sectionHeader>
<bodyText confidence="0.999973916666667">
We followed the setup of the previous studies
for a fair comparison with the two baseline sys-
tems by Pitler et al. (2009) and Park and Cardie
(2012). The task is formulated as four sepa-
rate one-against-all binary classification problems:
one for each top level sense of implicit discourse
relations. In addition, we add one more classifica-
tion task with which to test the system. We merge
ENTREL with EXPANSION relations to follow the
setup used by the two baseline systems. An argu-
ment pair is annotated with ENTREL in PDTB if
an entity-based coherence and no other type of re-
lation can be identified between the two arguments
in the pair. In this study, we assume that the gold
standard argument pairs are provided for each re-
lation. Most argument pairs for implicit discourse
relations are a pair of adjacent sentences or adja-
cent clauses separated by a semicolon and should
be easily extracted.
The PDTB corpus is split into a training set, de-
velopment set, and test set the same way as in the
baseline systems. Sections 2 to 20 are used to train
classifiers. Sections 0–1 are used for developing
feature sets and tuning models. Section 21–22 are
used for testing the systems.
The statistical models in the following exper-
iments are from MALLET implementation (Mc-
Callum, 2002) and libSVM (Chang and Lin,
2011). For all five binary classification tasks, we
try Balanced Winnow (Littlestone, 1988), Maxi-
mum Entropy, Naive Bayes, and Support Vector
Machine. The parameters and the hyperparame-
ters of each classifier are set to their default values.
The code for our model along with the data ma-
trices is available at github.com/attapol/
brown_coref_implicit.
</bodyText>
<sectionHeader confidence="0.999751" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.991944">
Unlike the baseline systems, all of the features
in the experiments use the output from automatic
natural language processing tools. We use the
Stanford CoreNLP suite to lemmatize and part-
of-speech tag each word (Toutanova et al., 2003;
</bodyText>
<page confidence="0.998545">
646
</page>
<bodyText confidence="0.987201833333333">
Toutanova and Manning, 2000), obtain the phrase
structure and dependency parses for each sentence
(De Marneffe et al., 2006; Klein and Manning,
2003), identify all named entities (Finkel et al.,
2005), and resolve coreference (Raghunathan et
al., 2010; Lee et al., 2011; Lee et al., 2013).
</bodyText>
<subsectionHeader confidence="0.997173">
4.1 Features used in previous work
</subsectionHeader>
<bodyText confidence="0.999462428571429">
The baseline features consist of the following:
First, last, and first 3 words, numerical ex-
pressions, time expressions, average verb phrase
length, modality, General Inquirer tags, polarity,
Levin verb classes, and production rules. These
features are described in greater detail by Pitler et
al. (2009).
</bodyText>
<subsectionHeader confidence="0.998491">
4.2 Brown cluster pair features
</subsectionHeader>
<bodyText confidence="0.998585590909091">
To generate Brown cluster assignment pair fea-
tures, we replace each word with its hard Brown
cluster assignment. We used the Brown word
clusters provided by MetaOptimize (Turian et
al., 2010). 3,200 clusters were induced from
RCV1 corpus, which contains about 63 million to-
kens from Reuters English newswire. Then we
take the Cartesian product of the Brown clus-
ter assignments of the words in Arg1 and the
ones of the words in Arg2. For example, sup-
pose Arg1 has two words w1,1, w1,2, Arg2 has
three words w2,1, w2,2, w2,3, and then B(.) maps
a word to its Brown cluster assignment. A
word wij is replaced by its corresponding Brown
cluster assignment bij = B(wij). The result-
ing word pair features are (b1,1, b2,1), (b1,1, b2,2),
(b1,1, b2,3), (b1,2, b2,1), (b1,2, b2,2), and (b1,2, b2,3).
Therefore, this feature set can generate
O(32002) binary features. The feature set size is
orders of magnitude smaller than using the actual
words, which can generate O(V 2) distinct binary
features where V is the size of the vocabulary.
</bodyText>
<subsectionHeader confidence="0.996997">
4.3 Coreference-based features
</subsectionHeader>
<bodyText confidence="0.999716428571429">
We want to take advantage of the semantics of
the sentence pairs even more by considering how
coreferential entities play out in the sentence pairs.
We consider various inter-sentential coreference
patterns to include as features and also to better
describe each type of discourse relation with re-
spect to its place in the coreference chain.
For compactness in explaining the following
features, we define similar words to be the words
assigned to the same Brown cluster.
Number of coreferential pairs: We count the
number of inter-sentential coreferential pairs.
We expect that EXPANSION relations should be
more likely to have coreferential pairs because the
detail or information about an entity mentioned
in Arg1 should be expanded in Arg2. Therefore,
entity sharing might be difficult to avoid.
Similar nouns and verbs: A binary feature
indicating whether similar or coreferential nouns
are the arguments of the similar predicates. Predi-
cates and arguments are identified by dependency
parses. We notice that sometimes the author uses
synonyms while trying to expand on the previous
predicates or entities. The words that indicate the
common topics might be paraphrased, so exact
string matching cannot detect whether the two ar-
guments still focus on the same topic. This might
be useful for identifying CONTINGENCY relations
as they usually discuss two causally-related events
that involve two seemingly unrelated agents
and/or predicates.
Similar subject or main predicates: A binary
feature indicating whether the main verbs of the
two arguments have the same subjects or not
and another binary feature indicating whether the
main verbs are similar or not. For our purposes,
the two subjects are said to be the same if they
are coreferential or assigned to the same Brown
cluster. We notice that COMPARISON relations
usually have different subjects for the same main
verbs and that TEMPORAL relations usually have
the same subjects but different main verbs.
</bodyText>
<subsectionHeader confidence="0.777741">
4.4 Feature selection and training sample
reweighting
</subsectionHeader>
<bodyText confidence="0.99999625">
The nature of the task and the dataset poses at
least two problems in creating a classifier. First,
the classification task requires a large number of
features, some of which are too rare and incon-
ducive to parameter estimation. Second, the la-
bel distribution is highly imbalanced (Table 1) and
this might degrade the performance of the classi-
fiers (Japkowicz, 2000). Recently, Park and Cardie
(2012) and Wang et al. (2012) addressed these
problems directly by optimally select a subset of
features and training samples. Unlike previous
work, we do not discard any of data in the training
set to balance the label distribution. Instead, we
reweight the training samples in each class during
parameter estimation such that the performance on
the development set is maximized. In addition, the
</bodyText>
<page confidence="0.995636">
647
</page>
<table confidence="0.999502">
P Current FI Park and Cardie (2012) Pitler et al. (2009)
R FI FI
COMPARISON vs others 27.34 72.41 39.70 31.32 21.96
CONTINGENCY vs others 44.52 69.96 54.42 49.82 47.13
EXPANSION vs others 59.59 85.50 70.23 - -
EXP+ENTREL vs others 69.26 95.92 80.44 79.22 76.42
TEMPORAL vs others 18.52 63.64 28.69 26.57 16.76
</table>
<tableCaption confidence="0.9990495">
Table 2: Our classifier outperform the previous systems across all four tasks without the use of gold-
standard parses and coreference resolution.
</tableCaption>
<table confidence="0.99997325">
COMPARISON Fl % change
Feature set
All features 39.70 -
All excluding Brown cluster pairs 35.71 -10.05%
All excluding Production rules 37.27 -6.80%
All excluding First, last, and First 3 39.18 -1.40%
All excluding Polarity 39.39 -0.79%
CONTINGENCY
Feature set Fl % change
All 54.42 -
All excluding Brown cluster pairs 51.50 -5.37%
All excluding First, last, and First 3 53.56 -1.58%
All excluding Polarity 53.82 -1.10%
All excluding Coreference 53.92 -0.92%
EXPANSION
Feature set Fl % change
All 70.23 -
All excluding Brown cluster pairs 67.48 -3.92%
All excluding First, last, and First 3 69.43 -1.14%
All excluding Inquirer tags 69.73 -0.71%
All excluding Polarity 69.92 -0.44%
TEMPORAL
Feature set Fl % change
All 28.69 -
All excluding Brown cluster pairs 24.53 -14.50%
All excluding Production rules 26.51 -7.60%
All excluding First, last, and First 3 26.56 -7.42%
All excluding Polarity 27.42 -4.43%
</table>
<tableCaption confidence="0.999378">
Table 3: Ablation study: The four most impact-
</tableCaption>
<bodyText confidence="0.995221285714286">
ful feature classes and their relative percentage
changes are shown. Brown cluster pair features
are the most impactful across all relation types.
number of occurrences for each feature must be
greater than a cut-off, which is also tuned on the
development set to yield the highest performance
on the development set.
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999204111111111">
Our experiments show that the Brown cluster and
coreference features along with the features from
the baseline systems improve the performance for
all discourse relations (Table 2). Consistent with
the results from previous work, the Naive Bayes
classifier outperforms MaxEnt, Balanced Winnow,
and Support Vector Machine across all tasks re-
gardless of feature pruning criteria and training
sample reweighting. A possible explanation is that
the small dataset size in comparison with the large
number of features might favor a generative model
like Naive Bayes (Jordan and Ng, 2002). So we
only report the performance from the Naive Bayes
classifiers.
It is noteworthy that the baseline systems use
the gold standard parses provided by the Penn
Treebank, but ours does not because we would
like to see how our system performs realistically in
conjunction with other pre-processing tasks such
as lemmatization, parsing, and coreference reso-
lution. Nevertheless, our system still manages to
outperform the baseline systems in all relations by
a sizable margin.
Our preliminary results on implicit sense classi-
fication suggest that the Brown cluster word rep-
resentation and coreference patterns might be in-
dicative of the senses of the discourse relations,
but we would like to know the extent of the im-
pact of these novel feature sets when used in con-
junction with other features. To this aim, we con-
duct an ablation study, where we exclude one of
the feature sets at a time and then test the result-
ing classifier on the test set. We then rank each
feature set by the relative percentage change in
FI score when excluded from the classifier. The
data split and experimental setup are identical to
the ones described in the previous section but only
with Naive Bayes classifiers.
The ablation study results imply that Brown
cluster features are the most impactful feature set
across all four types of implicit discourse rela-
tions. When ablated, Brown cluster features de-
grade the performance by the largest percentage
compared to the other feature sets regardless of the
relation types(Table 3). TEMPORAL relations ben-
</bodyText>
<page confidence="0.994241">
648
</page>
<bodyText confidence="0.99972275">
efit the most from Brown cluster features. With-
out them, the F1 score drops by 4.12 absolute or
14.50% relative to the system that uses all of the
features.
</bodyText>
<sectionHeader confidence="0.993583" genericHeader="method">
6 Feature analysis
</sectionHeader>
<subsectionHeader confidence="0.9993">
6.1 Brown cluster features
</subsectionHeader>
<bodyText confidence="0.999818964285714">
This feature set is inspired by the word pair fea-
tures, which are known for its effectiveness in pre-
dicting senses of discourse relations between the
two arguments. Marcu et al (2002), for instance,
artificially generated the implicit discourse rela-
tions and used word pair features to perform the
classification tasks. Those word pair features work
well in this case because their artificially gener-
ated dataset is an order of magnitude larger than
PDTB. Ideally, we would want to use the word
pair features instead of word cluster features if
we have enough data to fit the parameters. Con-
sequently, other less sparse handcrafted features
prove to be more effective than word pair features
for the PDTB data (Pitler et al., 2009). We remedy
the sparsity problem by clustering the words that
are distributionally similar together and greatly re-
duce the number of features.
Since the ablation study is not fine-grained
enough to spotlight the effectiveness of the indi-
vidual features, we quantify the predictiveness of
each feature by its mutual information. Under
Naive Bayes conditional independence assump-
tion, the mutual information between the features
and the labels can be efficiently computed in a
pairwise fashion. The mutual information be-
tween a binary feature XZ and class label Y is de-
fined as:
</bodyText>
<equation confidence="0.896758">
�
I(XZ, Y ) =
y
</equation>
<bodyText confidence="0.999919609375">
ˆp(·) is the probability distribution function whose
parameters are maximum likelihood estimates
from the training set. We compute mutual infor-
mation for all four one-vs-all classification tasks.
The computation is done as part of the training
pipeline in MALLET to ensure consistency in pa-
rameter estimation and smoothing techniques. We
then rank the cluster pair features by mutual in-
formation. The results are compactly summa-
rized in bipartite graphs shown in Figure 1, where
each edge represents a cluster pair. Since mu-
tual information itself does not indicate whether
a feature is favored by one or the other label, we
also verify the direction of the effects of each of
the features included in the following analysis by
comparing the class conditional parameters in the
Naive Bayes model.
The most dominant features for COMPARISON
classification are the pairs whose members are
from the same Brown clusters. We can distinctly
see this pattern from the bipartite graph because
the nodes on each side are sorted alphabetically.
The graph shows many parallel short edges, which
suggest that many informative pairs consist of the
same clusters. Some of the clusters that participate
in such pair consist of named-entities from vari-
ous categories such as airlines (King, Bell, Virgin,
Continental, ...), and companies (Thomson, Volk-
swagen, Telstra, Siemens). Some of the pairs form
a broad category such as political agents (citizens,
pilots, nationals, taxpayers) and industries (power,
insurance, mining). These parallel patterns in the
graph demonstrate that implicit COMPARISON re-
lations might be mainly characterized by juxtapos-
ing and explicitly contrasting two different entities
in two adjacent sentences.
Without the use of a named-entity recogni-
tion system, these Brown cluster pair features ef-
fectively act as features that detect whether the
two arguments in the relation contain named-
entities or nouns from the same categories or not.
These more subtle named-entity-related features
are cleanly discovered through replacing words
with their data-driven Brown clusters without the
need for additional layers of pre-processing.
If the words in one cluster semantically relates
to the words in another cluster, the two clusters
are more likely to become informative features
for CONTINGENCY classification. For instance,
technical terms in stock and trading (weighted,
Nikkei, composite, diffusion) pair up with eco-
nomic terms (Trading, Interest, Demand, Produc-
tion). The cluster with analysts and pundits pairs
up with the one that predominantly contains quan-
tifiers (actual, exact, ultimate, aggregate). In ad-
dition to this pattern, we observed the same par-
allel pair pattern we found in COMPARISON clas-
sification. These results suggest that in establish-
ing a CONTINGENCY relation implicitly the au-
thor might shape the sentences such that they have
semantically related words if they do not mention
named-entities of the same category.
Through Brown cluster pairs, we obtain features
that detect a shift between generality and speci-
</bodyText>
<figure confidence="0.89996834883721">
ˆp(x, y) log ˆp(x, y)
ˆp(x)ˆp(y)
x=0,1
649
Arg 1 COMPARISON Arg2
American American
Bank Big,Human,Civil,Greater,...
Centre,Bay,Park,Hospital,... Board,Corps
Congress Centre,Bay,Park,Hospital,...
East Congress
Exchange East
Fed,CWB Fed,CWB
Israelis,Moslems,Jews,terrorists,... GM,Ford,Barrick,Anglo,...
Japan Israelis,Moslems,Jews,terrorists,...
King,Bell,Virgin,Continental,... Japan
March King,Bell,Virgin,Continental,...
Miert,Lumpur,der,Metall,... March
Olivetti,Eurotunnel,Elf,Lagardere,... Miert,Lumpur,der,Metall,...
Power,Insurance,Mining,Engineering,... Olivetti,Eurotunnel,Elf,Lagardere,...
Soviet,Homeland,Patriotic Power,Insurance,Mining,Engineering,...
Standard,Hurricane,Time,Long,... Soviet,Homeland,Patriotic
Thomson,Volkswagen,Telstra,Siemens,... Standard,Hurricane,Time,Long,...
advertising,ad Thomson,Volkswagen,Telstra,Siemens,...
agency actual,exact,ultimate,aggregate,...
analysts,pundits advertising,ad
auto,semiconductor,automotive,automobile,... agency
average average
cash bank
chemicals,entertainment,machinery,packaging,... cars,vehicles,tyres,vans,...
citizens,pilots,nationals,taxpayers,... cash
Arg 1 CONTINGENCY Arg 2
: &amp;,und
Bank –
Christopher,Simitis,Perry,Waigel,... 10
Electric,Motor,Life,Chemical,... ;
Friday Bank
Holdings,Industries,Investments,Foods,... Bill,Mrs.
If,Unless,Whether,Maybe,... Christopher,Simitis,Perry,Waigel,...
King,Bell,Virgin,Continental,... Dow,shuttle,DAX,Ifo,...
Major,Howard,Arthuis,Chang,... Electric,Motor,Life,Chemical,...
March Friday
Power,Insurance,Mining,Engineering,... GM,Ford,Barrick,Anglo,...
Royal,Port,Cape,Santa,... Holdings,Industries,Investments,Foods,...
Senate,senate I
age,identity,integrity,identification,... If,Unless,Whether,Maybe,...
also King,Bell,Virgin,Continental,...
am,’m Major,Howard,Arthuis,Chang,...
analysts,pundits March
average Power,Insurance,Mining,Engineering,...
back Royal,Port,Cape,Santa,...
her Senate,senate
his To,Would
index Trading,Interest,Demand,Production,...
market actual,exact,ultimate,aggregate,...
no age,identity,integrity,identification,...
now ago
closed chemicals,entertainment,machinery,packaging,...
common citizens,pilots,nationals,taxpayers,...
computer,mainframe closed
sales common
computer,mainframe
our all
they
weighted,Nikkei,composite,diffusion,...
world
Arg 1 EXPANSION Arg 2
American –
Analysts,Economists,Diplomats,Forecasters,... American
But,Saying Boeing,BT,Airbus,Netscape,...
Democrats December
Dow,shuttle,DAX,Ifo,... Exchange
Dutroux,Lopez,Morris,Hamanaka,... GM,Ford,Barrick,Anglo,...
Electric,Motor,Life,Chemical,... I
For,Like Lynch,Fleming,Reagan,Brandford,...
Net,Operating,Primary,Minority,... No
Olivetti,Eurotunnel,Elf,Lagardere,... Olivetti,Eurotunnel,Elf,Lagardere,...
Plc,Oy,NV,AB,... Plc,Oy,NV,AB,...
S&amp;P,Burns,Tietmeyer,Rifkind,... Republicans,Bonds,Rangers,Greens,...
Soviet,Homeland,Patriotic Senate,senate
Telecommunications,Broadcasting,Futures,Rail,... Soviet,Homeland,Patriotic
Texas,Queensland,Ohio,Illinois,... That
U.S. Trading,Interest,Demand,Production,...
VW,Conrail,Bre-X,Texaco,... U.S.
advertising,ad VW,Conrail,Bre-X,Texaco,...
am,’m We,Things
businesses all
dollar,greenback,ecu analyst,meteorologist
five-year,three-year,two-year,four-year,... because
get been
if,whenever,wherever business
investors cents,pence,p.m.,a.m.,...
no compared,coupled,compares
occupation,Index,Kurdistan,Statements,... could
plan
stocks
under
Arg 1 TEMPORAL Arg2
’ve
*,@,**,—,...
14,13,16 ’ve
20 *,@,**,—,...
30 0.5,44,0.2,0.3,...
50,1.50,0.50,0.05,... 10
: 10-year,collective,dual,30-year,...
American 17,19,21
British 1991,1989,1949,1979,...
Friday 200,300,150,120,...
Investors,Banks,Companies,Farmers,... 26,28,29
Prices,Results,Sales,Thousands,... 27,22,23
Sept,Nov.,Oct.,Oct,... 3,A1,C1,C3,...
Trading,Interest,Demand,Production,... 30
Treasury,mortgage-backed 50,1.50,0.50,0.05,...
York,York-based
age,identity,integrity,identification,...
bond,floating-rate
books,words,budgets,clothes,...
consumer
convertible,bonus,Brady,subordinated,...
increase
interest
loss
months
no-fly,year-ago,corresponding,buffer,...
people
quarter
results
rose
there
’re
</figure>
<figureCaption confidence="0.8931905">
Figure 1: The bipartite graphs show the top 40 non-stopword Brown cluster pair features for all four
classification tasks. Each node on the left and on the right represents word cluster from Arg1 and Arg2
</figureCaption>
<bodyText confidence="0.994461761904762">
respectively. We only show the clusters that appear fewer than six times in the top 3,000 pairs to exclude
stopwords. Although the four tasks are interrelated, some of the highest mutual information features vary
substantially across tasks.
ficity within the scope of the relation. For exam-
ple, a cluster with industrial categories (Electric,
Motor, Life, Chemical, Automotive) couples with
specific brands or companies (GM, Ford, Barrick,
Anglo). Or such a pair might simply reflects a shift
in plurality e.g. businesses -business and Analysts
-analyst. EXPANSION relations capture relations
in which one argument provides a specification of
the previous and relations in which one argument
provides a generalization of the other. Thus, these
shift detection features could help distinguish EX-
PANSION relations.
We found a few common coreference patterns
of names in written English to be useful. First and
last name are used in the first sentence to refer to a
person who just enters the discourse. That person
is referred to just by his/her title and last name in
the following sentence. This pattern is found to be
</bodyText>
<page confidence="0.997257">
650
</page>
<figureCaption confidence="0.95651075">
Figure 2: The coreferential rate for TEMPORAL
relations is significantly higher than the other three
relations (p &lt; 0.05, corrected for multiple com-
parison).
</figureCaption>
<bodyText confidence="0.999941260869565">
informative for EXPANSION relations. For exam-
ple, the edges (not shown in the graph due to lack
of space) from the first name clusters to the title
(Mr, Mother, Judge, Dr) cluster.
Time expressions constitutes the majority of the
nodes in the bipartite graph for TEMPORAL rela-
tions. More strikingly, the specific dates (e.g. clus-
ters that have positive integers smaller than 31)
are more frequently found in Arg2 than Arg1 in
implicit TEMPORAL relations. It is possible that
TEMPORAL relations are more naturally expressed
without a discourse connective if a time point is
clearly specified in Arg2 but not in Arg1.
TEMPORAL relations might also be implicitly
inferred through detecting a shift in quantities. We
notice that clusters whose words indicate changes
e.g. increase, rose, loss pair with number clusters.
Sentences in which such pairs participate might be
part of a narrative or a report where one expects a
change over time. These changes conveyed by the
sentences constitute a natural sequence of events
that are temporally related but might not need ex-
plicit temporal expressions.
</bodyText>
<subsectionHeader confidence="0.999793">
6.2 Coreference features
</subsectionHeader>
<bodyText confidence="0.99910995">
Coreference features are very effective given that
they constitute a very small set compared to the
other feature sets. In particular, excluding them
from the model reduces F1 scores for TEMPORAL
and CONTINGENCY relations by approximately
1% relative to the system that uses all of the
features. We found that the sentence pairs in these
two types of relations have distinctive coreference
patterns.
We count the number of pairs of arguments that
are linked by a coreference chain for each type of
relation. The coreference chains used in this study
are detected automatically from the training set
through Stanford CoreNLP suite (Raghunathan et
al., 2010; Lee et al., 2011; Lee et al., 2013). TEM-
PORAL relations have a significantly higher coref-
erential rate than the other three relations (p &lt;
0.05, pair-wise t-test corrected for multiple com-
parisons). The differences between COMPARI-
SON, CONTINGENCY, and EXPANSION, however,
are not statistically significant (Figure 2).
The choice to use or not to use a discourse
connective is strongly motivated by linguistic fea-
tures at the discourse levels (Patterson and Kehler,
2013). Additionally, it is very uncommon to
have temporally-related sentences without using
explicit discourse connectives. The difference in
coreference patterns might be one of the factors
that influence the choice of using a discourse con-
nective to signal a TEMPORAL relation. If sen-
tences are coreferentially linked, then it might be
more natural to drop a discourse connective be-
cause the temporal ordering can be easily inferred
without it. For example,
(1) Her story is partly one of personal down-
fall. [previously] She was an unstinting
teacher who won laurels and inspired stu-
dents... (WSJ0044)
The coreference chain between the two
temporally-related sentences in (1) can easily
be detected. Inserting previously as suggested
by the annotation from the PDTB corpus does
not add to the temporal coherence of the sen-
tences and may be deemed unnecessary. But the
presence of coreferential link alone might bias
the inference toward TEMPORAL relation while
CONTINGENCY might also be inferred.
Additionally, we count the number of pairs of
arguments whose grammatical subjects are linked
by a coreference chain to reveal the syntactic-
coreferential patterns in different relation types.
Although this specific pattern seems rare, more
than eight percent of all relations have coreferen-
tial grammatical subjects. We observe the same
statistically significant differences between TEM-
PORAL relations and the other three types of re-
lations. More interestingly, the subject coreferen-
tial rate for CONTINGENCY relations is the lowest
among the three categories (p &lt; 0.05, pair-wise
t-test corrected for multiple comparisons).
</bodyText>
<figure confidence="0.998199888888889">
Subject coreference
All coreference
0.5
0.4
0.3
0.2
0.1
0.0
Coreferential rate
</figure>
<page confidence="0.996457">
651
</page>
<bodyText confidence="0.999285461538462">
It is possible that coreferential subject patterns
suggest temporal coherence between the two sen-
tences without using an explicit discourse connec-
tive. CONTINGENCY relations, which can only in-
dicate causal relationships when realized implic-
itly, impose the temporal ordering of events in the
arguments; i.e. if Arg1 is causally related to Arg2,
then the event described in Arg1 must temporally
precede the one in Arg2. Therefore, CONTIN-
GENCY and TEMPORAL can be highly confusable.
To understand why this pattern might help distin-
guish these two types of relations, consider these
examples:
</bodyText>
<listItem confidence="0.987537571428571">
(2) He also asserted that exact questions weren’t
replicated. [Then] When referred to the ques-
tions that match, he said it was coincidental.
(WSJ0045)
(3) He also asserted that exact questions weren’t
replicated. When referred to the questions
that match, she said it was coincidental.
</listItem>
<bodyText confidence="0.9999595">
When we switch out the coreferential subject
for an arbitrary uncoreferential pronoun as we do
in (3), we are more inclined to classify the relation
as CONTINGENCY.
</bodyText>
<sectionHeader confidence="0.999977" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.999948763157895">
Word-pair features are known to work very well
in predicting senses of discourse relations in an
artificially generated corpus (Marcu and Echi-
habi, 2002). But when used with a realistic cor-
pus, model parameter estimation suffers from data
sparsity problem due to the small dataset size. Bi-
ran and McKeown (2013) attempts to solve this
problem by aggregating word pairs and estimating
weights from an unannotated corpus but only with
limited success.
Recent efforts have focused on introducing
meaning abstraction and semantic representation
between the words in the sentence pair. Pitler et al.
(2009) uses external lexicons to replace the one-
hot word representation with semantic information
such as word polarity and various verb classifica-
tion based on specific theories (Stone et al., 1968;
Levin, 1993). Park and Cardie (2012) selects an
optimal subset of these features and establishes the
strongest baseline to best of our knowledge.
Brown word clusters are hierarchical clusters
induced by frequency of co-occurrences with other
words (Brown et al., 1992). The strength of this
word class induction method is that the words that
are classified to the same clusters usually make
an interpretable lexical class by the virtue of their
distributional properties. This word representation
has been used successfully to augment the perfor-
mance of many NLP systems (Ritter et al., 2011;
Turian et al., 2010).
Louis et al. (2010) uses multiple aspects of
coreference as features to classify implicit dis-
course relations without much success while sug-
gesting many aspects that are worth exploring. In a
corpus study by Louis and Nenkova (2010), coref-
erential rates alone cannot explain all of the rela-
tions, and more complex coreference patterns have
to be considered.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.9999775">
We present statistical classifiers for identifying
senses of implicit discourse relations and intro-
duce novel feature sets that exploit distributional
similarity and coreference information. Our clas-
sifiers outperform the classifiers from previous
work in all types of implicit discourse relations.
Altogether these results present a stronger base-
line for the future research endeavors in implicit
discourse relations.
In addition to enhancing the performance of the
classifier, Brown word cluster pair features dis-
close some of the new aspects of implicit dis-
course relations. The feature analysis confirms
our hypothesis that cluster pair features work well
because they encapsulate relevant word classes
which constitute more complex informative fea-
tures such as named-entity pairs of the same cat-
egories, semantically-related pairs, and pairs that
indicate specificity-generality shift. At the dis-
course level, Brown clustering is superior to a
one-hot word representation for identifying inter-
sentential patterns and the interactions between
words.
Coreference chains that traverse through the
discourse in the text shed the light on differ-
ent types of relations. The preliminary analy-
sis shows that TEMPORAL relations have much
higher inter-argument coreferential rates than the
other three senses of relations. Focusing on only
subject-coreferential rates, we observe that CON-
TINGENCY relations show the lowest coreferential
rate. The coreference patterns differ substantially
and meaningfully across discourse relations and
deserve further exploration.
</bodyText>
<page confidence="0.997404">
652
</page>
<sectionHeader confidence="0.995807" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996518790476191">
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 69–73. The Association for Compu-
tational Linguistics.
Peter F Brown, Peter V deSouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n -gram models of natural language.
Computational Linguistics, 18(4):467–479, Decem-
ber.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449–454.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Nathalie Japkowicz. 2000. Learning from imbalanced
data sets: a comparison of various strategies. In
AAAI workshop on learning from imbalanced data
sets, volume 68.
Michael Jordan and Andrew Ng. 2002. On discrimi-
native vs. generative classifiers: A comparison of lo-
gistic regression and naive bayes. Advances in neu-
ral information processing systems, 14:841.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. In the 41st Annual Meet-
ing, pages 423–430, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34. Association for Computational
Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation, volume 348.
University of Chicago press Chicago.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan.
2010. A PDTB-Styled End-to-End Discourse
Parser. arXiv.org, November.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. Machine learning, 2(4):285–318.
Annie Louis and Ani Nenkova. 2010. Creating lo-
cal coherence: An empirical assessment. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 313–316.
Association for Computational Linguistics.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify
implicit discourse relations. In Proceedings of the
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 59–62. Associa-
tion for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368–375. Association for Computational Lin-
guistics.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 108–112. Association for Com-
putational Linguistics.
Gary Patterson and Andrew Kehler. 2013. Predicting
the presence of discourse connectives. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008.
Easily identifiable discourse relations. Technical
Reports (CIS), page 884.
</reference>
<page confidence="0.990567">
653
</page>
<reference confidence="0.999761415384615">
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’10,
pages 492–501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524–1534. Association for Computational Linguis-
tics.
Philip Stone, Dexter C Dunphy, Marshall S Smith, and
DM Ogilvie. 1968. The general inquirer: A com-
puter approach to content analysis. Journal of Re-
gional Science, 8(1).
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical
methods in natural language processing and very
large corpora: held in conjunction with the 38th An-
nual Meeting of the Association for Computational
Linguistics-Volume 13, pages 63–70. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li. 2012.
Implicit discourse relation recognition by selecting
typical training examples. In Proceedings of COL-
ING 2012, pages 2757–2772, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Ben Wellner, James Pustejovsky, Catherine Havasi,
Anna Rumshisky, and Roser Sauri. 2009. Clas-
sification of discourse coherence relations: An ex-
ploratory study using multiple knowledge sources.
In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, pages 117–125. Association
for Computational Linguistics.
</reference>
<page confidence="0.999026">
654
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.678460">
<title confidence="0.9990985">Discovering Implicit Discourse Relations Through Brown Cluster Representation and Coreference Patterns</title>
<author confidence="0.987986">T Attapol</author>
<affiliation confidence="0.84647">Department of Computer Brandeis</affiliation>
<address confidence="0.993869">Waltham, MA 02453,</address>
<email confidence="0.999861">tet@brandeis.edu</email>
<abstract confidence="0.999702058823529">Sentences form coherent relations in a discourse without discourse connectives more frequently than with connectives. Senses of these implicit discourse relations that hold between a sentence pair, however, are challenging to infer. Here, we employ Brown cluster pairs to represent discourse relation and incorporate coreference patterns to identify senses of implicit discourse relations in naturally occurring text. Our system improves the baseline performance by as much as 25%. Feature analyses suggest that Brown cluster pairs and coreference patterns can reveal many key linguistic characteristics of each type of discourse relation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aggregated word pair features for implicit discourse relation disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>69--73</pages>
<contexts>
<context position="32480" citStr="Biran and McKeown (2013)" startWordPosition="4698" endWordPosition="4702">) He also asserted that exact questions weren’t replicated. When referred to the questions that match, she said it was coincidental. When we switch out the coreferential subject for an arbitrary uncoreferential pronoun as we do in (3), we are more inclined to classify the relation as CONTINGENCY. 7 Related work Word-pair features are known to work very well in predicting senses of discourse relations in an artificially generated corpus (Marcu and Echihabi, 2002). But when used with a realistic corpus, model parameter estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based on specific theories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest basel</context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 69–73. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n -gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="2508" citStr="Brown et al., 1992" startWordPosition="365" endWordPosition="368">estion-answering systems. Nianwen Xue Department of Computer Science Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Existing systems, which make heavy use of word pairs, suffer from data sparsity problem as a word pair in the training data may not appear in the test data. A better representation of two adjacent sentences beyond word pairs could have a significant impact on predicting the sense of the discourse relation that holds between them. Data-driven theory-independent word classification such as Brown clustering should be able to provide a more compact word representation (Brown et al., 1992). Brown clustering algorithm induces a hierarchy of words in a large unannotated corpus based on word co-occurrences within the window. The induced hierarchy might give rise to features that we would otherwise miss. In this paper, we propose to use the cartesian product of Brown cluster assignment of the sentence pair as an alternative abstract word representation for building an implicit discourse relation classifier. Through word-level semantic commonalities revealed by Brown clusters and entity-level relations revealed by coreference resolution, we might be able to paint a more complete pic</context>
<context position="33232" citStr="Brown et al., 1992" startWordPosition="4813" endWordPosition="4816">cess. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based on specific theories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010). Louis et al. (2010) uses multiple aspects of coreference as features to classify implicit discourse relations without much success while suggesting many aspects that are worth exploring. In a corpus study by Louis and Nenkova (2010), coreferential rates</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V deSouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n -gram models of natural language. Computational Linguistics, 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="8005" citStr="Chang and Lin, 2011" startWordPosition="1221" endWordPosition="1224">ment pairs are provided for each relation. Most argument pairs for implicit discourse relations are a pair of adjacent sentences or adjacent clauses separated by a semicolon and should be easily extracted. The PDTB corpus is split into a training set, development set, and test set the same way as in the baseline systems. Sections 2 to 20 are used to train classifiers. Sections 0–1 are used for developing feature sets and tuning models. Section 21–22 are used for testing the systems. The statistical models in the following experiments are from MALLET implementation (McCallum, 2002) and libSVM (Chang and Lin, 2011). For all five binary classification tasks, we try Balanced Winnow (Littlestone, 1988), Maximum Entropy, Naive Bayes, and Support Vector Machine. The parameters and the hyperparameters of each classifier are set to their default values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 T</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8804" citStr="Finkel et al., 2005" startWordPosition="1345" endWordPosition="1348">eters of each classifier are set to their default values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word c</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Japkowicz</author>
</authors>
<title>Learning from imbalanced data sets: a comparison of various strategies.</title>
<date>2000</date>
<booktitle>In AAAI workshop on learning from imbalanced data sets,</booktitle>
<volume>68</volume>
<contexts>
<context position="12713" citStr="Japkowicz, 2000" startWordPosition="1972" endWordPosition="1973">ame Brown cluster. We notice that COMPARISON relations usually have different subjects for the same main verbs and that TEMPORAL relations usually have the same subjects but different main verbs. 4.4 Feature selection and training sample reweighting The nature of the task and the dataset poses at least two problems in creating a classifier. First, the classification task requires a large number of features, some of which are too rare and inconducive to parameter estimation. Second, the label distribution is highly imbalanced (Table 1) and this might degrade the performance of the classifiers (Japkowicz, 2000). Recently, Park and Cardie (2012) and Wang et al. (2012) addressed these problems directly by optimally select a subset of features and training samples. Unlike previous work, we do not discard any of data in the training set to balance the label distribution. Instead, we reweight the training samples in each class during parameter estimation such that the performance on the development set is maximized. In addition, the 647 P Current FI Park and Cardie (2012) Pitler et al. (2009) R FI FI COMPARISON vs others 27.34 72.41 39.70 31.32 21.96 CONTINGENCY vs others 44.52 69.96 54.42 49.82 47.13 EX</context>
</contexts>
<marker>Japkowicz, 2000</marker>
<rawString>Nathalie Japkowicz. 2000. Learning from imbalanced data sets: a comparison of various strategies. In AAAI workshop on learning from imbalanced data sets, volume 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Jordan</author>
<author>Andrew Ng</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems,</title>
<date>2002</date>
<pages>14--841</pages>
<contexts>
<context position="15457" citStr="Jordan and Ng, 2002" startWordPosition="2408" endWordPosition="2411">ance on the development set. 5 Results Our experiments show that the Brown cluster and coreference features along with the features from the baseline systems improve the performance for all discourse relations (Table 2). Consistent with the results from previous work, the Naive Bayes classifier outperforms MaxEnt, Balanced Winnow, and Support Vector Machine across all tasks regardless of feature pruning criteria and training sample reweighting. A possible explanation is that the small dataset size in comparison with the large number of features might favor a generative model like Naive Bayes (Jordan and Ng, 2002). So we only report the performance from the Naive Bayes classifiers. It is noteworthy that the baseline systems use the gold standard parses provided by the Penn Treebank, but ours does not because we would like to see how our system performs realistically in conjunction with other pre-processing tasks such as lemmatization, parsing, and coreference resolution. Nevertheless, our system still manages to outperform the baseline systems in all relations by a sizable margin. Our preliminary results on implicit sense classification suggest that the Brown cluster word representation and coreference</context>
</contexts>
<marker>Jordan, Ng, 2002</marker>
<rawString>Michael Jordan and Andrew Ng. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems, 14:841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In the 41st Annual Meeting,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8753" citStr="Klein and Manning, 2003" startWordPosition="1337" endWordPosition="1340">pport Vector Machine. The parameters and the hyperparameters of each classifier are set to their default values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In the 41st Annual Meeting, pages 423–430, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8873" citStr="Lee et al., 2011" startWordPosition="1356" endWordPosition="1359">r model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 cluster</context>
<context position="28957" citStr="Lee et al., 2011" startWordPosition="4153" endWordPosition="4156">onstitute a very small set compared to the other feature sets. In particular, excluding them from the model reduces F1 scores for TEMPORAL and CONTINGENCY relations by approximately 1% relative to the system that uses all of the features. We found that the sentence pairs in these two types of relations have distinctive coreference patterns. We count the number of pairs of arguments that are linked by a coreference chain for each type of relation. The coreference chains used in this study are detected automatically from the training set through Stanford CoreNLP suite (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). TEMPORAL relations have a significantly higher coreferential rate than the other three relations (p &lt; 0.05, pair-wise t-test corrected for multiple comparisons). The differences between COMPARISON, CONTINGENCY, and EXPANSION, however, are not statistically significant (Figure 2). The choice to use or not to use a discourse connective is strongly motivated by linguistic features at the discourse levels (Patterson and Kehler, 2013). Additionally, it is very uncommon to have temporally-related sentences without using explicit discourse connectives. The difference in coreferen</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="8892" citStr="Lee et al., 2013" startWordPosition="1360" endWordPosition="1363"> the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 clusters were induced from</context>
<context position="28976" citStr="Lee et al., 2013" startWordPosition="4157" endWordPosition="4160">mall set compared to the other feature sets. In particular, excluding them from the model reduces F1 scores for TEMPORAL and CONTINGENCY relations by approximately 1% relative to the system that uses all of the features. We found that the sentence pairs in these two types of relations have distinctive coreference patterns. We count the number of pairs of arguments that are linked by a coreference chain for each type of relation. The coreference chains used in this study are detected automatically from the training set through Stanford CoreNLP suite (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). TEMPORAL relations have a significantly higher coreferential rate than the other three relations (p &lt; 0.05, pair-wise t-test corrected for multiple comparisons). The differences between COMPARISON, CONTINGENCY, and EXPANSION, however, are not statistically significant (Figure 2). The choice to use or not to use a discourse connective is strongly motivated by linguistic features at the discourse levels (Patterson and Kehler, 2013). Additionally, it is very uncommon to have temporally-related sentences without using explicit discourse connectives. The difference in coreference patterns might b</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation, volume 348.</title>
<date>1993</date>
<institution>University of Chicago press Chicago.</institution>
<contexts>
<context position="32976" citStr="Levin, 1993" startWordPosition="4776" endWordPosition="4777">odel parameter estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based on specific theories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations: A preliminary investigation, volume 348. University of Chicago press Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-Styled End-to-End Discourse Parser.</title>
<date>2010</date>
<tech>arXiv.org,</tech>
<contexts>
<context position="1809" citStr="Lin et al., 2010" startWordPosition="259" endWordPosition="262">ausally related (Pitler et al., 2008; Wellner et al., 2009). Discourse relations and their senses, however, can also be inferred by the reader even without discourse connectives. These implicit discourse relations in fact outnumber explicit discourse relations in naturally occurring text. Inferring types or senses of implicit discourse relations remains a key challenge in automatic discourse analysis. A discourse parser requires many subcomponents which form a long pipeline. The implicit discourse relation discovery has been shown to be the main performance bottleneck of an end-to-end parser (Lin et al., 2010). It is also central to many applications such as automatic summarization and question-answering systems. Nianwen Xue Department of Computer Science Brandeis University Waltham, MA 02453, USA xuen@brandeis.edu Existing systems, which make heavy use of word pairs, suffer from data sparsity problem as a word pair in the training data may not appear in the test data. A better representation of two adjacent sentences beyond word pairs could have a significant impact on predicting the sense of the discourse relation that holds between them. Data-driven theory-independent word classification such as</context>
</contexts>
<marker>Lin, Ng, Kan, 2010</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A PDTB-Styled End-to-End Discourse Parser. arXiv.org, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Littlestone</author>
</authors>
<title>Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.</title>
<date>1988</date>
<booktitle>Machine learning,</booktitle>
<pages>2--4</pages>
<contexts>
<context position="8091" citStr="Littlestone, 1988" startWordPosition="1235" endWordPosition="1236">lations are a pair of adjacent sentences or adjacent clauses separated by a semicolon and should be easily extracted. The PDTB corpus is split into a training set, development set, and test set the same way as in the baseline systems. Sections 2 to 20 are used to train classifiers. Sections 0–1 are used for developing feature sets and tuning models. Section 21–22 are used for testing the systems. The statistical models in the following experiments are from MALLET implementation (McCallum, 2002) and libSVM (Chang and Lin, 2011). For all five binary classification tasks, we try Balanced Winnow (Littlestone, 1988), Maximum Entropy, Naive Bayes, and Support Vector Machine. The parameters and the hyperparameters of each classifier are set to their default values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for eac</context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>Nick Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 2(4):285–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Creating local coherence: An empirical assessment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>313--316</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33811" citStr="Louis and Nenkova (2010)" startWordPosition="4907" endWordPosition="4910">rrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010). Louis et al. (2010) uses multiple aspects of coreference as features to classify implicit discourse relations without much success while suggesting many aspects that are worth exploring. In a corpus study by Louis and Nenkova (2010), coreferential rates alone cannot explain all of the relations, and more complex coreference patterns have to be considered. 8 Conclusions We present statistical classifiers for identifying senses of implicit discourse relations and introduce novel feature sets that exploit distributional similarity and coreference information. Our classifiers outperform the classifiers from previous work in all types of implicit discourse relations. Altogether these results present a stronger baseline for the future research endeavors in implicit discourse relations. In addition to enhancing the performance </context>
</contexts>
<marker>Louis, Nenkova, 2010</marker>
<rawString>Annie Louis and Ani Nenkova. 2010. Creating local coherence: An empirical assessment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 313–316. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Aravind Joshi</author>
<author>Rashmi Prasad</author>
<author>Ani Nenkova</author>
</authors>
<title>Using entity features to classify implicit discourse relations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>59--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33598" citStr="Louis et al. (2010)" startWordPosition="4873" endWordPosition="4876">ark and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010). Louis et al. (2010) uses multiple aspects of coreference as features to classify implicit discourse relations without much success while suggesting many aspects that are worth exploring. In a corpus study by Louis and Nenkova (2010), coreferential rates alone cannot explain all of the relations, and more complex coreference patterns have to be considered. 8 Conclusions We present statistical classifiers for identifying senses of implicit discourse relations and introduce novel feature sets that exploit distributional similarity and coreference information. Our classifiers outperform the classifiers from previous</context>
</contexts>
<marker>Louis, Joshi, Prasad, Nenkova, 2010</marker>
<rawString>Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani Nenkova. 2010. Using entity features to classify implicit discourse relations. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59–62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="5879" citStr="Mann and Thompson, 1988" startWordPosition="871" endWordPosition="874">ach discourse relation contains the information about the extent of the arguments, which can be a sentence, a constituent, or an incontiguous span of text. Each discourse relation is also annotated with the sense of the relation that holds between the two arguments. In the case of implicit discourse relations, where the discourse connectives are absent, the most appropriate connective is annotated. The senses are organized hierarchically. Our focus is on the top level senses because they are the four fundamental discourse relations that various discourse analytic theories seem to converge on (Mann and Thompson, 1988). The top level senses are COMPARISON, CONTINGENCY, EXPANSION, and TEMPORAL. The explicit and implicit discourse relations almost orthogonally differ in their distributions of senses (Table 1). This difference has a few implications for studying implicit discourse relations and uses of discourse connectives (Patterson and Kehler, 2013). For example, TEMPORAL relations constitute only 5% of the implicit relations but 33% of the explicit relations because they might not be as natural to create without discourse connectives. On the other hand, EXPANSION relations might be more cleanly achieved wi</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>368--375</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32322" citStr="Marcu and Echihabi, 2002" startWordPosition="4671" endWordPosition="4675">ples: (2) He also asserted that exact questions weren’t replicated. [Then] When referred to the questions that match, he said it was coincidental. (WSJ0045) (3) He also asserted that exact questions weren’t replicated. When referred to the questions that match, she said it was coincidental. When we switch out the coreferential subject for an arbitrary uncoreferential pronoun as we do in (3), we are more inclined to classify the relation as CONTINGENCY. 7 Related work Word-pair features are known to work very well in predicting senses of discourse relations in an artificially generated corpus (Marcu and Echihabi, 2002). But when used with a realistic corpus, model parameter estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based </context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 368–375. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="5252" citStr="Marcus et al., 1993" startWordPosition="771" endWordPosition="774"> in addition to using them to boost the performance of our classifier. These two sets of features along with previously used features outperform the baseline systems by approximately 5% absolute across all categories and reveal many important characteristics of implicit discourse relations. 2 Sense annotation in Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is the largest corpus richly annotated with explicit and implicit discourse relations and their senses (Prasad et al., 2008). PDTB is drawn from Wall Street Journal articles with overlapping annotations with the Penn Treebank (Marcus et al., 1993). Each discourse relation contains the information about the extent of the arguments, which can be a sentence, a constituent, or an incontiguous span of text. Each discourse relation is also annotated with the sense of the relation that holds between the two arguments. In the case of implicit discourse relations, where the discourse connectives are absent, the most appropriate connective is annotated. The senses are organized hierarchically. Our focus is on the top level senses because they are the four fundamental discourse relations that various discourse analytic theories seem to converge o</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://www.cs.umass.edu/ mccallum/mallet.</note>
<contexts>
<context position="7972" citStr="McCallum, 2002" startWordPosition="1216" endWordPosition="1218"> that the gold standard argument pairs are provided for each relation. Most argument pairs for implicit discourse relations are a pair of adjacent sentences or adjacent clauses separated by a semicolon and should be easily extracted. The PDTB corpus is split into a training set, development set, and test set the same way as in the baseline systems. Sections 2 to 20 are used to train classifiers. Sections 0–1 are used for developing feature sets and tuning models. Section 21–22 are used for testing the systems. The statistical models in the following experiments are from MALLET implementation (McCallum, 2002) and libSVM (Chang and Lin, 2011). For all five binary classification tasks, we try Balanced Winnow (Littlestone, 1988), Maximum Entropy, Naive Bayes, and Support Vector Machine. The parameters and the hyperparameters of each classifier are set to their default values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each wo</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://www.cs.umass.edu/ mccallum/mallet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Improving implicit discourse relation recognition through feature set optimization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>108--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6840" citStr="Park and Cardie (2012)" startWordPosition="1022" endWordPosition="1025">For example, TEMPORAL relations constitute only 5% of the implicit relations but 33% of the explicit relations because they might not be as natural to create without discourse connectives. On the other hand, EXPANSION relations might be more cleanly achieved without ones as indicated by its dominance in the implicit discourse relations. This imbalance in class distribution requires greater care in building statistical classifiers (Wang et al., 2012). 3 Experiment setup We followed the setup of the previous studies for a fair comparison with the two baseline systems by Pitler et al. (2009) and Park and Cardie (2012). The task is formulated as four separate one-against-all binary classification problems: one for each top level sense of implicit discourse relations. In addition, we add one more classification task with which to test the system. We merge ENTREL with EXPANSION relations to follow the setup used by the two baseline systems. An argument pair is annotated with ENTREL in PDTB if an entity-based coherence and no other type of relation can be identified between the two arguments in the pair. In this study, we assume that the gold standard argument pairs are provided for each relation. Most argumen</context>
<context position="12747" citStr="Park and Cardie (2012)" startWordPosition="1975" endWordPosition="1978"> that COMPARISON relations usually have different subjects for the same main verbs and that TEMPORAL relations usually have the same subjects but different main verbs. 4.4 Feature selection and training sample reweighting The nature of the task and the dataset poses at least two problems in creating a classifier. First, the classification task requires a large number of features, some of which are too rare and inconducive to parameter estimation. Second, the label distribution is highly imbalanced (Table 1) and this might degrade the performance of the classifiers (Japkowicz, 2000). Recently, Park and Cardie (2012) and Wang et al. (2012) addressed these problems directly by optimally select a subset of features and training samples. Unlike previous work, we do not discard any of data in the training set to balance the label distribution. Instead, we reweight the training samples in each class during parameter estimation such that the performance on the development set is maximized. In addition, the 647 P Current FI Park and Cardie (2012) Pitler et al. (2009) R FI FI COMPARISON vs others 27.34 72.41 39.70 31.32 21.96 CONTINGENCY vs others 44.52 69.96 54.42 49.82 47.13 EXPANSION vs others 59.59 85.50 70.2</context>
<context position="33000" citStr="Park and Cardie (2012)" startWordPosition="4778" endWordPosition="4781"> estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based on specific theories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010). Louis et al. (2010) u</context>
</contexts>
<marker>Park, Cardie, 2012</marker>
<rawString>Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 108–112. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Patterson</author>
<author>Andrew Kehler</author>
</authors>
<title>Predicting the presence of discourse connectives.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6216" citStr="Patterson and Kehler, 2013" startWordPosition="919" endWordPosition="922">nnectives are absent, the most appropriate connective is annotated. The senses are organized hierarchically. Our focus is on the top level senses because they are the four fundamental discourse relations that various discourse analytic theories seem to converge on (Mann and Thompson, 1988). The top level senses are COMPARISON, CONTINGENCY, EXPANSION, and TEMPORAL. The explicit and implicit discourse relations almost orthogonally differ in their distributions of senses (Table 1). This difference has a few implications for studying implicit discourse relations and uses of discourse connectives (Patterson and Kehler, 2013). For example, TEMPORAL relations constitute only 5% of the implicit relations but 33% of the explicit relations because they might not be as natural to create without discourse connectives. On the other hand, EXPANSION relations might be more cleanly achieved without ones as indicated by its dominance in the implicit discourse relations. This imbalance in class distribution requires greater care in building statistical classifiers (Wang et al., 2012). 3 Experiment setup We followed the setup of the previous studies for a fair comparison with the two baseline systems by Pitler et al. (2009) an</context>
<context position="29411" citStr="Patterson and Kehler, 2013" startWordPosition="4223" endWordPosition="4226">lation. The coreference chains used in this study are detected automatically from the training set through Stanford CoreNLP suite (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). TEMPORAL relations have a significantly higher coreferential rate than the other three relations (p &lt; 0.05, pair-wise t-test corrected for multiple comparisons). The differences between COMPARISON, CONTINGENCY, and EXPANSION, however, are not statistically significant (Figure 2). The choice to use or not to use a discourse connective is strongly motivated by linguistic features at the discourse levels (Patterson and Kehler, 2013). Additionally, it is very uncommon to have temporally-related sentences without using explicit discourse connectives. The difference in coreference patterns might be one of the factors that influence the choice of using a discourse connective to signal a TEMPORAL relation. If sentences are coreferentially linked, then it might be more natural to drop a discourse connective because the temporal ordering can be easily inferred without it. For example, (1) Her story is partly one of personal downfall. [previously] She was an unstinting teacher who won laurels and inspired students... (WSJ0044) T</context>
</contexts>
<marker>Patterson, Kehler, 2013</marker>
<rawString>Gary Patterson and Andrew Kehler. 2013. Predicting the presence of discourse connectives. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Mridhula Raghupathy</author>
<author>Hena Mehta</author>
<author>Ani Nenkova</author>
<author>Alan Lee</author>
<author>Aravind K Joshi</author>
</authors>
<title>Easily identifiable discourse relations. Technical Reports (CIS),</title>
<date>2008</date>
<pages>884</pages>
<contexts>
<context position="1228" citStr="Pitler et al., 2008" startWordPosition="170" endWordPosition="173">relations in naturally occurring text. Our system improves the baseline performance by as much as 25%. Feature analyses suggest that Brown cluster pairs and coreference patterns can reveal many key linguistic characteristics of each type of discourse relation. 1 Introduction Sentences must be pieced together logically in a discourse to form coherent text. Many discourse relations in the text are signaled explicitly through a closed set of discourse connectives. Simply disambiguating the meaning of discourse connectives can determine whether adjacent clauses are temporally or causally related (Pitler et al., 2008; Wellner et al., 2009). Discourse relations and their senses, however, can also be inferred by the reader even without discourse connectives. These implicit discourse relations in fact outnumber explicit discourse relations in naturally occurring text. Inferring types or senses of implicit discourse relations remains a key challenge in automatic discourse analysis. A discourse parser requires many subcomponents which form a long pipeline. The implicit discourse relation discovery has been shown to be the main performance bottleneck of an end-to-end parser (Lin et al., 2010). It is also centra</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind K Joshi. 2008. Easily identifiable discourse relations. Technical Reports (CIS), page 884.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>683--691</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6813" citStr="Pitler et al. (2009)" startWordPosition="1017" endWordPosition="1020">erson and Kehler, 2013). For example, TEMPORAL relations constitute only 5% of the implicit relations but 33% of the explicit relations because they might not be as natural to create without discourse connectives. On the other hand, EXPANSION relations might be more cleanly achieved without ones as indicated by its dominance in the implicit discourse relations. This imbalance in class distribution requires greater care in building statistical classifiers (Wang et al., 2012). 3 Experiment setup We followed the setup of the previous studies for a fair comparison with the two baseline systems by Pitler et al. (2009) and Park and Cardie (2012). The task is formulated as four separate one-against-all binary classification problems: one for each top level sense of implicit discourse relations. In addition, we add one more classification task with which to test the system. We merge ENTREL with EXPANSION relations to follow the setup used by the two baseline systems. An argument pair is annotated with ENTREL in PDTB if an entity-based coherence and no other type of relation can be identified between the two arguments in the pair. In this study, we assume that the gold standard argument pairs are provided for </context>
<context position="9233" citStr="Pitler et al. (2009)" startWordPosition="1411" endWordPosition="1414"> and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 clusters were induced from RCV1 corpus, which contains about 63 million tokens from Reuters English newswire. Then we take the Cartesian product of the Brown cluster assignments of the words in Arg1 and the ones of the words in Arg2. For example, suppose Arg1 has two words w1,1, w1,2, Arg2 has three words w2,1, w2,2, w2,3, and then B(.) maps a word to its Brown clu</context>
<context position="13199" citStr="Pitler et al. (2009)" startWordPosition="2050" endWordPosition="2053">ond, the label distribution is highly imbalanced (Table 1) and this might degrade the performance of the classifiers (Japkowicz, 2000). Recently, Park and Cardie (2012) and Wang et al. (2012) addressed these problems directly by optimally select a subset of features and training samples. Unlike previous work, we do not discard any of data in the training set to balance the label distribution. Instead, we reweight the training samples in each class during parameter estimation such that the performance on the development set is maximized. In addition, the 647 P Current FI Park and Cardie (2012) Pitler et al. (2009) R FI FI COMPARISON vs others 27.34 72.41 39.70 31.32 21.96 CONTINGENCY vs others 44.52 69.96 54.42 49.82 47.13 EXPANSION vs others 59.59 85.50 70.23 - - EXP+ENTREL vs others 69.26 95.92 80.44 79.22 76.42 TEMPORAL vs others 18.52 63.64 28.69 26.57 16.76 Table 2: Our classifier outperform the previous systems across all four tasks without the use of goldstandard parses and coreference resolution. COMPARISON Fl % change Feature set All features 39.70 - All excluding Brown cluster pairs 35.71 -10.05% All excluding Production rules 37.27 -6.80% All excluding First, last, and First 3 39.18 -1.40% A</context>
<context position="17928" citStr="Pitler et al., 2009" startWordPosition="2820" endWordPosition="2823">ses of discourse relations between the two arguments. Marcu et al (2002), for instance, artificially generated the implicit discourse relations and used word pair features to perform the classification tasks. Those word pair features work well in this case because their artificially generated dataset is an order of magnitude larger than PDTB. Ideally, we would want to use the word pair features instead of word cluster features if we have enough data to fit the parameters. Consequently, other less sparse handcrafted features prove to be more effective than word pair features for the PDTB data (Pitler et al., 2009). We remedy the sparsity problem by clustering the words that are distributionally similar together and greatly reduce the number of features. Since the ablation study is not fine-grained enough to spotlight the effectiveness of the individual features, we quantify the predictiveness of each feature by its mutual information. Under Naive Bayes conditional independence assumption, the mutual information between the features and the labels can be efficiently computed in a pairwise fashion. The mutual information between a binary feature XZ and class label Y is defined as: � I(XZ, Y ) = y ˆp(·) i</context>
<context position="32770" citStr="Pitler et al. (2009)" startWordPosition="4742" endWordPosition="4745">CY. 7 Related work Word-pair features are known to work very well in predicting senses of discourse relations in an artificially generated corpus (Marcu and Echihabi, 2002). But when used with a realistic corpus, model parameter estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based on specific theories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretabl</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 683–691. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In LREC.</title>
<date>2008</date>
<publisher>Citeseer.</publisher>
<contexts>
<context position="5129" citStr="Prasad et al., 2008" startWordPosition="751" endWordPosition="754">elations is imbalanced. Brown cluster pairs. We also study coreferential patterns in different types of discourse relations in addition to using them to boost the performance of our classifier. These two sets of features along with previously used features outperform the baseline systems by approximately 5% absolute across all categories and reveal many important characteristics of implicit discourse relations. 2 Sense annotation in Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is the largest corpus richly annotated with explicit and implicit discourse relations and their senses (Prasad et al., 2008). PDTB is drawn from Wall Street Journal articles with overlapping annotations with the Penn Treebank (Marcus et al., 1993). Each discourse relation contains the information about the extent of the arguments, which can be a sentence, a constituent, or an incontiguous span of text. Each discourse relation is also annotated with the sense of the relation that holds between the two arguments. In the case of implicit discourse relations, where the discourse connectives are absent, the most appropriate connective is annotated. The senses are organized hierarchically. Our focus is on the top level s</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The penn discourse treebank 2.0. In LREC. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>492--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8855" citStr="Raghunathan et al., 2010" startWordPosition="1352" endWordPosition="1355">lt values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 20</context>
<context position="28939" citStr="Raghunathan et al., 2010" startWordPosition="4149" endWordPosition="4152">ffective given that they constitute a very small set compared to the other feature sets. In particular, excluding them from the model reduces F1 scores for TEMPORAL and CONTINGENCY relations by approximately 1% relative to the system that uses all of the features. We found that the sentence pairs in these two types of relations have distinctive coreference patterns. We count the number of pairs of arguments that are linked by a coreference chain for each type of relation. The coreference chains used in this study are detected automatically from the training set through Stanford CoreNLP suite (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). TEMPORAL relations have a significantly higher coreferential rate than the other three relations (p &lt; 0.05, pair-wise t-test corrected for multiple comparisons). The differences between COMPARISON, CONTINGENCY, and EXPANSION, however, are not statistically significant (Figure 2). The choice to use or not to use a discourse connective is strongly motivated by linguistic features at the discourse levels (Patterson and Kehler, 2013). Additionally, it is very uncommon to have temporally-related sentences without using explicit discourse connectives. The diffe</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 492–501, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1524--1534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33555" citStr="Ritter et al., 2011" startWordPosition="4865" endWordPosition="4868">eories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010). Louis et al. (2010) uses multiple aspects of coreference as features to classify implicit discourse relations without much success while suggesting many aspects that are worth exploring. In a corpus study by Louis and Nenkova (2010), coreferential rates alone cannot explain all of the relations, and more complex coreference patterns have to be considered. 8 Conclusions We present statistical classifiers for identifying senses of implicit discourse relations and introduce novel feature sets that exploit distributional similarity and coreference information. Our classifie</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1524–1534. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>DM Ogilvie</author>
</authors>
<title>The general inquirer: A computer approach to content analysis.</title>
<date>1968</date>
<journal>Journal of Regional Science,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="32962" citStr="Stone et al., 1968" startWordPosition="4772" endWordPosition="4775"> realistic corpus, model parameter estimation suffers from data sparsity problem due to the small dataset size. Biran and McKeown (2013) attempts to solve this problem by aggregating word pairs and estimating weights from an unannotated corpus but only with limited success. Recent efforts have focused on introducing meaning abstraction and semantic representation between the words in the sentence pair. Pitler et al. (2009) uses external lexicons to replace the onehot word representation with semantic information such as word polarity and various verb classification based on specific theories (Stone et al., 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turia</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1968</marker>
<rawString>Philip Stone, Dexter C Dunphy, Marshall S Smith, and DM Ogilvie. 1968. The general inquirer: A computer approach to content analysis. Journal of Regional Science, 8(1).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 13,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8632" citStr="Toutanova and Manning, 2000" startWordPosition="1318" endWordPosition="1321">). For all five binary classification tasks, we try Balanced Winnow (Littlestone, 1988), Maximum Entropy, Naive Bayes, and Support Vector Machine. The parameters and the hyperparameters of each classifier are set to their default values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics-Volume 13, pages 63–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8598" citStr="Toutanova et al., 2003" startWordPosition="1313" endWordPosition="1316"> libSVM (Chang and Lin, 2011). For all five binary classification tasks, we try Balanced Winnow (Littlestone, 1988), Maximum Entropy, Naive Bayes, and Support Vector Machine. The parameters and the hyperparameters of each classifier are set to their default values. The code for our model along with the data matrices is available at github.com/attapol/ brown_coref_implicit. 4 Features Unlike the baseline systems, all of the features in the experiments use the output from automatic natural language processing tools. We use the Stanford CoreNLP suite to lemmatize and partof-speech tag each word (Toutanova et al., 2003; 646 Toutanova and Manning, 2000), obtain the phrase structure and dependency parses for each sentence (De Marneffe et al., 2006; Klein and Manning, 2003), identify all named entities (Finkel et al., 2005), and resolve coreference (Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in gre</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9458" citStr="Turian et al., 2010" startWordPosition="1447" endWordPosition="1450">than et al., 2010; Lee et al., 2011; Lee et al., 2013). 4.1 Features used in previous work The baseline features consist of the following: First, last, and first 3 words, numerical expressions, time expressions, average verb phrase length, modality, General Inquirer tags, polarity, Levin verb classes, and production rules. These features are described in greater detail by Pitler et al. (2009). 4.2 Brown cluster pair features To generate Brown cluster assignment pair features, we replace each word with its hard Brown cluster assignment. We used the Brown word clusters provided by MetaOptimize (Turian et al., 2010). 3,200 clusters were induced from RCV1 corpus, which contains about 63 million tokens from Reuters English newswire. Then we take the Cartesian product of the Brown cluster assignments of the words in Arg1 and the ones of the words in Arg2. For example, suppose Arg1 has two words w1,1, w1,2, Arg2 has three words w2,1, w2,2, w2,3, and then B(.) maps a word to its Brown cluster assignment. A word wij is replaced by its corresponding Brown cluster assignment bij = B(wij). The resulting word pair features are (b1,1, b2,1), (b1,1, b2,2), (b1,1, b2,3), (b1,2, b2,1), (b1,2, b2,2), and (b1,2, b2,3). </context>
<context position="33577" citStr="Turian et al., 2010" startWordPosition="4869" endWordPosition="4872"> 1968; Levin, 1993). Park and Cardie (2012) selects an optimal subset of these features and establishes the strongest baseline to best of our knowledge. Brown word clusters are hierarchical clusters induced by frequency of co-occurrences with other words (Brown et al., 1992). The strength of this word class induction method is that the words that are classified to the same clusters usually make an interpretable lexical class by the virtue of their distributional properties. This word representation has been used successfully to augment the performance of many NLP systems (Ritter et al., 2011; Turian et al., 2010). Louis et al. (2010) uses multiple aspects of coreference as features to classify implicit discourse relations without much success while suggesting many aspects that are worth exploring. In a corpus study by Louis and Nenkova (2010), coreferential rates alone cannot explain all of the relations, and more complex coreference patterns have to be considered. 8 Conclusions We present statistical classifiers for identifying senses of implicit discourse relations and introduce novel feature sets that exploit distributional similarity and coreference information. Our classifiers outperform the clas</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xun Wang</author>
<author>Sujian Li</author>
<author>Jiwei Li</author>
<author>Wenjie Li</author>
</authors>
<title>Implicit discourse relation recognition by selecting typical training examples.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2757--2772</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="6671" citStr="Wang et al., 2012" startWordPosition="991" endWordPosition="994">f senses (Table 1). This difference has a few implications for studying implicit discourse relations and uses of discourse connectives (Patterson and Kehler, 2013). For example, TEMPORAL relations constitute only 5% of the implicit relations but 33% of the explicit relations because they might not be as natural to create without discourse connectives. On the other hand, EXPANSION relations might be more cleanly achieved without ones as indicated by its dominance in the implicit discourse relations. This imbalance in class distribution requires greater care in building statistical classifiers (Wang et al., 2012). 3 Experiment setup We followed the setup of the previous studies for a fair comparison with the two baseline systems by Pitler et al. (2009) and Park and Cardie (2012). The task is formulated as four separate one-against-all binary classification problems: one for each top level sense of implicit discourse relations. In addition, we add one more classification task with which to test the system. We merge ENTREL with EXPANSION relations to follow the setup used by the two baseline systems. An argument pair is annotated with ENTREL in PDTB if an entity-based coherence and no other type of rela</context>
<context position="12770" citStr="Wang et al. (2012)" startWordPosition="1980" endWordPosition="1983">usually have different subjects for the same main verbs and that TEMPORAL relations usually have the same subjects but different main verbs. 4.4 Feature selection and training sample reweighting The nature of the task and the dataset poses at least two problems in creating a classifier. First, the classification task requires a large number of features, some of which are too rare and inconducive to parameter estimation. Second, the label distribution is highly imbalanced (Table 1) and this might degrade the performance of the classifiers (Japkowicz, 2000). Recently, Park and Cardie (2012) and Wang et al. (2012) addressed these problems directly by optimally select a subset of features and training samples. Unlike previous work, we do not discard any of data in the training set to balance the label distribution. Instead, we reweight the training samples in each class during parameter estimation such that the performance on the development set is maximized. In addition, the 647 P Current FI Park and Cardie (2012) Pitler et al. (2009) R FI FI COMPARISON vs others 27.34 72.41 39.70 31.32 21.96 CONTINGENCY vs others 44.52 69.96 54.42 49.82 47.13 EXPANSION vs others 59.59 85.50 70.23 - - EXP+ENTREL vs oth</context>
</contexts>
<marker>Wang, Li, Li, Li, 2012</marker>
<rawString>Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li. 2012. Implicit discourse relation recognition by selecting typical training examples. In Proceedings of COLING 2012, pages 2757–2772, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Wellner</author>
<author>James Pustejovsky</author>
<author>Catherine Havasi</author>
<author>Anna Rumshisky</author>
<author>Roser Sauri</author>
</authors>
<title>Classification of discourse coherence relations: An exploratory study using multiple knowledge sources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>117--125</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1251" citStr="Wellner et al., 2009" startWordPosition="174" endWordPosition="177">y occurring text. Our system improves the baseline performance by as much as 25%. Feature analyses suggest that Brown cluster pairs and coreference patterns can reveal many key linguistic characteristics of each type of discourse relation. 1 Introduction Sentences must be pieced together logically in a discourse to form coherent text. Many discourse relations in the text are signaled explicitly through a closed set of discourse connectives. Simply disambiguating the meaning of discourse connectives can determine whether adjacent clauses are temporally or causally related (Pitler et al., 2008; Wellner et al., 2009). Discourse relations and their senses, however, can also be inferred by the reader even without discourse connectives. These implicit discourse relations in fact outnumber explicit discourse relations in naturally occurring text. Inferring types or senses of implicit discourse relations remains a key challenge in automatic discourse analysis. A discourse parser requires many subcomponents which form a long pipeline. The implicit discourse relation discovery has been shown to be the main performance bottleneck of an end-to-end parser (Lin et al., 2010). It is also central to many applications </context>
</contexts>
<marker>Wellner, Pustejovsky, Havasi, Rumshisky, Sauri, 2009</marker>
<rawString>Ben Wellner, James Pustejovsky, Catherine Havasi, Anna Rumshisky, and Roser Sauri. 2009. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 117–125. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>