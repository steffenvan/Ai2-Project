<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.7109465">
American Journal of Computational Linguistics Microfiche 72
Ilmc THE FINITE STRING
</note>
<title confidence="0.855448">
NEWSLETTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS
</title>
<author confidence="0.806375">
VOLUME 14 - NUMBER 7 DECEMBER 1977
</author>
<note confidence="0.808948266666667">
Computational Semantics, edited by Eugene Charniak
and Yorick Wilks, Reviewed by Stuart Shapiro . 2
Introduction to Contemporary Linguistics Semantics, 22
by George L. Dillon, Reviewed by James D. McCawley
CURRENT BIBLIOGRAPHY ........... 33
BBN Publications on Intelligent CAI
95
RELEASED FOR PyBLICATION ON MARCH 3, 1978.
AMERICAN JOURNAL OF COMPUTATIONAL LINGUISTICS is published
by the Association for Computational Ltnguistics.
SECRETARY-TREASURER: Donald E. Walker, SRI International,
Menlo Park, California 94025.
EDITOR: David G. Rays, 5048 Lake Shore Road, Hamburg, New
York, 14075. ASSOCIATE EDITOR: George Heidorn,
EDITORIAL ASSISTANT: William Benzon
</note>
<author confidence="0.173083">
Copyrigis 0 1978
</author>
<affiliation confidence="0.41453">
Association for CoMiltationai Linguistics
</affiliation>
<note confidence="0.803371">
American Journal of Computational Linguistics Microfiche 72: 2
</note>
<sectionHeader confidence="0.854522" genericHeader="abstract">
C0f1PUTATIONAL SEMANTICS
AN INTRODUCTION TO ARTIFICIAL INTELLIGENCE
AND NATURAL LANGUAGE COMPREHENSION
</sectionHeader>
<author confidence="0.727606">
EDITED BY EUGENE CHARNIAK AND YORICK WILKS
</author>
<affiliation confidence="0.991078666666667">
Institute for Semantic and University of Edinburgh
Cognitive Studies Now: University of Essex
University of Geneva Colchester, Essex, England
</affiliation>
<sectionHeader confidence="0.850009" genericHeader="keywords">
NORTH-HOLLAND PUBLISHING COMPANY
</sectionHeader>
<figure confidence="0.44741225">
P. 0. Box 211 52 Vanderbilt Avenue
Amsterdam, The Netnerlands New York, New Ywk 10017
xiii + 294 pages LC 76-10133
$19.00 (Dfl 55) ISBN 0-7204-0469-X
</figure>
<author confidence="0.335336">
REVIEWED BY STUART C. SHAPIRO
</author>
<affiliation confidence="0.791288">
Computer Science Department
State University of New York
</affiliation>
<address confidence="0.419957">
Buffalo 14214
</address>
<sectionHeader confidence="0.761096" genericHeader="introduction">
IntroductiOn
</sectionHeader>
<bodyText confidence="0.9726072">
This is the first book I know of that is devoted to
teaching about computer understanding of natural language
other than research monographs or anthologies of research
papers. It derives from the lecture notes for a turorial
conference given at the Institute for Semantic and Cognitive
Studies in Switzerland in 1975. The theme is stated at the
outset,
Computational Semantics 3
&amp;quot;Computational Semantics, the name we have
given here to the study of language bared
upon Artificial Intelligence methods, there
fore approaches language by asking how
language is used in translating languages,
question answering based on language texts,
etc. This approach assumes that &apos;language
is as language does an idea not unknown to
the older disciplines of linguistics,
psychology, etc., but Computational Semantics
ig unique in making this idea the basis for
the entire study of language. It is the
contention of this approach that it is at
best ill advised, and At worst meaninglec
to talk of &apos;understanding&apos; without reference
to some task in which language is being used,
whether as narrow as sentence completion
questions in an IQ test, or as vague as
flirting at a party&amp;quot; 43.1)
The twelve chapters are written by seven different
authors. Below, each chapter is discussed separately,
after which some overall comments are given.
</bodyText>
<subsectionHeader confidence="0.50217">
Computational Semantics 4
</subsectionHeader>
<bodyText confidence="0.89655332">
Inference and Knowledge Part I by Eugene Charniak (21 pages)
In this Chapter, Charniak discusses first order
predicate calculus (FOPC), and the programming language
PLANNER.
His discussion of FOPC includes brief informal intro-
ductions to the usual syntax of classical FOPC and to the
resolution principle. It is too brief and too informal for
the reader to come to any understanding of the subject and
certainly inadequate to enable the reader to form an
opinion on the ud fulness of FOPC or resolution. Yet
Charniak states his opinion, including the patently fl Ge
notions that &amp;quot;the general idea behind FOPC is that one
only makes inferences when one is asked a question...
When you come right down to it, FOPC is primarily a theory
of inference mechanism&amp;quot;. [p.9] As Hayes [11] points out,
logic provides an analysis of meaning. It is not an
inference mechanism, rather &amp;quot;Logical meaning justifies
inferences&amp;quot;, [p.559, italics in original].
not even an infer ce machanism. It is not
Charniak does, that &amp;quot;resolution has only
inference&amp;quot; (p.8]. Resolution is a rule of inference as are
Modus Ponens and Substitution. Many different mechani ms
have ben studied for applying re lution, several of which
are discussed in (3). There are ev FOPCs which are
radically differ&apos;ent from the cla cical one that Charniak
</bodyText>
<footnote confidence="0.541678333333333">
Resolution is
correct to say,
one rule of
</footnote>
<page confidence="0.339742">
5
</page>
<bodyText confidence="0.998433333333333">
discusses. For example, in intuitionistic FOPC Av-A is not
a theorem (see [13]), and, contrary to Charniak&apos;s-statement
that &amp;quot;it is a well known ptoperty of FOPG that anything can
be proved from a contradiction&amp;quot;, in entailment logits [1]
(At A)DE. is not a theorem. It is important for computa-
tional semanticists to realize that such logics exist (see
[19]). It is also important to realize, as Charniak apparant-
ly does not, that the implementation of any inference
machanism entails an underlying logic. The implementor
should have some idea of what that logic is. Is it
consistent? If not, what is to be done when inconsistencies
become apparant? How complete is it? Is there any easily
identifiable class of questions that cannot be answered?
Charaiak discusses the useful distinctions of
inference at &amp;quot;question time&amp;quot; versus at &amp;quot;read time&amp;quot; and
&amp;quot;problem occasioned&amp;quot; versus &amp;quot;non-problem occasioned&amp;quot;
inference, but goes awry by using this discussion to fault
FOPC, which he again calls &amp;quot;a question driven system for
making inferences&amp;quot;. (p.13)
Charniak briefly describes Raphael&apos;s SIR and then
discusses PLANNER. This discussion is also brief, but well
written and should give the reader a good introduction to
the basic ideas of PLANNER. The final comparison of
PLANNER with FOPC is, of courne,, misguided since PLANNER
is a programming language while FOPC is not.
One feature of PLANNER that Charniak includes in his
example but does not discuss as much as it deserves is
THNOT. (THNOT A) succeeds if A is not in the data base
d cannot .be deduced by the invoked theorems. This is
quite different from A, which is true if A is false.
This brings up the interesting point that most quer3tion
answering systems assume a three valued logic (true, false,
neither) rather than the classical two valued logic (note
that logicians have stUdied multiple valued logics and
that intuitioni tic logic admits statue other than A or
A). It also kes one wonder what to do if after using
(MOT A) to deduce Bp A is put into the data base, a
concern Charniak does not discuns. Another incomplete
discussion concernn uhat Charniak cr.11s &amp;quot;coping with
contradiction&amp;quot; d Raphael has called the exception
principle. 115] The xrample problem is &amp;quot;all p pie have
two legs&amp;quot; but &amp;quot;Bill has one leg&amp;quot;. Charnia flays that &amp;quot;in
P NER this is done by making &apos;Bill ha one lrztge an
assertion, and &apos;All people have two leg a theor
Since the data bci3a i checked fir t wh n trying to
establish a goal, the system will find that Bill las one
leg before it attempts to use the general theorrt to show
that he has two legc;&amp;quot; [p.21) This is not coping with
</bodyText>
<page confidence="0.923691">
6
</page>
<bodyText confidence="0.943681285714286">
contradiction, it is ignoring it. What if wo then ack
&amp;quot;Who has two legs?&amp;quot; and learn via the theorem that Bill
does? Also, if we have the theorems &amp;quot;All birds fly&amp;quot; and
&amp;quot;All pengukns don&apos;t fly&amp;quot;, and the assertions &amp;quot;Billy is a
penguin&amp;quot; and &apos;Billy is a bird&amp;quot;, how are we tO know what
the oyotem will deduce about Billy&apos;s flying? We Acted a
ti4hter logical understanding of our inforonce chaniomn
than Charniak seems to think.
This reviewer&apos;s final conclusion about this chapter
is that it is a good introduction to several topico, but
it dangerously contribute,) to a misunderstanding of logic
among its intended audiences In the s ond chapter,
Charniak says, &amp;quot;AI programs are going to need knodge of
syntax, anyway, so why not use source r; hand&amp;quot;. (1).39)
</bodyText>
<page confidence="0.789423">
7
</page>
<bodyText confidence="0.741374272727273">
This chapter should have the stat ent, &amp;quot;Al progr wers
It
are going to need knowledge of logic, anyway, so why not
use sources at hand&amp;quot;.
Syntax in Linguistics by Eugene Charniak (18 pagan)
This chapter covers &amp;quot;syntax within the theory of
transformational grammar&amp;quot; as of approximately 1965, the
era of Cho ky&apos;s Aspects of the Theory of sintea [53.
It is a good introduction to the topic, including sections
on, &amp;quot;The Nature of Linguistic Arguments&amp;quot;, &amp;quot;Some Typical
Tranuformationr3&amp;quot;, &amp;quot;What is a Transformat,mal GI:Lmmar&amp;quot;,
</bodyText>
<subsectionHeader confidence="0.609191">
Computational Semantics 8
</subsectionHeader>
<bodyText confidence="0.998385375">
and &amp;quot;Reldtion of Transformational Grammar to Artificial
Intelligence&amp;quot;, in which Chardiak laudably argUes that &amp;quot;both
disciplines have something to say to each other&amp;quot; [p.363
while repeating one of the themes of the book, that &amp;quot;What
artificial intelligence does offer is the opportunity to
attack the real fundamental problem, language comprehen-
s:A, without worrying so much about sentence
grammaticality&amp;quot;. [p.401
</bodyText>
<subsectionHeader confidence="0.4718415">
Semantic Markers and Selectional Restrictions by Philip Hayes
(14 pages)
</subsectionHeader>
<bodyText confidence="0.901118571428571">
In this section, Hayes discusses the influential
semantic theory of Katz and Fodor [12] and the objections
raised to it by Dwight Bolinger [2]. He ties this to the
general tOpic with a section on the &amp;quot;Use of Semantic
Markers and Selectional Restrictions in Al&amp;quot;. Once again,
the chapter is brief, but well written and Serves to
introduce the topics.
Case Grammar by Wolfgang Samlowski (18 pages)
Samlowski begins by dismissing Fillmore&apos;s theory of
case grammar as presented in his &amp;quot;Case for Case&amp;quot; [10]. He
then discusses the case syst s used by three A.I. workers
S nons, Schank td Wilks - and compares their cases to
Fillmore&apos;s and to each oth r&apos;s. This chapter is a good
introduction to the topic.
</bodyText>
<sectionHeader confidence="0.244187" genericHeader="method">
9
</sectionHeader>
<subsectionHeader confidence="0.738101">
Generative Semantics by Margaret King (16 pageS)
</subsectionHeader>
<bodyText confidence="0.787595722222222">
&amp;quot;This chapter falls into two sections. The first
section attempts to display the Generative Semantics
position, by reporting, without much criticism or comment,
arguments which are believed by their authors to justify
that position... In the second section some aspects of
the Generative Semantics position will be considered more
critically&amp;quot; Up.73] The main criticism is that &amp;quot;the
attempt to inClude presupposition, in a very broad sense,
in a grammatical system leads to a situation where gr imar
breakS down&amp;quot;. [p.861 King&apos;s basic conclusion regarding
Generative Semantics is that, &amp;quot;while it is basically mis-
conceived as an activity within traditional linguistics,
[it] would nonetheless be a perfectly sensible activity
within the general area of work in Al&amp;quot;. 43.73]
Whether or not one would want to argue with Section II,
Section I gives a good summary of the arguments of McCawley,
Lakoff and others deriving the Generative Semantics position
from Chomsky&apos;s &amp;quot;Aspects&amp;quot; model.
</bodyText>
<subsectionHeader confidence="0.7481715">
Parsing English I by Yorick Wilks (12 pages)
This chapter only minimally fulfills the promise of its
</subsectionHeader>
<bodyText confidence="0.999048333333333">
title, probably due to Wilks&apos; conviction that &amp;quot;gr tical
(or syntactic) parsing of the sort described is not funda-
mental, and that it need not be even a preliminary to
</bodyText>
<page confidence="0.392403">
10
</page>
<bodyText confidence="0.98638572">
assigning a useful meaning structure to sentences&amp;quot;. (p.921
Wilks presents a six rule, four lexeme, non-recursive,
context free grammar and uses it to do a top-down and a
bottom-lp parse of &amp;quot;The dog likes the cat&amp;quot; (one of the
four sentences in the language of the grammar). He also
mentOts lexical ambiguity and breadth-first versus depth-
first parsing.
Wilks apends about half the chapter discussing
Winograd&apos;s SHRDLU. While this may be appropriate, Wilks,
beaAuse of his bias, fuzzes the distinction betwe
Winograd&apos;s parser (&amp;quot;The parser is an interpreter !Ihich
accepts recognition grammars written in a procedural form.
The formalism is a language called PROG R&amp;quot;. [20.1).3])
and his blocks world robot sy tem. Wilks clrim3 that,
&amp;quot;Indeed, it might be argued that, in a sense, and as regards
its semantics, Winograd&apos;s system is not about natural
language at all, but about the other technical questions of
how goals and sub-goals are to be org ized in a probl
solving system capable of manipulating simple physical
objects&amp;quot;. [p.9.9, italics in originali While that may be
true about the robot system, Winograd&apos;s own distinction
betwez parsing and problem solving (&amp;quot;Even though we used the
robot syst as our test area, the language progrcm do not
dep d on any special subject matter, and they have been
adapted to other uses&amp;quot;. (20,p.2)) should not be ignored,
</bodyText>
<page confidence="0.849164">
11
</page>
<bodyText confidence="0.998027181818182">
especially in a chapter o.) parsing. The same mistake
is made with respect to Woods&apos; Augmented Transition Network
(ATN) Grammars. While it is true that, &amp;quot;both Woods and
Winograd have argued in print that their two systems are
essentially equivalent&amp;quot; [p.99, italics in original], and
this equivalence is accepted by the Al community, the
systems that iire equivalent are the parsing systems not the
robot system and the lunar rocks qbestion-answering system
[21]. Thus it is quite wrong, relative to what should be
discussed in this chapter, to say that &amp;quot;both are grammar-
based deductive systems, operating within a question-
</bodyText>
<subsectionHeader confidence="0.52744">
answering environment in a highly limited domair of
</subsectionHeader>
<bodyText confidence="0.995124666666667">
discourse&amp;quot;. [p.99] It is also incorrect to say about the
parsers that &amp;quot;there is no need to discuss both, and
Winograd&apos;s is, within the Al community at least, the better
</bodyText>
<subsectionHeader confidence="0.7941762">
known of the two&amp;quot;. [p.99] Indeed, the large majority of
Al language understanding systems use ATN grammars, and the
absence of a discussion of them is one of the greatest
shortcomings of this book.
Semantig,Nets as Memory Models by Greg Scragg (27 pages)
</subsectionHeader>
<bodyText confidence="0.970658555555556">
14 this chapter, Scragg introduces and discusses
semantic networks from those with arc labels such as LIKES,
HIT and HAP (has-as-part), which should probably just bc
termed &amp;quot;relational&amp;quot; networks, to those with case relations
as- arcs, to discussions of quantification and of procedures
Computational Semantics 1 2
in semantic networks. He also takes a few pages each to
discuss Schank&apos;s and Simmons&apos; networks.
Scragg rightly discusses the difference between
individuals and classes (he uses the terms &amp;quot;tokens&amp;quot; and
&amp;quot;types&amp;quot;), and the importance of distinguishing set member-
ship from subset, a surprisingly often neglected and
cOnfused point. However, he continUes a closely related
confusion by using the same relation, HAP, between tokens
(a token of GIRL and a token of HAIR) as between types (the
types BIRD and WING). This is incorrect because the inter-
pretation of x HAP y cannot be consistent. In some cases,
it is &amp;quot;x has y as part&amp;quot;, and in others it must be &amp;quot;each x
has a (distinct) y as part&amp;quot;.
Scragg gives some insight into the data structures
for implementing semantic networks and so encourages the
reader to lop beyond the usual pictorial representation.
This is important when comparing different network formalisms.
For example, as Scragg points out, Schank&apos;s pictures look
radically different from any other semantic network pictures,
yet the ctual data structures are very similar.
Inference and Knowledge Part 2 by Eugene Charniak (26 pages)
</bodyText>
<footnote confidence="0.828016333333333">
Charniak begins this chapter by discussing the demon
based system of (4]. He criticizes this approach and also
PLANNER (demons are PLANNER antecedent theorems) because
</footnote>
<page confidence="0.817879">
13
</page>
<bodyText confidence="0.981154621052632">
of the fixed direction of excitation. For example, if the
possibility of rain activates the umbrella demon, how do we
understand &amp;quot;Jack began to worry when he realized that every-
one on the street was carrying an umbrella&amp;quot; [p.136]? The
basic problem seems to be that PLANNER requires one to
distinguish one of the propositions of an inference rule
as a pattern, burying the others inside the theorem. It
does not allow all of the propositions to be treated as
patterns, as necessary (as is allowed by the semantic
network deduction rules of [18]).
Next Charniak discusses McDermott&apos;s TOPLE [14]. The
interesting features discussed are TOPLE&apos;s sets of possible
worlds and its performing inferences in order to supply
support for believing new inputs. He also discusses Rieger&apos;s
inference program [16], concentrating on Rieger&apos;s belief in
massive read time inferencing and his classification of
sixteen types of inferences.
Finally, Charniak discusses the influential, though
controversial theory of frames. He cdmpares frames to
demons and finds frames preferable.
Parsing English_II by Yorick Wilks (30 pages)
This chapter is a continuation of Parsing English I.
Apparently, they were originally written as one chapter,
then separated for no obvious reason. In this chapter, Wilks
14.
warns that, &amp;quot;&apos;parsing&apos; is being used not only in its
standard sense in mathematical and computational
linguistics&amp;quot; [p.155], but includes building some meaning
structure representation of the surface language. Th4is
kind of parsing Wilks definitely favors: &amp;quot;The thesis
behind this chapter ... is that parsing is essential to
a system... The argument is not only that parsing
provides a test of a proposed structure, for that is
secondary, but that the parsing procedures define what
the significance of the proposed structure is&amp;quot; [p.179,
italics in original].
Wilks first discusses three &amp;quot;second generation&amp;quot;
parsing systems. The keyword and pattern parser of Colby
et al.&apos;s PARRY [6;7;9], Wilks&apos; own preferential semantics
system based on &amp;quot;formulas&amp;quot;, &amp;quot;templates&amp;quot; &amp;quot;paraplates&amp;quot; And
&amp;quot;inference rules&amp;quot;, and Riesbeck&apos;s parser for the MARGIE
system [17]. About tep pages are spent on Wilks&apos; system,
more than any other system discussed in the book.
Wilks considers all the systems discussed in this
chapter to use &amp;quot;frames&amp;quot;. He feels, &amp;quot;the key point about
any structures that are to be called frame-like is that
they attempt to specify in advance what is going to be
said, and how the world encountered is going to be
organized... In psychological ahd visual trms, frame
approaches envisage an understander as at least as much a
looker as seer&amp;quot; [p.156]. He distinguishes between &amp;quot;small
15
scAlu&amp;quot; and &amp;quot;large scale&amp;quot; frames (large scale frames are
what are commonly referred to as frames or &amp;quot;scripts&amp;quot;), and
has some scepticism about large scale frames, &amp;quot;It is not
being argued here that large scale frames have no
function, only that, as regards concrete problems of
language understanding, their function has not yet been
made explicit&amp;quot; (p 183).
The chapter ends with a nine page comparison of systems
cm ten different dimensions: level of representation;
centrality of information; the phenomen6logical level of
inferences; decoupling of parsing and inference; exhibition
of 1,4-mner of applicLUon to input texts; amount of formrd
inferencing; odularity; scale of representation; connection
with real world procedures; justification of adequacy.
Rashology_of_banguage and Memory by Walter F. Bischof
(19 pages)
&amp;quot;The intent of this chapter is twofold: first it should
provide the non-psychologist with some basic concepts and
soma important experimental findings in the field of
psycholinguistics and the psychology of m ory. The second
goal is to take a close loOk at the nature of psychological
arguments and psychological evidence&amp;quot; fp-.185). This is
done from an admitedly biased point of view: &amp;quot;the topics
chosen for review were chosen more because of their
popularity in Al than because of their relative importance in
Computational Semantics 16
psychology&amp;quot; [194]. The topics include association theory,
experiments designed to test the psychological reality of
phrase tructure and transformational rules, memory of
meaning versus memory of syntax, short term and long term
memory, the Collins and Quillian model (81 of hierarchical
memory organization. Although the discussions are brief,
the choices are good for the intended audience.
Bischof is not only selective in his view of
psychology, but highly skeptical: &amp;quot;the student of Al
should be able to see from the discussion here that the
ability of psychology to design and carry out experiments
uhich will give clear and indisputable resUlts is very
limited, and that their ability to provide a safe and clear
insight into human language understanding is similarly
limited&amp;quot; (p.194]. Therefore, &amp;quot;Al is well advised not to
over-estimate the importance of psychological arguments&amp;quot;
tp.201].
</bodyText>
<subsectionHeader confidence="0.837901">
nilasoplaalf_ETITAise by Yorick Wilks (29 pages)
</subsectionHeader>
<bodyText confidence="0.997091133333333">
In this chapter, Wilks is mainly interested in
discussing and comparing the work of Richard Montague
and Ludwig Wittgenstein. &amp;quot;These philosophers have been
chosen not so much for their influence on our subject
matter, which has been small, but because their views are
diametrically opposed on the key issue of formalization&amp;quot;
[p.205]. His belief is &amp;quot;that the influence of Wittgenstein
hag been largely beneficial while that of Montague has
been largely malign&amp;quot; [p.205].
First Wilks introduces some basic topics from the
work of Leibniz, Frege, Russell, early Wittgenstein, Carnap,
and Tarski. As in the previous chapter, the discussions
are brief but the topics well chosen. His introduction
to Montague io via the analysis of the sentence &amp;quot;Every
man loves some woman&amp;quot;. Montague is always difficult, but
Wilks&apos; presentation is relatively easy to follow and is
done without introducing the lambda calculus. His
discussion of Wittgenstein is intended, &amp;quot;simply to give
a flavor, to those unfamiliar with him, of what Wittgenstein
has to offer&amp;quot; [p.2221 1-1;ct style is to cover eight topics
by presenting for each a thesis, some quotes from Wittgenstein
and some comments. The overall impression is that Wilks
feels that Wittgenstein is relevant to Al workers and that
Wittgenstein basically supports the view of language under-
standing put forth by this book.
An Introduction to Programming in LISP by Margaret King and
Philip Hayes
(47 pages)
This chapter is precisely what its title suggests - an
introduction to LISP for either the non-programmer o the
</bodyText>
<page confidence="0.738131">
17
</page>
<bodyText confidence="0.955379913043478">
non-LISPing programmer. It takes a modern approach
(dotted pairs are not mentioned) and uses examples to
which the book&apos;s audience will be able to relate. It
al-o has a good set of exercises at the end of each
section with solutions at the end of the chapter. Of
necessity it moves fast, skimming through many topics
including PROGs, property lists, mapping functions and
FEXPRs. This sheds doubt on -how much of a LISPer the
naive reader will become after working through the
chapter. Like much of the book, all the right topics
are covered; but briefly.
Conclusions
If one were to design a course on natural langu go
understanding by computer, and list all the topics that
should be covered, one would find that almost all were
included in this book, ATN grammars being one notable
exception. However they re al covered only briefly
and from a vo y definite point of view. I have just
finished giving such a course using this book, additional
readings, and my own &amp;quot;corrective&amp;quot; vicupoint. The students
felt that the book&apos;s discussion of each topic was too
brief to be self-contained unless they already knew
something about it.
</bodyText>
<page confidence="0.876624">
18
</page>
<sectionHeader confidence="0.948162" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.661227913043478">
1. Anderson, A.R., and Belnap, N.D., Jr # Entailment: The
Log:11.c of Relevance and Necessity, Princeton University
Press, Princeton, NJ, 1975.
2. Bolinger, D. The atomization of meaning, LanapasenLq,
(1965), 555-5731.
3. Chang, C.-L., and Lee, R.C.T.
Mechanical Theorem Proving, Academic Press, New York,
1973.
4. Charniak, E. Toward a model of children&apos;s story
comprehension TR-266, MIT A.I. Lab.., Cambridge,
MA, 1972.
5, Chomsky, N. Aspects of the Theory of Syntax, MIT Press,
Cambridge, MA, 1965.
6. Colby, K.M., et al. Artificial paranoia. Artificial
Intelligence 2, (1971), 1-25.
7. Colby, K.M., and Parkinson, R.C. Pattern-matching rules
for the recognition of natural language dialogue
expressions. AJcL Microfiche 5, 1974.
8. Collins, A.M., and Quillian, M.R. Retrieval time, from
semantic memory. J. Verbal Learning and Verbal Behavior
8, (1969), 240-247.
9. Enea, H., and Colby, K.M. Idiolectic language analysis
for understanding doctor-patient dialogues. &apos;Proc. Third
</reference>
<figure confidence="0.6457115">
19
20
</figure>
<reference confidence="0.977885173913044">
IJCAI, SRI, Menlo Park, CA, 1973.
10. Fillmore, C.J. The case for case. In E. Bach and
R.T. Harms (Eds.), Universals in_Liaguistic Theory,
Holt, Rinehart and Winston, New York, 1968.
11, Hayes&apos;, P.J. In defence of logic, Proc. IJCAI-77, MIT,
Cambridge MA, 1977, 559-565.
12. Katz, J., and Fodor, J.A. The structure of a semantic
theory, Language 39, (1963), 170-210. Also in J.A.
Fodor and J.J. Katz (Eds.) The Structure of Language,
Prentice-Hall, Englewood Cliffs, NJ, 1964, 479-518.
13. Kleene, S.C. Introduction to Metamathematics, D. Van
Nostrand Co., Princeton, NJ, 1950.
14. McDermott, D.V. Assimilation of new information by a
natural language understanding system. TR-291, MIT
A.I. Lab., Cambridge, MA, 1974.
15. Raphael, B. SIR: a computer program for semantic
information retrieval. In M. Minsky (Ed.). Semantic
Information Processina, MIT Press, Cambridge, MA, 33-145.
16. Reiger, C. Conceptual Memory. Unpublished Ph.D. Thesis,
Stanford U., 1974.
17. Riesbeck, C.K. Conceptual analysis. In R. Schank.
Conceptual Information Processing, North-Holland,
sterd 1975, 83-156.
</reference>
<table confidence="0.940594269230769">
Computational Semantics 21
18. Shapiro, S.C. Representing and locating deduction rules
in a semantic network. Proc Workshop on Pattern&apos;
Directed Inference Systegis, SIGART Newsletter, 63
(June, 1977), 14-18s
19. Shapiro, S.C., and Wand, M. The relevance of relevance,
Technical Report No. 46, Computer Science Dept.,
Indiana U., Bloomington, IN, 1976.
20. Winograd, T. Understanding Natural Language. Academic
Press, New York, 1972.
21. Woods, W.A.; Kaplan, R.M.; Nash-Webber, B. The lunar
sciences natural language info nation system: final
report, Report 2378, Bolt Beranek and Neman,
Camitridge, MA, 1972.
American Journal of Computational Lin uistics Microfiche 72: 22
INTRODUCTION TO CONTEMPORARY
LINGUISTIC SEMANTICS
GEORGE L. DILLON
Indiana University -- Purdue University
Fort Wayne
PRENTICE-HALL, INC.
Englewood Cliffs, New jersey 07632
xx + 150 pages 1977 LC 76-41853
$6 95 paper ISBN 0-13-479469-9
REVIEWED BY JAMES D. MCCAWLEY
University of Chicago
</table>
<bodyText confidence="0.99519675">
This book reminds me of the 1969 Chicago Cubs who held a
commanding lead in the National League Until early September but
went into a tailspin and finished far behind the New York Mets,
The first five chapters of the book, in which Dillon goes
through a lot of lexical semantics, introduces many basic
notions of semantics, and discusses the relationship of
semantics to morphology, to extralinguistic knowledge, and to
metaphor, are excellent: he gives a clear, coherent, and
accurate presentation of a well-selected and quite broad-ranging
series &apos;3f topics. However, the sixth chapttr, dealing witn formal
logic, io so thoroughly bungled that I find it hard to believe
that it could have been written by the same person that produced
</bodyText>
<figure confidence="0.240178666666667">
23
the preceding chapters.
I should mention at the outset that I have an indirect
personal connection with this book, namely that I was the first
person (Dillon t8 the third) who was eng ed to write it; It
withdrew from the project a year after my deadline; with only
17 pages of manuscript written. Dillon&apos;s book is in fact not
all that different from what-I might haw) written if 1_ had becl-a
able to get v act together in about 197). (Dillon, I should:
point out, did not see my aborted -manuscript, not that it wouldt
have done him any good). Indeed, many of the bungles in chapter
6 for whA611 I chide Dillon can be attested in my own works Qf
</figure>
<bodyText confidence="0.82217152173913">
the late 1960&apos;s, which Dillon drew on in preparing chapter 6.
The fact that I will devote more space here to saying what is
wrong with chapter 6 than what is good about chapters 1-5 and 7
should not be taken as implying that I think the faults of the
one bad chapter outweigh the virtues of the other six. In fact,
the book could be used quite profitably in a course on semantics
or tn an introductory linguistic course, provided that the
instructor either has his students skip chapter 6 entirely or
has them read a good deal of supplementary material on the
topics covered. there.
The principal tailings of chapter 6 are a failure to
separate the many distinct questions that arise, systematic
errors in the employment of standard logical formalisms, bizarre
English paraphrases of logical formulas, and implausible claims
24
about the teanings of the English sentences used as examples.
Dillon appears to accept thp policy known as &apos;unrestricted
quantification&apos;, in which all variables range over the entire
universe of discourser, and the noun of the quantified NP is fit
into the logical structure by using a connective (;) in the
case of the universal quantifier, &amp; in the case of the
existential quantifier) to combine it With the &apos;matrix&apos; proposi-
tional function, e.g.
</bodyText>
<figure confidence="0.787068">
(1) a (vjx)(Philosopher x _7) Dangerous x)
&apos;All philosophers are dangerous&apos;
b. (3x)( Linguist x &amp; Qbnoxious x)
&apos;Some linguists are cbnoxious&apos;
</figure>
<bodyText confidence="0.7672488">
This policy is to be contrasted with an alternative not
considered by Dillon, the so-called &apos;restricted quantification&apos;,
in which each variable ranges over a restricted domain, given by
the noun of the quantified NP„ with a logical constituent
structure as in (2):
</bodyText>
<listItem confidence="0.9880245">
a. (ix: Philosopher x)(Dangerous x)
b. (ax: Linguist x)(0bnoxious x)
</listItem>
<bodyText confidence="0.991651419354839">
See McCa-Aley 1972 for arguments in favor of restricted quantif-
ication as part of a system of logic that can optimally be
integrated with natural language syntax and linguistic semantics.
For one thing, unrestricted quantifiers are hopeless as a basis
for the analysis of quantifiers other than the logicians&apos;
favorite ones: any viable analysis of most, many, few, or almost
25
all would have to involve some version of restricted quantifica-
tion.
When Dillon says (p.rn 95) &amp;quot;Generic sentences, though the
quantifier is less than universal, have also been sydlielized
with an entailmept&amp;quot;1, he conflates the issue of how genericity
Ais related to quantification and the issue of how quantifiers
fit into logical structure. He ought to have discussed the latter
issue long before he took up generics, so that he could deal
with the matters that are peculiar to generics ainst a back-
ground of clear alternatives for the treatment of those things
to which generics might be related. His elevation of an sea-
lary issue to central status here is made particularly glaring
by the fact that his brief discussion of generics comes in his
section on &amp;quot;connectives&amp;quot; rather than the one on quantifiers.
Dillon would have been wiser to do chapter 6 in to iS of
restricted quantifiers rather than unrestricted quantifiers,
since he would then have been able to avoid mechanical diffi-
culties that are inherent in the use of unrestricted quantifi-
ers and which he himself has not mastered. Specifically, in
representing the meanings of complex sentences, Dillon never
gets the material corresponding to the nouns in the right place.
For example, in representing the meaning of (3a) he gives the
nearly tautologous (3h) inbtead of the standard (and plausible)
(30:;
</bodyText>
<subsectionHeader confidence="0.2288">
Linguitics Semantics 26
</subsectionHeader>
<listItem confidence="0.837065666666667">
(3) a. Each boy kissed a girl.
b. (qx)(3Y)((B0Y(x) &amp; GIRL(y) —* KISS(x,y)))
c. (Vx)(BOY(x) (3y)(GIRI4(y) &amp; KISS(x,y)))
</listItem>
<bodyText confidence="0.8625615625">
According to the standard truth conditions, (3b) is true under
virtually all circumstances, namely any circumstances under
which there -1-e entities that are not girls (the number 10 is
not a girl, therefore (BOY(x) &amp; GIRL(10)) is false whatever x
is, whether a boy or a beanbag, and thus for any value of x
there is a value of y, namely 101 that makes the conditional in
(3b) true). The fact that Dillon allowed formulas like (3b) to
appear in his book can be attributed in part to his lack of
attention to the giving of rules for the relationship between
logical structure and surface structure. The most obvious rules
for mapping unrestricted quantifier formulas onto surface struc—
tures would cover the caue of (3c) but not that of (3b), since
(3c) but not (3h) conSists of structures like (lab) embedded in
one another. The very first example that he gives of an analysis
involving quantifiers is an analysis of (4a) as (4b) rather than
as
</bodyText>
<listItem confidence="0.913383666666667">
(4) a. A boy kissed a girl.
b. (3x)(3y)((B0X(x) &amp;A.IRL(y)) &amp; KISS(x,y)))
c. (3x)(BOY(x) &amp; (3y)(GIRL(y) &amp; KISS(x,y))
</listItem>
<table confidence="0.767938733333333">
While (4h) and (40) have exactly the same truth conditions, it
is (4c) that fits the general rules associating unrestricted
quantifier formulas with surface structrxes containing quantified
27
NP&apos;s. Had his discussion of (4a) been preceded by the material
that would need to be covered to make that fact obvious, Dillon
could scarcely have made such blunders as (3b).
One recurring distUrbing, feature of Dillon&apos;s informal
glosses to his logical formulas is his highly unidiomatic use Of
the word one, as when he glosses (4h) as &apos;There exists one that
is a boy and there exists One that is a girl such that he kissed
her&apos;.2 This same odd locution also occurs in earlier chapters,
as in his discussion of the semantics of Adjective + Noun
combinations (p. 62). where he glosses large car as &apos;one that is
a car with size greater than average size of cars&apos; and joint
</table>
<bodyText confidence="0.906229">
undertaking as &apos;one that is an action that is undertaken jointly&apos;.
In his Adjective + Noun glosses, the one is in fact superfluous
(e.g. one could omit &apos;one that is from the last two glosses and
from the other glosses in that section); however, the one&apos;s
would presumably reappear if Dillon were to employ consistentlj
the analysis of relative clauses as derived from coordinate
structures that figures in chapter 6 and were to stick to the
style of glosses that he uses for (4b). He in fact introduces
this odd use of one along with his very first example of a
logical formula, namely an analysis of (5a) as (5b), for which
he gives the gloss (50:
</bodyText>
<listItem confidence="0.926058">
(5) a. John kissed Mary.
b. ((JOHN(x)) 8c OARYCyD &amp; (KISSED(x,y)))
c. One is called John and one is called Mary and he kissed
</listItem>
<page confidence="0.772306">
28
</page>
<bodyText confidence="0.99787484">
her.
If the one&apos;s are interpreted literally, (5c) expre&apos;sses a rather
bizarre proposition that has no relctionship to (5a). A signif-
iant.improvement could be obtained by replacing both occur
ences of one by someone, but then the gloss would correspond to
a formula involving two existenti.ol quantifiers, and Dillon
makes clear later that he does not want to include any quanti-
fiers in the analysis of (5a). In fact, Dillon&apos;s use of one is
only an obscure way of repeating the very thing that it is
supposed to &apos;explain&apos;, namely the index x or y, and his glosses
would have been much clearer if he had sjmply written X&apos;s and
y&apos;s in them.
Dillon&apos;s discussion of (5h) is unclear and/or inconsistent
as to whether the x and y are constant&apos;s or variables. The
absence of a quantifier and the contrast timt he draws (p. 87)
between (5h) and structures involving quantifiers suggest that
they are constants. However, his tabulation of &amp;quot;seven distingu-
ishable situations&amp;quot; that the negation of (5h) is compatible with
consists of glosses that would make littlu sense if x and y were
constants, e.g. &amp;quot;One kissed one called Mary, but he wasn&apos;t
called John&amp;quot;: from the fact that Larry kissed Mary and Larry
isn&apos;t called John, it does not follow that John didn&apos;t kiss Mary
(perhaps they both kissed Mary). The different situations that
he is distinguishing really involve a third index, one corres-
ponding to the event of kissing. Some of the seven situations in
</bodyText>
<subsectionHeader confidence="0.984093">
Linguistic Semantics 29
</subsectionHeader>
<bodyText confidence="0.99943972">
ne list involve a presupposition that an act of kissing took
place and give specifications of who the participants in, that
act mere; others do not involve such a presupposition. There is
in fact no proposition of which the seven situations can all be
taten as illustrating the negation.
I turn now to chapters 1-5 and 7, which I rate in general
as very cell done. In these chapters Dillon discusses componen-
tirl analysis, productive and non-productive word-formation,
:1 metaphor in terms of a large number of well-chosen examples.
I find only one really major gap in the set of topics covered,
ely presupposition. While the term &apos;presupposition&apos; ()tours in
seve 1 places in the text, Dillon contents h elf with merely
advising the reader that the term has been used in a number of
differont senses and is involved in ongoing controversies, and
fers the reader to supplementary readings for further enlight-
ent. The various notions to which that term has been applied
tro of sufficient importance in linguistic semantics and are of
levance to so many of the examples and theoretical points that
be takes up that Dillon can hardly expect instructors using his
book to avoid serious disdussion of it.
Dillon&apos;sidiscussion of metaphor is distinguished by its use
of a large body of interesting examples and its avoidance of the
hackneyed examples (such as He danced his did, which is not a
metaphor at all) that usually figure so prominently in linguist
discussion of metaphor. However, in this pnerally enlightened
</bodyText>
<subsectionHeader confidence="0.232442">
30
</subsectionHeader>
<bodyText confidence="0.9834844">
presentation there are two errors for which I wish to take
Dillon to task. First, he speaks of metaphor as resting on &apos;some
sort of incompatibility between the usual senses of the word and
the context&apos; (p. 39). But if metaphoric uses were restricted to
which a literal use was incompatible, an express-
ion could never be ambiguouu as to whether it is to be inter-
preted literally or metaphorically, Whereas in fact such ambi-
guity is quite common (Reddy 1969). Indeed, as Ted Cohen
(lecture at University &apos;ofChicago) points out, there are meta-
phoric sentences that express truisms when interpreted literally
(e..7. No man is an island). Secondly, the adjustment involved in
interpreting a metaphor is not (ag Dillon as it is) the can-
cellation of semantic features but the assignment of a non-
standard referent, for example, I disagree with Dillon&apos;s state
ment that the interpretation of morsel in &amp;quot;Broad-fronted Caesar
/ When thou wast here about the ground, I was / A morsel for a
monarch&amp;quot; (Antony and Cleopatra I.v.29-31) involves &amp;quot;suppressing
the SMALL BIT OF FOOD component in favor of the associated
DELICACY component&amp;quot; (p. 40): Cleopatra here is speaking of her-
self as a snack for the emperor, and the reader transfers the
reference (though not the sense) of the &amp;quot;EATING&amp;quot; or &amp;quot;CONSUMING&amp;quot;
component to a different medium rather than simply suppressing
it (see again Reddy 1969).
Useful problems are given at the e0 of each chapter, with
suggested answers given at the end of the book. There is also a
</bodyText>
<page confidence="0.76069">
31
</page>
<bodyText confidence="0.971453111111111">
glossary that is quite useful though flawed in some respscts,
chiefly incompleteness, as whe Dillon declines to even hint at
a definition of &apos;presupposition&apos; and simply refers the reader to
other literature, and when he defines &apos;achievement verb&apos; (the
term is from Vendler 1957) by telling the reader everything that
it isn&apos;t and leaving the reader to dotermine by elimination what
it is. The definition of &apos;connectix contains a statement that
confuses an important issue:, &amp;quot;Connectives are held to be
predicates by some, but not, by logicians, because predicates
combine with arguments to form propositions, but connectives
combine propositions to form larger propositions&amp;quot; (p. 124).
Dillon has,: given no reason for supposing that propositions can
not be arguments of predicates and thus for not taking connect-
ives to be simply a special kind of predicates. Indeed, &apos;not to
allow propositions to serve as arguments is to commit oneneli to
the schizophrenic position (Prior 1971) tliat verbs such as plow
and believe are &amp;quot;predicates on the left and connectives on the
right&amp;quot;.
</bodyText>
<sectionHeader confidence="0.750765" genericHeader="method">
FOOTNOTES
1 Dillon consistently refers to his conditional connective as
</sectionHeader>
<bodyText confidence="0.7391924">
&amp;quot;entailment&amp;quot; but usually uses it in a way that would only make
sense if it were a material conditional rather than entailment.
He does not advise the reader of the distinction betwecn the
truth-functional material conditional connective that figures in
most logic texts and. the relation (not really a &amp;quot;connective&amp;quot;) of
</bodyText>
<subsectionHeader confidence="0.632899">
Linguistic Semantics 32
</subsectionHeader>
<bodyText confidence="0.768342">
entailment (A entails H if B is true in all states of affairs in
which A is true).
2 This gloss would in fact be more appropriate for (40) than for
(0), though the use Of one would be no more idiomatic.
</bodyText>
<sectionHeader confidence="0.955539" genericHeader="method">
REFERACES
</sectionHeader>
<bodyText confidence="0.678008">
McCawley, James D. 1972. A program for logic. In D. Davidson and
G. Harman (ed8.), Semantics of natural language (Dordrecht:
Reidel), 498-544.
Prior, A. N. 1971. Ob ects of thou ht. Oxford: Clarendon.
</bodyText>
<figureCaption confidence="0.4017452">
Reddy, Michael. 1969. Metaphor and reference. 2240112.1112_1122
FifthRte.i.:_ond.leetirelz, Chicago Linguistic Society.
Vendler, Zeno. 1957. Verbs and Times. 2221121.322.41221jeview 6&apos;
143-160. Also in Z. Vendler, Lin tics in Philos° h
(ItARca: Cornell University Press, 1967), 97421.
</figureCaption>
<table confidence="0.858419692307692">
American Journal of Computational Linguistics Microfiche 72: 33
CURRENT BIBLIOGRAPHY
GENERAL 34 COMPUTATION 74
PHILOSOPHICAL FOUNDATrON 34 INFERENCE . . . • 76
SPEECH UNDERSTANDING. . 36 PROGRAMMING 77
ARTIFICIAL INTELLIGENCE . 38 LANGUAGES 77
CONFERENCE-WORKSHOPS. . 40 INFORMATION STRUCTURES, ▪ 80
JOURNAL 44 DOCUMENTATION
PHONETICS-PHONOLOGY . . 0 0 46 ABSTRACTING &amp; INDEXING. . 84
PHONOLOGY 46 RETRIEVAL 85
RECOGNITION: PROSODY 47 TRANSLATION 86
WRITING SOCIAL-BEHAVIORAL SCIENCE • 86
RECOGNITION: HAND PSYCHOLOGY 87
</table>
<figure confidence="0.840387333333333">
WRITTEN CHAACTERS. • 0 0 47 LEARNING-ACQUISITION. • 90
LEXICOGRAPHY-LEXICOLOGY . . 48 POLITICAL SCIENCE . . . 91
DICTIONARY. . .. 49 HUMANITIES. . . . 92
TEXT HANDLING 49 ROBOTICS .. . . 93
0 0
GRAMMAR
PARSER 51
GENERATOR . . .
0 . . . 53
</figure>
<equation confidence="0.491312833333333">
CLASSES &amp; CONSTRUCTIONS . 54
SEMANTICS-DISOOURSE . . 0
THEORY 56
COMPREHENSION 61
SYSTEM 62
CONCEPTUAL DEPENDENCY 64
MEMORY 65
CONCEPTUAL DEPENDENCY 66
EXPRESSION. . 0 0 • 0 67
LINGUISTICS 68
METHODS: MATHEMATICAL. 68
LOGIC 71
</equation>
<bodyText confidence="0.798867833333333">
BRAIN THEORY. . 94
staff thanks Martin and Iris
Kay, and the Xerox Palo Alto
Research Center for their aid
in preparing this bibliography.
. 55 The production and editorial
</bodyText>
<note confidence="0.6775815">
GENERAL 34
The LIFER Manual: A Guide to Building Practical Natural Language Interfaces
</note>
<table confidence="0.866709">
Gary G. Hendrix
Artificial Intelligence Center, Stanford Research Institute, Menlo Park, CA 94025
Technical Note 138, 68 pp , February 1977
</table>
<tableCaption confidence="0.979416666666667">
LIFER is a practical system for creating English language interfaces to other computer
software and is composed of two main parts: a set of interactive language specification
functions and a parser. In standard practice, an interface builder uses the language
specification functions to define an application language (an appropriate subset of a NL).
Using this language specification, the LIFER parser can translate NL inputs into appropriate
interactions with the application software. Topics: A) The LIFER approach to language, B)
Specifying a language definition, C) Using the parser, D) The ellipsis feature, E) Spelling
correction and other error messages, F) Initial control characters, G) Auxiliary features, H)
Implementing pronouns, I) Implementation.
</tableCaption>
<sectionHeader confidence="0.777154" genericHeader="method">
GENERAL
</sectionHeader>
<subsectionHeader confidence="0.7865555">
Computer Construction of Crossword Puzzles Using Precedence Relationships
Lawrence J. Mazlack
</subsectionHeader>
<bodyText confidence="0.890703375">
Computing and Information Science, University of Guelph, Ontario, Canada
Artificial Intelligence 7: 1-19, Spring 1976
After an unsuccetsful attempt to construct puzzles by whole insertion of words, puzzles were
successfully constructed by a letter by letter method. Usually when a word was validly
formed by the letter by letter puzzle constructor it could remain permanently in the
constructed puzzle. A dynamic, heuristically determined, decision structure was required. The
constructor resolved questions of letter selection, ordering and reordering of the solution
sequence, dictionary structure-.and access, and decision path selection.
</bodyText>
<sectionHeader confidence="0.972347" genericHeader="method">
GENERAL: PHILOSOPHICAL FOUNDATIONS
</sectionHeader>
<subsectionHeader confidence="0.989744">
Artificial Intelligence, Language, and the Study of Knowledge
</subsectionHeader>
<author confidence="0.350905">
Ira Goldstein, and Seymour Papert
</author>
<affiliation confidence="0.510836">
Massachusetts institute of Technology, Cambridge, MA 02139
</affiliation>
<subsubsectionHeader confidence="0.373342">
Cognitive Science 1: 84-123, January 1977
</subsubsectionHeader>
<bodyText confidence="0.996727083333333">
Recent work in AI suggests that intelligence is based on the ability to use large amounts of
dherse kinds of knowledge in procedural ways (e.g. frames, scripts), rather than on the
possession of a few general and uniform principles (e.g, heuristic search). Within this general
framework a fundamental contribution of Al to epistemology is clear: the systematic
introduction of attive agents into epistemological theory construction so that items of
knowledge are active agents. Other contributions of Al include concepts of system &amp;quot;elf-
knowledge (the system&apos;s ability to observe its behavior, and to make use of those observations,
even to the point of learning to debug faults in its procedures) and the development of a
variety of control structures (e.g. ATNs). Finally, the paper considers ways in which Al may
have a radical impact on education if the principles which it utilizes to explore the
representation and use of knowledge are made available to the student to use in his own
learning experiences.
</bodyText>
<page confidence="0.834889">
35
</page>
<table confidence="0.340294">
GENERAL: PHILOSOPHICAL FOUNDATIONS
The Past, Present, and Future of Computational Linguistics
David G. Hays
Department of Linguistics, SUNY at Buffalo, Amherst, NY 14260
Papers in Computational Linguistics, Akademial Kiado, Budapest: 583-585, 1977?
</table>
<bodyText confidence="0.8514438">
The field of computational linguistics is part of the slowly unfolding-revolution in man&apos;s way
of thinking about thought itself, a revolution spurred and supported by the computer, As
revolutionarieS we have to do without the guidelines that tradition furnishes others; but the
luckiest scientists are those like ourselves who, being revolutionaries, have the beg chance to
make big contributions.
</bodyText>
<sectionHeader confidence="0.9137" genericHeader="method">
GENERAL: PHILOSQPHICAL FOUNDATIONS
</sectionHeader>
<subsectionHeader confidence="0.51930975">
The Field and Scope of Computational Linguistics
David G. Hays
Department of Linguistics, SUNY at Buffalo, Amherst, NY 14260
Papers in Computational Linguistics, Akudemiai Kraft°, Budapest: 21-25, 1977?
</subsectionHeader>
<bodyText confidence="0.998748">
A rich theory of information can be developed only in terms of the classes of machines that
are involved in processing it. But a theory of machines can be a competence theory or a
performance theory. I propose a four-way schema with psychology, computation, formal
linguistics, and descriptive linguistics at the poles. Psychol4y and computation are about
performance; formal sciences are abstract, and psychology and descriptive linguistics are
sciences. Psycholinguistics joins psychology with linguistics. Correspondngly, on the abstract
side computational linguistics joins computation with formal linguistics and also seems a
fruitful area. The most likely place to arrive at a working idea of how competence and
performance - algorithms and information - are related is computational linguistics. The
more we achieve on the formal, abstract side, the better the chance of formulating goals and
criteria for linguistics that will help the linguist decide whether a grammatical invention
merits prolonged study.
</bodyText>
<note confidence="0.652173">
GENERAL: PHILOSOPHICAL FOUNDATIONS 36
</note>
<title confidence="0.253201">
Computational Linguistics and the Design of Control Systems
</title>
<author confidence="0.350637">
David G. Hays
</author>
<affiliation confidence="0.536939">
Department of Linguistics, State University of New York at Buffalo, Amherst, NY, 14260
</affiliation>
<subsubsectionHeader confidence="0.73469">
Science and Culture 28: 1426-1432, December 1976 (Sciencia e Cultura)
</subsubsectionHeader>
<bodyText confidence="0.990459153846154">
Five sections: Language, Computation, Computational Linguistics, The Architecture of
Correspondence, Control systems. The traditional branches of mathematics analyze structures
in which time may play a role, but the analysis does not employ time. Algorithm theory uses
time as an analytic variable. Computational linguistics is the algorithmic analysis of
language. Control systems operate in time; they receive information about a process as it
occurs and return information that influences its continuation. The study of control systems,
cybernetics, must therefore, like computational linguistics, use time as an analytic variable.
Language is a vehicle for both social and personal control. Computational linguistics must
provide the theory with which to understand linguistic control processes. The *dal qualities
of language are needed in many control systems. Computational linguistics now has models
for recursive systems in which recursive components must be correlated and may therefore
serve usefully as a model for use by the manager dealing with markets within markets and
technologies within technologies.
</bodyText>
<sectionHeader confidence="0.740164" genericHeader="method">
GENERAL: SPEECH UNDERSTANDING
</sectionHeader>
<subsectionHeader confidence="0.771536">
Chapter IV: Experimental Studies
</subsectionHeader>
<author confidence="0.49148">
William H. Paxton
</author>
<affiliation confidence="0.566801">
Stanford Research Institute, Menlo Park, CA 94025
</affiliation>
<subsubsectionHeader confidence="0.4694045">
Speech Understandng Research. Final Technical Report, 15 October 1975 - 14 October 1976,
1V-1 to 1V-56, October 1976
</subsubsectionHeader>
<bodyText confidence="0.94337">
The first experiment concerned the acoustic processor (the mapper) and the second concerned
&apos;fanout&apos; - the number of alternatives at each word, both for the language alone and in
combination with the acoustics. The third experiment studied the effects of four control-
strategy design choices. Focus by inhibition and island driving had bad effects, while context
checks for priority setting had good effects. Mapping all at once had good effects on
everything except acoustic and total runtime, and these bad effects could probably be
eliminated by redesign of the mapper. The fourth experiment varied the size of allowed gaps
and overlaps between words and showed the potential value of special acoustics tests to verify
word-pair junctions. A fifth experiment concerned the effects of increased vozNubulary and
improved acoustic accuracy while a final study concerned detailed measurements of the
Executive performance and provided insights into the use of time and storage and the kinds
of errors made by the system.
</bodyText>
<page confidence="0.889239">
37
</page>
<note confidence="0.658256">
GENERAL: SPEECH UNDERSTANDING
Chapter I: Introduction
Ann E. Robinson, Donald E. Walker, William H. Paxton, and Jane J. Robliison
Stanford Research Institute, Menlo Park, CA 94025
</note>
<construct confidence="0.3161235">
Speech Understanding&apos; Research. Final Technical Report, 15 October 1975 - 14 October
1976, 1-1 to 1-28, October 1976
</construct>
<bodyText confidence="0.993192">
The system data base contains characteristics such as owner, builder, size, and speed for
several hundred ships in U.S., Soviet, and British fleets. The user can get information from
the system by simple English questions, commands, and dialog sequences using incomplete
sentences and pronouns. After a brief overview (covering the components developed by SDC,
the language definition, syntax, semantics, discourse, deduction, generation, and the executive)
an example of the system&apos;s operation is given and the paper closes with an historical
perspective on the system&apos;s development,
</bodyText>
<sectionHeader confidence="0.881619" genericHeader="method">
3ENERAL: SPEECH UNDERSTANDING
</sectionHeader>
<subsectionHeader confidence="0.5005186">
Speech Understandng Systems: Final Report, November 1974 - October
1976 Volume I: Introduction and Overview
William A. Wcods, Madeleine Bates, Geoffrey Brown, Bertram C. Bruce, Craig C. Cook, John
W. tilovstad, John I. Makhoul, Bonnie L. Nash-Webber, Richard M. Schwartz, Jared J. Wolf,
and Victor W. Zue
</subsectionHeader>
<bodyText confidence="0.714921588235294">
Bolt Beranek and Ne4rnian Inc. 50 Moulton Si., Cambridge, MA 02138
Report No. 3438, 82 pp., December 1976
The system arrives at a believable coherent theory that can account for all the stimuli by
Successive refinement and extension of partial theories until a best complete theory is found.
Predictions are handled with monitors, which are dormant processing requests passively
waiting for expected constituents, and proposals, which are elementary hypotheses that are to
be evaluated against the input. When a monitor is triggered an e..ent is created calling for
the evaluation of a new hypothesis. As a number of events are likely to be competing for
processin at any moment a great deal of attention has been given to the problem of
assigning them priorities. The HWIM system consists of several components coordinated by a
control strategy using: 1) a Left-Hybrid policy to constrain the formation of seed events and
their eitension, 2) a Verify-at-Pick method of employing the Verification component, and
3) a Shortfall Density method of computing priority scores for ordering the event cue. On a
final test involving 124 utterances (3 male speakers) 44% of the utterances were correctly
understood (though the system was not fully debugged nor finely tuned at the time).
Appendices include a sample set of sentence types and a sample tine of an utterance being
processed.
</bodyText>
<page confidence="0.374627">
38
</page>
<table confidence="0.3728692">
GENERAL: ARTIFICIAL INTELLIGENCE
Research at Yale in Natural Language Processing 1976
Roger C. Sehank
Department of Computer Science, Yale University, New Haven, Connecticut
Research &apos;Report No. 84, 30 pp, 1976
</table>
<tableCaption confidence="0.817968555555556">
1. SAM - A script-based story understanding procram. 2. FRUMP - A fast program designed
to skim a newspaper looking for events in which it is interested. 3. PAM - A; plan b &apos;
program designed to understand stories that call upon general knowledge of human goals and
relationships. 4. TALESPIN - A program intended to make stories to tell in an interactive
mode. 5. WEIS/POLITICS - This is a program designed to read newspaper headlines and do
two possible things: 1) It codes the sentences into a political coriding scheme used by political
scientists; 2) It simulatesi a person with an ideological belief system being informed of the
event in the headlines. The program is then capable of answering questions based oft its
belief system about appropriate responses of the U.S. to the new events.
</tableCaption>
<sectionHeader confidence="0.664414" genericHeader="method">
GENERAL: ARTIFICIAL INTELLIGENCE
</sectionHeader>
<subsectionHeader confidence="0.558386666666667">
Methodological Questions About Artificial Intelligence: Approaches to
Understanding Natural Language
Yorick Wilks
</subsectionHeader>
<affiliation confidence="0.8653365">
Umtersitv of Essex, Colchester
Journal of Pragmatics I: 69-84, /977
</affiliation>
<construct confidence="0.5935396">
I. Al research on natural language is best regarded as an engineering activity rather than a
scientific one; hence attempts to justify Al research on NL by appeal to the methods of the
sciences are in general misguided. II. Semantic primitives cannot be justified in the way that
theoretical objects in the sciences (such as neutrinos) are. Such primitives are not essentially
different from the surface words whose meanings they are used to express.
</construct>
<page confidence="0.672844">
39
</page>
<table confidence="0.818527833333333">
GENERAL: ARTIFICIAL INTELLIGENCE
Directed Recursive Labe!node Hypergraphs: A New Representation
_anguage
Harold Boley
Institut fuer Informatik, Universitaet Hamburg, Schlueter.str. 70, 2000 Hamburg 13
Artificial Intelligence 9: 49-85, August 1977
</table>
<tableCaption confidence="0.97051725">
Directed recursive labelnode hypergraphs (DRLHs) combine 3 generalizations of directed
labeled graphs: 1) Hypergraphs which have hyperarcs connecting 1,2,3 . , . n nodes. 2)
Recursive graphs which, in addition to atomic nodes, have complex nodes, i.e. other recursive
graphs that may have contact nodes. 3) Labe&apos;node graphs which, instead of distinguishing
labels and nodes, have only one set of labelnodes, each of whose members may function as a
label and/or as a node. DRLHs are discussed in relation to the predicate calculus, relational
DBs (Codd), LISP, simple semantic networks, active structural networks (LNR group). The
analysis of NL strings into DRLHs ana their processing is done with pattern-matching rules.
</tableCaption>
<table confidence="0.5899344">
GENERAL: ARTIFICIAL INTELLIGENCE
Artificial Intelligence - A Personal View
D. Marr
Artificial lntellience Laboratory, MIT, 545 Technology Sq., Cambridge, MA 02139
Artificial Intelligence 9: 37-48, August 1977
</table>
<bodyText confidence="0.976617">
A theory of computation for a given problem is concerned with what is to be computed and
win, while an implementation algorithm concerns how to do it.. There are likely to be many
algorithms for a given computation, with choice of one over others being highly-dependent
on hardware considerations. But once a computational theory has been established for a
domain it need never be done again. A problem which can be decomposed into a
computational theory plus algorithmic implementation is a Type I theory. Some problems,
especially those involving simultaneous action of a considerable number of processes, may not
have a Type 1 theory. These will require a Type 2 theory. i&apos;Aost Al programs have been
Type 1 tieory or not, and Type 2 theories may obscure the correct Type 1 decomposition of
the problem. Topics discussed include Schank&apos;s CD, Newell and Simon&apos;s production systems,
Norman and Rumelhart&apos;s active Structural network, Al vision work. It is slit lested that NL
may have no Type 1 decomposition.
</bodyText>
<figure confidence="0.718417384615385">
40
GENERAL: CONFERENCE-WORKSHOPS
Computational and Mathematical Linguistics: Proceedings of the International
Conference on Computational Linguistics
Florence: Leo S. Olschki Editore, 793 pp. in 2 Volumes, 1977
CONTENTS
Prefaces
B. Vauquois XI
H. Karlgren XIII
Committees XV
In
A. Zampolli XIX
Opening addresses XXIX
</figure>
<sectionHeader confidence="0.66071" genericHeader="method">
STUDY OF FORMAL PROPERTIES
</sectionHeader>
<reference confidence="0.94514805">
V. B. Borscev, M. V. Chomjakov, Neighbourhood Des:riptiOn of
Formal Languages 3
W. Brecht, Morphological Analysis (A Formal Approach) 9
J. P. Descles, Un modele mathematique d&apos;analyse transformationnelle
selon Z. S. Harris 23
* F. G. Pagan, Constructible Representations for Two Semantic
Relations 29
E. Pause, A Class of Transformational Recognition Grammars 45
S. K. Saumjan, P. A. S)boleva, Formal Metalanguage and Formal
Theory as Two Aspects of Generative Grammar 63
J. T. Wang, On Logical Formulation of the Computation Process
in Semantical Systems 87
TESTING AND SIMULATION
I. Baton, Working with the Interactive Version of the T. G. T.-System
of Joyce Friedman 103
S. Boisvert, A. Dugas, D. Belanger, Obling: a Tester for Transformational
Grammars 121
A. Riha, S. Machova, Computer Testing of a Generative Grammar 143
N.D. Andreyev. Algorithmisation of linguistic research using the
structural-probabilistic properties of language units 155
</reference>
<note confidence="0.484063">
GENERAL CONFERENCE-WORKSHOPS 41
E. Campanile, A. Zampolli Problems in Computerized Historical
Linguistics: the Old Cornish Lexicon 161
R. M. Frumkina, P. F. Andrukovich, A. Yu. Terekhina, Computational
Methods in the Analysis of Verbal Behaviour 171
</note>
<table confidence="0.92630564516129">
G. Gonenc, Unique Decipherability of Codes with Constraints with
Application to Syllabification of Turkish Words 183
A. S. Liberman, Towards a Phonological Algorithm 195
G. J. van der Steen„k Treatment of Independent Semantic
Components 201
B. V. Sukhotin, Deciphering Methods as a Means of Linguistic
Research 209
A. Tretiakoff, Results Obtained with a New Method for the Automatic
Analysis of Sentence Structures 215
V. Zivov, Une Procedure de Classification des Consonnes Destinee a
la Description de leurs Combinaisons 235
LEXICOLOGY
G. Ferrai, Dictionnaire automatique et dictionnaire-machine: une
hypothese 257
J. Hewson, Reconstructing Prehistoric Languages on the Computer
the Triumph of the Electronic Neogrammarian 263
J. Mathias, Cooperative File Improvement and Use of a Computei
based Chinese/English Dictionary 275
T. M. Paikeday, The American Heritage Intermediate Corpus 281
F. Papp, Automatic Analysis of Hungarian Texts and Linguistic Data 287
G. Potvin, Travaux de mise au point crun lexique en vue de la transcription francaisk.
automatique on semi-automatique du texte grec d&apos;Aristote 293
GENERAL: CONFERENCE-WORKSHOPS 42
TABLE OF CONTENTS VII
I. Prodanof, A la recherche d&apos;un modole de derivation en italien 297
R. N. Smith, E. Maxwell, An English Dictionary for Computerized
Syntactic and Semantic Processing Systems 303
A. Stachowitz, Beyond the Feasibility Study: Lexicographic Progress 323
M. T. Wilton, Bilingual Lexicography: Computer-aided Editing 337
G. R. Wood, Refinements in Tabular Models of Variation in Regional
American English 343
</table>
<sectionHeader confidence="0.327494" genericHeader="method">
TEXT CORPUS EDITING
</sectionHeader>
<reference confidence="0.820465375">
D&apos;A. S. Avalle, La formalisation des graphes cans le domaine de la rime 351
P. Baldacci, B. Cavagnola, 0 Ianovitz, E. Maretti, G. Masperi, P. 0, Michelotto,
G. P. Zarri, Polemon I: A program of automatic construction of indexes for the V
volume of Corpus litscriptionun Latinanim 365
G. Buccellati, The Old Babylonian Linguistic Analysis Project
Goals, Procedures ar.d First Results 385
0. Rola, Y. Karjalainen, Syntax Archives of Finnish Dialects for Computer Work . 405
R. Leonard, The Computer Archive of Modern English Texts 417
C. Saporetti, Le Projet Altan (&amp;quot;Analisi Linguistica dei testi Assiri e Nuziana&amp;quot;) 429
SEMANTICAL CAICULUS
N. G. Arsentjeva, From Semantic Features of a Word to its Surface
Syntactic Features 433
C. Fuchs, J. Rcuault, Etude formelle de l&apos;opposition situation/propriete
et des phenomenes de voix et de thematisation 445
T. Furugori, An Encyclopedic View of Understanding Natural Language . . . . . . 471
GENERAL: CONFERENCE-WORKSHOPS
VIII TABLE OF =TENTS
E. Ginsburg, The Mechnnics of the Inner Form of the Derivative
Word and its Description in the Two-level theory of Language 479
N. M. Goldman, Sentence Paraphrasing from a Conceptual Base 481
E. K. Gousseva, Operational Semantic Models in Soviet Linguistics 509
E. Hajicova, P. Sgall, A Semantic Representation of Topic and Focus 515
D. G. Hays, Types of Processes on Cognitive Networks 523
B. Henisz-Dostert, F. B. Thompson, The REL System and REL English 533
*S. Klein, Automatic Inference of Semantic Deep Structure Rules in
Generative Semantic Grammars 557
E. V. Paduceva, Modalities in Russian and in a Language of
557
S. IL Petrick, Semantic Interpretation in the Request System 585
*B. Phillips, Topic Analysis 611
*C. K. Riesbeck, Expectation as a Basic Mechanism of Langu fa Comprehension • 627
R. C. Schank, C. J. Rieger III, Inference and Conceptual Memory 653
</reference>
<table confidence="0.52579185">
L L Soya, Some Theoretical Problems of Computational Linguistics 695
L I. Vasilevskaya, The Genotype Language as a Means of Sematic °Representation . . . 705
D. E. Walker, Speech Understanding, Computational Linguistics,
and Artificial Intelligence 725
Y. Wilks, A. Herskovits, An Intelligent Analyser and Generator
for Natural Language 741
W. k Woods, Meaning and Machines 769
Author Index 793
*Abstracts of these papers appeared on AJCL Microfiche 1.
43
Semantic Representations
GENERAL: JOURNAL 44
Statistical Methods in Linguistics, 1976
SIIILNA
CONTENTS
THE NEED FOR A FRAME SEMANTICS WITHIN LINGUISTICS 5
Charles J. Fillmore
*QUESTION AND ANSWER IN LINGUISTICS AND IN
MAN-MACHINE COMMUNICATION (48-49) 30
Eva HajiCOVa
</table>
<sectionHeader confidence="0.708731" genericHeader="method">
SOME PROBLEMS OF INFERENCING: RELATION OF INFERENCING
TO DECOMPOSITION OF PREDICATES (38-41) 47
Aravind K. Joshi and Stanley J. Rosenschein
&apos;OBSERVATION ON CONTEXT FREE PA kSING (58-63) 71
B. A. Shell
TOWARDS A MODEL OF LANGUAGE PRODUCTION: LINGUISTIC
AND COMPUTATIONAL FOUNDATION 110
Henry Thompson
1UTOMATIC TRANSLATION - A SURVEY OF DIFFERENT APPROACHES . . . 127
13. Vauquois
</sectionHeader>
<reference confidence="0.9502705">
SOME RECENT CONTRIBUTIONS TO STATISTICAL LINGUISTICS 135
Charles Muller
*Abstracts of these articles have appeared previously in AJCL, Fiche 48. The specific frame
numbers are indicated in parentheses after the title. The other articles are abstracted
elsewhere on this fiche.
GENERAL: JOURNAL 45
The Prague Bulletin of Mathematical Linguistics 26
Universita Karlova, Praha, 1976
CONTENTS
1. Semantische Netze und selektive Beziehungen I, J. Kunze 3
2. Meaning of Sign, Cognitive Content, and Pragmatics, P. Sgal1 51
GENERAL: JOURNAL
The Prague Bulletin of Mathematical Linguistics 26
Universira Karlova, Praha 1976
CONTENTS
1. 0 znachenii rabot U. Revzina v oblasti teoretiko-mozhestvenoi
kontseptsii iazyka, O. G. Rev zina, 1. A. Shreider 3
2. Semantische Netze und Oektive Beziehungen II, J. Kunze 11
3. 0 rnodeli estestvennogo iazyka, osnovannoi na poniatii peremennykh okruzhenii,
J. S. Bien 41
4. On Some Relationships of Linguistics and Information Retrieval,
P. Sga11 51
PHONETICS-PHONOLOGY 46
Speech Understanding Systems: Final Report, Movembei 1974 - October
1976 Volume II: Acoustic Front End
William A. Woods, et al.
Bolt Berairiek and Newman Inc., 50 Moulton St., Cambridge, 111A 02138
Report 3438, 9/ pp., December 1Q76
</reference>
<bodyText confidence="0.996748076923077">
The initial signal processing component computes the following types of parameters: 1) zero
crossings, 2) LP analysis (in the 0-5 kHz region), 3) spectral energy, 4) formant extraction,
5) fundamental frequency. Acoustic-Phonetic Recognition (APR) consists of:
SEGMENTATION, which employs a segment lattice to handle alternative segmentations,
LABELING, which arrives at a rough phonetic characterization of each segment, and
SCORING, to determine a score for the correspondence of each phoneme possibility for each
segment. The speech synthesis-by-rule progrsm is used for response generation and, more
importantly, for word verification. The phonological component makes use of syntactic and
lexical information (and, potentially, semantic information) and outputs to the phonetic
component. The verification component contairs an &apos;analysis-by-synthesis procedure to
overcome inaccuracies present in preliminary phonetic analysis and to take account of the
effects of the phonological rules. Appendices: Dictionary Phonemes, APR labels, APR rules,
Parameter; for Scoring.
</bodyText>
<sectionHeader confidence="0.994296" genericHeader="method">
PHONETICS-PHONOLOGY PHONOLOGY
</sectionHeader>
<subsectionHeader confidence="0.677905333333333">
Allophonic Variations of Stop Consonants in a Speech Synthesis-by-Rule
Prograrn
W. A. Ainsworth, and J. B. Millar
</subsectionHeader>
<affiliation confidence="0.686897">
Department of Communication, University of Ktcle, Keele, Staffs., U.K.
</affiliation>
<subsubsectionHeader confidence="0.539795">
Internationaljournal of if an-Machine Studies 8: 159-168, March 1976
</subsubsectionHeader>
<bodyText confidence="0.998722">
A computer program for syntImizing speech by rule from phonetic data has been modified
so that the rules for generating stop consonants (/b, d, g, p, t, k/) depend on context.
Listening tests have shown that this expedient can increase the intelligibility of stop
consonants in isolated CV syllables from 68% to 92%, with about 3 allophones per consonant
being required to achieve this l!vel of performance.
</bodyText>
<reference confidence="0.3027682">
PHONETICS-PHONOLOGY: RECOGNITION: PROSODY 47
WHY &amp;quot;41/book&amp;quot;?
Bengt Orestrom
Survey of Spoken English, University of Lund, Helgonabacken 14, S-223 62 Lund, Sweden
SSE, 13 pp., October 1976
</reference>
<bodyText confidence="0.985294142857143">
It seems easiest to explain the use of /411 before a consonant as a hesitation phenomenon.
Unlike the regular, hesitational long AV, the short /A/ does not appear to be immediately
due to lexical selection (groping, for the right word) but is rather sparked off occasionally in
a hesitational surroundng. However, a broader study is needed of the whole discourse
situation to specify more exactly what constitutes a hesitational surrounding. Moreover, there
are probably other factors than lexical selection and planning that might cause hesitation,
such a frequent interruptions by another speaker.
</bodyText>
<sectionHeader confidence="0.811176" genericHeader="method">
WRITING: RECOGNITION: HANDWRITTEN CHARACTERS
</sectionHeader>
<reference confidence="0.81236475">
Using Knowledge in the Computer Interpretation of Handwritten FORTRAN
coding Sheets
R. Bornat, and J. M. Brody
Computing Centre, University of Essex, Colchester, U. K.
</reference>
<subsectionHeader confidence="0.960754">
International Journal of Man-Machine Studies 8: 13-27, January 19./6
</subsectionHeader>
<bodyText confidence="0.9998965">
It is argued that, in order to construct a computer program which can successfully read
casually hand-printed FORTRAN coding sheets, it is necessary to use processes which
explicitly incorporate consistency, checks based on our knowledge of how handprinted
characters must be constructed, how rORTRAN statements must be written and of how
statements must be put together to form a program. That is, we propose to treat FORTRAN
coding sheets as the object &amp;quot;world&amp;quot; of an Al program. This paper gives a brief description
and justification of the proposed methodo;ogy, including an argument that this approach Will
not lead to a combinatorial explosion of search time.
</bodyText>
<note confidence="0.628338">
LEXICOGRAPHY-LEXICOLOGY 48
</note>
<title confidence="0.853218">
A Scrabble Crossword Game Playing Program
</title>
<author confidence="0.893404">
Stuart C. Shapiro
</author>
<affiliation confidence="0.929158">
Department of Computer Science, SUNY at Buffalo, Amherst, NY 14226
</affiliation>
<author confidence="0.954127">
Howard R. Smith
</author>
<affiliation confidence="0.981325">
Department of Computer Sciences, The University of Texas at Austin, 78712
Department of Computer Science, SUNY at Buffalo, Technical Report 119, March 21, 1977
</affiliation>
<bodyText confidence="0.9988409">
A program has been designed and implemented in SIMULA 67 on a DECSystem-10 to play
the SCRABBLE Crossword Game interactively against a human opponent The heart of the
design is the data stiucture for the lexicon and the algorithm for searching it The lexicon is
represented as a letter table, or tree using canonical ordering of the letters in the words rather
than the .original spelling. The algorithm takes the tree and a collection of letters, including
blanks, and in a single backtrack search of the tree finds all words that can be formed from
any combination and permutations of the letters. Words using the higher valued letters are
found before words not using those letters, and words using a collection of letters are found
before words using a sub-collection of them. The Search procedure detaches after each group
of words is found and may be resumed if more words are desired.
</bodyText>
<sectionHeader confidence="0.779946" genericHeader="method">
LEXICOGRAPHY-LEXICOLOGY
</sectionHeader>
<reference confidence="0.973712055555556">
Speech Understanding Systems Final Report, November 1974 - October
1976 Volume 111: Lexicon, Lexical Retrieval and Control
William A. Woods, et al.
Bolt Beranek and Newman Inc., 50 Moulton Street, Cambridge, MA 02138
Report No. 3438, 110 pp., December 1976
Topics: Dictionary, Phonological rules, Dictionary expansions, Lexical retrieval, Control
strategy, Performance. The Lexical Retrieval component determines the n most probable word
matches in, a full lexicon or appropriate subset and operates on a phonetic segment lattice.
Words can &amp;quot;oe matched left-to-right and right-to-left. Control strategy options are governed
by 25 flags. Each strategy performs an initial scan of some region of the utterance, creating
one-word seed events. In &amp;quot;middle-cut&amp;quot; strategies the initial scan is done over the entire
utterance. In L-R strategies the initial scan only considers words that could begin the
utterance. In &amp;quot;hybrid&amp;quot; strategies the initial scan fixes op an initial portion of the utterance
and then middle-out analysis is done on this region with the remainder necessarily being
analyzed L-R. In all these op:ions events are ordered on the queue by their priority scores.
Appendices: Annotated phonological. rules, Format and examples Of dictionary files, Result
summaries for each token, Performance results for strategy variations, BIGDICT and
TRAVELDICT listings, Dictionary expansion - a user&apos;s guide.
</reference>
<page confidence="0.968069">
49
</page>
<sectionHeader confidence="0.897005" genericHeader="method">
LEXICOGRAPHY-LEXICOLOGY: DICTIONARY
</sectionHeader>
<reference confidence="0.760804">
Computers and the Production of Systematic Terminological Glossaries
M. L. Hann
UM1ST, Manchester, England
Bulletin of the Association for Literary and Linguistic Computing 5: 26-37, 1977
</reference>
<bodyText confidence="0.993459875">
It would be useful if specialized terminological glossaries were organized in a way which
reveals the conceptual structure of the domain glossed rather than being organized
alphabetically. For the purpose of formal description a terminological relation can be
considered as consisting, of two component relations: a logical and an ont6logical relation.
Logical relations are abstract relations and as such are expressible in yams of the usual set
theoretic predicates, whereas ontological relations are concrete and must be defined in terms
of reP1 world criteria. The basic notion is elaborated and then ilustrated through application
to the domain of computing terminology.
</bodyText>
<sectionHeader confidence="0.970344" genericHeader="method">
LEXICOGRAPHY-LEXICOLOGY: TEXT HANDLING
</sectionHeader>
<reference confidence="0.62547296">
SITAR: Interactive Text PrL)cessing System for Small Computers
Ben Ross Schneider, Jr.
English Department, Lawrence University, Appleton, WI 54911
Reid M. Watts
University of Kansas
Communications of the ACM 20: 495-499, July 1977
SITAR, a low-cost interactive text handling and text analysis system for nontechnical users, is
in many ways comparable .to interactive bibliographic search arid retrieval systems, but has
several additions features. It is implemented on a PDP/11 time-sharing computer invoked by
a CRT with microprogrammed editing functions. It uses a simple command language
designating a function, a file, and a search template consisting of the textual string desired
and strings delimiting the context in which the hit is to be delivered.
LEXICOGRAPHY=LEXICOLOGYI TEXT HANDLING 50
Manual for Terminal Input of Spoken English Material
Bengt Orestrom, Jan Svartvik,, and Cecilia Thavenius
Survey of Spoken English, University of Lund, Helgonabacken 14, S-223 62 Lund, Sweden
SSE, 23 pp., October 1976
The basic material of the Survey of Spoken English (SSE) at Lund University consists of the
spoken English. at the Survey of English material collected and transcribed at the Survey of
English Usage (SEU) under the direction of Profess&amp; Randolph Quirk, University College
London. The SSE material differs from the original SEU material in two respects: for
linguistic reasons (explained in the manual) it has been pruned of certain prosodic and
paralinguistic categories; and for technical reasons the prosodic and phonetic symbols have
been changed. The manual lists the symbol set used for SSE and code set, peripheral
equipment, and SSE procedures have been described in appendices.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.023162">
<note confidence="0.602857769230769">Journal of Computational Linguistics 72 Ilmc THE FINITE STRING NEWSLETTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS 14 - Computational Semantics, edited by Eugene Charniak and Yorick Wilks, Reviewed by Stuart Shapiro . 2 Contemporary Linguistics Semantics, by George L. Dillon, Reviewed by James D. McCawley 22 CURRENT BIBLIOGRAPHY ........... 33 BBN Publications on Intelligent CAI 95 PyBLICATION ON 3, 1978. AMERICAN JOURNAL OF COMPUTATIONAL LINGUISTICS is published by the Association for Computational Ltnguistics.</note>
<affiliation confidence="0.976554">SECRETARY-TREASURER: Donald E. Walker, SRI International,</affiliation>
<address confidence="0.922201666666667">Menlo Park, California 94025. EDITOR: David G. Rays, 5048 Lake Shore Road, Hamburg, New 14075. ASSOCIATE EDITOR: George</address>
<author confidence="0.547367">EDITORIAL ASSISTANT William Benzon</author>
<email confidence="0.295901">for</email>
<note confidence="0.8136795">Journal of Computational Linguistics 72: C0f1PUTATIONAL SEMANTICS</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A R Anderson</author>
<author>N D Belnap</author>
</authors>
<title>Jr # Entailment: The Log:11.c of Relevance and Necessity,</title>
<date>1975</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ,</location>
<marker>Anderson, Belnap, 1975</marker>
<rawString>1. Anderson, A.R., and Belnap, N.D., Jr # Entailment: The Log:11.c of Relevance and Necessity, Princeton University Press, Princeton, NJ, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bolinger</author>
</authors>
<title>The atomization of meaning, LanapasenLq,</title>
<date>1965</date>
<pages>555--5731</pages>
<marker>Bolinger, 1965</marker>
<rawString>2. Bolinger, D. The atomization of meaning, LanapasenLq, (1965), 555-5731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-L Chang</author>
<author>R C T Lee</author>
</authors>
<title>Mechanical Theorem Proving,</title>
<date>1973</date>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<marker>Chang, Lee, 1973</marker>
<rawString>3. Chang, C.-L., and Lee, R.C.T. Mechanical Theorem Proving, Academic Press, New York, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Toward a model of children&apos;s story comprehension TR-266,</title>
<date>1972</date>
<journal>MIT A.I. Lab..,</journal>
<location>Cambridge, MA,</location>
<marker>Charniak, 1972</marker>
<rawString>4. Charniak, E. Toward a model of children&apos;s story comprehension TR-266, MIT A.I. Lab.., Cambridge, MA, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax,</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Chomsky, 1965</marker>
<rawString>5, Chomsky, N. Aspects of the Theory of Syntax, MIT Press, Cambridge, MA, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Colby</author>
</authors>
<date>1971</date>
<journal>Artificial paranoia. Artificial Intelligence</journal>
<volume>2</volume>
<pages>1--25</pages>
<marker>Colby, 1971</marker>
<rawString>6. Colby, K.M., et al. Artificial paranoia. Artificial Intelligence 2, (1971), 1-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Colby</author>
<author>R C Parkinson</author>
</authors>
<title>Pattern-matching rules for the recognition of natural language dialogue expressions.</title>
<date>1974</date>
<journal>AJcL Microfiche</journal>
<volume>5</volume>
<marker>Colby, Parkinson, 1974</marker>
<rawString>7. Colby, K.M., and Parkinson, R.C. Pattern-matching rules for the recognition of natural language dialogue expressions. AJcL Microfiche 5, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Collins</author>
<author>M R Quillian</author>
</authors>
<title>Retrieval time, from semantic memory.</title>
<date>1969</date>
<journal>J. Verbal Learning and Verbal Behavior</journal>
<volume>8</volume>
<pages>240--247</pages>
<marker>Collins, Quillian, 1969</marker>
<rawString>8. Collins, A.M., and Quillian, M.R. Retrieval time, from semantic memory. J. Verbal Learning and Verbal Behavior 8, (1969), 240-247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Enea</author>
<author>K M Colby</author>
</authors>
<title>Idiolectic language analysis for understanding doctor-patient dialogues.</title>
<date>1973</date>
<booktitle>Proc. Third IJCAI, SRI,</booktitle>
<pages>10</pages>
<location>Menlo Park, CA,</location>
<marker>Enea, Colby, 1973</marker>
<rawString>9. Enea, H., and Colby, K.M. Idiolectic language analysis for understanding doctor-patient dialogues. &apos;Proc. Third IJCAI, SRI, Menlo Park, CA, 1973. 10. Fillmore, C.J. The case for case. In E. Bach and R.T. Harms (Eds.), Universals in_Liaguistic Theory, Holt, Rinehart and Winston, New York, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes&apos;</author>
</authors>
<date>1977</date>
<booktitle>In defence of logic, Proc. IJCAI-77, MIT,</booktitle>
<pages>559--565</pages>
<location>Cambridge MA,</location>
<marker>Hayes&apos;, 1977</marker>
<rawString>11, Hayes&apos;, P.J. In defence of logic, Proc. IJCAI-77, MIT, Cambridge MA, 1977, 559-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Katz</author>
<author>J A Fodor</author>
</authors>
<title>The structure of a semantic theory,</title>
<date>1963</date>
<journal>Language</journal>
<volume>39</volume>
<pages>170--210</pages>
<location>Englewood Cliffs, NJ,</location>
<note>Also in</note>
<marker>Katz, Fodor, 1963</marker>
<rawString>12. Katz, J., and Fodor, J.A. The structure of a semantic theory, Language 39, (1963), 170-210. Also in J.A. Fodor and J.J. Katz (Eds.) The Structure of Language, Prentice-Hall, Englewood Cliffs, NJ, 1964, 479-518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Kleene</author>
</authors>
<title>Introduction to Metamathematics,</title>
<date>1950</date>
<location>D. Van Nostrand Co., Princeton, NJ,</location>
<marker>Kleene, 1950</marker>
<rawString>13. Kleene, S.C. Introduction to Metamathematics, D. Van Nostrand Co., Princeton, NJ, 1950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V McDermott</author>
</authors>
<title>Assimilation of new information by a natural language understanding system.</title>
<date>1974</date>
<booktitle>TR-291, MIT A.I. Lab.,</booktitle>
<location>Cambridge, MA,</location>
<marker>McDermott, 1974</marker>
<rawString>14. McDermott, D.V. Assimilation of new information by a natural language understanding system. TR-291, MIT A.I. Lab., Cambridge, MA, 1974.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Raphael</author>
</authors>
<title>SIR: a computer program for semantic information retrieval.</title>
<booktitle>In M. Minsky (Ed.). Semantic Information Processina,</booktitle>
<pages>33--145</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker>Raphael, </marker>
<rawString>15. Raphael, B. SIR: a computer program for semantic information retrieval. In M. Minsky (Ed.). Semantic Information Processina, MIT Press, Cambridge, MA, 33-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Reiger</author>
</authors>
<title>Conceptual Memory. Unpublished Ph.D. Thesis, Stanford U.,</title>
<date>1974</date>
<marker>Reiger, 1974</marker>
<rawString>16. Reiger, C. Conceptual Memory. Unpublished Ph.D. Thesis, Stanford U., 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>Conceptual analysis.</title>
<date>1975</date>
<booktitle>In R. Schank. Conceptual Information Processing,</booktitle>
<pages>83--156</pages>
<location>North-Holland, sterd</location>
<marker>Riesbeck, 1975</marker>
<rawString>17. Riesbeck, C.K. Conceptual analysis. In R. Schank. Conceptual Information Processing, North-Holland, sterd 1975, 83-156.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V B Borscev</author>
<author>M V Chomjakov</author>
</authors>
<journal>Neighbourhood Des:riptiOn of Formal Languages</journal>
<volume>3</volume>
<marker>Borscev, Chomjakov, </marker>
<rawString>V. B. Borscev, M. V. Chomjakov, Neighbourhood Des:riptiOn of Formal Languages 3 W. Brecht, Morphological Analysis (A Formal Approach) 9</rawString>
</citation>
<citation valid="false">
<authors>
<author>J P Descles</author>
</authors>
<title>Un modele mathematique d&apos;analyse transformationnelle selon</title>
<journal>E. Pause, A Class of Transformational Recognition Grammars</journal>
<volume>45</volume>
<marker>Descles, </marker>
<rawString>J. P. Descles, Un modele mathematique d&apos;analyse transformationnelle selon Z. S. Harris 23 * F. G. Pagan, Constructible Representations for Two Semantic Relations 29 E. Pause, A Class of Transformational Recognition Grammars 45</rawString>
</citation>
<citation valid="false">
<authors>
<author>boleva</author>
</authors>
<title>Formal Metalanguage and Formal Theory as Two Aspects of Generative Grammar</title>
<journal>G. T.-System of Joyce Friedman</journal>
<booktitle>On Logical Formulation of the Computation Process in Semantical Systems 87 TESTING AND SIMULATION</booktitle>
<volume>63</volume>
<marker>boleva, </marker>
<rawString>S. K. Saumjan, P. A. S)boleva, Formal Metalanguage and Formal Theory as Two Aspects of Generative Grammar 63 J. T. Wang, On Logical Formulation of the Computation Process in Semantical Systems 87 TESTING AND SIMULATION I. Baton, Working with the Interactive Version of the T. G. T.-System of Joyce Friedman 103</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Boisvert</author>
<author>A Dugas</author>
<author>D Belanger</author>
</authors>
<title>Obling: a Tester for Transformational Grammars 121</title>
<marker>Boisvert, Dugas, Belanger, </marker>
<rawString>S. Boisvert, A. Dugas, D. Belanger, Obling: a Tester for Transformational Grammars 121</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Riha</author>
<author>S Machova</author>
</authors>
<title>Computer Testing of a Generative Grammar 143 N.D. Andreyev. Algorithmisation of linguistic research using the structural-probabilistic properties of language units</title>
<pages>155</pages>
<marker>Riha, Machova, </marker>
<rawString>A. Riha, S. Machova, Computer Testing of a Generative Grammar 143 N.D. Andreyev. Algorithmisation of linguistic research using the structural-probabilistic properties of language units 155</rawString>
</citation>
<citation valid="false">
<authors>
<author>D&apos;A S Avalle</author>
</authors>
<title>La formalisation des graphes cans le domaine de la rime 351</title>
<booktitle>Corpus litscriptionun Latinanim 365 G. Buccellati, The Old Babylonian Linguistic Analysis Project Goals, Procedures ar.d First Results</booktitle>
<volume>385</volume>
<pages>405</pages>
<marker>Avalle, </marker>
<rawString>D&apos;A. S. Avalle, La formalisation des graphes cans le domaine de la rime 351 P. Baldacci, B. Cavagnola, 0 Ianovitz, E. Maretti, G. Masperi, P. 0, Michelotto, G. P. Zarri, Polemon I: A program of automatic construction of indexes for the V volume of Corpus litscriptionun Latinanim 365 G. Buccellati, The Old Babylonian Linguistic Analysis Project Goals, Procedures ar.d First Results 385 0. Rola, Y. Karjalainen, Syntax Archives of Finnish Dialects for Computer Work . 405</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Leonard</author>
</authors>
<title>The Computer Archive of Modern English Texts 417 C. Saporetti, Le Projet Altan (&amp;quot;Analisi Linguistica dei testi Assiri e Nuziana&amp;quot;)</title>
<tech>429 SEMANTICAL CAICULUS</tech>
<marker>Leonard, </marker>
<rawString>R. Leonard, The Computer Archive of Modern English Texts 417 C. Saporetti, Le Projet Altan (&amp;quot;Analisi Linguistica dei testi Assiri e Nuziana&amp;quot;) 429 SEMANTICAL CAICULUS</rawString>
</citation>
<citation valid="false">
<authors>
<author>N G Arsentjeva</author>
</authors>
<title>From Semantic Features of a Word to its Surface Syntactic Features 433</title>
<pages>471</pages>
<marker>Arsentjeva, </marker>
<rawString>N. G. Arsentjeva, From Semantic Features of a Word to its Surface Syntactic Features 433 C. Fuchs, J. Rcuault, Etude formelle de l&apos;opposition situation/propriete et des phenomenes de voix et de thematisation 445 T. Furugori, An Encyclopedic View of Understanding Natural Language . . . . . . 471</rawString>
</citation>
<citation valid="false">
<authors>
<author>GENERAL CONFERENCE-WORKSHOPS TABLE OF TENTS E Ginsburg</author>
</authors>
<title>The Mechnnics of the Inner Form of the Derivative Word and its Description in the Two-level theory of Language 479</title>
<marker>Ginsburg, </marker>
<rawString>GENERAL: CONFERENCE-WORKSHOPS VIII TABLE OF =TENTS E. Ginsburg, The Mechnnics of the Inner Form of the Derivative Word and its Description in the Two-level theory of Language 479</rawString>
</citation>
<citation valid="false">
<authors>
<author>N M Goldman</author>
</authors>
<title>Sentence Paraphrasing from a Conceptual Base 481</title>
<journal>E. K. Gousseva, Operational Semantic Models in Soviet Linguistics</journal>
<volume>509</volume>
<marker>Goldman, </marker>
<rawString>N. M. Goldman, Sentence Paraphrasing from a Conceptual Base 481 E. K. Gousseva, Operational Semantic Models in Soviet Linguistics 509</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Hajicova</author>
<author>P Sgall</author>
</authors>
<title>A Semantic Representation of Topic and Focus</title>
<booktitle>Types of Processes on Cognitive Networks 523 B. Henisz-Dostert, F. B. Thompson, The REL System and REL English 533</booktitle>
<marker>Hajicova, Sgall, </marker>
<rawString>E. Hajicova, P. Sgall, A Semantic Representation of Topic and Focus 515 D. G. Hays, Types of Processes on Cognitive Networks 523 B. Henisz-Dostert, F. B. Thompson, The REL System and REL English 533</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Klein</author>
</authors>
<title>Automatic Inference of Semantic Deep Structure Rules</title>
<booktitle>in Generative Semantic Grammars 557 E. V. Paduceva, Modalities in Russian and in a Language of</booktitle>
<pages>557</pages>
<marker>Klein, </marker>
<rawString>*S. Klein, Automatic Inference of Semantic Deep Structure Rules in Generative Semantic Grammars 557 E. V. Paduceva, Modalities in Russian and in a Language of 557</rawString>
</citation>
<citation valid="false">
<authors>
<author>S IL Petrick</author>
</authors>
<booktitle>Semantic Interpretation in the Request System 585 *B. Phillips, Topic Analysis 611</booktitle>
<marker>Petrick, </marker>
<rawString>S. IL Petrick, Semantic Interpretation in the Request System 585 *B. Phillips, Topic Analysis 611</rawString>
</citation>
<citation valid="false">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>Expectation as a Basic Mechanism of Langu fa</title>
<journal>Comprehension •</journal>
<booktitle>Inference and Conceptual Memory 653 SOME RECENT CONTRIBUTIONS TO STATISTICAL LINGUISTICS</booktitle>
<volume>627</volume>
<pages>135</pages>
<marker>Riesbeck, </marker>
<rawString>*C. K. Riesbeck, Expectation as a Basic Mechanism of Langu fa Comprehension • 627 R. C. Schank, C. J. Rieger III, Inference and Conceptual Memory 653 SOME RECENT CONTRIBUTIONS TO STATISTICAL LINGUISTICS 135</rawString>
</citation>
<citation valid="false">
<authors>
<author>Charles Muller</author>
</authors>
<title>Abstracts of these articles have appeared previously in AJCL, Fiche 48. The specific frame numbers are indicated in parentheses after the title. The other articles are abstracted elsewhere on this fiche.</title>
<marker>Muller, </marker>
<rawString>Charles Muller *Abstracts of these articles have appeared previously in AJCL, Fiche 48. The specific frame numbers are indicated in parentheses after the title. The other articles are abstracted elsewhere on this fiche.</rawString>
</citation>
<citation valid="true">
<date>1976</date>
<booktitle>GENERAL: JOURNAL 45 The Prague Bulletin of Mathematical Linguistics 26 Universita Karlova,</booktitle>
<location>Praha,</location>
<marker>1976</marker>
<rawString>GENERAL: JOURNAL 45 The Prague Bulletin of Mathematical Linguistics 26 Universita Karlova, Praha, 1976</rawString>
</citation>
<citation valid="false">
<authors>
<author>CONTENTS</author>
</authors>
<title>Semantische Netze und selektive Beziehungen I,</title>
<journal>J. Kunze</journal>
<volume>3</volume>
<marker>CONTENTS, </marker>
<rawString>CONTENTS 1. Semantische Netze und selektive Beziehungen I, J. Kunze 3 2. Meaning of Sign, Cognitive Content, and Pragmatics, P. Sgal1 51</rawString>
</citation>
<citation valid="true">
<authors>
<author>GENERAL JOURNAL</author>
</authors>
<title>The Prague Bulletin of Mathematical Linguistics 26 Universira Karlova,</title>
<date>1976</date>
<location>Praha</location>
<marker>JOURNAL, 1976</marker>
<rawString>GENERAL: JOURNAL The Prague Bulletin of Mathematical Linguistics 26 Universira Karlova, Praha 1976</rawString>
</citation>
<citation valid="false">
<authors>
<author>CONTENTS</author>
</authors>
<title>0 znachenii rabot U. Revzina v oblasti teoretiko-mozhestvenoi kontseptsii iazyka,</title>
<date>1974</date>
<journal>O. G. Rev zina, 1. A. Shreider</journal>
<booktitle>On Some Relationships of Linguistics and Information Retrieval, P. Sga11 51 PHONETICS-PHONOLOGY 46 Speech Understanding Systems: Final Report, Movembei</booktitle>
<volume>3</volume>
<marker>CONTENTS, 1974</marker>
<rawString>CONTENTS 1. 0 znachenii rabot U. Revzina v oblasti teoretiko-mozhestvenoi kontseptsii iazyka, O. G. Rev zina, 1. A. Shreider 3 2. Semantische Netze und Oektive Beziehungen II, J. Kunze 11 3. 0 rnodeli estestvennogo iazyka, osnovannoi na poniatii peremennykh okruzhenii, J. S. Bien 41 4. On Some Relationships of Linguistics and Information Retrieval, P. Sga11 51 PHONETICS-PHONOLOGY 46 Speech Understanding Systems: Final Report, Movembei 1974 - October 1976 Volume II: Acoustic Front End</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<date></date>
<journal>Bolt Berairiek and Newman Inc.,</journal>
<volume>50</volume>
<pages>02138</pages>
<location>Moulton St., Cambridge,</location>
<marker>Woods, </marker>
<rawString>William A. Woods, et al. Bolt Berairiek and Newman Inc., 50 Moulton St., Cambridge, 111A 02138</rawString>
</citation>
<citation valid="true">
<authors>
<author>Report</author>
</authors>
<title>9/ pp.,</title>
<date></date>
<booktitle>1Q76 PHONETICS-PHONOLOGY: RECOGNITION: PROSODY 47 WHY &amp;quot;41/book&amp;quot;?</booktitle>
<marker>Report, </marker>
<rawString>Report 3438, 9/ pp., December 1Q76 PHONETICS-PHONOLOGY: RECOGNITION: PROSODY 47 WHY &amp;quot;41/book&amp;quot;?</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sweden SSE Lund</author>
</authors>
<title>Using Knowledge in the Computer Interpretation of Handwritten FORTRAN coding</title>
<date>1976</date>
<booktitle>Volume 111: Lexicon, Lexical Retrieval and Control</booktitle>
<volume>14</volume>
<pages>pp.,</pages>
<institution>Bengt Orestrom Survey of Spoken English, University of Lund, Helgonabacken</institution>
<marker>Lund, 1976</marker>
<rawString>Bengt Orestrom Survey of Spoken English, University of Lund, Helgonabacken 14, S-223 62 Lund, Sweden SSE, 13 pp., October 1976 Using Knowledge in the Computer Interpretation of Handwritten FORTRAN coding Sheets R. Bornat, and J. M. Brody Computing Centre, University of Essex, Colchester, U. K. Speech Understanding Systems Final Report, November 1974 - October 1976 Volume 111: Lexicon, Lexical Retrieval and Control</rawString>
</citation>
<citation valid="false">
<authors>
<author>William A Woods</author>
</authors>
<journal>Bolt Beranek and Newman Inc.,</journal>
<volume>50</volume>
<pages>02138</pages>
<location>Moulton Street, Cambridge, MA</location>
<marker>Woods, </marker>
<rawString>William A. Woods, et al. Bolt Beranek and Newman Inc., 50 Moulton Street, Cambridge, MA 02138</rawString>
</citation>
<citation valid="false">
<authors>
<author>Report No</author>
</authors>
<title>Topics: Dictionary, Phonological rules, Dictionary expansions, Lexical retrieval, Control strategy, Performance. The Lexical Retrieval component determines the n most probable word matches in, a full lexicon or appropriate subset and operates on a phonetic segment lattice. Words can &amp;quot;oe matched left-to-right and right-to-left. Control strategy options are governed by 25 flags. Each strategy performs an initial scan of some region of the utterance, creating one-word seed events. In &amp;quot;middle-cut&amp;quot; strategies the initial scan is done over the entire utterance.</title>
<date>1976</date>
<booktitle>In L-R strategies the</booktitle>
<volume>3438</volume>
<pages>pp.,</pages>
<marker>No, 1976</marker>
<rawString>Report No. 3438, 110 pp., December 1976 Topics: Dictionary, Phonological rules, Dictionary expansions, Lexical retrieval, Control strategy, Performance. The Lexical Retrieval component determines the n most probable word matches in, a full lexicon or appropriate subset and operates on a phonetic segment lattice. Words can &amp;quot;oe matched left-to-right and right-to-left. Control strategy options are governed by 25 flags. Each strategy performs an initial scan of some region of the utterance, creating one-word seed events. In &amp;quot;middle-cut&amp;quot; strategies the initial scan is done over the entire utterance. In L-R strategies the initial scan only considers words that could begin the utterance. In &amp;quot;hybrid&amp;quot; strategies the initial scan fixes op an initial portion of the utterance and then middle-out analysis is done on this region with the remainder necessarily being analyzed L-R. In all these op:ions events are ordered on the queue by their priority scores. Appendices: Annotated phonological. rules, Format and examples Of dictionary files, Result summaries for each token, Performance results for strategy variations, BIGDICT and TRAVELDICT listings, Dictionary expansion - a user&apos;s guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L</author>
</authors>
<title>Computers and the Production of Systematic Terminological Glossaries</title>
<date>1977</date>
<journal>Bulletin of the Association for Literary and Linguistic Computing</journal>
<volume>5</volume>
<pages>26--37</pages>
<location>Manchester, England</location>
<marker>L, 1977</marker>
<rawString>Computers and the Production of Systematic Terminological Glossaries M. L. Hann UM1ST, Manchester, England Bulletin of the Association for Literary and Linguistic Computing 5: 26-37, 1977 SITAR: Interactive Text PrL)cessing System for Small Computers</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ben Ross Schneider</author>
</authors>
<pages>54911</pages>
<institution>English Department, Lawrence University,</institution>
<location>Appleton, WI</location>
<marker>Schneider, </marker>
<rawString>Ben Ross Schneider, Jr. English Department, Lawrence University, Appleton, WI 54911</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Reid</author>
</authors>
<title>SITAR, a low-cost interactive text handling and text analysis system for nontechnical users, is in many ways comparable .to interactive bibliographic search arid retrieval systems, but has several additions features. It is implemented on a PDP/11 time-sharing computer invoked by a CRT with microprogrammed editing functions. It uses a simple command language designating a function, a file, and a search template consisting of the textual string desired and strings delimiting the context in which the hit is to be delivered.</title>
<date>1977</date>
<journal>Communications of the ACM</journal>
<volume>20</volume>
<pages>495--499</pages>
<institution>Watts University of Kansas</institution>
<location>Lund, Sweden</location>
<marker>Reid, 1977</marker>
<rawString>Reid M. Watts University of Kansas Communications of the ACM 20: 495-499, July 1977 SITAR, a low-cost interactive text handling and text analysis system for nontechnical users, is in many ways comparable .to interactive bibliographic search arid retrieval systems, but has several additions features. It is implemented on a PDP/11 time-sharing computer invoked by a CRT with microprogrammed editing functions. It uses a simple command language designating a function, a file, and a search template consisting of the textual string desired and strings delimiting the context in which the hit is to be delivered. LEXICOGRAPHY=LEXICOLOGYI TEXT HANDLING 50 Manual for Terminal Input of Spoken English Material Bengt Orestrom, Jan Svartvik,, and Cecilia Thavenius Survey of Spoken English, University of Lund, Helgonabacken 14, S-223 62 Lund, Sweden SSE, 23 pp., October 1976</rawString>
</citation>
<citation valid="false">
<title>The basic material of the Survey of Spoken English (SSE) at Lund University consists of the spoken English. at the Survey of English material collected and transcribed at the Survey of English Usage (SEU) under the direction of Profess&amp; Randolph Quirk, University College London. The SSE material differs from the original SEU material in two respects: for linguistic reasons (explained in the manual) it has been pruned of certain prosodic and paralinguistic categories; and for technical reasons the prosodic and phonetic symbols have been changed. The manual lists the symbol set used for SSE and code set, peripheral equipment, and SSE procedures have been described in appendices.</title>
<marker></marker>
<rawString>The basic material of the Survey of Spoken English (SSE) at Lund University consists of the spoken English. at the Survey of English material collected and transcribed at the Survey of English Usage (SEU) under the direction of Profess&amp; Randolph Quirk, University College London. The SSE material differs from the original SEU material in two respects: for linguistic reasons (explained in the manual) it has been pruned of certain prosodic and paralinguistic categories; and for technical reasons the prosodic and phonetic symbols have been changed. The manual lists the symbol set used for SSE and code set, peripheral equipment, and SSE procedures have been described in appendices.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>