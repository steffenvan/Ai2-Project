<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.9947305">
Developing a Nonsymbolic Phonetic
Notation for Speech Synthesis
</title>
<author confidence="0.998903">
Andrew Cohen*
</author>
<affiliation confidence="0.960356">
University of Reading
</affiliation>
<bodyText confidence="0.9984297">
The goal of the research presented here is to apply unsupervised neural network learning methods
to some of the lower-level problems in speech synthesis currently performed by rule-based systems.
The latter tend to be strongly influenced by notations developed by linguists (see figure 1 in Klatt
(1987)), which were primarily devised to deal with written rather than spoken language. In
general terms, what is needed in phonetics is a notation that captures information about ratios
rather than absolute values, as is typically seen in biological systems. The notations derived here
are based on an ordered pattern space that can be dealt with more easily by neural networks,
and by systems involving a neural and symbolic component. Hence, the approach described here
might also be useful in the design of a hybrid neural/symbolic system to operate in the speech
synthesis domain.
</bodyText>
<sectionHeader confidence="0.42556" genericHeader="abstract">
1. Background and phonetic motivation
</sectionHeader>
<bodyText confidence="0.999687083333334">
Phonological and phonetic notations have been developed by linguists primarily as
descriptive tools, using rewrite-rules operating on highly abstracted basic units de-
rived from articulatory phonetics. Even some connectionist work has followed this
tradition (Touretzky et al. 1990). The primary aim of these notations is explanation
and understanding, and there are difficulties in incorporating them into systems with
a practical aim such as deriving speech from text, which tend to be data-driven. One
recent study claimed that introduction of linguistic knowledge degrades performance
in grapheme-phoneme conversion (van den Bosch and Daelemans 1993). However,
typical purely data-driven systems are opaque from a phonetic or phonological point
of view. In order to handle many of the very hard problems remaining in speech syn-
thesis, there is a need to develop a basic underlying notation (or method of deriving
a notation) that can be parameterized for different speakers. This notation could be
based on articulatory phonetics (where a higher-level task, such as grapheme-phoneme
conversion, is being performed) or on a spectral/perceptual measure of similarity, for
more low-level tasks such as duration adjustment. This notation would ideally be rep-
resented in a low-dimensional, topological space so as to be both perspicuous and
flexible enough to use in further nonsymbolic modules.
Existing synthesis-by-rule (SBR) systems (Allen et al. 1987) have been concerned
with text-to-speech conversion, and have made use of a segmental approach derived
from traditional phonology. Among the simplifying assumptions remaining from this
approach are that transitions into and out of a consonant are identical, and that the
same transition may be used in each CV combination, regardless of the larger phonetic
environment. These assumptions need to be modified in a principled manner rather
than by tables of exceptions.
</bodyText>
<note confidence="0.797375">
* Department of Cybernetics, University of Reading. E-mail: cybadc@cyber.reading.ac.uk
C) 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999883666666667">
It has been argued by phoneticians that articulatory models cannot account for
all the variability found in natural speech (Bladon and Al-Bamerni 1976; Kelly and
Local 1986). Therefore, there is a need to find ways of incorporating other sources of
variability into synthetic speech, including, for example, the feedback a talker receives
from the perception of their own voice. Evidence that such feedback affects speech is
the degradation seen in the speech of persons with acquired deafness. One possible
way to introduce this kind of variability is through the development of representations
that encode (in a reduced dimensionality) a range of examples of the phenomenon to
be accounted for. Formant data can be used to introduce a perceptual measure of
similarity (see section 3 below).
This report describes the theoretical motivations of an experimental system that
has been implemented as a set of shell scripts and &apos;C&apos; programs; not all of the technical
details of this system have been finalized, and it has not been formally tested. While
formants have been made use of as training data (as well as acoustic tube data), as yet
no use has been made of a formant synthesizer for creating the output speech, due to
the need for handcrafting of values. At present, waveform segment concatenation is
being used to explore a parametric duration model based on the kind of proximity-
based notations described here.
</bodyText>
<subsectionHeader confidence="0.787042">
2. Application of the SOM to phoneme data
</subsectionHeader>
<bodyText confidence="0.999990379310345">
In outline, the Self-Organizing Map (SOM, Kohonen 1988) approximates to the prob-
ability density function of the input pattern space, by representing the N-dimensional
pattern vectors on a 2D array of reference vectors in such a way that the resulting clus-
terings conform to an elastic surface, where neighboring units share similar reference
vectors. This algorithm and Learning Vector Quantization (LVQ) are described in Ko-
honen (1990), which has practical advice for implementation, and in more theoretical
detail in Kohonen (1989).
It has been widely noted that 2D representations of speech are useful where there
is a need to transmit information to humans at a phonetic level—for example, in tactile
listening systems (Ellis and Robinson 1993). If a speech synthesis system has a phonetic
interface or level of operation, it is then possible to introduce learning techniques for
subsequent modules (e.g., those which calculate durations or an intonation contour)
and to have an idea of what is happening, in phonetic terms, when things go wrong,
and therefore how the training program or learning method may be adjusted. There
is a long tradition of two-dimensional representations of formant data in attempts
to classify vowels, going back at least to the study of Peterson and Barney (1952).
Another type of advantage lies in the flexibility given by the very large dimensionality
reductions achievable by Kohonen&apos;s technique. These reductions are possible even
where the input pattern space may be only sparsely populated, yielding a flexible
encoding with not too many degrees of freedom. It is possible for Kohonen&apos;s technique
to work in 3D (3D maps have been produced by the author, but are more difficult to
work with and are still undergoing evaluation). In 4D or above, interpretation becomes
much more difficult. Refinements such as the Growing Cells technique (Fritzke 1993)
might be preferable to a move to higher dimensionality, so as to retain transparency
of the notation and a possible link to symbol-based stages of operation.
Figure 1 shows a map resulting from applying the SOM algorithm to phoneme
feature data. The following nine binary articulatory features were used: continuant,
voiced, nasal, strident, grave, compact, vowel height(1), vowel height(2), and round.
The features hl and h2 are used for height simply because there are three possibilities:
</bodyText>
<page confidence="0.991632">
568
</page>
<figure confidence="0.938304916666667">
Cohen Nonsymbolic Phonetic Notation for Speech Synthesis
f S sh ch
J
v z dh
th nch
r 1
Y in nk
n b g
w
U i d
o e t P
a k
</figure>
<figureCaption confidence="0.963515">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.960591">
Clustering of phoneme data (8&gt;&lt; 12).
</subsectionHeader>
<bodyText confidence="0.99993">
open, mid and closed, which cannot be encoded by a binary bit.1 In this case, the point
is not to do feature extraction (since the features are already known), but to provide
a statistical clustering in 2D that can indicate whether the features chosen provide
a good basis for analysis. Figure 1 suggests that phoneticians have &apos;got it right&apos; in
that the features do result in a clustering of similar sounds such as stops, fricatives
and nasals, as well as the more obvious separation between vowels and consonants.
It is worth pointing out that neither the SOM nor the LVQ algorithm handles raw
data (such as waveform values or image intensity values), but each operates on data
such as spectral components or LPC coefficients that are themselves the output of a
significant processing stage, and can justifiably be called features.
The phoneme map is produced by a single Kohonen layer that self-organizes using
the standard algorithm (Kohonen, 1990), taking as input nine articulatory features
commonly used by phoneticians to describe the possible speech sounds. The features
were designed so that any phoneme (or syllable) may be uniquely specified as a cluster
of features, without reference to specific units (segments such as phones, syllables,
etc.)—any feature may run across unit boundaries. Figure 1 shows a 12 x 8 map created
(as are all the following maps) with hexagonal connections in the lattice indicating
which units are neighbors. A monotonically shrinking &apos;bubble&apos; neighborhood was
used in all the maps shown here. Kohonen refers to this type of kernel as a bubble
because it relates to certain kinds of activity bubbles in laterally connected networks
(see Kohonen 1989).
</bodyText>
<footnote confidence="0.864209">
1 Thanks to John Local for providing the basis for the data.
</footnote>
<page confidence="0.98743">
569
</page>
<note confidence="0.450187">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999971666666666">
The analysis of these maps is at a phonemic level of description; this is a very
abstract level compared to the phonetic descriptions typically used, which take into
account much more of the context. However, the trend in recent phonology has been
towards ever greater abstraction and more complex hierarchies of units (e.g., Gold-
smith 1990; Durand 1990). The abstractness of phonemes makes them more difficult
candidates for both recognition and synthesis, although most existing systems in the
two fields perversely make use of a phoneme stage. If a phoneme stage is held to be
essential (on grounds of parsimony, perhaps), then maps of the type shown in figure 1
may be one means to incorporate phonemes into a static or recurrent neural network-
based system in a more flexible fashion. However, the essential point is that trajectory
across the map provides a bridge between the symbolic description of the data and the
data itself. Robustness of mappings between domains (e.g., from text to phonemes)
should be increased, since similar sounds (words) will have similar trajectories across
the map.
Clearly, it would be interesting to repeat the process with formant data to see if a
similar 2D map can be formed. Formants are known to be important in perception, but
do not in general correspond to a particular vocal tract configuration. Many papers
have stressed the importance of formant peaks in speech perception, especially in the
case of vowels (Klatt 1982a). Changes in formant frequency are more important in
phonetic judgments than changes in formant amplitudes or bandwidths, or zeros in
the spectrum (Klatt 1982b).
The diphone approach embodies a signal-modeling, as opposed to a system-
modeling, viewpoint. Therefore, it is possible to incorporate sources of variability
other than the purely articulatory into the learning procedure that derives the basic
notation. Features such as accent-specific detail, stress and emotional quality are diffi-
cult to describe in purely articulatory terms, and, with current knowledge, cannot be
given a re-usable, speaker-independent representation at all.
</bodyText>
<subsectionHeader confidence="0.701447">
3. Application of the SOM to diphone formant data
</subsectionHeader>
<bodyText confidence="0.999929047619047">
The formant data obtained (F1, F2, F3, F4 and AV; formant bandwidths were not used)
in each frame were passed as a single vector to the SOM. (For a description of the SOM
algorithm see Kohonen 1990, 1989). Some diphones would run over more frames than
others, so shorter vectors were padded with zeros to make them up to the length of the
longest. The average length of a diphone (unpadded) was about 30 frames; the bulk of
this would naturally consist of steady-state rather than transitional data. However, the
training sets in the maps shown were chosen so that the significant variance would
lie in the transitional parts of the training vector, rather than in the less interesting
steady-state portions.
A similar type of experiment to that in section 1 can be carried out on diphone
formant data. For comparison, figure 2d shows the same data set as in figure 2c clus-
tered by means of Sammon&apos;s mapping (Sammon 1969). The latter is a supervised,
error minimization procedure that maps the input vectors onto a lower dimensional
space in such a way that the Euclidean distance between the endpoints is approxi-
mately preserved. It does not have the topographic property of the Kohonen map, but
is useful in indicating the underlying form and orientation of the data in 2D.
A number of maps showing clustering of similar sounds (formant vectors) have
been obtained (Figures 2a—c show examples). Larger maps containing up to 1250 di-
phones have also been created. In contrast to the one-point representations used in
Huang (1990) for vowels, the entire diphone is presented to the network for classi-
fication. The frame length (not more than 10ms) should mean that all perceptually
</bodyText>
<page confidence="0.943786">
570
</page>
<note confidence="0.218404">
Cohen Nonsymbolic Phonetic Notation for Speech Synthesis
</note>
<bodyText confidence="0.715412545454545">
aar . aag . aam .
aaw aap .
aay . aak
aazh . aat .
aas
aash . aaj . aath .
aal .
aaz aad . aah .
aaf . aang . aan . aadh
aab aach aav
Figure 2a
</bodyText>
<subsectionHeader confidence="0.489425">
Clustering of diphone data for aa-C.
</subsectionHeader>
<bodyText confidence="0.998981181818182">
relevant information is captured in the formant trajectories. Maps based on acoustic
tube data computed from the LPC coefficients have also been created, with much the
same kind of results as seen in the formant maps. That the results should be similar is
to be expected as this data is essentially spectral, and bears little resemblance to real
vocal tract data. Experiments are currently being carried out to determine whether
these maps or those based on formants will work better as part of a prototype speech
synthesis system.
To factor out the influence of the initial configuration of the network (the reference
vectors are initialized to small random values), twenty trials were run on each data
set, and the map with the lowest quantization error (QE) was selected as the best. The
QE is simply the mean error over the N pattern vectors in the set,
</bodyText>
<equation confidence="0.9937265">
ENIlx(t) — mc(t)II
QE= t=i
</equation>
<bodyText confidence="0.999325">
where x(t) is the input vector and mc the best matching reference vector for x(t).
In order to compare QEs, the topology (form of lateral connections) and adaptation
functions must be the same, since the amount of lateral interaction determines the
self-organizing power of the network. In the simplest case of competitive learning the
neighborhood contains only one unit, so a minimal QE may be achieved, but in this
case there is no self-organizing effect.
Schematically, then, resynthesis would take place on the basis of a trajectory across
a diphone map. The trajectory could be stored simply as a vector of co-ordinates that
are &apos;lit up&apos; on the map. These vectors would occupy little storage space, and might
be passed as input to a further SOM layer to try to cluster similar sounding words.
The time-varying, sequential properties of speech, which are difficult for neural nets
to handle, can thus be modeled as a spatial pattern in an accessible and straightfor-
ward manner. Vectors of addresses would be completely different (e.g., the endpoints
</bodyText>
<page confidence="0.992468">
571
</page>
<note confidence="0.660923">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.978226485714286">
daa . aab . aap . aach aaf
aag . aah
gaa . aaj . aas aash
. baa . aaz
aad aadh . aazh
f aa . aam
saa . paa . aal
haa . aay .
j aa . aan .
naa . aar
waa . zaa ngaa laa .
maa . raa
dhaa . chaa . vaa
taa . kaa thaa
Figure 2b
Clustering of data for aa-C and C-aa.
would be far apart), although the closeness of similar individual segments is main-
tained in &apos;diphone space&apos; on the map. Current practical work is focusing on devel-
oping a method for determining segment durations based on SOM distances between
successive diphones in a string.
The creation of a &apos;diphone space&apos; in 2D may assist both in choosing the correct
(best matching) segments and in interpolation at segment boundaries. In a text-to-
speech application, when mapping from text onto diphones, it is clearly an easier task
for an MLP to map neighboring regions of input space onto neighboring regions of
output space. In joining segments the amount of distance travelled across the map is
evidence of how different the two segments are in perceptual terms, and, therefore, of
the amount and type of smoothing of formant values needed. The maps also suggest an
approach to the problem of calculating segment durations, which is currently under
investigation. For example, consider the sequence &apos;aalaa&apos;. This can be made up of
the diphones &apos;aal&apos; and &apos;laa&apos;, which are close in diphone space. This suggests a shorter
duration for &apos;aalaa&apos; than for &apos;aalpaa&apos;, where there is a transition into a consonant, which
is acoustically very different (and further away on the SOM). These considerations are
of course subject to modification by prosodic factors, such as the need to stress a
particular word.
In the prototype system under development, the baseline system uses a diphone
</bodyText>
<page confidence="0.982553">
572
</page>
<figure confidence="0.957005592592593">
Cohen Nonsymbolic Phonetic Notation for Speech Synthesis
airp. ahp . aap . ep Key:
Sym Example
oap . iep . oip . op &amp;quot;air&amp;quot; /air/
&amp;quot;ah&amp;quot; /bard/
oop . &amp;quot;e&amp;quot; /bed/
&amp;quot;oa&amp;quot; /oak/
aip oorp . erp . awp . ip &amp;quot;ie&amp;quot; /pie/
&amp;quot;oi&amp;quot; /oil/
uup . floo&amp;quot; /good/
&amp;quot;ai&amp;quot; /pain/
arp . pee . eep &amp;quot;oor&amp;quot; /poor/
&amp;quot;er&amp;quot; /bird/
Pu po &amp;quot;aw&amp;quot; /board/
&amp;quot;i&amp;quot; /bid/
poi . paa . pe &amp;quot;uu&amp;quot; /brood/
&amp;quot;ee&amp;quot; /bead/
&amp;quot;u&amp;quot; /bud/
&amp;quot;o&amp;quot; /body/
poa . pair . pi &amp;quot;uh&amp;quot; /above/
&amp;quot;ou&amp;quot; /out/
peer . per . poo . &amp;quot;eer&amp;quot; /ear/
&amp;quot;ar&amp;quot; /art/
puh
poor. pou .
paw . pai puu .
pie . par
</figure>
<figureCaption confidence="0.983427">
Figure 2c
</figureCaption>
<table confidence="0.981451086956522">
Clustering of data for transitions into and out of &apos;p&apos;.
. &apos; It .rp•terp &apos; . . -
&apos;alp &amp;quot;_p_. sam&amp;quot; • -
- &amp;quot;tap &amp;quot;ep _
morp •ip -
Map MoptP
- .i.eP •_ap
&apos;t rP
taip blip
Mip
-
.t1wP
- Mup
&apos;pee &apos;eep
&apos;VI te &amp;quot;p aa &apos;paw /Da]
- &amp;quot;poa &amp;quot;poi &amp;quot;pie _
_ &apos;petr,•air P o o r . _
.,pi•poo 41) r -
_ e
•puh &amp;quot;puu &amp;quot;psi
-
•-p
-60000 -40000 -20000 0 20000 40000 60000 80000
</table>
<figureCaption confidence="0.807314">
Figure 2d
</figureCaption>
<figure confidence="0.941980166666667">
Clustering of data with Sammon&apos;s mapping.
50000
40000
30000
20000
10000
0
-10000
-20000
-30000
-40000
-50000
</figure>
<page confidence="0.935113">
573
</page>
<note confidence="0.703722">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999601285714286">
concatenation procedure, on which various enhancements based on the SOM are being
tried, which will be more fully described in future reports. Using the examples given
on a record supplied with Klatt&apos;s (1987) review article, informal comparison shows a
high degree of variability in quality of the sentences generated: the best are comparable
with the diphone concatenation methods (which have better transitions than DECtalk,
even if the prosody is in some cases not as well developed), while the worst are highly
unnatural, but usually intelligible.
</bodyText>
<sectionHeader confidence="0.904986" genericHeader="categories and subject descriptors">
4. Conclusion and further work
</sectionHeader>
<bodyText confidence="0.999981346153846">
The outline of a conventional SBR system has a series of symbolic stages, assuming a
modularity of data at each level, before the final low-level stage (&apos;synthesis routines&apos;)
calculates the synthesizer parameters. The essential feature is the &apos;abstract linguistic
description&apos;, which must be derived before any attempt is made to calculate parameter
values. In the proposed system, this middle stage is replaced by the SOM stage, which
introduces a learned notation based on acoustic data. Generation of an intonation
contour, though this has been implemented with neural nets, is probably best handled
with rules as it is almost purely a prosodic (i.e., sentence level) matter.
The SOM coding replaces the linguistic description, and leads to direct access
of waveform values for a given diphone, which then become default values for the
next stage to operate on. In conclusion, arguments have been presented for the use
of nonsymbolic codings as the central stage of a text to-speech system. These cod-
ings are both closer to the acoustic domain and capable of greater flexibility than
the standard phonetic notations. Additional sources of variability, such as stress and
emotional quality, could also be accounted for with this kind of trajectory in a low-
dimensional space, rather than attempting to derive a speaker-independent symbolic
notation. These maps are also capable of being operated on by a neural network in
further processing stages, opening the way to a different type of phonetics based on
a multitude of soft constraints rather than the rigid phoneme and rewrite rule.
Further work is needed to investigate the usefulness of the SOMs in speech synthe-
sis, and how they may be integrated in a hybrid system that uses rule-based prosody.
Other data sets need to be explored to introduce other kinds of variability. It would
also be important to determine whether the distance measure provided by the diphone
maps correlates better with subjective perception of the mismatch between successive
diphones than more standard measures of spectral distance, such as various distance
measures between frames of cepstral coefficients.
</bodyText>
<sectionHeader confidence="0.706449" genericHeader="general terms">
Acknowledgments References
</sectionHeader>
<bodyText confidence="0.983340923076923">
Thanks to Stephen Isard of Edinburgh CSTR Allen, J., Hunnicutt, M. S., and Klatt, D. H.
and Linda Shockey of the Linguistics (1987). From text to speech: The MITalk
Department, Reading University for help system. Cambridge University Press.
with diphones and related matters. The Bladon, A., and Al-Bamerni, A. (1976).
author is grateful to John Local for &amp;quot;Coarticulation resistance in English.&amp;quot;
enthusiasm and help with phonetics. Any Journal of Phonetics, 4, 137-150.
remaining mistakes are the author&apos;s van den Bosch, A., and Daelemans, W.
responsibility. Thanks to the Laboratory of (1993). &amp;quot;Data-Oriented Methods for
Computer and Information Science, Grapheme-to-Phoneme Conversion.&amp;quot;
Helsinki University of Technology for Proceedings, 6th Conference of the European
making available their &apos;SOM_PAK&apos; software, Chapter of the ACL, Utrecht, April 1993.
used here with minor modifications. Durand, J. (1990). Generative and Non-Linear
Phonology. Longman, London.
</bodyText>
<page confidence="0.981063">
574
</page>
<note confidence="0.326257">
Cohen Nonsymbolic Phonetic Notation for Speech Synthesis
</note>
<reference confidence="0.996927392857143">
Ellis, E. M., and Robinson, A. J. (1993). &amp;quot;A
Phonetic Tactile Speech Listening
System.&amp;quot; Cambridge University
Engineering Department Technical
Report, CUED/F-INFENG/TR122, May.
Fritzke, B. (1993). &amp;quot;Growing Cell
Structures—A Self-organizing Network
for Unsupervised and supervised
Learning.&amp;quot; International Computer
Science Institute Technical Report
TR-93-026, May.
Goldsmith, J. (1990). Autosegmental and
Metrical Phonology. Blackwell, Oxford.
Huang, C. B. (1990). &amp;quot;Modelling Human
Vowel Identification Using Aspects of
Formant Trajectory and Context.&amp;quot; In
Speech Perception, Production and Linguistic
Structure, edited by Y. Tohkura,
E. Vatikiotis-Bateson, and Y. Sagisaka.
Proceedings, ATR workshop, Kyoto, Japan,
November 1990, IOS press, Oxford, UK.
Kelly, J., and Local, J. (1986). &amp;quot;Long-domain
resonance patterns in English.&amp;quot; In
Proceedings IEEE Speech In
Conference, Pub. No. 258, 77-82.
Klatt, D. H. (1982a). &amp;quot;Speech processing
strategies based on auditory models.&amp;quot; In
The Representation of Speech in the Peripheral
Auditory System, edited by R. Carlson and
B. Granstrom, Elsevier, Amsterdam.
Klatt, D. H. (1982b). &amp;quot;Prediction of
perceived phonetic distance from critical
band spectra: a first step.&amp;quot; In Proceedings
IEEE ICASSP-82, 1278-1281.
Klatt, D. H. (1987). &amp;quot;Review of
text-to-speech conversion for English.&amp;quot;
JASA 82(3), 737-793.
Kohonen, T. (1988). &amp;quot;The &apos;neural&apos; phonetic
typewriter.&amp;quot; IEEE Computer 21, 11-22.
Kohonen, T. (1989). Self-Organization and
Associative Memory. Springer Verlag, 3rd
ed.
Kohonen, T. (1990). &amp;quot;The Self-Organizing
Map.&amp;quot; IEEE Proceedings 78(9), 1464-1480.
Peterson, G., and Barney, H. (1952).
&amp;quot;Control methods used in a study of the
vowels.&amp;quot; JASA 24, 175-184.
Sammon, J. W. (1969). &amp;quot;A nonlinear
mapping for data structure analysis.&amp;quot;
IEEE Trans. Computers, C-18, 401-409.
Touretzky, D. S., Wheeler, D. W., and
Elvgren III, G. (1990). &amp;quot;Rules and Maps
III: Further Progress in Connectionist
Phonology,&amp;quot; School Of Computer Science,
Carnegie Mellon, Technical Report
CMU-CS-90-138
</reference>
<page confidence="0.99845">
575
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.530267">
<title confidence="0.9998745">Developing a Nonsymbolic Phonetic Notation for Speech Synthesis</title>
<author confidence="0.999696">Andrew Cohen</author>
<affiliation confidence="0.993801">University of Reading</affiliation>
<abstract confidence="0.971155235294118">The goal of the research presented here is to apply unsupervised neural network learning methods to some of the lower-level problems in speech synthesis currently performed by rule-based systems. The latter tend to be strongly influenced by notations developed by linguists (see figure 1 in Klatt (1987)), which were primarily devised to deal with written rather than spoken language. In general terms, what is needed in phonetics is a notation that captures information about ratios rather than absolute values, as is typically seen in biological systems. The notations derived here are based on an ordered pattern space that can be dealt with more easily by neural networks, and by systems involving a neural and symbolic component. Hence, the approach described here might also be useful in the design of a hybrid neural/symbolic system to operate in the speech synthesis domain. 1. Background and phonetic motivation Phonological and phonetic notations have been developed by linguists primarily as descriptive tools, using rewrite-rules operating on highly abstracted basic units derived from articulatory phonetics. Even some connectionist work has followed this tradition (Touretzky et al. 1990). The primary aim of these notations is explanation and understanding, and there are difficulties in incorporating them into systems with a practical aim such as deriving speech from text, which tend to be data-driven. One</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E M Ellis</author>
<author>A J Robinson</author>
</authors>
<title>A Phonetic Tactile Speech Listening System.&amp;quot;</title>
<date>1993</date>
<tech>Technical Report, CUED/F-INFENG/TR122,</tech>
<institution>Cambridge University Engineering Department</institution>
<contexts>
<context position="5335" citStr="Ellis and Robinson 1993" startWordPosition="815" endWordPosition="818">n space, by representing the N-dimensional pattern vectors on a 2D array of reference vectors in such a way that the resulting clusterings conform to an elastic surface, where neighboring units share similar reference vectors. This algorithm and Learning Vector Quantization (LVQ) are described in Kohonen (1990), which has practical advice for implementation, and in more theoretical detail in Kohonen (1989). It has been widely noted that 2D representations of speech are useful where there is a need to transmit information to humans at a phonetic level—for example, in tactile listening systems (Ellis and Robinson 1993). If a speech synthesis system has a phonetic interface or level of operation, it is then possible to introduce learning techniques for subsequent modules (e.g., those which calculate durations or an intonation contour) and to have an idea of what is happening, in phonetic terms, when things go wrong, and therefore how the training program or learning method may be adjusted. There is a long tradition of two-dimensional representations of formant data in attempts to classify vowels, going back at least to the study of Peterson and Barney (1952). Another type of advantage lies in the flexibility</context>
</contexts>
<marker>Ellis, Robinson, 1993</marker>
<rawString>Ellis, E. M., and Robinson, A. J. (1993). &amp;quot;A Phonetic Tactile Speech Listening System.&amp;quot; Cambridge University Engineering Department Technical Report, CUED/F-INFENG/TR122, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fritzke</author>
</authors>
<title>Growing Cell Structures—A Self-organizing Network for Unsupervised and supervised</title>
<date>1993</date>
<tech>Technical Report TR-93-026,</tech>
<institution>Learning.&amp;quot; International Computer Science Institute</institution>
<contexts>
<context position="6477" citStr="Fritzke 1993" startWordPosition="999" endWordPosition="1000">on and Barney (1952). Another type of advantage lies in the flexibility given by the very large dimensionality reductions achievable by Kohonen&apos;s technique. These reductions are possible even where the input pattern space may be only sparsely populated, yielding a flexible encoding with not too many degrees of freedom. It is possible for Kohonen&apos;s technique to work in 3D (3D maps have been produced by the author, but are more difficult to work with and are still undergoing evaluation). In 4D or above, interpretation becomes much more difficult. Refinements such as the Growing Cells technique (Fritzke 1993) might be preferable to a move to higher dimensionality, so as to retain transparency of the notation and a possible link to symbol-based stages of operation. Figure 1 shows a map resulting from applying the SOM algorithm to phoneme feature data. The following nine binary articulatory features were used: continuant, voiced, nasal, strident, grave, compact, vowel height(1), vowel height(2), and round. The features hl and h2 are used for height simply because there are three possibilities: 568 Cohen Nonsymbolic Phonetic Notation for Speech Synthesis f S sh ch J v z dh th nch r 1 Y in nk n b g w </context>
</contexts>
<marker>Fritzke, 1993</marker>
<rawString>Fritzke, B. (1993). &amp;quot;Growing Cell Structures—A Self-organizing Network for Unsupervised and supervised Learning.&amp;quot; International Computer Science Institute Technical Report TR-93-026, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Autosegmental and Metrical Phonology.</title>
<date>1990</date>
<location>Blackwell, Oxford.</location>
<contexts>
<context position="9286" citStr="Goldsmith 1990" startWordPosition="1468" endWordPosition="1470">ere. Kohonen refers to this type of kernel as a bubble because it relates to certain kinds of activity bubbles in laterally connected networks (see Kohonen 1989). 1 Thanks to John Local for providing the basis for the data. 569 Computational Linguistics Volume 21, Number 4 The analysis of these maps is at a phonemic level of description; this is a very abstract level compared to the phonetic descriptions typically used, which take into account much more of the context. However, the trend in recent phonology has been towards ever greater abstraction and more complex hierarchies of units (e.g., Goldsmith 1990; Durand 1990). The abstractness of phonemes makes them more difficult candidates for both recognition and synthesis, although most existing systems in the two fields perversely make use of a phoneme stage. If a phoneme stage is held to be essential (on grounds of parsimony, perhaps), then maps of the type shown in figure 1 may be one means to incorporate phonemes into a static or recurrent neural networkbased system in a more flexible fashion. However, the essential point is that trajectory across the map provides a bridge between the symbolic description of the data and the data itself. Robu</context>
</contexts>
<marker>Goldsmith, 1990</marker>
<rawString>Goldsmith, J. (1990). Autosegmental and Metrical Phonology. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Huang</author>
</authors>
<title>Modelling Human Vowel Identification Using Aspects of Formant Trajectory and Context.&amp;quot; In Speech Perception, Production and Linguistic Structure, edited by</title>
<date>1990</date>
<booktitle>Proceedings, ATR workshop,</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="12635" citStr="Huang (1990)" startWordPosition="2014" endWordPosition="2015">969). The latter is a supervised, error minimization procedure that maps the input vectors onto a lower dimensional space in such a way that the Euclidean distance between the endpoints is approximately preserved. It does not have the topographic property of the Kohonen map, but is useful in indicating the underlying form and orientation of the data in 2D. A number of maps showing clustering of similar sounds (formant vectors) have been obtained (Figures 2a—c show examples). Larger maps containing up to 1250 diphones have also been created. In contrast to the one-point representations used in Huang (1990) for vowels, the entire diphone is presented to the network for classification. The frame length (not more than 10ms) should mean that all perceptually 570 Cohen Nonsymbolic Phonetic Notation for Speech Synthesis aar . aag . aam . aaw aap . aay . aak aazh . aat . aas aash . aaj . aath . aal . aaz aad . aah . aaf . aang . aan . aadh aab aach aav Figure 2a Clustering of diphone data for aa-C. relevant information is captured in the formant trajectories. Maps based on acoustic tube data computed from the LPC coefficients have also been created, with much the same kind of results as seen in the fo</context>
</contexts>
<marker>Huang, 1990</marker>
<rawString>Huang, C. B. (1990). &amp;quot;Modelling Human Vowel Identification Using Aspects of Formant Trajectory and Context.&amp;quot; In Speech Perception, Production and Linguistic Structure, edited by Y. Tohkura, E. Vatikiotis-Bateson, and Y. Sagisaka. Proceedings, ATR workshop, Kyoto, Japan, November 1990, IOS press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kelly</author>
<author>J Local</author>
</authors>
<title>Long-domain resonance patterns in English.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings IEEE Speech In Conference, Pub. No.</booktitle>
<volume>258</volume>
<pages>77--82</pages>
<contexts>
<context position="3299" citStr="Kelly and Local 1986" startWordPosition="487" endWordPosition="490">transitions into and out of a consonant are identical, and that the same transition may be used in each CV combination, regardless of the larger phonetic environment. These assumptions need to be modified in a principled manner rather than by tables of exceptions. * Department of Cybernetics, University of Reading. E-mail: cybadc@cyber.reading.ac.uk C) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 It has been argued by phoneticians that articulatory models cannot account for all the variability found in natural speech (Bladon and Al-Bamerni 1976; Kelly and Local 1986). Therefore, there is a need to find ways of incorporating other sources of variability into synthetic speech, including, for example, the feedback a talker receives from the perception of their own voice. Evidence that such feedback affects speech is the degradation seen in the speech of persons with acquired deafness. One possible way to introduce this kind of variability is through the development of representations that encode (in a reduced dimensionality) a range of examples of the phenomenon to be accounted for. Formant data can be used to introduce a perceptual measure of similarity (se</context>
</contexts>
<marker>Kelly, Local, 1986</marker>
<rawString>Kelly, J., and Local, J. (1986). &amp;quot;Long-domain resonance patterns in English.&amp;quot; In Proceedings IEEE Speech In Conference, Pub. No. 258, 77-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Klatt</author>
</authors>
<title>Speech processing strategies based on auditory models.&amp;quot;</title>
<date>1982</date>
<booktitle>In The Representation of Speech in the Peripheral Auditory System, edited</booktitle>
<publisher>Elsevier,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="10413" citStr="Klatt 1982" startWordPosition="1654" endWordPosition="1655">des a bridge between the symbolic description of the data and the data itself. Robustness of mappings between domains (e.g., from text to phonemes) should be increased, since similar sounds (words) will have similar trajectories across the map. Clearly, it would be interesting to repeat the process with formant data to see if a similar 2D map can be formed. Formants are known to be important in perception, but do not in general correspond to a particular vocal tract configuration. Many papers have stressed the importance of formant peaks in speech perception, especially in the case of vowels (Klatt 1982a). Changes in formant frequency are more important in phonetic judgments than changes in formant amplitudes or bandwidths, or zeros in the spectrum (Klatt 1982b). The diphone approach embodies a signal-modeling, as opposed to a systemmodeling, viewpoint. Therefore, it is possible to incorporate sources of variability other than the purely articulatory into the learning procedure that derives the basic notation. Features such as accent-specific detail, stress and emotional quality are difficult to describe in purely articulatory terms, and, with current knowledge, cannot be given a re-usable, </context>
</contexts>
<marker>Klatt, 1982</marker>
<rawString>Klatt, D. H. (1982a). &amp;quot;Speech processing strategies based on auditory models.&amp;quot; In The Representation of Speech in the Peripheral Auditory System, edited by R. Carlson and B. Granstrom, Elsevier, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Klatt</author>
</authors>
<title>Prediction of perceived phonetic distance from critical band spectra: a first step.&amp;quot;</title>
<date>1982</date>
<booktitle>In Proceedings IEEE ICASSP-82,</booktitle>
<pages>1278--1281</pages>
<contexts>
<context position="10413" citStr="Klatt 1982" startWordPosition="1654" endWordPosition="1655">des a bridge between the symbolic description of the data and the data itself. Robustness of mappings between domains (e.g., from text to phonemes) should be increased, since similar sounds (words) will have similar trajectories across the map. Clearly, it would be interesting to repeat the process with formant data to see if a similar 2D map can be formed. Formants are known to be important in perception, but do not in general correspond to a particular vocal tract configuration. Many papers have stressed the importance of formant peaks in speech perception, especially in the case of vowels (Klatt 1982a). Changes in formant frequency are more important in phonetic judgments than changes in formant amplitudes or bandwidths, or zeros in the spectrum (Klatt 1982b). The diphone approach embodies a signal-modeling, as opposed to a systemmodeling, viewpoint. Therefore, it is possible to incorporate sources of variability other than the purely articulatory into the learning procedure that derives the basic notation. Features such as accent-specific detail, stress and emotional quality are difficult to describe in purely articulatory terms, and, with current knowledge, cannot be given a re-usable, </context>
</contexts>
<marker>Klatt, 1982</marker>
<rawString>Klatt, D. H. (1982b). &amp;quot;Prediction of perceived phonetic distance from critical band spectra: a first step.&amp;quot; In Proceedings IEEE ICASSP-82, 1278-1281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Klatt</author>
</authors>
<title>Review of text-to-speech conversion for English.&amp;quot;</title>
<date>1987</date>
<journal>JASA</journal>
<volume>82</volume>
<issue>3</issue>
<pages>737--793</pages>
<marker>Klatt, 1987</marker>
<rawString>Klatt, D. H. (1987). &amp;quot;Review of text-to-speech conversion for English.&amp;quot; JASA 82(3), 737-793.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>The &apos;neural&apos; phonetic typewriter.&amp;quot;</title>
<date>1988</date>
<journal>IEEE Computer</journal>
<volume>21</volume>
<pages>11--22</pages>
<contexts>
<context position="4642" citStr="Kohonen 1988" startWordPosition="709" endWordPosition="710"> of shell scripts and &apos;C&apos; programs; not all of the technical details of this system have been finalized, and it has not been formally tested. While formants have been made use of as training data (as well as acoustic tube data), as yet no use has been made of a formant synthesizer for creating the output speech, due to the need for handcrafting of values. At present, waveform segment concatenation is being used to explore a parametric duration model based on the kind of proximitybased notations described here. 2. Application of the SOM to phoneme data In outline, the Self-Organizing Map (SOM, Kohonen 1988) approximates to the probability density function of the input pattern space, by representing the N-dimensional pattern vectors on a 2D array of reference vectors in such a way that the resulting clusterings conform to an elastic surface, where neighboring units share similar reference vectors. This algorithm and Learning Vector Quantization (LVQ) are described in Kohonen (1990), which has practical advice for implementation, and in more theoretical detail in Kohonen (1989). It has been widely noted that 2D representations of speech are useful where there is a need to transmit information to h</context>
</contexts>
<marker>Kohonen, 1988</marker>
<rawString>Kohonen, T. (1988). &amp;quot;The &apos;neural&apos; phonetic typewriter.&amp;quot; IEEE Computer 21, 11-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>Self-Organization and Associative Memory.</title>
<date>1989</date>
<publisher>Springer Verlag,</publisher>
<note>3rd ed.</note>
<contexts>
<context position="5120" citStr="Kohonen (1989)" startWordPosition="782" endWordPosition="783">oximitybased notations described here. 2. Application of the SOM to phoneme data In outline, the Self-Organizing Map (SOM, Kohonen 1988) approximates to the probability density function of the input pattern space, by representing the N-dimensional pattern vectors on a 2D array of reference vectors in such a way that the resulting clusterings conform to an elastic surface, where neighboring units share similar reference vectors. This algorithm and Learning Vector Quantization (LVQ) are described in Kohonen (1990), which has practical advice for implementation, and in more theoretical detail in Kohonen (1989). It has been widely noted that 2D representations of speech are useful where there is a need to transmit information to humans at a phonetic level—for example, in tactile listening systems (Ellis and Robinson 1993). If a speech synthesis system has a phonetic interface or level of operation, it is then possible to introduce learning techniques for subsequent modules (e.g., those which calculate durations or an intonation contour) and to have an idea of what is happening, in phonetic terms, when things go wrong, and therefore how the training program or learning method may be adjusted. There i</context>
<context position="8833" citStr="Kohonen 1989" startWordPosition="1394" endWordPosition="1395"> were designed so that any phoneme (or syllable) may be uniquely specified as a cluster of features, without reference to specific units (segments such as phones, syllables, etc.)—any feature may run across unit boundaries. Figure 1 shows a 12 x 8 map created (as are all the following maps) with hexagonal connections in the lattice indicating which units are neighbors. A monotonically shrinking &apos;bubble&apos; neighborhood was used in all the maps shown here. Kohonen refers to this type of kernel as a bubble because it relates to certain kinds of activity bubbles in laterally connected networks (see Kohonen 1989). 1 Thanks to John Local for providing the basis for the data. 569 Computational Linguistics Volume 21, Number 4 The analysis of these maps is at a phonemic level of description; this is a very abstract level compared to the phonetic descriptions typically used, which take into account much more of the context. However, the trend in recent phonology has been towards ever greater abstraction and more complex hierarchies of units (e.g., Goldsmith 1990; Durand 1990). The abstractness of phonemes makes them more difficult candidates for both recognition and synthesis, although most existing system</context>
</contexts>
<marker>Kohonen, 1989</marker>
<rawString>Kohonen, T. (1989). Self-Organization and Associative Memory. Springer Verlag, 3rd ed.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>The Self-Organizing Map.&amp;quot;</title>
<date>1990</date>
<journal>IEEE Proceedings</journal>
<volume>78</volume>
<issue>9</issue>
<pages>1464--1480</pages>
<contexts>
<context position="5023" citStr="Kohonen (1990)" startWordPosition="767" endWordPosition="769">egment concatenation is being used to explore a parametric duration model based on the kind of proximitybased notations described here. 2. Application of the SOM to phoneme data In outline, the Self-Organizing Map (SOM, Kohonen 1988) approximates to the probability density function of the input pattern space, by representing the N-dimensional pattern vectors on a 2D array of reference vectors in such a way that the resulting clusterings conform to an elastic surface, where neighboring units share similar reference vectors. This algorithm and Learning Vector Quantization (LVQ) are described in Kohonen (1990), which has practical advice for implementation, and in more theoretical detail in Kohonen (1989). It has been widely noted that 2D representations of speech are useful where there is a need to transmit information to humans at a phonetic level—for example, in tactile listening systems (Ellis and Robinson 1993). If a speech synthesis system has a phonetic interface or level of operation, it is then possible to introduce learning techniques for subsequent modules (e.g., those which calculate durations or an intonation contour) and to have an idea of what is happening, in phonetic terms, when th</context>
<context position="8093" citStr="Kohonen, 1990" startWordPosition="1277" endWordPosition="1278">ght&apos; in that the features do result in a clustering of similar sounds such as stops, fricatives and nasals, as well as the more obvious separation between vowels and consonants. It is worth pointing out that neither the SOM nor the LVQ algorithm handles raw data (such as waveform values or image intensity values), but each operates on data such as spectral components or LPC coefficients that are themselves the output of a significant processing stage, and can justifiably be called features. The phoneme map is produced by a single Kohonen layer that self-organizes using the standard algorithm (Kohonen, 1990), taking as input nine articulatory features commonly used by phoneticians to describe the possible speech sounds. The features were designed so that any phoneme (or syllable) may be uniquely specified as a cluster of features, without reference to specific units (segments such as phones, syllables, etc.)—any feature may run across unit boundaries. Figure 1 shows a 12 x 8 map created (as are all the following maps) with hexagonal connections in the lattice indicating which units are neighbors. A monotonically shrinking &apos;bubble&apos; neighborhood was used in all the maps shown here. Kohonen refers t</context>
<context position="11303" citStr="Kohonen 1990" startWordPosition="1791" endWordPosition="1792">ble to incorporate sources of variability other than the purely articulatory into the learning procedure that derives the basic notation. Features such as accent-specific detail, stress and emotional quality are difficult to describe in purely articulatory terms, and, with current knowledge, cannot be given a re-usable, speaker-independent representation at all. 3. Application of the SOM to diphone formant data The formant data obtained (F1, F2, F3, F4 and AV; formant bandwidths were not used) in each frame were passed as a single vector to the SOM. (For a description of the SOM algorithm see Kohonen 1990, 1989). Some diphones would run over more frames than others, so shorter vectors were padded with zeros to make them up to the length of the longest. The average length of a diphone (unpadded) was about 30 frames; the bulk of this would naturally consist of steady-state rather than transitional data. However, the training sets in the maps shown were chosen so that the significant variance would lie in the transitional parts of the training vector, rather than in the less interesting steady-state portions. A similar type of experiment to that in section 1 can be carried out on diphone formant </context>
</contexts>
<marker>Kohonen, 1990</marker>
<rawString>Kohonen, T. (1990). &amp;quot;The Self-Organizing Map.&amp;quot; IEEE Proceedings 78(9), 1464-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Peterson</author>
<author>H Barney</author>
</authors>
<title>Control methods used in a study of the vowels.&amp;quot;</title>
<date>1952</date>
<journal>JASA</journal>
<volume>24</volume>
<pages>175--184</pages>
<contexts>
<context position="5884" citStr="Peterson and Barney (1952)" startWordPosition="904" endWordPosition="907">tic level—for example, in tactile listening systems (Ellis and Robinson 1993). If a speech synthesis system has a phonetic interface or level of operation, it is then possible to introduce learning techniques for subsequent modules (e.g., those which calculate durations or an intonation contour) and to have an idea of what is happening, in phonetic terms, when things go wrong, and therefore how the training program or learning method may be adjusted. There is a long tradition of two-dimensional representations of formant data in attempts to classify vowels, going back at least to the study of Peterson and Barney (1952). Another type of advantage lies in the flexibility given by the very large dimensionality reductions achievable by Kohonen&apos;s technique. These reductions are possible even where the input pattern space may be only sparsely populated, yielding a flexible encoding with not too many degrees of freedom. It is possible for Kohonen&apos;s technique to work in 3D (3D maps have been produced by the author, but are more difficult to work with and are still undergoing evaluation). In 4D or above, interpretation becomes much more difficult. Refinements such as the Growing Cells technique (Fritzke 1993) might </context>
</contexts>
<marker>Peterson, Barney, 1952</marker>
<rawString>Peterson, G., and Barney, H. (1952). &amp;quot;Control methods used in a study of the vowels.&amp;quot; JASA 24, 175-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Sammon</author>
</authors>
<title>A nonlinear mapping for data structure analysis.&amp;quot;</title>
<date>1969</date>
<journal>IEEE Trans. Computers,</journal>
<volume>18</volume>
<pages>401--409</pages>
<contexts>
<context position="12027" citStr="Sammon 1969" startWordPosition="1915" endWordPosition="1916"> them up to the length of the longest. The average length of a diphone (unpadded) was about 30 frames; the bulk of this would naturally consist of steady-state rather than transitional data. However, the training sets in the maps shown were chosen so that the significant variance would lie in the transitional parts of the training vector, rather than in the less interesting steady-state portions. A similar type of experiment to that in section 1 can be carried out on diphone formant data. For comparison, figure 2d shows the same data set as in figure 2c clustered by means of Sammon&apos;s mapping (Sammon 1969). The latter is a supervised, error minimization procedure that maps the input vectors onto a lower dimensional space in such a way that the Euclidean distance between the endpoints is approximately preserved. It does not have the topographic property of the Kohonen map, but is useful in indicating the underlying form and orientation of the data in 2D. A number of maps showing clustering of similar sounds (formant vectors) have been obtained (Figures 2a—c show examples). Larger maps containing up to 1250 diphones have also been created. In contrast to the one-point representations used in Huan</context>
</contexts>
<marker>Sammon, 1969</marker>
<rawString>Sammon, J. W. (1969). &amp;quot;A nonlinear mapping for data structure analysis.&amp;quot; IEEE Trans. Computers, C-18, 401-409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Touretzky</author>
<author>D W Wheeler</author>
<author>G Elvgren</author>
</authors>
<title>Rules and Maps III: Further Progress in Connectionist Phonology,&amp;quot;</title>
<date>1990</date>
<tech>Technical Report CMU-CS-90-138</tech>
<institution>School Of Computer Science, Carnegie Mellon,</institution>
<contexts>
<context position="1302" citStr="Touretzky et al. 1990" startWordPosition="192" endWordPosition="195">ere are based on an ordered pattern space that can be dealt with more easily by neural networks, and by systems involving a neural and symbolic component. Hence, the approach described here might also be useful in the design of a hybrid neural/symbolic system to operate in the speech synthesis domain. 1. Background and phonetic motivation Phonological and phonetic notations have been developed by linguists primarily as descriptive tools, using rewrite-rules operating on highly abstracted basic units derived from articulatory phonetics. Even some connectionist work has followed this tradition (Touretzky et al. 1990). The primary aim of these notations is explanation and understanding, and there are difficulties in incorporating them into systems with a practical aim such as deriving speech from text, which tend to be data-driven. One recent study claimed that introduction of linguistic knowledge degrades performance in grapheme-phoneme conversion (van den Bosch and Daelemans 1993). However, typical purely data-driven systems are opaque from a phonetic or phonological point of view. In order to handle many of the very hard problems remaining in speech synthesis, there is a need to develop a basic underlyi</context>
</contexts>
<marker>Touretzky, Wheeler, Elvgren, 1990</marker>
<rawString>Touretzky, D. S., Wheeler, D. W., and Elvgren III, G. (1990). &amp;quot;Rules and Maps III: Further Progress in Connectionist Phonology,&amp;quot; School Of Computer Science, Carnegie Mellon, Technical Report CMU-CS-90-138</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>