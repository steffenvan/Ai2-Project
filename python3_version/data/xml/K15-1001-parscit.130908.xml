<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999635">
A Coactive Learning View of Online Structured Prediction
in Statistical Machine Translation
</title>
<author confidence="0.73969">
Artem Sokolov and Stefan Riezler* Shay B. Cohen
</author>
<affiliation confidence="0.698042">
Computational Linguistics &amp; IWR* University of Edinburgh
</affiliation>
<address confidence="0.619231">
69120 Heidelberg, Germany Edinburgh EH8 9LE, UK
</address>
<email confidence="0.996593">
{sokolov,riezler}@cl.uni-heidelberg.de scohen@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.99736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928782608696">
We present a theoretical analysis of online
parameter tuning in statistical machine
translation (SMT) from a coactive learn-
ing view. This perspective allows us to
give regret and generalization bounds for
latent perceptron algorithms that are com-
mon in SMT, but fall outside of the stan-
dard convex optimization scenario. Coac-
tive learning also introduces the concept of
weak feedback, which we apply in a proof-
of-concept experiment to SMT, showing
that learning from feedback that consists
of slight improvements over predictions
leads to convergence in regret and transla-
tion error rate. This suggests that coactive
learning might be a viable framework for
interactive machine translation. Further-
more, we find that surrogate translations
replacing references that are unreachable
in the decoder search space can be inter-
preted as weak feedback and lead to con-
vergence in learning, if they admit an un-
derlying linear model.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997246408163265">
Online learning has become the tool of choice for
large scale machine learning scenarios. Compared
to batch learning, its advantages include memory
efficiency, due to parameter updates being per-
formed on the basis of single examples, and run-
time efficiency, where a constant number of passes
over the training sample is sufficient for conver-
gence (Bottou and Bousquet, 2004). Statistical
Machine Translation (SMT) has embraced the po-
tential of online learning, both to handle millions
of features and/or millions of data in parameter
tuning via online structured prediction (see Liang
et al. (2006) for seminal early work), and in in-
teractive learning from user post-edits (see Cesa-
Bianchi et al. (2008) for pioneering work on on-
line computer-assisted translation). Online learn-
ing algorithms can be given a theoretical analy-
sis in the framework of online convex optimiza-
tion (Shalev-Shwartz, 2012), however, the appli-
cation of online learning techniques to SMT sac-
rifices convexity because of latent derivation vari-
ables, and because of surrogate translations replac-
ing human references that are unreachable in the
decoder search space. For example, the objective
function actually optimized in Liang et al.’s (2006)
application of Collins’ (2002) structure perceptron
has been analyzed by Gimpel and Smith (2012)
as a non-convex ramp loss function (McAllester
and Keshet, 2011; Do et al., 2008; Collobert et al.,
2006). Since online convex optimization does not
provide convergence guarantees for the algorithm
of Liang et al. (2006), Gimpel and Smith (2012)
recommend CCCP (Yuille and Rangarajan, 2003)
instead for optimization, but fail to provide a the-
oretical analysis of Liang et al.’s (2006) actual al-
gorithm under the new objective.
The goal of this paper is to present an alternative
theoretical analysis of online learning algorithms
for SMT from the viewpoint of coactive learning
(Shivaswamy and Joachims, 2012). This frame-
work allows us to make three main contributions:
• Firstly, the proof techniques of Shivaswamy
and Joachims (2012) are a simple and elegant tool
for a theoretical analysis of perceptron-style al-
gorithms that date back to the perceptron mistake
bound of Novikoff (1962). These techniques pro-
vide an alternative to an online gradient descent
view of perceptron-style algorithms, and can eas-
ily be extended to obtain regret bounds for a la-
</bodyText>
<page confidence="0.822554">
1
</page>
<note confidence="0.98102">
Proceedings of the 19th Conference on Computational Language Learning, pages 1–11,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.915375">
tent perceptron algorithm at a rate of O( 1 ), with
</bodyText>
<equation confidence="0.857084">
√
T
</equation>
<bodyText confidence="0.999003142857143">
possible improvements by using re-scaling. This
bound can be directly used to derive generalization
guarantees for online and online-to-batch conver-
sions of the algorithm, based on well-known con-
centration inequalities. Our analysis covers the ap-
proach of Liang et al. (2006) and supersedes Sun
et al. (2013)’s analysis of the latent perceptron by
providing simpler proofs and by adding a general-
ization analysis. Furthermore, an online learning
framework such as coactive learning covers prob-
lems such as changing n-best lists after each up-
date that were explicitly excluded from the batch
analysis of Gimpel and Smith (2012) and consid-
ered fixed in the analysis of Sun et al. (2013).
</bodyText>
<listItem confidence="0.88495925">
• Our second contribution is an extension of
the online learning scenario in SMT to include a
notion of “weak feedback” for the latent percep-
tron: Coactive learning follows an online learning
</listItem>
<bodyText confidence="0.810753166666667">
protocol, where at each round t, the learner pre-
dicts a structured object yt for an input xt, and
the user corrects the learner by responding with
an improved, but not necessarily optimal, object
¯yt with respect to a utility function U. The key as-
set of coactive learning is the ability of the learner
to converge to predictions that are close to opti-
mal structures y∗t , although the utility function is
unknown to the learner, and only weak feedback
in form of slightly improved structures ¯yt is seen
in training. We present a proof-of-concept ex-
periment in which translation feedback of varying
grades is chosen from the n-best list of an “opti-
mal” model that has access to full information. We
show that weak feedback structures correspond to
improvements in TER (Snover et al., 2006) over
predicted structures, and that learning from weak
feedback minimizes regret and TER.
</bodyText>
<listItem confidence="0.998837714285714">
• Our third contribution is to show that cer-
tain practices of computing surrogate references
actually can be understood as a form of weak
feedback. Coactive learning decouples the learner
(performing prediction and updates) from the user
(providing feedback in form of an improved trans-
lation) so that we can compare different surro-
</listItem>
<bodyText confidence="0.947992851851852">
gacy modes as different ways of approximate util-
ity maximization. We show experimentally that
learning from surrogate “hope” derivations (Chi-
ang, 2012) minimizes regret and TER, thus fa-
voring surrogacy modes that admit an underly-
ing linear model, over “local” updates (Liang et
al., 2006) or “oracle” derivations (Sokolov et al.,
2013), for which learning does not converge.
It is important to note that the goal of our ex-
periments is not to present improvements of coac-
tive learning over the “optimal” full-information
model in terms of standard SMT performance. In-
stead, our goal is to present experiments that serve
as a proof-of-concept of the feasibility of coactive
learning from weak feedback for SMT, and to pro-
pose a new perspective on standard practices of
learning from surrogate translations. The rest of
this paper is organized as follows. After a review
of related work (Section 2), we present a latent
percpetron algorithm and analyze its convergence
and generalization properties (Section 3). Our first
set of experiments (Section 4.1) confirms our the-
oretical analysis by showing convergence in regret
and TER for learning from weak and strong feed-
back. Our second set of experiments (Section 4.2)
analyzes the relation of different surrogacy modes
to minimization of regret and TER.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998929620689655">
Our work builds on the framework of coactive
learning, introduced by Shivaswamy and Joachims
(2012). We extend their algorithms and proofs to
the area of SMT where latent variable models are
appropriate, and additionally present generaliza-
tion guarantees and an online-to-batch conversion.
Our theoretical analysis is easily extendable to the
full information case of Sun et al. (2013). We
also extend our own previous work (Sokolov et al.,
2015) with theory and experiments for online-to-
batch conversion, and with experiments on coac-
tive learning from surrogate translations.
Online learning has been applied for discrimi-
native training in SMT, based on perceptron-type
algorithms (Shen et al. (2004), Watanabe et al.
(2006), Liang et al. (2006), Yu et al. (2013), inter
alia), or large-margin approaches (Tillmann and
Zhang (2006), Watanabe et al. (2007), Chiang et
al. (2008), Chiang et al. (2009), Chiang (2012), in-
ter alia). The latest incarnations are able to handle
millions of features and millions of parallel sen-
tences (Simianer et al. (2012), Eidelmann (2012),
Watanabe (2012), Green et al. (2013), inter alia).
Most approaches rely on hidden derivation vari-
ables, use some form of surrogate references, and
involve n-best lists that change after each update.
Online learning from post-edits has mostly been
confined to “simulated post-editing” where inde-
pendently created human reference translations,
</bodyText>
<page confidence="0.988314">
2
</page>
<bodyText confidence="0.9991324375">
or post-edits on the output from similar SMT
systems, are used as for online learning (Cesa-
Bianchi et al. (2008), L´opez-Salcedo et al. (2012),
Mart´ınez-G´omez et al. (2012), Saluja et al. (2012),
Saluja and Zhang (2014), inter alia). Recent
approaches extend online parameter updating by
online phrase extraction (W¨aschle et al. (2013),
Bertoldi et al. (2014), Denkowski et al. (2014),
Green et al. (2014), inter alia). We exclude dy-
namic phrase table extension, which has shown to
be important in online learning for post-editing, in
our theoretical analysis (Denkowski et al., 2014).
Learning from weak feedback is related to bi-
nary response-based learning where a meaning
representation is “tried out” by iteratively generat-
ing system outputs, receiving feedback from world
interaction, and updating the model parameters.
Such world interaction consists of database access
in semantic parsing (Kwiatowski et al. (2013), Be-
rant et al. (2013), or Goldwasser and Roth (2013),
inter alia). Feedback in response-based learning
is given by a user accepting or rejecting system
predictions, but not by user corrections.
Lastly, feedback in form of numerical utility
values for actions is studied in the frameworks of
reinforcement learning (Sutton and Barto, 1998)
or in online learning with limited feedback, e.g.,
multi-armed bandit models (Cesa-Bianchi and Lu-
gosi, 2006). Our framework replaces quantitative
feedback with immediate qualitative feedback in
form of a structured object that improves upon the
utility of the prediction.
</bodyText>
<sectionHeader confidence="0.852428" genericHeader="method">
3 Coactive Learning for Online Latent
Structured Prediction
</sectionHeader>
<subsectionHeader confidence="0.999503">
3.1 Notation and Background
</subsectionHeader>
<bodyText confidence="0.999987866666667">
Let X denote a set of input examples, e.g.,
sentences, and let Y(x) denote a set of structured
outputs for x E X, e.g., translations. We define
Y = UxY(x). Furthermore, by H(x, y) we
denote a set of possible hidden derivations for a
structured output y E Y(x), e.g., for phrase-based
SMT, the hidden derivation is determined by a
phrase segmentation and a phrase alignment be-
tween source and target sentences. Every hidden
derivation h E H(x, y) deterministically identifies
an output y E Y(x). We define H = Ux,yH(x, y).
Let O: X xY xH —* Rd denote a feature function
that maps a triplet (x, y, h) to a d-dimensional
vector. For phrase-based SMT, we use 14 fea-
tures, defined by phrase translation probabilities,
</bodyText>
<construct confidence="0.338498">
Algorithm 1 Feedback-based Latent Perceptron
</construct>
<listItem confidence="0.975016625">
1: Initialize w ← 0
2: fort= 1,. . . ,Tdo
3: Observe xt
4: (yt, ht) ← arg max(y,h) wt φ(xt, y, h)
5: Obtain weak feedback ¯yt
6: if yt =6 ¯yt then
7: ¯ht ← arg maxh wt φ(xt, ¯yt, h)¯
8: wt+1 ← wt+Δ¯ht,ht W xt, ¯yt, ht)−φ(xt yt, ht))
</listItem>
<bodyText confidence="0.989254826086956">
language model probability, distance-based and
lexicalized reordering probabilities, and word
and phrase penalty. We assume that the fea-
ture function has a bounded radius, i.e. that
I IO(x, y, h)I I &lt; R for all x, y, h. By Oh,hl we
denote a distance function that is defined for any
h, h&apos; E H, and is used to scale the step size of
updates during learning. In our experiments, we
use the ordinary Euclidean distance between the
feature vectors of derivations. We assume a linear
model with fixed parameters w* such that each
input example is mapped to its correct deriva-
tion and structured output by using (y*, h*) =
arg maxyEY(x),hEW(x,y) w*TO(x, y, h). We define
for each given input x, its highest scoring deriva-
tion over all outputs Y(x) such that h(x; w) =
arg maxh,EW(x,y) maxyEY(x) wTO(x, y, h&apos;)
and the highest scoring derivation for
a given output y E Y(x) such that
h(x|y; w) = arg maxh,EW(x,y) wTO(x, y, h&apos;). In
the following theoretical exposition we assume
that the arg max operation can be computed
exactly.
</bodyText>
<subsectionHeader confidence="0.999463">
3.2 Feedback-based Latent Perceptron
</subsectionHeader>
<bodyText confidence="0.999897294117647">
We assume an online setting, in which examples
are presented one-by-one. The learner observes
an input xt, predicts an output structure yt, and
is presented with feedback ¯yt about its prediction,
which is used to make an update to an existing pa-
rameter vector. Algorithm 1 is called ”Feedback-
based Latent Perceptron” to stress the fact that
it only uses weak feedback to its predictions for
learning, but does not necessarily observe optimal
structures as in the full information case (Sun et
al., 2013). Learning from full information can be
recovered by setting the informativeness parame-
ter α to 1 in Equation (2) below, in which case
the feedback structure ¯yt equals the optimal struc-
ture yt *. Algorithm 1 differs from the algorithm
of Shivaswamy and Joachims (2012) by a joint
maximization over output structures y and hid-
</bodyText>
<page confidence="0.997068">
3
</page>
<bodyText confidence="0.999935222222222">
den derivations h in prediction (line 4), by choos-
ing a hidden derivation h¯ for the feedback struc-
ture y¯ (line 7), and by the use of the re-scaling
factor A¯ht,ht in the update (line 8), where ht =
h(xt|¯yt; wt) and ht = h(xt; wt) are the deriva-
tions of the feedback structure and the prediction
at time t, respectively. In our theoretical exposi-
tion, we assume that ¯yt is reachable in the search
space of possible outputs, that is, ¯yt E Y(xt).
</bodyText>
<subsectionHeader confidence="0.999888">
3.3 Feedback of Graded Utility
</subsectionHeader>
<bodyText confidence="0.999829">
The key in the theoretical analysis in Shivaswamy
and Joachims (2012) is the notion of a linear utility
function, determined by parameter vector w*, that
is unknown to the learner:
</bodyText>
<equation confidence="0.881162">
Uh(x, y) = w*Tφ(x, y, h).
</equation>
<bodyText confidence="0.999585333333333">
Upon a system prediction, the user approximately
maximizes utility, and returns an improved object
¯yt that has higher utility than the predicted yt s.t.
</bodyText>
<equation confidence="0.700218">
U(xt, ¯yt) &gt; U(xt, yt)
</equation>
<bodyText confidence="0.9997408">
where for given x E X, y E Y(x), and h* =
arg maxhEH(x,y) Uh(x, y), we define U(x, y) =
Uh∗(x, y) and drop the subscript unless h =� h*.
Importantly, the feedback is typically not the opti-
mal structure y*t that is defined as
</bodyText>
<equation confidence="0.9264375">
y*t = arg max U(xt, y).
yEY(xt)
</equation>
<bodyText confidence="0.9997656">
While not receiving optimal structures in training,
the learning goal is to predict objects with util-
ity close to optimal structures yt *. The regret that
is suffered by the algorithm when predicting ob-
ject yt instead of the optimal object y*t is
</bodyText>
<equation confidence="0.9913">
(U(xt, y*t ) − U(xt, yt)). (1)
</equation>
<bodyText confidence="0.999468545454545">
To quantify the amount of information in the
weak feedback, Shivaswamy and Joachims (2012)
define a notion of α-informative feedback, which
we generalize as follows for the case of latent
derivations. We assume that there exists a deriva-
tion ¯ht for the feedback structure ¯yt, such that
for all predictions yt, the (re-scaled) utility of the
weak feedback ¯yt is higher than the (re-scaled)
utility of the prediction yt by a fraction α of the
maximum possible utility range (under the given
utility model). Thus bt, ]¯ht, bh and for α E (0, 1]:
</bodyText>
<equation confidence="0.9925775">
At(xt,¯yt) − Uh(xt, yt)) X A¯ht,h
&gt; α(U(xt, y*t ) − U(xt, yt)) − ξt, (2)
</equation>
<bodyText confidence="0.999856333333333">
where ξt &gt; 0 are slack variables allowing for vio-
lations of (2) for given α. For slack ξt = 0, user
feedback is called strictly α-informative.
</bodyText>
<subsectionHeader confidence="0.99929">
3.4 Convergence Analysis
</subsectionHeader>
<bodyText confidence="0.999264777777778">
A central theoretical result in learning from weak
feedback is an analysis that shows that Algo-
rithm 1 minimizes an upper bound on the average
regret (1), despite the fact that optimal structures
are not used in learning:
Theorem 1. Let DT = PTt=1 A2¯ht,ht. Then the
average regret of the feedback-based latent per-
ceptron can be upper bounded for any α E (0, 1],
for any w* E Rd:
</bodyText>
<equation confidence="0.99772">
1
REGT : αT
</equation>
<bodyText confidence="0.99997092">
A proof for Theorem 1 is similar to the proof
of Shivaswamy and Joachims (2012) and the orig-
inal mistake bound for the perceptron of Novikoff
(1962).1 The theorem can be interpreted as fol-
lows: we expect lower average regret for higher
values of α; due to the dominant term T, regret
will approach the minimum of the accumulated
slack (in case feedback structures violate Equa-
tion (2)) or 0 (in case of strictly α-informative
feedback). The main difference between the above
result and the result of Shivaswamy and Joachims
(2012) is the term DT following from the re-
scaled distance of latent derivations. Their anal-
ysis is agnostic of latent derivations, and can be
recovered by setting this scaling factor to 1. This
yields DT = T, and thus recovers the main fac-
tor ADT = A1T in their regret bound. In our al-
gorithm, penalizing large distances of derivations
can help to move derivations ht closer to ¯ht, there-
fore decreasing DT as learning proceeds. Thus in
case DT &lt; T, our bound is better than the original
bound of Shivaswamy and Joachims (2012) for a
perceptron without re-scaling. As we will show
experimentally, re-scaling leads to a faster conver-
gence in practice.
</bodyText>
<subsectionHeader confidence="0.975462">
3.5 Generalization Analysis
</subsectionHeader>
<bodyText confidence="0.934954571428572">
Regret bounds measure how good the average pre-
diction of the current model is on the next example
in the given sequence, thus it seems plausible that
a low regret on a sequence of examples should im-
ply good generalization performance on the entire
domain of examples.
1Short proofs are provided in the appendix.
</bodyText>
<equation confidence="0.988942166666667">
1
REGT = T
T
X
t=1
2R11w*11
ξt +
α
XT
t=1
✓DT
T .
</equation>
<page confidence="0.930981">
4
</page>
<bodyText confidence="0.984446121212121">
Generalization for Online Learning. First we
present a generalization bound for the case of on-
line learning on a sequence of random examples,
based on generalization bounds for expected aver-
age regret as given by Cesa-Bianchi et al. (2004).
Let probabilities P and expectations E be de-
fined with respect to the fixed unknown underly-
ing distribution according to which all examples
are drawn. Furthermore, we bound our loss func-
tion `t = U(xt, y∗t ) − U(xt, yt) to [0, 1] by adding
a normalization factor 2R||w∗ ||s.t. REGT =
T Et 1 `t. Plugging the bound on REGT of The-
orem 1 directly into Proposition 1 of Cesa-Bianchi
et al. (2004) gives the following theorem:
Theorem 2. Let 0 &lt; δ &lt; 1, and let x1, ... , xT be
a sequence of examples that Algorithm 1 observes.
Then with probability at least 1 − δ,
Condition 1. Algorithm 1 has converged on train-
ing instances x1, ... , xT after K epochs if the
predictions on x1, ... , xT using the final weight
vector wT,K are the same as the predictions on
x1, ... , xT in the Kth epoch.
Denote by EX(`(x)) the expected loss on
unseen data when using wT,K where `(x) =
U(x, y∗) − U(x, y0), y∗ = arg maxy U(x, y) and
y0 = arg maxy maxh w&gt;T,Kφ(x, y, h). We can
now state the following result:
Theorem 3. Let 0 &lt; δ &lt; 1, and let x1, ... , xT
be a sample for the multiple-epoch perceptron al-
gorithm such that the algorithm converged on it
(Condition 1). Then, with probability at least 1−δ,
the expected loss of the feedback-based latent per-
ceptron satisfies:
</bodyText>
<equation confidence="0.99575312">
1
E[REGT] ≤ αT
2Rkw∗k
ξt +
α
1
EX(`(x)) ≤
αT
2Rkw∗k
ξt,K +
α
VIDT,K
T
T
t=1
√DT
T
T
t=1
+ 2||w∗||R�2 ln 1
δ .
�
8 ln 2
+ Rkw∗k T .
δ
</equation>
<bodyText confidence="0.995660384615385">
The generalization bound tells us how far the
expected average regret E[REGT] (or average
risk, in terms of Cesa-Bianchi et al. (2004)) is from
the average regret that we actually observe in a
specific instantiation of the algorithm.
Generalization for Online-to-Batch Conver-
sion. In practice, perceptron-type algorithms are
often applied in a batch learning scenario, i.e.,
the algorithm is applied for K epochs to a train-
ing sample of size T and then used for predic-
tion on an unseen test set (Freund and Schapire,
1999; Collins, 2002). The difference to the online
learning scenario is that we treat the multi-epoch
algorithm as an empirical risk minimizer that se-
lects a final weight vector wT,K whose expected
loss on unseen data we would like to bound. We
assume that the algorithm is fed with a sequence
of examples x1, ... , xT, and at each epoch k =
1, ... , K it makes a prediction yt,k. The correct
label is y∗t . For k = 1, ... , K and t = 1, ... , T,
let `t,k = U(xt, y∗t ) − U(xt, yt,k), and denote by
Δt,k and ξt,k the distance at epoch k for example
t, and the slack at epoch k for example t, respec-
tively. Finally, we denote by DT,K = �Tt=1 Δ2t,K,
and by wT,K the final weight vector returned after
K epochs. We state a condition of convergence2:
</bodyText>
<footnote confidence="0.9587975">
2This condition is too strong for large datasets. However,
we believe that a weaker condition based on ideas from the
</footnote>
<bodyText confidence="0.992958">
The theorem can be interpreted as a bound on
the generalization error (lefthand-side) by the em-
pirical error (the first two righthand-side terms)
and the variance caused by the finite sample (the
third term in the theorem). The result follows di-
rectly from McDiarmid’s concentration inequality.
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.985810388888889">
We used the LIG corpus3 which consists of 10,881
tuples of French-English post-edits (Potet et al.,
2012). The corpus is a subset of the news-
commentary dataset provided at WMT4 and con-
tains input French sentences, MT outputs, post-
edited outputs and English references. To prepare
SMT outputs for post-editing, the creators of the
corpus used their own WMT10 system (Potet et
al., 2010), based on the Moses phrase-based de-
coder (Koehn et al., 2007) with dense features.
We replicated a similar Moses system using the
same monolingual and parallel data: a 5-gram
language model was estimated with the KenLM
toolkit (Heafield, 2011) on news.en data (48.65M
sentences, 1.13B tokens), pre-processed with the
tools from the cdec toolkit (Dyer et al., 2010).
perceptron cycling theorem (Block and Levin, 1970; Gelfand
et al., 2010) should suffice to show a similar bound.
</bodyText>
<footnote confidence="0.997549">
3http://www-clips.imag.fr/geod/User/marion.potet/
index.php?page=download
4http://statmt.org/wmt10/translation-task.html
</footnote>
<page confidence="0.942787">
5
</page>
<figure confidence="0.99980037735849">
regret
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
α = 0.1
α = 0.5
α = 1.0
α = 0.1
α
= 0.5
α = 1.0
0.32
0.31
0.30
0.29
TER
0 4000 8000 12000 16000 20000
iterations
0 4000 8000 12000 16000 20000
iterations
0.32
0.31
0.30
0.29
TER
regret
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
scaled; α = 0.1
scaled; α = 0.5
scaled; α = 1.0
scaled; α = 0.1
scaled; α = 0.5
scaled; α = 1.0
0 4000 8000 12000 16000 20000
iterations
0 4000 8000 12000 16000 20000
iterations
</figure>
<figureCaption confidence="0.7782105">
Figure 1: Regret and TER vs. iterations for α-informative feedback ranging from weak (α = 0.1) to
strong (α = 1.0) informativeness, with (lower part) and without re-scaling (upper part).
</figureCaption>
<bodyText confidence="0.999964592592593">
Parallel data (europarl+news-comm, 1.64M sen-
tences) were similarly pre-processed and aligned
with fast align (Dyer et al., 2013). In all ex-
periments, training is started with the Moses de-
fault weights. The size of the n-best list, where
used, was set to 1,000. Irrespective of the use of
re-scaling in perceptron training, a constant learn-
ing rate of 10−5 was used for learning from simu-
lated feedback, and 10−4 for learning from surro-
gate translations.
Our experiments on online learning require
a random sequence of examples for learning.
Following the techniques described in Bertsekas
(2011) to generate random sequences for incre-
mental optimization, we compared cyclic order (K
epochs of T examples in fixed order), randomized
order (sampling datapoints with replacement), and
random shuffling of datapoints after each cycle,
and found nearly identical regret curves for all
three scenarios. In the following, all figures are
shown for sequences in the cyclic order, with re-
decoding after each update. Furthermore note that
in all three definitions of sequence, we never see
the fixed optimal feedback y*t in training, but in-
stead in general a different feedback structure ¯yt
(and a different prediction yt) every time we see
the same input xt.
</bodyText>
<subsectionHeader confidence="0.98921">
4.1 Idealized Weak and Strong Feedback
</subsectionHeader>
<bodyText confidence="0.9921895">
In a first experiment, we apply Algorithm 1 to
user feedback of varying utility grade. The goal of
</bodyText>
<table confidence="0.8609068">
strict (fit = 0) slack (fit &gt; 0)
# datapoints 5,725 1,155
TER(¯yt) &lt; TER(yt) 52.17% 32.55%
TER(¯yt) = TER(yt) 23.95% 20.52%
TER(¯yt) &gt; TER(yt) 23.88% 46.93%
</table>
<tableCaption confidence="0.94345">
Table 1: Improved utility vs. improved TER dis-
</tableCaption>
<bodyText confidence="0.974911708333333">
tance to human post-edits for α-informative feed-
back ¯yt compared to prediction yt using default
weights at α = 0.1.
this experiment is to confirm our theoretical anal-
ysis by showing convergence in regret for learn-
ing from weak and strong feedback. We select
feedback of varying grade by directly inspecting
the optimal w*, thus this feedback is idealized.
However, the experiment also has a realistic back-
ground since we show that α-informative feedback
corresponds to improvements under standard eval-
uation metrics such as lowercased and tokenized
TER, and that learning from weak and strong feed-
back leads to convergence in TER on test data.
For this experiment, the post-edit data from the
LIG corpus were randomly split into 3 subsets:
PE-train (6,881 sentences), PE-dev, and PE-test
(2,000 sentences each). PE-train was used for
our online learning experiments. PE-test was held
out for testing the algorithms’ progress on unseen
data. PE-dev was used to obtain w* to define the
utility model. This was done by MERT optimiza-
tion (Och, 2003) towards post-edits under the TER
target metric. Note that the goal of our experi-
</bodyText>
<page confidence="0.999062">
6
</page>
<table confidence="0.97082425">
% strictly α-informative
local 39.46%
filtered 47.73%
hope 83.30%
</table>
<tableCaption confidence="0.997803">
Table 2: α-informativeness of surrogacy modes.
</tableCaption>
<bodyText confidence="0.960061340425532">
ments is not to improve SMT performance over
any algorithm that has access to full information to
compute w*. Rather, we want to show that learn-
ing from weak feedback leads to convergence in
regret with respect to the optimal model, albeit
at a slower rate than learning from strong feed-
back. The feedback data in this experiment were
generated by searching the n-best list for transla-
tions that are α-informative at α E 10.1, 0.5, 1.01
(with possible non-zero slack). This is achieved
by scanning the n-best list output for every input
xt and returning the first ¯yt =� yt that satisfies
Equation (2).5 This setting can be thought of as an
idealized scenario where a user picks translations
from the n-best list that are considered improve-
ments under the optimal w*.
In order to verify that our notion of graded util-
ity corresponds to a realistic concept of graded
translation quality, we compared improvements in
utility to improved TER distance to human post-
edits. Table 1 shows that for predictions under
default weights, we obtain strictly α-informative
(for α = 0.1) feedback for 5,725 out of 6,881
datapoints in PE-train. These feedback structures
improve utility per definition, and they also yield
better TER distance to post-edits in the majority
of cases. A non-negative slack has to be used in
1,155 datapoins. Here the majority of feedback
structures do not improve TER distance.
Convergence results for different learning sce-
narios are shown in Figure 1. The left upper part
of Figure 1 shows average utility regret against
iterations for a setup without re-scaling, i.e., set-
ting A¯h,h = 1 in the definition of α-informative
feedback (Equation (2)) and in the update of Al-
gorithm 1 (line 8). As predicted by our regret
analysis, higher α leads to faster convergence, but
all three curves converge towards a minimal re-
gret. Also, the difference between the curves for
5Note that feedback provided in this way might be
stronger than required at a particular value of α since for all
β ≥ α, strictly β-informative feedback is also strictly α-
informative. On the other hand, because of the limited size of
the n-best list, we cannot assume strictly α-informative user
feedback with zero slack fit. In experiments where updates
are only done if feedback is strictly α-informative we found
similar convergence behavior.
</bodyText>
<figure confidence="0.812384">
1 2 3 4 5 6 7 8 9 10
epochs
</figure>
<figureCaption confidence="0.999634">
Figure 3: Average loss Et on heldout and train data.
</figureCaption>
<bodyText confidence="0.999974121212121">
α = 0.1 and α = 1.0 is much smaller than a fac-
tor of ten. As expected from the correspondence of
α-informative feedback to improvements in TER,
similar relations are obtained when plotting TER
scores on test data for training from weak feed-
back at different utility grades. This is shown in
the right upper part of Figure 1.
The left lower part of Figure 1 shows average
utility regret plotted against iterations for a setup
that uses re-scaling. We define A¯ht,h by the E2-
distance between the feature vectors O(xt, ¯yt, ¯
of the derivation of the feedback structure and the
feature vector O(xt, yt, ht) of the derivation of the
predicted structure. We see that the curves for all
grades of feedback converge faster than the corre-
sponding curves for un-scaled feedback shown in
the upper part Figure 1. Furthermore, as shown in
the right lower part of Figure 1, TER is decreased
on test data as well at a faster rate.6
Lastly, we present an experimental validation of
the online-to-batch application of our algorithm.
That is, we would like to evaluate predictions that
use the final weight vector wT,K by comparing the
generalization error with the empirical error stated
in Theorem 3. The standard way to do this is to
compare the average loss on heldout data with the
the average loss on the training sequence. Fig-
ure 3 shows these results for models trained on
α-informative feedback of α E 10.1, 0.5, 1.01 for
10 epochs. Similar to the online learning setup,
higher α results in faster convergence. Further-
more, curves for training and heldout evaluation
converge at the same rate.
</bodyText>
<subsectionHeader confidence="0.983197">
4.2 Feedback from Surrogate Translations
</subsectionHeader>
<bodyText confidence="0.992584">
In this section, we present experiments on learn-
ing from real human post-edits. The goal of
this experiment is to investigate whether the stan-
</bodyText>
<footnote confidence="0.663866333333333">
6We also conducted online-to-batch experiments for sim-
ulated feedback at α E {0.1, 0.5, 1.0}. Similar to the online
learning setup, higher α results in faster convergence.
</footnote>
<bodyText confidence="0.945409666666667">
test, α = 0.1
test, α = 0.5
test, α = 1.0
train, α = 0.1
train, α = 0.5
train, α = 1.0
</bodyText>
<figure confidence="0.995128666666667">
average loss it 0.25
0.20
0.15
0.10
0.05
ht)
</figure>
<page confidence="0.578414">
7
</page>
<figure confidence="0.9998428125">
regret 1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.00
0.35
α = 0.1
α = 1.0
oracles
local
filtered
hope
0.34
0.33
TER
0.32
α = 0.1
α = 1.0
oracles
local
filtered
hope
0.31
0.30
0.29
0 4000 8000 12000 16000 20000
iterations
0 4000 8000 12000 16000 20000
iterations
</figure>
<figureCaption confidence="0.999954">
Figure 2: Regret and TER for online learning from oracles, local, filtered, and hope surrogates.
</figureCaption>
<bodyText confidence="0.999976351351351">
dard practices for extracting feedback from ob-
served user post-edits for discriminative SMT can
be matched with the modeling assumptions of
the coactive learning framework. The custom-
ary practice in discriminative learning for SMT is
to replace observed user translations by surrogate
translations since the former are often not reach-
able in the search space of the SMT decoder. In
our case, only 29% of the post-edits in the LIG-
corpus were reachable by the decoder. We com-
pare four heuristics of generating surrogate trans-
lations: oracles are generated using the lattice or-
acle approach of Sokolov et al. (2013) which re-
turns the closest path in the decoder search graph
as reachable surrogate translation.7 A local sur-
rogate y˜ is chosen from the n-best list of the
linear model as the translation that achieves the
best TER score with respect to the actual post-
edit y: y˜ = arg miny,En-best(xt;wt) TER(y&apos;, y).
This corresponds to the local update mode of
Liang et al. (2006). A filtered surrogate trans-
lation y˜ is found by scanning down the n-best
list, and accepting the first translation as feed-
back that improves TER score with respect to the
human post-edit y over the 1-best prediction yt
of the linear model: TER(˜y, y) &lt; TER(yt, y).
Finally, a hope surrogate is chosen from the n-
best list as the translation that jointly maximizes
model score under the linear model and nega-
tive TER score with respect to the human post-
edit: y˜ = arg maxy,En-best(xt;wt)(−TER(y&apos;, y) +
wt φ(xt, y&apos;, h)). This corresponds to what Chi-
ang (2012) termed “hope derivations”. Informally,
oracles are model-agnostic, as they can pick a
surrogate even from outside of the n-best list;
local is constrained to the n-best list, though
still ignoring the ordering according to the linear
</bodyText>
<footnote confidence="0.925333">
7While the original algorithm is designed to maximize the
BLEU score of the returned path, we tuned its two free pa-
rameters to maximize TER.
</footnote>
<bodyText confidence="0.996389111111111">
model; finally, filtered and hope represent dif-
ferent ways of letting the model score influence
the selected surrogate.
As shown in Figure 2, regret and TER de-
crease with the increased amount of information
about the assumed linear model that is induced by
the surrogate translations: Learning from oracle
surrogates does not converge in regret and TER.
The local surrogates extracted from 1,000-best
lists still do not make effective use of the linear
model, while filtered surrogates enforce an im-
provement over the prediction under TER towards
the human post-edit, and improve convergence in
learning. Empirically, convergence is achieved
only for hope surrogates that jointly maximize
negative TER and linear model score, with a con-
vergence behavior that is very similar to learning
from weak α-informative feedback at α ^_ 0.1.
We quantify this in Table 2 where we see that the
improvement in TER over the prediction that holds
for any hope derivation, corresponds to an im-
provement in α-informativeness: hope surrogates
are strictly α-informative in 83.3% of the cases
in our experiment, whereas we find a correspon-
dence to strict α-informativeness only in 45.74%
or 39.46% of the cases for filtered and local
surrogates, respectively.
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999973454545454">
We presented a theoretical analysis of online
learning for SMT from a coactive learning per-
spective. This viewpoint allowed us to give regret
and generalization bounds for perceptron-style on-
line learners that fall outside the convex opti-
mization scenario because of latent variables and
changing feedback structures. We introduced the
concept of weak feedback into online learning for
SMT, and provided proof-of-concept experiments
whose goal was to show that learning from weak
feedback converges to minimal regret, albeit at a
</bodyText>
<page confidence="0.995264">
8
</page>
<bodyText confidence="0.999940476190476">
slower rate than learning from strong feedback.
Furthermore, we showed that the SMT standard
of learning from surrogate hope derivations can to
be interpreted as a search for weak improvements
under the assumed linear model. This justifies
the importance of admitting an underlying linear
model in computing surrogate derivations from a
coactive learning perspective.
Finally, we hope that our analysis motivates fur-
ther work in which the idea of learning from weak
feedback is taken a step further. For example,
our results could perhaps be strengthened by ap-
plying richer feature sets or dynamic phrase table
extension in experiments on interactive SMT. Our
theory would support a new post-editing scenario
where users pick translations from the n-best list
that they consider improvements over the predic-
tion. Furthermore, it would be interesting to see if
“light” post-edits that are better reachable and eas-
ier elicitable than “full” post-edits provide a strong
enough signal for learning.
</bodyText>
<sectionHeader confidence="0.989008" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.977733666666667">
This research was supported in part by DFG
grant RI-2221/2-1 “Grounding Statistical Machine
Translation in Perception and Action.”
</bodyText>
<subsectionHeader confidence="0.732896">
Appendix: Proofs of Theorems
Proof of Theorem 1
</subsectionHeader>
<bodyText confidence="0.939527">
Proof. First we bound w&gt;T+1wT+1 from above:
</bodyText>
<equation confidence="0.996128857142857">
&gt; &gt;
wT+1wT+1 = wT wT
+ 2w&gt; �φ(xT, ¯yT, ¯hT) − φ(xT, yT, hT)�A¯hT ,hT
T
+ (φ(xT, ¯yT, ¯hT) − φ(xT, yT, hT))&gt;A¯hT ,hT
WxT, ¯yT,¯hT) − φ(xT, yT, hT))A¯hT ,hT
≤ w&gt;T wT + 4R2A2¯hT ,hT ≤ 4R2DT. (3)
</equation>
<bodyText confidence="0.922698">
The first equality uses the update rule from Algorithm
1. The second uses the fact that w&gt;T (φ(xT, ¯yT, ¯hT) −
φ(xT, yT, hT)) ≤ 0 by definition of (yT, hT) in Algo-
rithm 1. By assumption kφ(x, y, h)k ≤ R, ∀x, y, h and
by the triangle inequality, kφ(x, y, h) − φ(x, y0, h0)k ≤
kφ(x, y, h)k + kφ(x, y0, h0)k ≤ 2R. Finally, DT =
PTt=1 A2¯ht,ht by definition, and the last inequality follows
by induction.
The connection to average regret is as follows:
The first equality again uses the update rule from Algorithm
1. The second follows by induction. The last equality applies
the definition of utility.
</bodyText>
<equation confidence="0.305729">
Next we upper bound the utility difference:
A¯ht,ht (U¯ht(xt, ¯yt) − Uht(xt, yt))
√
≤ kw∗kkwT+1k ≤ kw∗k2R DT. (5)
</equation>
<bodyText confidence="0.339854">
The first inequality follows from applying the Cauchy-
Schwartz inequality w&gt;T+1w∗ ≤ kw∗kkwT+1k to Equa-
tion (4). The seond follows from applying Equation (3) to
</bodyText>
<equation confidence="0.555775">
qkwT+1k = w&gt; T +1wT+1.
</equation>
<bodyText confidence="0.67391">
The final result is obtained simply by lower bounding
Equation (5) using the assumption in Equation (2).
</bodyText>
<figure confidence="0.334438">
√
kw∗k2R DT
T
A¯ht,ht (U¯ht(xt, ¯yt) − Uht(xt, yt))
= α T REGT − T ξt.
X
t=1
Proof of Theorem 3
Proof. The theorem can be shown by an application of Mc-
Diarmid’s concentration inequality:
Theorem 4 (McDiarmid, 1989). Let Z1, ... , Zm be a set
of random variables taking value in a set Z. Further, let
</figure>
<equation confidence="0.745859727272727">
f : Zm → R be a function that satisfies for all i and
z1, ... , zm, z0i ∈ Z:
|f(z1, ... ,zi, ... , zm)
− f(z1, ... , z0i,..., zm) |≤ c, (6)
for some c. Then for all 1 &gt; 0,
P(|f − E(f) |&gt; E) ≤ 2 exp(− 212). (7)
mc
Let f be the average loss for predicting yt on example xt
PT
in epoch K: f(x1, ... , xT) = REGT,K = 1 t=1 `t,K.
T
</equation>
<bodyText confidence="0.679117">
Because of the convergence condition (Condition 1), `t,K =
</bodyText>
<sectionHeader confidence="0.271812" genericHeader="conclusions">
PT
</sectionHeader>
<bodyText confidence="0.843032">
`(xt). The expectation of f is E(f) = 1 t=1 E[`t,k] =
</bodyText>
<equation confidence="0.99444375">
T
PT
1 t=1 E[`(xt)] = EX(`(x)).
T
</equation>
<bodyText confidence="0.99903825">
The first and second term on the righthand-side of Theo-
rem 3 follow from upper bounding REGT in the Kth epochs,
using Theorem 1. The third term is derived by calculating c
in Equation (6) as follows:
</bodyText>
<equation confidence="0.609962125">
|f(x1, ... , xt, ... , xT) − f(x1,... ,x0 t,. . . , xT)|
1 T
`0t,K |=  |T
X
t=1
(U(xt, y∗t ) − U(xt, yt)) −
T
X
t=1
≥ α
ξt
T
X
t=1
T
X
t=1
X
≥
t=1
=  |1 XT
T Xt=1 1
`t,K − T
t=1
�`t,K − `0 � |
t,K
&gt;
wT+1 w∗ = w
+ A¯hT ,hT WxT, ¯yT, ¯hT)) − φ(xT, yT, hT))&gt;w∗
(|`t,k |+ |`0t,K|) ≤ 4Rkw∗k
T = c.
&gt;
T w∗
T
X
t=1
≤ 1
T
A¯ht,ht Wxt, ¯yt, ¯ht) − φ(xt, yt, ht))&gt;w∗
A¯ht,ht (U¯ht(xt, ¯yt) − Uht(xt, yt)). (4)
=
T
X
t=1
=
T
X
t=1
</equation>
<bodyText confidence="0.972688">
The first inequality uses the triangle inequality; the sec-
ond uses the upper bound |`t,k |≤ 2R||w∗||. Setting the
righthand-side of Equation (7) to at least δ and solving for 1,
using c, concludes the proof.
</bodyText>
<page confidence="0.997118">
9
</page>
<sectionHeader confidence="0.996262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871817307693">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, Seattle, WA.
Nicola Bertoldi, Patrick Simianer, Mauro Cettolo,
Katharina W¨aschle, Marcello Federico, and Stefan
Riezler. 2014. Online adaptation to post-edits for
phrase-based statistical machine translation. Ma-
chine Translation, 29:309–339.
Dimitri P. Bertsekas. 2011. Incremental gradient,
subgradient, and proximal methods for convex op-
timization: A survey. In Suvrit Sra, Sebastian
Nowozin, and Stephen J. Wright, editors, Optimiza-
tion for Machine Learning. MIT Press.
Henry D. Block and Simon A. Levin. 1970. On the
boundedness of an iterative procedure for solving
a system of linear inequalities. Proceedings of the
American Mathematical Society, 26(2):229–235.
Leon Bottou and Olivier Bousquet. 2004. Large scale
online learning. In NIPS, Vancouver, Canada.
Nicol`o Cesa-Bianchi and G`abor Lugosi. 2006. Predic-
tion, Learning, and Games. Cambridge University
Press.
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gen-
tile. 2004. On the generalization ablility of on-line
learning algorithms. IEEE Transactions on Infor-
mation Theory, 50(9):2050–2057.
Nicol`o Cesa-Bianchi, Gabriele Reverberi, and San-
dor Szedmak. 2008. Online learning algorithms
for computer-assisted translation. Technical report,
SMART (www.smart-project.eu).
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, Waikiki, HA.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, Boulder, CO.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 12:1159–1187.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and exper-
iments with perceptron algorithms. In EMNLP,
Philadelphia, PA.
Ronan Collobert, Fabian Sinz, Jason Weston, and Leon
Bottou. 2006. Trading convexity for scalability. In
ICML, Pittsburgh, PA.
Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
EACL, Gothenburg, Sweden.
Chuong B. Do, Quoc Le, and Choon Hui Teo. 2008.
Tighter bounds for structured estimation. In NIPS,
Vancouver, Canada.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan T¨ure, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL, Uppsala, Sweden.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In NAACL, Atlanta, GA.
Vladimir Eidelmann. 2012. Optimization strategies
for online large-margin learning in machine transla-
tion. In WMT, Montreal, Canada.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Journal of Machine Learning Research, 37:277–
296.
Andrew E. Gelfand, Yutian Chen, Max Welling, and
Laurens van der Maaten. 2010. On herding and the
perceptron cycling theorem. In NIPS, Vancouver,
Canada.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
NAACL, Montreal, Canada.
Dan Goldwasser and Dan Roth. 2013. Learning from
natural instructions. Machine Learning, 94(2):205–
232.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In CHI, Paris, France.
Spence Green, Sida I. Wang, Jason Chuang, Jeffrey
Heer, Sebastian Schuster, and Christopher D. Man-
ning. 2014. Human effort and machine learnabil-
ity in computer aided translation. In EMNLP, Doha,
Qatar.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT, Edinburgh, UK.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Prague, Czech Republic.
Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In EMNLP, Seattle,
WA.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL, Sydney, Australia.
</reference>
<page confidence="0.949281">
10
</page>
<reference confidence="0.999904096774194">
Francisco-Javier L´opez-Salcedo, Germ´an Sanchis-
Trilles, and Francisco Casacuberta. 2012. Online
learning of log-linear weights in interactive machine
translation. In IberSpeech, Madrid, Spain.
Pascual Martinez-G´omez, Germ´an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45(9):3193–
3202.
David McAllester and Joseph Keshet. 2011. General-
ization bounds and consistency for latent structural
probit and ramp loss. In NIPS, Granada, Spain.
Colin McDiarmid. 1989. On the method of bounded
differences. Surveys in combinatorics, 141(1):148–
188.
Albert B.J. Novikoff. 1962. On convergence proofs on
perceptrons. Symposium on the Mathematical The-
ory of Automata, 12:615–622.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In NAACL, Edmon-
ton, Canada.
Marion Potet, Laurent Besacier, and Herv´e Blanchon.
2010. The LIG machine translation system for
WMT 2010. In WMT, Upsala, Sweden.
Marion Potet, Emanuelle Esperanc¸a-Rodier, Laurent
Besacier, and Herv´e Blanchon. 2012. Collection
of a large database of French-English SMT output
corrections. In LREC, Istanbul, Turkey.
Avneesh Saluja and Ying Zhang. 2014. On-
line discriminative learning for machine translation
with binary-valued feedback. Machine Translation,
28:69–90.
Avneesh Saluja, Ian Lane, and Ying Zhang. 2012.
Machine translation with binary feedback: A large-
margin approach. In AMTA, San Diego, CA.
Shai Shalev-Shwartz. 2012. Online learning and on-
line convex optimization. Foundations and Trends
in Machine Learning, 4(2):107–194.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
NAACL, Boston, MA.
Pannaga Shivaswamy and Thorsten Joachims. 2012.
Online structured prediction via coactive learning.
In ICML, Edinburgh, UK.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT.
In ACL, Jeju, Korea.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, Cambridge, MA.
Artem Sokolov, Guillaume Wisniewski, and Franc¸ois
Yvon. 2013. Lattice BLEU oracles in machine
translation. Transactions on Speech and Language
Processing, 10(4):18.
Artem Sokolov, Stefan Riezler, and Shay B. Cohen.
2015. Coactive learning for interactive machine
translation. In ICML Workshop on Machine Learn-
ing for Interactive Systems (MLIS), Lille, France.
Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013.
Latent structured perceptrons for large scale learn-
ing with hidden information. IEEE Transactions
on Knowledge and Data Engineering, 25(9):2064–
2075.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning. An Introduction. The MIT
Press.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In COLING-ACL, Sydney, Australia.
Katharina W¨aschle, Patrick Simianer, Nicola Bertoldi,
Stefan Riezler, and Marcello Federico. 2013. Gen-
erative and discriminative methods for online adap-
tation in SMT. In MT Summit, Nice, France.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2006. NTT statistical machine
translation for IWSLT 2006. In IWSLT, Kyoto,
Japan.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In EMNLP,
Prague, Czech Republic.
Taro Watanabe. 2012. Optimized online rank learn-
ing for machine translation. In NAACL, Montreal,
Canada.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In EMNLP, Seattle,
WA.
Alan Yuille and Anand Rangarajan. 2003. The
concave-convex procedure. Neural Computation,
15:915–936.
</reference>
<page confidence="0.999484">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953230">
<title confidence="0.999191">A Coactive Learning View of Online Structured in Statistical Machine Translation</title>
<author confidence="0.999715">Sokolov B Cohen</author>
<affiliation confidence="0.99794">Linguistics &amp; of Edinburgh</affiliation>
<address confidence="0.99999">69120 Heidelberg, Germany Edinburgh EH8 9LE, UK</address>
<email confidence="0.997688">scohen@inf.ed.ac.uk</email>
<abstract confidence="0.998057541666667">We present a theoretical analysis of online parameter tuning in statistical machine translation (SMT) from a coactive learning view. This perspective allows us to give regret and generalization bounds for latent perceptron algorithms that are common in SMT, but fall outside of the standard convex optimization scenario. Coactive learning also introduces the concept of weak feedback, which we apply in a proofof-concept experiment to SMT, showing that learning from feedback that consists of slight improvements over predictions leads to convergence in regret and translation error rate. This suggests that coactive learning might be a viable framework for interactive machine translation. Furthermore, we find that surrogate translations replacing references that are unreachable in the decoder search space can be interpreted as weak feedback and lead to convergence in learning, if they admit an underlying linear model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="9618" citStr="Berant et al. (2013)" startWordPosition="1501" endWordPosition="1505">t al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the prediction. 3 Coactive</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Patrick Simianer</author>
<author>Mauro Cettolo</author>
<author>Katharina W¨aschle</author>
<author>Marcello Federico</author>
<author>Stefan Riezler</author>
</authors>
<title>Online adaptation to post-edits for phrase-based statistical machine translation.</title>
<date>2014</date>
<booktitle>Machine Translation,</booktitle>
<pages>29--309</pages>
<marker>Bertoldi, Simianer, Cettolo, W¨aschle, Federico, Riezler, 2014</marker>
<rawString>Nicola Bertoldi, Patrick Simianer, Mauro Cettolo, Katharina W¨aschle, Marcello Federico, and Stefan Riezler. 2014. Online adaptation to post-edits for phrase-based statistical machine translation. Machine Translation, 29:309–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri P Bertsekas</author>
</authors>
<title>Incremental gradient, subgradient, and proximal methods for convex optimization: A survey.</title>
<date>2011</date>
<booktitle>Optimization for Machine Learning.</booktitle>
<editor>In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="22881" citStr="Bertsekas (2011)" startWordPosition="3861" endWordPosition="3862">caling (upper part). Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013). In all experiments, training is started with the Moses default weights. The size of the n-best list, where used, was set to 1,000. Irrespective of the use of re-scaling in perceptron training, a constant learning rate of 10−5 was used for learning from simulated feedback, and 10−4 for learning from surrogate translations. Our experiments on online learning require a random sequence of examples for learning. Following the techniques described in Bertsekas (2011) to generate random sequences for incremental optimization, we compared cyclic order (K epochs of T examples in fixed order), randomized order (sampling datapoints with replacement), and random shuffling of datapoints after each cycle, and found nearly identical regret curves for all three scenarios. In the following, all figures are shown for sequences in the cyclic order, with redecoding after each update. Furthermore note that in all three definitions of sequence, we never see the fixed optimal feedback y*t in training, but instead in general a different feedback structure ¯yt (and a differ</context>
</contexts>
<marker>Bertsekas, 2011</marker>
<rawString>Dimitri P. Bertsekas. 2011. Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry D Block</author>
<author>Simon A Levin</author>
</authors>
<title>On the boundedness of an iterative procedure for solving a system of linear inequalities.</title>
<date>1970</date>
<journal>Proceedings of the American Mathematical Society,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="21442" citStr="Block and Levin, 1970" startWordPosition="3617" endWordPosition="3620"> contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4http://statmt.org/wmt10/translation-task.html 5 regret 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 α = 0.1 α = 0.5 α = 1.0 α = 0.1 α = 0.5 α = 1.0 0.32 0.31 0.30 0.29 TER 0 4000 8000 12000 16000 20000 iterations 0 4000 8000 12000 16000 20000 iterations 0.32 0.31 0.30 0.29 TER regret 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 0 4000 8000 12000 16000 2</context>
</contexts>
<marker>Block, Levin, 1970</marker>
<rawString>Henry D. Block and Simon A. Levin. 1970. On the boundedness of an iterative procedure for solving a system of linear inequalities. Proceedings of the American Mathematical Society, 26(2):229–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Bottou</author>
<author>Olivier Bousquet</author>
</authors>
<title>Large scale online learning.</title>
<date>2004</date>
<booktitle>In NIPS,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1627" citStr="Bottou and Bousquet, 2004" startWordPosition="239" endWordPosition="242">n. Furthermore, we find that surrogate translations replacing references that are unreachable in the decoder search space can be interpreted as weak feedback and lead to convergence in learning, if they admit an underlying linear model. 1 Introduction Online learning has become the tool of choice for large scale machine learning scenarios. Compared to batch learning, its advantages include memory efficiency, due to parameter updates being performed on the basis of single examples, and runtime efficiency, where a constant number of passes over the training sample is sufficient for convergence (Bottou and Bousquet, 2004). Statistical Machine Translation (SMT) has embraced the potential of online learning, both to handle millions of features and/or millions of data in parameter tuning via online structured prediction (see Liang et al. (2006) for seminal early work), and in interactive learning from user post-edits (see CesaBianchi et al. (2008) for pioneering work on online computer-assisted translation). Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices c</context>
</contexts>
<marker>Bottou, Bousquet, 2004</marker>
<rawString>Leon Bottou and Olivier Bousquet. 2004. Large scale online learning. In NIPS, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>G`abor Lugosi</author>
</authors>
<title>Prediction, Learning, and Games.</title>
<date>2006</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10044" citStr="Cesa-Bianchi and Lugosi, 2006" startWordPosition="1565" endWordPosition="1569">utputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the prediction. 3 Coactive Learning for Online Latent Structured Prediction 3.1 Notation and Background Let X denote a set of input examples, e.g., sentences, and let Y(x) denote a set of structured outputs for x E X, e.g., translations. We define Y = UxY(x). Furthermore, by H(x, y) we denote a set of possible hidden derivations for a structured output y E Y(x), e.g., for phrase-based SMT, the hidden derivation is determined by a phrase segmentatio</context>
</contexts>
<marker>Cesa-Bianchi, Lugosi, 2006</marker>
<rawString>Nicol`o Cesa-Bianchi and G`abor Lugosi. 2006. Prediction, Learning, and Games. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolo Cesa-Bianchi</author>
<author>Alex Conconi</author>
<author>Claudio Gentile</author>
</authors>
<title>On the generalization ablility of on-line learning algorithms.</title>
<date>2004</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>50</volume>
<issue>9</issue>
<contexts>
<context position="17563" citStr="Cesa-Bianchi et al. (2004)" startWordPosition="2900" endWordPosition="2903">eneralization Analysis Regret bounds measure how good the average prediction of the current model is on the next example in the given sequence, thus it seems plausible that a low regret on a sequence of examples should imply good generalization performance on the entire domain of examples. 1Short proofs are provided in the appendix. 1 REGT = T T X t=1 2R11w*11 ξt + α XT t=1 ✓DT T . 4 Generalization for Online Learning. First we present a generalization bound for the case of online learning on a sequence of random examples, based on generalization bounds for expected average regret as given by Cesa-Bianchi et al. (2004). Let probabilities P and expectations E be defined with respect to the fixed unknown underlying distribution according to which all examples are drawn. Furthermore, we bound our loss function `t = U(xt, y∗t ) − U(xt, yt) to [0, 1] by adding a normalization factor 2R||w∗ ||s.t. REGT = T Et 1 `t. Plugging the bound on REGT of Theorem 1 directly into Proposition 1 of Cesa-Bianchi et al. (2004) gives the following theorem: Theorem 2. Let 0 &lt; δ &lt; 1, and let x1, ... , xT be a sequence of examples that Algorithm 1 observes. Then with probability at least 1 − δ, Condition 1. Algorithm 1 has converged</context>
<context position="19084" citStr="Cesa-Bianchi et al. (2004)" startWordPosition="3206" endWordPosition="3209">rg maxy U(x, y) and y0 = arg maxy maxh w&gt;T,Kφ(x, y, h). We can now state the following result: Theorem 3. Let 0 &lt; δ &lt; 1, and let x1, ... , xT be a sample for the multiple-epoch perceptron algorithm such that the algorithm converged on it (Condition 1). Then, with probability at least 1−δ, the expected loss of the feedback-based latent perceptron satisfies: 1 E[REGT] ≤ αT 2Rkw∗k ξt + α 1 EX(`(x)) ≤ αT 2Rkw∗k ξt,K + α VIDT,K T T t=1 √DT T T t=1 + 2||w∗||R�2 ln 1 δ . � 8 ln 2 + Rkw∗k T . δ The generalization bound tells us how far the expected average regret E[REGT] (or average risk, in terms of Cesa-Bianchi et al. (2004)) is from the average regret that we actually observe in a specific instantiation of the algorithm. Generalization for Online-to-Batch Conversion. In practice, perceptron-type algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire, 1999; Collins, 2002). The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector wT,K whose expected loss on unseen data</context>
</contexts>
<marker>Cesa-Bianchi, Conconi, Gentile, 2004</marker>
<rawString>Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. 2004. On the generalization ablility of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050–2057.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicol`o Cesa-Bianchi</author>
<author>Gabriele Reverberi</author>
<author>Sandor Szedmak</author>
</authors>
<title>Online learning algorithms for computer-assisted translation.</title>
<date>2008</date>
<tech>Technical report, SMART (www.smart-project.eu).</tech>
<marker>Cesa-Bianchi, Reverberi, Szedmak, 2008</marker>
<rawString>Nicol`o Cesa-Bianchi, Gabriele Reverberi, and Sandor Szedmak. 2008. Online learning algorithms for computer-assisted translation. Technical report, SMART (www.smart-project.eu).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<location>Waikiki, HA.</location>
<contexts>
<context position="8134" citStr="Chiang et al. (2008)" startWordPosition="1276" endWordPosition="1279"> an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In EMNLP, Waikiki, HA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation. In NAACL,</title>
<date>2009</date>
<location>Boulder, CO.</location>
<contexts>
<context position="8156" citStr="Chiang et al. (2009)" startWordPosition="1280" endWordPosition="1283">nversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learnin</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In NAACL, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--1159</pages>
<contexts>
<context position="6091" citStr="Chiang, 2012" startWordPosition="953" endWordPosition="955">provements in TER (Snover et al., 2006) over predicted structures, and that learning from weak feedback minimizes regret and TER. • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user (providing feedback in form of an improved translation) so that we can compare different surrogacy modes as different ways of approximate utility maximization. We show experimentally that learning from surrogate “hope” derivations (Chiang, 2012) minimizes regret and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2013), for which learning does not converge. It is important to note that the goal of our experiments is not to present improvements of coactive learning over the “optimal” full-information model in terms of standard SMT performance. Instead, our goal is to present experiments that serve as a proof-of-concept of the feasibility of coactive learning from weak feedback for SMT, and to propose a new perspective on standa</context>
<context position="8171" citStr="Chiang (2012)" startWordPosition="1284" endWordPosition="1285">cal analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi </context>
<context position="31497" citStr="Chiang (2012)" startWordPosition="5331" endWordPosition="5333">esponds to the local update mode of Liang et al. (2006). A filtered surrogate translation y˜ is found by scanning down the n-best list, and accepting the first translation as feedback that improves TER score with respect to the human post-edit y over the 1-best prediction yt of the linear model: TER(˜y, y) &lt; TER(yt, y). Finally, a hope surrogate is chosen from the nbest list as the translation that jointly maximizes model score under the linear model and negative TER score with respect to the human postedit: y˜ = arg maxy,En-best(xt;wt)(−TER(y&apos;, y) + wt φ(xt, y&apos;, h)). This corresponds to what Chiang (2012) termed “hope derivations”. Informally, oracles are model-agnostic, as they can pick a surrogate even from outside of the n-best list; local is constrained to the n-best list, though still ignoring the ordering according to the linear 7While the original algorithm is designed to maximize the BLEU score of the returned path, we tuned its two free parameters to maximize TER. model; finally, filtered and hope represent different ways of letting the model score influence the selected surrogate. As shown in Figure 2, regret and TER decrease with the increased amount of information about the assumed</context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learning Research, 12:1159–1187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="19487" citStr="Collins, 2002" startWordPosition="3274" endWordPosition="3275"> + α VIDT,K T T t=1 √DT T T t=1 + 2||w∗||R�2 ln 1 δ . � 8 ln 2 + Rkw∗k T . δ The generalization bound tells us how far the expected average regret E[REGT] (or average risk, in terms of Cesa-Bianchi et al. (2004)) is from the average regret that we actually observe in a specific instantiation of the algorithm. Generalization for Online-to-Batch Conversion. In practice, perceptron-type algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire, 1999; Collins, 2002). The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector wT,K whose expected loss on unseen data we would like to bound. We assume that the algorithm is fed with a sequence of examples x1, ... , xT, and at each epoch k = 1, ... , K it makes a prediction yt,k. The correct label is y∗t . For k = 1, ... , K and t = 1, ... , T, let `t,k = U(xt, y∗t ) − U(xt, yt,k), and denote by Δt,k and ξt,k the distance at epoch k for example t, and the slack at epoch k for example t, respectively. Finally, we de</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Fabian Sinz</author>
<author>Jason Weston</author>
<author>Leon Bottou</author>
</authors>
<title>Trading convexity for scalability.</title>
<date>2006</date>
<booktitle>In ICML,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2673" citStr="Collobert et al., 2006" startWordPosition="400" endWordPosition="403">be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joachims, 2012). This framework allows us to make three main contributions: • Firstly, the proof techniques of </context>
</contexts>
<marker>Collobert, Sinz, Weston, Bottou, 2006</marker>
<rawString>Ronan Collobert, Fabian Sinz, Jason Weston, and Leon Bottou. 2006. Trading convexity for scalability. In ICML, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
</authors>
<title>Learning from post-editing: Online model adaptation for statistical machine translation.</title>
<date>2014</date>
<booktitle>In EACL, Gothenburg,</booktitle>
<contexts>
<context position="9059" citStr="Denkowski et al. (2014)" startWordPosition="1417" endWordPosition="1420">some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter al</context>
</contexts>
<marker>Denkowski, Dyer, Lavie, 2014</marker>
<rawString>Michael Denkowski, Chris Dyer, and Alon Lavie. 2014. Learning from post-editing: Online model adaptation for statistical machine translation. In EACL, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuong B Do</author>
<author>Quoc Le</author>
<author>Choon Hui Teo</author>
</authors>
<title>Tighter bounds for structured estimation. In NIPS,</title>
<date>2008</date>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2648" citStr="Do et al., 2008" startWordPosition="396" endWordPosition="399">g algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joachims, 2012). This framework allows us to make three main contributions: • Firstly,</context>
</contexts>
<marker>Do, Le, Teo, 2008</marker>
<rawString>Chuong B. Do, Quoc Le, and Choon Hui Teo. 2008. Tighter bounds for structured estimation. In NIPS, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan T¨ure</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<location>Uppsala,</location>
<marker>Dyer, Lopez, Ganitkevitch, Weese, T¨ure, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan T¨ure, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM model 2. In NAACL,</title>
<date>2013</date>
<location>Atlanta, GA.</location>
<contexts>
<context position="22414" citStr="Dyer et al., 2013" startWordPosition="3782" endWordPosition="3785">ns 0 4000 8000 12000 16000 20000 iterations 0.32 0.31 0.30 0.29 TER regret 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 0 4000 8000 12000 16000 20000 iterations 0 4000 8000 12000 16000 20000 iterations Figure 1: Regret and TER vs. iterations for α-informative feedback ranging from weak (α = 0.1) to strong (α = 1.0) informativeness, with (lower part) and without re-scaling (upper part). Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013). In all experiments, training is started with the Moses default weights. The size of the n-best list, where used, was set to 1,000. Irrespective of the use of re-scaling in perceptron training, a constant learning rate of 10−5 was used for learning from simulated feedback, and 10−4 for learning from surrogate translations. Our experiments on online learning require a random sequence of examples for learning. Following the techniques described in Bertsekas (2011) to generate random sequences for incremental optimization, we compared cyclic order (K epochs of T examples in fixed order), randomi</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In NAACL, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelmann</author>
</authors>
<title>Optimization strategies for online large-margin learning in machine translation.</title>
<date>2012</date>
<booktitle>In WMT,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="8326" citStr="Eidelmann (2012)" startWordPosition="1309" endWordPosition="1310">eory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches </context>
</contexts>
<marker>Eidelmann, 2012</marker>
<rawString>Vladimir Eidelmann. 2012. Optimization strategies for online large-margin learning in machine translation. In WMT, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>37</volume>
<pages>296</pages>
<contexts>
<context position="19471" citStr="Freund and Schapire, 1999" startWordPosition="3270" endWordPosition="3273">1 EX(`(x)) ≤ αT 2Rkw∗k ξt,K + α VIDT,K T T t=1 √DT T T t=1 + 2||w∗||R�2 ln 1 δ . � 8 ln 2 + Rkw∗k T . δ The generalization bound tells us how far the expected average regret E[REGT] (or average risk, in terms of Cesa-Bianchi et al. (2004)) is from the average regret that we actually observe in a specific instantiation of the algorithm. Generalization for Online-to-Batch Conversion. In practice, perceptron-type algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire, 1999; Collins, 2002). The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector wT,K whose expected loss on unseen data we would like to bound. We assume that the algorithm is fed with a sequence of examples x1, ... , xT, and at each epoch k = 1, ... , K it makes a prediction yt,k. The correct label is y∗t . For k = 1, ... , K and t = 1, ... , T, let `t,k = U(xt, y∗t ) − U(xt, yt,k), and denote by Δt,k and ξt,k the distance at epoch k for example t, and the slack at epoch k for example t, respectively</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Journal of Machine Learning Research, 37:277– 296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew E Gelfand</author>
<author>Yutian Chen</author>
<author>Max Welling</author>
<author>Laurens van der Maaten</author>
</authors>
<title>On herding and the perceptron cycling theorem.</title>
<date>2010</date>
<booktitle>In NIPS,</booktitle>
<location>Vancouver, Canada.</location>
<marker>Gelfand, Chen, Welling, van der Maaten, 2010</marker>
<rawString>Andrew E. Gelfand, Yutian Chen, Max Welling, and Laurens van der Maaten. 2010. On herding and the perceptron cycling theorem. In NIPS, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation. In NAACL,</title>
<date>2012</date>
<location>Montreal, Canada.</location>
<contexts>
<context position="2567" citStr="Gimpel and Smith (2012)" startWordPosition="382" endWordPosition="385">t al. (2008) for pioneering work on online computer-assisted translation). Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joach</context>
<context position="4475" citStr="Gimpel and Smith (2012)" startWordPosition="683" endWordPosition="686">sible improvements by using re-scaling. This bound can be directly used to derive generalization guarantees for online and online-to-batch conversions of the algorithm, based on well-known concentration inequalities. Our analysis covers the approach of Liang et al. (2006) and supersedes Sun et al. (2013)’s analysis of the latent perceptron by providing simpler proofs and by adding a generalization analysis. Furthermore, an online learning framework such as coactive learning covers problems such as changing n-best lists after each update that were explicitly excluded from the batch analysis of Gimpel and Smith (2012) and considered fixed in the analysis of Sun et al. (2013). • Our second contribution is an extension of the online learning scenario in SMT to include a notion of “weak feedback” for the latent perceptron: Coactive learning follows an online learning protocol, where at each round t, the learner predicts a structured object yt for an input xt, and the user corrects the learner by responding with an improved, but not necessarily optimal, object ¯yt with respect to a utility function U. The key asset of coactive learning is the ability of the learner to converge to predictions that are close to </context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In NAACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Learning from natural instructions.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<volume>94</volume>
<issue>2</issue>
<pages>232</pages>
<contexts>
<context position="9649" citStr="Goldwasser and Roth (2013)" startWordPosition="1507" endWordPosition="1510"> al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the prediction. 3 Coactive Learning for Online Latent Str</context>
</contexts>
<marker>Goldwasser, Roth, 2013</marker>
<rawString>Dan Goldwasser and Dan Roth. 2013. Learning from natural instructions. Machine Learning, 94(2):205– 232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Jeffrey Heer</author>
<author>Christopher D Manning</author>
</authors>
<title>The efficacy of human post-editing for language translation. In CHI,</title>
<date>2013</date>
<location>Paris, France.</location>
<contexts>
<context position="8364" citStr="Green et al. (2013)" startWordPosition="1313" endWordPosition="1316">atch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by on</context>
</contexts>
<marker>Green, Heer, Manning, 2013</marker>
<rawString>Spence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human post-editing for language translation. In CHI, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Sida I Wang</author>
<author>Jason Chuang</author>
<author>Jeffrey Heer</author>
<author>Sebastian Schuster</author>
<author>Christopher D Manning</author>
</authors>
<title>Human effort and machine learnability in computer aided translation. In EMNLP,</title>
<date>2014</date>
<location>Doha, Qatar.</location>
<contexts>
<context position="9080" citStr="Green et al. (2014)" startWordPosition="1421" endWordPosition="1424">ferences, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in resp</context>
</contexts>
<marker>Green, Wang, Chuang, Heer, Schuster, Manning, 2014</marker>
<rawString>Spence Green, Sida I. Wang, Jason Chuang, Jeffrey Heer, Sebastian Schuster, and Christopher D. Manning. 2014. Human effort and machine learnability in computer aided translation. In EMNLP, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries. In WMT,</title>
<date>2011</date>
<location>Edinburgh, UK.</location>
<contexts>
<context position="21270" citStr="Heafield, 2011" startWordPosition="3593" endWordPosition="3594">G corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4http://statmt.org/wmt10/translation-task.html 5 regret 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 α = 0.1 α = 0.5 α = 1.0 α = 0.1 α = 0.5 α = 1.0 0.32 0.31 0.30 0.29 TER 0 4000 8000 12000 16000 20000 iterations 0 4000 8000 12000 16000 20000 iterations 0.32 0.31 0.30 0.29 TER regret</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In WMT, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Birch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="21088" citStr="Koehn et al., 2007" startWordPosition="3563" endWordPosition="3566">e terms) and the variance caused by the finite sample (the third term in the theorem). The result follows directly from McDiarmid’s concentration inequality. 4 Experiments We used the LIG corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4http://statmt.org/wmt10/translation-task.html 5 regret 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 α </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Birch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Birch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="9596" citStr="Kwiatowski et al. (2013)" startWordPosition="1497" endWordPosition="1500">ase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the </context>
</contexts>
<marker>Kwiatowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In EMNLP, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In COLINGACL,</booktitle>
<location>Sydney, Australia.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In COLINGACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco-Javier L´opez-Salcedo</author>
<author>Germ´an SanchisTrilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online learning of log-linear weights in interactive machine translation. In IberSpeech,</title>
<date>2012</date>
<location>Madrid,</location>
<marker>L´opez-Salcedo, SanchisTrilles, Casacuberta, 2012</marker>
<rawString>Francisco-Javier L´opez-Salcedo, Germ´an SanchisTrilles, and Francisco Casacuberta. 2012. Online learning of log-linear weights in interactive machine translation. In IberSpeech, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascual Martinez-G´omez</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online adaptation strategies for statistical machine translation in postediting scenarios.</title>
<date>2012</date>
<journal>Pattern Recognition,</journal>
<volume>45</volume>
<issue>9</issue>
<pages>3202</pages>
<marker>Martinez-G´omez, Sanchis-Trilles, Casacuberta, 2012</marker>
<rawString>Pascual Martinez-G´omez, Germ´an Sanchis-Trilles, and Francisco Casacuberta. 2012. Online adaptation strategies for statistical machine translation in postediting scenarios. Pattern Recognition, 45(9):3193– 3202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McAllester</author>
<author>Joseph Keshet</author>
</authors>
<title>Generalization bounds and consistency for latent structural probit and ramp loss.</title>
<date>2011</date>
<booktitle>In NIPS,</booktitle>
<location>Granada,</location>
<contexts>
<context position="2631" citStr="McAllester and Keshet, 2011" startWordPosition="392" endWordPosition="395"> translation). Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joachims, 2012). This framework allows us to make three main contribu</context>
</contexts>
<marker>McAllester, Keshet, 2011</marker>
<rawString>David McAllester and Joseph Keshet. 2011. Generalization bounds and consistency for latent structural probit and ramp loss. In NIPS, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin McDiarmid</author>
</authors>
<title>On the method of bounded differences. Surveys in combinatorics,</title>
<date>1989</date>
<volume>141</volume>
<issue>1</issue>
<pages>188</pages>
<contexts>
<context position="36300" citStr="McDiarmid, 1989" startWordPosition="6124" endWordPosition="6125"> Next we upper bound the utility difference: A¯ht,ht (U¯ht(xt, ¯yt) − Uht(xt, yt)) √ ≤ kw∗kkwT+1k ≤ kw∗k2R DT. (5) The first inequality follows from applying the CauchySchwartz inequality w&gt;T+1w∗ ≤ kw∗kkwT+1k to Equation (4). The seond follows from applying Equation (3) to qkwT+1k = w&gt; T +1wT+1. The final result is obtained simply by lower bounding Equation (5) using the assumption in Equation (2). √ kw∗k2R DT T A¯ht,ht (U¯ht(xt, ¯yt) − Uht(xt, yt)) = α T REGT − T ξt. X t=1 Proof of Theorem 3 Proof. The theorem can be shown by an application of McDiarmid’s concentration inequality: Theorem 4 (McDiarmid, 1989). Let Z1, ... , Zm be a set of random variables taking value in a set Z. Further, let f : Zm → R be a function that satisfies for all i and z1, ... , zm, z0i ∈ Z: |f(z1, ... ,zi, ... , zm) − f(z1, ... , z0i,..., zm) |≤ c, (6) for some c. Then for all 1 &gt; 0, P(|f − E(f) |&gt; E) ≤ 2 exp(− 212). (7) mc Let f be the average loss for predicting yt on example xt PT in epoch K: f(x1, ... , xT) = REGT,K = 1 t=1 `t,K. T Because of the convergence condition (Condition 1), `t,K = PT `(xt). The expectation of f is E(f) = 1 t=1 E[`t,k] = T PT 1 t=1 E[`(xt)] = EX(`(x)). T The first and second term on the righ</context>
</contexts>
<marker>McDiarmid, 1989</marker>
<rawString>Colin McDiarmid. 1989. On the method of bounded differences. Surveys in combinatorics, 141(1):148– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert B J Novikoff</author>
</authors>
<title>On convergence proofs on perceptrons.</title>
<date>1962</date>
<booktitle>Symposium on the Mathematical Theory of Automata,</booktitle>
<pages>12--615</pages>
<contexts>
<context position="3457" citStr="Novikoff (1962)" startWordPosition="525" endWordPosition="526">angarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joachims, 2012). This framework allows us to make three main contributions: • Firstly, the proof techniques of Shivaswamy and Joachims (2012) are a simple and elegant tool for a theoretical analysis of perceptron-style algorithms that date back to the perceptron mistake bound of Novikoff (1962). These techniques provide an alternative to an online gradient descent view of perceptron-style algorithms, and can easily be extended to obtain regret bounds for a la1 Proceedings of the 19th Conference on Computational Language Learning, pages 1–11, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics tent perceptron algorithm at a rate of O( 1 ), with √ T possible improvements by using re-scaling. This bound can be directly used to derive generalization guarantees for online and online-to-batch conversions of the algorithm, based on well-known concentration in</context>
<context position="15904" citStr="Novikoff (1962)" startWordPosition="2611" endWordPosition="2612">, user feedback is called strictly α-informative. 3.4 Convergence Analysis A central theoretical result in learning from weak feedback is an analysis that shows that Algorithm 1 minimizes an upper bound on the average regret (1), despite the fact that optimal structures are not used in learning: Theorem 1. Let DT = PTt=1 A2¯ht,ht. Then the average regret of the feedback-based latent perceptron can be upper bounded for any α E (0, 1], for any w* E Rd: 1 REGT : αT A proof for Theorem 1 is similar to the proof of Shivaswamy and Joachims (2012) and the original mistake bound for the perceptron of Novikoff (1962).1 The theorem can be interpreted as follows: we expect lower average regret for higher values of α; due to the dominant term T, regret will approach the minimum of the accumulated slack (in case feedback structures violate Equation (2)) or 0 (in case of strictly α-informative feedback). The main difference between the above result and the result of Shivaswamy and Joachims (2012) is the term DT following from the rescaled distance of latent derivations. Their analysis is agnostic of latent derivations, and can be recovered by setting this scaling factor to 1. This yields DT = T, and thus recov</context>
</contexts>
<marker>Novikoff, 1962</marker>
<rawString>Albert B.J. Novikoff. 1962. On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata, 12:615–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In NAACL,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="24924" citStr="Och, 2003" startWordPosition="4195" endWordPosition="4196">formative feedback corresponds to improvements under standard evaluation metrics such as lowercased and tokenized TER, and that learning from weak and strong feedback leads to convergence in TER on test data. For this experiment, the post-edit data from the LIG corpus were randomly split into 3 subsets: PE-train (6,881 sentences), PE-dev, and PE-test (2,000 sentences each). PE-train was used for our online learning experiments. PE-test was held out for testing the algorithms’ progress on unseen data. PE-dev was used to obtain w* to define the utility model. This was done by MERT optimization (Och, 2003) towards post-edits under the TER target metric. Note that the goal of our experi6 % strictly α-informative local 39.46% filtered 47.73% hope 83.30% Table 2: α-informativeness of surrogacy modes. ments is not to improve SMT performance over any algorithm that has access to full information to compute w*. Rather, we want to show that learning from weak feedback leads to convergence in regret with respect to the optimal model, albeit at a slower rate than learning from strong feedback. The feedback data in this experiment were generated by searching the n-best list for translations that are α-in</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In NAACL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Potet</author>
<author>Laurent Besacier</author>
<author>Herv´e Blanchon</author>
</authors>
<title>The LIG machine translation system for WMT</title>
<date>2010</date>
<booktitle>In WMT, Upsala,</booktitle>
<contexts>
<context position="21026" citStr="Potet et al., 2010" startWordPosition="3552" endWordPosition="3555">hand-side) by the empirical error (the first two righthand-side terms) and the variance caused by the finite sample (the third term in the theorem). The result follows directly from McDiarmid’s concentration inequality. 4 Experiments We used the LIG corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4http://statmt.org/wmt10/translation-task.html</context>
</contexts>
<marker>Potet, Besacier, Blanchon, 2010</marker>
<rawString>Marion Potet, Laurent Besacier, and Herv´e Blanchon. 2010. The LIG machine translation system for WMT 2010. In WMT, Upsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Potet</author>
<author>Emanuelle Esperanc¸a-Rodier</author>
<author>Laurent Besacier</author>
<author>Herv´e Blanchon</author>
</authors>
<title>Collection of a large database of French-English SMT output corrections.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<location>Istanbul, Turkey.</location>
<marker>Potet, Esperanc¸a-Rodier, Besacier, Blanchon, 2012</marker>
<rawString>Marion Potet, Emanuelle Esperanc¸a-Rodier, Laurent Besacier, and Herv´e Blanchon. 2012. Collection of a large database of French-English SMT output corrections. In LREC, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avneesh Saluja</author>
<author>Ying Zhang</author>
</authors>
<title>Online discriminative learning for machine translation with binary-valued feedback.</title>
<date>2014</date>
<booktitle>Machine Translation,</booktitle>
<pages>28--69</pages>
<contexts>
<context position="8893" citStr="Saluja and Zhang (2014)" startWordPosition="1393" endWordPosition="1396">llel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model param</context>
</contexts>
<marker>Saluja, Zhang, 2014</marker>
<rawString>Avneesh Saluja and Ying Zhang. 2014. Online discriminative learning for machine translation with binary-valued feedback. Machine Translation, 28:69–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avneesh Saluja</author>
<author>Ian Lane</author>
<author>Ying Zhang</author>
</authors>
<title>Machine translation with binary feedback: A largemargin approach.</title>
<date>2012</date>
<booktitle>In AMTA,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="8868" citStr="Saluja et al. (2012)" startWordPosition="1389" endWordPosition="1392">s and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and</context>
</contexts>
<marker>Saluja, Lane, Zhang, 2012</marker>
<rawString>Avneesh Saluja, Ian Lane, and Ying Zhang. 2012. Machine translation with binary feedback: A largemargin approach. In AMTA, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
</authors>
<title>Online learning and online convex optimization. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="2151" citStr="Shalev-Shwartz, 2012" startWordPosition="322" endWordPosition="323">mber of passes over the training sample is sufficient for convergence (Bottou and Bousquet, 2004). Statistical Machine Translation (SMT) has embraced the potential of online learning, both to handle millions of features and/or millions of data in parameter tuning via online structured prediction (see Liang et al. (2006) for seminal early work), and in interactive learning from user post-edits (see CesaBianchi et al. (2008) for pioneering work on online computer-assisted translation). Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for</context>
</contexts>
<marker>Shalev-Shwartz, 2012</marker>
<rawString>Shai Shalev-Shwartz. 2012. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz Josef Och</author>
</authors>
<title>Discriminative reranking for machine translation. In NAACL,</title>
<date>2004</date>
<location>Boston, MA.</location>
<contexts>
<context position="7957" citStr="Shen et al. (2004)" startWordPosition="1247" endWordPosition="1250"> Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edit</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pannaga Shivaswamy</author>
<author>Thorsten Joachims</author>
</authors>
<title>Online structured prediction via coactive learning.</title>
<date>2012</date>
<booktitle>In ICML,</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="3177" citStr="Shivaswamy and Joachims, 2012" startWordPosition="478" endWordPosition="481">pel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joachims, 2012). This framework allows us to make three main contributions: • Firstly, the proof techniques of Shivaswamy and Joachims (2012) are a simple and elegant tool for a theoretical analysis of perceptron-style algorithms that date back to the perceptron mistake bound of Novikoff (1962). These techniques provide an alternative to an online gradient descent view of perceptron-style algorithms, and can easily be extended to obtain regret bounds for a la1 Proceedings of the 19th Conference on Computational Language Learning, pages 1–11, Beijing, China, July 30-31, 2015. c�2015 Association for Computatio</context>
<context position="7355" citStr="Shivaswamy and Joachims (2012)" startWordPosition="1156" endWordPosition="1159">ogate translations. The rest of this paper is organized as follows. After a review of related work (Section 2), we present a latent percpetron algorithm and analyze its convergence and generalization properties (Section 3). Our first set of experiments (Section 4.1) confirms our theoretical analysis by showing convergence in regret and TER for learning from weak and strong feedback. Our second set of experiments (Section 4.2) analyzes the relation of different surrogacy modes to minimization of regret and TER. 2 Related Work Our work builds on the framework of coactive learning, introduced by Shivaswamy and Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (200</context>
<context position="13119" citStr="Shivaswamy and Joachims (2012)" startWordPosition="2100" endWordPosition="2103"> is presented with feedback ¯yt about its prediction, which is used to make an update to an existing parameter vector. Algorithm 1 is called ”Feedbackbased Latent Perceptron” to stress the fact that it only uses weak feedback to its predictions for learning, but does not necessarily observe optimal structures as in the full information case (Sun et al., 2013). Learning from full information can be recovered by setting the informativeness parameter α to 1 in Equation (2) below, in which case the feedback structure ¯yt equals the optimal structure yt *. Algorithm 1 differs from the algorithm of Shivaswamy and Joachims (2012) by a joint maximization over output structures y and hid3 den derivations h in prediction (line 4), by choosing a hidden derivation h¯ for the feedback structure y¯ (line 7), and by the use of the re-scaling factor A¯ht,ht in the update (line 8), where ht = h(xt|¯yt; wt) and ht = h(xt; wt) are the derivations of the feedback structure and the prediction at time t, respectively. In our theoretical exposition, we assume that ¯yt is reachable in the search space of possible outputs, that is, ¯yt E Y(xt). 3.3 Feedback of Graded Utility The key in the theoretical analysis in Shivaswamy and Joachim</context>
<context position="14667" citStr="Shivaswamy and Joachims (2012)" startWordPosition="2382" endWordPosition="2385"> yt) where for given x E X, y E Y(x), and h* = arg maxhEH(x,y) Uh(x, y), we define U(x, y) = Uh∗(x, y) and drop the subscript unless h =� h*. Importantly, the feedback is typically not the optimal structure y*t that is defined as y*t = arg max U(xt, y). yEY(xt) While not receiving optimal structures in training, the learning goal is to predict objects with utility close to optimal structures yt *. The regret that is suffered by the algorithm when predicting object yt instead of the optimal object y*t is (U(xt, y*t ) − U(xt, yt)). (1) To quantify the amount of information in the weak feedback, Shivaswamy and Joachims (2012) define a notion of α-informative feedback, which we generalize as follows for the case of latent derivations. We assume that there exists a derivation ¯ht for the feedback structure ¯yt, such that for all predictions yt, the (re-scaled) utility of the weak feedback ¯yt is higher than the (re-scaled) utility of the prediction yt by a fraction α of the maximum possible utility range (under the given utility model). Thus bt, ]¯ht, bh and for α E (0, 1]: At(xt,¯yt) − Uh(xt, yt)) X A¯ht,h &gt; α(U(xt, y*t ) − U(xt, yt)) − ξt, (2) where ξt &gt; 0 are slack variables allowing for violations of (2) for giv</context>
<context position="16286" citStr="Shivaswamy and Joachims (2012)" startWordPosition="2673" endWordPosition="2676">dback-based latent perceptron can be upper bounded for any α E (0, 1], for any w* E Rd: 1 REGT : αT A proof for Theorem 1 is similar to the proof of Shivaswamy and Joachims (2012) and the original mistake bound for the perceptron of Novikoff (1962).1 The theorem can be interpreted as follows: we expect lower average regret for higher values of α; due to the dominant term T, regret will approach the minimum of the accumulated slack (in case feedback structures violate Equation (2)) or 0 (in case of strictly α-informative feedback). The main difference between the above result and the result of Shivaswamy and Joachims (2012) is the term DT following from the rescaled distance of latent derivations. Their analysis is agnostic of latent derivations, and can be recovered by setting this scaling factor to 1. This yields DT = T, and thus recovers the main factor ADT = A1T in their regret bound. In our algorithm, penalizing large distances of derivations can help to move derivations ht closer to ¯ht, therefore decreasing DT as learning proceeds. Thus in case DT &lt; T, our bound is better than the original bound of Shivaswamy and Joachims (2012) for a perceptron without re-scaling. As we will show experimentally, re-scali</context>
</contexts>
<marker>Shivaswamy, Joachims, 2012</marker>
<rawString>Pannaga Shivaswamy and Thorsten Joachims. 2012. Online structured prediction via coactive learning. In ICML, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Stefan Riezler</author>
<author>Chris Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT.</title>
<date>2012</date>
<booktitle>In ACL, Jeju,</booktitle>
<contexts>
<context position="8308" citStr="Simianer et al. (2012)" startWordPosition="1305" endWordPosition="1308">ov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). </context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT. In ACL, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In AMTA,</title>
<date>2006</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="5517" citStr="Snover et al., 2006" startWordPosition="864" endWordPosition="867">cessarily optimal, object ¯yt with respect to a utility function U. The key asset of coactive learning is the ability of the learner to converge to predictions that are close to optimal structures y∗t , although the utility function is unknown to the learner, and only weak feedback in form of slightly improved structures ¯yt is seen in training. We present a proof-of-concept experiment in which translation feedback of varying grades is chosen from the n-best list of an “optimal” model that has access to full information. We show that weak feedback structures correspond to improvements in TER (Snover et al., 2006) over predicted structures, and that learning from weak feedback minimizes regret and TER. • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user (providing feedback in form of an improved translation) so that we can compare different surrogacy modes as different ways of approximate utility maximization. We show experimentally that learning from surrogate “hope” derivations (Chiang, 2012) minimizes regret and TER,</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Artem Sokolov</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<date>2013</date>
<booktitle>Lattice BLEU oracles in machine translation. Transactions on Speech and Language Processing,</booktitle>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="6275" citStr="Sokolov et al., 2013" startWordPosition="982" endWordPosition="985">in practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user (providing feedback in form of an improved translation) so that we can compare different surrogacy modes as different ways of approximate utility maximization. We show experimentally that learning from surrogate “hope” derivations (Chiang, 2012) minimizes regret and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2013), for which learning does not converge. It is important to note that the goal of our experiments is not to present improvements of coactive learning over the “optimal” full-information model in terms of standard SMT performance. Instead, our goal is to present experiments that serve as a proof-of-concept of the feasibility of coactive learning from weak feedback for SMT, and to propose a new perspective on standard practices of learning from surrogate translations. The rest of this paper is organized as follows. After a review of related work (Section 2), we present a latent percpetron algorit</context>
<context position="30574" citStr="Sokolov et al. (2013)" startWordPosition="5166" endWordPosition="5169">rogates. dard practices for extracting feedback from observed user post-edits for discriminative SMT can be matched with the modeling assumptions of the coactive learning framework. The customary practice in discriminative learning for SMT is to replace observed user translations by surrogate translations since the former are often not reachable in the search space of the SMT decoder. In our case, only 29% of the post-edits in the LIGcorpus were reachable by the decoder. We compare four heuristics of generating surrogate translations: oracles are generated using the lattice oracle approach of Sokolov et al. (2013) which returns the closest path in the decoder search graph as reachable surrogate translation.7 A local surrogate y˜ is chosen from the n-best list of the linear model as the translation that achieves the best TER score with respect to the actual postedit y: y˜ = arg miny,En-best(xt;wt) TER(y&apos;, y). This corresponds to the local update mode of Liang et al. (2006). A filtered surrogate translation y˜ is found by scanning down the n-best list, and accepting the first translation as feedback that improves TER score with respect to the human post-edit y over the 1-best prediction yt of the linear </context>
</contexts>
<marker>Sokolov, Wisniewski, Yvon, 2013</marker>
<rawString>Artem Sokolov, Guillaume Wisniewski, and Franc¸ois Yvon. 2013. Lattice BLEU oracles in machine translation. Transactions on Speech and Language Processing, 10(4):18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Artem Sokolov</author>
<author>Stefan Riezler</author>
<author>Shay B Cohen</author>
</authors>
<title>Coactive learning for interactive machine translation.</title>
<date>2015</date>
<booktitle>In ICML Workshop on Machine Learning for Interactive Systems (MLIS),</booktitle>
<location>Lille, France.</location>
<contexts>
<context position="7702" citStr="Sokolov et al., 2015" startWordPosition="1210" endWordPosition="1213">rom weak and strong feedback. Our second set of experiments (Section 4.2) analyzes the relation of different surrogacy modes to minimization of regret and TER. 2 Related Work Our work builds on the framework of coactive learning, introduced by Shivaswamy and Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. </context>
</contexts>
<marker>Sokolov, Riezler, Cohen, 2015</marker>
<rawString>Artem Sokolov, Stefan Riezler, and Shay B. Cohen. 2015. Coactive learning for interactive machine translation. In ICML Workshop on Machine Learning for Interactive Systems (MLIS), Lille, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Takuya Matsuzaki</author>
<author>Wenjie Li</author>
</authors>
<title>Latent structured perceptrons for large scale learning with hidden information.</title>
<date>2013</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>25</volume>
<issue>9</issue>
<contexts>
<context position="4157" citStr="Sun et al. (2013)" startWordPosition="633" endWordPosition="636">ceptron-style algorithms, and can easily be extended to obtain regret bounds for a la1 Proceedings of the 19th Conference on Computational Language Learning, pages 1–11, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics tent perceptron algorithm at a rate of O( 1 ), with √ T possible improvements by using re-scaling. This bound can be directly used to derive generalization guarantees for online and online-to-batch conversions of the algorithm, based on well-known concentration inequalities. Our analysis covers the approach of Liang et al. (2006) and supersedes Sun et al. (2013)’s analysis of the latent perceptron by providing simpler proofs and by adding a generalization analysis. Furthermore, an online learning framework such as coactive learning covers problems such as changing n-best lists after each update that were explicitly excluded from the batch analysis of Gimpel and Smith (2012) and considered fixed in the analysis of Sun et al. (2013). • Our second contribution is an extension of the online learning scenario in SMT to include a notion of “weak feedback” for the latent perceptron: Coactive learning follows an online learning protocol, where at each round </context>
<context position="7641" citStr="Sun et al. (2013)" startWordPosition="1199" endWordPosition="1202">s by showing convergence in regret and TER for learning from weak and strong feedback. Our second set of experiments (Section 4.2) analyzes the relation of different surrogacy modes to minimization of regret and TER. 2 Related Work Our work builds on the framework of coactive learning, introduced by Shivaswamy and Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of </context>
<context position="12850" citStr="Sun et al., 2013" startWordPosition="2055" endWordPosition="2058">al exposition we assume that the arg max operation can be computed exactly. 3.2 Feedback-based Latent Perceptron We assume an online setting, in which examples are presented one-by-one. The learner observes an input xt, predicts an output structure yt, and is presented with feedback ¯yt about its prediction, which is used to make an update to an existing parameter vector. Algorithm 1 is called ”Feedbackbased Latent Perceptron” to stress the fact that it only uses weak feedback to its predictions for learning, but does not necessarily observe optimal structures as in the full information case (Sun et al., 2013). Learning from full information can be recovered by setting the informativeness parameter α to 1 in Equation (2) below, in which case the feedback structure ¯yt equals the optimal structure yt *. Algorithm 1 differs from the algorithm of Shivaswamy and Joachims (2012) by a joint maximization over output structures y and hid3 den derivations h in prediction (line 4), by choosing a hidden derivation h¯ for the feedback structure y¯ (line 7), and by the use of the re-scaling factor A¯ht,ht in the update (line 8), where ht = h(xt|¯yt; wt) and ht = h(xt; wt) are the derivations of the feedback str</context>
</contexts>
<marker>Sun, Matsuzaki, Li, 2013</marker>
<rawString>Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013. Latent structured perceptrons for large scale learning with hidden information. IEEE Transactions on Knowledge and Data Engineering, 25(9):2064– 2075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Reinforcement Learning. An Introduction.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9935" citStr="Sutton and Barto, 1998" startWordPosition="1550" endWordPosition="1553">sponse-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the prediction. 3 Coactive Learning for Online Latent Structured Prediction 3.1 Notation and Background Let X denote a set of input examples, e.g., sentences, and let Y(x) denote a set of structured outputs for x E X, e.g., translations. We define Y = UxY(x). Furthermore, by H(x, y) we denote a set of possible hidden derivations for a struct</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning. An Introduction. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical MT.</title>
<date>2006</date>
<booktitle>In COLING-ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="8088" citStr="Tillmann and Zhang (2006)" startWordPosition="1268" endWordPosition="1271"> additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2006. A discriminative global training algorithm for statistical MT. In COLING-ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina W¨aschle</author>
<author>Patrick Simianer</author>
<author>Nicola Bertoldi</author>
<author>Stefan Riezler</author>
<author>Marcello Federico</author>
</authors>
<title>Generative and discriminative methods for online adaptation in SMT.</title>
<date>2013</date>
<booktitle>In MT Summit,</booktitle>
<location>Nice, France.</location>
<marker>W¨aschle, Simianer, Bertoldi, Riezler, Federico, 2013</marker>
<rawString>Katharina W¨aschle, Patrick Simianer, Nicola Bertoldi, Stefan Riezler, and Marcello Federico. 2013. Generative and discriminative methods for online adaptation in SMT. In MT Summit, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>NTT statistical machine translation for IWSLT</title>
<date>2006</date>
<booktitle>In IWSLT, Kyoto,</booktitle>
<contexts>
<context position="7981" citStr="Watanabe et al. (2006)" startWordPosition="1251" endWordPosition="1254"> extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confin</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2006. NTT statistical machine translation for IWSLT 2006. In IWSLT, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8112" citStr="Watanabe et al. (2007)" startWordPosition="1272" endWordPosition="1275">alization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
</authors>
<title>Optimized online rank learning for machine translation. In NAACL,</title>
<date>2012</date>
<location>Montreal, Canada.</location>
<contexts>
<context position="8343" citStr="Watanabe (2012)" startWordPosition="1311" endWordPosition="1312">ts for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, 2 or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online par</context>
</contexts>
<marker>Watanabe, 2012</marker>
<rawString>Taro Watanabe. 2012. Optimized online rank learning for machine translation. In NAACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable MT training.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="8020" citStr="Yu et al. (2013)" startWordPosition="1259" endWordPosition="1262">ea of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where in</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable MT training. In EMNLP, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Yuille</author>
<author>Anand Rangarajan</author>
</authors>
<date>2003</date>
<booktitle>The concave-convex procedure. Neural Computation,</booktitle>
<pages>15--915</pages>
<contexts>
<context position="2858" citStr="Yuille and Rangarajan, 2003" startWordPosition="427" endWordPosition="430">exity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joachims, 2012). This framework allows us to make three main contributions: • Firstly, the proof techniques of Shivaswamy and Joachims (2012) are a simple and elegant tool for a theoretical analysis of perceptron-style algorithms that date back to the perceptron mistake bound of Novikoff (1962).</context>
</contexts>
<marker>Yuille, Rangarajan, 2003</marker>
<rawString>Alan Yuille and Anand Rangarajan. 2003. The concave-convex procedure. Neural Computation, 15:915–936.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>