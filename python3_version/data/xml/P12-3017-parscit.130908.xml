<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.104399">
<title confidence="0.9977875">
Demonstration of IlluMe: Creating Ambient
According to Instant Message Logs
</title>
<author confidence="0.998675">
Lun-Wei Ku Cheng-Wei Sun Ya-Hsin Hsueh
</author>
<affiliation confidence="0.99972">
National Yunlin University of Science and Technology
</affiliation>
<address confidence="0.6403035">
123 University Road, Section 3
Douliou, Yunlin 64002, Taiwan
</address>
<email confidence="0.996189">
lwku@yuntech.edu.tw;chengwei.kenny.sun@gmail.com;hsuehyh@yuntech.edu.tw
</email>
<sectionHeader confidence="0.997356" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994">
We present IlluMe, a software tool pack
which creates a personalized ambient using
the music and lighting. IlluMe includes an
emotion analysis software, the small space
ambient lighting, and a multimedia
controller. The software analyzes
emotional changes from instant message
logs and corresponds the detected emotion
to the best sound and light settings. The
ambient lighting can sparkle with different
forms of light and the smart phone can
broadcast music respectively according to
different atmosphere. All settings can be
modified by the multimedia controller at
any time and the new settings will be
feedback to the emotion analysis software.
The IlluMe system, equipped with the
learning function, provides a link between
residential situation and personal emotion.
It works in a Chinese chatting environment
to illustrate the language technology in life.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999763363636364">
Emotion analysis as well as recommendation
technology has drawn a lot attention in the natural
language processing research community. The
development of fundamental approaches as well as
applications has been proposed (Das, 2011; Sarwar
et al., 2001; Zheng et al., 2010). However, most of
them were Internet applications, and to the best
knowledge of the authors, these technologies have
not yet been involved in the ambient creation. To
create an intelligent living space, some researchers
utilized the facial expression and speech recognizer
</bodyText>
<page confidence="0.993607">
97
</page>
<bodyText confidence="0.999939457142857">
to detect emotions (Busso et al., 2004), but then
the accompanied cameras and microphones were
necessary. Some researchers tried to use sensors to
watch the heart beat and the body temperature of
residents to know their current emotion for further
applications, but the problem was that users had to
wear sensors and it was inconvenient. Instead of
watching body signals, we postulate that the
communications among people is one of the
important factors to influence their emotions.
Therefore, we tried to find clues from the textual
conversations of the residents in order to detect
their psychological state.
There are many ways to categorize emotions.
Different emotion states were used for experiments
in previous research (Bellegarda, 2010). To find
suitable categories of emotions, we adopted the
three-layered emotion hierarchy proposed by
Parrott (2001)1. Six emotions are in the first layer,
including love, joy, surprise, anger, sadness and
fear. The second layer includes 25 emotions, and
the third layer includes 135 emotions. Using this
hierarchical classification benefits the system. We
can categorize emotions from rough to fine
granularities and degrade to the upper level when
the experimental materials are insufficient. How to
map categories in other researches to ours becomes
clearer, and annotators have more information
when marking their current emotion.
As to the music, most researchers looked for the
emotions in songs or rhythms (Yang and Chen,
2011; Zbikowski, 2011). They classified music
into different emotional categories and developed
the system to tell what emotion a song might bring
to a listener. However, if the aim is to create a
</bodyText>
<footnote confidence="0.998762">
1 http://changingminds.org/explanations/emotions/
basic%20emotions.htm
</footnote>
<note confidence="0.87885">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 97–102,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999360086956522">
comfortable ambient, what songs a person in a
certain emotional state wants to listen to becomes
the question. A happy user does not always enjoy
happy songs, and vice versa. In this case, the
technology developed in the previous work did not
meet the new requirement.
IlluMe was designed for a small space personal
environment. We expect that users would like to
use it because this system could interactively
respond to their personal status to provide a feeling
of the companion. We view the IlluMe system as a
realization of detecting emotions from users’
textual conversations and then recommending the
best ambient accordingly. There are three major
contributions in the development of the system.
First, a corpus for ambient creation according to
emotions was constructed. Second, IlluMe
demonstrates a way to apply the state of the art
technology of emotion analysis and
recommendation to create an intelligent living
space. Third, along with the developed technology,
several further applications utilizing the
components of IlluMe become feasible.
</bodyText>
<sectionHeader confidence="0.980518" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999955090909091">
The potential working area for IlluMe is home or a
small space. The system was designed to fit in with
the modern people’s life style: programs are
installed in users’ personal computer and smart
phone. The smart phone functions as the remote
control and the music player, while all setting
signals are sent out from the personal computer.
The smart phone and the personal computer
communicate through the wireless network. The
only additional hardware requirement is the
lighting set.
</bodyText>
<subsectionHeader confidence="0.994865">
2.1 System Features
</subsectionHeader>
<bodyText confidence="0.998648296296296">
Emotion Detection Switch: The system detects
users’ current emotion according to messenger logs
once a preset time period. It is ON/OFF switchable
if users do not want the conversations to be
recorded or utilized when determining the ambient.
Auto Ambient Setting: The system sets the
current ambient by a specific combination of a
song and a light group which corresponds to the
emotion or represents a special atmosphere.
Manual Ambient Adjustment: IlluMe provides
a friendly user interface to change the settings of
music and lighting at any time.
Personal Preference Learning: When users
change the settings, the new ones are recorded.
IlluMe learns the preference and then performs the
user adaptation. After a period of time users will
have their unique ambient creating system.
Unlimited Melodies and Rich Light Colors:
Users can add their songs in the smart phone for
selection at any time. The learning process will
help propose the new songs to create ambient later.
Instant State Update: IlluMe watches the user
input from messenger when the software is on.
Therefore, it is able to change the music and
lighting according to the detected emotion within a
preset time period and users will feel like the
environment is interacting with them.
</bodyText>
<subsectionHeader confidence="0.993544">
2.2 System Framework
</subsectionHeader>
<bodyText confidence="0.999902538461538">
Figure 1 demonstrates the system framework of
IlluMe. The system automatically watches the
User Messages from messenger logs. The Emotion
Analysis component detects the emotion of users,
while the Ambient Learning Model determines the
music and lighting accordingly, considering also
the Personal Information of users.
After the lights are on and the music is played,
the user can change the settings they are not
satisfying. A smart phone (Mobile Device) is used
to change the settings, with two controllers on it:
the Preference Controller and the Ambient
Controller. The former takes the User Input for
new settings, and then the music and lighting are
changed by the latter. At the same time, the
Preference Controller also sends the new settings
to Ambient Learning Model to be recorded for user
adaptation when creating the next ambient.
The Emotion Analysis Component and Ambient
Learning Model are two programs in a personal
computer, and the Personal Info is saved in the
personal computer, too. ANT wireless personal
network protocol (Dynastream) is adopted to send
the control signals to the Lighting. The LED
lighting board is utilized to implement the Lighting
of 65,536 colors.
</bodyText>
<subsectionHeader confidence="0.998571">
2.3 Operation Flowchart of User Interface
</subsectionHeader>
<bodyText confidence="0.9999174">
The IlluMe system provides a user interface to
change the settings by a smart phone (Mobile
Device), functioning as a remote control. Users can
select the location of music or the lighting, e.g. the
living room or the bedroom, and the control mode,
</bodyText>
<page confidence="0.996809">
98
</page>
<figure confidence="0.988163642857143">
Media
Control
Location
Mode
Bedroom Living Room
Manual
Auto
Location
Play Music
Lights
Color
EmotionSet
Atmosphere
Display Lighting Effect
</figure>
<bodyText confidence="0.9987102">
i.e. manual or automatic. In the manual mode,
users can set the color of a specific light; in the
automatic mode, users select an emotional color set
or a desired atmosphere for the lighting. Figure 2
shows the operational flow of the user interface.
</bodyText>
<subsectionHeader confidence="0.999816">
2.4 Ambient Lighting and Music Playing
</subsectionHeader>
<bodyText confidence="0.999609">
To design the ambient lighting, one has to take
LED array board, controlling mode and the light-
mixing effect of the lampshade into consideration.
The LED lamp should sprinkle the LED
components of red, cyan, green, white and orange
lights equally onto the LED array board, so as to
achieve uniform distribution. The controlling
module distinguishes each lamp by its own code to
modify the brightness of different colored LEDs
within.
</bodyText>
<figureCaption confidence="0.99728">
Figure 1. System Framework of IlluMe
</figureCaption>
<bodyText confidence="0.999979153846154">
As the LED lighting changes its color according
to the controlling signals from the remote
controller, the system transfer appropriate RF
signals from the user’s personal computer to the
ANT board, and then the ANT board controls the
LED lighting board to change the color of lights.
Music is broadcasted according to the detected
emotional state. The broadcasting function and the
controlling function are both realized by the
software in the smart phone. Music is broadcasted
directly through the phone, which conforms to the
habits of modern people. Figure 3 shows the
illustration of the usage of IlluMe.
</bodyText>
<figureCaption confidence="0.997655">
Figure 2. Operation Flowchart
</figureCaption>
<sectionHeader confidence="0.991692" genericHeader="method">
3 Emotion Analysis
</sectionHeader>
<bodyText confidence="0.9999491">
The emotion analysis that IlluMe performed is to
find the emotions that texts in messenger logs bear
in order to create a comfort ambient by sound and
lighting accordingly. To achieve this, the system
needs to understand the Internet language first, and
then detect emotions and categorize them. The
system works on the Chinese chatting environment
and analyzes Chinese texts to detect emotions. The
materials, approaches, and preliminary results in
the development phase are described in this section.
</bodyText>
<subsectionHeader confidence="0.983474">
3.1 Experimental Materials
</subsectionHeader>
<bodyText confidence="0.9997765">
Two dictionaries, the Chinese sentiment dictionary
NTUSD (Ku and Chen, 2007) and the Chinese
emotion dictionary (Lin et al., 2008), were adopted
for detecting emotions. The former categorized
sentiment words into positive and negative, while
the latter into eight emotion types: awesome,
heartwarming, surprising, sad, useful, happy,
boring, and angry. Notice that these eight emotion
</bodyText>
<figure confidence="0.998972166666667">
Preference
Controller
User
Input
Emotion Analysis
Component
Mobile Device
User Messages
Settings
Ambient
Controller
Music
Ambient Learning
Model
Personal
Info
Lighting
ANT
</figure>
<figureCaption confidence="0.989348">
Figure 3. Usage Illustration
</figureCaption>
<page confidence="0.987682">
99
</page>
<bodyText confidence="0.996532">
types appeared in Yahoo! News Taiwan in the year
2008 and not all of them were general emotion
states. Therefore, we tried to find Lin’s emotion
categories in Parrott’s emotion hierarchy before
using this dictionary. Those could not be found
were categorized in the Other class.
Messenger logs were used as the source to detect
emotions. We collected texts from Yahoo!
Messenger and MSN Messenger logs of 8
annotators. When the installed collecting program
in their computers was on, it ran as a service and
continuously logged their messages. Whenever
there was at least one new message, once an hour
the collecting program would pop up the menu and
ask them to annotate the current emotion together
with the preferred settings of the music and
lighting. There were 3,290 songs, 15 emotional
lighting colors and 6 atmospheres for selection.
When selecting the settings of lighting, a full-
screen colored photo would be displayed to help
annotators make their decisions. A total of 150
records are annotated for experiments and statistics
are shown in Table 1.
</bodyText>
<table confidence="0.99876375">
Emo 1 2 3 4 5 6
11 80 1 15 39 4
Color 1 2 3 4 5 6 7
14 6 5 25 9 5 11 14
8 9 10 11 12 13 14 15
11 7 4 13 7 15 5 13
Atm 1 2 3 4 5 6
28 40 16 33 17 16
</table>
<tableCaption confidence="0.857637">
Table 1. Statistics of Annotated Materials
(Emo: Emotions, 1=Love, 2=Joy, 3=Surprise, 4=Angry,
5=Sad, 6=Fear; Color:15 color sets; Atm:6 atmospheres)
</tableCaption>
<subsectionHeader confidence="0.999676">
3.2 Interpretation of Zhuyin Wen
</subsectionHeader>
<bodyText confidence="0.999977909090909">
When processing Internet Chinese texts, IlluMe
transformed messenger logs and sentiment
dictionaries into zhuyin (Su, 2003) before looking
for emotions2. There were many reasons to do this.
Zhuyin Wen (注音文) is one of many creative uses
of writing systems in the Internet language. As
Blakeman (2004) found in his study of English,
Internet language is fraught with initializations.
However, as to the traditional Chinese, both
Wikipedia and Zhang and Dai (2006) indicated
that stylized initials and stylized numbers are
</bodyText>
<page confidence="0.59356">
2 Lookup Table: http://cclookup.cctserver.com/
</page>
<bodyText confidence="0.999769217391304">
rarely used in Taiwan. Su reported that the most
popular type of creative use of writing systems is
“Zhuyin Wen” (注音文). In “Zhuyin Wen” the
complete phonetic representation of a character is
reduced to a consonant, or sometimes a vowel.
This creative use appeared commonly in the
collected conversations. Generally we had to figure
out the missing vowels to understand the word, but
in our system a reversed approach (dropping
vowels) was adopted to make sure the system did
not miss any possible match of dictionary terms
observed in the conversations.
When messenger users typed characters by their
phonetics (consonants and vowels), very often they
selected the wrong one from candidates of the
same pronunciation, or they were just too lazy to
select so the writing system chose the default
candidate for them. In these cases, the system
could not find a match because of wrong
composite characters. Transforming characters in
both dictionaries and conversations into their
zhuyin representations before detecting emotions
also help recover this kind of errors.
</bodyText>
<subsectionHeader confidence="0.998626">
3.3 Emotion Detection from Texts
</subsectionHeader>
<bodyText confidence="0.999907">
Section 3.2 shows how the system dealt with the
error prone Internet texts and found the
dictionaries terms. Ku and Chen’s (2007) approach
for calculating sentiment scores was then adopted
to give scores to these terms. The scores of terms
of different emotional categories were summed up
and the emotion category of the highest score was
selected as the detected emotion. The Ambient
Learning Model takes the detected emotion and
selects the corresponding music and lighting by the
Naïve Bayes classifier trained by the annotated
materials.
</bodyText>
<subsectionHeader confidence="0.978066">
3.4 Experiment and Preliminary Results
</subsectionHeader>
<bodyText confidence="0.999572181818182">
Table 2 shows that using enhanced NTUSD (an
augmented version of NTUSD) together with
zhuyin transformation achieves the best results for
emotion classification (positive/negative).
Ku (2008) reported the set precision of their
approach was 0.489 when texts were categorized
into positive, neutral and negative. Though they
had one additional neutral category, our system
achieved the precision of 0.620 when processing
the noisy Internet texts without word segmentation
and part of speech tagging, which was satisfactory.
</bodyText>
<page confidence="0.982053">
100
</page>
<bodyText confidence="0.999200541666667">
Because IlluMe would always recommend a
new or unchanged ambient setting, it would always
find the closest emotion category of the user’s
current emotion. In other words, the chatting
content would always be connected to one of six
emotion categories, so precision is the best metric
to evaluate the performance of the system. The
micro-average precision of the emotion detection
was 0.207, while the macro-average precision was
0.338. Bellegarda reported that his best f-measure
was 0.340 also for 6 categories. Notice that the
categories in Lin’s Chinese emotional dictionary
were not identical to ours and hence we could not
find terms for some categories in it. Therefore,
though Bellegarda’s and our results were done on
different datasets and evaluated by different
metrics, considering our system suffered for the
lack of terms in some categories and the
ambiguous texts from the creative writing, the
performance was considered acceptable.
For the ambient recommendation, the micro-
average precision of selecting the settings of
lighting according to the detected emotion was
0.441 for 15 color sets and 0.461 for 6 atmospheres.
</bodyText>
<table confidence="0.999257">
Positive Negative Total
A 0.489 0.534 0.507
B 0.902 0.155 0.613
A+C 0.902 0.172 0.620
</table>
<tableCaption confidence="0.981599">
Table 2. Precision of Emotion Detection
(A: NTUSD; B: Enhanced NTUSD; C:Zhuyin
transformation)
</tableCaption>
<subsectionHeader confidence="0.992006">
3.5 Ambient Learning Function
</subsectionHeader>
<bodyText confidence="0.999967888888889">
Because bringing up the settings to users is like a
behavior of recommendation, we adopted the
concept of collaborative filtering to design the
function of the Ambient Learning Model. In the
early stage of using IlluMe, it proposes the most
frequently selected settings, that is, the choice of a
group of people in the specific emotional state. If
the user is connected to the Internet, the user
experience will be transferred back to the servers
to help recommend a better ambient to other users.
The user experience optimization was feasible in
this system because of the use of the smart phone,
and this function was also implemented. As the
users update the settings, the system knows their
preference. In the later stage of using IlluMe, the
Ambient Learning Model considers the preference
of both the individual and the group to create a
unique ambient for each user.
</bodyText>
<sectionHeader confidence="0.980995" genericHeader="method">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99999129032258">
Through the work we aim to apply the language
technology to redefine the concept of a small house
or working space. They should be a family-like
existence which possesses the intellectual capacity
to observe human behavior and emotion, and
create consoling spaces according to the residents’
different status. Therefore we implemented
emotion analysis technique to equip a space with
the ability to observe the status of its residents and
interact with them accordingly. The instant interior
lightings and music change can be viewed as a new
form of “conversation”. Residents can not only
take the ambient provided by IlluMe, but can also
give feedbacks. The concept of collaborative
filtering was also implemented as we viewed the
proposing of ambient as a kind of recommendation.
Through the demonstration of the IlluMe system,
we hope to show another way to apply language
technology in life and retrieve the positive and
relaxing atmosphere to rebuild our sense of trust
and safety toward space, and finally recollect the
long-lost attachment toward it.
We will continue collecting annotated
materials and user feedbacks for learning, and
make the materials a corpus for the research
community. Facebook will be a source of text
collection to gather more complete personal
conversations for emotion detection. Making the
IlluMe components real products like the home
lighting system, the intelligent table lamp, or the
music album promoter is also a future plan.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="conclusions">
5 Demonstration
</sectionHeader>
<bodyText confidence="0.999991428571429">
As demonstrating the IlluMe system by our
original model house may be difficult in
transportation and it may need a large space for
demonstration, we will demonstrate the lightings
by several table lamps, in which the LED lighting
board resides. Other software will be performed on
the smart phone and the personal computer.
</bodyText>
<subsectionHeader confidence="0.983813">
5.1 Demonstration Outline
</subsectionHeader>
<bodyText confidence="0.99982425">
There are three purposes of the demonstration: first,
to show how we apply the emotion analysis and
recommendation technique in an ambient creating
system; second, to illustrate actual and live
</bodyText>
<page confidence="0.996688">
101
</page>
<bodyText confidence="0.99997175">
operation of the system to the potential users; third,
to show the annotation process of the experiment
materials and the underlying algorithms for those
interested in the technical details.
Potential users might be interested in how the
system will work if they have it in their personal
computers and smart phones. Therefore, we
demonstrate the whole IlluMe system with the
actual music and lighting. Users can type Chinese
words in messengers from the personal computer,
and then the IlluMe system will change the music
and lighting according to the proposed settings in a
short time. The user can also control the music and
lighting from the interface by the smart phone.
In addition to demonstrating the functionality of
the system, we will also provide accompanying
visual aids that illustrate the underlying algorithms
and the technical details. For example, zhuyin,
terms found in the dictionaries, emotion scores, the
detected emotion and the suggested settings.
</bodyText>
<sectionHeader confidence="0.997005" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.968337333333333">
Research of this paper was partially supported by
National Science Council, Taiwan, under the
contract NSC100-2218-E-224-013-.
</bodyText>
<sectionHeader confidence="0.998553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99942502739726">
Bellegarda, Jerome R. 2010. Emotion Analysis Using
Latent Affective Folding and Embedding.
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and
Generation of Emotion in Text, Los Angeles, 1-9.
Blakeman, Adam. 2004. An Investigation of the
Language of Internet Chat Rooms.
http://www.lancs.ac.uk/fss/courses/ling/ling201/res/d
issertations.html.
Busso, Carlos, Deng, Zhigang, Yildirim, Serdar, Bulut,
Murtaza, Lee, Chul Min, Kazemzadeh, Abe, Lee,
Sungbok, Neumann, Ulrich and Narayanan,
Shrikanth. 2004. Analysis of Emotion Recognition
using Facial Expressions, Speech and Multimodal
Information. Proceedings of ACM 6th International
Conference on Mutlmodal Interfaces (ICMI 2004),
State College, PA, Oct 2004
Das, Dipankar, 2011. Analysis and Tracking of
Emotions in English and Bengali Texts: A
Computational Approach. Proceedings of the
International World Wide Web Conference (WWW
2011), Ph. D. Symposium. 343-347.
Dynastream Innovations Inc., ANT AT3 RF Transceiver
Chipset _wDatasheet _tRev 1.2,
http://ww.thisi san. com/.
Ku, Lun-Wei and Chen, Hsin-Hsi. 2007. Mining
Opinions from the Web: Beyond Relevance Retrieval.
Journal of American Society for Information Science
and Technology, Special Issue on Mining Web
Resources for Enhancing Information Retrieval,
58(12), 1838-1850.
Ku, Lun-Wei, Liu, I-Chien, Lee, Chia-Ying, Chen,
Kuan-hua. and Chen, Hsin-His. 2008. Sentence-
Level Opinion Analysis by CopeOpi in NTCIR-7.
Proceedings of the 7th NTCIR Workshop Meeting,
Tokyo, Japan. 260-267.
Lin, Kevin Hsin-Yih, Yang, Changhua, and Chen, Hsin-
His. 2008. Emotion Classification of Online News
Articles from the Reader’s Perspective. Proceedings
of the 2008 IEEE/WIC/ACM International
Conference on Web Intelligence. 220-226.
Ortony, A. and Turner, T. J. 1990. What&apos;s basic about
basic emotions? Psychological Review, 97, 315-331.
Parrott, W. 2001. Emotions in Social Psychology,
Psychology Press, Philadelphia.
Sarwar, Badrul, Karypis, George, Konstan, Joseph, and
Riedl, John. 2001. ItemBased Collaborative Filtering
Recommendation Algorithms. Proceedings of the
International World Wide Web Conference (WWW
2001), 285-295.
Su, Hsi-Yao. 2003. The Multilingual and Multi-
Orthographic Taiwan-Based Internet: Creative Uses
of Writing Systems on College-Affiliated BBSs.
Journal of Computer-Mediated Communication 9(1).
http://jcmc.indiana.edu/vol9/issue1/su.html.
Yang, Yi-Hsuan and Chen, Homer H., Fellow, IEEE.
2011. Ranking-Based Emotion Recognition for
Music Organization and Retrieval. IEEE
Transactions on audio, speech, and language
processing, 19(4).
Zbikowski, Lawrence M., 2011. Music, Emotion,
Analysis. Music Analysis, Blackwell Publishing Ltd,
Oxford, UK.
Zhang, Jiawei and Dai, Jiaxing. 2006. Qiantan shixia
qingnian wangluo yongyu –huoxing wen Apff-F
-F *� f141%Mi — x R1 3Z
http://www.shs.edu.tw/works/essay/2006/03/2006032
816043532.pdf
Zheng, Vincent W., Cao, Bin, Zheng, Yu, Xie, Xing and
Yang, Qiang. 2010. Collaborative Filtering Meets
Mobile Recommendation: A User-centered Approach
Proceedings of Twenty-Fourth National Conference
on Artificial Intelligence (AAAI-10).
</reference>
<page confidence="0.998624">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874791">
<title confidence="0.9985375">Demonstration of IlluMe: Creating Ambient According to Instant Message Logs</title>
<author confidence="0.998281">Lun-Wei Ku Cheng-Wei Sun Ya-Hsin Hsueh</author>
<affiliation confidence="0.999721">National Yunlin University of Science and</affiliation>
<address confidence="0.949849">123 University Road, Section Douliou, Yunlin 64002,</address>
<email confidence="0.989474">lwku@yuntech.edu.tw;chengwei.kenny.sun@gmail.com;hsuehyh@yuntech.edu.tw</email>
<abstract confidence="0.999089954545455">We present IlluMe, a software tool pack which creates a personalized ambient using the music and lighting. IlluMe includes an emotion analysis software, the small space ambient lighting, and a multimedia controller. The software analyzes emotional changes from instant message logs and corresponds the detected emotion to the best sound and light settings. The ambient lighting can sparkle with different forms of light and the smart phone can broadcast music respectively according to different atmosphere. All settings can be modified by the multimedia controller at any time and the new settings will be feedback to the emotion analysis software. The IlluMe system, equipped with the learning function, provides a link between residential situation and personal emotion. It works in a Chinese chatting environment to illustrate the language technology in life.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Emotion Analysis Using Latent Affective Folding and Embedding.</title>
<date>2010</date>
<booktitle>Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>1--9</pages>
<location>Los Angeles,</location>
<contexts>
<context position="2482" citStr="Bellegarda, 2010" startWordPosition="363" endWordPosition="364"> to use sensors to watch the heart beat and the body temperature of residents to know their current emotion for further applications, but the problem was that users had to wear sensors and it was inconvenient. Instead of watching body signals, we postulate that the communications among people is one of the important factors to influence their emotions. Therefore, we tried to find clues from the textual conversations of the residents in order to detect their psychological state. There are many ways to categorize emotions. Different emotion states were used for experiments in previous research (Bellegarda, 2010). To find suitable categories of emotions, we adopted the three-layered emotion hierarchy proposed by Parrott (2001)1. Six emotions are in the first layer, including love, joy, surprise, anger, sadness and fear. The second layer includes 25 emotions, and the third layer includes 135 emotions. Using this hierarchical classification benefits the system. We can categorize emotions from rough to fine granularities and degrade to the upper level when the experimental materials are insufficient. How to map categories in other researches to ours becomes clearer, and annotators have more information w</context>
</contexts>
<marker>Bellegarda, 2010</marker>
<rawString>Bellegarda, Jerome R. 2010. Emotion Analysis Using Latent Affective Folding and Embedding. Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, Los Angeles, 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Blakeman</author>
</authors>
<title>An Investigation of the Language of Internet Chat Rooms.</title>
<date>2004</date>
<note>http://www.lancs.ac.uk/fss/courses/ling/ling201/res/d issertations.html.</note>
<contexts>
<context position="12384" citStr="Blakeman (2004)" startWordPosition="1948" endWordPosition="1949"> 11 80 1 15 39 4 Color 1 2 3 4 5 6 7 14 6 5 25 9 5 11 14 8 9 10 11 12 13 14 15 11 7 4 13 7 15 5 13 Atm 1 2 3 4 5 6 28 40 16 33 17 16 Table 1. Statistics of Annotated Materials (Emo: Emotions, 1=Love, 2=Joy, 3=Surprise, 4=Angry, 5=Sad, 6=Fear; Color:15 color sets; Atm:6 atmospheres) 3.2 Interpretation of Zhuyin Wen When processing Internet Chinese texts, IlluMe transformed messenger logs and sentiment dictionaries into zhuyin (Su, 2003) before looking for emotions2. There were many reasons to do this. Zhuyin Wen (注音文) is one of many creative uses of writing systems in the Internet language. As Blakeman (2004) found in his study of English, Internet language is fraught with initializations. However, as to the traditional Chinese, both Wikipedia and Zhang and Dai (2006) indicated that stylized initials and stylized numbers are 2 Lookup Table: http://cclookup.cctserver.com/ rarely used in Taiwan. Su reported that the most popular type of creative use of writing systems is “Zhuyin Wen” (注音文). In “Zhuyin Wen” the complete phonetic representation of a character is reduced to a consonant, or sometimes a vowel. This creative use appeared commonly in the collected conversations. Generally we had to figure </context>
</contexts>
<marker>Blakeman, 2004</marker>
<rawString>Blakeman, Adam. 2004. An Investigation of the Language of Internet Chat Rooms. http://www.lancs.ac.uk/fss/courses/ling/ling201/res/d issertations.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Busso</author>
<author>Zhigang Deng</author>
<author>Serdar Yildirim</author>
<author>Murtaza Bulut</author>
<author>Chul Min Lee</author>
<author>Abe Kazemzadeh</author>
<author>Sungbok Lee</author>
<author>Ulrich Neumann</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Analysis of Emotion Recognition using Facial Expressions, Speech and Multimodal Information.</title>
<date>2004</date>
<booktitle>Proceedings of ACM 6th International Conference on Mutlmodal Interfaces (ICMI 2004),</booktitle>
<location>State College, PA,</location>
<contexts>
<context position="1776" citStr="Busso et al., 2004" startWordPosition="252" endWordPosition="255">hnology in life. 1 Introduction Emotion analysis as well as recommendation technology has drawn a lot attention in the natural language processing research community. The development of fundamental approaches as well as applications has been proposed (Das, 2011; Sarwar et al., 2001; Zheng et al., 2010). However, most of them were Internet applications, and to the best knowledge of the authors, these technologies have not yet been involved in the ambient creation. To create an intelligent living space, some researchers utilized the facial expression and speech recognizer 97 to detect emotions (Busso et al., 2004), but then the accompanied cameras and microphones were necessary. Some researchers tried to use sensors to watch the heart beat and the body temperature of residents to know their current emotion for further applications, but the problem was that users had to wear sensors and it was inconvenient. Instead of watching body signals, we postulate that the communications among people is one of the important factors to influence their emotions. Therefore, we tried to find clues from the textual conversations of the residents in order to detect their psychological state. There are many ways to categ</context>
</contexts>
<marker>Busso, Deng, Yildirim, Bulut, Lee, Kazemzadeh, Lee, Neumann, Narayanan, 2004</marker>
<rawString>Busso, Carlos, Deng, Zhigang, Yildirim, Serdar, Bulut, Murtaza, Lee, Chul Min, Kazemzadeh, Abe, Lee, Sungbok, Neumann, Ulrich and Narayanan, Shrikanth. 2004. Analysis of Emotion Recognition using Facial Expressions, Speech and Multimodal Information. Proceedings of ACM 6th International Conference on Mutlmodal Interfaces (ICMI 2004), State College, PA, Oct 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipankar Das</author>
</authors>
<title>Analysis and Tracking of Emotions in English and Bengali Texts: A Computational Approach.</title>
<date>2011</date>
<booktitle>Proceedings of the International World Wide Web Conference (WWW 2011), Ph. D. Symposium.</booktitle>
<pages>343--347</pages>
<contexts>
<context position="1418" citStr="Das, 2011" startWordPosition="197" endWordPosition="198">osphere. All settings can be modified by the multimedia controller at any time and the new settings will be feedback to the emotion analysis software. The IlluMe system, equipped with the learning function, provides a link between residential situation and personal emotion. It works in a Chinese chatting environment to illustrate the language technology in life. 1 Introduction Emotion analysis as well as recommendation technology has drawn a lot attention in the natural language processing research community. The development of fundamental approaches as well as applications has been proposed (Das, 2011; Sarwar et al., 2001; Zheng et al., 2010). However, most of them were Internet applications, and to the best knowledge of the authors, these technologies have not yet been involved in the ambient creation. To create an intelligent living space, some researchers utilized the facial expression and speech recognizer 97 to detect emotions (Busso et al., 2004), but then the accompanied cameras and microphones were necessary. Some researchers tried to use sensors to watch the heart beat and the body temperature of residents to know their current emotion for further applications, but the problem was</context>
</contexts>
<marker>Das, 2011</marker>
<rawString>Das, Dipankar, 2011. Analysis and Tracking of Emotions in English and Bengali Texts: A Computational Approach. Proceedings of the International World Wide Web Conference (WWW 2011), Ph. D. Symposium. 343-347.</rawString>
</citation>
<citation valid="false">
<booktitle>Dynastream Innovations Inc., ANT AT3 RF Transceiver Chipset _wDatasheet _tRev 1.2, http://ww.thisi san. com/.</booktitle>
<marker></marker>
<rawString>Dynastream Innovations Inc., ANT AT3 RF Transceiver Chipset _wDatasheet _tRev 1.2, http://ww.thisi san. com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Mining Opinions from the Web: Beyond Relevance Retrieval.</title>
<date>2007</date>
<journal>Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval,</journal>
<volume>58</volume>
<issue>12</issue>
<pages>1838--1850</pages>
<contexts>
<context position="10181" citStr="Ku and Chen, 2007" startWordPosition="1572" endWordPosition="1575">on Analysis The emotion analysis that IlluMe performed is to find the emotions that texts in messenger logs bear in order to create a comfort ambient by sound and lighting accordingly. To achieve this, the system needs to understand the Internet language first, and then detect emotions and categorize them. The system works on the Chinese chatting environment and analyzes Chinese texts to detect emotions. The materials, approaches, and preliminary results in the development phase are described in this section. 3.1 Experimental Materials Two dictionaries, the Chinese sentiment dictionary NTUSD (Ku and Chen, 2007) and the Chinese emotion dictionary (Lin et al., 2008), were adopted for detecting emotions. The former categorized sentiment words into positive and negative, while the latter into eight emotion types: awesome, heartwarming, surprising, sad, useful, happy, boring, and angry. Notice that these eight emotion Preference Controller User Input Emotion Analysis Component Mobile Device User Messages Settings Ambient Controller Music Ambient Learning Model Personal Info Lighting ANT Figure 3. Usage Illustration 99 types appeared in Yahoo! News Taiwan in the year 2008 and not all of them were general </context>
</contexts>
<marker>Ku, Chen, 2007</marker>
<rawString>Ku, Lun-Wei and Chen, Hsin-Hsi. 2007. Mining Opinions from the Web: Beyond Relevance Retrieval. Journal of American Society for Information Science and Technology, Special Issue on Mining Web Resources for Enhancing Information Retrieval, 58(12), 1838-1850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-His Chen</author>
</authors>
<title>SentenceLevel Opinion Analysis by CopeOpi</title>
<date>2008</date>
<booktitle>in NTCIR-7. Proceedings of the 7th NTCIR Workshop Meeting,</booktitle>
<pages>260--267</pages>
<location>Tokyo,</location>
<marker>Chen, 2008</marker>
<rawString>Ku, Lun-Wei, Liu, I-Chien, Lee, Chia-Ying, Chen, Kuan-hua. and Chen, Hsin-His. 2008. SentenceLevel Opinion Analysis by CopeOpi in NTCIR-7. Proceedings of the 7th NTCIR Workshop Meeting, Tokyo, Japan. 260-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Hsin-Yih Lin</author>
<author>Changhua Yang</author>
<author>HsinHis Chen</author>
</authors>
<title>Emotion Classification of Online News Articles from the Reader’s Perspective.</title>
<date>2008</date>
<booktitle>Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence.</booktitle>
<pages>220--226</pages>
<contexts>
<context position="10235" citStr="Lin et al., 2008" startWordPosition="1581" endWordPosition="1584">is to find the emotions that texts in messenger logs bear in order to create a comfort ambient by sound and lighting accordingly. To achieve this, the system needs to understand the Internet language first, and then detect emotions and categorize them. The system works on the Chinese chatting environment and analyzes Chinese texts to detect emotions. The materials, approaches, and preliminary results in the development phase are described in this section. 3.1 Experimental Materials Two dictionaries, the Chinese sentiment dictionary NTUSD (Ku and Chen, 2007) and the Chinese emotion dictionary (Lin et al., 2008), were adopted for detecting emotions. The former categorized sentiment words into positive and negative, while the latter into eight emotion types: awesome, heartwarming, surprising, sad, useful, happy, boring, and angry. Notice that these eight emotion Preference Controller User Input Emotion Analysis Component Mobile Device User Messages Settings Ambient Controller Music Ambient Learning Model Personal Info Lighting ANT Figure 3. Usage Illustration 99 types appeared in Yahoo! News Taiwan in the year 2008 and not all of them were general emotion states. Therefore, we tried to find Lin’s emot</context>
</contexts>
<marker>Lin, Yang, Chen, 2008</marker>
<rawString>Lin, Kevin Hsin-Yih, Yang, Changhua, and Chen, HsinHis. 2008. Emotion Classification of Online News Articles from the Reader’s Perspective. Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence. 220-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ortony</author>
<author>T J Turner</author>
</authors>
<title>What&apos;s basic about basic emotions?</title>
<date>1990</date>
<journal>Psychological Review,</journal>
<volume>97</volume>
<pages>315--331</pages>
<marker>Ortony, Turner, 1990</marker>
<rawString>Ortony, A. and Turner, T. J. 1990. What&apos;s basic about basic emotions? Psychological Review, 97, 315-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Parrott</author>
</authors>
<date>2001</date>
<booktitle>Emotions in Social</booktitle>
<publisher>Psychology, Psychology Press,</publisher>
<location>Philadelphia.</location>
<contexts>
<context position="2598" citStr="Parrott (2001)" startWordPosition="379" endWordPosition="380"> applications, but the problem was that users had to wear sensors and it was inconvenient. Instead of watching body signals, we postulate that the communications among people is one of the important factors to influence their emotions. Therefore, we tried to find clues from the textual conversations of the residents in order to detect their psychological state. There are many ways to categorize emotions. Different emotion states were used for experiments in previous research (Bellegarda, 2010). To find suitable categories of emotions, we adopted the three-layered emotion hierarchy proposed by Parrott (2001)1. Six emotions are in the first layer, including love, joy, surprise, anger, sadness and fear. The second layer includes 25 emotions, and the third layer includes 135 emotions. Using this hierarchical classification benefits the system. We can categorize emotions from rough to fine granularities and degrade to the upper level when the experimental materials are insufficient. How to map categories in other researches to ours becomes clearer, and annotators have more information when marking their current emotion. As to the music, most researchers looked for the emotions in songs or rhythms (Ya</context>
</contexts>
<marker>Parrott, 2001</marker>
<rawString>Parrott, W. 2001. Emotions in Social Psychology, Psychology Press, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Badrul Sarwar</author>
<author>George Karypis</author>
<author>Joseph Konstan</author>
<author>John Riedl</author>
</authors>
<title>ItemBased Collaborative Filtering Recommendation Algorithms.</title>
<date>2001</date>
<booktitle>Proceedings of the International World Wide Web Conference (WWW</booktitle>
<pages>285--295</pages>
<contexts>
<context position="1439" citStr="Sarwar et al., 2001" startWordPosition="199" endWordPosition="202">l settings can be modified by the multimedia controller at any time and the new settings will be feedback to the emotion analysis software. The IlluMe system, equipped with the learning function, provides a link between residential situation and personal emotion. It works in a Chinese chatting environment to illustrate the language technology in life. 1 Introduction Emotion analysis as well as recommendation technology has drawn a lot attention in the natural language processing research community. The development of fundamental approaches as well as applications has been proposed (Das, 2011; Sarwar et al., 2001; Zheng et al., 2010). However, most of them were Internet applications, and to the best knowledge of the authors, these technologies have not yet been involved in the ambient creation. To create an intelligent living space, some researchers utilized the facial expression and speech recognizer 97 to detect emotions (Busso et al., 2004), but then the accompanied cameras and microphones were necessary. Some researchers tried to use sensors to watch the heart beat and the body temperature of residents to know their current emotion for further applications, but the problem was that users had to we</context>
</contexts>
<marker>Sarwar, Karypis, Konstan, Riedl, 2001</marker>
<rawString>Sarwar, Badrul, Karypis, George, Konstan, Joseph, and Riedl, John. 2001. ItemBased Collaborative Filtering Recommendation Algorithms. Proceedings of the International World Wide Web Conference (WWW 2001), 285-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsi-Yao Su</author>
</authors>
<title>The Multilingual and MultiOrthographic Taiwan-Based Internet: Creative Uses of Writing Systems on College-Affiliated BBSs.</title>
<date>2003</date>
<journal>Journal of Computer-Mediated Communication</journal>
<volume>9</volume>
<issue>1</issue>
<note>http://jcmc.indiana.edu/vol9/issue1/su.html.</note>
<contexts>
<context position="12208" citStr="Su, 2003" startWordPosition="1918" endWordPosition="1919">hoto would be displayed to help annotators make their decisions. A total of 150 records are annotated for experiments and statistics are shown in Table 1. Emo 1 2 3 4 5 6 11 80 1 15 39 4 Color 1 2 3 4 5 6 7 14 6 5 25 9 5 11 14 8 9 10 11 12 13 14 15 11 7 4 13 7 15 5 13 Atm 1 2 3 4 5 6 28 40 16 33 17 16 Table 1. Statistics of Annotated Materials (Emo: Emotions, 1=Love, 2=Joy, 3=Surprise, 4=Angry, 5=Sad, 6=Fear; Color:15 color sets; Atm:6 atmospheres) 3.2 Interpretation of Zhuyin Wen When processing Internet Chinese texts, IlluMe transformed messenger logs and sentiment dictionaries into zhuyin (Su, 2003) before looking for emotions2. There were many reasons to do this. Zhuyin Wen (注音文) is one of many creative uses of writing systems in the Internet language. As Blakeman (2004) found in his study of English, Internet language is fraught with initializations. However, as to the traditional Chinese, both Wikipedia and Zhang and Dai (2006) indicated that stylized initials and stylized numbers are 2 Lookup Table: http://cclookup.cctserver.com/ rarely used in Taiwan. Su reported that the most popular type of creative use of writing systems is “Zhuyin Wen” (注音文). In “Zhuyin Wen” the complete phoneti</context>
</contexts>
<marker>Su, 2003</marker>
<rawString>Su, Hsi-Yao. 2003. The Multilingual and MultiOrthographic Taiwan-Based Internet: Creative Uses of Writing Systems on College-Affiliated BBSs. Journal of Computer-Mediated Communication 9(1). http://jcmc.indiana.edu/vol9/issue1/su.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi-Hsuan Yang</author>
<author>Homer H Chen</author>
<author>IEEE Fellow</author>
</authors>
<title>Ranking-Based Emotion Recognition for Music Organization and Retrieval.</title>
<date>2011</date>
<booktitle>IEEE Transactions on audio, speech, and language processing,</booktitle>
<pages>19--4</pages>
<marker>Yang, Chen, Fellow, 2011</marker>
<rawString>Yang, Yi-Hsuan and Chen, Homer H., Fellow, IEEE. 2011. Ranking-Based Emotion Recognition for Music Organization and Retrieval. IEEE Transactions on audio, speech, and language processing, 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence M Zbikowski</author>
</authors>
<title>Music, Emotion, Analysis. Music Analysis,</title>
<date>2011</date>
<publisher>Blackwell Publishing Ltd,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="3233" citStr="Zbikowski, 2011" startWordPosition="476" endWordPosition="477"> in the first layer, including love, joy, surprise, anger, sadness and fear. The second layer includes 25 emotions, and the third layer includes 135 emotions. Using this hierarchical classification benefits the system. We can categorize emotions from rough to fine granularities and degrade to the upper level when the experimental materials are insufficient. How to map categories in other researches to ours becomes clearer, and annotators have more information when marking their current emotion. As to the music, most researchers looked for the emotions in songs or rhythms (Yang and Chen, 2011; Zbikowski, 2011). They classified music into different emotional categories and developed the system to tell what emotion a song might bring to a listener. However, if the aim is to create a 1 http://changingminds.org/explanations/emotions/ basic%20emotions.htm Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 97–102, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics comfortable ambient, what songs a person in a certain emotional state wants to listen to becomes the question. A happy user does not always enjoy happy songs, an</context>
</contexts>
<marker>Zbikowski, 2011</marker>
<rawString>Zbikowski, Lawrence M., 2011. Music, Emotion, Analysis. Music Analysis, Blackwell Publishing Ltd, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiawei Zhang</author>
<author>Jiaxing Dai</author>
</authors>
<title>Qiantan shixia qingnian wangluo yongyu –huoxing wen</title>
<date>2006</date>
<booktitle>Apff-F -F *� f141%Mi — x R1 3Z</booktitle>
<contexts>
<context position="12546" citStr="Zhang and Dai (2006)" startWordPosition="1971" endWordPosition="1974">nnotated Materials (Emo: Emotions, 1=Love, 2=Joy, 3=Surprise, 4=Angry, 5=Sad, 6=Fear; Color:15 color sets; Atm:6 atmospheres) 3.2 Interpretation of Zhuyin Wen When processing Internet Chinese texts, IlluMe transformed messenger logs and sentiment dictionaries into zhuyin (Su, 2003) before looking for emotions2. There were many reasons to do this. Zhuyin Wen (注音文) is one of many creative uses of writing systems in the Internet language. As Blakeman (2004) found in his study of English, Internet language is fraught with initializations. However, as to the traditional Chinese, both Wikipedia and Zhang and Dai (2006) indicated that stylized initials and stylized numbers are 2 Lookup Table: http://cclookup.cctserver.com/ rarely used in Taiwan. Su reported that the most popular type of creative use of writing systems is “Zhuyin Wen” (注音文). In “Zhuyin Wen” the complete phonetic representation of a character is reduced to a consonant, or sometimes a vowel. This creative use appeared commonly in the collected conversations. Generally we had to figure out the missing vowels to understand the word, but in our system a reversed approach (dropping vowels) was adopted to make sure the system did not miss any possib</context>
</contexts>
<marker>Zhang, Dai, 2006</marker>
<rawString>Zhang, Jiawei and Dai, Jiaxing. 2006. Qiantan shixia qingnian wangluo yongyu –huoxing wen Apff-F -F *� f141%Mi — x R1 3Z</rawString>
</citation>
<citation valid="false">
<note>http://www.shs.edu.tw/works/essay/2006/03/2006032 816043532.pdf</note>
<marker></marker>
<rawString>http://www.shs.edu.tw/works/essay/2006/03/2006032 816043532.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent W Zheng</author>
<author>Bin Cao</author>
<author>Yu Zheng</author>
<author>Xing Xie</author>
<author>Qiang Yang</author>
</authors>
<title>Collaborative Filtering Meets Mobile Recommendation: A User-centered Approach</title>
<date>2010</date>
<booktitle>Proceedings of Twenty-Fourth National Conference on Artificial Intelligence (AAAI-10).</booktitle>
<contexts>
<context position="1460" citStr="Zheng et al., 2010" startWordPosition="203" endWordPosition="206">ified by the multimedia controller at any time and the new settings will be feedback to the emotion analysis software. The IlluMe system, equipped with the learning function, provides a link between residential situation and personal emotion. It works in a Chinese chatting environment to illustrate the language technology in life. 1 Introduction Emotion analysis as well as recommendation technology has drawn a lot attention in the natural language processing research community. The development of fundamental approaches as well as applications has been proposed (Das, 2011; Sarwar et al., 2001; Zheng et al., 2010). However, most of them were Internet applications, and to the best knowledge of the authors, these technologies have not yet been involved in the ambient creation. To create an intelligent living space, some researchers utilized the facial expression and speech recognizer 97 to detect emotions (Busso et al., 2004), but then the accompanied cameras and microphones were necessary. Some researchers tried to use sensors to watch the heart beat and the body temperature of residents to know their current emotion for further applications, but the problem was that users had to wear sensors and it was</context>
</contexts>
<marker>Zheng, Cao, Zheng, Xie, Yang, 2010</marker>
<rawString>Zheng, Vincent W., Cao, Bin, Zheng, Yu, Xie, Xing and Yang, Qiang. 2010. Collaborative Filtering Meets Mobile Recommendation: A User-centered Approach Proceedings of Twenty-Fourth National Conference on Artificial Intelligence (AAAI-10).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>