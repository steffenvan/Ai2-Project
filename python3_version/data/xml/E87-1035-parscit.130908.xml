<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<sectionHeader confidence="0.913486666666667" genericHeader="abstract">
DETERMINISTIC PARSING
AND
UNBOUNDED DEPENDENCIES
</sectionHeader>
<author confidence="0.733689">
Ted Briscoe
</author>
<affiliation confidence="0.948259">
Dept of Linguistics, Lancaster University
</affiliation>
<address confidence="0.7119225">
Bailrigg, Lancashire
LA1 4YT, UK
</address>
<email confidence="0.803842">
ABSTRACT
</email>
<bodyText confidence="0.999882294117647">
This paper assesses two new approaches to
deterministic parsing with respect to the analysis of
unbounded dependencies (UDs). UDs in English are highly
locally (and often globally) ambiguous. Several researchers
have argued that the difficulty of UDs undermines the
programme of deterministic parsing. However, their
conclusion is based on critiques of various versions of the
Marcus parser which represents only one of many possible
approaches to deterministic parsing. We examine the
predictions made by a LR(1) deterministic parser and the
Lexicat deterministic parser concerning the analysis of
UDs. The LR(1) technique is powerful enough to resolve
the local ambiguities we examine. However, the Lexicat
model provides a more psychologically plausible account
of the parsing of UDs, which also offers a unified account
of the resolution of local and global ambiguities in these
constructions.
</bodyText>
<sectionHeader confidence="0.99433" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999806333333333">
Church (1980:117) and Johnson-Laird (1983:313) have
argued that the high degree of ambiguity in unbounded
dependencies undermines the programme of deterministic
parsing. Their conclusion is based on critiques of various
versions of the Marcus parser (Marcus, 1980; Berwick &amp;
Weinberg, 1984). This parser represents only one of many
possible approaches to deterministic parsing. Therefore, the
conclusion that deterministic parsing, in general, is
impractical or psychologically implausible may be
premature.
In the next section, we outline the problems for the
deterministic analysis of unbounded dependencies. In the
succeeding sections, we present two alternative parsing
techniques (and associated grammars) which make
differing predictions concerning the onset and location of
indeterminacy in the analysis of unbounded dependencies.
We argue that the LR(1) parser is capable of
deterministically resolving the local ambiguities which
occur in these constructions, whilst the Lexicat parser is
not. In the final section, we evaluate these predictions in
the light of the Determinism Hypothesis (Marcus, 1980)
and the Interactive Determinism Hypothesis (Briscoe &amp;
Boguraev, 1984; Briscoe, in press) and argue that the
Lexicat parser in conjunction with the Interactive
Determinism Hypothesis provides the most psychologically
plausible and unified account of the parsing of unbounded
dependencies.
</bodyText>
<sectionHeader confidence="0.993065" genericHeader="method">
UNBOUNDED DEPENDENCY AMBIGUITIES
</sectionHeader>
<bodyText confidence="0.981882333333333">
Unbounded dependencies are found in English
constituent questions, relative clauses and topicalised
constructions. The dependency is between the preposed
constituent and its point of attachment. For example, in
(1) Who is preposed and functioning as direct object of
the transitive verb like.
</bodyText>
<listItem confidence="0.897161">
(1) Who do you like e
</listItem>
<bodyText confidence="0.9990006875">
Most current theories of grammar represent the
grammatical role of the preposed constituent by
associating it with the normal position of a constituent
having that grammatical role. In several theories, this
position is occupied by a phonologically null category or
trace which is grammatically linked to the preposed
constituent. We will use e to mark this position because
each of the grammars associated with the parsers we
consider adopts a &apos;positional&apos; account of the recovery of
the grammatical role of the preposed constituent.
However, we use e to mark an unambiguous point of
attachment without any commitment to the presence of
phonologically null categories or structure in the syntactic
representation of unbounded dependencies.
The dependency between preposed constituent and
point of attachment is unbounded because an unlimited
amount of lexical material can occur between these two
points in grammatical English constructions of this type.
For example, it is possible to construct more and more
embedded examples like those in (2) which exhibit the
same dependency as (1).
Who do you think Kim likes e
Who do you expect that Kim hopes Sandy likes e
A parser for English capable of producing a syntactic
representation adequate to guide semantic interpretation
must recover the grammatical role of the preposed
constituent. However, whenever these constructions
contain verbs of ambiguous valency the correct point of
attachment for the preposed constituent also becomes
ambiguous. For example, in (3) there are two potential
attachment points, or doubtful gaps (Fodor, 1979), written
e?.
</bodyText>
<listItem confidence="0.964594">
(3) Who do you want e? to succeed e?
</listItem>
<bodyText confidence="0.996055">
The correct attachment of Who is ambiguous because
both want and succeed can take, but do not require, NP
objects.
</bodyText>
<page confidence="0.997218">
211
</page>
<bodyText confidence="0.9991165">
The ambiguity in (3) is global with respect to the
sentence; however, identical local ambiguities exist for a
parser operating incrementally from left to right. For
example, in (4) the attachment of Who as object of want,
although correct, remains doubtful until the end of the
sentence.
</bodyText>
<listItem confidence="0.89894">
(4) Who do you want e? to succeed Bill
</listItem>
<bodyText confidence="0.999806">
Thus, at the point when the parser reaches a potential but
ambiguous attachment point in the left-to-right analysis of
the input, it cannot be sure that this is the correct
attachment point because there may be another further
downstream in the input, as in (3). Moreover, the point of
attachment further downstream may be unambiguous and
obligatory, resolving the local ambiguity in the other
direction, as in (5).
</bodyText>
<listItem confidence="0.749153">
(5) Who do you want e? to replace e
</listItem>
<bodyText confidence="0.9739359375">
To resolve the local ambiguities in unbounded
dependencies the parser requires access to an unbounded
amount of left and right context, measured in terms of
lexical material. Firstly, when a potential attachment point
is found, the parser must know whether or not a preposed
constituent exists to be attached. This requires potentially
unbounded access to the left context of the analysis since
the preposed constituent could have occurred an
unbounded distance back from its point of attachment.
Secondly, when a potential but ambiguous attachment
point is found, the parser must decide whether it is the
correct point of attachment. However, since this decision
cannot be made determinately when the potential
attachment point occurs, the parser requires access to the
right context of forthcoming material downstream from
the current position. The examples in (6) illustrate that
this would require unbounded lookahead.
Who does Kim want e? to think that the boss will
replace Sandy (with e)
Who does Kim want e? to think that the boss
expects the directors to replace Sandy (with e)
In (6) the correct point of attachment cannot be
determined until the end of the sentence which can be
arbitrarily far away (in terms of lexical material) in the
right context.
Berwick &amp; Weinberg (1984:153f) argue that the
Marcus parser can adequately represent an unbounded left
context with finite resources if a successive cyclic and
trace theoretic analysis (eg. Chomsky, 1981) of
unbounded dependencies is adopted. However, both
Church (1980) and Fodor (1985) demonstrate that the
three cell lookahead buffer in the Marcus parser is not
powerful enough to provide the required access to the
right context in order to choose the correct point of
attachment deterministically in many unbounded
dependency constructions.
Marcus&apos; (1980) Determinism Hypothesis claims that
local ambiguities which are not resolvable in terms of the
lookahead buffer are resolved by parsing strategy and that
therefore, many unbounded dependency constructions
should be psychologically complex, &apos;garden paths&apos;
requiring extensive reanalysis. There is some evidence for
syntactic preferences in the analysis of unbounded
dependencies; the oddity of the examples in (7), which all
require attachment of the preposed phrase in a doubtful
position, suggests that the human parser prefers to ignore
doubtful points of attachment and wait for a later one.
a) Who did you want to give the present to Sue?
b) I gave the boy who you wanted to give the
books to three books.
c) Sue wouldn&apos;t give the man who was reading the
book
However, as Fodor (1985) points out, the great majority
of unbounded dependency constructions are not complex.
Furthermore, the Marcus parser predicts a sharp
distinction between &apos;short&apos; unbounded dependencies
which fall within the buffer and the remainder. No such
distinction seems to be supported by the psychological
data. Finally, unbounded dependencies exhibit an identical
kind of ambiguity which can be either local or global.
Therefore, we would expect a unified account of their
resolution, but the Determinism Hypothesis offers no
account of the resolution of global ambiguities (see eg.
Briscoe, in press).
</bodyText>
<sectionHeader confidence="0.993996" genericHeader="method">
ALTERNATIVE DETERMINISTIC PARSERS
</sectionHeader>
<bodyText confidence="0.999994935483871">
There are a number of alternative deterministic parsing
techniques many of which are in common use in
compilers for high-level computer programming
languages. Berwick (1985:3130 compares the Marcus
parser to the most general of these techniques, LR(1)
parsing (eg. Aho &amp; Ullman, 1972), and argues that the
Marcus parser can be seen as both an extension and
restriction of the LR(1) technique. In fact, he argues that
it is equivalent to a bounded context parser (eg. Aho &amp;
Ullman, 1972) which only allows literal access to
grammatical symbols in the c-command domain in the
left context and to two grammatical symbols in the right
context.
To date, little attention has been given to alternative
deterministic techniques as models of natural language
parsing in their own right, though. One exception is the
work of Shieber (1983) and Pereira (1985), who have
proposed that a simple extension of the LALR(1)
technique can be used to model human natural language
parsing strategies. The LALR(1) technique is a more
efficient variant of the LR(1) technique. Since our
implementation of the Shieber/Pereira model uses the
latter technique, we will refer throughout to LR(1). With
the grammar discussed below, the behaviour of a parser
using either technique should be identical (see Aho &amp;
Ullman, 1972). In addition, Briscoe &amp; Boguraev (1984)
and Briscoe (in press) propose that a bounded context,
deterministic parser in conjunction with an extended
categorial grammar will also model these strategies.
Below these two alternative approaches are compared
with respect to unbounded dependency constructions.
</bodyText>
<page confidence="0.988608">
212
</page>
<subsectionHeader confidence="0.889248">
The Shieber/Pereira Parser
</subsectionHeader>
<bodyText confidence="0.959704129032258">
The LR(1) technique involves extensive preprocessing
of the grammar used by the parser to compute all the
distinct analysis &apos;paths&apos; licensed by the grammar. This
preprocessing results in a parse table which will
deterministically specify the operations of a shift-reduce
parser provided that the input grammar is an `LR(1)
grammar&apos;; that is, provided that it is drawn from a subset
of the unambiguous context-free grammars (see Aho &amp;
Ullman, 1972; Briscoe, in press). The parse table encodes
the set of possible left contexts for an LR(1) grammar as
a deterministic finite-state machine. In intuitive terms, the
LR(1) technique is able to resolve deterministically a
subset of the possible local ambiguities which can be
represented in a context-free grammar, and none of the
global ambiguities. If an LR(1) parsing table is
constructed from a grammar covering a realistic,
ambiguous fragment of English, the resulting non-
deterministic parsing table will contain &apos;clashes&apos; between
shift and reduce operations and between different reduce
operations. Shieber and Pereira demonstrate that if
shift/reduce clashes are resolved in favour of shifting and
reduce/reduce clashes in favour of reducing with the rule
containing the most daughters, then the parser will model
several psychologically plausible parsing strategies, such
as right association (eg. Frazier, 1979).
Shieber (1983) and Pereira (1985) both provide
grammars with a GPSG-style (Gazdar et al., 1985)
SLASH feature analysis of unbounded dependencies. (8)
presents a- grammar fragment written in the same style to
mimic the GPSG account of unbounded dependencies in
a context-free notation.
</bodyText>
<figure confidence="0.943553333333333">
(8)
Terminals
Det N Vtr V Aux want to wh $
Non-terminals
SENT S VP VPinf VP/NP VPinf/NP NPwh NP
0) SENT —&gt; S $
</figure>
<listItem confidence="0.982545166666666">
1) S —&gt; NP VP
2) NP —&gt; Det N
3) VP —&gt; Vtr NP
4) VP/NP —&gt; Vtr
5) NP —&gt; N
6) NPwh —&gt; wh
7) VP/NP —&gt; want VPinf/NP
8) VP/NP —&gt; want VPinf
9) VPinf —&gt; to VP
10) VP —&gt; V NP
11) VP —&gt; V
12) VPinf/NP —&gt; to VP/NP
13) VP/NP —&gt; V
14) S —&gt; NPwh Aux VP
15) S —&gt; NPwh Aux NP VP/NP
16) VP —&gt; want VPinf
17) VP —&gt; want NP VPinf
18) VP/NP —&gt; want NP VPinf/NP
</listItem>
<bodyText confidence="0.926623">
The LR(1) technique applied to this grammar is very
successful at resolving local ambiguities in these
constructions; neither of the sentences in (9) result in any
indeterminacy during parsing, despite the potential local
ambiguity over the attachment of the preposed
constituent.
</bodyText>
<figure confidence="0.690566">
(9)
a) Who do you want Bill to succeed?
b) Who do you want to succeed Bill?
</figure>
<bodyText confidence="0.8925375">
That is, these local ambiguities fall within the subset of
possible local ambiguities representable in a context-free
grammar which are resolvable by this technique. On the
other hand, parsing the globally ambiguous example in
(3) using the same parse table derived from this grammar
results in a reduce/reduce conflict, because the LR(1)
technique cannot resolve global ambiguity (by definition).
The conflict arises when the parser is in the configuration
shown in Figure 1. At this point, the parser must choose
between reducing succeed to VP or to VP/NP. When this
indeterminacy arises the entire sentence has been shifted
onto the parse stack.
</bodyText>
<subsectionHeader confidence="0.307346">
Stack Input Buffer
</subsectionHeader>
<bodyText confidence="0.4080055">
NPwh Aux NP want to V $
Who do you want to succeed
</bodyText>
<figureCaption confidence="0.989567">
Figure 1 - Configuration of LR(1) Parser
</figureCaption>
<bodyText confidence="0.880008655172414">
In general, because of the LR technique of preprocessing
the grammar and despite the unbounded nature of the
ambiguity, the decision point will always be at the end of
the sentence. Therefore, local ambiguities involving the
point of attachment of preposed constituents will not
involve parsing indeterminacy using this technique. In
this instance, the suspicion arises that the power of
technique may be too great for a model of human parsing
because examples such as those in (7) above do appear to
be garden paths. However, normally such effects are only
predicted when a parsing conflict is resolved incorrectly
by the rules of resolution (eg. Shieber, 1983) and no
conflict will arise parsing these examples with a grammar
like that in (8).
At first sight it is surprising that these local
ambiguities cause no problems since an LR(1) parser
appears to have less access to the right context than the
Marcus parser. However, the LR(1) parser makes greater
use of the left context and also delays many syntactic
decisions until most of the input is in the parse stack; in
the configuration in Figure 1 no clause level attachments
have been made, despite the fact that the complete
sentence has been shifted into the parse stack.
The reduce/reduce conflict in the globally ambiguous
case occurs much later than the position of the initial
doubtful attachment point. Moreover, the conflict cannot
be resolved using the Shieber/Pereira resolution rules as
they stand, since both possible reductions (VP —&gt; V;
VP/NP —&gt; V) only involve one daughter.
</bodyText>
<page confidence="0.995569">
213
</page>
<subsectionHeader confidence="0.683498">
The Lexicat Parser
</subsectionHeader>
<bodyText confidence="0.999957133333333">
The LEXIcal-CATegorial parser is a deterministic,
shift-reduce parser developed for extended categorial
grammars which include a rule of syntactic composition,
as well as the more usual rule of application. An earlier
version of the parser is briefly described in Briscoe &amp;
Boguraev (1984). Briscoe (in press) provides a complete
description of Lexicat. Ades &amp; Steedman (1982),
Steedman (1985) and Briscoe (in press) discuss
composition in further detail from the perspectives of
syntax, semantics and parsing.
In a categorial grammar most syntactic information is
located in the assignment of categories to lexical items.
The rules of composition and application and a lexicon
which suffices for the fragment under consideration are
given in (10).
</bodyText>
<equation confidence="0.969936444444444">
(10)
Function-Argument Application
X YIX =&gt; Y
Function-Function Composition
X1Y YIZ =&gt; XIZ
Bill : NP to : VPinf/VP
you : NP do : S/VP/NP
who : NP succeed : VPINP, VP
want : VP/VPinfINP, VP/VPinf
</equation>
<bodyText confidence="0.860297333333333">
This grammar assigns the two analyses shown in Figure 2
to the ambiguous example (3).
who do you want to succeed
</bodyText>
<figure confidence="0.615532523809524">
NP S/VP/NP NP VP/VPinf VPinf/VP VPINP
App
S/VP
—Comp
S/VPinf
—Comp
S/VP
—Comp
SINP
— — — —App
who do you want to suc.
NP S/VP/NP NP VP/VPinflNP VPinf/VP VP
App
S/VP
-7-
SNPinfiNP
App
S/VPinf
—Comp
S/VP
App
</figure>
<figureCaption confidence="0.885664">
Figure 2 - Analysis of Unbounded Dependencies
</figureCaption>
<bodyText confidence="0.996142133333333">
The grammar represents the grammatical role of the
preposed constituent by relating it directly to the verbal
category. The material intervening between the preposed
constituent and its point of attachment is composed into
one (partial) constituent. Steedman (1985) provides
linguistic motivation for a very similar analysis.
Lexicat employs one push down stack for analysis and
storage of partially analysed material in the left and right
context. Parsing proceeds on the basis of a three cell
window into the stack. The item in the first cell (at the
right hand end) of the stack represents a one word
lookahead into the right context. This cell can only
contain the lexical entry for the next word in the input.
So, in common with LR(1) parsers but unlike the Marcus
parser, Lexicat is restricted to lookahead of one lexical
item. The second cell contains the syntactic category or
categories associated with the current (partial) constituent
under analysis. The third cell provides the left context for
the parse and can contain the syntactic category or
categories associated with the adjacent (partial)
constituent to the left of the current constituent. Cells
further down the stack contain (partial) constituents
awaiting further integration into the analysis, but do not
form part of the left context for parsing decisions.
Lexicat is a (1,1)-bounded context parser because it
only has access to one set of grammatical symbols to the
left and one set of grammatical symbols to the right of
the current constituent (see Briscoe, in press). As such it
is demonstrably less powerful than the LR(1) technique,
which allows access to any aspect of the left context
which can be represented as a regular expression, and the
Marcus parser, which allows access to grammatical
symbols in the c-command domain in the left context and
two (not necessarily terminal) symbols in the right
context (eg. Berwick, 1985:3130. However, it is unclear,
at present, what precisely can be concluded on the basis
of these differences in the parsing techniques because of
the differing properties of the grammatical theories
employed in each model.
The Lexicat parser does not employ a parse table or
use parsing states to maintain information about the
nature of the left context. Rules of application and
composition (with various restrictions on the directionality
of reduction and the range of categories to which each
rule applies) are used directly by the parser to perform
reductions. There are two stages to each step in the
parsing algorithm; a checking phase and a reduction
phase. A rule of forward application and a rule of
forward composition are used to check for the possibility
of reduction between the categories in Cell 1 and Ce112. If
reduction is possible, Lexicat shifts the next item from
the input buffer onto the stack. If reduction is impossible,
Lexicat moves to the reduction phase and attempts a
reduction between Ce112 and Ce113 using rules of
backward and forward application and a more constrained
rule of forward composition. If this fails, then Lexicat
shifts. This completes one step of the parsing algorithm,
so Lexicat returns to the checking phase. This process
continues until the parse is complete or the input is
—Comp
</bodyText>
<page confidence="0.996897">
214
</page>
<bodyText confidence="0.990051">
exhausted, at which point the parser halts under the usual
conditions.
Quite often, a shift/reduce conflict arises during the
checking phase in which case Lexicat opts, by default, to
shift. In most constructions this resolution rule results in
analyses which conform to the parsing strategies of late
closure and right association (eg. Frazier, 1979).
However, in unbounded dependencies it results in late
attachment of the preposed constituent. For example, if
Lexicat is in the configuration shown in Figure 3, then a
shift/reduce conflict will occur in the checking phase.
</bodyText>
<figure confidence="0.5948445">
Stack Input Buffer
3 2 1
NP S/VPinflNP VPinfNP succeed
S/VPinf
Who (do you want) to
Fig= 3 - Configuration of Lexicat Parser
</figure>
<bodyText confidence="0.995932648648648">
The ambiguity of the partial constituent do you want
results from the ambiguous valency of want. Depending
on which category in Ce112 is chosen, reduction by
forward composition will or will not be possible. By
default, Lexicat will shift in the face of this conflict; thus
the potential for reduction by backwards application
between Ce112 and Ce113 will not be considered during
this step. In the next configuration, the preposed
constituent will be in Ce114 outside the parser&apos;s &apos;window&apos;
into the stack. Therefore, the possibility of attaching the
preposed constituent does not arise again until do you
want to succeed has been composed into one (partial)
constituent. At this point, the only possible attachment
which remains is as object of succeed.
If the parser is analysing (9a), and Bill rather than to
is the item in Celli in the configuration in Figure 3, then
essentially the same situation obtains; there will be a
shift/reduce conflict, shift will be chosen by default and
the parser will go on to build the late attachment
analysis. If, on the other hand, the parser is analysing
(9b) and Bill is in the input buffer at the end of the
sentence, the parse configuration at the moment of
indeterminacy will still be as in Figure 3 and the same
default analysis will be chosen since the parser has no
access to the contents of the input buffer to guide its
decisions. However, in this case the parse will fail
because Bill will be attached as object of succeed and
Who will be left dangling.
Unlike the LR(1) model, Lexicat faces parsing
indeterminacy at the point when the first potential point
of attachment occurs. The resolution rule in favour of
shifting predicts that late attachment of preposed
constituents is preferred and this prediction is compatible
with the garden path data in (7). The Lexicat parser
employs the grammar directly without preprocessing and
therefore conforms to Berwick &amp; Weinberg&apos;s (1984)
transparency condition.
</bodyText>
<sectionHeader confidence="0.843617" genericHeader="method">
INTERACTIVE DETERMINISM
</sectionHeader>
<bodyText confidence="0.997953537037037">
Marcus&apos; (1980) Determinism Hypothesis claims that
local ambiguity is resolved autonomously either by
lookahead or, if this fails, by parsing strategy. This
predicts that strategy-violating local ambiguities which
fall outside the span of the Marcus parser&apos;s lookahead
buffer will be garden paths. The theory tells us little
about the resolution of global ambiguities, but implies
that the mechanism employed must be similar to that
used to recover from garden paths, involving interaction
with other components of the language comprehension
system.
Using the Determinism Hypothesis, it is difficult to
select between the two models outlined above (or indeed
to conclusively rule out the Marcus parser) because the
differing predictions concerning the onset of
indeterminacy in the face of identical ambiguities are
unimportant. The Determinism Hypothesis concerns only
judgements of psychological complexity. These
judgements marginally favour the Lexicat parser, but the
data relating to garden paths with unbounded
dependencies is hardly overwhelming. Moreover, the
Determinism Hypothesis seems highly suspect in the light
of the unbounded dependency examples because it
predicts that local and global ambiguities are resolved
using completely different mechanisms. At the very least,
this approach is unparsimonious and leaves the resolution
of global ambiguity largely unexplained. In addition, the
extreme similarity between local and global ambiguities
in unbounded dependency constructions suggests that one
mechanism for the resolution of local and global
ambiguity is quite feasible.
Briscoe &amp; Boguraev (1984) and Briscoe (in press)
propose a different account of the relationship between
parsing and other components of comprehension than that
entailed by the Determinism Hypothesis, dubbed the
Interactive Determinism Hypothesis (IDH). Under this
account of deterministic parsing, the parser proceeds
autonomously until it is faced with an indeterminacy and
then requests help from other components of the
comprehension system. By default, the parser will apply a
resolution rule or parsing strategy at such a point, but this
can be overruled by specific non-syntactic information at
the onset of the indeterminacy. The IDH implies that both
local and global ambiguity is resolved at its onset
(relative to some parsing technique) either by strategy or
interactive blocking of the strategy. The IDH predicts, in
addition, that garden paths will arise when a strategy-
violating, local ambiguity is not resolved interactively, as
a result of the absence or removal of the relevant non-
syntactic information.
Under the IDH, the differing predictions concerning
the onset of indeterminacy in ambiguous unbounded
dependency constructions become crucial in any
comparison of the two parsing models outlined above.
</bodyText>
<page confidence="0.996189">
215
</page>
<bodyText confidence="0.990478257425743">
The Lexicat parser makes far stronger predictions because
indeterminacy occurs much earlier in the analysis when
less of the input is available in the left context. Since
Lexicat prefers late attachment by default, it predicts that
when a doubtful point of attachment is reached, which is
the correct point of attachment, non-syntactic information
in the available left context should block the preference
to shift and force early reduction with the preposed
constituent. By contrast, the Shieber/Pereira parser does
not meet an indeterminacy except in globally ambiguous
cases and then not until all the input is in the left
context. It therefore predicts in conjunction with the IDH
that there should be no garden paths involving unbounded
dependencies and that there should be some non-syntactic
information in the entire input which resolves the global
ambiguity. The former prediction appears to be wrong in
the light of the examples in (7) and the latter is so weak
as to be trivial.
It turns out that there is some evidence supporting the
far stronger predictions of the Lexicat model in
conjunction with the IDH. This evidence comes from the
the distribution of prosodic boundaries in relation to the
onset of strategy-violating syntactic ambiguities. For
example in (11)
(11) Without her, contributions to the fund would be
inadequate.
the comma (an orthographic counterpart to certain types
of prosodic boundary) marks the location of an
intonational or major tone group boundary which would
normally occur in the spoken version of this sentence.
The prosodic boundary prevents the potential
misinterpretation in which her contributions is reduced as
one NP. In unbounded dependency constructions, Danly
(reported in Cooper &amp; Paccia-Cooper, 1980:1590 has
demonstrated that the final syllable of the verb preceeding
a correct attachment point is lengthened relative to an
environment without a potential attachment point, or with
a potential but incorrect one. Syllable lengthening is
indicative of a phrasal or minor tone group boundary.
Paul Warren and the author have since tested Danly&apos;s
result by acoustically analysing ten readers&apos; productions
of four examples containing doubtful but correct early
points of attachment and four similar examples with
doubtful and incorrect early attachment points. The results
tend to confirm the original finding since lengthening was
found consistently (although the measurements did not
achieve statistical significance; see Briscoe, in press). A
final piece of evidence that lengthening occurs before a
correct point of attachment comes from the acceptability
of contraction in (12a), but not in (12b).
a) Who do you wanna succeed
b) *Who do you wanna succeed Bill
Contraction forces late attachment of Who in a), but b) is
unacceptable because the only possible interpretation
involves attachment `into&apos; the contracted form. Fodor
(1979:277n17) notes that it is only the occurrence of
contraction which appears to provide determinate
information about the correct analysis and that, since
contraction is optional, this information cannot be relied
on. However, metrical phonologists (eg. Nespor &amp; Vogel,
1986) argue that such rules are not blocked syntactically
by the presence of the trace/gap, but by an intervening
prosodic boundary and that this explains the coincidence
of other phonetic effects, such as lengthening, at points
where contraction is blocked (Cooper &amp; Paccia-Cooper,
1980:Ch10). In other words, contraction is the tip of a far
more systematic prosodic iceberg which does reliably cue
the presence of a correct attachment point.
When Lexicat reaches a potential point of attachment,
it is faced with a shift/reduce ambiguity. By default,
Lexicat prefers to shift, but this strategy can be blocked
by a prosodic boundary intervening between the verb and
item about to be shifted into the parse stack. Therefore,
the parser opts for early attachment of the preposed
constituent. In terms of Lexicat&apos;s operation, the prosodic
boundary in the unbounded dependency construction
plays the same role as the prosodic boundary in (11);
they both block the shift operation. By contrast, in the
Shieber/Pereira parser it is difficult to see how a prosodic
boundary in unbounded dependencies could be used to
select one of two possible reductions, whilst in an
example like (11) it would need to force the parser to
shift rather than reduce. In addition, the relevant non-
syntactic information occurs at the onset of the
indeterminacy for the Lexicat model but well before this
point for the Shieber/Pereira model. This corroborates the
far stronger prediction made by Lexicat, and also makes
the mechanism of interaction for this model simpler (see
Briscoe, in press).
Finally, we should note that in the garden paths in (7)
it is intuitively clear that examples b) and c) would be
spoken with prosodic boundaries at the correct attachment
point, and probably written with commas otherwise.
Example a) on the other hand, is more subtle, but the
experimental results reported above suggest that want
would be lengthened in this context signalling the early
attachment point. Thus, the IDH&apos;s prediction that garden
paths are the result of the removal or distortion of non-
syntactic information which functions to prevent the
parser&apos;s default analysis in the face of indeterminacy is
corroborated.
</bodyText>
<sectionHeader confidence="0.996425" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999991076923077">
The paper has presented two approaches to the
deterministic analysis of unbounded dependencies. The
LR(1) technique is capable of resolving the type of local
ambiguities which appear to occur in these constructions,
suggesting that Church and Johnson-Laird were wrong to
reject deterministic parsing on the basis of this data.
However, we have argued that the Lexicat parser
provides a better psychological model of the parsing of
unbounded dependencies because a) it predicts the garden
path data and b), in conjunction with the IDH, it predicts
the apparent distribution of prosodic boundaries in these
constructions more successfully, and c) it provides a
unified account of the resolution of local and global
</bodyText>
<page confidence="0.995313">
216
</page>
<bodyText confidence="0.999939">
ambiguities, and d) it is a simpler model of detenninistc
parsing which does not require preprocessing the
grammar or maintaining state information concerning the
left context.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998050854166667">
Ades, A. &amp; Steedman, M. (1982). On the order of words.
Linguistics &amp; Philosophy, 4, 517-558.
Aho, A. &amp; Ullman, J. (1972). The Theory of Parsing,
Translating and Compiling. Vol. I, Englewood Cliffs, NJ:
Prentice-Hall.
Berwick, R. (1985). The Acquisition of Syntactic
Knowledge. Cambridge, Mass.: MIT Press.
Berwick, R. &amp; Weinberg, A. (1984). The Grammatical
Basis of Linguistic Performance. Cambridge, Mass.: MIT
Press.
Briscoe, E. (In press). Modelling Human Speech
Comprehension; A Computational Approach. Chichester,
UK: Ellis Horwood.
Briscoe, E. &amp; Boguraev, B. (1984). Control structures
and theories of interaction in speech understanding
systems. In Proc. of Coling84, Stanford, Ca, pp. 259-266.
Chomsky, N. (1981). Lectures on Government and
Binding. Dordrecht, Holland: Foris.
Church, K. (1980). On Memory Limitations in Natural
Language Processing. Bloomington, Ind.: Indiana
University Linguistics Club.
Cooper, W. &amp; Paccia-Cooper, J. (1980). Syntax &amp;
Speech. Cambridge, Mass.: Harvard University Press.
Fodor, J.D. (1979). Superstrategy. In Walker, E. &amp;
Cooper, W. (eds.) Sentence Processing., Hillsdale, NJ:
Lawrence Erlbaum.
Fodor, J.D. (1985). Deterministic parsing and subjacency.
Language and Cognitive Processes, 1.1, 3-42.
Frazier, L. (1979). On Comprehending Sentences:
Syntactic Parsing Strategies. Bloomington, Ind.: Indiana
University Linguistics Club.
Gazdar, G., Klein, E., Pullum, G., &amp; Sag, I. (1985).
Generalized Phrase Structure Grammar. Oxford, UK:
Blackwell.
Johnson-Laird, P. (1983). Mental Models. Cambridge,
UK: CUP.
Marcus, M. (1980). A Theory of Syntactic Recognition for
Natural Language. Cambridge, Mass.: MIT Press.
Nespor, M. &amp; Vogel, I. (1986). Prosodic Phonology.
Dordrecht, Holland: Foris.
Pereira, F. (1985). A new characterisation of attachment
preferences. In Dowty, D., Karttunen, L., &amp; Zwicky, A.
(eds.) Natural Language Parsing. Cambridge, UK: CUP.
Shieber, S. (1983). Sentence disambiguation by a shift-
reduce parsing technique. In Proc. of 21st ACL,
Cambridge, Mass., pp. 113-118.
Steedman, M. (1985). Dependency and coordination in
the grammar of Dutch and English. Language 55, 523-68.
</reference>
<page confidence="0.998343">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.834941">
<title confidence="0.995573">DETERMINISTIC PARSING AND UNBOUNDED DEPENDENCIES</title>
<author confidence="0.99997">Ted Briscoe</author>
<affiliation confidence="0.999999">Dept of Linguistics, Lancaster University</affiliation>
<address confidence="0.932355">Bailrigg, Lancashire LA1 4YT, UK</address>
<abstract confidence="0.998572833333333">This paper assesses two new approaches to deterministic parsing with respect to the analysis of unbounded dependencies (UDs). UDs in English are highly locally (and often globally) ambiguous. Several researchers have argued that the difficulty of UDs undermines the programme of deterministic parsing. However, their conclusion is based on critiques of various versions of the Marcus parser which represents only one of many possible approaches to deterministic parsing. We examine the predictions made by a LR(1) deterministic parser and the Lexicat deterministic parser concerning the analysis of UDs. The LR(1) technique is powerful enough to resolve the local ambiguities we examine. However, the Lexicat model provides a more psychologically plausible account of the parsing of UDs, which also offers a unified account of the resolution of local and global ambiguities in these constructions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Ades</author>
<author>M Steedman</author>
</authors>
<title>On the order of words.</title>
<date>1982</date>
<journal>Linguistics &amp; Philosophy,</journal>
<volume>4</volume>
<pages>517--558</pages>
<contexts>
<context position="15391" citStr="Ades &amp; Steedman (1982)" startWordPosition="2423" endWordPosition="2426"> of the initial doubtful attachment point. Moreover, the conflict cannot be resolved using the Shieber/Pereira resolution rules as they stand, since both possible reductions (VP —&gt; V; VP/NP —&gt; V) only involve one daughter. 213 The Lexicat Parser The LEXIcal-CATegorial parser is a deterministic, shift-reduce parser developed for extended categorial grammars which include a rule of syntactic composition, as well as the more usual rule of application. An earlier version of the parser is briefly described in Briscoe &amp; Boguraev (1984). Briscoe (in press) provides a complete description of Lexicat. Ades &amp; Steedman (1982), Steedman (1985) and Briscoe (in press) discuss composition in further detail from the perspectives of syntax, semantics and parsing. In a categorial grammar most syntactic information is located in the assignment of categories to lexical items. The rules of composition and application and a lexicon which suffices for the fragment under consideration are given in (10). (10) Function-Argument Application X YIX =&gt; Y Function-Function Composition X1Y YIZ =&gt; XIZ Bill : NP to : VPinf/VP you : NP do : S/VP/NP who : NP succeed : VPINP, VP want : VP/VPinfINP, VP/VPinf This grammar assigns the two ana</context>
</contexts>
<marker>Ades, Steedman, 1982</marker>
<rawString>Ades, A. &amp; Steedman, M. (1982). On the order of words. Linguistics &amp; Philosophy, 4, 517-558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aho</author>
<author>J Ullman</author>
</authors>
<title>The Theory of Parsing,</title>
<date>1972</date>
<journal>Translating and Compiling.</journal>
<volume>Vol. I,</volume>
<publisher>Prentice-Hall.</publisher>
<location>Englewood Cliffs, NJ:</location>
<contexts>
<context position="8869" citStr="Aho &amp; Ullman, 1972" startWordPosition="1351" endWordPosition="1354">al data. Finally, unbounded dependencies exhibit an identical kind of ambiguity which can be either local or global. Therefore, we would expect a unified account of their resolution, but the Determinism Hypothesis offers no account of the resolution of global ambiguities (see eg. Briscoe, in press). ALTERNATIVE DETERMINISTIC PARSERS There are a number of alternative deterministic parsing techniques many of which are in common use in compilers for high-level computer programming languages. Berwick (1985:3130 compares the Marcus parser to the most general of these techniques, LR(1) parsing (eg. Aho &amp; Ullman, 1972), and argues that the Marcus parser can be seen as both an extension and restriction of the LR(1) technique. In fact, he argues that it is equivalent to a bounded context parser (eg. Aho &amp; Ullman, 1972) which only allows literal access to grammatical symbols in the c-command domain in the left context and to two grammatical symbols in the right context. To date, little attention has been given to alternative deterministic techniques as models of natural language parsing in their own right, though. One exception is the work of Shieber (1983) and Pereira (1985), who have proposed that a simple e</context>
<context position="10667" citStr="Aho &amp; Ullman, 1972" startWordPosition="1633" endWordPosition="1636">ar will also model these strategies. Below these two alternative approaches are compared with respect to unbounded dependency constructions. 212 The Shieber/Pereira Parser The LR(1) technique involves extensive preprocessing of the grammar used by the parser to compute all the distinct analysis &apos;paths&apos; licensed by the grammar. This preprocessing results in a parse table which will deterministically specify the operations of a shift-reduce parser provided that the input grammar is an `LR(1) grammar&apos;; that is, provided that it is drawn from a subset of the unambiguous context-free grammars (see Aho &amp; Ullman, 1972; Briscoe, in press). The parse table encodes the set of possible left contexts for an LR(1) grammar as a deterministic finite-state machine. In intuitive terms, the LR(1) technique is able to resolve deterministically a subset of the possible local ambiguities which can be represented in a context-free grammar, and none of the global ambiguities. If an LR(1) parsing table is constructed from a grammar covering a realistic, ambiguous fragment of English, the resulting nondeterministic parsing table will contain &apos;clashes&apos; between shift and reduce operations and between different reduce operatio</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, A. &amp; Ullman, J. (1972). The Theory of Parsing, Translating and Compiling. Vol. I, Englewood Cliffs, NJ: Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
</authors>
<title>The Acquisition of Syntactic Knowledge.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="8757" citStr="Berwick (1985" startWordPosition="1335" endWordPosition="1336">ich fall within the buffer and the remainder. No such distinction seems to be supported by the psychological data. Finally, unbounded dependencies exhibit an identical kind of ambiguity which can be either local or global. Therefore, we would expect a unified account of their resolution, but the Determinism Hypothesis offers no account of the resolution of global ambiguities (see eg. Briscoe, in press). ALTERNATIVE DETERMINISTIC PARSERS There are a number of alternative deterministic parsing techniques many of which are in common use in compilers for high-level computer programming languages. Berwick (1985:3130 compares the Marcus parser to the most general of these techniques, LR(1) parsing (eg. Aho &amp; Ullman, 1972), and argues that the Marcus parser can be seen as both an extension and restriction of the LR(1) technique. In fact, he argues that it is equivalent to a bounded context parser (eg. Aho &amp; Ullman, 1972) which only allows literal access to grammatical symbols in the c-command domain in the left context and to two grammatical symbols in the right context. To date, little attention has been given to alternative deterministic techniques as models of natural language parsing in their own </context>
<context position="18235" citStr="Berwick, 1985" startWordPosition="2892" endWordPosition="2893">eft context for parsing decisions. Lexicat is a (1,1)-bounded context parser because it only has access to one set of grammatical symbols to the left and one set of grammatical symbols to the right of the current constituent (see Briscoe, in press). As such it is demonstrably less powerful than the LR(1) technique, which allows access to any aspect of the left context which can be represented as a regular expression, and the Marcus parser, which allows access to grammatical symbols in the c-command domain in the left context and two (not necessarily terminal) symbols in the right context (eg. Berwick, 1985:3130. However, it is unclear, at present, what precisely can be concluded on the basis of these differences in the parsing techniques because of the differing properties of the grammatical theories employed in each model. The Lexicat parser does not employ a parse table or use parsing states to maintain information about the nature of the left context. Rules of application and composition (with various restrictions on the directionality of reduction and the range of categories to which each rule applies) are used directly by the parser to perform reductions. There are two stages to each step </context>
</contexts>
<marker>Berwick, 1985</marker>
<rawString>Berwick, R. (1985). The Acquisition of Syntactic Knowledge. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
<author>A Weinberg</author>
</authors>
<title>The Grammatical Basis of Linguistic Performance.</title>
<date>1984</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="1346" citStr="Berwick &amp; Weinberg, 1984" startWordPosition="189" endWordPosition="192">erning the analysis of UDs. The LR(1) technique is powerful enough to resolve the local ambiguities we examine. However, the Lexicat model provides a more psychologically plausible account of the parsing of UDs, which also offers a unified account of the resolution of local and global ambiguities in these constructions. INTRODUCTION Church (1980:117) and Johnson-Laird (1983:313) have argued that the high degree of ambiguity in unbounded dependencies undermines the programme of deterministic parsing. Their conclusion is based on critiques of various versions of the Marcus parser (Marcus, 1980; Berwick &amp; Weinberg, 1984). This parser represents only one of many possible approaches to deterministic parsing. Therefore, the conclusion that deterministic parsing, in general, is impractical or psychologically implausible may be premature. In the next section, we outline the problems for the deterministic analysis of unbounded dependencies. In the succeeding sections, we present two alternative parsing techniques (and associated grammars) which make differing predictions concerning the onset and location of indeterminacy in the analysis of unbounded dependencies. We argue that the LR(1) parser is capable of determi</context>
<context position="6632" citStr="Berwick &amp; Weinberg (1984" startWordPosition="1011" endWordPosition="1014">cannot be made determinately when the potential attachment point occurs, the parser requires access to the right context of forthcoming material downstream from the current position. The examples in (6) illustrate that this would require unbounded lookahead. Who does Kim want e? to think that the boss will replace Sandy (with e) Who does Kim want e? to think that the boss expects the directors to replace Sandy (with e) In (6) the correct point of attachment cannot be determined until the end of the sentence which can be arbitrarily far away (in terms of lexical material) in the right context. Berwick &amp; Weinberg (1984:153f) argue that the Marcus parser can adequately represent an unbounded left context with finite resources if a successive cyclic and trace theoretic analysis (eg. Chomsky, 1981) of unbounded dependencies is adopted. However, both Church (1980) and Fodor (1985) demonstrate that the three cell lookahead buffer in the Marcus parser is not powerful enough to provide the required access to the right context in order to choose the correct point of attachment deterministically in many unbounded dependency constructions. Marcus&apos; (1980) Determinism Hypothesis claims that local ambiguities which are </context>
</contexts>
<marker>Berwick, Weinberg, 1984</marker>
<rawString>Berwick, R. &amp; Weinberg, A. (1984). The Grammatical Basis of Linguistic Performance. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Briscoe</author>
</authors>
<title>(In press). Modelling Human Speech Comprehension; A Computational Approach.</title>
<publisher>Ellis Horwood.</publisher>
<location>Chichester, UK:</location>
<marker>Briscoe, </marker>
<rawString>Briscoe, E. (In press). Modelling Human Speech Comprehension; A Computational Approach. Chichester, UK: Ellis Horwood.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>B Boguraev</author>
</authors>
<title>Control structures and theories of interaction in speech understanding systems.</title>
<date>1984</date>
<booktitle>In Proc. of Coling84,</booktitle>
<pages>259--266</pages>
<location>Stanford, Ca,</location>
<contexts>
<context position="2237" citStr="Briscoe &amp; Boguraev, 1984" startWordPosition="313" endWordPosition="316">the deterministic analysis of unbounded dependencies. In the succeeding sections, we present two alternative parsing techniques (and associated grammars) which make differing predictions concerning the onset and location of indeterminacy in the analysis of unbounded dependencies. We argue that the LR(1) parser is capable of deterministically resolving the local ambiguities which occur in these constructions, whilst the Lexicat parser is not. In the final section, we evaluate these predictions in the light of the Determinism Hypothesis (Marcus, 1980) and the Interactive Determinism Hypothesis (Briscoe &amp; Boguraev, 1984; Briscoe, in press) and argue that the Lexicat parser in conjunction with the Interactive Determinism Hypothesis provides the most psychologically plausible and unified account of the parsing of unbounded dependencies. UNBOUNDED DEPENDENCY AMBIGUITIES Unbounded dependencies are found in English constituent questions, relative clauses and topicalised constructions. The dependency is between the preposed constituent and its point of attachment. For example, in (1) Who is preposed and functioning as direct object of the transitive verb like. (1) Who do you like e Most current theories of grammar</context>
<context position="9924" citStr="Briscoe &amp; Boguraev (1984)" startWordPosition="1523" endWordPosition="1526">niques as models of natural language parsing in their own right, though. One exception is the work of Shieber (1983) and Pereira (1985), who have proposed that a simple extension of the LALR(1) technique can be used to model human natural language parsing strategies. The LALR(1) technique is a more efficient variant of the LR(1) technique. Since our implementation of the Shieber/Pereira model uses the latter technique, we will refer throughout to LR(1). With the grammar discussed below, the behaviour of a parser using either technique should be identical (see Aho &amp; Ullman, 1972). In addition, Briscoe &amp; Boguraev (1984) and Briscoe (in press) propose that a bounded context, deterministic parser in conjunction with an extended categorial grammar will also model these strategies. Below these two alternative approaches are compared with respect to unbounded dependency constructions. 212 The Shieber/Pereira Parser The LR(1) technique involves extensive preprocessing of the grammar used by the parser to compute all the distinct analysis &apos;paths&apos; licensed by the grammar. This preprocessing results in a parse table which will deterministically specify the operations of a shift-reduce parser provided that the input g</context>
<context position="15304" citStr="Briscoe &amp; Boguraev (1984)" startWordPosition="2410" endWordPosition="2413"> reduce/reduce conflict in the globally ambiguous case occurs much later than the position of the initial doubtful attachment point. Moreover, the conflict cannot be resolved using the Shieber/Pereira resolution rules as they stand, since both possible reductions (VP —&gt; V; VP/NP —&gt; V) only involve one daughter. 213 The Lexicat Parser The LEXIcal-CATegorial parser is a deterministic, shift-reduce parser developed for extended categorial grammars which include a rule of syntactic composition, as well as the more usual rule of application. An earlier version of the parser is briefly described in Briscoe &amp; Boguraev (1984). Briscoe (in press) provides a complete description of Lexicat. Ades &amp; Steedman (1982), Steedman (1985) and Briscoe (in press) discuss composition in further detail from the perspectives of syntax, semantics and parsing. In a categorial grammar most syntactic information is located in the assignment of categories to lexical items. The rules of composition and application and a lexicon which suffices for the fragment under consideration are given in (10). (10) Function-Argument Application X YIX =&gt; Y Function-Function Composition X1Y YIZ =&gt; XIZ Bill : NP to : VPinf/VP you : NP do : S/VP/NP who</context>
<context position="23872" citStr="Briscoe &amp; Boguraev (1984)" startWordPosition="3777" endWordPosition="3780">en paths with unbounded dependencies is hardly overwhelming. Moreover, the Determinism Hypothesis seems highly suspect in the light of the unbounded dependency examples because it predicts that local and global ambiguities are resolved using completely different mechanisms. At the very least, this approach is unparsimonious and leaves the resolution of global ambiguity largely unexplained. In addition, the extreme similarity between local and global ambiguities in unbounded dependency constructions suggests that one mechanism for the resolution of local and global ambiguity is quite feasible. Briscoe &amp; Boguraev (1984) and Briscoe (in press) propose a different account of the relationship between parsing and other components of comprehension than that entailed by the Determinism Hypothesis, dubbed the Interactive Determinism Hypothesis (IDH). Under this account of deterministic parsing, the parser proceeds autonomously until it is faced with an indeterminacy and then requests help from other components of the comprehension system. By default, the parser will apply a resolution rule or parsing strategy at such a point, but this can be overruled by specific non-syntactic information at the onset of the indete</context>
</contexts>
<marker>Briscoe, Boguraev, 1984</marker>
<rawString>Briscoe, E. &amp; Boguraev, B. (1984). Control structures and theories of interaction in speech understanding systems. In Proc. of Coling84, Stanford, Ca, pp. 259-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding.</booktitle>
<location>Dordrecht, Holland: Foris.</location>
<contexts>
<context position="6812" citStr="Chomsky, 1981" startWordPosition="1039" endWordPosition="1040">mples in (6) illustrate that this would require unbounded lookahead. Who does Kim want e? to think that the boss will replace Sandy (with e) Who does Kim want e? to think that the boss expects the directors to replace Sandy (with e) In (6) the correct point of attachment cannot be determined until the end of the sentence which can be arbitrarily far away (in terms of lexical material) in the right context. Berwick &amp; Weinberg (1984:153f) argue that the Marcus parser can adequately represent an unbounded left context with finite resources if a successive cyclic and trace theoretic analysis (eg. Chomsky, 1981) of unbounded dependencies is adopted. However, both Church (1980) and Fodor (1985) demonstrate that the three cell lookahead buffer in the Marcus parser is not powerful enough to provide the required access to the right context in order to choose the correct point of attachment deterministically in many unbounded dependency constructions. Marcus&apos; (1980) Determinism Hypothesis claims that local ambiguities which are not resolvable in terms of the lookahead buffer are resolved by parsing strategy and that therefore, many unbounded dependency constructions should be psychologically complex, &apos;gar</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. (1981). Lectures on Government and Binding. Dordrecht, Holland: Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>On Memory Limitations in Natural Language Processing.</title>
<date>1980</date>
<institution>Indiana University Linguistics Club.</institution>
<location>Bloomington, Ind.:</location>
<contexts>
<context position="1068" citStr="Church (1980" startWordPosition="152" endWordPosition="153">their conclusion is based on critiques of various versions of the Marcus parser which represents only one of many possible approaches to deterministic parsing. We examine the predictions made by a LR(1) deterministic parser and the Lexicat deterministic parser concerning the analysis of UDs. The LR(1) technique is powerful enough to resolve the local ambiguities we examine. However, the Lexicat model provides a more psychologically plausible account of the parsing of UDs, which also offers a unified account of the resolution of local and global ambiguities in these constructions. INTRODUCTION Church (1980:117) and Johnson-Laird (1983:313) have argued that the high degree of ambiguity in unbounded dependencies undermines the programme of deterministic parsing. Their conclusion is based on critiques of various versions of the Marcus parser (Marcus, 1980; Berwick &amp; Weinberg, 1984). This parser represents only one of many possible approaches to deterministic parsing. Therefore, the conclusion that deterministic parsing, in general, is impractical or psychologically implausible may be premature. In the next section, we outline the problems for the deterministic analysis of unbounded dependencies. I</context>
<context position="6878" citStr="Church (1980)" startWordPosition="1048" endWordPosition="1049">. Who does Kim want e? to think that the boss will replace Sandy (with e) Who does Kim want e? to think that the boss expects the directors to replace Sandy (with e) In (6) the correct point of attachment cannot be determined until the end of the sentence which can be arbitrarily far away (in terms of lexical material) in the right context. Berwick &amp; Weinberg (1984:153f) argue that the Marcus parser can adequately represent an unbounded left context with finite resources if a successive cyclic and trace theoretic analysis (eg. Chomsky, 1981) of unbounded dependencies is adopted. However, both Church (1980) and Fodor (1985) demonstrate that the three cell lookahead buffer in the Marcus parser is not powerful enough to provide the required access to the right context in order to choose the correct point of attachment deterministically in many unbounded dependency constructions. Marcus&apos; (1980) Determinism Hypothesis claims that local ambiguities which are not resolvable in terms of the lookahead buffer are resolved by parsing strategy and that therefore, many unbounded dependency constructions should be psychologically complex, &apos;garden paths&apos; requiring extensive reanalysis. There is some evidence </context>
</contexts>
<marker>Church, 1980</marker>
<rawString>Church, K. (1980). On Memory Limitations in Natural Language Processing. Bloomington, Ind.: Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cooper</author>
<author>J Paccia-Cooper</author>
</authors>
<title>Syntax &amp; Speech.</title>
<date>1980</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="26830" citStr="Cooper &amp; Paccia-Cooper, 1980" startWordPosition="4232" endWordPosition="4235">vidence comes from the the distribution of prosodic boundaries in relation to the onset of strategy-violating syntactic ambiguities. For example in (11) (11) Without her, contributions to the fund would be inadequate. the comma (an orthographic counterpart to certain types of prosodic boundary) marks the location of an intonational or major tone group boundary which would normally occur in the spoken version of this sentence. The prosodic boundary prevents the potential misinterpretation in which her contributions is reduced as one NP. In unbounded dependency constructions, Danly (reported in Cooper &amp; Paccia-Cooper, 1980:1590 has demonstrated that the final syllable of the verb preceeding a correct attachment point is lengthened relative to an environment without a potential attachment point, or with a potential but incorrect one. Syllable lengthening is indicative of a phrasal or minor tone group boundary. Paul Warren and the author have since tested Danly&apos;s result by acoustically analysing ten readers&apos; productions of four examples containing doubtful but correct early points of attachment and four similar examples with doubtful and incorrect early attachment points. The results tend to confirm the original </context>
<context position="28520" citStr="Cooper &amp; Paccia-Cooper, 1980" startWordPosition="4485" endWordPosition="4488">nterpretation involves attachment `into&apos; the contracted form. Fodor (1979:277n17) notes that it is only the occurrence of contraction which appears to provide determinate information about the correct analysis and that, since contraction is optional, this information cannot be relied on. However, metrical phonologists (eg. Nespor &amp; Vogel, 1986) argue that such rules are not blocked syntactically by the presence of the trace/gap, but by an intervening prosodic boundary and that this explains the coincidence of other phonetic effects, such as lengthening, at points where contraction is blocked (Cooper &amp; Paccia-Cooper, 1980:Ch10). In other words, contraction is the tip of a far more systematic prosodic iceberg which does reliably cue the presence of a correct attachment point. When Lexicat reaches a potential point of attachment, it is faced with a shift/reduce ambiguity. By default, Lexicat prefers to shift, but this strategy can be blocked by a prosodic boundary intervening between the verb and item about to be shifted into the parse stack. Therefore, the parser opts for early attachment of the preposed constituent. In terms of Lexicat&apos;s operation, the prosodic boundary in the unbounded dependency construction</context>
</contexts>
<marker>Cooper, Paccia-Cooper, 1980</marker>
<rawString>Cooper, W. &amp; Paccia-Cooper, J. (1980). Syntax &amp; Speech. Cambridge, Mass.: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<date>1979</date>
<booktitle>Sentence Processing.,</booktitle>
<editor>Superstrategy. In Walker, E. &amp; Cooper, W. (eds.)</editor>
<location>Hillsdale, NJ: Lawrence Erlbaum.</location>
<contexts>
<context position="4395" citStr="Fodor, 1979" startWordPosition="640" endWordPosition="641">sible to construct more and more embedded examples like those in (2) which exhibit the same dependency as (1). Who do you think Kim likes e Who do you expect that Kim hopes Sandy likes e A parser for English capable of producing a syntactic representation adequate to guide semantic interpretation must recover the grammatical role of the preposed constituent. However, whenever these constructions contain verbs of ambiguous valency the correct point of attachment for the preposed constituent also becomes ambiguous. For example, in (3) there are two potential attachment points, or doubtful gaps (Fodor, 1979), written e?. (3) Who do you want e? to succeed e? The correct attachment of Who is ambiguous because both want and succeed can take, but do not require, NP objects. 211 The ambiguity in (3) is global with respect to the sentence; however, identical local ambiguities exist for a parser operating incrementally from left to right. For example, in (4) the attachment of Who as object of want, although correct, remains doubtful until the end of the sentence. (4) Who do you want e? to succeed Bill Thus, at the point when the parser reaches a potential but ambiguous attachment point in the left-to-ri</context>
<context position="27965" citStr="Fodor (1979" startWordPosition="4405" endWordPosition="4406">incorrect early attachment points. The results tend to confirm the original finding since lengthening was found consistently (although the measurements did not achieve statistical significance; see Briscoe, in press). A final piece of evidence that lengthening occurs before a correct point of attachment comes from the acceptability of contraction in (12a), but not in (12b). a) Who do you wanna succeed b) *Who do you wanna succeed Bill Contraction forces late attachment of Who in a), but b) is unacceptable because the only possible interpretation involves attachment `into&apos; the contracted form. Fodor (1979:277n17) notes that it is only the occurrence of contraction which appears to provide determinate information about the correct analysis and that, since contraction is optional, this information cannot be relied on. However, metrical phonologists (eg. Nespor &amp; Vogel, 1986) argue that such rules are not blocked syntactically by the presence of the trace/gap, but by an intervening prosodic boundary and that this explains the coincidence of other phonetic effects, such as lengthening, at points where contraction is blocked (Cooper &amp; Paccia-Cooper, 1980:Ch10). In other words, contraction is the ti</context>
</contexts>
<marker>Fodor, 1979</marker>
<rawString>Fodor, J.D. (1979). Superstrategy. In Walker, E. &amp; Cooper, W. (eds.) Sentence Processing., Hillsdale, NJ: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<title>Deterministic parsing and subjacency.</title>
<date>1985</date>
<journal>Language and Cognitive Processes,</journal>
<volume>1</volume>
<pages>3--42</pages>
<contexts>
<context position="6895" citStr="Fodor (1985)" startWordPosition="1051" endWordPosition="1052">t e? to think that the boss will replace Sandy (with e) Who does Kim want e? to think that the boss expects the directors to replace Sandy (with e) In (6) the correct point of attachment cannot be determined until the end of the sentence which can be arbitrarily far away (in terms of lexical material) in the right context. Berwick &amp; Weinberg (1984:153f) argue that the Marcus parser can adequately represent an unbounded left context with finite resources if a successive cyclic and trace theoretic analysis (eg. Chomsky, 1981) of unbounded dependencies is adopted. However, both Church (1980) and Fodor (1985) demonstrate that the three cell lookahead buffer in the Marcus parser is not powerful enough to provide the required access to the right context in order to choose the correct point of attachment deterministically in many unbounded dependency constructions. Marcus&apos; (1980) Determinism Hypothesis claims that local ambiguities which are not resolvable in terms of the lookahead buffer are resolved by parsing strategy and that therefore, many unbounded dependency constructions should be psychologically complex, &apos;garden paths&apos; requiring extensive reanalysis. There is some evidence for syntactic pre</context>
</contexts>
<marker>Fodor, 1985</marker>
<rawString>Fodor, J.D. (1985). Deterministic parsing and subjacency. Language and Cognitive Processes, 1.1, 3-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<title>On Comprehending Sentences: Syntactic Parsing Strategies.</title>
<date>1979</date>
<institution>Indiana University Linguistics Club.</institution>
<location>Bloomington, Ind.:</location>
<contexts>
<context position="11586" citStr="Frazier, 1979" startWordPosition="1770" endWordPosition="1771">grammar, and none of the global ambiguities. If an LR(1) parsing table is constructed from a grammar covering a realistic, ambiguous fragment of English, the resulting nondeterministic parsing table will contain &apos;clashes&apos; between shift and reduce operations and between different reduce operations. Shieber and Pereira demonstrate that if shift/reduce clashes are resolved in favour of shifting and reduce/reduce clashes in favour of reducing with the rule containing the most daughters, then the parser will model several psychologically plausible parsing strategies, such as right association (eg. Frazier, 1979). Shieber (1983) and Pereira (1985) both provide grammars with a GPSG-style (Gazdar et al., 1985) SLASH feature analysis of unbounded dependencies. (8) presents a- grammar fragment written in the same style to mimic the GPSG account of unbounded dependencies in a context-free notation. (8) Terminals Det N Vtr V Aux want to wh $ Non-terminals SENT S VP VPinf VP/NP VPinf/NP NPwh NP 0) SENT —&gt; S $ 1) S —&gt; NP VP 2) NP —&gt; Det N 3) VP —&gt; Vtr NP 4) VP/NP —&gt; Vtr 5) NP —&gt; N 6) NPwh —&gt; wh 7) VP/NP —&gt; want VPinf/NP 8) VP/NP —&gt; want VPinf 9) VPinf —&gt; to VP 10) VP —&gt; V NP 11) VP —&gt; V 12) VPinf/NP —&gt; to VP/</context>
<context position="19923" citStr="Frazier, 1979" startWordPosition="3165" endWordPosition="3166">ation and a more constrained rule of forward composition. If this fails, then Lexicat shifts. This completes one step of the parsing algorithm, so Lexicat returns to the checking phase. This process continues until the parse is complete or the input is —Comp 214 exhausted, at which point the parser halts under the usual conditions. Quite often, a shift/reduce conflict arises during the checking phase in which case Lexicat opts, by default, to shift. In most constructions this resolution rule results in analyses which conform to the parsing strategies of late closure and right association (eg. Frazier, 1979). However, in unbounded dependencies it results in late attachment of the preposed constituent. For example, if Lexicat is in the configuration shown in Figure 3, then a shift/reduce conflict will occur in the checking phase. Stack Input Buffer 3 2 1 NP S/VPinflNP VPinfNP succeed S/VPinf Who (do you want) to Fig= 3 - Configuration of Lexicat Parser The ambiguity of the partial constituent do you want results from the ambiguous valency of want. Depending on which category in Ce112 is chosen, reduction by forward composition will or will not be possible. By default, Lexicat will shift in the fac</context>
</contexts>
<marker>Frazier, 1979</marker>
<rawString>Frazier, L. (1979). On Comprehending Sentences: Syntactic Parsing Strategies. Bloomington, Ind.: Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<date>1985</date>
<booktitle>Generalized Phrase Structure Grammar.</booktitle>
<publisher>Blackwell.</publisher>
<location>Oxford, UK:</location>
<contexts>
<context position="11683" citStr="Gazdar et al., 1985" startWordPosition="1783" endWordPosition="1786">a grammar covering a realistic, ambiguous fragment of English, the resulting nondeterministic parsing table will contain &apos;clashes&apos; between shift and reduce operations and between different reduce operations. Shieber and Pereira demonstrate that if shift/reduce clashes are resolved in favour of shifting and reduce/reduce clashes in favour of reducing with the rule containing the most daughters, then the parser will model several psychologically plausible parsing strategies, such as right association (eg. Frazier, 1979). Shieber (1983) and Pereira (1985) both provide grammars with a GPSG-style (Gazdar et al., 1985) SLASH feature analysis of unbounded dependencies. (8) presents a- grammar fragment written in the same style to mimic the GPSG account of unbounded dependencies in a context-free notation. (8) Terminals Det N Vtr V Aux want to wh $ Non-terminals SENT S VP VPinf VP/NP VPinf/NP NPwh NP 0) SENT —&gt; S $ 1) S —&gt; NP VP 2) NP —&gt; Det N 3) VP —&gt; Vtr NP 4) VP/NP —&gt; Vtr 5) NP —&gt; N 6) NPwh —&gt; wh 7) VP/NP —&gt; want VPinf/NP 8) VP/NP —&gt; want VPinf 9) VPinf —&gt; to VP 10) VP —&gt; V NP 11) VP —&gt; V 12) VPinf/NP —&gt; to VP/NP 13) VP/NP —&gt; V 14) S —&gt; NPwh Aux VP 15) S —&gt; NPwh Aux NP VP/NP 16) VP —&gt; want VPinf 17) VP —&gt; </context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., Klein, E., Pullum, G., &amp; Sag, I. (1985). Generalized Phrase Structure Grammar. Oxford, UK: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Johnson-Laird</author>
</authors>
<title>Mental Models.</title>
<date>1983</date>
<publisher>CUP.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="1097" citStr="Johnson-Laird (1983" startWordPosition="155" endWordPosition="156">sed on critiques of various versions of the Marcus parser which represents only one of many possible approaches to deterministic parsing. We examine the predictions made by a LR(1) deterministic parser and the Lexicat deterministic parser concerning the analysis of UDs. The LR(1) technique is powerful enough to resolve the local ambiguities we examine. However, the Lexicat model provides a more psychologically plausible account of the parsing of UDs, which also offers a unified account of the resolution of local and global ambiguities in these constructions. INTRODUCTION Church (1980:117) and Johnson-Laird (1983:313) have argued that the high degree of ambiguity in unbounded dependencies undermines the programme of deterministic parsing. Their conclusion is based on critiques of various versions of the Marcus parser (Marcus, 1980; Berwick &amp; Weinberg, 1984). This parser represents only one of many possible approaches to deterministic parsing. Therefore, the conclusion that deterministic parsing, in general, is impractical or psychologically implausible may be premature. In the next section, we outline the problems for the deterministic analysis of unbounded dependencies. In the succeeding sections, we</context>
</contexts>
<marker>Johnson-Laird, 1983</marker>
<rawString>Johnson-Laird, P. (1983). Mental Models. Cambridge, UK: CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="1319" citStr="Marcus, 1980" startWordPosition="187" endWordPosition="188">ic parser concerning the analysis of UDs. The LR(1) technique is powerful enough to resolve the local ambiguities we examine. However, the Lexicat model provides a more psychologically plausible account of the parsing of UDs, which also offers a unified account of the resolution of local and global ambiguities in these constructions. INTRODUCTION Church (1980:117) and Johnson-Laird (1983:313) have argued that the high degree of ambiguity in unbounded dependencies undermines the programme of deterministic parsing. Their conclusion is based on critiques of various versions of the Marcus parser (Marcus, 1980; Berwick &amp; Weinberg, 1984). This parser represents only one of many possible approaches to deterministic parsing. Therefore, the conclusion that deterministic parsing, in general, is impractical or psychologically implausible may be premature. In the next section, we outline the problems for the deterministic analysis of unbounded dependencies. In the succeeding sections, we present two alternative parsing techniques (and associated grammars) which make differing predictions concerning the onset and location of indeterminacy in the analysis of unbounded dependencies. We argue that the LR(1) p</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. (1980). A Theory of Syntactic Recognition for Natural Language. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nespor</author>
<author>I Vogel</author>
</authors>
<title>Prosodic Phonology.</title>
<date>1986</date>
<location>Dordrecht, Holland: Foris.</location>
<contexts>
<context position="28238" citStr="Nespor &amp; Vogel, 1986" startWordPosition="4442" endWordPosition="4445">curs before a correct point of attachment comes from the acceptability of contraction in (12a), but not in (12b). a) Who do you wanna succeed b) *Who do you wanna succeed Bill Contraction forces late attachment of Who in a), but b) is unacceptable because the only possible interpretation involves attachment `into&apos; the contracted form. Fodor (1979:277n17) notes that it is only the occurrence of contraction which appears to provide determinate information about the correct analysis and that, since contraction is optional, this information cannot be relied on. However, metrical phonologists (eg. Nespor &amp; Vogel, 1986) argue that such rules are not blocked syntactically by the presence of the trace/gap, but by an intervening prosodic boundary and that this explains the coincidence of other phonetic effects, such as lengthening, at points where contraction is blocked (Cooper &amp; Paccia-Cooper, 1980:Ch10). In other words, contraction is the tip of a far more systematic prosodic iceberg which does reliably cue the presence of a correct attachment point. When Lexicat reaches a potential point of attachment, it is faced with a shift/reduce ambiguity. By default, Lexicat prefers to shift, but this strategy can be b</context>
</contexts>
<marker>Nespor, Vogel, 1986</marker>
<rawString>Nespor, M. &amp; Vogel, I. (1986). Prosodic Phonology. Dordrecht, Holland: Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>A new characterisation of attachment preferences.</title>
<date>1985</date>
<editor>In Dowty, D., Karttunen, L., &amp; Zwicky, A. (eds.)</editor>
<publisher>CUP.</publisher>
<location>Cambridge, UK:</location>
<contexts>
<context position="9434" citStr="Pereira (1985)" startWordPosition="1448" endWordPosition="1449">echniques, LR(1) parsing (eg. Aho &amp; Ullman, 1972), and argues that the Marcus parser can be seen as both an extension and restriction of the LR(1) technique. In fact, he argues that it is equivalent to a bounded context parser (eg. Aho &amp; Ullman, 1972) which only allows literal access to grammatical symbols in the c-command domain in the left context and to two grammatical symbols in the right context. To date, little attention has been given to alternative deterministic techniques as models of natural language parsing in their own right, though. One exception is the work of Shieber (1983) and Pereira (1985), who have proposed that a simple extension of the LALR(1) technique can be used to model human natural language parsing strategies. The LALR(1) technique is a more efficient variant of the LR(1) technique. Since our implementation of the Shieber/Pereira model uses the latter technique, we will refer throughout to LR(1). With the grammar discussed below, the behaviour of a parser using either technique should be identical (see Aho &amp; Ullman, 1972). In addition, Briscoe &amp; Boguraev (1984) and Briscoe (in press) propose that a bounded context, deterministic parser in conjunction with an extended c</context>
<context position="11621" citStr="Pereira (1985)" startWordPosition="1775" endWordPosition="1776">iguities. If an LR(1) parsing table is constructed from a grammar covering a realistic, ambiguous fragment of English, the resulting nondeterministic parsing table will contain &apos;clashes&apos; between shift and reduce operations and between different reduce operations. Shieber and Pereira demonstrate that if shift/reduce clashes are resolved in favour of shifting and reduce/reduce clashes in favour of reducing with the rule containing the most daughters, then the parser will model several psychologically plausible parsing strategies, such as right association (eg. Frazier, 1979). Shieber (1983) and Pereira (1985) both provide grammars with a GPSG-style (Gazdar et al., 1985) SLASH feature analysis of unbounded dependencies. (8) presents a- grammar fragment written in the same style to mimic the GPSG account of unbounded dependencies in a context-free notation. (8) Terminals Det N Vtr V Aux want to wh $ Non-terminals SENT S VP VPinf VP/NP VPinf/NP NPwh NP 0) SENT —&gt; S $ 1) S —&gt; NP VP 2) NP —&gt; Det N 3) VP —&gt; Vtr NP 4) VP/NP —&gt; Vtr 5) NP —&gt; N 6) NPwh —&gt; wh 7) VP/NP —&gt; want VPinf/NP 8) VP/NP —&gt; want VPinf 9) VPinf —&gt; to VP 10) VP —&gt; V NP 11) VP —&gt; V 12) VPinf/NP —&gt; to VP/NP 13) VP/NP —&gt; V 14) S —&gt; NPwh Aux</context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>Pereira, F. (1985). A new characterisation of attachment preferences. In Dowty, D., Karttunen, L., &amp; Zwicky, A. (eds.) Natural Language Parsing. Cambridge, UK: CUP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Sentence disambiguation by a shiftreduce parsing technique.</title>
<date>1983</date>
<booktitle>In Proc. of 21st ACL,</booktitle>
<pages>113--118</pages>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="9415" citStr="Shieber (1983)" startWordPosition="1445" endWordPosition="1446"> general of these techniques, LR(1) parsing (eg. Aho &amp; Ullman, 1972), and argues that the Marcus parser can be seen as both an extension and restriction of the LR(1) technique. In fact, he argues that it is equivalent to a bounded context parser (eg. Aho &amp; Ullman, 1972) which only allows literal access to grammatical symbols in the c-command domain in the left context and to two grammatical symbols in the right context. To date, little attention has been given to alternative deterministic techniques as models of natural language parsing in their own right, though. One exception is the work of Shieber (1983) and Pereira (1985), who have proposed that a simple extension of the LALR(1) technique can be used to model human natural language parsing strategies. The LALR(1) technique is a more efficient variant of the LR(1) technique. Since our implementation of the Shieber/Pereira model uses the latter technique, we will refer throughout to LR(1). With the grammar discussed below, the behaviour of a parser using either technique should be identical (see Aho &amp; Ullman, 1972). In addition, Briscoe &amp; Boguraev (1984) and Briscoe (in press) propose that a bounded context, deterministic parser in conjunction</context>
<context position="11602" citStr="Shieber (1983)" startWordPosition="1772" endWordPosition="1773">e of the global ambiguities. If an LR(1) parsing table is constructed from a grammar covering a realistic, ambiguous fragment of English, the resulting nondeterministic parsing table will contain &apos;clashes&apos; between shift and reduce operations and between different reduce operations. Shieber and Pereira demonstrate that if shift/reduce clashes are resolved in favour of shifting and reduce/reduce clashes in favour of reducing with the rule containing the most daughters, then the parser will model several psychologically plausible parsing strategies, such as right association (eg. Frazier, 1979). Shieber (1983) and Pereira (1985) both provide grammars with a GPSG-style (Gazdar et al., 1985) SLASH feature analysis of unbounded dependencies. (8) presents a- grammar fragment written in the same style to mimic the GPSG account of unbounded dependencies in a context-free notation. (8) Terminals Det N Vtr V Aux want to wh $ Non-terminals SENT S VP VPinf VP/NP VPinf/NP NPwh NP 0) SENT —&gt; S $ 1) S —&gt; NP VP 2) NP —&gt; Det N 3) VP —&gt; Vtr NP 4) VP/NP —&gt; Vtr 5) NP —&gt; N 6) NPwh —&gt; wh 7) VP/NP —&gt; want VPinf/NP 8) VP/NP —&gt; want VPinf 9) VPinf —&gt; to VP 10) VP —&gt; V NP 11) VP —&gt; V 12) VPinf/NP —&gt; to VP/NP 13) VP/NP —&gt; </context>
<context position="14106" citStr="Shieber, 1983" startWordPosition="2218" endWordPosition="2219">ocessing the grammar and despite the unbounded nature of the ambiguity, the decision point will always be at the end of the sentence. Therefore, local ambiguities involving the point of attachment of preposed constituents will not involve parsing indeterminacy using this technique. In this instance, the suspicion arises that the power of technique may be too great for a model of human parsing because examples such as those in (7) above do appear to be garden paths. However, normally such effects are only predicted when a parsing conflict is resolved incorrectly by the rules of resolution (eg. Shieber, 1983) and no conflict will arise parsing these examples with a grammar like that in (8). At first sight it is surprising that these local ambiguities cause no problems since an LR(1) parser appears to have less access to the right context than the Marcus parser. However, the LR(1) parser makes greater use of the left context and also delays many syntactic decisions until most of the input is in the parse stack; in the configuration in Figure 1 no clause level attachments have been made, despite the fact that the complete sentence has been shifted into the parse stack. The reduce/reduce conflict in </context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, S. (1983). Sentence disambiguation by a shiftreduce parsing technique. In Proc. of 21st ACL, Cambridge, Mass., pp. 113-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Dependency and coordination in the grammar of Dutch and English.</title>
<date>1985</date>
<journal>Language</journal>
<volume>55</volume>
<pages>523--68</pages>
<contexts>
<context position="15408" citStr="Steedman (1985)" startWordPosition="2427" endWordPosition="2428"> attachment point. Moreover, the conflict cannot be resolved using the Shieber/Pereira resolution rules as they stand, since both possible reductions (VP —&gt; V; VP/NP —&gt; V) only involve one daughter. 213 The Lexicat Parser The LEXIcal-CATegorial parser is a deterministic, shift-reduce parser developed for extended categorial grammars which include a rule of syntactic composition, as well as the more usual rule of application. An earlier version of the parser is briefly described in Briscoe &amp; Boguraev (1984). Briscoe (in press) provides a complete description of Lexicat. Ades &amp; Steedman (1982), Steedman (1985) and Briscoe (in press) discuss composition in further detail from the perspectives of syntax, semantics and parsing. In a categorial grammar most syntactic information is located in the assignment of categories to lexical items. The rules of composition and application and a lexicon which suffices for the fragment under consideration are given in (10). (10) Function-Argument Application X YIX =&gt; Y Function-Function Composition X1Y YIZ =&gt; XIZ Bill : NP to : VPinf/VP you : NP do : S/VP/NP who : NP succeed : VPINP, VP want : VP/VPinfINP, VP/VPinf This grammar assigns the two analyses shown in Fi</context>
</contexts>
<marker>Steedman, 1985</marker>
<rawString>Steedman, M. (1985). Dependency and coordination in the grammar of Dutch and English. Language 55, 523-68.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>