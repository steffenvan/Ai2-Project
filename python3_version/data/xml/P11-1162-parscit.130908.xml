<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001883">
<title confidence="0.975257">
Insights from Network Structure for Text Mining
</title>
<author confidence="0.854753">
Zornitsa Kozareva and Eduard Hovy
</author>
<affiliation confidence="0.818968">
USC Information Sciences Institute
</affiliation>
<address confidence="0.8475865">
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</address>
<email confidence="0.999212">
{kozareva,hovy}@isi.edu
</email>
<sectionHeader confidence="0.995666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99925915">
Text mining and data harvesting algorithms
have become popular in the computational lin-
guistics community. They employ patterns
that specify the kind of information to be har-
vested, and usually bootstrap either the pat-
tern learning or the term harvesting process (or
both) in a recursive cycle, using data learned
in one step to generate more seeds for the next.
They therefore treat the source text corpus as
a network, in which words are the nodes and
relations linking them are the edges. The re-
sults of computational network analysis, espe-
cially from the world wide web, are thus ap-
plicable. Surprisingly, these results have not
yet been broadly introduced into the computa-
tional linguistics community. In this paper we
show how various results apply to text mining,
how they explain some previously observed
phenomena, and how they can be helpful for
computational linguistics applications.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953255813954">
Text mining / harvesting algorithms have been ap-
plied in recent years for various uses, including
learning of semantic constraints for verb participants
(Lin and Pantel, 2002) related pairs in various rela-
tions, such as part-whole (Girju et al., 2003), cause
(Pantel and Pennacchiotti, 2006), and other typical
information extraction relations, large collections
of entities (Soderland et al., 1999; Etzioni et al.,
2005), features of objects (Pasca, 2004) and ontolo-
gies (Carlson et al., 2010). They generally start with
one or more seed terms and employ patterns that
specify the desired information as it relates to the
seed(s). Several approaches have been developed
specifically for learning patterns, including guided
pattern collection with manual filtering (Riloff and
Shepherd, 1997) automated surface-level pattern in-
duction (Agichtein and Gravano, 2000; Ravichan-
dran and Hovy, 2002) probabilistic methods for tax-
onomy relation learning (Snow et al., 2005) and ker-
nel methods for relation learning (Zelenko et al.,
2003). Generally, the harvesting procedure is recur-
sive, in which data (terms or patterns) gathered in
one step of a cycle are used as seeds in the following
step, to gather more terms or patterns.
This method treats the source text as a graph or
network, consisting of terms (words) as nodes and
inter-term relations as edges. Each relation type in-
duces a different network1. Text mining is a process
of network traversal, and faces the standard prob-
lems of handling cycles, ranking search alternatives,
estimating yield maxima, etc.
The computational properties of large networks
and large network traversal have been studied inten-
sively (Sabidussi, 1966; Freeman, 1979; Watts and
Strogatz, 1998) and especially, over the past years,
in the context of the world wide web (Page et al.,
1999; Broder et al., 2000; Kleinberg and Lawrence,
2001; Li et al., 2005; Clauset et al., 2009). Surpris-
ingly, except in (Talukdar and Pereira, 2010), this
work has not yet been related to text mining research
in the computational linguistics community.
The work is, however, relevant in at least two
ways. It sometimes explains why text mining algo-
</bodyText>
<footnote confidence="0.989433333333333">
1These networks are generally far larger and more densely
interconnected than the world wide web’s network of pages and
hyperlinks.
</footnote>
<page confidence="0.826407">
1616
</page>
<note confidence="0.980851">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616–1625,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999614583333333">
rithms have the limitations and thresholds that are
empirically found (or suspected), and it may suggest
ways to improve text mining algorithms for some
applications.
In Section 2, we review some related work. In
Section 3 we describe the general harvesting proce-
dure, and follow with an examination of the various
statistical properties of implicit semantic networks
in Section 4, using our implemented harvester to
provide illustrative statistics. In Section 5 we dis-
cuss implications for computational linguistics re-
search.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999944975308642">
The Natural Language Processing knowledge har-
vesting community has developed a good under-
standing of how to harvests various kinds of se-
mantic information and use this information to im-
prove the performance of tasks such as information
extraction (Riloff, 1993), textual entailment (Zan-
zotto et al., 2006), question answering (Katz et
al., 2003), and ontology creation (Suchanek et al.,
2007), among others. Researchers have focused
on the automated extraction of semantic lexicons
(Hearst, 1992; Riloff and Shepherd, 1997; Girju et
al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva
et al., 2008). While clustering approaches tend to
extract general facts, pattern based approaches have
shown to produce more constrained but accurate lists
of semantic terms. To extract this information, (Lin
and Pantel, 2002) showed the effect of using differ-
ent sizes and genres of corpora such as news and
Web documents. The latter has been shown to pro-
vide broader and more complete information.
Researchers outside computational linguistics
have studied complex networks such as the World
Wide Web, the Social Web, the network of scien-
tific papers, among others. They have investigated
the properties of these text-based networks with the
objective of understanding their structure and ap-
plying this knowledge to determine node impor-
tance/centrality, connectivity, growth and decay of
interest, etc. In particular, the ability to analyze net-
works, identify influential nodes, and discover hid-
den structures has led to important scientific and
technological breakthroughs such as the discovery
of communities of like-minded individuals (New-
man and Girvan, 2004), the identification of influ-
ential people (Kempe et al., 2003), the ranking of
scientists by their citation indexes (Radicchi et al.,
2009), and the discovery of important scientific pa-
pers (Walker et al., 2006; Chen et al., 2007; Sayyadi
and Getoor, 2009). Broder et al. (2000) demon-
strated that the Web link structure has a “bow-tie”
shape, while (2001) classified Web pages into au-
thorities (pages with relevant information) and hubs
(pages with useful references). These findings re-
sulted in the development of the PageRank (Page et
al., 1999) algorithm which analyzes the structure of
the hyperlinks of Web documents to find pages with
authoritative information. PageRank has revolution-
ized the whole Internet search society.
However, no-one has studied the properties of the
text-based semantic networks induced by semantic
relations between terms with the objective of un-
derstanding their structure and applying this knowl-
edge to improve concept discovery. Most relevant
to this theme is the work of Steyvers and Tenen-
baum (Steyvers and Tenenbaum, 2004), who stud-
ied three manually built lexical networks (associa-
tion norms, WordNet, and Roget’s Thesaurus (Ro-
get, 1911)) and proposed a model of the growth of
the semantic structure over time. These networks are
limited to the semantic relations among nouns.
In this paper we take a step further to explore the
statistical properties of semantic networks relating
proper names, nouns, verbs, and adjectives. Under-
standing the semantics of nouns, verbs, and adjec-
tives has been of great interest to linguists and cog-
nitive scientists such as (Gentner, 1981; Levin and
Somers, 1993; Gasser and Smith, 1998). We imple-
ment a general harvesting procedure and show its re-
sults for these word types. A fundamental difference
with the work of (Steyvers and Tenenbaum, 2004)
is that we study very large semantic networks built
‘naturally’ by (millions of) users rather than ‘artifi-
cially’ by a small set of experts. The large networks
capture the semantic intuitions and knowledge of the
collective mass. It is conceivable that an analysis
of this knowledge can begin to form the basis of a
large-scale theory of semantic meaning and its inter-
connections, support observation of the process of
lexical development and usage in humans, and even
suggest explanations of how knowledge is organized
in our brains, especially when performed for differ-
</bodyText>
<page confidence="0.987401">
1617
</page>
<bodyText confidence="0.978007">
ent languages on the WWW.
</bodyText>
<sectionHeader confidence="0.959628" genericHeader="method">
3 Inducing Semantic Networks in the Web
</sectionHeader>
<bodyText confidence="0.999892825">
Text mining algorithms such as those mentioned
above raise certain questions, such as: Why are some
seed terms more powerful (provide a greater yield)
than others?, How can one find high-yield terms?,
How many steps does one need, typically, to learn
all terms for a given relation?, Can one estimate the
total eventual yield of a given relation?, and so on.
On the face of it, one would need to know the struc-
ture of the network a priori to be able to provide an-
swers. But research has shown that some surpris-
ing regularities hold. For example, in the text min-
ing community, (Kozareva and Hovy, 2010b) have
shown that one can obtain a quite accurate estimate
of the eventual yield of a pattern and seed after only
five steps of harvesting. Why is this? They do not
provide an answer, but research from the network
community does.
To illustrate the properties of networks of the kind
induced by semantic relations, and to show the ap-
plicability of network research to text harvesting, we
implemented a harvesting algorithm and applied it
to a representative set of relations and seeds in two
languages.
Since the goal of this paper is not the development
of a new text harvesting algorithm, we implemented
a version of an existing one: the so-called DAP
(doubly-anchored pattern) algorithm (Kozareva et
al., 2008), because it (1) is easy to implement, (2)
requires minimum input (one pattern and one seed
example), (3) achieves very high precision com-
pared to existing methods (Pasca, 2004; Etzioni et
al., 2005; Pasca, 2007), (4) enriches existing se-
mantic lexical repositories such as WordNet and
Yago (Suchanek et al., 2007), (5) can be formulated
to learn semantic lexicons and relations for noun,
verb and verb+preposition syntactic constructions;
(6) functions equally well in different languages.
Next we describe the knowledge harvesting proce-
dure and the construction of the text-mined semantic
networks.
</bodyText>
<subsectionHeader confidence="0.999905">
3.1 Harvesting to Induce Semantic Networks
</subsectionHeader>
<bodyText confidence="0.999773166666667">
For a given semantic class of interest say singers, the
algorithm starts with a seed example of the class, say
Madonna. The seed term is inserted in the lexico-
syntactic pattern “class such as seed and *”, which
learns on the position of the * new terms of type
class. The newly learned terms are then individually
placed into the position of the seed in the pattern,
and the bootstrapping process is repeated until no
new terms are found. The output of the algorithm
is a set of terms for the semantic class. The algo-
rithm is implemented as a breadth-first search and
its mechanism is described as follows:
</bodyText>
<listItem confidence="0.639881">
1. Given:
</listItem>
<figure confidence="0.91472975">
a language L={English, Spanish}
a pattern Pi={such as, including, verb prep,
noun}
a seed term seed for Pi
</figure>
<listItem confidence="0.997831625">
2. Build a query for Pi using template Ti ‘class such
as seed and *’, ‘class including seed and *’, ‘*
and seed verb prep’, ‘* and seed noun’, ‘seed
and * noun’
3. Submit Ti to Yahoo! or other search engine
4. Extract terms occupying the * position
5. Feed terms from 4. into 2.
6. Repeat steps 2–5. until no new terms are found
</listItem>
<bodyText confidence="0.999860526315789">
The output of the knowledge harvesting algorithm
is a network of semantic terms interconnected by
the semantic relation captured in the pattern. We
can represent the traversed (implicit) network as a
directed graph G(V,E) with nodes V (|V  |= n)
and edges E(|E |= m). A node u in the net-
work corresponds to a term discovered during boot-
strapping. An edge (u, v) E E represents an ex-
isting link between two terms. The direction of the
edge indicates that the term v was generated by the
term u. For example, given the sentence (where
the pattern is in italics and the extracted term is un-
derlined) “He loves singers such as Madonna and
Michael Jackson”, two nodes Madonna and Michael
Jackson with an edge e=(Madonna, Michael Jack-
son) would be created in the graph G. Figure 1
shows a small example of the singer network. The
starting seed term Madonna is shown in red color
and the harvested terms are in blue.
</bodyText>
<subsectionHeader confidence="0.997447">
3.2 Data
</subsectionHeader>
<bodyText confidence="0.999766">
We harvested data from the Web for a representa-
tive selection of semantic classes and relations, of
</bodyText>
<page confidence="0.988415">
1618
</page>
<figureCaption confidence="0.999861">
Figure 1: Harvesting Procedure.
</figureCaption>
<bodyText confidence="0.986078">
the type used in (Etzioni et al., 2005; Pasca, 2007;
Kozareva and Hovy, 2010a):
</bodyText>
<listItem confidence="0.991949">
• semantic classes that can be learned using dif-
ferent seeds (e.g., “singers such as Madonna
and *” and “singers such as Placido Domingo
and *”);
• semantic classes that are expressed through dif-
ferent lexico-syntactic patterns (e.g., “weapons
such as bombs and *” and “weapons including
bombs and *”);
• verbs and adjectives characterizing the seman-
tic class (e.g., “expensive and * car”, “dogs
run and *”);
• semantic relations with more complex lexico-
syntactic structure (e.g., “* and Easyjet fly to”,
“* and Sam live in”);
• semantic classes that are obtained in differ-
ent languages, such as English and Spanish
(e.g., “singers such as Madonna and *” and
“cantantes como Madonna y *”);
</listItem>
<bodyText confidence="0.996446285714286">
While most of these variations have been explored
in individual papers, we have found no paper that
covers them all, and none whatsoever that uses verbs
and adjectives as seeds.
Using the above procedure to generate the data,
each pattern was submitted as a query to Ya-
hoo!Boss. For each query the top 1000 text snippets
were retrieved. The algorithm ran until exhaustion.
In total, we collected 10GB of data which was part-
of-speech tagged with Treetagger (Schmid, 1994)
and used for the semantic term extraction. Table 1
summarizes the number of nodes and edges learned
for each semantic network using pattern Pi and the
initial seed shown in italics.
</bodyText>
<table confidence="0.9895615">
Lexico-Syntactic Pattern Nodes Edges
P1=“singers such as Madonna and *” 1115 1942
P2=“singers such as Placido Domingo and *” 815 1114
P3=“emotions including anger and *” 113 250
P4=“emotions such as anger and *” 748 2547
P5=“diseases such as malaria and *” 3168 6752
P6=“drugs such as ibuprofen and *” 2513 9428
P7=“expensive and * cars” 4734 22089
Ps=“* and tasty fruits” 1980 7874
P9=“whales swim and *” 869 2163
P10=“dogs chase and *” 4252 20212
P11=“Britney Spears dances and *” 354 540
P12=“John reads and *” 3894 18545
P13=“* and Easyjet fly to” 3290 6480
P14=“* and Charlie work for” 2125 3494
P15=“* and Sam live in” 6745 24348
P16=“cantantes como Madonna y *” 240 318
P17=“gente como Jorge y *” 572 701
</table>
<tableCaption confidence="0.999744">
Table 1: Size of the Semantic Networks.
</tableCaption>
<sectionHeader confidence="0.8281495" genericHeader="method">
4 Statistical Properties of Text-Mined
Semantic Networks
</sectionHeader>
<bodyText confidence="0.999872666666667">
In this section we apply a range of relevant mea-
sures from the network analysis community to the
networks described above.
</bodyText>
<subsectionHeader confidence="0.991712">
4.1 Centrality
</subsectionHeader>
<bodyText confidence="0.999989681818182">
The first statistical property we explore is centrality.
It measures the degree to which the network struc-
ture determines the importance of a node in the net-
work (Sabidussi, 1966; Freeman, 1979).
We explore the effect of two centrality measures:
indegree and outdegree. The indegree of a node
u denoted as indegree(u)=E(v, u) considers the
sum of all incoming edges to u and captures the abil-
ity of a semantic term to be discovered by other se-
mantic terms. The outdegree of a node u denoted
as outdegree(u)=E(u, v) considers the number of
outgoing edges of the node u and measures the abil-
ity of a semantic term to discover new terms. In-
tuitively, the more central the node u is, the more
confident we are that it is a correct term.
Since harvesting algorithms are notorious for ex-
tracting erroneous information, we use the two cen-
trality measures to rerank the harvested elements.
Table 2 shows the accuracy2 of the singer seman-
tic terms at different ranks using the in and out
degree measures. Consistently, outdegree outper-
forms indegree and reaches higher accuracy. This
</bodyText>
<footnote confidence="0.6961985">
2Accuracy is calculated as the number of correct terms at
rank R divided by the total number of terms at rank R.
</footnote>
<figure confidence="0.999063222222222">
Madonna
Ce(ine Dion
Michae(
Jackson
Emma Prince Beyonce Kary
Rihanna
E(ton John
Do((y Paton
Tina Turner
Stevie
Wonder
Freddie
Mercury
Shakira
A(icia Keys
D11
Rickey
Martin
</figure>
<page confidence="0.987561">
1619
</page>
<bodyText confidence="0.995451428571429">
shows that for the text-mined semantic networks, the
ability of a term to discover new terms is more im-
portant than the ability to be discovered.
network, we plot the best-fitting power-law function
(Clauset et al., 2009) which fits well all degree dis-
tributions. Table 4 shows the power-law exponent
values for all text-mined semantic networks.
</bodyText>
<table confidence="0.994478571428571">
@rank in-degree out-degree
10 .92 1.0
25 .91 1.0
50 .90 .97
75 .90 .96
100 .89 .96
150 .88 .95
</table>
<tableCaption confidence="0.99823">
Table 2: Accuracy of the Singer Terms.
</tableCaption>
<bodyText confidence="0.983478666666667">
This poses the question “What are the terms with
high and low outdegree?”. Table 3 shows the top
and bottom 10 terms of the semantic class.
</bodyText>
<table confidence="0.999123181818182">
Semantic Class top 10 outDegree bottom 10 outDegree
Singers Frank Sinatra Alanis Morisette
Ella Fitzgerald Christine Agulera
Billie Holiday Buffy Sainte-Marie
Britney Spears Cece Winans
Aretha Franklin Wolfman Jack
Michael Jackson Billie Celebration
Celine Dion Alejandro Sanz
Beyonce France Gall
Bessie Smith Peter
Joni Mitchell Sarah
</table>
<tableCaption confidence="0.999193">
Table 3: Singer Term Ranking with Centrality Measures.
</tableCaption>
<bodyText confidence="0.999967181818182">
The nodes with high outdegree correspond to fa-
mous or contemporary singers. The lower-ranked
nodes are mostly spelling errors such as Alanis
Morisette and Christine Agulera, less known singers
such as Buffy Sainte-Marie and Cece Winans, non-
American singers such as Alejandro Sanz and
France Gall, extractions due to part-of-speech tag-
ging errors such as Billie Celebration, and general
terms such as Peter and Sarah. Potentially, know-
ing which terms have a high outdegree allows one to
rerank candidate seeds for more effective harvesting.
</bodyText>
<subsectionHeader confidence="0.991589">
4.2 Power-law Degree Distribution
</subsectionHeader>
<bodyText confidence="0.9992056">
We next study the degree distributions of the net-
works. Similarly to the Web (Broder et al., 2000)
and social networks like Orkut and Flickr, the text-
mined semantic networks also exhibit a power-law
distribution. This means that while a few terms have
a significantly high degree, the majority of the se-
mantic terms have small degree. Figure 2 shows the
indegree and outdegree distributions for different
semantic classes, lexico-syntactic patterns, and lan-
guages (English and Spanish). For each semantic
</bodyText>
<table confidence="0.9983428">
Patt. &apos;Pin &apos;Pout Patt. &apos;Pin &apos;Pout
P1 2.37 1.27 P10 1.65 1.12
P2 2.25 1.21 P11 2.42 1.41
P3 2.20 1.76 P12 1.60 1.13
P4 2.28 1.18 P13 2.26 1.20
P5 2.49 1.18 P14 2.43 1.25
P6 2.42 1.30 P15 2.51 1.43
P7 1.95 1.20 P16 2.74 1.31
Pa 1.94 1.07 P17 2.90 1.20
P9 1.96 1.30
</table>
<tableCaption confidence="0.997659">
Table 4: Power-Law Exponents of Semantic Networks.
</tableCaption>
<bodyText confidence="0.999966125">
It is interesting to note that the indegree power-
law exponents for all semantic networks fall within
the same range (-yin ≈ 2.4), and similarly for the
outdegree exponents (gout ≈ 1.3). However, the
values of the indegree and outdegree exponents
differ from each other. This observation is consistent
with Web degree distributions (Broder et al., 2000).
The difference in the distributions can be explained
by the link asymmetry of semantic terms: A discov-
ering B does not necessarily mean that B will dis-
cover A. In the text-mined semantic networks, this
asymmetry is caused by patterns of language use,
such as the fact that people use first adjectives of the
size and then of the color (e.g., big red car), or prefer
to place male before female proper names. Harvest-
ing patterns should take into account this tendency.
</bodyText>
<subsectionHeader confidence="0.992753">
4.3 Sparsity
</subsectionHeader>
<bodyText confidence="0.999879333333333">
Another relevant property of the semantic networks
concerns sparsity. Following Preiss (Preiss, 1999), a
graph is sparse if |E |= O(|V |k) and 1 &lt; k &lt; 2,
where |E |is the number of edges and |V  |is the num-
ber of nodes, otherwise the graph is dense. For the
studied text-semantic networks, k is ≈ 1.08. Spar-
sity can be also captured through the density of the
semantic network which is computed as V (El
11).All
networks have low density which suggests that the
networks exhibit a sparse connectivity pattern. On
average a node (semantic term) is connected to a
very small percentage of other nodes. Similar be-
havior was reported for the WordNet and Roget’s se-
mantic networks (Steyvers and Tenenbaum, 2004).
</bodyText>
<page confidence="0.982268">
1620
</page>
<figureCaption confidence="0.997009">
Figure 2: Degree Distributions of Semantic Networks.
</figureCaption>
<subsectionHeader confidence="0.98645">
4.4 Connectedness
</subsectionHeader>
<bodyText confidence="0.999985133333333">
For every network, we computed the strongly con-
nected component (SCC) such that for all nodes (se-
mantic terms) in the SCC, there is a path from any
node to another node in the SCC considering the di-
rection of the edges between the nodes. For each
network, we found that there is only one SCC. The
size of the component is shown in Table 5. Un-
like WordNet and Roget’s semantic networks where
the SCC consists 96% of all semantic terms, in the
text-mined semantic networks only 12 to 55% of the
terms are in the SCC. This shows that not all nodes
can reach (discover) every other node in the net-
work. This also explains the findings of (Kozareva
et al., 2008; Vyas et al., 2009) why starting with a
good seed is important.
</bodyText>
<subsectionHeader confidence="0.998917">
4.5 Path Lengths and Diameter
</subsectionHeader>
<bodyText confidence="0.999928">
Next, we describe the properties of the shortest paths
between the semantic terms in the SCC. The dis-
tance between two nodes in the SCC is measured as
the length of the shortest path connecting the terms.
The direction of the edges between the terms is taken
into consideration. The average distance is the aver-
age value of the shortest path lengths over all pairs
of nodes in the SCC. The diameter of the SCC is
calculated as the maximum distance over all pairs of
nodes (u, v), such that a node v is reachable from
node u. Table 5 shows the average distance and the
diameter of the semantic networks.
</bodyText>
<table confidence="0.999728833333334">
Patt. #nodes in SCC SCC Average Distance SCC Diameter
P1 364 (.33) 5.27 16
P2 285 (.35) 4.65 13
P3 48 (.43) 2.85 6
P4 274 (.37) 2.94 7
P5 1249 (.38) 5.99 17
P6 1471 (.29) 4.82 15
P7 2255 (.46 ) 3.51 11
Pa 1012 (.50) 3.87 11
P9 289 (.33) 4.93 13
P10 2342 (.55) 4.50 12
P11 87 (.24) 5.00 11
P12 1967 (.51) 3.20 13
P13 1249 (.38) 4.75 13
P14 608 (.29) 7.07 23
P15 1752 (.26) 5.32 15
P16 56 (.23) 4.79 12
P17 69 (.12 ) 5.01 13
</table>
<tableCaption confidence="0.9318525">
Table 5: SCC, SCC Average Distance and SCC Diameter
of the Semantic Networks.
</tableCaption>
<bodyText confidence="0.999978933333333">
The diameter shows the maximum number of
steps necessary to reach from any node to any other,
while the average distance shows the number of
steps necessary on average. Overall, all networks
have very short average path lengths and small di-
ameters that are consistent with Watt’s finding for
small-world networks. Therefore, the yield of har-
vesting seeds can be predicted within five steps ex-
plaining (Kozareva and Hovy, 2010b; Vyas et al.,
2009).
We also compute for any randomly selected node
in the semantic network on average how many hops
(steps) are necessary to reach from one node to an-
other. Figure 3 shows the obtained results for some
of the studied semantic networks.
</bodyText>
<subsectionHeader confidence="0.990698">
4.6 Clustering
</subsectionHeader>
<bodyText confidence="0.961321333333333">
The clustering coefficient (C) is another measure
to study the connectivity structure of the networks
(Watts and Strogatz, 1998). This measure captures
the probability that the two neighbors of a randomly
selected node will be neighbors. The clustering co-
efficient of a node u is calculated as Cu= �����
</bodyText>
<figure confidence="0.995325494845361">
ki(ki�1)
0 20 40 60 80 100 120
Outdegree
0 10 20 30 40 50 60
Indegree
500
450
400
350
300
250
200
150
100
50
0
1 2 3 4 5 6 7 8
Indegree
0 5 10 15 20 25 30 35
Outdegree
&apos;gente&apos;
power-law exponent=1.20
0 2 4
6 8 10 12 14
Outdegree
Number of Nodes
Number of Nodes
500
450
400
350
300
250
200
150
100
50
0
&apos;emotions&apos;
power-law exponent=2.28
Number of Nodes
120
100
80
60
40
20
0
&apos;emotions&apos;
power-law exponent=1.18
0 10 20 30 40 50 60 70 80 90
Indegree
Number of Nodes
2500
2000
1500
1000
500
0
&apos;travel_to&apos;
power-law exponent=2.26
Number of Nodes
700
600
500
400
300
200
100
0
&apos;fly_to&apos;
power-law exponent=1.20
120
&apos;gente&apos;
power-law exponent=2.90
100
Number of Nodes
40
20
0
80
60
1621
Britney Spears (verb harvesting)
fruits (adjective harvesting)
Number of Nodes
Number of Nodes
30
work for
gente
25
20
15
10
Number of Nodes
Number of Nodes
5
</figure>
<tableCaption confidence="0.415589">
Table 6: Clustering Coefficient of the Semantic Networks.
</tableCaption>
<table confidence="0.942778">
0
Patt. C ClosedTriads OpenTriads
P1 .01 14096 (.97) 388 (.03)
P2 .01 6487 (.97) 213 (.03)
P3 .30 1898 (.94) 129 (.06)
P4 .33 60734 (.94) 3944 (.06)
P5 .10 79986 (.97) 2321 (.03)
P6 .11 78716 (.97) 2336 (.03)
P7 .17 910568 (.95) 43412 (.05)
Pa .19 21138 (.95) 10728 (.05)
Py .20 27830 (.95) 1354 (.05)
P10 .15 712227 (.96) 62101(.04)
P11 .09 3407 (.98) 63 (.02)
P12 .15 734724 (.96) 32517 (.04)
P13 .06 66162 (.99) 858 (.01)
P14 .05 28216 (.99) 408 (.01)
P15 .09 1336679 (.97) 47110 (.03)
P16 .09 1525 (.98) 37 ( .02)
P17 .05 2222 (.99) 21 (.01)
</table>
<figure confidence="0.98012059375">
400
350
300
250
200
150
100
50
0
1 2 3 4 5 6 7 8 9 10 11
Distance (Hops)
70
60
50
40
30
20
10
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Distance (Hops)
250
200
150
100
50
0
2 4 6 8 10 12 14 16 18 20 22 24
Distance (Hops)
2 4 6 8 10 12 14 16 18
20
Distance (Hops)
</figure>
<figureCaption confidence="0.999871">
Figure 3: Hop Plot of the Semantic Networks.
</figureCaption>
<bodyText confidence="0.9994964375">
: vi, vj E Nu, eij E E, where ku is the total degree
of the node u and Nu is the neighborhood of u. The
clustering coefficient C for the whole semantic net-
work is the average clustering coefficient of all its
nodes, C= 1n E Ci. The value of the clustering coef-
ficient ranges between [0, 1], where 0 indicates that
the nodes do not have neighbors which are them-
selves connected, while 1 indicates that all nodes are
connected. Table 6 shows the clustering coefficient
for all text-mined semantic networks together with
the number of closed and open triads3. The analysis
suggests the presence of a strong local cluster, how-
ever there are few possibilities to form overlapping
neighborhoods of nodes. The clustering coefficient
of WordNet (Steyvers and Tenenbaum, 2004) is sim-
ilar to those of the text-mined networks.
</bodyText>
<subsectionHeader confidence="0.997302">
4.7 Joint Degree Distribution
</subsectionHeader>
<bodyText confidence="0.999974111111111">
In social networks, understanding the preferential at-
tachment of nodes is important to identify the speed
with which epidemics or gossips spread. Similarly,
we are interested in understanding how the nodes of
the semantic networks connect to each other. For
this purpose, we examine the Joint Degree Distribu-
tion (JDD) (Li et al., 2005; Newman, 2003). JDD
is approximated by the degree correlation function
knn which maps the outdegree and the average
</bodyText>
<footnote confidence="0.6750195">
3A triad is three nodes that are connected by either two (open
triad) or three (closed triad) directed ties.
</footnote>
<bodyText confidence="0.998822285714286">
indegree of all nodes connected to a node with
that outdegree. High values of knn indicate that
high-degree nodes tend to connect to other high-
degree nodes (forming a “core” in the network),
while lower values of knn suggest that the high-
degree nodes tend to connect to low-degree ones.
Figure 4 shows the knn for the singer, whale, live
in, cars, cantantes, and gente networks. The figure
plots the outdegree and the average indegree of the
semantic terms in the networks on a log-log scale.
We can see that for all networks the high-degree
nodes tend to connect to other high-degree ones.
This explains why text mining algorithms should fo-
cus their effort on high-degree nodes.
</bodyText>
<subsectionHeader confidence="0.994885">
4.8 Assortivity
</subsectionHeader>
<bodyText confidence="0.999990411764706">
The property of the nodes to connect to other nodes
with similar degrees can be captured through the as-
sortivity coefficient r (Newman, 2003). The range of
r is [−1, 1]. A positive assortivity coefficient means
that the nodes tend to connect to nodes of similar
degree, while negative coefficient means that nodes
are likely to connect to nodes with degree very dif-
ferent from their own. We find that the assortivi-
tiy coefficient of our semantic networks is positive,
ranging from 0.07 to 0.20. In this respect, the se-
mantic networks differ from the Web, which has a
negative assortivity (Newman, 2003). This implies
a difference in text mining and web search traver-
sal strategies: since starting from a highly-connected
seed term will tend to lead to other highly-connected
terms, text mining algorithms should prefer depth-
first traversal, while web search algorithms starting
</bodyText>
<page confidence="0.995739">
1622
</page>
<figureCaption confidence="0.9971975">
Figure 4: Joint Degree Distribution of the Semantic Net-
works.
</figureCaption>
<bodyText confidence="0.95611">
from a highly-connected seed page should prefer a
breadth-first strategy.
</bodyText>
<sectionHeader confidence="0.998197" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999975137931034">
The above studies show that many of the proper-
ties discovered of the network formed by the web
hold also for the networks induced by semantic rela-
tions in text mining applications, for various seman-
tic classes, semantic relations, and languages. We
can therefore apply some of the research from net-
work analysis to text mining.
The small-world phenomenon, for example, holds
that any node is connected to any other node in at
most six steps. Since as shown in Section 4.5 the se-
mantic networks also exhibit this phenomenon, we
can explain the observation of (Kozareva and Hovy,
2010b) that one can quite accurately predict the rel-
ative ‘goodness’ of a seed term (its eventual total
yield and the number of steps required to obtain that)
within five harvesting steps. We have shown that due
to the strongly connected components in text min-
ing networks, not all elements within the harvested
graph can discover each other. This implies that har-
vesting algorithms have to be started with several
seeds to obtain adequate Recall (Vyas et al., 2009).
We have shown that centrality measures can be used
successfully to rank harvested terms to guide the net-
work traversal, and to validate the correctness of the
harvested terms.
In the future, the knowledge and observations
made in this study can be used to model the lexi-
cal usage of people over time and to develop new
semantic search technology.
</bodyText>
<sectionHeader confidence="0.999562" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9997468125">
In this paper we describe the implicit ‘hidden’ se-
mantic network graph structure induced over the text
of the web and other sources by the semantic rela-
tions people use in sentences. We describe how term
harvesting patterns whose seed terms are harvested
and then applied recursively can be used to discover
these semantic term networks. Although these net-
works differ considerably from the web in relation
density, type, and network size, we show, some-
what surprisingly, that the same power-law, small-
world effect, transitivity, and most other character-
istics that apply to the web’s hyperlinked network
structure hold also for the implicit semantic term
graphs—certainly for the semantic relations and lan-
guages we have studied, and most probably for al-
most all semantic relations and human languages.
This rather interesting observation leads us to sur-
mise that the hyperlinks people create in the web are
of essentially the same type as the semantic relations
people use in normal sentences, and that they form
an extension of normal language that was not needed
before because people did not have the ability within
the span of a single sentence to ‘embed’ structures
larger than a clause—certainly not a whole other
page’s worth of information. The principal excep-
tion is the academic citation reference (lexicalized
as “see”), which is not used in modern webpages.
Rather, the ‘lexicalization’ now used is a formatting
convention: the hyperlink is colored and often un-
derlined, facilities offered by computer screens but
not available to speech or easy in traditional typeset-
ting.
</bodyText>
<figure confidence="0.9977024375">
singer (seed is Madonna)
knn 100
10
1
100
1
whale (verb harvesting)
1 10 100 1 10 100
Outdegree Outdegree
live in
cars (adjective harvesting)
knn 100
10
1
100
1
1 10 Ou ,� 1 10 100
tdegree Outdegree
cantantes
gente
knn
10
1
1
Outdegree 10 1 Outdegree 10
10
knn
10
knn
10
knn
1
</figure>
<page confidence="0.953584">
1623
</page>
<sectionHeader confidence="0.997891" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99993425">
We acknowledge the support of DARPA contract
number FA8750-09-C-3705 and NSF grant IIS-
0429360. We would like to thank Sujith Ravi for
his useful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998231546391752">
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
pages 85–94.
Andrei Broder, Ravi Kumar, Farzin Maghoul, Prabhakar
Raghavan, Sridhar Rajagopalan, Raymie Stata, An-
drew Tomkins, and Janet Wiener. 2000. Graph struc-
ture in the web. Comput. Netw., 33(1-6):309–320.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. pages 101–110.
Peng Chen, Huafeng Xie, Sergei Maslov, and Sid Redner.
2007. Finding scientific gems with google’s pagerank
algorithm. Journal of Informetrics, 1(1):8–15, Jan-
uary.
Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. New-
man. 2009. Power-law distributions in empirical data.
SIAM Rev., 51(4):661–703.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91–134,
June.
Linton Freeman. 1979. Centrality in social networks
conceptual clarification. Social Networks, 1(3):215–
239.
Michael Gasser and Linda B. Smith. 1998. Learning
nouns and adjectives: A connectionist account. In
Language and Cognitive Processes, pages 269–306.
Demdre Gentner. 1981. Some interesting differences be-
tween nouns and verbs. Cognition and Brain Theory,
pages 161–178.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 1–8.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics, pages 539–
545.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003. In-
tegrating web-based and corpus-based techniques for
question answering. In Proceedings of the twelfth text
retrieval conference (TREC), pages 426–435.
David Kempe, Jon Kleinberg, and ´Eva Tardos. 2003.
Maximizing the spread of influence through a social
network. In KDD ’03: Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 137–146.
Jon Kleinberg and Steve Lawrence. 2001. The structure
of the web. Science, 29:1849–1850.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL 2010, pages 1482–1491, July.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not all
seeds are equal: Measuring the quality of text mining
seeds. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
618–626.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics ACL-08: HLT, pages 1048–1056.
Beth Levin and Harold Somers. 1993. English verb
classes and alternations: A preliminary investigation.
Lun Li, David Alderson, Reiko Tanaka, John C. Doyle,
and Walter Willinger. 2005. Towards a Theory of
Scale-Free Graphs: Definition, Properties, and Impli-
cations (Extended Version). Internet Mathematica,
2(4):431–523.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of the 19th international confer-
ence on Computational linguistics, pages 1–7.
Mark E. Newman and Michelle Girvan. 2004. Find-
ing and evaluating community structure in networks.
Physical Review, 69(2).
Mark Newman. 2003. Mixing patterns in networks.
Physical Review E, 67.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. pages 113–120.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137–145.
</reference>
<page confidence="0.877959">
1624
</page>
<reference confidence="0.999691136986302">
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of the Sixteenth ACM Conference on Information
and Knowledge Management, CIKM 2007, pages 683–
690.
Bruno R. Preiss. 1999. Data structures and algorithms
with object-oriented design patterns in C++.
Filippo Radicchi, Santo Fortunato, Benjamin Markines,
and Alessandro Vespignani. 2009. Diffusion of scien-
tific credits and the ranking of scientists. In Phys. Rev.
E 80, 056103.
Deepack Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. pages 41–47.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Empirical Methods for Natural Language
Processing, pages 117–124.
Ellen Riloff. 1993. Automatically constructing a dictio-
nary for information extraction tasks. pages 811–816.
Peter Mark Roget. 1911. Roget’s thesaurus of English
Words and Phrases. New York Thomas Y. Crowell
company.
Gert Sabidussi. 1966. The centrality index of a graph.
Psychometrika, 31(4):581–603.
Hassan Sayyadi and Lise Getoor. 2009. Future rank:
Ranking scientific articles by predicting their future
pagerank. In 2009 SIAM International Conference on
Data Mining (SDM09).
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44–49.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. pages 1297–1304.
Stephen Soderland, Claire Cardie, and Raymond
Mooney. 1999. Learning information extraction rules
for semi-structured and free text. Machine Learning,
34(1-3), pages 233–272.
Mark Steyvers and Joshua B. Tenenbaum. 2004. The
large-scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cognitive
Science, 29:41–78.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ’07: Proceedings of the 16th international
conference on World Wide Web, pages 697–706.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Graph-based weakly-supervised methods for informa-
tion extraction and integration. pages 1473–1481.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Management,
CIKM, pages 225–234.
Dylan Walker, Huafeng Xie, Koon-Kiu Yan, and Sergei
Maslov. 2006. Ranking scientific publications using a
simple model of network traffic. December.
Duncan Watts and Steven Strogatz. 1998. Collec-
tive dynamics of ’small-world’ networks. Nature,
393(6684):440–442.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In ACL-44: Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 849–856.
Dmitry Zelenko, Chinatsu Aone, Anthony Richardella,
Jaz K, Thomas Hofmann, Tomaso Poggio, and John
Shawe-taylor. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research 3.
</reference>
<page confidence="0.992365">
1625
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754416">
<title confidence="0.996178">Insights from Network Structure for Text Mining</title>
<author confidence="0.976953">Zornitsa Kozareva</author>
<author confidence="0.976953">Eduard</author>
<affiliation confidence="0.993471">USC Information Sciences</affiliation>
<address confidence="0.987056">4676 Admiralty</address>
<author confidence="0.795949">Marina del Rey</author>
<author confidence="0.795949">CA</author>
<abstract confidence="0.999170047619048">Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<pages>85--94</pages>
<contexts>
<context position="1973" citStr="Agichtein and Gravano, 2000" startWordPosition="296" endWordPosition="299">u et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the </context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. pages 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Broder</author>
<author>Ravi Kumar</author>
<author>Farzin Maghoul</author>
<author>Prabhakar Raghavan</author>
<author>Sridhar Rajagopalan</author>
<author>Raymie Stata</author>
<author>Andrew Tomkins</author>
<author>Janet Wiener</author>
</authors>
<title>Graph structure in the web.</title>
<date>2000</date>
<journal>Comput. Netw.,</journal>
<pages>33--1</pages>
<contexts>
<context position="2947" citStr="Broder et al., 2000" startWordPosition="456" endWordPosition="459">rns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 1616 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616–1625, Portland, Oregon, June 19-24, 201</context>
<context position="6074" citStr="Broder et al. (2000)" startWordPosition="934" endWordPosition="937">ine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ability to analyze networks, identify influential nodes, and discover hidden structures has led to important scientific and technological breakthroughs such as the discovery of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However, no-one has studied the properties of the text-based semantic networks induced by semantic relations between terms with the objective of un</context>
<context position="17744" citStr="Broder et al., 2000" startWordPosition="2895" endWordPosition="2898">r contemporary singers. The lower-ranked nodes are mostly spelling errors such as Alanis Morisette and Christine Agulera, less known singers such as Buffy Sainte-Marie and Cece Winans, nonAmerican singers such as Alejandro Sanz and France Gall, extractions due to part-of-speech tagging errors such as Billie Celebration, and general terms such as Peter and Sarah. Potentially, knowing which terms have a high outdegree allows one to rerank candidate seeds for more effective harvesting. 4.2 Power-law Degree Distribution We next study the degree distributions of the networks. Similarly to the Web (Broder et al., 2000) and social networks like Orkut and Flickr, the textmined semantic networks also exhibit a power-law distribution. This means that while a few terms have a significantly high degree, the majority of the semantic terms have small degree. Figure 2 shows the indegree and outdegree distributions for different semantic classes, lexico-syntactic patterns, and languages (English and Spanish). For each semantic Patt. &apos;Pin &apos;Pout Patt. &apos;Pin &apos;Pout P1 2.37 1.27 P10 1.65 1.12 P2 2.25 1.21 P11 2.42 1.41 P3 2.20 1.76 P12 1.60 1.13 P4 2.28 1.18 P13 2.26 1.20 P5 2.49 1.18 P14 2.43 1.25 P6 2.42 1.30 P15 2.51 1.</context>
</contexts>
<marker>Broder, Kumar, Maghoul, Raghavan, Rajagopalan, Stata, Tomkins, Wiener, 2000</marker>
<rawString>Andrei Broder, Ravi Kumar, Farzin Maghoul, Prabhakar Raghavan, Sridhar Rajagopalan, Raymie Stata, Andrew Tomkins, and Janet Wiener. 2000. Graph structure in the web. Comput. Netw., 33(1-6):309–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Richard C Wang</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<pages>101--110</pages>
<contexts>
<context position="1604" citStr="Carlson et al., 2010" startWordPosition="244" endWordPosition="247">ain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which dat</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. pages 101–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Chen</author>
<author>Huafeng Xie</author>
<author>Sergei Maslov</author>
<author>Sid Redner</author>
</authors>
<title>Finding scientific gems with google’s pagerank algorithm.</title>
<date>2007</date>
<journal>Journal of Informetrics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="6025" citStr="Chen et al., 2007" startWordPosition="926" endWordPosition="929">tructure and applying this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ability to analyze networks, identify influential nodes, and discover hidden structures has led to important scientific and technological breakthroughs such as the discovery of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However, no-one has studied the properties of the text-based semantic networks induced by semantic</context>
</contexts>
<marker>Chen, Xie, Maslov, Redner, 2007</marker>
<rawString>Peng Chen, Huafeng Xie, Sergei Maslov, and Sid Redner. 2007. Finding scientific gems with google’s pagerank algorithm. Journal of Informetrics, 1(1):8–15, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Clauset</author>
<author>Cosma Rohilla Shalizi</author>
<author>M E J Newman</author>
</authors>
<title>Power-law distributions in empirical data.</title>
<date>2009</date>
<journal>SIAM Rev.,</journal>
<volume>51</volume>
<issue>4</issue>
<contexts>
<context position="3017" citStr="Clauset et al., 2009" startWordPosition="468" endWordPosition="471">sting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 1616 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616–1625, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics rithms have the li</context>
<context position="16282" citStr="Clauset et al., 2009" startWordPosition="2662" endWordPosition="2665">he in and out degree measures. Consistently, outdegree outperforms indegree and reaches higher accuracy. This 2Accuracy is calculated as the number of correct terms at rank R divided by the total number of terms at rank R. Madonna Ce(ine Dion Michae( Jackson Emma Prince Beyonce Kary Rihanna E(ton John Do((y Paton Tina Turner Stevie Wonder Freddie Mercury Shakira A(icia Keys D11 Rickey Martin 1619 shows that for the text-mined semantic networks, the ability of a term to discover new terms is more important than the ability to be discovered. network, we plot the best-fitting power-law function (Clauset et al., 2009) which fits well all degree distributions. Table 4 shows the power-law exponent values for all text-mined semantic networks. @rank in-degree out-degree 10 .92 1.0 25 .91 1.0 50 .90 .97 75 .90 .96 100 .89 .96 150 .88 .95 Table 2: Accuracy of the Singer Terms. This poses the question “What are the terms with high and low outdegree?”. Table 3 shows the top and bottom 10 terms of the semantic class. Semantic Class top 10 outDegree bottom 10 outDegree Singers Frank Sinatra Alanis Morisette Ella Fitzgerald Christine Agulera Billie Holiday Buffy Sainte-Marie Britney Spears Cece Winans Aretha Franklin</context>
</contexts>
<marker>Clauset, Shalizi, Newman, 2009</marker>
<rawString>Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. Newman. 2009. Power-law distributions in empirical data. SIAM Rev., 51(4):661–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1531" citStr="Etzioni et al., 2005" startWordPosition="232" endWordPosition="235">his paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et a</context>
<context position="4718" citStr="Etzioni et al., 2005" startWordPosition="726" endWordPosition="729">ional linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, the Social Web, the network of scientific papers, among others. They have investigat</context>
<context position="9772" citStr="Etzioni et al., 2005" startWordPosition="1543" endWordPosition="1546"> by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implemented a version of an existing one: the so-called DAP (doubly-anchored pattern) algorithm (Kozareva et al., 2008), because it (1) is easy to implement, (2) requires minimum input (one pattern and one seed example), (3) achieves very high precision compared to existing methods (Pasca, 2004; Etzioni et al., 2005; Pasca, 2007), (4) enriches existing semantic lexical repositories such as WordNet and Yago (Suchanek et al., 2007), (5) can be formulated to learn semantic lexicons and relations for noun, verb and verb+preposition syntactic constructions; (6) functions equally well in different languages. Next we describe the knowledge harvesting procedure and the construction of the text-mined semantic networks. 3.1 Harvesting to Induce Semantic Networks For a given semantic class of interest say singers, the algorithm starts with a seed example of the class, say Madonna. The seed term is inserted in the l</context>
<context position="12364" citStr="Etzioni et al., 2005" startWordPosition="1998" endWordPosition="2001">ated by the term u. For example, given the sentence (where the pattern is in italics and the extracted term is underlined) “He loves singers such as Madonna and Michael Jackson”, two nodes Madonna and Michael Jackson with an edge e=(Madonna, Michael Jackson) would be created in the graph G. Figure 1 shows a small example of the singer network. The starting seed term Madonna is shown in red color and the harvested terms are in blue. 3.2 Data We harvested data from the Web for a representative selection of semantic classes and relations, of 1618 Figure 1: Harvesting Procedure. the type used in (Etzioni et al., 2005; Pasca, 2007; Kozareva and Hovy, 2010a): • semantic classes that can be learned using different seeds (e.g., “singers such as Madonna and *” and “singers such as Placido Domingo and *”); • semantic classes that are expressed through different lexico-syntactic patterns (e.g., “weapons such as bombs and *” and “weapons including bombs and *”); • verbs and adjectives characterizing the semantic class (e.g., “expensive and * car”, “dogs run and *”); • semantic relations with more complex lexicosyntactic structure (e.g., “* and Easyjet fly to”, “* and Sam live in”); • semantic classes that are obt</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linton Freeman</author>
</authors>
<title>Centrality in social networks conceptual clarification.</title>
<date>1979</date>
<journal>Social Networks,</journal>
<volume>1</volume>
<issue>3</issue>
<pages>239</pages>
<contexts>
<context position="2806" citStr="Freeman, 1979" startWordPosition="432" endWordPosition="433">n which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 161</context>
<context position="14890" citStr="Freeman, 1979" startWordPosition="2425" endWordPosition="2426">3=“* and Easyjet fly to” 3290 6480 P14=“* and Charlie work for” 2125 3494 P15=“* and Sam live in” 6745 24348 P16=“cantantes como Madonna y *” 240 318 P17=“gente como Jorge y *” 572 701 Table 1: Size of the Semantic Networks. 4 Statistical Properties of Text-Mined Semantic Networks In this section we apply a range of relevant measures from the network analysis community to the networks described above. 4.1 Centrality The first statistical property we explore is centrality. It measures the degree to which the network structure determines the importance of a node in the network (Sabidussi, 1966; Freeman, 1979). We explore the effect of two centrality measures: indegree and outdegree. The indegree of a node u denoted as indegree(u)=E(v, u) considers the sum of all incoming edges to u and captures the ability of a semantic term to be discovered by other semantic terms. The outdegree of a node u denoted as outdegree(u)=E(u, v) considers the number of outgoing edges of the node u and measures the ability of a semantic term to discover new terms. Intuitively, the more central the node u is, the more confident we are that it is a correct term. Since harvesting algorithms are notorious for extracting erro</context>
</contexts>
<marker>Freeman, 1979</marker>
<rawString>Linton Freeman. 1979. Centrality in social networks conceptual clarification. Social Networks, 1(3):215– 239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gasser</author>
<author>Linda B Smith</author>
</authors>
<title>Learning nouns and adjectives: A connectionist account.</title>
<date>1998</date>
<booktitle>In Language and Cognitive Processes,</booktitle>
<pages>269--306</pages>
<contexts>
<context position="7457" citStr="Gasser and Smith, 1998" startWordPosition="1151" endWordPosition="1154">s and Tenenbaum, 2004), who studied three manually built lexical networks (association norms, WordNet, and Roget’s Thesaurus (Roget, 1911)) and proposed a model of the growth of the semantic structure over time. These networks are limited to the semantic relations among nouns. In this paper we take a step further to explore the statistical properties of semantic networks relating proper names, nouns, verbs, and adjectives. Understanding the semantics of nouns, verbs, and adjectives has been of great interest to linguists and cognitive scientists such as (Gentner, 1981; Levin and Somers, 1993; Gasser and Smith, 1998). We implement a general harvesting procedure and show its results for these word types. A fundamental difference with the work of (Steyvers and Tenenbaum, 2004) is that we study very large semantic networks built ‘naturally’ by (millions of) users rather than ‘artificially’ by a small set of experts. The large networks capture the semantic intuitions and knowledge of the collective mass. It is conceivable that an analysis of this knowledge can begin to form the basis of a large-scale theory of semantic meaning and its interconnections, support observation of the process of lexical development</context>
</contexts>
<marker>Gasser, Smith, 1998</marker>
<rawString>Michael Gasser and Linda B. Smith. 1998. Learning nouns and adjectives: A connectionist account. In Language and Cognitive Processes, pages 269–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Demdre Gentner</author>
</authors>
<title>Some interesting differences between nouns and verbs. Cognition and Brain Theory,</title>
<date>1981</date>
<pages>161--178</pages>
<contexts>
<context position="7408" citStr="Gentner, 1981" startWordPosition="1145" endWordPosition="1146">work of Steyvers and Tenenbaum (Steyvers and Tenenbaum, 2004), who studied three manually built lexical networks (association norms, WordNet, and Roget’s Thesaurus (Roget, 1911)) and proposed a model of the growth of the semantic structure over time. These networks are limited to the semantic relations among nouns. In this paper we take a step further to explore the statistical properties of semantic networks relating proper names, nouns, verbs, and adjectives. Understanding the semantics of nouns, verbs, and adjectives has been of great interest to linguists and cognitive scientists such as (Gentner, 1981; Levin and Somers, 1993; Gasser and Smith, 1998). We implement a general harvesting procedure and show its results for these word types. A fundamental difference with the work of (Steyvers and Tenenbaum, 2004) is that we study very large semantic networks built ‘naturally’ by (millions of) users rather than ‘artificially’ by a small set of experts. The large networks capture the semantic intuitions and knowledge of the collective mass. It is conceivable that an analysis of this knowledge can begin to form the basis of a large-scale theory of semantic meaning and its interconnections, support </context>
</contexts>
<marker>Gentner, 1981</marker>
<rawString>Demdre Gentner. 1981. Some interesting differences between nouns and verbs. Cognition and Brain Theory, pages 161–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1361" citStr="Girju et al., 2003" startWordPosition="209" endWordPosition="212">cially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and G</context>
<context position="4683" citStr="Girju et al., 2003" startWordPosition="720" endWordPosition="723">discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, the Social Web, the network of scientific papers,</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="4636" citStr="Hearst, 1992" startWordPosition="714" endWordPosition="715">illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, th</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics, pages 539– 545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
<author>Daniel Loreto</author>
<author>Wesley Hildebrandt</author>
<author>Matthew Bilotti</author>
<author>Sue Felshin</author>
<author>Aaron Fernandes</author>
<author>Gregory Marton</author>
<author>Federico Mora</author>
</authors>
<title>Integrating web-based and corpus-based techniques for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the twelfth text retrieval conference (TREC),</booktitle>
<pages>426--435</pages>
<contexts>
<context position="4486" citStr="Katz et al., 2003" startWordPosition="691" endWordPosition="694">and follow with an examination of the various statistical properties of implicit semantic networks in Section 4, using our implemented harvester to provide illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to pro</context>
</contexts>
<marker>Katz, Lin, Loreto, Hildebrandt, Bilotti, Felshin, Fernandes, Marton, Mora, 2003</marker>
<rawString>Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hildebrandt, Matthew Bilotti, Sue Felshin, Aaron Fernandes, Gregory Marton, and Federico Mora. 2003. Integrating web-based and corpus-based techniques for question answering. In Proceedings of the twelfth text retrieval conference (TREC), pages 426–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kempe</author>
<author>Jon Kleinberg</author>
<author>´Eva Tardos</author>
</authors>
<title>Maximizing the spread of influence through a social network. In</title>
<date>2003</date>
<booktitle>KDD ’03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>137--146</pages>
<contexts>
<context position="5858" citStr="Kempe et al., 2003" startWordPosition="898" endWordPosition="901">ocial Web, the network of scientific papers, among others. They have investigated the properties of these text-based networks with the objective of understanding their structure and applying this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ability to analyze networks, identify influential nodes, and discover hidden structures has led to important scientific and technological breakthroughs such as the discovery of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative informa</context>
</contexts>
<marker>Kempe, Kleinberg, Tardos, 2003</marker>
<rawString>David Kempe, Jon Kleinberg, and ´Eva Tardos. 2003. Maximizing the spread of influence through a social network. In KDD ’03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 137–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
<author>Steve Lawrence</author>
</authors>
<date>2001</date>
<booktitle>The structure of the web. Science,</booktitle>
<pages>29--1849</pages>
<contexts>
<context position="2977" citStr="Kleinberg and Lawrence, 2001" startWordPosition="460" endWordPosition="463">ts the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 1616 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616–1625, Portland, Oregon, June 19-24, 2011. c�2011 Association for Comp</context>
</contexts>
<marker>Kleinberg, Lawrence, 2001</marker>
<rawString>Jon Kleinberg and Steve Lawrence. 2001. The structure of the web. Science, 29:1849–1850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning arguments and supertypes of semantic relations using recursive patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010,</booktitle>
<pages>1482--1491</pages>
<contexts>
<context position="8861" citStr="Kozareva and Hovy, 2010" startWordPosition="1391" endWordPosition="1394">ic Networks in the Web Text mining algorithms such as those mentioned above raise certain questions, such as: Why are some seed terms more powerful (provide a greater yield) than others?, How can one find high-yield terms?, How many steps does one need, typically, to learn all terms for a given relation?, Can one estimate the total eventual yield of a given relation?, and so on. On the face of it, one would need to know the structure of the network a priori to be able to provide answers. But research has shown that some surprising regularities hold. For example, in the text mining community, (Kozareva and Hovy, 2010b) have shown that one can obtain a quite accurate estimate of the eventual yield of a pattern and seed after only five steps of harvesting. Why is this? They do not provide an answer, but research from the network community does. To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implem</context>
<context position="12402" citStr="Kozareva and Hovy, 2010" startWordPosition="2004" endWordPosition="2007">ven the sentence (where the pattern is in italics and the extracted term is underlined) “He loves singers such as Madonna and Michael Jackson”, two nodes Madonna and Michael Jackson with an edge e=(Madonna, Michael Jackson) would be created in the graph G. Figure 1 shows a small example of the singer network. The starting seed term Madonna is shown in red color and the harvested terms are in blue. 3.2 Data We harvested data from the Web for a representative selection of semantic classes and relations, of 1618 Figure 1: Harvesting Procedure. the type used in (Etzioni et al., 2005; Pasca, 2007; Kozareva and Hovy, 2010a): • semantic classes that can be learned using different seeds (e.g., “singers such as Madonna and *” and “singers such as Placido Domingo and *”); • semantic classes that are expressed through different lexico-syntactic patterns (e.g., “weapons such as bombs and *” and “weapons including bombs and *”); • verbs and adjectives characterizing the semantic class (e.g., “expensive and * car”, “dogs run and *”); • semantic relations with more complex lexicosyntactic structure (e.g., “* and Easyjet fly to”, “* and Sam live in”); • semantic classes that are obtained in different languages, such as </context>
<context position="22363" citStr="Kozareva and Hovy, 2010" startWordPosition="3727" endWordPosition="3730"> 3.20 13 P13 1249 (.38) 4.75 13 P14 608 (.29) 7.07 23 P15 1752 (.26) 5.32 15 P16 56 (.23) 4.79 12 P17 69 (.12 ) 5.01 13 Table 5: SCC, SCC Average Distance and SCC Diameter of the Semantic Networks. The diameter shows the maximum number of steps necessary to reach from any node to any other, while the average distance shows the number of steps necessary on average. Overall, all networks have very short average path lengths and small diameters that are consistent with Watt’s finding for small-world networks. Therefore, the yield of harvesting seeds can be predicted within five steps explaining (Kozareva and Hovy, 2010b; Vyas et al., 2009). We also compute for any randomly selected node in the semantic network on average how many hops (steps) are necessary to reach from one node to another. Figure 3 shows the obtained results for some of the studied semantic networks. 4.6 Clustering The clustering coefficient (C) is another measure to study the connectivity structure of the networks (Watts and Strogatz, 1998). This measure captures the probability that the two neighbors of a randomly selected node will be neighbors. The clustering coefficient of a node u is calculated as Cu= ����� ki(ki�1) 0 20 40 60 80 100</context>
<context position="28401" citStr="Kozareva and Hovy, 2010" startWordPosition="4818" endWordPosition="4821">r a breadth-first strategy. 5 Discussion The above studies show that many of the properties discovered of the network formed by the web hold also for the networks induced by semantic relations in text mining applications, for various semantic classes, semantic relations, and languages. We can therefore apply some of the research from network analysis to text mining. The small-world phenomenon, for example, holds that any node is connected to any other node in at most six steps. Since as shown in Section 4.5 the semantic networks also exhibit this phenomenon, we can explain the observation of (Kozareva and Hovy, 2010b) that one can quite accurately predict the relative ‘goodness’ of a seed term (its eventual total yield and the number of steps required to obtain that) within five harvesting steps. We have shown that due to the strongly connected components in text mining networks, not all elements within the harvested graph can discover each other. This implies that harvesting algorithms have to be started with several seeds to obtain adequate Recall (Vyas et al., 2009). We have shown that centrality measures can be used successfully to rank harvested terms to guide the network traversal, and to validate </context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010a. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010, pages 1482–1491, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Not all seeds are equal: Measuring the quality of text mining seeds.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>618--626</pages>
<contexts>
<context position="8861" citStr="Kozareva and Hovy, 2010" startWordPosition="1391" endWordPosition="1394">ic Networks in the Web Text mining algorithms such as those mentioned above raise certain questions, such as: Why are some seed terms more powerful (provide a greater yield) than others?, How can one find high-yield terms?, How many steps does one need, typically, to learn all terms for a given relation?, Can one estimate the total eventual yield of a given relation?, and so on. On the face of it, one would need to know the structure of the network a priori to be able to provide answers. But research has shown that some surprising regularities hold. For example, in the text mining community, (Kozareva and Hovy, 2010b) have shown that one can obtain a quite accurate estimate of the eventual yield of a pattern and seed after only five steps of harvesting. Why is this? They do not provide an answer, but research from the network community does. To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implem</context>
<context position="12402" citStr="Kozareva and Hovy, 2010" startWordPosition="2004" endWordPosition="2007">ven the sentence (where the pattern is in italics and the extracted term is underlined) “He loves singers such as Madonna and Michael Jackson”, two nodes Madonna and Michael Jackson with an edge e=(Madonna, Michael Jackson) would be created in the graph G. Figure 1 shows a small example of the singer network. The starting seed term Madonna is shown in red color and the harvested terms are in blue. 3.2 Data We harvested data from the Web for a representative selection of semantic classes and relations, of 1618 Figure 1: Harvesting Procedure. the type used in (Etzioni et al., 2005; Pasca, 2007; Kozareva and Hovy, 2010a): • semantic classes that can be learned using different seeds (e.g., “singers such as Madonna and *” and “singers such as Placido Domingo and *”); • semantic classes that are expressed through different lexico-syntactic patterns (e.g., “weapons such as bombs and *” and “weapons including bombs and *”); • verbs and adjectives characterizing the semantic class (e.g., “expensive and * car”, “dogs run and *”); • semantic relations with more complex lexicosyntactic structure (e.g., “* and Easyjet fly to”, “* and Sam live in”); • semantic classes that are obtained in different languages, such as </context>
<context position="22363" citStr="Kozareva and Hovy, 2010" startWordPosition="3727" endWordPosition="3730"> 3.20 13 P13 1249 (.38) 4.75 13 P14 608 (.29) 7.07 23 P15 1752 (.26) 5.32 15 P16 56 (.23) 4.79 12 P17 69 (.12 ) 5.01 13 Table 5: SCC, SCC Average Distance and SCC Diameter of the Semantic Networks. The diameter shows the maximum number of steps necessary to reach from any node to any other, while the average distance shows the number of steps necessary on average. Overall, all networks have very short average path lengths and small diameters that are consistent with Watt’s finding for small-world networks. Therefore, the yield of harvesting seeds can be predicted within five steps explaining (Kozareva and Hovy, 2010b; Vyas et al., 2009). We also compute for any randomly selected node in the semantic network on average how many hops (steps) are necessary to reach from one node to another. Figure 3 shows the obtained results for some of the studied semantic networks. 4.6 Clustering The clustering coefficient (C) is another measure to study the connectivity structure of the networks (Watts and Strogatz, 1998). This measure captures the probability that the two neighbors of a randomly selected node will be neighbors. The clustering coefficient of a node u is calculated as Cu= ����� ki(ki�1) 0 20 40 60 80 100</context>
<context position="28401" citStr="Kozareva and Hovy, 2010" startWordPosition="4818" endWordPosition="4821">r a breadth-first strategy. 5 Discussion The above studies show that many of the properties discovered of the network formed by the web hold also for the networks induced by semantic relations in text mining applications, for various semantic classes, semantic relations, and languages. We can therefore apply some of the research from network analysis to text mining. The small-world phenomenon, for example, holds that any node is connected to any other node in at most six steps. Since as shown in Section 4.5 the semantic networks also exhibit this phenomenon, we can explain the observation of (Kozareva and Hovy, 2010b) that one can quite accurately predict the relative ‘goodness’ of a seed term (its eventual total yield and the number of steps required to obtain that) within five harvesting steps. We have shown that due to the strongly connected components in text mining networks, not all elements within the harvested graph can discover each other. This implies that harvesting algorithms have to be started with several seeds to obtain adequate Recall (Vyas et al., 2009). We have shown that centrality measures can be used successfully to rank harvested terms to guide the network traversal, and to validate </context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010b. Not all seeds are equal: Measuring the quality of text mining seeds. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 618–626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="4742" citStr="Kozareva et al., 2008" startWordPosition="730" endWordPosition="733">arch. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, the Social Web, the network of scientific papers, among others. They have investigated the properties of the</context>
<context position="9574" citStr="Kozareva et al., 2008" startWordPosition="1510" endWordPosition="1513">ern and seed after only five steps of harvesting. Why is this? They do not provide an answer, but research from the network community does. To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implemented a version of an existing one: the so-called DAP (doubly-anchored pattern) algorithm (Kozareva et al., 2008), because it (1) is easy to implement, (2) requires minimum input (one pattern and one seed example), (3) achieves very high precision compared to existing methods (Pasca, 2004; Etzioni et al., 2005; Pasca, 2007), (4) enriches existing semantic lexical repositories such as WordNet and Yago (Suchanek et al., 2007), (5) can be formulated to learn semantic lexicons and relations for noun, verb and verb+preposition syntactic constructions; (6) functions equally well in different languages. Next we describe the knowledge harvesting procedure and the construction of the text-mined semantic networks.</context>
<context position="20739" citStr="Kozareva et al., 2008" startWordPosition="3420" endWordPosition="3423">strongly connected component (SCC) such that for all nodes (semantic terms) in the SCC, there is a path from any node to another node in the SCC considering the direction of the edges between the nodes. For each network, we found that there is only one SCC. The size of the component is shown in Table 5. Unlike WordNet and Roget’s semantic networks where the SCC consists 96% of all semantic terms, in the text-mined semantic networks only 12 to 55% of the terms are in the SCC. This shows that not all nodes can reach (discover) every other node in the network. This also explains the findings of (Kozareva et al., 2008; Vyas et al., 2009) why starting with a good seed is important. 4.5 Path Lengths and Diameter Next, we describe the properties of the shortest paths between the semantic terms in the SCC. The distance between two nodes in the SCC is measured as the length of the shortest path connecting the terms. The direction of the edges between the terms is taken into consideration. The average distance is the average value of the shortest path lengths over all pairs of nodes in the SCC. The diameter of the SCC is calculated as the maximum distance over all pairs of nodes (u, v), such that a node v is rea</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics ACL-08: HLT, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Harold Somers</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation.</title>
<date>1993</date>
<contexts>
<context position="7432" citStr="Levin and Somers, 1993" startWordPosition="1147" endWordPosition="1150">s and Tenenbaum (Steyvers and Tenenbaum, 2004), who studied three manually built lexical networks (association norms, WordNet, and Roget’s Thesaurus (Roget, 1911)) and proposed a model of the growth of the semantic structure over time. These networks are limited to the semantic relations among nouns. In this paper we take a step further to explore the statistical properties of semantic networks relating proper names, nouns, verbs, and adjectives. Understanding the semantics of nouns, verbs, and adjectives has been of great interest to linguists and cognitive scientists such as (Gentner, 1981; Levin and Somers, 1993; Gasser and Smith, 1998). We implement a general harvesting procedure and show its results for these word types. A fundamental difference with the work of (Steyvers and Tenenbaum, 2004) is that we study very large semantic networks built ‘naturally’ by (millions of) users rather than ‘artificially’ by a small set of experts. The large networks capture the semantic intuitions and knowledge of the collective mass. It is conceivable that an analysis of this knowledge can begin to form the basis of a large-scale theory of semantic meaning and its interconnections, support observation of the proce</context>
</contexts>
<marker>Levin, Somers, 1993</marker>
<rawString>Beth Levin and Harold Somers. 1993. English verb classes and alternations: A preliminary investigation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun Li</author>
<author>David Alderson</author>
<author>Reiko Tanaka</author>
<author>John C Doyle</author>
<author>Walter Willinger</author>
</authors>
<date>2005</date>
<booktitle>Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications (Extended Version). Internet Mathematica,</booktitle>
<pages>2--4</pages>
<contexts>
<context position="2994" citStr="Li et al., 2005" startWordPosition="464" endWordPosition="467">or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 1616 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616–1625, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguis</context>
<context position="25865" citStr="Li et al., 2005" startWordPosition="4395" endWordPosition="4398">sis suggests the presence of a strong local cluster, however there are few possibilities to form overlapping neighborhoods of nodes. The clustering coefficient of WordNet (Steyvers and Tenenbaum, 2004) is similar to those of the text-mined networks. 4.7 Joint Degree Distribution In social networks, understanding the preferential attachment of nodes is important to identify the speed with which epidemics or gossips spread. Similarly, we are interested in understanding how the nodes of the semantic networks connect to each other. For this purpose, we examine the Joint Degree Distribution (JDD) (Li et al., 2005; Newman, 2003). JDD is approximated by the degree correlation function knn which maps the outdegree and the average 3A triad is three nodes that are connected by either two (open triad) or three (closed triad) directed ties. indegree of all nodes connected to a node with that outdegree. High values of knn indicate that high-degree nodes tend to connect to other highdegree nodes (forming a “core” in the network), while lower values of knn suggest that the highdegree nodes tend to connect to low-degree ones. Figure 4 shows the knn for the singer, whale, live in, cars, cantantes, and gente netwo</context>
</contexts>
<marker>Li, Alderson, Tanaka, Doyle, Willinger, 2005</marker>
<rawString>Lun Li, David Alderson, Reiko Tanaka, John C. Doyle, and Walter Willinger. 2005. Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications (Extended Version). Internet Mathematica, 2(4):431–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="1285" citStr="Lin and Pantel, 2002" startWordPosition="196" endWordPosition="199">inking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff an</context>
<context position="4956" citStr="Lin and Pantel, 2002" startWordPosition="761" endWordPosition="764"> performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, the Social Web, the network of scientific papers, among others. They have investigated the properties of these text-based networks with the objective of understanding their structure and applying this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ab</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>Dekang Lin and Patrick Pantel. 2002. Concept discovery from text. In Proc. of the 19th international conference on Computational linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E Newman</author>
<author>Michelle Girvan</author>
</authors>
<title>Finding and evaluating community structure in networks.</title>
<date>2004</date>
<journal>Physical Review,</journal>
<volume>69</volume>
<issue>2</issue>
<contexts>
<context position="5795" citStr="Newman and Girvan, 2004" startWordPosition="887" endWordPosition="891">tics have studied complex networks such as the World Wide Web, the Social Web, the network of scientific papers, among others. They have investigated the properties of these text-based networks with the objective of understanding their structure and applying this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ability to analyze networks, identify influential nodes, and discover hidden structures has led to important scientific and technological breakthroughs such as the discovery of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyper</context>
</contexts>
<marker>Newman, Girvan, 2004</marker>
<rawString>Mark E. Newman and Michelle Girvan. 2004. Finding and evaluating community structure in networks. Physical Review, 69(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Newman</author>
</authors>
<title>Mixing patterns in networks. Physical Review E,</title>
<date>2003</date>
<contexts>
<context position="25880" citStr="Newman, 2003" startWordPosition="4399" endWordPosition="4400">presence of a strong local cluster, however there are few possibilities to form overlapping neighborhoods of nodes. The clustering coefficient of WordNet (Steyvers and Tenenbaum, 2004) is similar to those of the text-mined networks. 4.7 Joint Degree Distribution In social networks, understanding the preferential attachment of nodes is important to identify the speed with which epidemics or gossips spread. Similarly, we are interested in understanding how the nodes of the semantic networks connect to each other. For this purpose, we examine the Joint Degree Distribution (JDD) (Li et al., 2005; Newman, 2003). JDD is approximated by the degree correlation function knn which maps the outdegree and the average 3A triad is three nodes that are connected by either two (open triad) or three (closed triad) directed ties. indegree of all nodes connected to a node with that outdegree. High values of knn indicate that high-degree nodes tend to connect to other highdegree nodes (forming a “core” in the network), while lower values of knn suggest that the highdegree nodes tend to connect to low-degree ones. Figure 4 shows the knn for the singer, whale, live in, cars, cantantes, and gente networks. The figure</context>
<context position="27389" citStr="Newman, 2003" startWordPosition="4656" endWordPosition="4657">ssortivity The property of the nodes to connect to other nodes with similar degrees can be captured through the assortivity coefficient r (Newman, 2003). The range of r is [−1, 1]. A positive assortivity coefficient means that the nodes tend to connect to nodes of similar degree, while negative coefficient means that nodes are likely to connect to nodes with degree very different from their own. We find that the assortivitiy coefficient of our semantic networks is positive, ranging from 0.07 to 0.20. In this respect, the semantic networks differ from the Web, which has a negative assortivity (Newman, 2003). This implies a difference in text mining and web search traversal strategies: since starting from a highly-connected seed term will tend to lead to other highly-connected terms, text mining algorithms should prefer depthfirst traversal, while web search algorithms starting 1622 Figure 4: Joint Degree Distribution of the Semantic Networks. from a highly-connected seed page should prefer a breadth-first strategy. 5 Discussion The above studies show that many of the properties discovered of the network formed by the web hold also for the networks induced by semantic relations in text mining app</context>
</contexts>
<marker>Newman, 2003</marker>
<rawString>Mark Newman. 2003. Mixing patterns in networks. Physical Review E, 67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<contexts>
<context position="2926" citStr="Page et al., 1999" startWordPosition="452" endWordPosition="455">more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 1616 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616–1625, Portland, Or</context>
<context position="6343" citStr="Page et al., 1999" startWordPosition="977" endWordPosition="980">ry of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However, no-one has studied the properties of the text-based semantic networks induced by semantic relations between terms with the objective of understanding their structure and applying this knowledge to improve concept discovery. Most relevant to this theme is the work of Steyvers and Tenenbaum (Steyvers and Tenenbaum, 2004), who studied three manually built lexical networks (association norms, WordNet, and Ro</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<pages>113--120</pages>
<contexts>
<context position="1401" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="214" endWordPosition="217">eb, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 200</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>137--145</pages>
<contexts>
<context position="1566" citStr="Pasca, 2004" startWordPosition="239" endWordPosition="240">to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvestin</context>
<context position="4696" citStr="Pasca, 2004" startWordPosition="724" endWordPosition="725"> for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, the Social Web, the network of scientific papers, among others</context>
<context position="9750" citStr="Pasca, 2004" startWordPosition="1541" endWordPosition="1542"> kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implemented a version of an existing one: the so-called DAP (doubly-anchored pattern) algorithm (Kozareva et al., 2008), because it (1) is easy to implement, (2) requires minimum input (one pattern and one seed example), (3) achieves very high precision compared to existing methods (Pasca, 2004; Etzioni et al., 2005; Pasca, 2007), (4) enriches existing semantic lexical repositories such as WordNet and Yago (Suchanek et al., 2007), (5) can be formulated to learn semantic lexicons and relations for noun, verb and verb+preposition syntactic constructions; (6) functions equally well in different languages. Next we describe the knowledge harvesting procedure and the construction of the text-mined semantic networks. 3.1 Harvesting to Induce Semantic Networks For a given semantic class of interest say singers, the algorithm starts with a seed example of the class, say Madonna. The seed ter</context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>Marius Pasca. 2004. Acquisition of categorized named entities for web search. In Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management, CIKM</booktitle>
<pages>683--690</pages>
<contexts>
<context position="9786" citStr="Pasca, 2007" startWordPosition="1547" endWordPosition="1548">, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implemented a version of an existing one: the so-called DAP (doubly-anchored pattern) algorithm (Kozareva et al., 2008), because it (1) is easy to implement, (2) requires minimum input (one pattern and one seed example), (3) achieves very high precision compared to existing methods (Pasca, 2004; Etzioni et al., 2005; Pasca, 2007), (4) enriches existing semantic lexical repositories such as WordNet and Yago (Suchanek et al., 2007), (5) can be formulated to learn semantic lexicons and relations for noun, verb and verb+preposition syntactic constructions; (6) functions equally well in different languages. Next we describe the knowledge harvesting procedure and the construction of the text-mined semantic networks. 3.1 Harvesting to Induce Semantic Networks For a given semantic class of interest say singers, the algorithm starts with a seed example of the class, say Madonna. The seed term is inserted in the lexicosyntactic</context>
<context position="12377" citStr="Pasca, 2007" startWordPosition="2002" endWordPosition="2003">r example, given the sentence (where the pattern is in italics and the extracted term is underlined) “He loves singers such as Madonna and Michael Jackson”, two nodes Madonna and Michael Jackson with an edge e=(Madonna, Michael Jackson) would be created in the graph G. Figure 1 shows a small example of the singer network. The starting seed term Madonna is shown in red color and the harvested terms are in blue. 3.2 Data We harvested data from the Web for a representative selection of semantic classes and relations, of 1618 Figure 1: Harvesting Procedure. the type used in (Etzioni et al., 2005; Pasca, 2007; Kozareva and Hovy, 2010a): • semantic classes that can be learned using different seeds (e.g., “singers such as Madonna and *” and “singers such as Placido Domingo and *”); • semantic classes that are expressed through different lexico-syntactic patterns (e.g., “weapons such as bombs and *” and “weapons including bombs and *”); • verbs and adjectives characterizing the semantic class (e.g., “expensive and * car”, “dogs run and *”); • semantic relations with more complex lexicosyntactic structure (e.g., “* and Easyjet fly to”, “* and Sam live in”); • semantic classes that are obtained in diff</context>
</contexts>
<marker>Pasca, 2007</marker>
<rawString>Marius Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management, CIKM 2007, pages 683– 690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno R Preiss</author>
</authors>
<title>Data structures and algorithms with object-oriented design patterns in C++.</title>
<date>1999</date>
<contexts>
<context position="19401" citStr="Preiss, 1999" startWordPosition="3177" endWordPosition="3178">butions (Broder et al., 2000). The difference in the distributions can be explained by the link asymmetry of semantic terms: A discovering B does not necessarily mean that B will discover A. In the text-mined semantic networks, this asymmetry is caused by patterns of language use, such as the fact that people use first adjectives of the size and then of the color (e.g., big red car), or prefer to place male before female proper names. Harvesting patterns should take into account this tendency. 4.3 Sparsity Another relevant property of the semantic networks concerns sparsity. Following Preiss (Preiss, 1999), a graph is sparse if |E |= O(|V |k) and 1 &lt; k &lt; 2, where |E |is the number of edges and |V |is the number of nodes, otherwise the graph is dense. For the studied text-semantic networks, k is ≈ 1.08. Sparsity can be also captured through the density of the semantic network which is computed as V (El 11).All networks have low density which suggests that the networks exhibit a sparse connectivity pattern. On average a node (semantic term) is connected to a very small percentage of other nodes. Similar behavior was reported for the WordNet and Roget’s semantic networks (Steyvers and Tenenbaum, 2</context>
</contexts>
<marker>Preiss, 1999</marker>
<rawString>Bruno R. Preiss. 1999. Data structures and algorithms with object-oriented design patterns in C++.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filippo Radicchi</author>
<author>Santo Fortunato</author>
<author>Benjamin Markines</author>
<author>Alessandro Vespignani</author>
</authors>
<title>Diffusion of scientific credits and the ranking of scientists.</title>
<date>2009</date>
<journal>In Phys. Rev. E</journal>
<volume>80</volume>
<pages>056103</pages>
<contexts>
<context position="5935" citStr="Radicchi et al., 2009" startWordPosition="910" endWordPosition="913">tigated the properties of these text-based networks with the objective of understanding their structure and applying this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ability to analyze networks, identify influential nodes, and discover hidden structures has led to important scientific and technological breakthroughs such as the discovery of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However,</context>
</contexts>
<marker>Radicchi, Fortunato, Markines, Vespignani, 2009</marker>
<rawString>Filippo Radicchi, Santo Fortunato, Benjamin Markines, and Alessandro Vespignani. 2009. Diffusion of scientific credits and the ranking of scientists. In Phys. Rev. E 80, 056103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepack Ravichandran</author>
<author>Eduard H Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<pages>41--47</pages>
<contexts>
<context position="2003" citStr="Ravichandran and Hovy, 2002" startWordPosition="300" endWordPosition="304">l and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling </context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepack Ravichandran and Eduard H. Hovy. 2002. Learning surface text patterns for a question answering system. pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Empirical Methods for Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="1902" citStr="Riloff and Shepherd, 1997" startWordPosition="287" endWordPosition="290">el, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different </context>
<context position="4663" citStr="Riloff and Shepherd, 1997" startWordPosition="716" endWordPosition="719">tatistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, the Social Web, the network o</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A corpus-based approach for building semantic lexicons. In Proceedings of the Empirical Methods for Natural Language Processing, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<pages>811--816</pages>
<contexts>
<context position="4402" citStr="Riloff, 1993" startWordPosition="680" endWordPosition="681"> some related work. In Section 3 we describe the general harvesting procedure, and follow with an examination of the various statistical properties of implicit semantic networks in Section 4, using our implemented harvester to provide illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes an</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. pages 811–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Mark Roget</author>
</authors>
<title>Roget’s thesaurus of English Words and Phrases.</title>
<date>1911</date>
<location>New York</location>
<note>Crowell company.</note>
<contexts>
<context position="6972" citStr="Roget, 1911" startWordPosition="1073" endWordPosition="1075">analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However, no-one has studied the properties of the text-based semantic networks induced by semantic relations between terms with the objective of understanding their structure and applying this knowledge to improve concept discovery. Most relevant to this theme is the work of Steyvers and Tenenbaum (Steyvers and Tenenbaum, 2004), who studied three manually built lexical networks (association norms, WordNet, and Roget’s Thesaurus (Roget, 1911)) and proposed a model of the growth of the semantic structure over time. These networks are limited to the semantic relations among nouns. In this paper we take a step further to explore the statistical properties of semantic networks relating proper names, nouns, verbs, and adjectives. Understanding the semantics of nouns, verbs, and adjectives has been of great interest to linguists and cognitive scientists such as (Gentner, 1981; Levin and Somers, 1993; Gasser and Smith, 1998). We implement a general harvesting procedure and show its results for these word types. A fundamental difference w</context>
</contexts>
<marker>Roget, 1911</marker>
<rawString>Peter Mark Roget. 1911. Roget’s thesaurus of English Words and Phrases. New York Thomas Y. Crowell company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gert Sabidussi</author>
</authors>
<title>The centrality index of a graph.</title>
<date>1966</date>
<journal>Psychometrika,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="2791" citStr="Sabidussi, 1966" startWordPosition="430" endWordPosition="431">e is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and </context>
<context position="14874" citStr="Sabidussi, 1966" startWordPosition="2423" endWordPosition="2424"> *” 3894 18545 P13=“* and Easyjet fly to” 3290 6480 P14=“* and Charlie work for” 2125 3494 P15=“* and Sam live in” 6745 24348 P16=“cantantes como Madonna y *” 240 318 P17=“gente como Jorge y *” 572 701 Table 1: Size of the Semantic Networks. 4 Statistical Properties of Text-Mined Semantic Networks In this section we apply a range of relevant measures from the network analysis community to the networks described above. 4.1 Centrality The first statistical property we explore is centrality. It measures the degree to which the network structure determines the importance of a node in the network (Sabidussi, 1966; Freeman, 1979). We explore the effect of two centrality measures: indegree and outdegree. The indegree of a node u denoted as indegree(u)=E(v, u) considers the sum of all incoming edges to u and captures the ability of a semantic term to be discovered by other semantic terms. The outdegree of a node u denoted as outdegree(u)=E(u, v) considers the number of outgoing edges of the node u and measures the ability of a semantic term to discover new terms. Intuitively, the more central the node u is, the more confident we are that it is a correct term. Since harvesting algorithms are notorious for</context>
</contexts>
<marker>Sabidussi, 1966</marker>
<rawString>Gert Sabidussi. 1966. The centrality index of a graph. Psychometrika, 31(4):581–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sayyadi</author>
<author>Lise Getoor</author>
</authors>
<title>Future rank: Ranking scientific articles by predicting their future pagerank. In</title>
<date>2009</date>
<booktitle>SIAM International Conference on Data Mining (SDM09).</booktitle>
<contexts>
<context position="6052" citStr="Sayyadi and Getoor, 2009" startWordPosition="930" endWordPosition="933">ng this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ability to analyze networks, identify influential nodes, and discover hidden structures has led to important scientific and technological breakthroughs such as the discovery of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However, no-one has studied the properties of the text-based semantic networks induced by semantic relations between terms wi</context>
</contexts>
<marker>Sayyadi, Getoor, 2009</marker>
<rawString>Hassan Sayyadi and Lise Getoor. 2009. Future rank: Ranking scientific articles by predicting their future pagerank. In 2009 SIAM International Conference on Data Mining (SDM09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="13566" citStr="Schmid, 1994" startWordPosition="2201" endWordPosition="2202">hat are obtained in different languages, such as English and Spanish (e.g., “singers such as Madonna and *” and “cantantes como Madonna y *”); While most of these variations have been explored in individual papers, we have found no paper that covers them all, and none whatsoever that uses verbs and adjectives as seeds. Using the above procedure to generate the data, each pattern was submitted as a query to Yahoo!Boss. For each query the top 1000 text snippets were retrieved. The algorithm ran until exhaustion. In total, we collected 10GB of data which was partof-speech tagged with Treetagger (Schmid, 1994) and used for the semantic term extraction. Table 1 summarizes the number of nodes and edges learned for each semantic network using pattern Pi and the initial seed shown in italics. Lexico-Syntactic Pattern Nodes Edges P1=“singers such as Madonna and *” 1115 1942 P2=“singers such as Placido Domingo and *” 815 1114 P3=“emotions including anger and *” 113 250 P4=“emotions such as anger and *” 748 2547 P5=“diseases such as malaria and *” 3168 6752 P6=“drugs such as ibuprofen and *” 2513 9428 P7=“expensive and * cars” 4734 22089 Ps=“* and tasty fruits” 1980 7874 P9=“whales swim and *” 869 2163 P1</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<pages>1297--1304</pages>
<contexts>
<context position="2076" citStr="Snow et al., 2005" startWordPosition="312" endWordPosition="315">ge collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The co</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. pages 1297–1304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Claire Cardie</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning information extraction rules for semi-structured and free text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1508" citStr="Soderland et al., 1999" startWordPosition="228" endWordPosition="231">guistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation</context>
</contexts>
<marker>Soderland, Cardie, Mooney, 1999</marker>
<rawString>Stephen Soderland, Claire Cardie, and Raymond Mooney. 1999. Learning information extraction rules for semi-structured and free text. Machine Learning, 34(1-3), pages 233–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth.</title>
<date>2004</date>
<journal>Cognitive Science,</journal>
<pages>29--41</pages>
<contexts>
<context position="6856" citStr="Steyvers and Tenenbaum, 2004" startWordPosition="1054" endWordPosition="1057"> hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However, no-one has studied the properties of the text-based semantic networks induced by semantic relations between terms with the objective of understanding their structure and applying this knowledge to improve concept discovery. Most relevant to this theme is the work of Steyvers and Tenenbaum (Steyvers and Tenenbaum, 2004), who studied three manually built lexical networks (association norms, WordNet, and Roget’s Thesaurus (Roget, 1911)) and proposed a model of the growth of the semantic structure over time. These networks are limited to the semantic relations among nouns. In this paper we take a step further to explore the statistical properties of semantic networks relating proper names, nouns, verbs, and adjectives. Understanding the semantics of nouns, verbs, and adjectives has been of great interest to linguists and cognitive scientists such as (Gentner, 1981; Levin and Somers, 1993; Gasser and Smith, 1998</context>
<context position="20005" citStr="Steyvers and Tenenbaum, 2004" startWordPosition="3286" endWordPosition="3289">ing Preiss (Preiss, 1999), a graph is sparse if |E |= O(|V |k) and 1 &lt; k &lt; 2, where |E |is the number of edges and |V |is the number of nodes, otherwise the graph is dense. For the studied text-semantic networks, k is ≈ 1.08. Sparsity can be also captured through the density of the semantic network which is computed as V (El 11).All networks have low density which suggests that the networks exhibit a sparse connectivity pattern. On average a node (semantic term) is connected to a very small percentage of other nodes. Similar behavior was reported for the WordNet and Roget’s semantic networks (Steyvers and Tenenbaum, 2004). 1620 Figure 2: Degree Distributions of Semantic Networks. 4.4 Connectedness For every network, we computed the strongly connected component (SCC) such that for all nodes (semantic terms) in the SCC, there is a path from any node to another node in the SCC considering the direction of the edges between the nodes. For each network, we found that there is only one SCC. The size of the component is shown in Table 5. Unlike WordNet and Roget’s semantic networks where the SCC consists 96% of all semantic terms, in the text-mined semantic networks only 12 to 55% of the terms are in the SCC. This sh</context>
<context position="25451" citStr="Steyvers and Tenenbaum, 2004" startWordPosition="4328" endWordPosition="4331">hole semantic network is the average clustering coefficient of all its nodes, C= 1n E Ci. The value of the clustering coefficient ranges between [0, 1], where 0 indicates that the nodes do not have neighbors which are themselves connected, while 1 indicates that all nodes are connected. Table 6 shows the clustering coefficient for all text-mined semantic networks together with the number of closed and open triads3. The analysis suggests the presence of a strong local cluster, however there are few possibilities to form overlapping neighborhoods of nodes. The clustering coefficient of WordNet (Steyvers and Tenenbaum, 2004) is similar to those of the text-mined networks. 4.7 Joint Degree Distribution In social networks, understanding the preferential attachment of nodes is important to identify the speed with which epidemics or gossips spread. Similarly, we are interested in understanding how the nodes of the semantic networks connect to each other. For this purpose, we examine the Joint Degree Distribution (JDD) (Li et al., 2005; Newman, 2003). JDD is approximated by the degree correlation function knn which maps the outdegree and the average 3A triad is three nodes that are connected by either two (open triad)</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2004</marker>
<rawString>Mark Steyvers and Joshua B. Tenenbaum. 2004. The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. Cognitive Science, 29:41–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In WWW ’07: Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="4533" citStr="Suchanek et al., 2007" startWordPosition="698" endWordPosition="701">us statistical properties of implicit semantic networks in Section 4, using our implemented harvester to provide illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Res</context>
<context position="9888" citStr="Suchanek et al., 2007" startWordPosition="1561" endWordPosition="1564">vesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implemented a version of an existing one: the so-called DAP (doubly-anchored pattern) algorithm (Kozareva et al., 2008), because it (1) is easy to implement, (2) requires minimum input (one pattern and one seed example), (3) achieves very high precision compared to existing methods (Pasca, 2004; Etzioni et al., 2005; Pasca, 2007), (4) enriches existing semantic lexical repositories such as WordNet and Yago (Suchanek et al., 2007), (5) can be formulated to learn semantic lexicons and relations for noun, verb and verb+preposition syntactic constructions; (6) functions equally well in different languages. Next we describe the knowledge harvesting procedure and the construction of the text-mined semantic networks. 3.1 Harvesting to Induce Semantic Networks For a given semantic class of interest say singers, the algorithm starts with a seed example of the class, say Madonna. The seed term is inserted in the lexicosyntactic pattern “class such as seed and *”, which learns on the position of the * new terms of type class. Th</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Fernando Pereira</author>
</authors>
<title>Graph-based weakly-supervised methods for information extraction and integration.</title>
<date>2010</date>
<pages>1473--1481</pages>
<contexts>
<context position="3071" citStr="Talukdar and Pereira, 2010" startWordPosition="476" endWordPosition="479">elations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 1616 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616–1625, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics rithms have the limitations and thresholds that are empirically found (o</context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>Partha Pratim Talukdar and Fernando Pereira. 2010. Graph-based weakly-supervised methods for information extraction and integration. pages 1473–1481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vishnu Vyas</author>
</authors>
<title>Patrick Pantel, and Eric Crestan.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM,</booktitle>
<pages>225--234</pages>
<marker>Vyas, 2009</marker>
<rawString>Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009. Helping editors choose better seed sets for entity set expansion. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM, pages 225–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dylan Walker</author>
<author>Huafeng Xie</author>
<author>Koon-Kiu Yan</author>
<author>Sergei Maslov</author>
</authors>
<title>Ranking scientific publications using a simple model of network traffic.</title>
<date>2006</date>
<contexts>
<context position="6006" citStr="Walker et al., 2006" startWordPosition="922" endWordPosition="925">understanding their structure and applying this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. In particular, the ability to analyze networks, identify influential nodes, and discover hidden structures has led to important scientific and technological breakthroughs such as the discovery of communities of like-minded individuals (Newman and Girvan, 2004), the identification of influential people (Kempe et al., 2003), the ranking of scientists by their citation indexes (Radicchi et al., 2009), and the discovery of important scientific papers (Walker et al., 2006; Chen et al., 2007; Sayyadi and Getoor, 2009). Broder et al. (2000) demonstrated that the Web link structure has a “bow-tie” shape, while (2001) classified Web pages into authorities (pages with relevant information) and hubs (pages with useful references). These findings resulted in the development of the PageRank (Page et al., 1999) algorithm which analyzes the structure of the hyperlinks of Web documents to find pages with authoritative information. PageRank has revolutionized the whole Internet search society. However, no-one has studied the properties of the text-based semantic networks </context>
</contexts>
<marker>Walker, Xie, Yan, Maslov, 2006</marker>
<rawString>Dylan Walker, Huafeng Xie, Koon-Kiu Yan, and Sergei Maslov. 2006. Ranking scientific publications using a simple model of network traffic. December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duncan Watts</author>
<author>Steven Strogatz</author>
</authors>
<title>Collective dynamics of ’small-world’ networks.</title>
<date>1998</date>
<journal>Nature,</journal>
<volume>393</volume>
<issue>6684</issue>
<contexts>
<context position="2833" citStr="Watts and Strogatz, 1998" startWordPosition="434" endWordPosition="437">erms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al., 1999; Broder et al., 2000; Kleinberg and Lawrence, 2001; Li et al., 2005; Clauset et al., 2009). Surprisingly, except in (Talukdar and Pereira, 2010), this work has not yet been related to text mining research in the computational linguistics community. The work is, however, relevant in at least two ways. It sometimes explains why text mining algo1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. 1616 Proceedings of the 49th A</context>
<context position="22761" citStr="Watts and Strogatz, 1998" startWordPosition="3793" endWordPosition="3796">ry short average path lengths and small diameters that are consistent with Watt’s finding for small-world networks. Therefore, the yield of harvesting seeds can be predicted within five steps explaining (Kozareva and Hovy, 2010b; Vyas et al., 2009). We also compute for any randomly selected node in the semantic network on average how many hops (steps) are necessary to reach from one node to another. Figure 3 shows the obtained results for some of the studied semantic networks. 4.6 Clustering The clustering coefficient (C) is another measure to study the connectivity structure of the networks (Watts and Strogatz, 1998). This measure captures the probability that the two neighbors of a randomly selected node will be neighbors. The clustering coefficient of a node u is calculated as Cu= ����� ki(ki�1) 0 20 40 60 80 100 120 Outdegree 0 10 20 30 40 50 60 Indegree 500 450 400 350 300 250 200 150 100 50 0 1 2 3 4 5 6 7 8 Indegree 0 5 10 15 20 25 30 35 Outdegree &apos;gente&apos; power-law exponent=1.20 0 2 4 6 8 10 12 14 Outdegree Number of Nodes Number of Nodes 500 450 400 350 300 250 200 150 100 50 0 &apos;emotions&apos; power-law exponent=2.28 Number of Nodes 120 100 80 60 40 20 0 &apos;emotions&apos; power-law exponent=1.18 0 10 20 30 40 </context>
</contexts>
<marker>Watts, Strogatz, 1998</marker>
<rawString>Duncan Watts and Steven Strogatz. 1998. Collective dynamics of ’small-world’ networks. Nature, 393(6684):440–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Discovering asymmetric entailment relations between verbs using selectional preferences.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="4446" citStr="Zanzotto et al., 2006" startWordPosition="684" endWordPosition="688"> describe the general harvesting procedure, and follow with an examination of the various statistical properties of implicit semantic networks in Section 4, using our implemented harvester to provide illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web doc</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Maria Teresa Pazienza. 2006. Discovering asymmetric entailment relations between verbs using selectional preferences. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
<author>K Jaz</author>
<author>Thomas Hofmann</author>
<author>Tomaso Poggio</author>
<author>John Shawe-taylor</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<contexts>
<context position="2140" citStr="Zelenko et al., 2003" startWordPosition="323" endWordPosition="326">t al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1. Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. The computational properties of large networks and large network trave</context>
</contexts>
<marker>Zelenko, Aone, Richardella, Jaz, Hofmann, Poggio, Shawe-taylor, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, Anthony Richardella, Jaz K, Thomas Hofmann, Tomaso Poggio, and John Shawe-taylor. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research 3.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>