<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000661">
<title confidence="0.935785">
Sentiment Analysis of Citations using Sentence Structure-Based Features
</title>
<author confidence="0.970945">
Awais Athar
</author>
<affiliation confidence="0.975368">
University of Cambridge
Computer Laboratory
</affiliation>
<address confidence="0.993839">
15 JJ Thompson Avenue
Cambridge, CB3 0FD, U.K.
</address>
<email confidence="0.999296">
awais.athar@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995687" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998912235294118">
Sentiment analysis of citations in scientific pa-
pers and articles is a new and interesting prob-
lem due to the many linguistic differences be-
tween scientific texts and other genres. In
this paper, we focus on the problem of auto-
matic identification of positive and negative
sentiment polarity in citations to scientific pa-
pers. Using a newly constructed annotated ci-
tation sentiment corpus, we explore the effec-
tiveness of existing and novel features, includ-
ing n-grams, specialised science-specific lex-
ical features, dependency relations, sentence
splitting and negation features. Our results
show that 3-grams and dependencies perform
best in this task; they outperform the sentence
splitting, science lexicon and negation based
features.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997569642857143">
Sentiment analysis is the task of identifying positive
and negative opinions, sentiments, emotions and at-
titudes expressed in text. Although there has been in
the past few years a growing interest in this field for
different text genres such as newspaper text, reviews
and narrative text, relatively less emphasis has been
placed on extraction of opinions from scientific liter-
ature, more specifically, citations. Analysis of cita-
tion sentiment would open up many exciting new ap-
plications in bibliographic search and in bibliomet-
rics, i.e., the automatic evaluation the influence and
impact of individuals and journals via citations.
Existing bibliometric measures like H-Index
(Hirsch, 2005) and adapted graph ranking algo-
</bodyText>
<page confidence="0.979007">
81
</page>
<bodyText confidence="0.999510121212121">
rithms like PageRank (Radev et al., 2009) treat all ci-
tations as equal. However, Bonzi (1982) argued that
if a cited work is criticised, it should consequently
carry lower or even negative weight for bibliometric
measures. Automatic citation sentiment detection is
a prerequisite for such a treatment.
Moreover, citation sentiment detection can also
help researchers during search, by detecting prob-
lems with a particular approach. It can be used as
a first step to scientific summarisation, enable users
to recognise unaddressed issues and possible gaps
in the current research, and thus help them set their
research directions.
For other genres a rich literature on sentiment de-
tection exists and researchers have used a number
of features such as n-grams, presence of adjectives,
adverbs and other parts-of-speech (POS), negation,
grammatical and dependency relations as well as
specialised lexicons in order to detect sentiments
from phrases, words, sentences and documents.
State-of-the-art systems report around 85-90% ac-
curacy for different genres of text (Nakagawa et al.,
2010; Yessenalina et al., 2010; T¨ackstr¨om and Mc-
Donald, 2011).
Given such good results, one might think that a
sentence-based sentiment detection system trained
on a different genre could be used equally well to
classify citations. We argue that this might not be
the case; our citation sentiment recogniser uses spe-
cialised training data and tests the performance of
specialised features against current state-of-the-art
features. The reasons for this are based on the fol-
lowing observations:
</bodyText>
<listItem confidence="0.998591">
• Sentiment in citations is often hidden. This might
</listItem>
<subsubsectionHeader confidence="0.470393">
Proceedings of the ACL-HLT 2011 Student Session, pages 81–87,
</subsubsectionHeader>
<bodyText confidence="0.9723795">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
be because of the general strategy to avoid overt
criticism due to the sociological aspect of cit-
ing (MacRoberts and MacRoberts, 1984; Thomp-
son and Yiyun, 1991). Ziman (1968) states that
many works are cited out of “politeness, policy or
piety”. Negative sentiment, while still present and
detectable for humans, is expressed in subtle ways
and might be hedged, especially when it cannot be
quantitatively justified (Hyland, 1995).
</bodyText>
<tableCaption confidence="0.401138">
While SCL has been successfully applied to POS tag-
ging and Sentiment Analysis (Blitzer et al., 2006), its
effectiveness for parsing was rather unexplored.
</tableCaption>
<bodyText confidence="0.923524777777778">
• Citation sentences are often neutral with respect
to sentiment, either because they describe an al-
gorithm, approach or methodology objectively, or
because they are used to support a fact or state-
ment.
There are five different IBM translation models (Brown
et al. , 1993).
This gives rise to a far higher proportion of objec-
tive sentences than in other genres.
</bodyText>
<listItem confidence="0.9819732">
• Negative polarity is often expressed in contrastive
terms, e.g. in evaluation sections. Although the
sentiment is indirect in these cases, its negativity
is implied by the fact that the authors’ own work
is clearly evaluated positively in comparison.
</listItem>
<bodyText confidence="0.946705">
This method was shown to outperform the class based
model proposed in (Brown et al., 1992) ...
</bodyText>
<listItem confidence="0.8142305">
• There is also much variation between scientific
texts and other genres concerning the lexical
items chosen to convey sentiment. Sentiment car-
rying science-specific terms exist and are rela-
tively frequent, which motivates the use of a sen-
timent lexicon specialised to science.
Similarity-based smoothing (Dagan, Lee, and Pereira
1999) provides an intuitively appealing approach to
language modeling.
• Technical terms play a large role overall in scien-
tific text (Justeson and Katz, 1995). Some of these
carry sentiment as well.
</listItem>
<bodyText confidence="0.80281725">
Current state of the art machine translation systems
(Och, 2003) use phrasal (n-gram) features ...
For this reason, using higher order n-grams might
prove to be useful in sentiment detection.
</bodyText>
<listItem confidence="0.978753333333333">
• The scope of influence of citations varies widely
from a single clause (as in the example below) to
several paragraphs:
</listItem>
<bodyText confidence="0.96271825">
As reported in Table 3, small increases in METEOR
(Banerjee and Lavie, 2005), BLEU (Papineni et al.,
2002) and NIST scores (Doddington, 2002) suggest
that...
This affects lexical features directly since there
could be “sentiment overlap” associated with
neighbouring citations. Ritchie et al. (2008)
showed that assuming larger citation scopes has
a positive effect in retrieval. We will test the op-
posite direction here, i.e., we assume short scopes
and use a parser to split sentences, so that the fea-
tures associated with the clauses not directly con-
nected to the citation are disregarded.
We created a new sentiment-annotated corpus of
scientific text in the form of a sentence-based col-
lection of over 8700 citations. Our experiments
use a supervised classifier with the state-of-the-art
features from the literature, as well as new fea-
tures based on the observations above. Our results
show that the most successful feature combination
includes dependency features and n-grams longer
than for other genres (n = 3), but the assumption
of a smaller scope (sentence splitting) decreased re-
sults.
2 Training and Test Corpus
We manually annotated 8736 citations from 310 re-
search papers taken from the ACL Anthology (Bird
et al., 2008). The citation summary data from the
ACL Anthology Network1 (Radev et al., 2009) was
used. We identified the actual text of the citations
by regular expressions and replaced it with a special
token &lt;CIT&gt; in order to remove any lexical bias
associated with proper names of researchers. We la-
belled each sentence as positive, negative or objec-
tive, and separated 1472 citations for development
and training. The rest were used as the test set con-
taining 244 negative, 743 positive and 6277 objec-
tive citations. Thus our dataset is heavily skewed,
with subjective citations accounting for only around
14% of the corpus.
</bodyText>
<footnote confidence="0.996826">
1http://www.aclweb.org
</footnote>
<page confidence="0.999078">
82
</page>
<sectionHeader confidence="0.999145" genericHeader="introduction">
3 Features
</sectionHeader>
<bodyText confidence="0.999568875">
We represent each citation as a feature set in a Sup-
port Vector Machine (SVM) (Cortes and Vapnik,
1995) framework which has been shown to produce
good results for sentiment classification (Pang et
al., 2002). The corpus is processed using WEKA
(Hall et al., 2008) and the Weka LibSVM library
(EL-Manzalawy and Honavar, 2005; Chang and Lin,
2001) with the following features.
</bodyText>
<subsectionHeader confidence="0.999627">
3.1 Word Level Features
</subsectionHeader>
<bodyText confidence="0.999996235294118">
In accordance with Pang et al. (2002), we use uni-
grams and bigrams as features and also add 3-grams
as new features to capture longer technical terms.
POS tags are also included using two approaches:
attaching the tag to the word by a delimiter, and ap-
pending all tags at the end of the sentence. This may
help in distinguishing between homonyms with dif-
ferent POS tags and signalling the presence of ad-
jectives (e.g., JJ) respectively. Name of the primary
author of the cited paper is also used as a feature.
A science-specific sentiment lexicon is also added
to the feature set. This lexicon consists of 83 polar
phrases which have been manually extracted from
the development set of 736 citations. Some of the
most frequently occurring polar phrases in this set
consists of adjectives such as efficient, popular, suc-
cessful, state-of-the-art and effective.
</bodyText>
<subsectionHeader confidence="0.99976">
3.2 Contextual Polarity Features
</subsectionHeader>
<bodyText confidence="0.999954">
Features previously found to be useful for detect-
ing phrase-level contextual polarity (Wilson et al.,
2009) are also included. Since the task at hand is
sentence-based, we use only the sentence-based fea-
tures from the literature e.g., presence of subjectiv-
ity clues which have been compiled from several
sources2 along with the number of adjectives, ad-
verbs, pronouns, modals and cardinals.
To handle negation, we include the count of nega-
tion phrases found within the citation sentence. Sim-
ilarly, the number of valance shifters (Polanyi and
Zaenen, 2006) in the sentence are also used. The
polarity shifter and negation phrase lists have been
taken from the OpinionFinder system (Wilson et al.,
2005).
</bodyText>
<footnote confidence="0.768651">
2Available for download at http://www.cs.pitt.edu/mpqa/
</footnote>
<subsectionHeader confidence="0.999195">
3.3 Sentence Structure Based Features
</subsectionHeader>
<bodyText confidence="0.99999275">
We explore three different feature sets which focus
on the lexical and grammatical structure of a sen-
tence and have not been explored previously for the
task of sentiment analysis of scientific text.
</bodyText>
<subsectionHeader confidence="0.857452">
3.3.1 Dependency Structures
</subsectionHeader>
<bodyText confidence="0.989387066666666">
The first set of these features include typed depen-
dency structures (de Marneffe and Manning, 2008)
which describe the grammatical relationships be-
tween words. We aim to capture the long distance
relationships between words. For instance in the
sentence below, the relationship between results and
competitive will be missed by trigrams but the de-
pendency representation captures it in a single fea-
ture nsubj competitive results.
&lt;CIT&gt; showed that the results for French-English
were competitive to state-of-the-art alignment systems.
A variation we experimented with, but gave up
on as it did not show any improvements, concerns
backing-off the dependent and governor to their POS
tags (Joshi and Penstein-Ros´e, 2009).
</bodyText>
<subsectionHeader confidence="0.967732">
3.3.2 Sentence Splitting
</subsectionHeader>
<bodyText confidence="0.999114">
Removing irrelevant polar phrases around a ci-
tation might improve results. For this purpose, we
split each sentence by trimming its parse tree. Walk-
ing from the citation node (&lt;CIT&gt;) towards the
root, we select the subtree rooted at the first sentence
node (S) and ignore the rest. For example, in Figure
1, the cited paper is not included in the scope of the
discarded polar phrase significant improvements.
</bodyText>
<figureCaption confidence="0.994454">
Figure 1: An example of parse tree trimming
</figureCaption>
<page confidence="0.993385">
83
</page>
<sectionHeader confidence="0.478698" genericHeader="method">
3.3.3 Negation
</sectionHeader>
<bodyText confidence="0.9548615625">
Dependencies and parse trees attach negation
nodes, such as not, to the clause subtree and this
shows no interaction with other nodes with respect
to valence shifting. To handle this effect, we take
a simple window-based inversion approach. All
words inside a k-word window of any negation term
are suffixed with a token neg to distinguish them
from their non-polar versions. For example, a 2-
word negation window inverts the polarity of the
positive phrase work well in the sentence below.
Turney’s method did not work neg well neg although
they reported 80% accuracy in &lt;CIT&gt;.
The negation term list has been taken from the
OpinionFinder system. Khan (2007) has shown that
this approach produces results comparable to gram-
matical relations based negation models.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.997078">
Because of our skewed dataset, we report both
the macro-F and the micro-F scores using 10-fold
cross-validation (Lewis, 1991). The bold values in
Table 1 show the best results.
</bodyText>
<table confidence="0.999425923076923">
Features macro-F micro-F
1 grams 0.581 0.863
1-2 grams 0.592 0.864
1-3 grams 0.597 0.862
&amp;quot; + POS 0.535 0.859
&amp;quot; + POS (tokenised) 0.596 0.859
&amp;quot; + scilex 0.597 0.860
&amp;quot; + wlev 0.535 0.859
&amp;quot; + cpol 0.418 0.859
&amp;quot; + dep 0.760 0.897
&amp;quot; + dep + split + neg 0.683 0.872
&amp;quot; + dep + split 0.642 0.866
&amp;quot; + dep + neg 0.764 0.898
</table>
<tableCaption confidence="0.951382333333333">
Table 1: Results using science lexicon (scilex), contex-
tual polarity (cpol), dependencies (dep), negation (neg),
sentence splitting (split) and word-level (wlev) features.
</tableCaption>
<bodyText confidence="0.999980117647059">
The selection of the features is on the basis of im-
provements over a baseline of 1-3 grams i.e. if a
feature (e.g. scilex) did not shown any improvement,
it is has been excluded from the subsequent experi-
ments.
The results show that contextual polarity features
do not work well on citation text. Adding a science-
specific lexicon does not help either. This may indi-
cate that n-grams are sufficient to capture discrim-
inating lexical structures. We find that word level
and contextual polarity features are surpassed by de-
pendency features. Sentence splitting does not help,
possibly due to longer citation scope. Adding a
negation window (k=15) improves the performance
but the improvement was not found to be statistically
significant. This might be due to skewed class dis-
tribution and a larger dataset may prove to be useful.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9975069">
While different schemes have been proposed for
annotating citations according to their function
(Spiegel-R¨osing, 1977; Nanba and Okumura, 1999;
Garzone and Mercer, 2000), there have been no at-
tempts on citation sentiment detection in a large cor-
pus.
Teufel et al. (2006) worked on a 2829 sentence ci-
tation corpus using a 12-class classification scheme.
However, this corpus has been annotated for the task
of determining the author’s reason for citing a given
paper and is thus built on top of sentiment of cita-
tion. It considers usage, modification and similar-
ity with a cited paper as positive even when there is
no sentiment attributed to it. Moreover, contrast be-
tween two cited methods (CoCoXY) is categorized
as objective in the annotation scheme even if the text
indicates that one method performs better than the
other. For example, the sentence below talks about
a positive attribute but is marked as neutral in the
scheme.
Lexical transducers are more efficient for analysis and
generation than the classical two-level systems (Kosken-
niemi,1983) because ...
Using this corpus is thus more likely to lead to
inconsistent representation of sentiment in any sys-
tem which relies on lexical features. Teufel et al.
(2006) group the 12 categories into 3 in an at-
tempt to perform a rough approximation of senti-
ment analysis over the classifications and report a
0.710 macro-F score. Unfortunately, we have ac-
</bodyText>
<page confidence="0.996107">
84
</page>
<bodyText confidence="0.999941">
cess to only a subset3 of this citation function cor-
pus. We have extracted 1-3 grams, dependencies and
negation features from the reduced citation function
dataset and used them in our system with 10-fold
cross-validation. This results in an improved macro-
F score of 0.797 for the subset. This shows that
our system is comparable to Teufel et al. (2006).
When this subset is used to test the system trained on
our newly annotated corpus, a low macro-F score of
0.484 is achieved. This indicates that there is a mis-
match in the annotated class labels. Therefore, we
can infer that citation sentiment classification is dif-
ferent from citation function classification.
Other approaches to citation annotation and clas-
sification include Wilbur et al. (2006) who annotated
a small 101 sentence corpus on focus, polarity, cer-
tainty, evidence and directionality. Piao et al. (2007)
proposed a system to attach sentiment information
to the citation links between biomedical papers.
Different dependency relations have been ex-
plored by Dave et al. (2003), Wilson et al. (2004)
and Ng et al. (2006) for sentiment detection. Nak-
agawa et al. (2010) report that using dependencies
on conditional random fields with lexicon based po-
larity reversal results in improvements over n-grams
for news and reviews corpora.
A common approach is to use a sentiment la-
belled lexicon to score sentences (Hatzivassiloglou
and McKeown, 1997; Turney, 2002; Yu and Hatzi-
vassiloglou, 2003). Research suggests that creating
a general sentiment classifier is a difficult task and
existing approaches are highly topic dependent (En-
gstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al.,
2007).
</bodyText>
<sectionHeader confidence="0.995039" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999827">
In this paper, we focus on automatic identification
of sentiment polarity in citations. Using a newly
constructed annotated citation sentiment corpus, we
examine the effectiveness of existing and novel fea-
tures, including n-grams, scientific lexicon, depen-
dency relations and sentence splitting. Our results
show that 3-grams and dependencies perform best
in this task; they outperform the scientific lexicon
and the sentence splitting features. Future direc-
</bodyText>
<footnote confidence="0.939167">
3This subset contains 591 positive, 59 negative and 1259
objective citations.
</footnote>
<bodyText confidence="0.999787875">
tions include trying to improve the performance by
modelling negations using a more sophisticated ap-
proach. New techniques for detection of the nega-
tion scope such as the one proposed by Councill et
al. (2010) might also be helpful in citations. Explor-
ing longer citation scopes by including citation con-
texts might also improve citation sentiment detec-
tion.
</bodyText>
<sectionHeader confidence="0.99129" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998394952380952">
S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph,
M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and Y.F.
Tan. 2008. The acl anthology reference corpus: A
reference dataset for bibliographic research in compu-
tational linguistics. In Proc. of the 6th International
Conference on Language Resources and Evaluation
Conference (LREC08), pages 1755–1759. Citeseer.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL, volume 45,
page 440.
S. Bonzi. 1982. Characteristics of a literature as pre-
dictors of relatedness between cited and citing works.
Journal of the American Society for Information Sci-
ence, 33(4):208–216.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a li-
brary for support vector machines, 2001. Software
available at http://www.csie.ntu.edu.tw/
cjlin/libsvm.
C. Cortes and V. Vapnik. 1995. Support-vector networks.
Machine learning, 20(3):273–297.
I.G. Councill, R. McDonald, and L. Velikovich. 2010.
What’s great and what’s not: learning to classify the
scope of negation for improved sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 51–59.
Association for Computational Linguistics.
K. Dave, S. Lawrence, and D.M. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic
classification of product reviews. In Proceedings of
the 12th international conference on World Wide Web,
pages 519–528. ACM.
M.C. de Marneffe and C.D. Manning. 2008. The Stan-
ford typed dependencies representation. In COLING,
pages 1–8. Association for Computational Linguistics.
Y. EL-Manzalawy and V. Honavar, 2005. WLSVM:
Integrating LibSVM into Weka Environment. Soft-
ware available at http://www.cs.iastate.
edu/˜yasser/wlsvm.
C. Engstr¨om. 2004. Topic dependence in sentiment clas-
sification. Unpublished MPhil Dissertation. Univer-
sity of Cambridge.
</reference>
<page confidence="0.99735">
85
</page>
<reference confidence="0.998260644859813">
M. Gamon and A. Aue. 2005. Automatic identification
of sentiment vocabulary: exploiting low association
with known sentiment terms. In Proceedings of the
ACL Workshop on Feature Engineering for Machine
Learning in Natural Language Processing, pages 57–
64. Association for Computational Linguistics.
M. Garzone and R. Mercer. 2000. Towards an automated
citation classifier. Advances in Artificial Intelligence,
pages 337–346.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Studying
the history of ideas using topic models. In EMNLP,
pages 363–371.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of EACL, pages 174–181. Association for Com-
putational Linguistics.
J.E. Hirsch. 2005. An index to quantify an individual’s
scientific research output. Proceedings of the National
Academy of Sciences of the United States of America,
102(46):16569.
K. Hyland. 1995. The Author in the Text: Hedging Sci-
entific Writing. Hong Kong papers in linguistics and
language teaching, 18:11.
M. Joshi and C. Penstein-Ros´e. 2009. Generalizing de-
pendency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 313–316. Association for Computational
Linguistics.
J.S. Justeson and S.M. Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural language engineering,
1(01):9–27.
S. Khan. 2007. Negation and Antonymy in Sentiment
Classification. Ph.D. thesis, Computer Lab, Univer-
sity of Cambridge.
D.D. Lewis. 1991. Evaluating text categorization. In
Proceedings of Speech and Natural Language Work-
shop, pages 312–318.
M.H. MacRoberts and B.R. MacRoberts. 1984. The
negational reference: Or the art of dissembling. So-
cial Studies of Science, 14(1):91–94.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Depen-
dency tree-based sentiment classification using CRFs
with hidden variables. In NAACL HLT, pages 786–
794. Association for Computational Linguistics.
H. Nanba and M. Okumura. 1999. Towards multi-paper
summarization using reference information. In IJCAI,
volume 16, pages 926–931. Citeseer.
V. Ng, S. Dasgupta, and SM Arifin. 2006. Examining
the role of linguistic knowledge sources in the auto-
matic identification and classification of reviews. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 611–618. Association for Com-
putational Linguistics.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In EMNLP, pages 79–86. Association for
Computational Linguistics.
S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. Mc-
Naught. 2007. Mining opinion polarity relations of ci-
tations. In International Workshop on Computational
Semantics (IWCS), pages 366–371. Citeseer.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing attitude and affect in text: Theory
and applications, pages 1–10.
D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis of
the field of Computational Linguistics. Journal of the
American Society for Information Science and Tech-
nology, 1001:48109–1092.
A. Ritchie, S. Robertson, and S. Teufel. 2008. Compar-
ing citation contexts for information retrieval. In Pro-
ceeding of the 17th ACM Conference on Information
and Knowledge Management, pages 213–222. ACM.
I. Spiegel-R¨osing. 1977. Science studies: Bibliomet-
ric and content analysis. Social Studies of Science,
7(1):97–113.
O. T¨ackstr¨om and R. McDonald. 2011. Discovering
fine-grained sentiment with latent variable structured
prediction models. In Proceedings of the ECIR.
S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Auto-
matic classification of citation function. In EMNLP,
pages 103–110. Association for Computational Lin-
guistics.
G. Thompson and Y. Yiyun. 1991. Evaluation in the
reporting verbs used in academic papers. Applied lin-
guistics, 12(4):365.
P.D. Turney. 2002. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classification of
reviews. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
417–424. Association for Computational Linguistics.
W.J. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC bioinfor-
matics, 7(1):356.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the National Conference on Artificial
Intelligence, pages 761–769. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Rec-
ognizing contextual polarity in phrase-level sentiment
analysis. In EMNLP, pages 347–354. Association for
Computational Linguistics.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
</reference>
<page confidence="0.969215">
86
</page>
<reference confidence="0.9988848">
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399–433.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In Proceedings ofEMNLP, pages 1046–
1056, Cambridge, MA, October. Association for Com-
putational Linguistics.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP, pages 129–136. Association
for Computational Linguistics.
J.M. Ziman. 1968. Public Knowledge: An essay con-
cerning the social dimension of science. Cambridge
Univ. Press, College Station, Texas.
</reference>
<page confidence="0.999473">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.515753">
<title confidence="0.982282">Sentiment Analysis of Citations using Sentence Structure-Based Features</title>
<author confidence="0.632777">Awais</author>
<affiliation confidence="0.988285">University of Computer</affiliation>
<address confidence="0.9747485">15 JJ Thompson Cambridge, CB3 0FD,</address>
<email confidence="0.999151">awais.athar@cl.cam.ac.uk</email>
<abstract confidence="0.992511222222222">Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, includspecialised science-specific lexical features, dependency relations, sentence splitting and negation features. Our results that and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bird</author>
<author>R Dale</author>
<author>B J Dorr</author>
<author>B Gibson</author>
<author>M T Joseph</author>
<author>M Y Kan</author>
<author>D Lee</author>
<author>B Powley</author>
<author>D R Radev</author>
<author>Y F Tan</author>
</authors>
<title>The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics.</title>
<date>2008</date>
<booktitle>In Proc. of the 6th International Conference on Language Resources and Evaluation Conference (LREC08),</booktitle>
<pages>1755--1759</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6844" citStr="Bird et al., 2008" startWordPosition="1050" endWordPosition="1053">t-annotated corpus of scientific text in the form of a sentence-based collection of over 8700 citations. Our experiments use a supervised classifier with the state-of-the-art features from the literature, as well as new features based on the observations above. Our results show that the most successful feature combination includes dependency features and n-grams longer than for other genres (n = 3), but the assumption of a smaller scope (sentence splitting) decreased results. 2 Training and Test Corpus We manually annotated 8736 citations from 310 research papers taken from the ACL Anthology (Bird et al., 2008). The citation summary data from the ACL Anthology Network1 (Radev et al., 2009) was used. We identified the actual text of the citations by regular expressions and replaced it with a special token &lt;CIT&gt; in order to remove any lexical bias associated with proper names of researchers. We labelled each sentence as positive, negative or objective, and separated 1472 citations for development and training. The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14</context>
</contexts>
<marker>Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, Tan, 2008</marker>
<rawString>S. Bird, R. Dale, B.J. Dorr, B. Gibson, M.T. Joseph, M.Y. Kan, D. Lee, B. Powley, D.R. Radev, and Y.F. Tan. 2008. The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In Proc. of the 6th International Conference on Language Resources and Evaluation Conference (LREC08), pages 1755–1759. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>45</volume>
<pages>440</pages>
<contexts>
<context position="16384" citStr="Blitzer et al., 2007" startWordPosition="2605" endWordPosition="2608">t al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusion In this paper, we focus on automatic identification of sentiment polarity in citations. Using a newly constructed annotated citation sentiment corpus, we examine the effectiveness of existing and novel features, including n-grams, scientific lexicon, dependency relations and sentence splitting. Our results show that 3-grams and dependencies perform best in this task; they outperform the scientific lexicon and the sentence splitting features. Future direc3This subset contains 591 positive, 59 negative and 1259 objective citations. tions include trying to improve the performance b</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL, volume 45, page 440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bonzi</author>
</authors>
<title>Characteristics of a literature as predictors of relatedness between cited and citing works.</title>
<date>1982</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="1784" citStr="Bonzi (1982)" startWordPosition="262" endWordPosition="263">ld for different text genres such as newspaper text, reviews and narrative text, relatively less emphasis has been placed on extraction of opinions from scientific literature, more specifically, citations. Analysis of citation sentiment would open up many exciting new applications in bibliographic search and in bibliometrics, i.e., the automatic evaluation the influence and impact of individuals and journals via citations. Existing bibliometric measures like H-Index (Hirsch, 2005) and adapted graph ranking algo81 rithms like PageRank (Radev et al., 2009) treat all citations as equal. However, Bonzi (1982) argued that if a cited work is criticised, it should consequently carry lower or even negative weight for bibliometric measures. Automatic citation sentiment detection is a prerequisite for such a treatment. Moreover, citation sentiment detection can also help researchers during search, by detecting problems with a particular approach. It can be used as a first step to scientific summarisation, enable users to recognise unaddressed issues and possible gaps in the current research, and thus help them set their research directions. For other genres a rich literature on sentiment detection exist</context>
</contexts>
<marker>Bonzi, 1982</marker>
<rawString>S. Bonzi. 1982. Characteristics of a literature as predictors of relatedness between cited and citing works. Journal of the American Society for Information Science, 33(4):208–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C C Chang</author>
<author>C J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines,</title>
<date>2001</date>
<note>Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</note>
<contexts>
<context position="7843" citStr="Chang and Lin, 2001" startWordPosition="1214" endWordPosition="1217"> development and training. The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. 1http://www.aclweb.org 82 3 Features We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework which has been shown to produce good results for sentiment classification (Pang et al., 2002). The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001) with the following features. 3.1 Word Level Features In accordance with Pang et al. (2002), we use unigrams and bigrams as features and also add 3-grams as new features to capture longer technical terms. POS tags are also included using two approaches: attaching the tag to the word by a delimiter, and appending all tags at the end of the sentence. This may help in distinguishing between homonyms with different POS tags and signalling the presence of adjectives (e.g., JJ) respectively. Name of the primary author of the cited paper is also used as a feature. A science-specific sentiment lexicon</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.C. Chang and C.J. Lin. 2001. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="7601" citStr="Cortes and Vapnik, 1995" startWordPosition="1175" endWordPosition="1178">ions by regular expressions and replaced it with a special token &lt;CIT&gt; in order to remove any lexical bias associated with proper names of researchers. We labelled each sentence as positive, negative or objective, and separated 1472 citations for development and training. The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. 1http://www.aclweb.org 82 3 Features We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework which has been shown to produce good results for sentiment classification (Pang et al., 2002). The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001) with the following features. 3.1 Word Level Features In accordance with Pang et al. (2002), we use unigrams and bigrams as features and also add 3-grams as new features to capture longer technical terms. POS tags are also included using two approaches: attaching the tag to the word by a delimiter, and appending all tags at the end of the sentence. This ma</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I G Councill</author>
<author>R McDonald</author>
<author>L Velikovich</author>
</authors>
<title>What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,</booktitle>
<pages>51--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>I.G. Councill, R. McDonald, and L. Velikovich. 2010. What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 51–59. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="15775" citStr="Dave et al. (2003)" startWordPosition="2509" endWordPosition="2512">ed corpus, a low macro-F score of 0.484 is achieved. This indicates that there is a mismatch in the annotated class labels. Therefore, we can infer that citation sentiment classification is different from citation function classification. Other approaches to citation annotation and classification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et a</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>K. Dave, S. Lawrence, and D.M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of the 12th international conference on World Wide Web, pages 519–528. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C de Marneffe</author>
<author>C D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>M.C. de Marneffe and C.D. Manning. 2008. The Stanford typed dependencies representation. In COLING, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y EL-Manzalawy</author>
<author>V Honavar</author>
</authors>
<title>WLSVM: Integrating LibSVM into Weka Environment. Software available at http://www.cs.iastate.</title>
<date>2005</date>
<note>edu/˜yasser/wlsvm.</note>
<contexts>
<context position="7821" citStr="EL-Manzalawy and Honavar, 2005" startWordPosition="1210" endWordPosition="1213">and separated 1472 citations for development and training. The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. 1http://www.aclweb.org 82 3 Features We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework which has been shown to produce good results for sentiment classification (Pang et al., 2002). The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001) with the following features. 3.1 Word Level Features In accordance with Pang et al. (2002), we use unigrams and bigrams as features and also add 3-grams as new features to capture longer technical terms. POS tags are also included using two approaches: attaching the tag to the word by a delimiter, and appending all tags at the end of the sentence. This may help in distinguishing between homonyms with different POS tags and signalling the presence of adjectives (e.g., JJ) respectively. Name of the primary author of the cited paper is also used as a feature. A science-spec</context>
</contexts>
<marker>EL-Manzalawy, Honavar, 2005</marker>
<rawString>Y. EL-Manzalawy and V. Honavar, 2005. WLSVM: Integrating LibSVM into Weka Environment. Software available at http://www.cs.iastate. edu/˜yasser/wlsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Engstr¨om</author>
</authors>
<title>Topic dependence in sentiment classification. Unpublished MPhil Dissertation.</title>
<date>2004</date>
<institution>University of Cambridge.</institution>
<marker>Engstr¨om, 2004</marker>
<rawString>C. Engstr¨om. 2004. Topic dependence in sentiment classification. Unpublished MPhil Dissertation. University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>A Aue</author>
</authors>
<title>Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="16361" citStr="Gamon and Aue, 2005" startWordPosition="2601" endWordPosition="2604">en explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusion In this paper, we focus on automatic identification of sentiment polarity in citations. Using a newly constructed annotated citation sentiment corpus, we examine the effectiveness of existing and novel features, including n-grams, scientific lexicon, dependency relations and sentence splitting. Our results show that 3-grams and dependencies perform best in this task; they outperform the scientific lexicon and the sentence splitting features. Future direc3This subset contains 591 positive, 59 negative and 1259 objective citations. tions include trying to im</context>
</contexts>
<marker>Gamon, Aue, 2005</marker>
<rawString>M. Gamon and A. Aue. 2005. Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms. In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing, pages 57– 64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Garzone</author>
<author>R Mercer</author>
</authors>
<title>Towards an automated citation classifier.</title>
<date>2000</date>
<booktitle>Advances in Artificial Intelligence,</booktitle>
<pages>337--346</pages>
<contexts>
<context position="13485" citStr="Garzone and Mercer, 2000" startWordPosition="2131" endWordPosition="2134">ufficient to capture discriminating lexical structures. We find that word level and contextual polarity features are surpassed by dependency features. Sentence splitting does not help, possibly due to longer citation scope. Adding a negation window (k=15) improves the performance but the improvement was not found to be statistically significant. This might be due to skewed class distribution and a larger dataset may prove to be useful. 5 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-R¨osing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), there have been no attempts on citation sentiment detection in a large corpus. Teufel et al. (2006) worked on a 2829 sentence citation corpus using a 12-class classification scheme. However, this corpus has been annotated for the task of determining the author’s reason for citing a given paper and is thus built on top of sentiment of citation. It considers usage, modification and similarity with a cited paper as positive even when there is no sentiment attributed to it. Moreover, contrast between two cited methods (CoCoXY) is categorized as objective in the annotation scheme even if the text</context>
</contexts>
<marker>Garzone, Mercer, 2000</marker>
<rawString>M. Garzone and R. Mercer. 2000. Towards an automated citation classifier. Advances in Artificial Intelligence, pages 337–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hall</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Studying the history of ideas using topic models.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>363--371</pages>
<contexts>
<context position="7761" citStr="Hall et al., 2008" startWordPosition="1201" endWordPosition="1204">ch sentence as positive, negative or objective, and separated 1472 citations for development and training. The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. 1http://www.aclweb.org 82 3 Features We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework which has been shown to produce good results for sentiment classification (Pang et al., 2002). The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001) with the following features. 3.1 Word Level Features In accordance with Pang et al. (2002), we use unigrams and bigrams as features and also add 3-grams as new features to capture longer technical terms. POS tags are also included using two approaches: attaching the tag to the word by a delimiter, and appending all tags at the end of the sentence. This may help in distinguishing between homonyms with different POS tags and signalling the presence of adjectives (e.g., JJ) respectively. Name of the primary author </context>
</contexts>
<marker>Hall, Jurafsky, Manning, 2008</marker>
<rawString>D. Hall, D. Jurafsky, and C.D. Manning. 2008. Studying the history of ideas using topic models. In EMNLP, pages 363–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>174--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16142" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="2568" endWordPosition="2571">1 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusion In this paper, we focus on automatic identification of sentiment polarity in citations. Using a newly constructed annotated citation sentiment corpus, we examine the effectiveness of existing and novel features, including n-grams, scientific lexicon, dependency relations and sentence splitting. Our results show that 3-grams and dependencies </context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K.R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of EACL, pages 174–181. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hirsch</author>
</authors>
<title>An index to quantify an individual’s scientific research output.</title>
<date>2005</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>102</volume>
<issue>46</issue>
<contexts>
<context position="1657" citStr="Hirsch, 2005" startWordPosition="240" endWordPosition="241">ntiments, emotions and attitudes expressed in text. Although there has been in the past few years a growing interest in this field for different text genres such as newspaper text, reviews and narrative text, relatively less emphasis has been placed on extraction of opinions from scientific literature, more specifically, citations. Analysis of citation sentiment would open up many exciting new applications in bibliographic search and in bibliometrics, i.e., the automatic evaluation the influence and impact of individuals and journals via citations. Existing bibliometric measures like H-Index (Hirsch, 2005) and adapted graph ranking algo81 rithms like PageRank (Radev et al., 2009) treat all citations as equal. However, Bonzi (1982) argued that if a cited work is criticised, it should consequently carry lower or even negative weight for bibliometric measures. Automatic citation sentiment detection is a prerequisite for such a treatment. Moreover, citation sentiment detection can also help researchers during search, by detecting problems with a particular approach. It can be used as a first step to scientific summarisation, enable users to recognise unaddressed issues and possible gaps in the curr</context>
</contexts>
<marker>Hirsch, 2005</marker>
<rawString>J.E. Hirsch. 2005. An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences of the United States of America, 102(46):16569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hyland</author>
</authors>
<title>The Author in the Text: Hedging Scientific Writing. Hong Kong papers in linguistics and language teaching,</title>
<date>1995</date>
<pages>18--11</pages>
<contexts>
<context position="3897" citStr="Hyland, 1995" startWordPosition="582" endWordPosition="583">itations is often hidden. This might Proceedings of the ACL-HLT 2011 Student Session, pages 81–87, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics be because of the general strategy to avoid overt criticism due to the sociological aspect of citing (MacRoberts and MacRoberts, 1984; Thompson and Yiyun, 1991). Ziman (1968) states that many works are cited out of “politeness, policy or piety”. Negative sentiment, while still present and detectable for humans, is expressed in subtle ways and might be hedged, especially when it cannot be quantitatively justified (Hyland, 1995). While SCL has been successfully applied to POS tagging and Sentiment Analysis (Blitzer et al., 2006), its effectiveness for parsing was rather unexplored. • Citation sentences are often neutral with respect to sentiment, either because they describe an algorithm, approach or methodology objectively, or because they are used to support a fact or statement. There are five different IBM translation models (Brown et al. , 1993). This gives rise to a far higher proportion of objective sentences than in other genres. • Negative polarity is often expressed in contrastive terms, e.g. in evaluation s</context>
</contexts>
<marker>Hyland, 1995</marker>
<rawString>K. Hyland. 1995. The Author in the Text: Hedging Scientific Writing. Hong Kong papers in linguistics and language teaching, 18:11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Joshi</author>
<author>C Penstein-Ros´e</author>
</authors>
<title>Generalizing dependency features for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>313--316</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Joshi, Penstein-Ros´e, 2009</marker>
<rawString>M. Joshi and C. Penstein-Ros´e. 2009. Generalizing dependency features for opinion mining. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 313–316. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Justeson</author>
<author>S M Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in text. Natural language engineering,</title>
<date>1995</date>
<pages>1--01</pages>
<contexts>
<context position="5253" citStr="Justeson and Katz, 1995" startWordPosition="795" endWordPosition="798">rly evaluated positively in comparison. This method was shown to outperform the class based model proposed in (Brown et al., 1992) ... • There is also much variation between scientific texts and other genres concerning the lexical items chosen to convey sentiment. Sentiment carrying science-specific terms exist and are relatively frequent, which motivates the use of a sentiment lexicon specialised to science. Similarity-based smoothing (Dagan, Lee, and Pereira 1999) provides an intuitively appealing approach to language modeling. • Technical terms play a large role overall in scientific text (Justeson and Katz, 1995). Some of these carry sentiment as well. Current state of the art machine translation systems (Och, 2003) use phrasal (n-gram) features ... For this reason, using higher order n-grams might prove to be useful in sentiment detection. • The scope of influence of citations varies widely from a single clause (as in the example below) to several paragraphs: As reported in Table 3, small increases in METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al., 2002) and NIST scores (Doddington, 2002) suggest that... This affects lexical features directly since there could be “sentiment overlap” associa</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>J.S. Justeson and S.M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for identification in text. Natural language engineering, 1(01):9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khan</author>
</authors>
<title>Negation and Antonymy in Sentiment Classification.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Lab, University of Cambridge.</institution>
<contexts>
<context position="11693" citStr="Khan (2007)" startWordPosition="1832" endWordPosition="1833">s, such as not, to the clause subtree and this shows no interaction with other nodes with respect to valence shifting. To handle this effect, we take a simple window-based inversion approach. All words inside a k-word window of any negation term are suffixed with a token neg to distinguish them from their non-polar versions. For example, a 2- word negation window inverts the polarity of the positive phrase work well in the sentence below. Turney’s method did not work neg well neg although they reported 80% accuracy in &lt;CIT&gt;. The negation term list has been taken from the OpinionFinder system. Khan (2007) has shown that this approach produces results comparable to grammatical relations based negation models. 4 Results Because of our skewed dataset, we report both the macro-F and the micro-F scores using 10-fold cross-validation (Lewis, 1991). The bold values in Table 1 show the best results. Features macro-F micro-F 1 grams 0.581 0.863 1-2 grams 0.592 0.864 1-3 grams 0.597 0.862 &amp;quot; + POS 0.535 0.859 &amp;quot; + POS (tokenised) 0.596 0.859 &amp;quot; + scilex 0.597 0.860 &amp;quot; + wlev 0.535 0.859 &amp;quot; + cpol 0.418 0.859 &amp;quot; + dep 0.760 0.897 &amp;quot; + dep + split + neg 0.683 0.872 &amp;quot; + dep + split 0.642 0.866 &amp;quot; + dep + neg 0.764</context>
</contexts>
<marker>Khan, 2007</marker>
<rawString>S. Khan. 2007. Negation and Antonymy in Sentiment Classification. Ph.D. thesis, Computer Lab, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>Evaluating text categorization.</title>
<date>1991</date>
<booktitle>In Proceedings of Speech and Natural Language Workshop,</booktitle>
<pages>312--318</pages>
<contexts>
<context position="11934" citStr="Lewis, 1991" startWordPosition="1868" endWordPosition="1869"> term are suffixed with a token neg to distinguish them from their non-polar versions. For example, a 2- word negation window inverts the polarity of the positive phrase work well in the sentence below. Turney’s method did not work neg well neg although they reported 80% accuracy in &lt;CIT&gt;. The negation term list has been taken from the OpinionFinder system. Khan (2007) has shown that this approach produces results comparable to grammatical relations based negation models. 4 Results Because of our skewed dataset, we report both the macro-F and the micro-F scores using 10-fold cross-validation (Lewis, 1991). The bold values in Table 1 show the best results. Features macro-F micro-F 1 grams 0.581 0.863 1-2 grams 0.592 0.864 1-3 grams 0.597 0.862 &amp;quot; + POS 0.535 0.859 &amp;quot; + POS (tokenised) 0.596 0.859 &amp;quot; + scilex 0.597 0.860 &amp;quot; + wlev 0.535 0.859 &amp;quot; + cpol 0.418 0.859 &amp;quot; + dep 0.760 0.897 &amp;quot; + dep + split + neg 0.683 0.872 &amp;quot; + dep + split 0.642 0.866 &amp;quot; + dep + neg 0.764 0.898 Table 1: Results using science lexicon (scilex), contextual polarity (cpol), dependencies (dep), negation (neg), sentence splitting (split) and word-level (wlev) features. The selection of the features is on the basis of improvements </context>
</contexts>
<marker>Lewis, 1991</marker>
<rawString>D.D. Lewis. 1991. Evaluating text categorization. In Proceedings of Speech and Natural Language Workshop, pages 312–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H MacRoberts</author>
<author>B R MacRoberts</author>
</authors>
<title>The negational reference: Or the art of dissembling.</title>
<date>1984</date>
<journal>Social Studies of Science,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="3600" citStr="MacRoberts and MacRoberts, 1984" startWordPosition="534" endWordPosition="537"> equally well to classify citations. We argue that this might not be the case; our citation sentiment recogniser uses specialised training data and tests the performance of specialised features against current state-of-the-art features. The reasons for this are based on the following observations: • Sentiment in citations is often hidden. This might Proceedings of the ACL-HLT 2011 Student Session, pages 81–87, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics be because of the general strategy to avoid overt criticism due to the sociological aspect of citing (MacRoberts and MacRoberts, 1984; Thompson and Yiyun, 1991). Ziman (1968) states that many works are cited out of “politeness, policy or piety”. Negative sentiment, while still present and detectable for humans, is expressed in subtle ways and might be hedged, especially when it cannot be quantitatively justified (Hyland, 1995). While SCL has been successfully applied to POS tagging and Sentiment Analysis (Blitzer et al., 2006), its effectiveness for parsing was rather unexplored. • Citation sentences are often neutral with respect to sentiment, either because they describe an algorithm, approach or methodology objectively, </context>
</contexts>
<marker>MacRoberts, MacRoberts, 1984</marker>
<rawString>M.H. MacRoberts and B.R. MacRoberts. 1984. The negational reference: Or the art of dissembling. Social Studies of Science, 14(1):91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>K Inui</author>
<author>S Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<booktitle>In NAACL HLT,</booktitle>
<pages>786--794</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2776" citStr="Nakagawa et al., 2010" startWordPosition="408" endWordPosition="411">o scientific summarisation, enable users to recognise unaddressed issues and possible gaps in the current research, and thus help them set their research directions. For other genres a rich literature on sentiment detection exists and researchers have used a number of features such as n-grams, presence of adjectives, adverbs and other parts-of-speech (POS), negation, grammatical and dependency relations as well as specialised lexicons in order to detect sentiments from phrases, words, sentences and documents. State-of-the-art systems report around 85-90% accuracy for different genres of text (Nakagawa et al., 2010; Yessenalina et al., 2010; T¨ackstr¨om and McDonald, 2011). Given such good results, one might think that a sentence-based sentiment detection system trained on a different genre could be used equally well to classify citations. We argue that this might not be the case; our citation sentiment recogniser uses specialised training data and tests the performance of specialised features against current state-of-the-art features. The reasons for this are based on the following observations: • Sentiment in citations is often hidden. This might Proceedings of the ACL-HLT 2011 Student Session, pages </context>
<context position="15866" citStr="Nakagawa et al. (2010)" startWordPosition="2525" endWordPosition="2529">match in the annotated class labels. Therefore, we can infer that citation sentiment classification is different from citation function classification. Other approaches to citation annotation and classification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusion In this paper, we focus on automatic identification of sentiment po</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>T. Nakagawa, K. Inui, and S. Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In NAACL HLT, pages 786– 794. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nanba</author>
<author>M Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In IJCAI,</booktitle>
<volume>16</volume>
<pages>926--931</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="13458" citStr="Nanba and Okumura, 1999" startWordPosition="2127" endWordPosition="2130">dicate that n-grams are sufficient to capture discriminating lexical structures. We find that word level and contextual polarity features are surpassed by dependency features. Sentence splitting does not help, possibly due to longer citation scope. Adding a negation window (k=15) improves the performance but the improvement was not found to be statistically significant. This might be due to skewed class distribution and a larger dataset may prove to be useful. 5 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-R¨osing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), there have been no attempts on citation sentiment detection in a large corpus. Teufel et al. (2006) worked on a 2829 sentence citation corpus using a 12-class classification scheme. However, this corpus has been annotated for the task of determining the author’s reason for citing a given paper and is thus built on top of sentiment of citation. It considers usage, modification and similarity with a cited paper as positive even when there is no sentiment attributed to it. Moreover, contrast between two cited methods (CoCoXY) is categorized as objective in the annotat</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>H. Nanba and M. Okumura. 1999. Towards multi-paper summarization using reference information. In IJCAI, volume 16, pages 926–931. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>S Dasgupta</author>
<author>SM Arifin</author>
</authors>
<title>Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>611--618</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15818" citStr="Ng et al. (2006)" startWordPosition="2518" endWordPosition="2521">hieved. This indicates that there is a mismatch in the annotated class labels. Therefore, we can infer that citation sentiment classification is different from citation function classification. Other approaches to citation annotation and classification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusion In this paper, we f</context>
</contexts>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>V. Ng, S. Dasgupta, and SM Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 611–618. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7705" citStr="Pang et al., 2002" startWordPosition="1191" endWordPosition="1194">ociated with proper names of researchers. We labelled each sentence as positive, negative or objective, and separated 1472 citations for development and training. The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. 1http://www.aclweb.org 82 3 Features We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework which has been shown to produce good results for sentiment classification (Pang et al., 2002). The corpus is processed using WEKA (Hall et al., 2008) and the Weka LibSVM library (EL-Manzalawy and Honavar, 2005; Chang and Lin, 2001) with the following features. 3.1 Word Level Features In accordance with Pang et al. (2002), we use unigrams and bigrams as features and also add 3-grams as new features to capture longer technical terms. POS tags are also included using two approaches: attaching the tag to the word by a delimiter, and appending all tags at the end of the sentence. This may help in distinguishing between homonyms with different POS tags and signalling the presence of adjecti</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In EMNLP, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Piao</author>
<author>S Ananiadou</author>
<author>Y Tsuruoka</author>
<author>Y Sasaki</author>
<author>J McNaught</author>
</authors>
<title>Mining opinion polarity relations of citations.</title>
<date>2007</date>
<booktitle>In International Workshop on Computational Semantics (IWCS),</booktitle>
<pages>366--371</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="15604" citStr="Piao et al. (2007)" startWordPosition="2483" endWordPosition="2486">F score of 0.797 for the subset. This shows that our system is comparable to Teufel et al. (2006). When this subset is used to test the system trained on our newly annotated corpus, a low macro-F score of 0.484 is achieved. This indicates that there is a mismatch in the annotated class labels. Therefore, we can infer that citation sentiment classification is different from citation function classification. Other approaches to citation annotation and classification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research sugge</context>
</contexts>
<marker>Piao, Ananiadou, Tsuruoka, Sasaki, McNaught, 2007</marker>
<rawString>S. Piao, S. Ananiadou, Y. Tsuruoka, Y. Sasaki, and J. McNaught. 2007. Mining opinion polarity relations of citations. In International Workshop on Computational Semantics (IWCS), pages 366–371. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual valence shifters. Computing attitude and affect in text: Theory and applications,</title>
<date>2006</date>
<pages>1--10</pages>
<contexts>
<context position="9346" citStr="Polanyi and Zaenen, 2006" startWordPosition="1459" endWordPosition="1462">ul, state-of-the-art and effective. 3.2 Contextual Polarity Features Features previously found to be useful for detecting phrase-level contextual polarity (Wilson et al., 2009) are also included. Since the task at hand is sentence-based, we use only the sentence-based features from the literature e.g., presence of subjectivity clues which have been compiled from several sources2 along with the number of adjectives, adverbs, pronouns, modals and cardinals. To handle negation, we include the count of negation phrases found within the citation sentence. Similarly, the number of valance shifters (Polanyi and Zaenen, 2006) in the sentence are also used. The polarity shifter and negation phrase lists have been taken from the OpinionFinder system (Wilson et al., 2005). 2Available for download at http://www.cs.pitt.edu/mpqa/ 3.3 Sentence Structure Based Features We explore three different feature sets which focus on the lexical and grammatical structure of a sentence and have not been explored previously for the task of sentiment analysis of scientific text. 3.3.1 Dependency Structures The first set of these features include typed dependency structures (de Marneffe and Manning, 2008) which describe the grammatical</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>L. Polanyi and A. Zaenen. 2006. Contextual valence shifters. Computing attitude and affect in text: Theory and applications, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>M T Joseph</author>
<author>B Gibson</author>
<author>P Muthukrishnan</author>
</authors>
<title>A Bibliometric and Network Analysis of the field of Computational Linguistics.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<pages>1001--48109</pages>
<contexts>
<context position="1732" citStr="Radev et al., 2009" startWordPosition="251" endWordPosition="254">s been in the past few years a growing interest in this field for different text genres such as newspaper text, reviews and narrative text, relatively less emphasis has been placed on extraction of opinions from scientific literature, more specifically, citations. Analysis of citation sentiment would open up many exciting new applications in bibliographic search and in bibliometrics, i.e., the automatic evaluation the influence and impact of individuals and journals via citations. Existing bibliometric measures like H-Index (Hirsch, 2005) and adapted graph ranking algo81 rithms like PageRank (Radev et al., 2009) treat all citations as equal. However, Bonzi (1982) argued that if a cited work is criticised, it should consequently carry lower or even negative weight for bibliometric measures. Automatic citation sentiment detection is a prerequisite for such a treatment. Moreover, citation sentiment detection can also help researchers during search, by detecting problems with a particular approach. It can be used as a first step to scientific summarisation, enable users to recognise unaddressed issues and possible gaps in the current research, and thus help them set their research directions. For other g</context>
<context position="6924" citStr="Radev et al., 2009" startWordPosition="1063" endWordPosition="1066">n of over 8700 citations. Our experiments use a supervised classifier with the state-of-the-art features from the literature, as well as new features based on the observations above. Our results show that the most successful feature combination includes dependency features and n-grams longer than for other genres (n = 3), but the assumption of a smaller scope (sentence splitting) decreased results. 2 Training and Test Corpus We manually annotated 8736 citations from 310 research papers taken from the ACL Anthology (Bird et al., 2008). The citation summary data from the ACL Anthology Network1 (Radev et al., 2009) was used. We identified the actual text of the citations by regular expressions and replaced it with a special token &lt;CIT&gt; in order to remove any lexical bias associated with proper names of researchers. We labelled each sentence as positive, negative or objective, and separated 1472 citations for development and training. The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. 1http://www.aclweb.org 82 3 Features We represent each citation</context>
</contexts>
<marker>Radev, Joseph, Gibson, Muthukrishnan, 2009</marker>
<rawString>D.R. Radev, M.T. Joseph, B. Gibson, and P. Muthukrishnan. 2009. A Bibliometric and Network Analysis of the field of Computational Linguistics. Journal of the American Society for Information Science and Technology, 1001:48109–1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritchie</author>
<author>S Robertson</author>
<author>S Teufel</author>
</authors>
<title>Comparing citation contexts for information retrieval.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>213--222</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5907" citStr="Ritchie et al. (2008)" startWordPosition="897" endWordPosition="900">s well. Current state of the art machine translation systems (Och, 2003) use phrasal (n-gram) features ... For this reason, using higher order n-grams might prove to be useful in sentiment detection. • The scope of influence of citations varies widely from a single clause (as in the example below) to several paragraphs: As reported in Table 3, small increases in METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al., 2002) and NIST scores (Doddington, 2002) suggest that... This affects lexical features directly since there could be “sentiment overlap” associated with neighbouring citations. Ritchie et al. (2008) showed that assuming larger citation scopes has a positive effect in retrieval. We will test the opposite direction here, i.e., we assume short scopes and use a parser to split sentences, so that the features associated with the clauses not directly connected to the citation are disregarded. We created a new sentiment-annotated corpus of scientific text in the form of a sentence-based collection of over 8700 citations. Our experiments use a supervised classifier with the state-of-the-art features from the literature, as well as new features based on the observations above. Our results show th</context>
</contexts>
<marker>Ritchie, Robertson, Teufel, 2008</marker>
<rawString>A. Ritchie, S. Robertson, and S. Teufel. 2008. Comparing citation contexts for information retrieval. In Proceeding of the 17th ACM Conference on Information and Knowledge Management, pages 213–222. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Spiegel-R¨osing</author>
</authors>
<title>Science studies: Bibliometric and content analysis.</title>
<date>1977</date>
<journal>Social Studies of Science,</journal>
<volume>7</volume>
<issue>1</issue>
<marker>Spiegel-R¨osing, 1977</marker>
<rawString>I. Spiegel-R¨osing. 1977. Science studies: Bibliometric and content analysis. Social Studies of Science, 7(1):97–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O T¨ackstr¨om</author>
<author>R McDonald</author>
</authors>
<title>Discovering fine-grained sentiment with latent variable structured prediction models.</title>
<date>2011</date>
<booktitle>In Proceedings of the ECIR.</booktitle>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>O. T¨ackstr¨om and R. McDonald. 2011. Discovering fine-grained sentiment with latent variable structured prediction models. In Proceedings of the ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>D Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>103--110</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13586" citStr="Teufel et al. (2006)" startWordPosition="2150" endWordPosition="2153">atures are surpassed by dependency features. Sentence splitting does not help, possibly due to longer citation scope. Adding a negation window (k=15) improves the performance but the improvement was not found to be statistically significant. This might be due to skewed class distribution and a larger dataset may prove to be useful. 5 Related Work While different schemes have been proposed for annotating citations according to their function (Spiegel-R¨osing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), there have been no attempts on citation sentiment detection in a large corpus. Teufel et al. (2006) worked on a 2829 sentence citation corpus using a 12-class classification scheme. However, this corpus has been annotated for the task of determining the author’s reason for citing a given paper and is thus built on top of sentiment of citation. It considers usage, modification and similarity with a cited paper as positive even when there is no sentiment attributed to it. Moreover, contrast between two cited methods (CoCoXY) is categorized as objective in the annotation scheme even if the text indicates that one method performs better than the other. For example, the sentence below talks abou</context>
<context position="15083" citStr="Teufel et al. (2006)" startWordPosition="2400" endWordPosition="2403"> sentiment in any system which relies on lexical features. Teufel et al. (2006) group the 12 categories into 3 in an attempt to perform a rough approximation of sentiment analysis over the classifications and report a 0.710 macro-F score. Unfortunately, we have ac84 cess to only a subset3 of this citation function corpus. We have extracted 1-3 grams, dependencies and negation features from the reduced citation function dataset and used them in our system with 10-fold cross-validation. This results in an improved macroF score of 0.797 for the subset. This shows that our system is comparable to Teufel et al. (2006). When this subset is used to test the system trained on our newly annotated corpus, a low macro-F score of 0.484 is achieved. This indicates that there is a mismatch in the annotated class labels. Therefore, we can infer that citation sentiment classification is different from citation function classification. Other approaches to citation annotation and classification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links betwee</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>S. Teufel, A. Siddharthan, and D. Tidhar. 2006. Automatic classification of citation function. In EMNLP, pages 103–110. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Thompson</author>
<author>Y Yiyun</author>
</authors>
<title>Evaluation in the reporting verbs used in academic papers.</title>
<date>1991</date>
<journal>Applied linguistics,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="3627" citStr="Thompson and Yiyun, 1991" startWordPosition="538" endWordPosition="542">ns. We argue that this might not be the case; our citation sentiment recogniser uses specialised training data and tests the performance of specialised features against current state-of-the-art features. The reasons for this are based on the following observations: • Sentiment in citations is often hidden. This might Proceedings of the ACL-HLT 2011 Student Session, pages 81–87, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics be because of the general strategy to avoid overt criticism due to the sociological aspect of citing (MacRoberts and MacRoberts, 1984; Thompson and Yiyun, 1991). Ziman (1968) states that many works are cited out of “politeness, policy or piety”. Negative sentiment, while still present and detectable for humans, is expressed in subtle ways and might be hedged, especially when it cannot be quantitatively justified (Hyland, 1995). While SCL has been successfully applied to POS tagging and Sentiment Analysis (Blitzer et al., 2006), its effectiveness for parsing was rather unexplored. • Citation sentences are often neutral with respect to sentiment, either because they describe an algorithm, approach or methodology objectively, or because they are used to</context>
</contexts>
<marker>Thompson, Yiyun, 1991</marker>
<rawString>G. Thompson and Y. Yiyun. 1991. Evaluation in the reporting verbs used in academic papers. Applied linguistics, 12(4):365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16156" citStr="Turney, 2002" startWordPosition="2572" endWordPosition="2573">, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusion In this paper, we focus on automatic identification of sentiment polarity in citations. Using a newly constructed annotated citation sentiment corpus, we examine the effectiveness of existing and novel features, including n-grams, scientific lexicon, dependency relations and sentence splitting. Our results show that 3-grams and dependencies perform best i</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P.D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 417–424. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Wilbur</author>
<author>A Rzhetsky</author>
<author>H Shatkay</author>
</authors>
<title>New directions in biomedical text annotation: definitions, guidelines and corpus construction.</title>
<date>2006</date>
<journal>BMC bioinformatics,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="15483" citStr="Wilbur et al. (2006)" startWordPosition="2464" endWordPosition="2467">uced citation function dataset and used them in our system with 10-fold cross-validation. This results in an improved macroF score of 0.797 for the subset. This shows that our system is comparable to Teufel et al. (2006). When this subset is used to test the system trained on our newly annotated corpus, a low macro-F score of 0.484 is achieved. This indicates that there is a mismatch in the annotated class labels. Therefore, we can infer that citation sentiment classification is different from citation function classification. Other approaches to citation annotation and classification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lex</context>
</contexts>
<marker>Wilbur, Rzhetsky, Shatkay, 2006</marker>
<rawString>W.J. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New directions in biomedical text annotation: definitions, guidelines and corpus construction. BMC bioinformatics, 7(1):356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>761--769</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="15797" citStr="Wilson et al. (2004)" startWordPosition="2513" endWordPosition="2516">ro-F score of 0.484 is achieved. This indicates that there is a mismatch in the annotated class labels. Therefore, we can infer that citation sentiment classification is different from citation function classification. Other approaches to citation annotation and classification include Wilbur et al. (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusio</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings of the National Conference on Artificial Intelligence, pages 761–769. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In EMNLP,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9492" citStr="Wilson et al., 2005" startWordPosition="1483" endWordPosition="1486">ity (Wilson et al., 2009) are also included. Since the task at hand is sentence-based, we use only the sentence-based features from the literature e.g., presence of subjectivity clues which have been compiled from several sources2 along with the number of adjectives, adverbs, pronouns, modals and cardinals. To handle negation, we include the count of negation phrases found within the citation sentence. Similarly, the number of valance shifters (Polanyi and Zaenen, 2006) in the sentence are also used. The polarity shifter and negation phrase lists have been taken from the OpinionFinder system (Wilson et al., 2005). 2Available for download at http://www.cs.pitt.edu/mpqa/ 3.3 Sentence Structure Based Features We explore three different feature sets which focus on the lexical and grammatical structure of a sentence and have not been explored previously for the task of sentiment analysis of scientific text. 3.3.1 Dependency Structures The first set of these features include typed dependency structures (de Marneffe and Manning, 2008) which describe the grammatical relationships between words. We aim to capture the long distance relationships between words. For instance in the sentence below, the relationshi</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In EMNLP, pages 347–354. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="8897" citStr="Wilson et al., 2009" startWordPosition="1387" endWordPosition="1390">ling the presence of adjectives (e.g., JJ) respectively. Name of the primary author of the cited paper is also used as a feature. A science-specific sentiment lexicon is also added to the feature set. This lexicon consists of 83 polar phrases which have been manually extracted from the development set of 736 citations. Some of the most frequently occurring polar phrases in this set consists of adjectives such as efficient, popular, successful, state-of-the-art and effective. 3.2 Contextual Polarity Features Features previously found to be useful for detecting phrase-level contextual polarity (Wilson et al., 2009) are also included. Since the task at hand is sentence-based, we use only the sentence-based features from the literature e.g., presence of subjectivity clues which have been compiled from several sources2 along with the number of adjectives, adverbs, pronouns, modals and cardinals. To handle negation, we include the count of negation phrases found within the citation sentence. Similarly, the number of valance shifters (Polanyi and Zaenen, 2006) in the sentence are also used. The polarity shifter and negation phrase lists have been taken from the OpinionFinder system (Wilson et al., 2005). 2Av</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>Y Yue</author>
<author>C Cardie</author>
</authors>
<title>Multilevel structured models for document-level sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>1046--1056</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2802" citStr="Yessenalina et al., 2010" startWordPosition="412" endWordPosition="415">ion, enable users to recognise unaddressed issues and possible gaps in the current research, and thus help them set their research directions. For other genres a rich literature on sentiment detection exists and researchers have used a number of features such as n-grams, presence of adjectives, adverbs and other parts-of-speech (POS), negation, grammatical and dependency relations as well as specialised lexicons in order to detect sentiments from phrases, words, sentences and documents. State-of-the-art systems report around 85-90% accuracy for different genres of text (Nakagawa et al., 2010; Yessenalina et al., 2010; T¨ackstr¨om and McDonald, 2011). Given such good results, one might think that a sentence-based sentiment detection system trained on a different genre could be used equally well to classify citations. We argue that this might not be the case; our citation sentiment recogniser uses specialised training data and tests the performance of specialised features against current state-of-the-art features. The reasons for this are based on the following observations: • Sentiment in citations is often hidden. This might Proceedings of the ACL-HLT 2011 Student Session, pages 81–87, Portland, OR, USA 1</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multilevel structured models for document-level sentiment classification. In Proceedings ofEMNLP, pages 1046– 1056, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>129--136</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16188" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="2574" endWordPosition="2578">vidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. Different dependency relations have been explored by Dave et al. (2003), Wilson et al. (2004) and Ng et al. (2006) for sentiment detection. Nakagawa et al. (2010) report that using dependencies on conditional random fields with lexicon based polarity reversal results in improvements over n-grams for news and reviews corpora. A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr¨om, 2004; Gamon and Aue, 2005; Blitzer et al., 2007). 6 Conclusion In this paper, we focus on automatic identification of sentiment polarity in citations. Using a newly constructed annotated citation sentiment corpus, we examine the effectiveness of existing and novel features, including n-grams, scientific lexicon, dependency relations and sentence splitting. Our results show that 3-grams and dependencies perform best in this task; they outperform the</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP, pages 129–136. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ziman</author>
</authors>
<title>Public Knowledge: An essay concerning the social dimension of science.</title>
<date>1968</date>
<publisher>Cambridge Univ. Press,</publisher>
<location>College Station, Texas.</location>
<contexts>
<context position="3641" citStr="Ziman (1968)" startWordPosition="543" endWordPosition="544">t not be the case; our citation sentiment recogniser uses specialised training data and tests the performance of specialised features against current state-of-the-art features. The reasons for this are based on the following observations: • Sentiment in citations is often hidden. This might Proceedings of the ACL-HLT 2011 Student Session, pages 81–87, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics be because of the general strategy to avoid overt criticism due to the sociological aspect of citing (MacRoberts and MacRoberts, 1984; Thompson and Yiyun, 1991). Ziman (1968) states that many works are cited out of “politeness, policy or piety”. Negative sentiment, while still present and detectable for humans, is expressed in subtle ways and might be hedged, especially when it cannot be quantitatively justified (Hyland, 1995). While SCL has been successfully applied to POS tagging and Sentiment Analysis (Blitzer et al., 2006), its effectiveness for parsing was rather unexplored. • Citation sentences are often neutral with respect to sentiment, either because they describe an algorithm, approach or methodology objectively, or because they are used to support a fac</context>
</contexts>
<marker>Ziman, 1968</marker>
<rawString>J.M. Ziman. 1968. Public Knowledge: An essay concerning the social dimension of science. Cambridge Univ. Press, College Station, Texas.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>