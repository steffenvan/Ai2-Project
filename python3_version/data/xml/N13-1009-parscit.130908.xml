<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000317">
<title confidence="0.9987175">
Extracting the Native Language Signal
for Second Language Acquisition
</title>
<author confidence="0.999306">
Ben Swanson Eugene Charniak
</author>
<affiliation confidence="0.999684">
Brown University Brown University
</affiliation>
<address confidence="0.923067">
Providence, RI Providence, RI
</address>
<email confidence="0.999745">
chonger@cs.brown.edu ec@cs.brown.edu
</email>
<sectionHeader confidence="0.995654" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836764705883">
We develop a method for effective extraction
of linguistic patterns that are differentially ex-
pressed based on the native language of the
author. This method uses multiple corpora
to allow for the removal of data set specific
patterns, and addresses both feature relevancy
and redundancy. We evaluate different rel-
evancy ranking metrics and show that com-
mon measures of relevancy can be inappro-
priate for data with many rare features. Our
feature set is a broad class of syntactic pat-
terns, and to better capture the signal we ex-
tend the Bayesian Tree Substitution Grammar
induction algorithm to a supervised mixture of
latent grammars. We show that this extension
can be used to extract a larger set of relevant
features.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999854469387755">
Native Language Identification (NLI) is a classifi-
cation task in which a statistical signal is exploited
to determine an author’s native language (L1) from
their writing in a second language (L2). This aca-
demic exercise is often motivated not only by fraud
detection or authorship attribution for which L1 can
be an informative feature, but also by its potential to
assist in Second Language Acquisition (SLA).
Our work focuses on the latter application and on
the observation that the actual ability to automati-
cally determine L1 from text is of limited utility in
the SLA domain, where the native language of a stu-
dent is either known or easily solicited. Instead, the
likely role of NLP in the context of SLA is to pro-
vide a set of linguistic patterns that students with
certain L1 backgrounds use with a markedly unusual
frequency. Experiments have shown that such L1
specific information can be incorporated into lesson
plans that improve student performance (Laufer and
Girsai, 2008; Horst et al, 2008).
This is essentially a feature selection task with the
additional caveat that features should be individually
discriminative between native languages in order to
facilitate the construction of focused educational ex-
cersizes. With this goal, we consider metrics for
data set dependence, relevancy, and redundancy. We
show that measures of relevancy based on mutual in-
formation can be inappropriate in problems such as
ours where rare features are important.
While the majority of the methods that we con-
sider generalize to any of the various feature sets
employed in NLI, we focus on the use of Tree Sub-
stitution Grammar rules as features. Obtaining a
compact feature set is possible with the well known
Bayesian grammar induction algorithm (Cohn and
Blunsom, 2010), but its rich get richer dynamics can
make it difficult to find rare features. We extend the
induction model to a supervised mixture of latent
grammars and show how it can be used to incorpo-
rate linguistic knowledge and extract discriminative
features more effectively.
The end result of this technique is a filtered list of
patterns along with their usage statistics. This pro-
vides an enhanced resource for SLA research such
as Jarvis and Crossley (2012) which tackles the man-
ual connection of highly discriminative features with
plausible linguistic transfer explanations. We output
a compact list of language patterns that are empiri-
cally associated with native language labels, avoid-
</bodyText>
<page confidence="0.996921">
85
</page>
<note confidence="0.47011">
Proceedings of NAACL-HLT 2013, pages 85–94,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999546">
ing redundancy and artifacts from the corpus cre-
ation process. We release this list for use by the
linguistics and SLA research communities, and plan
to expand it with upcoming releases of L1 labeled
corpora1.
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999855897435898">
Our work is closely related to the recent surge of re-
search in NLI. Beginning with Koppel et al (2005),
several papers have proposed different feature sets
to be used as predictors of L1 (Tsur and Rappa-
port, 2007; Wong and Dras, 2011a; Swanson and
Charniak, 2012). However, due to the ubiquitous
use of random subsamples, different data prepara-
tion methods, and severe topic and annotation biases
of the data set employed, there is little consensus on
which feature sets are ideal or sufficient, or if any
reported accuracies reflect some generalizable truth
of the problem’s difficulty. To combat the bias of
a single data set, a new strain of work has emerged
in which train and test documents come from dif-
ferent corpora (Brooke and Hirst, 2012; Tetreault et
al, 2012; Bykh and Meurers, 2012). We follow this
cross corpus approach, as it is crucial to any claims
of feature relevance.
Feature selection itself is a well studied problem,
and the most thorough systems address both rele-
vancy and redundancy. While some work tackles
these problems by optimizing a metric over both si-
multaneously (Peng et al, 2005), we decouple the
notions of relevancy and redundancy to allow ad-hoc
metrics for either, similar to the method of Yu and
Liu (2004). The measurement of feature relevancy
in NLI has to this point been handled primarily with
Information Gain, and elimination of feature redun-
dancy has not been considered.
Tree Substitution Grammars have recently been
successfully applied in several domains using the
induction algorithm presented by Cohn and Blun-
som (2010). Our hierarchical treatment builds on
this work by incorporating supervised mixtures over
latent grammars into this induction process. Latent
mixture techniques for NLI have been explored with
other feature types (Wong and Dras, 2011b; Wong
and Dras, 2012), but have not previously led to mea-
surable empirical gains.
</bodyText>
<footnote confidence="0.687468">
1bllip.cs.brown.edu/download/nli corpus.pdf
</footnote>
<sectionHeader confidence="0.938577" genericHeader="method">
3 Corpus Description
</sectionHeader>
<bodyText confidence="0.999953565217391">
We first make explicit our experimental setup in or-
der to provide context for the discussion to follow.
We perform analysis of English text from Chinese,
German, Spanish, and Japanese L1 backgrounds
drawn from four corpora. The first three consist of
responses to essay prompts in educational settings,
while the fourth is submitted by users in an internet
forum.
The first corpus is the International Corpus of
Learner English (ICLE) (Granger et al, 2002), a
mainstay in NLI that has been shown to exhibit a
large topic bias due to correlations between L1 and
the essay prompts used (Brooke and Hirst, 2011).
The second is the International Corpus of Crosslin-
guistic Interlanguage (ICCI) (Tono et al, 2012),
which is annotated with sentence boundaries and has
yet to be used in NLI. The third is the public sample
of the Cambridge International Corpus (FCE), and
consists of short prompted responses. One quirk of
the FCE data is that several responses are written in
the form of letters, leading to skewed distributions
of the specialized syntax involved with use of the
second person. The fourth is the Lang8 data set in-
troduced by Brooke and Hirst (2011). This data set
is free of format, with no prompts or constraints on
writing aids. The samples are often very short and
are qualitatively the most noisy of the four data sets.
One distinctive experimental decision is to treat
each sentence as an individual datum. As document
length can vary dramatically, especially across cor-
pora, this gives increased regularity to the number
of features per data item. More importantly, this
creates a rough correspondence between feature co-
occurrence and the expression of the same under-
lying linguistic phenomenon, which is desirable for
automatic redundancy metrics.
We automatically detect sentence boundaries
when they are not provided, and parse all corpora
with the 6-split Berkeley Parser. As in previous NLI
work, we then replace all word tokens that do not oc-
cur in a list of 614 common words with an unknown
word symbol, UNK.
While these are standard data preprocessing steps,
from our experience with this problem we propose
additional practical considerations. First, we filter
the parsed corpora, retaining only sentences that are
</bodyText>
<page confidence="0.97791">
86
</page>
<figure confidence="0.8864633">
ROOT
S
NP VP
VBZ
loves
NP
NN
DT
NN
man
</figure>
<bodyText confidence="0.999318551724138">
parsed to a Clause Level2 tag. This is primarily due
to the fact that automatic sentence boundary detec-
tors must be used on the ICLE, Lang8, and FCE data
sets, and false positives lead to sentence fragments
that are parsed as NP, VP, FRAG, etc. The wild inter-
net text found in the Lang8 data set also yields many
non-Clause Level parses from non-English text or
emotive punctuation. Sentence detection false neg-
atives, on the other hand, lead to run-on sentences,
and so we additionally remove sentences with more
than 40 words.
We also impose a simple preprocessing step for
better treatment of proper nouns. Due to the geo-
graphic distribution of languages, the proper nouns
used in a writer’s text naturally present a strong L1
signal. The obvious remedy is to replace all proper
nouns with UNK, but this is unfortunately insuffi-
cient as the structure of the proper noun itself can
be a covert signal of these geographical trends. To
fix this, we also remove all proper noun left sisters
of proper nouns. We choose to retain the rightmost
sister node in order to preserve the plurality of the
noun phrase, as the rightmost noun is most likely
the lexical head.
From these parsed, UNKed, and filtered corpora
we draw 2500 sentences from each L1 background
at random, for a total of 10000 sentences per corpus.
The exception is the FCE corpus, from which we
draw 1500 sentences per L1 due to its small size.
</bodyText>
<sectionHeader confidence="0.995176" genericHeader="method">
4 Tree Substitution Grammars
</sectionHeader>
<bodyText confidence="0.999447933333333">
A Tree Substitution Grammar (TSG) is a model
of parse tree derivations that begins with a sin-
gle ROOT nonterminal node and iteratively rewrites
nonterminal leaves until none remain. A TSG
rewrite rule is a tree of any depth, as illustrated in
Figure 1, and can be used as a binary feature of a
parsed sentence that is triggered if the rule appears
in any derivation of that sentence.
Related NLI work compares a plethora of sug-
gested feature sets, ranging from character n-grams
to latent topic activations to labeled dependency
arcs, but TSG rules are best able to represent com-
plex lexical and syntactic behavior in a homoge-
neous feature type. This property is summed up
nicely by the desire for features that capture rather
</bodyText>
<page confidence="0.458243">
2S, SINV, SQ, SBAR, or SBARQ
</page>
<figure confidence="0.95986175">
NN
woman
the
NP
</figure>
<figureCaption confidence="0.9983265">
Figure 1: A Tree Substitution Grammar capable of de-
scribing the feelings of people of all sexual orientations.
</figureCaption>
<bodyText confidence="0.999967552631579">
than cover linguistic phenomena (Johnson, 2012);
while features such as character n-grams, POS tag
sequences, and CFG rules may provide a usable L1
signal, each feature is likely covering some compo-
nent of a pattern instead of capturing it in full. TSG
rules, on the other hand, offer remarkable flexibil-
ity in the patterns that they can represent, potentially
capturing any contiguous parse tree structure.
As it is intractable to rank and filter the entire set
of possible TSG rules given a corpus, we start with
the large subset produced by Bayesian grammar in-
duction. The most widely used algorithm for TSG
induction uses a Dirichlet Process to choose a subset
of frequently reoccurring rules by repeatedly sam-
pling derivations for a corpus of parse trees (Cohn
and Blunsom, 2010). The rich get richer dynamic of
the DP leads to the use of a compact set of rules
that is an effective feature set for NLI (Swanson
and Charniak, 2012). However, this same property
makes rare rules harder to find.
To address this weakness, we define a general
model for TSG induction in labeled documents that
combines a Hierarchical Dirichlet Process (Teh et al,
2005), with supervised labels in a manner similar to
upstream supervised LDA (Mimno and McCallum,
2008). In the context of our work the document label
η indicates both its authors native language L and
data set D. Each η is associated with an observed
Dirichlet prior ν., and a hidden multinomial θ. over
grammars is drawn from this prior. The traditional
grammatical model of nonterminal expansion is aug-
mented such that to rewrite a symbol we first choose
a grammar from the document’s θ. and then choose
a rule from that grammar.
For those unfamiliar with these models, the basic
idea is to jointly estimate a mixture distribution over
grammars for each η, as well as the parameters of
these grammars. The HDP is necessary as the size
</bodyText>
<page confidence="0.993583">
87
</page>
<bodyText confidence="0.999961">
of each of these grammars is essentially infinite. We
can express the generative model formally by defin-
ing the probability of a rule r expanding a symbol s
in a sentence labeled q as
</bodyText>
<equation confidence="0.9991646">
0η — Dir(vη)
ziη — Mul40η)
Hs — DP(-y, P0(•|s))
Gks — DP(αs, Hs)
riηs — Gzz,7s
</equation>
<bodyText confidence="0.999964857142857">
This is closely related to the application of the
Hierarchical Pitman Yor Process used in (Blunsom
and Cohn, 2010) and (Shindo et al, 2012), which
interpolates between multiple coarse and fine map-
pings of the data items being clustered to deal with
sparse data. While the underlying Chinese Restau-
rant Process sampling algorithm is quite similar, our
approach differs in that it models several different
distributions with the same support that share a com-
mon prior.
By careful choice of the number of grammars K,
the Dirichlet priors v, and the backoff concentration
parameter -y, a variety of interesting models can eas-
ily be defined, as demonstrated in our experiments.
</bodyText>
<sectionHeader confidence="0.997953" genericHeader="method">
5 Feature Selection
</sectionHeader>
<subsectionHeader confidence="0.996894">
5.1 Dataset Independence
</subsectionHeader>
<bodyText confidence="0.999968684210526">
The first step in our L1 signal extraction pipeline
controls for patterns that occur too frequently in cer-
tain combinations of native language and data set.
Such patterns arise primarily from the reuse of es-
say prompts in the creation of certain corpora, and
we construct a hard filter to exclude features of this
type.
A simple first choice would be to rank the rules
in order of dependence on the corpus, as we expect
an irregularly represented topic to be confined to a
single data set. However, this misses the subtle but
important point that corpora have different qualities
such as register and author proficiency. Instead we
treat the set of sentences containing an arbitrary fea-
ture X as a set of observations of a pair of categor-
ical random variables L and D, representing native
language and data set respectively.
To see why this treatment is superior, consider the
outcomes for the two hypothetical features shown
</bodyText>
<figure confidence="0.487572666666667">
L1 L2
D1 1000 500 D1 1000 500
D2 100 50 D2 750 750
</figure>
<figureCaption confidence="0.750416">
Figure 2: Two hypothetical feature profiles that illustrate
</figureCaption>
<bodyText confidence="0.986467606060606">
the problems with filtering only on data set independence,
which prefers the right profile over the left. Our method
has the opposite preference.
in Figure 2. The left table has a high data set de-
pendence but exhibits a clean twofold preference for
L1 in both data sets, making it a desirable feature to
retain. Conversely, the right table shows a feature
where the distribution is uniform over data sets, but
has language preference in only one. This is a sign
of either a large variance in usage or some data set
specific tendency, and in either case we can not make
confident claims as to this feature’s association with
any native language.
The L-D dependence can be measured with Pear-
son’s x2 test, although the specifics of its use as
a filter deserve some discussion. As we eliminate
the features for which the null hypothesis of inde-
pendence is rejected, our noisy data will cause us
to overzealously reject. In order to prevent the un-
neccesary removal of interesting patterns, we use a
very small p value as a cutoff point for rejection. In
all of our experiments the x2 value corresponding to
p &lt; .001 is in the twenties; we use x2 &gt; 100 as our
criteria for rejection.
Another possible source of error is the sparsity of
some features in our data. To avoid making pre-
dictions of rules for which we have not observed
a sufficient number of examples, we automatically
exclude any rule with a count less than five for any
L-D combination q. This also satisfies the common
requirements for validity of the x2 test that require
a minimum number of 5 expected counts for every
outcome.
</bodyText>
<subsectionHeader confidence="0.958343">
5.2 Relevancy
</subsectionHeader>
<bodyText confidence="0.9993485">
We next rank the features in terms of their ability to
discriminate between L1 labels. We consider three
relevancy ranking metrics: Information Gain (IG),
Symmetric Uncertainty (SU), and x2 statistic.
</bodyText>
<equation confidence="0.733939">
L1 L2
</equation>
<page confidence="0.867048">
88
</page>
<figure confidence="0.7354485">
IG SU x2
r .84 .72 .15
</figure>
<figureCaption confidence="0.864151">
Figure 3: Sample Pearson correlation coefficients be-
tween different ranking functions and feature frequency
over a large set of TSG features.
</figureCaption>
<equation confidence="0.959682833333333">
IG(L, Xi) = H(L) − H(L|Xi)
5U(L, Xi) = 2 IG(L, Xi)
H(L) + H(Xi)
(nim − NiM )2
Ni
M
</equation>
<bodyText confidence="0.999516682926829">
We define L as the Multinomial distributed L1 la-
bel taking values in {1, ..., M} and Xi as a Bernoulli
distributed indicator of the presence or absence of
the ith feature, which we represent with the events
X+i and X. i respectively. We use the Maximum
Likelihood estimates of these distributions from the
training data to compute the necessary entropies for
IG and SU. For the x2 metric we use nim, the count
of sentences with L1 label m that contain feature Xi,
and their sum over classes Ni.
While SU is often preferred over IG in feature se-
lection for several reasons, their main difference in
the context of selection of binary features is the addi-
tion of H(Xi) in the denominator, leading to higher
values for rare features under SU. This helps to
counteract a subtle preference for common features
that these metrics can exhibit in data such as ours, as
shown in Figure 3. The source of this preference is
the overwhelming contribution of p(XZ )H(L|XZ )
in IG(L, Xi) for rare features, which will be essen-
tially the maximum value of log(M). In most clas-
sification problems a frequent feature bias is a desir-
able trait, as a rare feature is naturally less likely to
appear and contribute to decision making.
We note that binary features in sentences are
sparsely observed, as the opportunity for use of the
majority of patterns will not exist in any given sen-
tence. This leads to a large number of rare features
that are nevertheless indicative of their author’s L1.
The x2 statistic we employ is better suited to retain
such features as it only deals with counts of sen-
tences containing Xi.
The ranking behavior of these metrics is high-
lighted in Figure 4. We expect that features with
profiles like Xa and Xb will be more useful than
those like Xd, and only x2 ranks these features ac-
cordingly. Another view of the difference between
the metrics is taken in Figure 5. As shown in the
left plot, IG and 5U are nearly identical for the
most highly ranked features and significantly differ-
ent from x2.
</bodyText>
<table confidence="0.9809438">
L1 L2 L3 L4 IG SU x2
X. 20 5 5 5 .0008 .0012 19.29
Xb 40 20 20 20 .0005 .0008 12.0
X. 2000 500 500 500 .0178 .0217 385.7
Xd 1700 1800 1700 1800 .0010 .0010 5.71
</table>
<figureCaption confidence="0.99685">
Figure 4: Four hypothetical features in a 4 label clas-
sification problem, with the number of training items
from each class using the feature listed in the first four
columns. The top three features under each ranking are
shown in bold.
Figure 5: For all pairs of relevancy metrics, we show the
</figureCaption>
<bodyText confidence="0.782078333333333">
number of features that appear in the top n of both. The
result for low n is highlighted in the left plot, showing a
high similarity between SU and IG.
</bodyText>
<subsectionHeader confidence="0.997066">
5.3 Redundancy
</subsectionHeader>
<bodyText confidence="0.999873222222222">
The second component of thorough feature selection
is the removal of redundant features. From an ex-
perimental point of view, it is inaccurate to compare
feature selection systems under evaluation of the top
n features or the number of features with ranking
statistic at or beyond some threshold if redundancy
has not been taken into account. Furthermore, as
our stated goal is a list of discriminative patterns,
multiple representations of the same pattern clearly
</bodyText>
<figure confidence="0.988585592592593">
X-IG
X-SU
SU-IG
0 10 20 30 40 50
Top n features
0 50 100 150 200 250 300
Top n features
300
250
200
150
100
50
0
X-IG
X-SU
SU-IG
50
40
30
20
10
# of shared features
0
�
x2(Xi) =
m
</figure>
<page confidence="0.997972">
89
</page>
<bodyText confidence="0.999953590909091">
degrade the quality of our output. This is especially
necessary when using TSG rules as features, as it is
possible to define many slightly different rules that
essentially represent the same linguistic act.
Redundancy detection must be able to both deter-
mine that a set of features are redundant and also
select the feature to retain from such a set. We use
a greedy method that allows us to investigate differ-
ent relevancy metrics for selection of the representa-
tive feature for a redundant set (Yu and Liu, 2004).
The algorithm begins with a list S containing the
full list of features, sorted by an arbitrary metric of
relevancy. While S is not empty, the most relevant
feature X* in S is selected for retention, and all fea-
tures Xi are removed from S if R(X*, Xi) &gt; ρ for
some redundancy metric R and some threshold ρ.
We consider two probabilistic metrics for redun-
dancy detection, the first being SU, as defined in
the previous section. We contrast this metric with
Normalized Pointwise Mutual Information (NPMI)
which uses only the events A = X+a and B = Xb+
and has a range of [-1,1].
</bodyText>
<equation confidence="0.990826">
log(P(A|B)) − log(P (A))
NPMI(Xa, Xb) = − log(P (A, B))
</equation>
<bodyText confidence="0.9991045">
Another option that we explore is the structural
redundancy between TSG rules themselves. We de-
fine a 0-1 redundancy metric such that R(Xa, Xb) is
one if there exists a fragment that contains both Xa
and Xb with a total number of CFG rules less than
the sum of the number of CFG rules in Xa and Xb.
The latter constraint ensures that Xa and Xb overlap
in the containing fragment. Note that this is not the
same as a nonempty set intersection of CFG rules,
as can be seen in Figure 6.
</bodyText>
<figureCaption confidence="0.652791">
Figure 6: Three similar fragments that highlight the be-
havior of the structural redundancy metric; the first two
fragments are not considered redundant, while the third
is made redundant by either of the others.
</figureCaption>
<sectionHeader confidence="0.997093" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997965">
6.1 Relevancy Metrics
</subsectionHeader>
<bodyText confidence="0.999942055555556">
The traditional evaluation criterion for a feature se-
lection system such as ours is classification accuracy
or expected risk. However, as our desired output is
not a set of features that capture a decision bound-
ary as an ensemble, a per feature risk evaluation bet-
ter quantifies the performance of a system for our
purposes. We plot average risk against number of
predicted features to view the rate of quality degra-
dation under a relevancy measure to give a picture
of a each metric’s utility.
The per feature risk for a feature X is an eval-
uation of the ML estimate of PX(L) = P(L|X+)
from the training data on TX, the test sentences that
contain the feature X. The decision to evaluate only
sentences in which the feature occurs removes an
implicit bias towards more common features.
We calculate the expected risk R(X) using a 0-1
loss function, averaging over TX.
</bodyText>
<equation confidence="0.856395">
R(X) = |1 1:PX(L =6 L*t)
</equation>
<bodyText confidence="0.999946173913044">
where L*t is the gold standard L1 label of test item
t. This metric has two important properties. First,
given any true distribution over class labels in TX,
the best possible PX(L) is the one that matches
these proportions exactly, ensuring that preferred
features make generalizable predictions. Second, it
assigns less risk to rules with lower entropy, as long
as their predictions remain generalizable. This cor-
responds to features that find larger differences in
usage frequency across L1 labels.
The alternative metric of per feature classifica-
tion accuracy creates a one to one mapping between
features and native languages. This unnecessarily
penalizes features that are associated with multiple
native languages, as well as features that are selec-
tively dispreferred by certain L1 speakers. Also, we
wish to correctly quantify the distribution of a fea-
ture over all native languages, which goes beyond
correct prediction of the most probable.
Using cross validation with each corpus as a fold,
we plot the average R(X) for the best n features
against n for each relevancy metric in Figure 7. This
clearly shows that for highly ranked features χ2 is
</bodyText>
<figure confidence="0.9840682">
NN
NP
S
VP
PRP
NP
S
VP
S
NP VP
VBZ
tETx
90
0 20 40 60 80 100 120 140 160 180 200
Top n features
</figure>
<figureCaption confidence="0.956363666666667">
Figure 7: Per-feature Average Expected Loss plotted
against top N features using x2, IG, and SU as a rele-
vancy metric
</figureCaption>
<bodyText confidence="0.996164375">
able to best single out the type of features we de-
sire. Another point to be taken from the plot is
that it is that the top ten features under SU are
remarkably inferior. Inspection of these rules re-
veals that they are precisely the type of overly fre-
quent but only slightly discriminative features that
we predicted would corrupt feature selection using
IG based measures.
</bodyText>
<subsectionHeader confidence="0.999156">
6.2 Redundancy Metrics
</subsectionHeader>
<bodyText confidence="0.999991">
We evaluate the redundancy metrics by using the top
n features retained by redundancy filtering for en-
semble classification. Under this evaluation, if re-
dundancy is not being effectively eliminated perfor-
mance should increase more slowly with n as the
set of test items that can be correctly classified re-
mains relatively constant. Additionally, if the metric
is overzealous in its elimination of redundancy, use-
ful patterns will be eliminated leading to diminished
increase in performance. Figure 8 shows the tradeoff
between Expected Loss on the test set and the num-
ber of features used with SU, NPMI, and the overlap
based structural redundancy metric described above.
We performed a coarse grid search to find the opti-
mal values of p for SU and NPMI.
Both the structural overlap hueristic and SU per-
form similarly, and outperform NPMI. Analysis re-
veals that NPMI seems to overstate the similarity of
large fragments with their small subcomponents. We
choose to proceed with SU, as it is not only faster in
our implementation but also can generalize to fea-
ture types beyond TSG rules.
</bodyText>
<figure confidence="0.9745865">
0 50 100 150 200 250 300
Top n features
</figure>
<figureCaption confidence="0.7855975">
Figure 8: The effects of redundancy filtering on classi-
fication performance using different redundancy metrics.
The cutoff values (p) used for SU and NPMI are .2 and .7
respectively.
</figureCaption>
<subsectionHeader confidence="0.992499">
6.3 TSG Induction
</subsectionHeader>
<bodyText confidence="0.999001774193548">
We demonstrate the flexibility and effectiveness of
our general model of mixtures of TSGs for labeled
data by example. The tunable parameters are the
number of grammars K, the Dirichlet priors v. over
grammar distributions for each label q, and the con-
centration parameter -y of the smoothing DP.
For a first baseline we set the number of grammars
K = 1, making the Dirichlet priors v irrelevant.
With a large -y = 1020, we essentially recover the
basic block sampling algorithm of Cohn and Blun-
som (2010). We refer to this model as M1. Our
second baseline model, M2, sets K to the number of
native language labels, and sets the v variables such
that each q is mapped to a single grammar by its L1
label, creating a naive Bayes model. For M2 and
the subsequent models we use -y = 1000 to allow
moderate smoothing.
We also construct a model (M3) in which we set
K = 9 and v. is such that three grammars are likely
for any single q; one shared by all q with the same
L1 label, one shared by all q with the same corpus
label, and one shared by all q. We compare this with
another K = 9 model (M4) where the v are set to
be uniform across all 9 grammars.
We evaluate these systems on the percent of their
resulting grammar that rejects the hypothesis of lan-
guage independence using a x2 test. Slight adjust-
ments were made to α for these models to bring
their output grammar size into the range of approxi-
mately 12000 rules. We average our results for each
model over single states drawn from five indepen-
</bodyText>
<figure confidence="0.998108826086956">
Average Expected Loss
0.75
0.74
0.73
0.72
0.71
0.69
0.7
X2
IG
SU
0.73
0.72
0.71
0.7
0.69
0.68
0.67
0.66
overlap
SU
NPMI
Expected Loss
</figure>
<page confidence="0.985477">
91
</page>
<table confidence="0.986809">
p &lt; .1 p &lt; .05 p &lt; .01 p &lt; .001
56.5(3.1) 54.5(3.0) 49.8(2.7) 45.1(2.5)
55.3(3.7) 53.7(3.6) 49.1(3.3) 44.7(3.0)
59.0(4.1) 57.2(4.1) 52.4(3.6) 48.4(3.3)
58.9(3.8) 57.0(3.7) 51.9(3.4) 47.2(3.1)
</table>
<figureCaption confidence="0.998644833333333">
Figure 9: The percentage of rules from each model that
reject L1 independence at varying levels of statistical sig-
nificance. The first number is with respect to the number
rules that pass the L1/corpus independence and redun-
dancy tests, and the second is in proportion to the full list
returned by grammar induction.
</figureCaption>
<bodyText confidence="0.99543625">
dent Markov chains.
Our results in Figure 9 show that using a mixture
of grammars allows the induction algorithm to find
more patterns that fit arbitrary criteria for language
dependence. The intuition supporting this is that in
simpler models a given grammar must represent a
larger amount of data that is better represented with
more vague, general purpose rules. Dividing the re-
sponsibility among several grammars lets rare pat-
terns form clusters more easily. The incorporation of
informed structure in M3 further improves the per-
formance of this latent mixture technique.
</bodyText>
<sectionHeader confidence="0.999521" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999971612903226">
Using these methods, we produce a list of L1 as-
sociated TSG rules that we release for public use.
We perform grammar induction using model M3,
apply our data dependence and redundancy filters,
rank for relevancy using x2 and filter at the level of
p &lt; .1 statistical significance for relevancy. Each
entry consists of a TSG rule and its matrix of counts
with each q. We provide the total for each L1 la-
bel, which shows the overall prediction of the pro-
portional use of that item. We also provide the x2
statistics for L1 dependence and the dependence of
L1 and corpus.
It is speculative to assign causes to the discrimi-
native rules we report, and we leave quantification
of such statements to future work. However, the
strength of the signal, as evidenced by actual counts
in data, and the high level interpretation that can be
easily assigned to the TSG rules is promising. As
understanding the features requires basic knowledge
of Treebank symbols, we provide our interpretations
for some of the more interesting rules and summa-
rize their L1 distributions. Note that by describing a
rule as being preferred by a certain set of L1 labels,
our claim is relative to the other labels only; the true
cause could also be a dispreference in the comple-
ment of this set.
One interesting comparison made easy by our
method is the identification of similar structures that
have complementary L1 usage. An example is the
use of a prepositional phrase just before the first
noun phrase in a sentence, which is preferred in Ger-
man and Spanish, especially in the former. However,
German speakers disprefer a prepositional phrase
followed by a comma at the beginning of the sen-
tence, and Chinese speakers use this pattern more
frequently than the other L1s. Another contrastable
pair is the use of the word “because” with upper or
lower case, signifying sentence initial or medial use.
The former is preferred in Chinese and Japanese
text, while the latter is preferred in German and even
more so in Spanish L1 data.
As these examples suggest, the data shows a
strong division of preference between European
and Asian languages, but many patterns exist that
are uniquely preferred in single languages as well.
Japanese speakers are seen to frequently use a per-
sonal pronoun as the subject of the sentence, while
Spanish speakers use the phrase “the X of Y”, the
verb “go”, and the determiner “this” with markedly
higher frequency. Germans tend to begin sentences
with adverbs, and various modal verb constructions
are popular with Chinese speakers. We suspect these
patterns to be evidence of preference in the speci-
fied language, rather than dispreference in the other
three.
Our strategy in regard to the hard filters for L1-
corpus dependence and redundancy has been to pre-
fer recall to precision, as false positives can be easily
ignored through subsequent inspection of the data
we supply. This makes the list suitable for human
qualitative analysis, but further work is required for
its use in downstream automatic systems.
</bodyText>
<sectionHeader confidence="0.998303" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9946165">
This work contributes to the goal of leveraging NLI
data in SLA applications. We provide evidence for
</bodyText>
<note confidence="0.73141425">
M1
M2
M3
M4
</note>
<page confidence="0.958049">
92
</page>
<bodyText confidence="0.999963137931034">
our hypothesis that relevancy metrics based on mu-
tual information are ill-suited for this task, and rec-
ommend the use of the x2 statistic for rejecting the
hypothesis of language independence. Explicit con-
trols for dependence between L1 and corpus are
proposed, and redundancy between features are ad-
dressed as well. We argue for the use of TSG rules as
features, and develop an induction algorithm that is
a supervised mixture of hierarchical grammars. This
generalizable formalism is used to capture linguistic
assumptions about the data and increase the amount
of relevant features extracted at several thresholds.
This project motivates continued incorporation of
more data and induction of TSGs over these larger
data sets. This will improve the quality and scope of
the resulting list of discriminative syntax, allowing
broader use in linguistics and SLA research. The
prospect of high precision and recall in the extrac-
tion of such patterns suggests several interesting av-
enues for future work, such as determination of the
actual language transfer phenomena evidenced by an
arbitrary count profile. To achieve the goal of auto-
matic detection of plausible transfer the native lan-
guages themselves must be considered, as well as a
way to distinguish between preference and dispref-
erence based on usage statistics. Another exciting
application of such a refined list of patterns is the
automatic integration of its features in L1 targeted
SLA software.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999039732394366">
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. Empirical Methods in Natural Lan-
guage Processing.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ‘cheap’ learner corpora. Conference of
Learner Corpus Research.
Julian Brooke and Graeme Hirst. 2012. Measuring In-
terlanguage: Native Language Identification with L1-
influence Metrics. LREC
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. COLING.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification Using Recurring N-grams - Inves-
tigating Abstraction and Domain Dependence. COL-
ING.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing Compact but Accurate Tree-
Substitution Grammars. In Proceedings NAACL.
Trevor Cohn, and Phil Blunsom. 2010. Blocked infer-
ence in Bayesian tree substitution grammars. Associa-
tion for Computational Linguistics.
Gilquin, Ga¨etanelle and Granger, Sylviane. 2011. From
EFL to ESL: Evidence from the International Corpus
of Learner English. Exploring Second-Language Va-
rieties of English and Learner Englishes: Bridging a
Paradigm Gap (Book Chapter).
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod et al. chapter 8..
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus of Learner English, (ICLE).
Horst M., White J., Bell P. 2010. First and second lan-
guage knowledge in the language classroom. Interna-
tional Journal of Bilingualism.
Scott Jarvis and Scott Crossley 2012. Approaching Lan-
guage Transfer through Text Classification.
Mark Johnson 2011. How relevant is linguistics to com-
putational linguistics?. Linguistic Issues in Language
Technology.
Ekaterina Kochmar. 2011. Identification of a writer’s
native language by error analysis. Master’s Thesis.
Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir.
2005. Determining an author’s native language by
mining a text for errors. Proceedings of the eleventh
ACM SIGKDD international conference on Knowl-
edge discovery in data mining.
Laufer, B and Girsai, N. 2008. Form-focused Instruction
in Second Language Vocabulary Learning: A Case for
Contrastive Analysis and Translation. Applied Lin-
guistics.
David Mimno and Andrew McCallum. 2008. Topic
Models Conditioned on Arbitrary Features with
Dirichlet-multinomial Regression. UAI.
Hanchuan Peng and Fuhui Long and Chris Ding. 2005.
Feature selection based on mutual information cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. Association for Compu-
tational Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. Association for Com-
putational Linguistics.
Tono, Y., Kawaguchi, Y. &amp; Minegishi, M. (eds.) . 2012.
Developmental and Cross-linguistic Perspectives in
Learner Corpus Research..
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language on
the choice of written second language words. CACLA.
</reference>
<page confidence="0.98324">
93
</page>
<reference confidence="0.999535352941177">
Shindo, Hiroyuki and Miyao, Yusuke and Fujino, Aki-
nori and Nagata, Masaaki 2012. Bayesian Symbol-
Refined Tree Substitution Grammars for Syntactic
Parsing. Association for Computational Linguistics.
Ben Swanson and Eugene Charniak. 2012. Native
Language Detection with Tree Substitution Grammars.
Association for Computational Linguistics.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2005. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, Beata
Beigman-Klebanov and Martin Chodorow. 2012. Na-
tive Tongues, Lost and Found: Resources and Em-
pirical Evaluations in Native Language Identification.
COLING.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing Parse Structures for Native Language Identifica-
tion. Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing.
Sze-Meng Jojo Wong and Mark Dras. 2011. Topic Mod-
eling for Native Language Identification. Proceedings
of the Australasian Language Technology Association
Workshop.
Sze-Meng Jojo Wong, Mark Dras, Mark Johnson. 2012.
Exploring Adaptor Grammars for Native Language
Identification. EMNLP-CoNLL.
Lei Yu and Huan Liu. 2004. Efficient Feature Selection
via Analysis of Relevance and Redundancy. Journal
of Machine Learning Research.
</reference>
<page confidence="0.99955">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875075">
<title confidence="0.999803">Extracting the Native Language for Second Language Acquisition</title>
<author confidence="0.999754">Ben Swanson Eugene Charniak</author>
<affiliation confidence="0.999994">Brown University Brown University</affiliation>
<address confidence="0.958108">Providence, RI Providence, RI</address>
<email confidence="0.99846">chonger@cs.brown.eduec@cs.brown.edu</email>
<abstract confidence="0.995263555555556">We develop a method for effective extraction of linguistic patterns that are differentially expressed based on the native language of the author. This method uses multiple corpora to allow for the removal of data set specific patterns, and addresses both feature relevancy and redundancy. We evaluate different relevancy ranking metrics and show that common measures of relevancy can be inappropriate for data with many rare features. Our feature set is a broad class of syntactic patterns, and to better capture the signal we extend the Bayesian Tree Substitution Grammar induction algorithm to a supervised mixture of latent grammars. We show that this extension can be used to extract a larger set of relevant features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. Empirical Methods in Natural Language Processing.</title>
<date>2010</date>
<contexts>
<context position="12585" citStr="Blunsom and Cohn, 2010" startWordPosition="2086" endWordPosition="2089">then choose a rule from that grammar. For those unfamiliar with these models, the basic idea is to jointly estimate a mixture distribution over grammars for each η, as well as the parameters of these grammars. The HDP is necessary as the size 87 of each of these grammars is essentially infinite. We can express the generative model formally by defining the probability of a rule r expanding a symbol s in a sentence labeled q as 0η — Dir(vη) ziη — Mul40η) Hs — DP(-y, P0(•|s)) Gks — DP(αs, Hs) riηs — Gzz,7s This is closely related to the application of the Hierarchical Pitman Yor Process used in (Blunsom and Cohn, 2010) and (Shindo et al, 2012), which interpolates between multiple coarse and fine mappings of the data items being clustered to deal with sparse data. While the underlying Chinese Restaurant Process sampling algorithm is quite similar, our approach differs in that it models several different distributions with the same support that share a common prior. By careful choice of the number of grammars K, the Dirichlet priors v, and the backoff concentration parameter -y, a variety of interesting models can easily be defined, as demonstrated in our experiments. 5 Feature Selection 5.1 Dataset Independe</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<date>2011</date>
<booktitle>Native language detection with ‘cheap’ learner corpora. Conference of Learner Corpus Research.</booktitle>
<contexts>
<context position="6324" citStr="Brooke and Hirst, 2011" startWordPosition="1011" endWordPosition="1014">us Description We first make explicit our experimental setup in order to provide context for the discussion to follow. We perform analysis of English text from Chinese, German, Spanish, and Japanese L1 backgrounds drawn from four corpora. The first three consist of responses to essay prompts in educational settings, while the fourth is submitted by users in an internet forum. The first corpus is the International Corpus of Learner English (ICLE) (Granger et al, 2002), a mainstay in NLI that has been shown to exhibit a large topic bias due to correlations between L1 and the essay prompts used (Brooke and Hirst, 2011). The second is the International Corpus of Crosslinguistic Interlanguage (ICCI) (Tono et al, 2012), which is annotated with sentence boundaries and has yet to be used in NLI. The third is the public sample of the Cambridge International Corpus (FCE), and consists of short prompted responses. One quirk of the FCE data is that several responses are written in the form of letters, leading to skewed distributions of the specialized syntax involved with use of the second person. The fourth is the Lang8 data set introduced by Brooke and Hirst (2011). This data set is free of format, with no prompts</context>
</contexts>
<marker>Brooke, Hirst, 2011</marker>
<rawString>Julian Brooke and Graeme Hirst. 2011. Native language detection with ‘cheap’ learner corpora. Conference of Learner Corpus Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<title>Measuring Interlanguage: Native Language Identification with L1-influence Metrics.</title>
<date>2012</date>
<publisher>LREC</publisher>
<contexts>
<context position="4509" citStr="Brooke and Hirst, 2012" startWordPosition="723" endWordPosition="726">roposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Informati</context>
</contexts>
<marker>Brooke, Hirst, 2012</marker>
<rawString>Julian Brooke and Graeme Hirst. 2012. Measuring Interlanguage: Native Language Identification with L1-influence Metrics. LREC</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<title>Robust, Lexicalized Native Language Identification.</title>
<date>2012</date>
<publisher>COLING.</publisher>
<contexts>
<context position="4509" citStr="Brooke and Hirst, 2012" startWordPosition="723" endWordPosition="726">roposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Informati</context>
</contexts>
<marker>Brooke, Hirst, 2012</marker>
<rawString>Julian Brooke and Graeme Hirst. 2012. Robust, Lexicalized Native Language Identification. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serhiy Bykh</author>
<author>Detmar Meurers</author>
</authors>
<title>Native Language Identification Using Recurring N-grams - Investigating Abstraction and Domain Dependence.</title>
<date>2012</date>
<publisher>COLING.</publisher>
<contexts>
<context position="4557" citStr="Bykh and Meurers, 2012" startWordPosition="731" endWordPosition="734">edictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and elimination of feature redundancy h</context>
</contexts>
<marker>Bykh, Meurers, 2012</marker>
<rawString>Serhiy Bykh and Detmar Meurers. 2012. Native Language Identification Using Recurring N-grams - Investigating Abstraction and Domain Dependence. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing Compact but Accurate TreeSubstitution Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings NAACL.</booktitle>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing Compact but Accurate TreeSubstitution Grammars. In Proceedings NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked inference in Bayesian tree substitution grammars. Association for Computational Linguistics.</title>
<date>2010</date>
<contexts>
<context position="2719" citStr="Cohn and Blunsom, 2010" startWordPosition="431" endWordPosition="434">anguages in order to facilitate the construction of focused educational excersizes. With this goal, we consider metrics for data set dependence, relevancy, and redundancy. We show that measures of relevancy based on mutual information can be inappropriate in problems such as ours where rare features are important. While the majority of the methods that we consider generalize to any of the various feature sets employed in NLI, we focus on the use of Tree Substitution Grammar rules as features. Obtaining a compact feature set is possible with the well known Bayesian grammar induction algorithm (Cohn and Blunsom, 2010), but its rich get richer dynamics can make it difficult to find rare features. We extend the induction model to a supervised mixture of latent grammars and show how it can be used to incorporate linguistic knowledge and extract discriminative features more effectively. The end result of this technique is a filtered list of patterns along with their usage statistics. This provides an enhanced resource for SLA research such as Jarvis and Crossley (2012) which tackles the manual connection of highly discriminative features with plausible linguistic transfer explanations. We output a compact list</context>
<context position="5333" citStr="Cohn and Blunsom (2010)" startWordPosition="855" endWordPosition="859">most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and elimination of feature redundancy has not been considered. Tree Substitution Grammars have recently been successfully applied in several domains using the induction algorithm presented by Cohn and Blunsom (2010). Our hierarchical treatment builds on this work by incorporating supervised mixtures over latent grammars into this induction process. Latent mixture techniques for NLI have been explored with other feature types (Wong and Dras, 2011b; Wong and Dras, 2012), but have not previously led to measurable empirical gains. 1bllip.cs.brown.edu/download/nli corpus.pdf 3 Corpus Description We first make explicit our experimental setup in order to provide context for the discussion to follow. We perform analysis of English text from Chinese, German, Spanish, and Japanese L1 backgrounds drawn from four co</context>
<context position="11102" citStr="Cohn and Blunsom, 2010" startWordPosition="1823" endWordPosition="1826">feature is likely covering some component of a pattern instead of capturing it in full. TSG rules, on the other hand, offer remarkable flexibility in the patterns that they can represent, potentially capturing any contiguous parse tree structure. As it is intractable to rank and filter the entire set of possible TSG rules given a corpus, we start with the large subset produced by Bayesian grammar induction. The most widely used algorithm for TSG induction uses a Dirichlet Process to choose a subset of frequently reoccurring rules by repeatedly sampling derivations for a corpus of parse trees (Cohn and Blunsom, 2010). The rich get richer dynamic of the DP leads to the use of a compact set of rules that is an effective feature set for NLI (Swanson and Charniak, 2012). However, this same property makes rare rules harder to find. To address this weakness, we define a general model for TSG induction in labeled documents that combines a Hierarchical Dirichlet Process (Teh et al, 2005), with supervised labels in a manner similar to upstream supervised LDA (Mimno and McCallum, 2008). In the context of our work the document label η indicates both its authors native language L and data set D. Each η is associated </context>
<context position="25944" citStr="Cohn and Blunsom (2010)" startWordPosition="4443" endWordPosition="4447">ifferent redundancy metrics. The cutoff values (p) used for SU and NPMI are .2 and .7 respectively. 6.3 TSG Induction We demonstrate the flexibility and effectiveness of our general model of mixtures of TSGs for labeled data by example. The tunable parameters are the number of grammars K, the Dirichlet priors v. over grammar distributions for each label q, and the concentration parameter -y of the smoothing DP. For a first baseline we set the number of grammars K = 1, making the Dirichlet priors v irrelevant. With a large -y = 1020, we essentially recover the basic block sampling algorithm of Cohn and Blunsom (2010). We refer to this model as M1. Our second baseline model, M2, sets K to the number of native language labels, and sets the v variables such that each q is mapped to a single grammar by its L1 label, creating a naive Bayes model. For M2 and the subsequent models we use -y = 1000 to allow moderate smoothing. We also construct a model (M3) in which we set K = 9 and v. is such that three grammars are likely for any single q; one shared by all q with the same L1 label, one shared by all q with the same corpus label, and one shared by all q. We compare this with another K = 9 model (M4) where the v</context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn, and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ga¨etanelle Gilquin</author>
<author>Sylviane Granger</author>
</authors>
<title>From EFL to ESL: Evidence from the International Corpus of Learner English. Exploring Second-Language Varieties of English and Learner Englishes: Bridging a Paradigm Gap (Book Chapter).</title>
<date>2011</date>
<marker>Gilquin, Granger, 2011</marker>
<rawString>Gilquin, Ga¨etanelle and Granger, Sylviane. 2011. From EFL to ESL: Evidence from the International Corpus of Learner English. Exploring Second-Language Varieties of English and Learner Englishes: Bridging a Paradigm Gap (Book Chapter).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<note>In Bod et al. chapter 8..</note>
<marker>Goodman, 2003</marker>
<rawString>Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Bod et al. chapter 8..</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>E Dagneaux</author>
<author>F Meunier</author>
</authors>
<date>2002</date>
<booktitle>International Corpus of Learner English,</booktitle>
<location>(ICLE).</location>
<contexts>
<context position="6172" citStr="Granger et al, 2002" startWordPosition="983" endWordPosition="986"> and Dras, 2011b; Wong and Dras, 2012), but have not previously led to measurable empirical gains. 1bllip.cs.brown.edu/download/nli corpus.pdf 3 Corpus Description We first make explicit our experimental setup in order to provide context for the discussion to follow. We perform analysis of English text from Chinese, German, Spanish, and Japanese L1 backgrounds drawn from four corpora. The first three consist of responses to essay prompts in educational settings, while the fourth is submitted by users in an internet forum. The first corpus is the International Corpus of Learner English (ICLE) (Granger et al, 2002), a mainstay in NLI that has been shown to exhibit a large topic bias due to correlations between L1 and the essay prompts used (Brooke and Hirst, 2011). The second is the International Corpus of Crosslinguistic Interlanguage (ICCI) (Tono et al, 2012), which is annotated with sentence boundaries and has yet to be used in NLI. The third is the public sample of the Cambridge International Corpus (FCE), and consists of short prompted responses. One quirk of the FCE data is that several responses are written in the form of letters, leading to skewed distributions of the specialized syntax involved</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, 2002</marker>
<rawString>S. Granger, E. Dagneaux and F. Meunier. 2002. International Corpus of Learner English, (ICLE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Horst</author>
<author>J White</author>
<author>P Bell</author>
</authors>
<title>First and second language knowledge in the language classroom.</title>
<date>2010</date>
<journal>International Journal of Bilingualism.</journal>
<marker>Horst, White, Bell, 2010</marker>
<rawString>Horst M., White J., Bell P. 2010. First and second language knowledge in the language classroom. International Journal of Bilingualism.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Jarvis</author>
<author>Scott Crossley</author>
</authors>
<title>Approaching Language Transfer through Text Classification.</title>
<date>2012</date>
<contexts>
<context position="3175" citStr="Jarvis and Crossley (2012)" startWordPosition="507" endWordPosition="510">Tree Substitution Grammar rules as features. Obtaining a compact feature set is possible with the well known Bayesian grammar induction algorithm (Cohn and Blunsom, 2010), but its rich get richer dynamics can make it difficult to find rare features. We extend the induction model to a supervised mixture of latent grammars and show how it can be used to incorporate linguistic knowledge and extract discriminative features more effectively. The end result of this technique is a filtered list of patterns along with their usage statistics. This provides an enhanced resource for SLA research such as Jarvis and Crossley (2012) which tackles the manual connection of highly discriminative features with plausible linguistic transfer explanations. We output a compact list of language patterns that are empirically associated with native language labels, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1. 2 Related Work Our work is c</context>
</contexts>
<marker>Jarvis, Crossley, 2012</marker>
<rawString>Scott Jarvis and Scott Crossley 2012. Approaching Language Transfer through Text Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>How relevant is linguistics to computational linguistics?. Linguistic Issues in Language Technology.</title>
<date>2011</date>
<marker>Johnson, 2011</marker>
<rawString>Mark Johnson 2011. How relevant is linguistics to computational linguistics?. Linguistic Issues in Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Kochmar</author>
</authors>
<title>Identification of a writer’s native language by error analysis.</title>
<date>2011</date>
<tech>Master’s Thesis.</tech>
<marker>Kochmar, 2011</marker>
<rawString>Ekaterina Kochmar. 2011. Identification of a writer’s native language by error analysis. Master’s Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Determining an author’s native language by mining a text for errors.</title>
<date>2005</date>
<booktitle>Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining.</booktitle>
<contexts>
<context position="3864" citStr="Koppel et al (2005)" startWordPosition="615" endWordPosition="618"> with plausible linguistic transfer explanations. We output a compact list of language patterns that are empirically associated with native language labels, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1. 2 Related Work Our work is closely related to the recent surge of research in NLI. Beginning with Koppel et al (2005), several papers have proposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come f</context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Koppel, Moshe and Schler, Jonathan and Zigdon, Kfir. 2005. Determining an author’s native language by mining a text for errors. Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Laufer</author>
<author>N Girsai</author>
</authors>
<title>Form-focused Instruction in Second Language Vocabulary Learning: A Case for Contrastive Analysis and Translation. Applied Linguistics.</title>
<date>2008</date>
<contexts>
<context position="1934" citStr="Laufer and Girsai, 2008" startWordPosition="306" endWordPosition="309"> assist in Second Language Acquisition (SLA). Our work focuses on the latter application and on the observation that the actual ability to automatically determine L1 from text is of limited utility in the SLA domain, where the native language of a student is either known or easily solicited. Instead, the likely role of NLP in the context of SLA is to provide a set of linguistic patterns that students with certain L1 backgrounds use with a markedly unusual frequency. Experiments have shown that such L1 specific information can be incorporated into lesson plans that improve student performance (Laufer and Girsai, 2008; Horst et al, 2008). This is essentially a feature selection task with the additional caveat that features should be individually discriminative between native languages in order to facilitate the construction of focused educational excersizes. With this goal, we consider metrics for data set dependence, relevancy, and redundancy. We show that measures of relevancy based on mutual information can be inappropriate in problems such as ours where rare features are important. While the majority of the methods that we consider generalize to any of the various feature sets employed in NLI, we focus</context>
</contexts>
<marker>Laufer, Girsai, 2008</marker>
<rawString>Laufer, B and Girsai, N. 2008. Form-focused Instruction in Second Language Vocabulary Learning: A Case for Contrastive Analysis and Translation. Applied Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression.</title>
<date>2008</date>
<publisher>UAI.</publisher>
<contexts>
<context position="11570" citStr="Mimno and McCallum, 2008" startWordPosition="1903" endWordPosition="1906"> Dirichlet Process to choose a subset of frequently reoccurring rules by repeatedly sampling derivations for a corpus of parse trees (Cohn and Blunsom, 2010). The rich get richer dynamic of the DP leads to the use of a compact set of rules that is an effective feature set for NLI (Swanson and Charniak, 2012). However, this same property makes rare rules harder to find. To address this weakness, we define a general model for TSG induction in labeled documents that combines a Hierarchical Dirichlet Process (Teh et al, 2005), with supervised labels in a manner similar to upstream supervised LDA (Mimno and McCallum, 2008). In the context of our work the document label η indicates both its authors native language L and data set D. Each η is associated with an observed Dirichlet prior ν., and a hidden multinomial θ. over grammars is drawn from this prior. The traditional grammatical model of nonterminal expansion is augmented such that to rewrite a symbol we first choose a grammar from the document’s θ. and then choose a rule from that grammar. For those unfamiliar with these models, the basic idea is to jointly estimate a mixture distribution over grammars for each η, as well as the parameters of these grammars</context>
</contexts>
<marker>Mimno, McCallum, 2008</marker>
<rawString>David Mimno and Andrew McCallum. 2008. Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanchuan Peng</author>
<author>Fuhui Long</author>
<author>Chris Ding</author>
</authors>
<title>Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence.</journal>
<contexts>
<context position="4876" citStr="Peng et al, 2005" startWordPosition="784" endWordPosition="787">r if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and elimination of feature redundancy has not been considered. Tree Substitution Grammars have recently been successfully applied in several domains using the induction algorithm presented by Cohn and Blunsom (2010). Our hierarchical treatment builds on this work by incorporating supervised mixtures over latent grammars into this induction process. Latent </context>
</contexts>
<marker>Peng, Long, Ding, 2005</marker>
<rawString>Hanchuan Peng and Fuhui Long and Chris Ding. 2005. Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation. Association for Computational Linguistics.</title>
<date>2006</date>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian Learning of a Tree Substitution Grammar. Association for Computational Linguistics.</title>
<date>2009</date>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian Learning of a Tree Substitution Grammar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<booktitle>2012. Developmental and Cross-linguistic Perspectives in Learner Corpus Research..</booktitle>
<editor>Tono, Y., Kawaguchi, Y. &amp; Minegishi, M. (eds.)</editor>
<marker></marker>
<rawString>Tono, Y., Kawaguchi, Y. &amp; Minegishi, M. (eds.) . 2012. Developmental and Cross-linguistic Perspectives in Learner Corpus Research..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Using classifier features for studying the effect of native language on the choice of written second language words.</title>
<date>2007</date>
<publisher>CACLA.</publisher>
<marker>Tsur, Rappoport, 2007</marker>
<rawString>Oren Tsur and Ari Rappoport. 2007. Using classifier features for studying the effect of native language on the choice of written second language words. CACLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Yusuke Miyao</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Bayesian SymbolRefined Tree Substitution Grammars for Syntactic Parsing. Association for Computational Linguistics.</title>
<date>2012</date>
<contexts>
<context position="12610" citStr="Shindo et al, 2012" startWordPosition="2091" endWordPosition="2094">grammar. For those unfamiliar with these models, the basic idea is to jointly estimate a mixture distribution over grammars for each η, as well as the parameters of these grammars. The HDP is necessary as the size 87 of each of these grammars is essentially infinite. We can express the generative model formally by defining the probability of a rule r expanding a symbol s in a sentence labeled q as 0η — Dir(vη) ziη — Mul40η) Hs — DP(-y, P0(•|s)) Gks — DP(αs, Hs) riηs — Gzz,7s This is closely related to the application of the Hierarchical Pitman Yor Process used in (Blunsom and Cohn, 2010) and (Shindo et al, 2012), which interpolates between multiple coarse and fine mappings of the data items being clustered to deal with sparse data. While the underlying Chinese Restaurant Process sampling algorithm is quite similar, our approach differs in that it models several different distributions with the same support that share a common prior. By careful choice of the number of grammars K, the Dirichlet priors v, and the backoff concentration parameter -y, a variety of interesting models can easily be defined, as demonstrated in our experiments. 5 Feature Selection 5.1 Dataset Independence The first step in our</context>
</contexts>
<marker>Shindo, Miyao, Fujino, Nagata, 2012</marker>
<rawString>Shindo, Hiroyuki and Miyao, Yusuke and Fujino, Akinori and Nagata, Masaaki 2012. Bayesian SymbolRefined Tree Substitution Grammars for Syntactic Parsing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
<author>Eugene Charniak</author>
</authors>
<title>Native Language Detection with Tree Substitution Grammars. Association for Computational Linguistics.</title>
<date>2012</date>
<contexts>
<context position="4025" citStr="Swanson and Charniak, 2012" startWordPosition="642" endWordPosition="645">ls, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1. 2 Related Work Our work is closely related to the recent surge of research in NLI. Beginning with Koppel et al (2005), several papers have proposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any clai</context>
<context position="11254" citStr="Swanson and Charniak, 2012" startWordPosition="1853" endWordPosition="1856">n the patterns that they can represent, potentially capturing any contiguous parse tree structure. As it is intractable to rank and filter the entire set of possible TSG rules given a corpus, we start with the large subset produced by Bayesian grammar induction. The most widely used algorithm for TSG induction uses a Dirichlet Process to choose a subset of frequently reoccurring rules by repeatedly sampling derivations for a corpus of parse trees (Cohn and Blunsom, 2010). The rich get richer dynamic of the DP leads to the use of a compact set of rules that is an effective feature set for NLI (Swanson and Charniak, 2012). However, this same property makes rare rules harder to find. To address this weakness, we define a general model for TSG induction in labeled documents that combines a Hierarchical Dirichlet Process (Teh et al, 2005), with supervised labels in a manner similar to upstream supervised LDA (Mimno and McCallum, 2008). In the context of our work the document label η indicates both its authors native language L and data set D. Each η is associated with an observed Dirichlet prior ν., and a hidden multinomial θ. over grammars is drawn from this prior. The traditional grammatical model of nontermina</context>
</contexts>
<marker>Swanson, Charniak, 2012</marker>
<rawString>Ben Swanson and Eugene Charniak. 2012. Native Language Detection with Tree Substitution Grammars. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet Processes.</title>
<date>2005</date>
<journal>Journal of the American Statistical Association.</journal>
<contexts>
<context position="11472" citStr="Teh et al, 2005" startWordPosition="1888" endWordPosition="1891">ed by Bayesian grammar induction. The most widely used algorithm for TSG induction uses a Dirichlet Process to choose a subset of frequently reoccurring rules by repeatedly sampling derivations for a corpus of parse trees (Cohn and Blunsom, 2010). The rich get richer dynamic of the DP leads to the use of a compact set of rules that is an effective feature set for NLI (Swanson and Charniak, 2012). However, this same property makes rare rules harder to find. To address this weakness, we define a general model for TSG induction in labeled documents that combines a Hierarchical Dirichlet Process (Teh et al, 2005), with supervised labels in a manner similar to upstream supervised LDA (Mimno and McCallum, 2008). In the context of our work the document label η indicates both its authors native language L and data set D. Each η is associated with an observed Dirichlet prior ν., and a hidden multinomial θ. over grammars is drawn from this prior. The traditional grammatical model of nonterminal expansion is augmented such that to rewrite a symbol we first choose a grammar from the document’s θ. and then choose a rule from that grammar. For those unfamiliar with these models, the basic idea is to jointly est</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2005</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2005. Hierarchical Dirichlet Processes. Journal of the American Statistical Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
<author>Beata Beigman-Klebanov</author>
<author>Martin Chodorow</author>
</authors>
<title>Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification.</title>
<date>2012</date>
<publisher>COLING.</publisher>
<contexts>
<context position="4532" citStr="Tetreault et al, 2012" startWordPosition="727" endWordPosition="730">e sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and eliminatio</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, Beigman-Klebanov, Chodorow, 2012</marker>
<rawString>Joel Tetreault, Daniel Blanchard, Aoife Cahill, Beata Beigman-Klebanov and Martin Chodorow. 2012. Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Contrastive analysis and native language identification.</title>
<date>2009</date>
<booktitle>Proceedings of the Australasian Language Technology Association Workshop.</booktitle>
<marker>Wong, Dras, 2009</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive analysis and native language identification. Proceedings of the Australasian Language Technology Association Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Exploiting Parse Structures for Native Language Identification.</title>
<date>2011</date>
<booktitle>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3995" citStr="Wong and Dras, 2011" startWordPosition="638" endWordPosition="641">h native language labels, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1. 2 Related Work Our work is closely related to the recent surge of research in NLI. Beginning with Koppel et al (2005), several papers have proposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach</context>
<context position="5567" citStr="Wong and Dras, 2011" startWordPosition="890" endWordPosition="893">trics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and elimination of feature redundancy has not been considered. Tree Substitution Grammars have recently been successfully applied in several domains using the induction algorithm presented by Cohn and Blunsom (2010). Our hierarchical treatment builds on this work by incorporating supervised mixtures over latent grammars into this induction process. Latent mixture techniques for NLI have been explored with other feature types (Wong and Dras, 2011b; Wong and Dras, 2012), but have not previously led to measurable empirical gains. 1bllip.cs.brown.edu/download/nli corpus.pdf 3 Corpus Description We first make explicit our experimental setup in order to provide context for the discussion to follow. We perform analysis of English text from Chinese, German, Spanish, and Japanese L1 backgrounds drawn from four corpora. The first three consist of responses to essay prompts in educational settings, while the fourth is submitted by users in an internet forum. The first corpus is the International Corpus of Learner English (ICLE) (Granger et al, </context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting Parse Structures for Native Language Identification. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Topic Modeling for Native Language Identification.</title>
<date>2011</date>
<booktitle>Proceedings of the Australasian Language Technology Association Workshop.</booktitle>
<contexts>
<context position="3995" citStr="Wong and Dras, 2011" startWordPosition="638" endWordPosition="641">h native language labels, avoid85 Proceedings of NAACL-HLT 2013, pages 85–94, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics ing redundancy and artifacts from the corpus creation process. We release this list for use by the linguistics and SLA research communities, and plan to expand it with upcoming releases of L1 labeled corpora1. 2 Related Work Our work is closely related to the recent surge of research in NLI. Beginning with Koppel et al (2005), several papers have proposed different feature sets to be used as predictors of L1 (Tsur and Rappaport, 2007; Wong and Dras, 2011a; Swanson and Charniak, 2012). However, due to the ubiquitous use of random subsamples, different data preparation methods, and severe topic and annotation biases of the data set employed, there is little consensus on which feature sets are ideal or sufficient, or if any reported accuracies reflect some generalizable truth of the problem’s difficulty. To combat the bias of a single data set, a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach</context>
<context position="5567" citStr="Wong and Dras, 2011" startWordPosition="890" endWordPosition="893">trics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and elimination of feature redundancy has not been considered. Tree Substitution Grammars have recently been successfully applied in several domains using the induction algorithm presented by Cohn and Blunsom (2010). Our hierarchical treatment builds on this work by incorporating supervised mixtures over latent grammars into this induction process. Latent mixture techniques for NLI have been explored with other feature types (Wong and Dras, 2011b; Wong and Dras, 2012), but have not previously led to measurable empirical gains. 1bllip.cs.brown.edu/download/nli corpus.pdf 3 Corpus Description We first make explicit our experimental setup in order to provide context for the discussion to follow. We perform analysis of English text from Chinese, German, Spanish, and Japanese L1 backgrounds drawn from four corpora. The first three consist of responses to essay prompts in educational settings, while the fourth is submitted by users in an internet forum. The first corpus is the International Corpus of Learner English (ICLE) (Granger et al, </context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2011. Topic Modeling for Native Language Identification. Proceedings of the Australasian Language Technology Association Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
<author>Mark Johnson</author>
</authors>
<title>Exploring Adaptor Grammars for Native Language Identification.</title>
<date>2012</date>
<tech>EMNLP-CoNLL.</tech>
<marker>Wong, Dras, Johnson, 2012</marker>
<rawString>Sze-Meng Jojo Wong, Mark Dras, Mark Johnson. 2012. Exploring Adaptor Grammars for Native Language Identification. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Yu</author>
<author>Huan Liu</author>
</authors>
<title>Efficient Feature Selection via Analysis of Relevance and Redundancy.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="5008" citStr="Yu and Liu (2004)" startWordPosition="807" endWordPosition="810">a new strain of work has emerged in which train and test documents come from different corpora (Brooke and Hirst, 2012; Tetreault et al, 2012; Bykh and Meurers, 2012). We follow this cross corpus approach, as it is crucial to any claims of feature relevance. Feature selection itself is a well studied problem, and the most thorough systems address both relevancy and redundancy. While some work tackles these problems by optimizing a metric over both simultaneously (Peng et al, 2005), we decouple the notions of relevancy and redundancy to allow ad-hoc metrics for either, similar to the method of Yu and Liu (2004). The measurement of feature relevancy in NLI has to this point been handled primarily with Information Gain, and elimination of feature redundancy has not been considered. Tree Substitution Grammars have recently been successfully applied in several domains using the induction algorithm presented by Cohn and Blunsom (2010). Our hierarchical treatment builds on this work by incorporating supervised mixtures over latent grammars into this induction process. Latent mixture techniques for NLI have been explored with other feature types (Wong and Dras, 2011b; Wong and Dras, 2012), but have not pre</context>
<context position="20081" citStr="Yu and Liu, 2004" startWordPosition="3422" endWordPosition="3425">atures 300 250 200 150 100 50 0 X-IG X-SU SU-IG 50 40 30 20 10 # of shared features 0 � x2(Xi) = m 89 degrade the quality of our output. This is especially necessary when using TSG rules as features, as it is possible to define many slightly different rules that essentially represent the same linguistic act. Redundancy detection must be able to both determine that a set of features are redundant and also select the feature to retain from such a set. We use a greedy method that allows us to investigate different relevancy metrics for selection of the representative feature for a redundant set (Yu and Liu, 2004). The algorithm begins with a list S containing the full list of features, sorted by an arbitrary metric of relevancy. While S is not empty, the most relevant feature X* in S is selected for retention, and all features Xi are removed from S if R(X*, Xi) &gt; ρ for some redundancy metric R and some threshold ρ. We consider two probabilistic metrics for redundancy detection, the first being SU, as defined in the previous section. We contrast this metric with Normalized Pointwise Mutual Information (NPMI) which uses only the events A = X+a and B = Xb+ and has a range of [-1,1]. log(P(A|B)) − log(P (</context>
</contexts>
<marker>Yu, Liu, 2004</marker>
<rawString>Lei Yu and Huan Liu. 2004. Efficient Feature Selection via Analysis of Relevance and Redundancy. Journal of Machine Learning Research.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>