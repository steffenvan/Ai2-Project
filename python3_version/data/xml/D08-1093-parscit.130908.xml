<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000509">
<title confidence="0.993325">
Automatic Prediction of Parser Accuracy
</title>
<author confidence="0.99096">
Sujith Ravi and Kevin Knight
</author>
<affiliation confidence="0.873838333333333">
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
</affiliation>
<email confidence="0.999438">
{sravi,knight}@isi.edu
</email>
<author confidence="0.587685">
Radu Soricut
</author>
<affiliation confidence="0.536508666666667">
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, California 90292
</affiliation>
<email confidence="0.99482">
rsoricut@languageweaver.com
</email>
<sectionHeader confidence="0.996622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999416428571429">
Statistical parsers have become increasingly
accurate, to the point where they are useful in
many natural language applications. However,
estimating parsing accuracy on a wide variety
of domains and genres is still a challenge in
the absence of gold-standard parse trees.
In this paper, we propose a technique that au-
tomatically takes into account certain charac-
teristics of the domains of interest, and ac-
curately predicts parser performance on data
from these new domains. As a result, we have
a cheap (no annotation involved) and effective
recipe for measuring the performance of a sta-
tistical parser on any given domain.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995659431372549">
Statistical natural language parsers have recently
become more accurate and more widely available.
As a result, they are being used in a variety of
applications, such as question answering (Herm-
jakob, 2001), speech recognition (Chelba and Je-
linek, 1998), language modeling (Roark, 2001), lan-
guage generation (Soricut, 2006) and, most notably,
machine translation (Charniak et al., 2003; Galley et
al., 2004; Collins et al., 2005; Marcu et al., 2006;
Huang et al., 2006; Avramidis and Koehn, 2008).
These applications are employed on a wide range of
domains and genres, and therefore the question of
how accurate a parser is on the domain and genre of
interest becomes acute. Ideally, one would want to
have available a recipe for precisely answering this
question: “given a parser and a particular domain of
interest, how accurate are the parse trees produced?”
The only recipe that is implicitly given in the large
literature on parsing to date is to have human anno-
tators build parse trees for a sample set from the do-
main of interest, and consequently use them to com-
pute a PARSEVAL (Black et al., 1991) score that is
indicative of the intrinsic performance of the parser.
Given the wide range of domains and genres for
which NLP applications are of interest, combined
with the high expertise required from human anno-
tators to produce parse tree annotations, this recipe
is, albeit precise, too expensive. The other recipe
that is currently used on a large scale is to measure
the performance of a parser on existing treebanks,
such as WSJ (Marcus et al., 1993), and assume that
the accuracy measure will carry over to the domains
of interest. This recipe, albeit cheap, cannot provide
any guarantee regarding the performance of a parser
on a new domain, and, as experiments in this paper
show, can give wrong indications regarding impor-
tant decisions for the design of NLP systems that
use a syntactic parser as an important component.
This paper proposes another method for measur-
ing the performance of a parser on a given domain
that is both cheap and effective. It is a fully auto-
mated procedure (no expensive annotation involved)
that uses properties of both the domain of interest
and the domain on which the parser was trained in
order to measure the performance of the parser on
the domain of interest. It is, in essence, a solution to
the following prediction problem:
Input: (1) a statistical parser and its training data,
(2) some chunk of text from a new domain or genre
Output: an estimate of the accuracy of the parse
trees produced for that chunk of text
</bodyText>
<page confidence="0.964306">
887
</page>
<note confidence="0.962274">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887–896,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999531571428571">
Accurate estimations for this prediction problem
will allow a system designer to make the right de-
cisions for the given domain of interest. Such deci-
sions include, but are not restricted to, the choice of
the parser, the choice of the training data, the choice
of how to implement various components such as the
treatment of unknown words, etc. Altogether, a cor-
rect estimation of the impact of such decisions on the
resulting parse trees can guide a system designer in a
hill-climbing scenario for which an extrinsic metric
(such as the impact on the overall quality of the sys-
tem) is usually too expensive to be employed often
enough. To provide an example, a machine transla-
tion engine that requires parse trees as training data
in order to learn syntax-based translation rules (Gal-
ley et al., 2006) needs to employ a syntactic parser
as soon as the training process starts, but it can take
up to hundreds and even thousands of CPU hours
(for large training data sets) to train the engine be-
fore translations can be produced and measured. Al-
though a real estimate of the impact of a parser de-
sign decision in this scenario can only be gauged
from the quality of the translations produced, it is
impractical to create such estimates for each design
decision. On the other hand, estimates using the so-
lution proposed in this paper can be obtained fast,
before submitting the parser output to a costly train-
ing procedure.
</bodyText>
<sectionHeader confidence="0.9846205" genericHeader="introduction">
2 Related Work and Experimental
Framework
</sectionHeader>
<bodyText confidence="0.999880965517241">
There have been previous studies which explored the
problem of automatically predicting the task diffi-
culty for various NLP applications. (Albrecht and
Hwa, 2007) presented a regression based method
for developing automatic evaluation metrics for ma-
chine translation systems without directly relying on
human reference translations. (Hoshino and Nak-
agawa, 2007) built a computer-adaptive system for
generating questions to teach English grammar and
vocabulary to students, by predicting the difficulty
level of a question using various features. There
have been a few studies of English parser accuracy
in domains/genres other than WSJ (Gildea, 2001;
Bacchiani et al., 2006; McClosky et al., 2006), but
in order to make measurements for such studies, it
is necessary to have gold-standard parses in the non-
WSJ domain of interest.
Gildea (2001) studied how well WSJ-trained
parsers do on the Brown corpus, for which a gold
standard exists. He looked at sentences with 40
words or less. (Bacchiani et al., 2006) carried out
a similar experiment on sentences of all lengths,
and (McClosky et al., 2006) report additional re-
sults. The table below shows results from our own
measurements of Charniak parser1 (Charniak and
Johnson, 2005) accuracy (F-measure on sentences of
all lengths), which are consistent with these studies.
For the Brown corpus, the test set was formed from
every tenth sentence in the corpus (Gildea, 2001).
</bodyText>
<table confidence="0.998328666666667">
Training Set Test Set Sent. Charniak
count accuracy
WSJ sec. 02-21 WSJ sec. 24 1308 90.48
(39,832 sent.)
WSJ sec. 23 2343 91.13
Brown-test 2186 86.34
</table>
<bodyText confidence="0.999811">
Here we investigate algorithms for predicting the
accuracy of a parser P on sentences, chunks of sen-
tences, and whole corpora. We also investigate and
contrast several scenarios for prediction: (1) the pre-
dictor looks at the input text only, (2) the predictor
looks at the input text and the output parse trees of
P, and (3) the predictor looks at the input text, the
output parse trees of P, and the outputs of other pro-
grams, such as the output parse trees of a different
parser Pref used as a reference. Under none of these
scenarios is the predictor allowed to look at gold-
standard parses in the new domain/genre.
The intuition behind what we are trying to achieve
here can be compared to an analogous task—trying
to assess the performance of a median student from
a math class on a given test, without having access to
the answer sheet. Looking at the test only, we could
probably tell whether the test looks hard or not, and
therefore whether the student will do well or not.
Looking at the student’s answers will likely give us
an even better idea of the performance. Finally, the
answers of a second student with similar proficiency
will provide even better clues: if the students agree
on every answer, then they probably both did well,
but if they disagree frequently, then they (and hence
our student) probably did not do as well.
Our first experiments are concerned with validat-
ing the idea itself: can a predictor be trained such
</bodyText>
<footnote confidence="0.9995955">
1Downloaded from ftp.cs.brown.edu/pub/nlparser/reranking-
parserAug06.tar.gz in February, 2007.
</footnote>
<page confidence="0.994962">
888
</page>
<bodyText confidence="0.77922475">
that it predicts the same F-scores as the ones ob-
tained using gold-trees? We first validate this using
the WSJ corpus itself, by dividing the WSJ treebank
into several sections:
</bodyText>
<listItem confidence="0.9442757">
1. Training (WSJ section 02-21). The parser P is
trained on this data.
2. Development (WSJ section 24). We use this
data for training our predictor.
3. Test (WSJ section 23). We use this data for
measuring our predictions. For each test sentence,
we compute (1) the PARSEVAL F-measure score
using the test gold standard, and (2) our predicted
F-measure. We report the correlation coefficient (r)
between the actual F-scores and our predicted F-
</listItem>
<bodyText confidence="0.914097461538461">
scores. We will also use a root-mean-square error
(rms error) metric to compare actual and predicted
F-scores.
Section 3 describes the features used by our pre-
dictor. Given these features, as well as actual
F-scores computed for the development data, we
use supervised learning to set the feature weights.
To this end, we use SVM-Regression2 (Smola and
Schoelkopf, 1998) with an RBF kernel, to learn the
feature weights and build our predictor system.3 We
validate the accuracy of the predictor trained in this
fashion on both WSJ (Section 4) and the Brown cor-
pus (Section 5).
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="method">
3 Features Used for Predicting Parser
Accuracy
</sectionHeader>
<subsectionHeader confidence="0.999493">
3.1 Text-based Features
</subsectionHeader>
<bodyText confidence="0.998221">
One hypothesis we explore is that (all other things
being equal) longer sentences are harder to parse
correctly than shorter sentences. When exposed
to the development set, SVM-Regression learns
weights to best predict F-scores using the values for
this feature corresponding to each sentence in the
corpus.
Does the predicted F-score correlate with actual
F-score on a sentence by sentence basis? There was
a positive but weak correlation:
</bodyText>
<footnote confidence="0.9958635">
2Weka software (http://www.cs.waikato.ac.nz/ml/weka/)
3We compared a few regression algorithms like SVM-
Regression (using different kernels and parameter settings) and
Multi-Layer Perceptron (neural networks) – we trained the al-
gorithms separately on dev data and picked the one that gave
the best cross-validation accuracy (F-measure).
</footnote>
<bodyText confidence="0.985635355555556">
Feature set dev (r) test (r)
Length 0.13 0.19
Another hypothesis is that the parser performance
is influenced by the number of UNKNOWN words
in the sentence to be parsed, i.e., the number of
words in the test sentence that were never seen be-
fore in the training set. Training the predictor with
this feature produces a positive correlation, slightly
weaker compared to the Length feature.
Feature set dev (r) test (r)
UNK 0.11 0.11
Unknown words are not the only ones that can in-
fluence the performance of a parser. Rare words,
for which statistical models do not have reliable es-
timates, are also likely to impact parsing accuracy.
To test this hypothesis, we add a language model
perplexity–based (LM-PPL) feature. We extract the
yield of the training trees, on which we train a tri-
gram language model.4 We compute the perplexity
of each test sentence with respect to this language
model, and use it as feature in our predictor system.
Note that this feature is meant as a refinement of the
previous UNK feature, in the sense that perplexity
numbers are meant to signal the occurrence of un-
known words, as well as rare (from the training data
perspective) words. However, the correlation we ob-
serve for this feature is similar to the correlation ob-
served for the UNK feature, which seems to suggest
that the smoothing techniques used by the parsers
employed in these experiments lead to correct treat-
ment of the rare words.
Feature set dev (r) test (r)
LM-PPL 0.11 0.10
We also look at the possibility of automatically
detecting certain “cue” words that are appropriate
for our prediction problem. That is, we want to see
if we can detect certain words that have a discrimi-
nating power in deciding whether parsing a sentence
that contains them is difficult or easy. To this end,
we use a subset of the development data, which con-
tains the 200 best-parsed and 200 worst-parsed sen-
tences (based on F-measure scores). For each word
in the development dataset, we compute the infor-
mation gain (IG) (Yang and Pedersen, 1997) score
for that word with respect to the best/worst parsed
</bodyText>
<footnote confidence="0.924275">
4We trained using the SRILM language modeling toolkit,
with default settings.
</footnote>
<page confidence="0.99764">
889
</page>
<bodyText confidence="0.983106555555555">
dataset. These words are then ranked by their IG
scores, and the top 100 words are included as lex-
ical features in our predictor system. As expected,
the correlation on the development set is quite high
(given that these lexical cues are extracted from this
particular set), but a positive correlation holds for
the test set as well.
Feature set dev (r) test (r)
lexCount100 0.43 0.18
</bodyText>
<subsectionHeader confidence="0.999855">
3.2 Parser P–based Features
</subsectionHeader>
<bodyText confidence="0.98811592">
Besides exploiting the information present in the in-
put text, we can also inspect the output tree of the
parser P for which we are interested in predicting
accuracy. We create a rootSYN feature based on
the syntactic category found at the root of the out-
put tree (“is it S?”, “is it FRAG?”). We also create
a puncSYN feature based on the number of words
labeled as punctuation tags (based on the intuition
that heavy use of punctuation can be indicative of
the difficulty of the input sentences), and a label-
SYN feature in which we bundled together informa-
tion regarding the number of internal nodes in the
parse tree output that have particular labels (“how
many nodes are labeled with PP?”). In our predictor,
we use 72 such labelSYN features corresponding to
all the syntactic labels found in the parse tree out-
put for the development set. The test set correlation
given by the rootSYN and the labelSYN features are
higher than some of the text-based features, whereas
the puncSYN feature seems to have little discrimi-
native power.
Feature set dev (r) test (r)
rootSYN 0.21 0.17
puncSYN 0.09 0.01
labelSYN 0.33 0.28
</bodyText>
<subsectionHeader confidence="0.999142">
3.3 Reference Parser Pref–based Features
</subsectionHeader>
<bodyText confidence="0.9937845">
In addition to the text-based features and parser P–
based features, we can bring in an additional parser
Pref whose output is used as a reference against
which the output of parser P is measured. For the
reference parser feature, our goal is to measure how
similar/different are the results from the two parsers.
We find that if the parses are similar, they are more
likely to be right. In order to compute similarity, we
can compare the constituents in the two parse trees
from P and Pref, and see how many constituents
match. This is most easily accomplished by consid-
ering Pref to be a “gold standard” (even though it is
not necessarily a correct parse) and computing the
F-measure score of parser P against Pref . We use
this F-measure score as a feature for prediction.
For the experiments presented in this section we
use as Pref , the parser from (Bikel, 2002). Intu-
itively, the requirement for choosing parser Pref in
conjunction with parser P seems to be that they
are different enough to produce non-identical trees
when presented with the same input, and at the
same time to be accurate enough to produce reli-
able parse trees. The choice of P as (Charniak and
Johnson, 2005) and Pref as (Bikel, 2002) fits this
bill, but many other choices can be made regarding
Pref, such as (Klein and Manning, 2003; Petrov and
Klein, 2007; McClosky et al., 2006; Huang, 2008).
We leave the task of creating features based on the
consensus of multiple parsers as future work.
The correlation given by the reference parser–
based feature Pref on the test set is the highest
among all the features we explored.
Feature set dev (r) test (r)
Pref 0.40 0.36
</bodyText>
<subsectionHeader confidence="0.997837">
3.4 The Aggregated Power of Features
</subsectionHeader>
<bodyText confidence="0.999917666666667">
The table below lists all the individual features we
have described in this section, sorted according to
the correlation value obtained on the test set.
</bodyText>
<table confidence="0.998371428571429">
Feature set dev (r) test (r)
Pref 0.40 0.36
labelSYN 0.33 0.28
lexCount500 0.56 0.23
lexBool500 0.58 0.20
lexCount1000 0.67 0.20
lexBool1000 0.58 0.20
Length 0.13 0.19
lexCount100 0.43 0.18
lexBool100 0.43 0.18
rootSYN 0.21 0.17
UNK 0.11 0.11
LM-PPL 0.11 0.10
puncSYN 0.09 0.01
</table>
<bodyText confidence="0.999196714285714">
Note how the lexical features tend to over-fit the
development data—the words were specifically cho-
sen for their discriminating power on that particular
set. Hence, adding more lexical features to the pre-
dictor system improves the correlation on develop-
ment (due to over-fitting), but it does not produce
consistent improvement on the test set. However,
</bodyText>
<page confidence="0.977379">
890
</page>
<table confidence="0.9998937">
Method (using 3 features: # of random dev (r)
Length, UNK, Pref ) restarts
SVM Regression 0.42
1 0.138
5 0.136
Maximum Correlation 10 0.166
Training (MCT) 25 0.178
100 0.232
1000 0.27
10,000 0.401
</table>
<tableCaption confidence="0.99826">
Table 1: Comparison of correlation (r) obtained using MCT versus
SVM-Regression on development corpus.
</tableCaption>
<bodyText confidence="0.9991585">
there is some indication that the counts of the lex-
ical features are important, and count-based lexical
features tend to have similar or better performance
compared to their boolean-based counterparts.
Since these features measure different but over-
lapping pieces of the information available, it is to
be expected that some of the feature combinations
would provide better correlation that the individual
features, but the gains are not strictly additive. By
taking the individual features that provide the best
discriminative power, we are able to get a correla-
tion score of 0.42 on the test set.
</bodyText>
<table confidence="0.800084333333333">
Feature set dev (r) test (r)
Pref + labelSYN + Length + lexCount100 + 0.55 0.42
rootSYN + UNK + LM-PPL
</table>
<subsectionHeader confidence="0.558713">
3.5 Optimizing for Maximum Correlation
</subsectionHeader>
<bodyText confidence="0.999932">
If our goal is to obtain the highest correlations
with the F-score measure, is SVM regression the
best method? Liu and Gildea (2007) recently in-
troduced Maximum Correlation Training (MCT), a
search procedure that follows the gradient of the for-
mula for correlation coefficient (r). We implemented
MCT, but obtained no better results. Moreover, it
required many random re-starts just to obtain results
comparable to SVM regression (Table 1).
</bodyText>
<sectionHeader confidence="0.907733" genericHeader="method">
4 Predicting Accuracy on Multiple
Sentences
</sectionHeader>
<bodyText confidence="0.999935333333333">
The results for the scenario presented in Section 3
are encouraging, but other scenarios are also im-
portant from a practical perspective. For instance,
we are interested in predicting the performance of a
particular parser not on a sentence-by-sentence ba-
sis, but for a representative chunk of sentences from
the new domain. In order to predict the F-measure
on multiple sentences, we modify our feature set to
generate information on a whole chunk of sentences
</bodyText>
<table confidence="0.999922857142857">
Sentences in WSJ-test (r) WSJ-test
chunk (n) (rms error)
1 0.42 0.098
20 0.61 0.026
50 0.62 0.019
100 0.69 0.015
500 0.79 0.011
</table>
<tableCaption confidence="0.9883335">
Table 2: Performance of predictor on n-sentence chunks from WSJ-test
(Correlation and rms error between actual/predicted accuracies).
</tableCaption>
<bodyText confidence="0.99997925">
rather than a single sentence. Predicting the corre-
lation at chunk level is, not unexpectedly, an eas-
ier problem than predicting correlation at sentence
level, as the results in the first two columns of Ta-
ble 2 show.
For 100-sentence chunks, we also plot the pre-
dicted accuracies versus actual accuracies for the
WSJ-test set in Figure 1. This scatterplot brings to
light an artifact of using correlation metric (r) for
evaluating our predictor’s performance. Although
our objective is to improve correlation between ac-
tual and predicted F-scores, the correlation metric (r)
does not tell us directly how well the predictor is
doing. In Figure 1, the system predicts that on
an average, most sentence chunks can be parsed
with an accuracy of 0.9085 (which is the mean pre-
dicted F-score on WSJ-test). But the range of pre-
dictions from our system [0.89,0.92] is smaller than
the actual F-score range [0.86,0.95]. Hence, even
though the correlation scores are high, this does not
necessarily mean that our predictions are on target.
An additional metric, root-mean-square (rms) error,
which measures the distance between actual and pre-
dicted F-measures, can be used to gauge the qual-
ity of our predictions. For a particular chunk-size,
lowering the rms error translates into aligning the
points of a scatterplot as the one in Figure 1, closer
to the x=y line, implying that the predictor is getting
better at exactly predicting the F-score values. The
third column in Table 2 shows the rms error for our
predictor at different chunk sizes. The results using
this metric also show that the prediction problem be-
comes easier as the chunk size increases.
Assuming that we have the test set of WSJ sec-
tion 23, but without the gold-standard trees, how
can we get an approximation for the overall accu-
racy of a parser P on this test set? One possibility,
which we use here as a baseline, is to compute the
F-score on a set for which we do have gold-standard
trees. If we use our development set (WSJ section
</bodyText>
<page confidence="0.998254">
891
</page>
<figureCaption confidence="0.977253666666667">
Figure 1: Plot showing Actual vs. Predicted accuracies for
WSJ-test (100-sentence chunks). Each plot point represents a
100-sentence chunk. (rms error = 0.015)
</figureCaption>
<table confidence="0.99826725">
System F-measure
Charniak F-measure on WSJ-dev (baseline) 90.48 (fd)
Predictor (feature weights set with WSJ-dev) 90.85 (fp)
Actual Charniak accuracy 91.13 (ft)
</table>
<tableCaption confidence="0.993268">
Table 3: Comparing Charniak parser accuracy (from different systems)
on entire WSJ-test corpus
</tableCaption>
<bodyText confidence="0.999795076923077">
24) for this purpose, and (Charniak and Johnson,
2005) as the parser P, the baseline is an F-score of
90.48 (fd), which is the actual Charniak parser accu-
racy on WSJ section 24. Instead, if we run our pre-
dictor on the test set (a single chunk containing all
the sentences in the test set), it predicts an F-score
of 90.85 (fp). These two predictions are listed as
the first two rows in Table 3. Of course, having the
actual gold-standard trees for WSJ section 23 helps
us decide which prediction is better: the actual ac-
curacy of the Charniak parser on WSJ section 23 is
an F-score of 91.13 (ft), which makes our prediction
better than the baseline.
</bodyText>
<subsectionHeader confidence="0.7970005">
4.1 Shifting Predictions to Match Actual
Accuracy
</subsectionHeader>
<bodyText confidence="0.999807909090909">
We correctly predict (in Table 3) that the
WSJ-test is easier to parse than the WSJ-
dev (90.85 &gt; 90.48). However, our predictor is too
conservative—the WSJ-test is actually even easier
to parse (91.13 &gt; 90.85). We can fix this by shift-
ing the mean predicted F-score (which is equal to
fp) further away from the dev F-measure (fd), and
closer to the actual F-measure (ft). This is achieved
by shifting all the individual predictions by a certain
amount as shown below.
Let p be an individual prediction from our system.
</bodyText>
<figureCaption confidence="0.992446333333333">
Figure 2: Plot showing Actual vs. Adjusted Predicted accu-
racies (shifting with α = 0.757, skewing with ,C3 = 1.0) for
WSJ-test (100-sentence chunks). (rms error= 0.014)
</figureCaption>
<bodyText confidence="0.95278">
The shifted prediction p0 is given by:
</bodyText>
<equation confidence="0.999396">
p0 = p + α(fp − fd) (1)
</equation>
<bodyText confidence="0.9011305">
We can tune α to make the new mean predic-
tion (f0p) to be equal to the actual F-measure (ft).
</bodyText>
<equation confidence="0.9956495">
f0p = fp + α(fp − fd) (2)
ft − fp
α = (3)
fp − fd
</equation>
<bodyText confidence="0.9997754">
Using the F-score values from Table 3, we get an
α = 0.757 and an exact prediction of 91.13. Of
course, this is because we tune on test, so we need
to validate this idea on a new test set to see if it leads
to improved predictions (Section 5).
</bodyText>
<subsectionHeader confidence="0.999031">
4.2 Skewing to Widen Prediction Range
</subsectionHeader>
<bodyText confidence="0.9986033">
Our predictor is also too conservative about its dis-
tribution (see Figure 1). It knows (roughly) which
chunks are easier to parse and which are harder, but
its range of predictions is lower than the range of
actual F-measure scores.
We can skew individual predictions so that sen-
tences predicted to be easy are re-predicted to be
even easier (and those that are hard to be even
harder). For each prediction p0 (from Equation 1),
we compute
</bodyText>
<equation confidence="0.999785">
p00 = p0 + β(p0 − f0 p) (4)
</equation>
<bodyText confidence="0.996611">
We simply set Q to 1.0, doubling the distance
of each prediction p0 (in Equation 1) from the (ad-
justed) mean prediction f0p, to obtain the skewed pre-
diction p00.
Figure 2 shows how the points representing 100-
sentence chunks in Figure 1 look after the predic-
tions have been shifted (α = 0.757) and skewed
(Q = 1.0). These two operations have the desired
effect of changing the range of predictions from
[0.89,0.92] to [0.87,0.94], much closer to the actual
</bodyText>
<figure confidence="0.999374575757576">
0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95
Predicted Accuracy
Actual Accuracy
0.95
0.94
0.93
0.92
0.91
0.89
0.88
0.87
0.86
0.85
0.9
per-chunk-accuracy
x=y line
Fitted-line
0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95
Predicted Accuracy
Actual Accuracy
0.95
0.94
0.93
0.92
0.91
0.89
0.88
0.87
0.86
0.85
0.9
per-chunk-acy
Mne
</figure>
<page confidence="0.990491">
892
</page>
<table confidence="0.999467444444444">
Sentences WSJ-test Brown-test Brown-test
in chunk (rms error) Prediction Adjusted
(n) (rms error) Prediction
(rms error)
1 0.098 0.129 0.139
20 0.026 0.039 0.036
50 0.019 0.032 0.029
100 0.015 0.025 0.020
500 0.011 0.038 0.024
</table>
<tableCaption confidence="0.9731655">
Table 4: Performance of predictor on n-sentence chunks from WSJ-test
and Brown-test (rms error between actual/predicted accuracies).
</tableCaption>
<bodyText confidence="0.9999639">
range of [0.86,0.95]. The points in the new plot (Fig-
ure 2) also align closer to the “x=y” line than in the
original graph (Figure 1). The rms error also drops
from 0.015 to 0.014 (7% relative reduction), show-
ing that the predictions have improved.
Since we use the WSJ-test corpus to tune the pa-
rameter values for shifting and skewing, we need to
apply our predictor on a different test set to see if we
get similar improvements by using these techniques,
which we do in the next section.
</bodyText>
<sectionHeader confidence="0.5032665" genericHeader="method">
5 Predicting Accuracy on the Brown
Corpus
</sectionHeader>
<bodyText confidence="0.99957036">
The Brown corpus represents a genuine challenge
for our predictor, as it presents us with the oppor-
tunity to test the performance of our predictor in
an out-of-domain scenario. Our predictor, trained
on WSJ data, is now employed to predict the per-
formance of a WSJ-trained parser P on the Brown-
test corpus. As in the previous experiments, we use
(Charniak and Johnson, 2005) trained on WSJ sec-
tions 02-21 as parser P. The feature weights for our
predictor are again trained on section 24 of WSJ, and
the shifting and skewing parameters (α = 0.757,
Q = 1.0) are determined using section 23 of WSJ.
The results on the Brown-test, both the origi-
nal predictions and after they have been adjusted
(shifted/skewed), are shown in Table 4, at different
level of chunking. For chunks of size n &gt; 1, the
shifting and skewing techniques help in lowering the
rms error. On 100-sentence chunks from the Brown
test, shifting and skewing (α = 0.757, Q = 1.0)
leads to a 20% relative reduction in the rms error.
In a similar vein with the evaluation done in Sec-
tion 4, we are interested in estimating the overall ac-
curacy of a WSJ-trained parser P given an out-of-
domain set such as the Brown test set (for which, at
least for now, we do not have access to gold-standard
</bodyText>
<table confidence="0.998653166666667">
System F-measure
Baseline1 (F-measure on WSJ sec. 23) 91.13
Baseline2 (F-measure on WSJ sec. 24) 90.48
Predictor (base) 88.48
Adjusted Predictor (shifting using α = 0.757) 86.96
Actual accuracy 86.34
</table>
<tableCaption confidence="0.999725">
Table 5: Charniak parser accuracy on entire Brown-test corpus
</tableCaption>
<bodyText confidence="0.999955823529412">
trees). If we use (Charniak and Johnson, 2005) as
parser P, a cheap and readily-available answer is
to approximate the performance using the Charniak
parser performance on WSJ section 23, which has
an F-score of 91.13. Another cheap and readily-
available answer is to take the Charniak parser per-
formance on WSJ section 24 with an F-score of
90.48. Table 5 lists these baselines, along with the
prediction made by our system when using a single
chunk containing all the sentences in the Brown test
set (both base predictions and adjusted predictions,
i.e. shifting using α = 0.757). Again, having gold-
standard trees for the Brown test set helps us decide
which prediction is better. Our predictions are much
closer to the actual Charniak parser performance on
the Brown-test set, with the adjusted prediction at
86.96 compared to the actual F-score of 86.34.
</bodyText>
<sectionHeader confidence="0.781833" genericHeader="method">
6 Ranking Parser Performance
</sectionHeader>
<bodyText confidence="0.9997675">
One of the main goals for computing F-score figures
(either by traditional PARSEVAL evaluation against
gold standards or by methods such as the one pro-
posed in this paper) is to compare parsing accu-
racy when confronted with a choice between vari-
ous parser deployments. Not only are there many
parsing techniques available (Collins, 2003; Char-
niak and Johnson, 2005; Petrov and Klein, 2007;
McClosky et al., 2006; Huang, 2008), but recent
annotation efforts in providing training material for
statistical parsing (LDC, 2005; LDC, 2006a; LDC,
2006b; LDC, 2006c; LDC, 2007) have compounded
the difficulty of the choices (“Do I parse using parser
X?”, “Do I train parser X using the treebank Y or
Z?”). In this section, we show how our predictor can
provide guidance when dealing with some of these
choices, namely the choice of the training material
to use with a statistical parser, prior to its applica-
tion in an NLP task.
For the experiments reported in this paper, we
use as parser P, our in-house implementation of
the Collins parser (Collins, 2003), to which various
</bodyText>
<page confidence="0.997794">
893
</page>
<bodyText confidence="0.999300764705882">
speed-related enhancements (Goodman, 1997) have
been applied. This choice has been made to better
reflect a scenario in which parser P would be used
in a data-intensive application such as syntax-driven
machine translation, in which the parser must be
able to run through hundreds of millions of training
words in a timely manner. We use the more accurate,
but slower Charniak parser (Charniak and Johnson,
2005) as the reference parser Pref in our predictor
(see Section 3.3). In order to predict the Collins-
style parser behavior on the ranking task, we use the
same predictor model (including feature weights and
adjustment parameters) that was used for predicting
Charniak parser behavior on the Brown corpus (Sec-
tion 5).
We compare three training scenarios that make for
three different parsers:
</bodyText>
<listItem confidence="0.9944948">
(1) PWSJ - trained on sections 02-21 of WSJ.
(2) PNews - trained on the union of the English
Chinese Translation Treebank (LDC, 2007) (news
stories from Xinhua News Agency translated from
Chinese into English) and the English Newswire
Translation Treebank (LDC, 2005; LDC, 2006a;
LDC, 2006b; LDC, 2006c) (An-Nahar new stories
translated from Arabic into English).
(3) PWSJ−News - trained on the union of all the
above training material.
</listItem>
<bodyText confidence="0.998123666666667">
When comparing the performance of these three
parsers on a development set from WSJ (section 0),
we get the following F-scores.5
</bodyText>
<table confidence="0.8244784">
Parser WSJ (sec. 0) Accuracy
(F-scores)
PWSJ 88.25
PNew. 83.00
PWSJ−News 88.00
</table>
<bodyText confidence="0.989894153846154">
Consider now that we are interested in compar-
ing the parsing accuracy of these parsers on a do-
main completely different from WSJ. The ranking
PWSJ&gt;PWSJ−News&gt;PNews, given by the evalua-
tion above, provides some guidance, but is this guid-
ance accurate? The intuition here is that the in-
formation that we already have about the new do-
main of interest (which implicitly appears in texts
5Because of tokenization differences between the different
treebanks involved in these experiments, we have to adopt a to-
kenization scheme different from the one used in the Penn Tree-
bank, and therefore the F-scores, albeit in the same range, are
not directly comparable with the ones in the parsing literature.
</bodyText>
<table confidence="0.998126">
Parser Xinhua News Xinhua News
Prediction Accuracy
(F-scores) (F-scores)
PWSJ 85.1 79.14
PNew. 87.0 84.84
PWSJ−News 89.4 85.14
</table>
<tableCaption confidence="0.980227">
Table 6: Performance of predictor on the Xinhua News domain, com-
pared with actual F-scores.
</tableCaption>
<bodyText confidence="0.999933432432433">
extracted from this domain), can be used to bet-
ter guide this decision. Our predictor is able to
capitalize on this information, and provide domain-
informed guidance for choosing the most accurate
parser to use with the new data, which in this case
relates to choosing the best training strategy for the
parser P. If we consider as our domain of interest,
news stories from Xinhua News Agency, then using
our predictor on a chunk of 1866 sentences from this
domain gives the F-scores shown in the second col-
umn of Table 6.
As with the previous experiments, we can com-
pute the actual PARSEVAL F-scores (using gold-
standard) for this particular 1866-sentence test set,
as it happens to be part of the English Chinese Trans-
lation Treebank (LDC, 2007). These F-score fig-
ures are shown in the third column of Table 6. As
these results show, for this particular domain the cor-
rect ranking is PWSJ−News&gt;PNews&gt;PWSJ, which
is exactly the ranking predicted by our method, with-
out the aid of gold-standard trees.
We observe that even though the system predicts
the ranking correctly, the predictions in the Xinhua
News domain might not be as accurate in compar-
ison to the predictions on Brown corpus (predicted
F-score = 86.96, actual F-score = 86.34). One pos-
sible reason for this lower accuracy is that we use
the same prediction model without optimizing for
the particular parser on which we wish to make pre-
dictions. Still, the model was able to make distinc-
tions between multiple parsers for the ranking task
correctly, and decide the best parser to use with the
given data. We believe this to be useful in typical
NLP applications which use parsing as a component,
and where making the right choice between differ-
ent parsers can affect the end-to-end accuracy of the
system.
</bodyText>
<sectionHeader confidence="0.997776" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9926605">
The steady advances in statistical parsing over the
last years have taken this technology to the point
</bodyText>
<page confidence="0.995449">
894
</page>
<bodyText confidence="0.9999979">
where it is accurate enough to be useful in a va-
riety of natural language applications. However,
due to large variations in the characteristics of the
domains for which these applications are devel-
oped, estimating parsing accuracy becomes more
involved than simply taking for granted accuracy
estimates done on a certain well-studied domain,
such as WSJ. As the results in this paper show, it
is possible to take into account these variations in
the domain characteristics (encoded in our predictor
as text-based, syntax-based, and agreement-based
features)—to make better predictions about the ac-
curacy of certain statistical parsers (and under dif-
ferent training scenarios), instead of relying on accu-
racy estimates done on a standard domain. We have
provided a mechanism to incorporate these domain
variations for making predictions about parsing ac-
curacy, without the costly requirement of creating
human annotations for each of the domains of inter-
est. The experiments shown in the paper were lim-
ited to readily available statistical parsers (which are
widely deployed in a number of applications), and
certain domains/genres (because of ready access to
gold-standard data on which we could verify predic-
tions). However, the features we use in our predic-
tor are independent of the particular type of parser
or domain, and the same technique could be applied
for making predictions on other parsers as well.
There are many avenues for future work opened
up by the work presented here. The accuracy of the
predictor can be further improved by incorporating
more complex syntax-based features and multiple-
agreement features. Moreover, rather than predict-
ing an intrinsic metric such as the PARSEVAL F-
score, the metric that the predictor learns to pre-
dict can be chosen to better fit the final metric on
which an end-to-end system is measured, in the style
of (Och, 2003). The end-result is a finely-tuned tool
for predicting the impact of various parser design de-
cisions on the overall quality of a system.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999961">
We wish to acknowledge our colleagues at ISI, who
provided useful suggestions and constructive criti-
cism on this work. We are also grateful to all the
reviewers for their detailed comments. This work
was supported in part by NSF grant IIS-0428020.
</bodyText>
<sectionHeader confidence="0.994334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99908525">
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references.
In Proc. of ACL.
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proc. of ACL.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech &amp; Language, 20(1).
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Proc.
of HLT.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cover-
age of english grammars. In Proc. of Speech and Nat-
ural Language Workshop.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of ACL.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proc. of MT Summit IX. IAMT.
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting
syntactic structure for language modeling. In Proc. of
ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4).
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc.
of HLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. of
ACL.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proc. of EMNLP.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proc. of ACL Work-
shop on Open-Domain Question Answering.
Ayako Hoshino and Hiroshi Nakagawa. 2007. A cloze
test authoring system and its automation. In Proc. of
ICWL.
</reference>
<page confidence="0.988066">
895
</page>
<reference confidence="0.99921775">
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. of ACL.
LDC. 2005. English newswire translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2005E85.
LDC. 2006a. English newswire translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2006E36.
LDC. 2006b. GALE Y1 Q3 release - English translation
treebank. Linguistic Data Consortium, Catalog num-
ber LDC2006E82.
LDC. 2006c. GALE Y1 Q4 release - English translation
treebank. Linguistic Data Consortium, Catalog num-
ber LDC2006E95.
LDC. 2007. English chinese translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2007T02.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In Proc. of NAACL-HLT.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phraases. In
Proc. of EMNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2).
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proc. of COLING-ACL.
Franz Joseph Och. 2003. Minimum error rate training in
machine translation. In Proc. of ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of HLT/NAACL.
Brian Roark. 2001. Probabilistic top-down parsing
and language modelling. Computational Linguistics,
27(2).
A.J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Report
NC2-TR-1998-030.
Radu Soricut. 2006. Natural Language Generation us-
ing an Information-Slim Representation. Ph.D. thesis,
University of Southern California,.
Y. Yang and J. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proc. of
ICML.
</reference>
<page confidence="0.999004">
896
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.184844">
<title confidence="0.998701">Automatic Prediction of Parser Accuracy</title>
<author confidence="0.99435">Sujith Ravi</author>
<author confidence="0.99435">Kevin</author>
<affiliation confidence="0.943109">University of Southern Information Sciences</affiliation>
<address confidence="0.597777">Marina del Rey, California</address>
<email confidence="0.905869">Radu</email>
<affiliation confidence="0.640711">Language Weaver,</affiliation>
<address confidence="0.7640885">4640 Admiralty Way, Suite Marina del Rey, California</address>
<email confidence="0.999928">rsoricut@languageweaver.com</email>
<abstract confidence="0.996994666666666">Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications. However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for sentence-level mt evaluation with pseudo references.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5344" citStr="Albrecht and Hwa, 2007" startWordPosition="875" endWordPosition="878">slations can be produced and measured. Although a real estimate of the impact of a parser design decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision. On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly training procedure. 2 Related Work and Experimental Framework There have been previous studies which explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is neces</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2007. Regression for sentence-level mt evaluation with pseudo references. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Philipp Koehn</author>
</authors>
<title>Enriching morphologically poor languages for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1462" citStr="Avramidis and Koehn, 2008" startWordPosition="215" endWordPosition="218">notation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et </context>
</contexts>
<marker>Avramidis, Koehn, 2008</marker>
<rawString>Eleftherios Avramidis and Philipp Koehn. 2008. Enriching morphologically poor languages for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="5855" citStr="Bacchiani et al., 2006" startWordPosition="949" endWordPosition="952"> problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the nonWSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all l</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech &amp; Language, 20(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Design of a multi-lingual, parallel-processing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proc. of HLT.</booktitle>
<contexts>
<context position="14947" citStr="Bikel, 2002" startWordPosition="2484" endWordPosition="2485">w similar/different are the results from the two parsers. We find that if the parses are similar, they are more likely to be right. In order to compute similarity, we can compare the constituents in the two parse trees from P and Pref, and see how many constituents match. This is most easily accomplished by considering Pref to be a “gold standard” (even though it is not necessarily a correct parse) and computing the F-measure score of parser P against Pref . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref, such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future w</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>Daniel M. Bikel. 2002. Design of a multi-lingual, parallel-processing statistical parsing engine. In Proc. of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In Proc. of Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="2072" citStr="Black et al., 1991" startWordPosition="323" endWordPosition="326">hn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee reg</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of english grammars. In Proc. of Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="6413" citStr="Charniak and Johnson, 2005" startWordPosition="1042" endWordPosition="1045">n domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the nonWSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). Training Set Test Set Sent. Charniak count accuracy WSJ sec. 02-21 WSJ sec. 24 1308 90.48 (39,832 sent.) WSJ sec. 23 2343 91.13 Brown-test 2186 86.34 Here we investigate algorithms for predicting the accuracy of a parser P on sentences, chunks of sentences, and whole corpora. We also investigate and contrast several scenarios for prediction: (1) the predictor looks at the input text only, (2) the predic</context>
<context position="15265" citStr="Charniak and Johnson, 2005" startWordPosition="2537" endWordPosition="2540">ed by considering Pref to be a “gold standard” (even though it is not necessarily a correct parse) and computing the F-measure score of parser P against Pref . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref, such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set dev (r) test (r) Pref 0.40 0.36 3.4 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted accordi</context>
<context position="21278" citStr="Charniak and Johnson, 2005" startWordPosition="3521" endWordPosition="3524">which we use here as a baseline, is to compute the F-score on a set for which we do have gold-standard trees. If we use our development set (WSJ section 891 Figure 1: Plot showing Actual vs. Predicted accuracies for WSJ-test (100-sentence chunks). Each plot point represents a 100-sentence chunk. (rms error = 0.015) System F-measure Charniak F-measure on WSJ-dev (baseline) 90.48 (fd) Predictor (feature weights set with WSJ-dev) 90.85 (fp) Actual Charniak accuracy 91.13 (ft) Table 3: Comparing Charniak parser accuracy (from different systems) on entire WSJ-test corpus 24) for this purpose, and (Charniak and Johnson, 2005) as the parser P, the baseline is an F-score of 90.48 (fd), which is the actual Charniak parser accuracy on WSJ section 24. Instead, if we run our predictor on the test set (a single chunk containing all the sentences in the test set), it predicts an F-score of 90.85 (fp). These two predictions are listed as the first two rows in Table 3. Of course, having the actual gold-standard trees for WSJ section 23 helps us decide which prediction is better: the actual accuracy of the Charniak parser on WSJ section 23 is an F-score of 91.13 (ft), which makes our prediction better than the baseline. 4.1 </context>
<context position="25640" citStr="Charniak and Johnson, 2005" startWordPosition="4304" endWordPosition="4307">us to tune the parameter values for shifting and skewing, we need to apply our predictor on a different test set to see if we get similar improvements by using these techniques, which we do in the next section. 5 Predicting Accuracy on the Brown Corpus The Brown corpus represents a genuine challenge for our predictor, as it presents us with the opportunity to test the performance of our predictor in an out-of-domain scenario. Our predictor, trained on WSJ data, is now employed to predict the performance of a WSJ-trained parser P on the Browntest corpus. As in the previous experiments, we use (Charniak and Johnson, 2005) trained on WSJ sections 02-21 as parser P. The feature weights for our predictor are again trained on section 24 of WSJ, and the shifting and skewing parameters (α = 0.757, Q = 1.0) are determined using section 23 of WSJ. The results on the Brown-test, both the original predictions and after they have been adjusted (shifted/skewed), are shown in Table 4, at different level of chunking. For chunks of size n &gt; 1, the shifting and skewing techniques help in lowering the rms error. On 100-sentence chunks from the Brown test, shifting and skewing (α = 0.757, Q = 1.0) leads to a 20% relative reduct</context>
<context position="28033" citStr="Charniak and Johnson, 2005" startWordPosition="4712" endWordPosition="4716">own test set helps us decide which prediction is better. Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of MT Summit IX. IAMT.</booktitle>
<contexts>
<context position="1351" citStr="Charniak et al., 2003" startWordPosition="195" endWordPosition="198">accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. In Proc. of MT Summit IX. IAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1219" citStr="Chelba and Jelinek, 1998" startWordPosition="176" endWordPosition="180">es. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees pr</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1394" citStr="Collins et al., 2005" startWordPosition="203" endWordPosition="206">ta from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of </context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="28005" citStr="Collins, 2003" startWordPosition="4710" endWordPosition="4711">rees for the Brown test set helps us decide which prediction is better. Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experi</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="1372" citStr="Galley et al., 2004" startWordPosition="199" endWordPosition="202">ser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample s</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inferences and training of context-rich syntax translation models.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4520" citStr="Galley et al., 2006" startWordPosition="734" endWordPosition="738">cted to, the choice of the parser, the choice of the training data, the choice of how to implement various components such as the treatment of unknown words, etc. Altogether, a correct estimation of the impact of such decisions on the resulting parse trees can guide a system designer in a hill-climbing scenario for which an extrinsic metric (such as the impact on the overall quality of the system) is usually too expensive to be employed often enough. To provide an example, a machine translation engine that requires parse trees as training data in order to learn syntax-based translation rules (Galley et al., 2006) needs to employ a syntactic parser as soon as the training process starts, but it can take up to hundreds and even thousands of CPU hours (for large training data sets) to train the engine before translations can be produced and measured. Although a real estimate of the impact of a parser design decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision. On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inferences and training of context-rich syntax translation models. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5831" citStr="Gildea, 2001" startWordPosition="947" endWordPosition="948">h explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the nonWSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measu</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple-pass parsing.</title>
<date>1997</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="28785" citStr="Goodman, 1997" startWordPosition="4839" endWordPosition="4840">l parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various 893 speed-related enhancements (Goodman, 1997) have been applied. This choice has been made to better reflect a scenario in which parser P would be used in a data-intensive application such as syntax-driven machine translation, in which the parser must be able to run through hundreds of millions of training words in a timely manner. We use the more accurate, but slower Charniak parser (Charniak and Johnson, 2005) as the reference parser Pref in our predictor (see Section 3.3). In order to predict the Collinsstyle parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that</context>
</contexts>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple-pass parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
</authors>
<title>Parsing and question classification for question answering.</title>
<date>2001</date>
<booktitle>In Proc. of ACL Workshop on Open-Domain Question Answering.</booktitle>
<contexts>
<context position="1172" citStr="Hermjakob, 2001" startWordPosition="171" endWordPosition="173">the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain o</context>
</contexts>
<marker>Hermjakob, 2001</marker>
<rawString>Ulf Hermjakob. 2001. Parsing and question classification for question answering. In Proc. of ACL Workshop on Open-Domain Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ayako Hoshino</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>A cloze test authoring system and its automation.</title>
<date>2007</date>
<booktitle>In Proc. of ICWL.</booktitle>
<contexts>
<context position="5543" citStr="Hoshino and Nakagawa, 2007" startWordPosition="901" endWordPosition="905">impractical to create such estimates for each design decision. On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly training procedure. 2 Related Work and Experimental Framework There have been previous studies which explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the nonWSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences</context>
</contexts>
<marker>Hoshino, Nakagawa, 2007</marker>
<rawString>Ayako Hoshino and Hiroshi Nakagawa. 2007. A cloze test authoring system and its automation. In Proc. of ICWL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA.</booktitle>
<contexts>
<context position="1434" citStr="Huang et al., 2006" startWordPosition="211" endWordPosition="214"> have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to c</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="15452" citStr="Huang, 2008" startWordPosition="2572" endWordPosition="2573">or prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref, such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set dev (r) test (r) Pref 0.40 0.36 3.4 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set dev (r) test (r) Pref 0.40 0.36 labelSYN 0.33 0.28 lexCount500 0.56 0.23 lexBool500 0.58 0.20 lexCount1000 0.67 0.20 lexB</context>
<context position="28094" citStr="Huang, 2008" startWordPosition="4725" endWordPosition="4726">e much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Coll</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="15391" citStr="Klein and Manning, 2003" startWordPosition="2560" endWordPosition="2563">re of parser P against Pref . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref, such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set dev (r) test (r) Pref 0.40 0.36 3.4 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set dev (r) test (r) Pref 0.40 0.36 labelSYN 0.33 0.28 lexCount5</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>English newswire translation treebank. Linguistic Data Consortium, Catalog number LDC2005E85.</title>
<date>2005</date>
<contexts>
<context position="28191" citStr="LDC, 2005" startWordPosition="4738" endWordPosition="4739">diction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various 893 speed-related enhancements (Goodman, 1997) have </context>
<context position="29809" citStr="LDC, 2005" startWordPosition="5006" endWordPosition="5007">r (see Section 3.3). In order to predict the Collinsstyle parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that was used for predicting Charniak parser behavior on the Brown corpus (Section 5). We compare three training scenarios that make for three different parsers: (1) PWSJ - trained on sections 02-21 of WSJ. (2) PNews - trained on the union of the English Chinese Translation Treebank (LDC, 2007) (news stories from Xinhua News Agency translated from Chinese into English) and the English Newswire Translation Treebank (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c) (An-Nahar new stories translated from Arabic into English). (3) PWSJ−News - trained on the union of all the above training material. When comparing the performance of these three parsers on a development set from WSJ (section 0), we get the following F-scores.5 Parser WSJ (sec. 0) Accuracy (F-scores) PWSJ 88.25 PNew. 83.00 PWSJ−News 88.00 Consider now that we are interested in comparing the parsing accuracy of these parsers on a domain completely different from WSJ. The ranking PWSJ&gt;PWSJ−News&gt;PNews, given by the evaluation above, provides some guidance, bu</context>
</contexts>
<marker>LDC, 2005</marker>
<rawString>LDC. 2005. English newswire translation treebank. Linguistic Data Consortium, Catalog number LDC2005E85.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2006a</author>
</authors>
<title>English newswire translation treebank. Linguistic Data Consortium, Catalog number LDC2006E36.</title>
<marker>2006a, </marker>
<rawString>LDC. 2006a. English newswire translation treebank. Linguistic Data Consortium, Catalog number LDC2006E36.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2006b</author>
</authors>
<title>GALE Y1 Q3 release - English translation treebank. Linguistic Data Consortium, Catalog number LDC2006E82.</title>
<marker>2006b, </marker>
<rawString>LDC. 2006b. GALE Y1 Q3 release - English translation treebank. Linguistic Data Consortium, Catalog number LDC2006E82.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2006c</author>
</authors>
<title>GALE Y1 Q4 release - English translation treebank. Linguistic Data Consortium, Catalog number LDC2006E95.</title>
<marker>2006c, </marker>
<rawString>LDC. 2006c. GALE Y1 Q4 release - English translation treebank. Linguistic Data Consortium, Catalog number LDC2006E95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>English chinese translation treebank. Linguistic Data Consortium, Catalog number LDC2007T02.</title>
<date>2007</date>
<contexts>
<context position="28239" citStr="LDC, 2007" startWordPosition="4746" endWordPosition="4747"> of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various 893 speed-related enhancements (Goodman, 1997) have been applied. This choice has been made to bette</context>
<context position="29676" citStr="LDC, 2007" startWordPosition="4987" endWordPosition="4988">manner. We use the more accurate, but slower Charniak parser (Charniak and Johnson, 2005) as the reference parser Pref in our predictor (see Section 3.3). In order to predict the Collinsstyle parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that was used for predicting Charniak parser behavior on the Brown corpus (Section 5). We compare three training scenarios that make for three different parsers: (1) PWSJ - trained on sections 02-21 of WSJ. (2) PNews - trained on the union of the English Chinese Translation Treebank (LDC, 2007) (news stories from Xinhua News Agency translated from Chinese into English) and the English Newswire Translation Treebank (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c) (An-Nahar new stories translated from Arabic into English). (3) PWSJ−News - trained on the union of all the above training material. When comparing the performance of these three parsers on a development set from WSJ (section 0), we get the following F-scores.5 Parser WSJ (sec. 0) Accuracy (F-scores) PWSJ 88.25 PNew. 83.00 PWSJ−News 88.00 Consider now that we are interested in comparing the parsing accuracy of these parsers o</context>
<context position="31846" citStr="LDC, 2007" startWordPosition="5344" endWordPosition="5345">ovide domaininformed guidance for choosing the most accurate parser to use with the new data, which in this case relates to choosing the best training strategy for the parser P. If we consider as our domain of interest, news stories from Xinhua News Agency, then using our predictor on a chunk of 1866 sentences from this domain gives the F-scores shown in the second column of Table 6. As with the previous experiments, we can compute the actual PARSEVAL F-scores (using goldstandard) for this particular 1866-sentence test set, as it happens to be part of the English Chinese Translation Treebank (LDC, 2007). These F-score figures are shown in the third column of Table 6. As these results show, for this particular domain the correct ranking is PWSJ−News&gt;PNews&gt;PWSJ, which is exactly the ranking predicted by our method, without the aid of gold-standard trees. We observe that even though the system predicts the ranking correctly, the predictions in the Xinhua News domain might not be as accurate in comparison to the predictions on Brown corpus (predicted F-score = 86.96, actual F-score = 86.34). One possible reason for this lower accuracy is that we use the same prediction model without optimizing f</context>
</contexts>
<marker>LDC, 2007</marker>
<rawString>LDC. 2007. English chinese translation treebank. Linguistic Data Consortium, Catalog number LDC2007T02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Source-language features and maximum correlation training for machine translation evaluation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="17728" citStr="Liu and Gildea (2007)" startWordPosition="2944" endWordPosition="2947">ping pieces of the information available, it is to be expected that some of the feature combinations would provide better correlation that the individual features, but the gains are not strictly additive. By taking the individual features that provide the best discriminative power, we are able to get a correlation score of 0.42 on the test set. Feature set dev (r) test (r) Pref + labelSYN + Length + lexCount100 + 0.55 0.42 rootSYN + UNK + LM-PPL 3.5 Optimizing for Maximum Correlation If our goal is to obtain the highest correlations with the F-score measure, is SVM regression the best method? Liu and Gildea (2007) recently introduced Maximum Correlation Training (MCT), a search procedure that follows the gradient of the formula for correlation coefficient (r). We implemented MCT, but obtained no better results. Moreover, it required many random re-starts just to obtain results comparable to SVM regression (Table 1). 4 Predicting Accuracy on Multiple Sentences The results for the scenario presented in Section 3 are encouraging, but other scenarios are also important from a practical perspective. For instance, we are interested in predicting the performance of a particular parser not on a sentence-by-sen</context>
</contexts>
<marker>Liu, Gildea, 2007</marker>
<rawString>Ding Liu and Daniel Gildea. 2007. Source-language features and maximum correlation training for machine translation evaluation. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Spmt: Statistical machine translation with syntactified target language phraases.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1414" citStr="Marcu et al., 2006" startWordPosition="207" endWordPosition="210">ins. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and conseq</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phraases. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2530" citStr="Marcus et al., 1993" startWordPosition="400" endWordPosition="403">e is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee regarding the performance of a parser on a new domain, and, as experiments in this paper show, can give wrong indications regarding important decisions for the design of NLP systems that use a syntactic parser as an important component. This paper proposes another method for measuring the performance of a parser on a given domain that is both cheap and effective. It is a fully automated procedure (no expensive annotation involved) that uses properties of bo</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="5879" citStr="McClosky et al., 2006" startWordPosition="953" endWordPosition="956">y predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the nonWSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consi</context>
<context position="15438" citStr="McClosky et al., 2006" startWordPosition="2568" endWordPosition="2571">re score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref, such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set dev (r) test (r) Pref 0.40 0.36 3.4 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set dev (r) test (r) Pref 0.40 0.36 labelSYN 0.33 0.28 lexCount500 0.56 0.23 lexBool500 0.58 0.20 lexCount1000 </context>
<context position="28080" citStr="McClosky et al., 2006" startWordPosition="4721" endWordPosition="4724">ter. Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementati</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="15415" citStr="Petrov and Klein, 2007" startWordPosition="2564" endWordPosition="2567">ef . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref, such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set dev (r) test (r) Pref 0.40 0.36 3.4 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set dev (r) test (r) Pref 0.40 0.36 labelSYN 0.33 0.28 lexCount500 0.56 0.23 lexBool500 </context>
<context position="28057" citStr="Petrov and Klein, 2007" startWordPosition="4717" endWordPosition="4720"> which prediction is better. Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, ou</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modelling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="1252" citStr="Roark, 2001" startWordPosition="183" endWordPosition="184"> automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modelling. Computational Linguistics, 27(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>B Schoelkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>1998</date>
<tech>NeuroCOLT2 Technical Report NC2-TR-1998-030.</tech>
<contexts>
<context position="9289" citStr="Smola and Schoelkopf, 1998" startWordPosition="1525" endWordPosition="1528"> data for measuring our predictions. For each test sentence, we compute (1) the PARSEVAL F-measure score using the test gold standard, and (2) our predicted F-measure. We report the correlation coefficient (r) between the actual F-scores and our predicted Fscores. We will also use a root-mean-square error (rms error) metric to compare actual and predicted F-scores. Section 3 describes the features used by our predictor. Given these features, as well as actual F-scores computed for the development data, we use supervised learning to set the feature weights. To this end, we use SVM-Regression2 (Smola and Schoelkopf, 1998) with an RBF kernel, to learn the feature weights and build our predictor system.3 We validate the accuracy of the predictor trained in this fashion on both WSJ (Section 4) and the Brown corpus (Section 5). 3 Features Used for Predicting Parser Accuracy 3.1 Text-based Features One hypothesis we explore is that (all other things being equal) longer sentences are harder to parse correctly than shorter sentences. When exposed to the development set, SVM-Regression learns weights to best predict F-scores using the values for this feature corresponding to each sentence in the corpus. Does the predi</context>
</contexts>
<marker>Smola, Schoelkopf, 1998</marker>
<rawString>A.J. Smola and B. Schoelkopf. 1998. A tutorial on support vector regression. NeuroCOLT2 Technical Report NC2-TR-1998-030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
</authors>
<title>Natural Language Generation using an Information-Slim Representation.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California,.</institution>
<contexts>
<context position="1289" citStr="Soricut, 2006" startWordPosition="188" endWordPosition="189">ertain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literat</context>
</contexts>
<marker>Soricut, 2006</marker>
<rawString>Radu Soricut. 2006. Natural Language Generation using an Information-Slim Representation. Ph.D. thesis, University of Southern California,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="12365" citStr="Yang and Pedersen, 1997" startWordPosition="2034" endWordPosition="2037">treatment of the rare words. Feature set dev (r) test (r) LM-PPL 0.11 0.10 We also look at the possibility of automatically detecting certain “cue” words that are appropriate for our prediction problem. That is, we want to see if we can detect certain words that have a discriminating power in deciding whether parsing a sentence that contains them is difficult or easy. To this end, we use a subset of the development data, which contains the 200 best-parsed and 200 worst-parsed sentences (based on F-measure scores). For each word in the development dataset, we compute the information gain (IG) (Yang and Pedersen, 1997) score for that word with respect to the best/worst parsed 4We trained using the SRILM language modeling toolkit, with default settings. 889 dataset. These words are then ranked by their IG scores, and the top 100 words are included as lexical features in our predictor system. As expected, the correlation on the development set is quite high (given that these lexical cues are extracted from this particular set), but a positive correlation holds for the test set as well. Feature set dev (r) test (r) lexCount100 0.43 0.18 3.2 Parser P–based Features Besides exploiting the information present in </context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Y. Yang and J. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. of ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>