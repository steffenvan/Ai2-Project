<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001569">
<title confidence="0.997793">
Simultaneous English-Japanese Spoken Language Translation
Based on Incremental Dependency Parsing and Transfer
</title>
<author confidence="0.878037">
Koichiro Ryu Shigeki Matsubara YasuyoshiInagaki
</author>
<affiliation confidence="0.983095">
Graduate School of Information Technology Center, Faculty of
Information Science, Nagoya University Information Science
Nagoya University Furo-cho, Chikusa-ku, and Technology,
</affiliation>
<address confidence="0.9154485">
Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan Aichi Prefectural University
Nagoya, 464-8601, Japan Nagakute-cho, Aichi-gun,
</address>
<email confidence="0.940458">
ryu@el.itc.nagoya-u.ac.jp Aichi-ken, 480-1198, Japan
</email>
<sectionHeader confidence="0.990878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993703703704">
This paper proposes a method for incre-
mentally translating English spoken lan-
guage into Japanese. To realize simulta-
neous translation between languages with
different word order, such as English and
Japanese, our method utilizes the feature
that the word order of a target language
is flexible. To resolve the problem of
generating a grammatically incorrect sen-
tence, our method uses dependency struc-
tures and Japanese dependency constraints
to determine the word order of a transla-
tion. Moreover, by considering the fact
that the inversion of predicate expressions
occurs more frequently in Japanese spo-
ken language, our method takes advan-
tage of a predicate inversion to resolve the
problem that Japanese has the predicate at
the end of a sentence. Furthermore, our
method includes the function of canceling
an inversion by restating a predicate when
the translation is incomprehensible due to
the inversion. We implement a prototype
translation system and conduct an exper-
iment with all 578 sentences in the ATIS
corpus. The results indicate improvements
in comparison to two other methods.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987728642857143">
Recently, speech-to-speech translation has be-
come one of the important research topics in
machine translation. Projects concerning speech
translation such as TC-STAR (Hoge, 2002) and
DARPA Babylon have been executed, and con-
ferences on spoken language translation such as
IWSLT have been held. Though some speech
translation systems have been developed so far
(Frederking et al., 2002; Isotani et al., 2003; Liu
et al., 2003; Takezawa et al., 1998), these systems,
because of their sentence-by-sentence translation,
cannot start to translate a sentence until it has been
fully uttered. The following problems may arise in
cross-language communication:
</bodyText>
<listItem confidence="0.752368714285714">
• The conversation time become long since it
takes much time to translate
• The listener has to wait for the translation
since such systems increase the difference be-
tween the beginning time of the speaker’s ut-
terance and the beginning time of its transla-
tion
</listItem>
<bodyText confidence="0.999869727272727">
These problems are likely to cause some awk-
wardness in conversations. One effective method
of improving these problems is that a translation
system begins to translate the words without wait-
ing for the end of the speaker’s utterance, much as
a simultaneous interpreter does. This has been ver-
ified as possible by a study on comparing simul-
taneous interpretation with consecutive interpreta-
tion from the viewpoint of efficiency and smooth-
ness of cross-language conversations (Ohara et al.,
2003).
There has also been some research on simulta-
neous machine interpretation with the aim of de-
veloping environments that support multilingual
communication (Mima et al., 1998; Casacuberta
et al., 2002; Matsubara and Inagaki, 1997).
To realize simultaneous translation between
languages with different word order, such as En-
glish and Japanese, our method utilizes the feature
that the word order of a target language is flexi-
ble. To resolve the problem that translation sys-
tems generates grammatically dubious sentence,
</bodyText>
<page confidence="0.988934">
683
</page>
<bodyText confidence="0.974981923076923">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683–690,
Sydney, July 2006. c�2006 Association for Computational Linguistics
our method utilizes dependency structures and
Japanese dependency constraints to determine the
word order of a translation. Moreover, by consid-
ering the fact that the inversion of predicate ex-
pressions occurs more frequently in Japanese spo-
ken language, our method employs predicate in-
version to resolve the problem that Japanese has
the predicate at the end of the sentence. Further-
more, our method features the function of cancel-
ing an inversion by restating a predicate when the
translation is incomprehensible due to the inver-
sion. In the research described in this paper, we
implement a prototype translation system, and to
evaluate it, we conduct an experiment with all 578
sentences in the ATIS corpus.
This paper is organized as follows: Section
2 discusses an important problem in English-
Japanese simultaneous translation and explains the
idea of utilizing flexible word order. Section 3 in-
troduces our method for the generation in English-
Japanese simultaneous translation, and Section 4
describes the configuration of our system. Section
5 reports the experimental results, and the paper
concludes in Section 6.
</bodyText>
<sectionHeader confidence="0.8874425" genericHeader="introduction">
2 Japanese in Simultaneous
English-Japanese Translation
</sectionHeader>
<bodyText confidence="0.999837">
In this section, we describe the problem of the
difference of word order between English and
Japanese in incremental English-Japanese transla-
tion. In addition, we outline an approach of si-
multaneous machine translation utilizing linguis-
tic phenomena, flexible word order, and inversion,
characterizing Japanese speech.
</bodyText>
<subsectionHeader confidence="0.9964185">
2.1 Difference of Word Order between
English and Japanese
</subsectionHeader>
<bodyText confidence="0.991323142857143">
Let us consider the following English:
(E1) I want to fly from San Francisco to Denver
next Monday.
The standard Japanese for (E1) is
(J1) raishu-no (‘next’) getsuyobi-ni (‘Monday’)
San Francisco-kara (‘from’) Denver-he (‘to’)
tobi-tai-to omoi-masu (‘want to fly’).
Figure 1 shows the output timing when the trans-
lation is generated as incrementally as possible
in consideration of the word alignments between
(E1) and (J1). In Fig. 1, the flow of time is shown
from top to bottom. In this study, we assume
that the system translates input words chunk-by-
chunk. We define a simple noun phrase (e.g. San
</bodyText>
<figureCaption confidence="0.9999505">
Figure 1: The output timing of the translation (J1)
Figure 2: The output timing of the translation (J2)
</figureCaption>
<bodyText confidence="0.999391666666667">
Francisco, Denver and next Monday), a predicate
(e.g. want to fly) and each other word (e.g. I, from,
to) as a chunk. There is “raishu-no getsuyobi-ni”
(‘next Monday’) at the beginning of the transla-
tion (J1), and there is “next Monday” correspond-
ing to “raishu-no getsuyobi-ni” at the end of the
sentence (E1). Thus, the system cannot output
“raishu-no getsuyobi-ni” and its following trans-
lation until the whole sentence is uttered. This is
a fatal flaw in incremental English-Japanese trans-
lation because there exists an essential difference
between English and Japanese in the word order. It
is fundamentally impossible to cancel these prob-
lems as long as we assume (J1) to be the transla-
tion of (E1).
</bodyText>
<subsectionHeader confidence="0.9866395">
2.2 Utilizing Flexible Word Order in
Japanese
</subsectionHeader>
<bodyText confidence="0.9999468">
Japanese is a language with a relatively flexible
word order. Thus, it is possible that a Japanese
translation can be accepted even if it keeps the
word order of an English sentence. Let us con-
sider the following Japanese:
</bodyText>
<listItem confidence="0.732378666666667">
(J2) San Francisco-kara (‘from’) Denver-he (‘to’)
tobi-tai-to omoi-masu (‘want to fly’) raishu-no
(‘next’) getsuyobi-ni (‘Monday’).
</listItem>
<bodyText confidence="0.895505571428571">
(J2) can be accepted as the translation of the sen-
tence (E1) and still keep the word order as close as
possible to the sentence (E1). Figure 2 shows the
output timing when the translation is generated as
incrementally as possible in consideration of the
word alignments between (E1) and (J2). The fig-
ure demonstrates that a translation system might
</bodyText>
<page confidence="0.994858">
684
</page>
<bodyText confidence="0.999876555555556">
be able to output “San Francisco -kara (‘from’)”
when “San Francisco” is input and “Denver-he
(‘to’) tobi-tai-to omoi-masu (‘want to fly’)” when
“Denver” is input. If a translation system out-
puts the sentence (J2) as the translation of the
sentence (E1), the system can translate it incre-
mentally. The translation (J2) is not necessarily
an ideal translation because its word order differs
from that of the standard translation and it has an
inverted sentence structure. However the transla-
tion (J2) can be easily understood due to the high
flexibility of word order in Japanese. Moreover, in
spoken language machine translation, the high de-
gree of incrementality is preferred to that of qual-
ity. Therefore, our study positively utilizes flexi-
ble word order and inversion to realize incremen-
tal English-Japanese translation while keeping the
translation quality acceptable.
</bodyText>
<sectionHeader confidence="0.856032" genericHeader="method">
3 Japanese Generation based on
Dependency Structure
</sectionHeader>
<bodyText confidence="0.999964333333333">
When an English-Japanese translation system in-
crementally translates an input sentence by utiliz-
ing flexible word order and inversion, it is pos-
sible that the system will generate a grammati-
cally incorrect Japanese sentence. Therefore, it
is necessary for the system to generate the trans-
lation while maintaining the translation quality at
an acceptable level as a correct Japanese sentence.
In this section, we describe how to generate an
English-Japanese translation that retains the word
order of the input sentence as much as possible
while keeping the quality acceptable.
</bodyText>
<subsectionHeader confidence="0.988288">
3.1 Dependency Grammar in English and
Japanese
</subsectionHeader>
<bodyText confidence="0.999800888888889">
Dependency grammar illustrates the syntactic
structure of a sentence by linking individual
words. In each link, modifiers (dependents) are
connected to the word that they modify (head). In
Japanese, the dependency structure is usually de-
fined in terms of the relation between phrasal units
called bunsetsu1. The Japanese dependency rela-
tions are satisfied with the following constraints
(Kurohashi and Nagao, 1997):
</bodyText>
<listItem confidence="0.9998575">
• No dependency is directed from right to left.
• Dependencies do not cross each other.
</listItem>
<construct confidence="0.696423">
1A bunsetsu is one of the linguistic units in Japanese, and
roughly corresponds to a basic phrase in English. A bunsetsu
consists of one independent word and more than zero ancil-
lary words. A dependency is a modification relation between
two bunsetsus.
</construct>
<figureCaption confidence="0.999992">
Figure 3: The dependency structures of translation (J1)
Figure 4: The dependency structures of translation (J2)
</figureCaption>
<bodyText confidence="0.983959777777778">
• Each bunsetsu, except the last one, depends
on only one bunsetsu.
The translation (J1) is satisfied with these con-
straints as shown in Fig. 3. A sentence satis-
fying these constraints is deemed grammatically
correct sentence in Japanese. To meet this require-
ment, our method parses the dependency relations
between input chunks and generates a translation
satisfying Japanese dependency constraints.
</bodyText>
<subsectionHeader confidence="0.968198">
3.2 Inversion
</subsectionHeader>
<bodyText confidence="0.999747259259259">
In this paper, we call the dependency relations
heading from right to left ”inversions”. Inversions
occur more frequently in spontaneous speech than
in written text in Japanese. That is to say, there
are some sentences in Japanese spoken language
that do not satisfy the constraint mentioned above.
Translation (J2) does not satisfy this constraint, as
shown in Fig. 4. We investigated the inversions
using the CIAIR corpus (Ohno et al., 2003) and
found the following features:
Feature 1 92.2% of the inversions are that the
head bunsetsu of the dependency relation is
a predicate. (predicate inversion)
Feature 2 The more the number of dependency
relations that depend on a predicate increases,
the more the frequency of predicate inver-
sions increases.
Feature 3 There are not three or more inversions
in a sentence.
From Feature 1, our method utilizes a predicate
inversion to retain the word order of an input sen-
tence. It also generates a predicate when the num-
ber of dependency relations that depend on a pred-
icate exceeds the constant R (from Feature 2). If
there are three or more inversions in the transla-
tion, the system cancels an inversion by restating
a predicate (from Feature 3).
</bodyText>
<page confidence="0.996685">
685
</page>
<figureCaption confidence="0.999756">
Figure 5: Configuration of our system
</figureCaption>
<sectionHeader confidence="0.978035" genericHeader="method">
4 System Configuration
</sectionHeader>
<bodyText confidence="0.999935647058823">
Figure 5 shows the configuration of our system.
The system translates an English speech transcript
into Japanese incrementally. It is composed of
three modules: incremental parsing, transfer and
generation. In the parsing module the parser deter-
mines the English dependency structure for input
words incrementally. In the transfer module, struc-
ture and lexicon transfer rules transform the En-
glish dependency structure into the Japanese case
structure. As for the generation module, the sys-
tem judges whether the translation of each chunk
can be output, and if so, outputs the translation
of the chunk. Figure 6 shows the processing flow
when the fragment “I want to fly from San Fran-
cisco to Denver” of 2.1 is input. In the follow-
ing subsections we explain each module, referring
to Fig. 6.
</bodyText>
<subsectionHeader confidence="0.976925">
4.1 Incremental Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.998610307692308">
First, the system performs POS tagging for input
words and chunking (c.f. “Chunk” in Fig. 6).
Next, we explain how to parse the English
phrase structure (c.f. “English phrase structure” in
Fig. 6). When we parse the phrase structure for in-
put words incrementally, there arises the problem
of ambiguity; our method needs to determine only
one parsing result at a time. To resolve this prob-
lem our system selects the phrase structure of the
maximum likelihood at that time by using PCFG
(Probabilistic Context-Free Grammar) rules. To
resolve the problem of the processing time our sys-
tem sets a cut-off value.
</bodyText>
<figureCaption confidence="0.77417">
Figure 6: The translation flow for the fragment “I
want to fly from San Francisco to Denver”
</figureCaption>
<bodyText confidence="0.999814409090909">
Furthermore, the system transforms the English
phrase structure into an English dependency struc-
ture (c.f. “English dependency structure” in Fig.
6). The dependency structure for the sentence can
be computed from the phrase structure for the in-
put words by defining the category for each rule in
CFG, called a ”head child” (Collins, 1999). The
head is indicated using an asterisk * in the phrase
structure of Fig. 6. In the “English phrase struc-
ture,” the chunk in parentheses at each node is
the head chunk of the node that is determined by
the head information of the syntax rules. If the
head chunk (e.g. “from”) of a child node (e.g.
PP(from)) differs from that of its parent node (e.g.
VP(want-to-fly)), the head chunk (e.g. “from”) of
the child node depends on the head chunk (e.g.
“want-to-fly”) of the parent node. Some syntax
rules are also annotated with subject and object
information. Our system uses such information to
add Japanese function words to the translation of
the subject chunk or the object chunk in the gener-
ation module. To use a predicate inversion in the
</bodyText>
<page confidence="0.998048">
686
</page>
<bodyText confidence="0.99975225">
generation module the system has to recognize the
predicate of an input sentence. This system recog-
nizes the chunk (e.g. “want to fly”) on which the
subject chunk (e.g. “I”) depends as a predicate.
</bodyText>
<subsectionHeader confidence="0.989184">
4.2 Incremental Transfer
</subsectionHeader>
<bodyText confidence="0.999988666666667">
In the transfer module, structure and lexicon trans-
ferrules transform the English dependency struc-
ture into the Japanese case structure (“Japanese
case structure” in Fig. 6). In the structure transfer,
the system adds a type of relation to each depen-
dency relation according to the following rules.
</bodyText>
<listItem confidence="0.936657846153846">
• If the dependent chunk of a dependency rela-
tion is a subject or object (e.g. “I”), then the
type of such dependency relation is “subj” or
“obj”.
• If a chunk A (e.g. “San Francisco”) indirectly
depends on another chunk B (e.g. “want-
to-fly”) through a preposition (e.g. “from”),
then the system creates a new dependency re-
lation where A depends on B directly, and the
type of the relation is the preposition.
• The type of the other relations is ”null”.
In the lexicon transfer, the system transforms each
English chunk into its Japanese translation.
</listItem>
<subsectionHeader confidence="0.994256">
4.3 Incremental Generation
</subsectionHeader>
<bodyText confidence="0.999860416666667">
In the generation module, the system transforms
the Japanese case structure into the Japanese de-
pendency structure by translating a particle and
a predicate. In attaching a particle (e.g. “kara”
(from)) to the translation of a chunk (e.g. “San
Francisco”), the system determines the attached
particle (e.g. “kara” (from)) by particle transla-
tion rules. In translating a predicate (e.g. “want
to fly”), the system translates a predicate by pred-
icate translation rules, and outputs the translation
of each chunk using the method described in Sec-
tion 3.
</bodyText>
<subsectionHeader confidence="0.996481">
4.4 Example of Translation Process
</subsectionHeader>
<bodyText confidence="0.77347825">
Figure 7 shows the processing flow for the En-
glish sentence, “I want to fly from San Francisco
to Denver next Monday.” In Fig. 7 the underlined
words indicate that they can be output at that time.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.987261">
5.1 Outline of Experiment
</subsectionHeader>
<bodyText confidence="0.999943545454545">
To evaluate our method, we conducted a transla-
tion experiment was made as follows. We imple-
mented the system in Java language on a 1.0-GHz
PentiumM PC with 512 MB of RAM. The OS was
Windows XP. The experiment used all 578 sen-
tences in the ATIS corpus with a parse tree, in the
Penn Treebank (Marcus et al. 1993). In addition,
we used 533 syntax rules, which were extracted
from the corpus’ parse tree. The position of the
head child in the grammatical rule was defined ac-
cording to Collins’ method (Collins, 1999).
</bodyText>
<subsectionHeader confidence="0.998957">
5.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.997118853658537">
Since an incremental translation system for spo-
ken dialogues is required to realize a quick and
informative response to support smooth communi-
cation, we evaluated the translation results of our
system in terms of both simultaneity and quality.
To evaluate the translation quality of our sys-
tem, each translation result of our system was as-
signed one of four ranks for translation quality by
a human translator:
A (Perfect): no problems in either information or
grammar
B (Fair): easy to understand but some important
information is missing or it is grammatically
flawed
C (Acceptable): broken but understandable with
effort
D (Nonsense): important information has been
translated incorrectly
To evaluate the simultaneity of our system, we
calculated the average delay time for translating
chunks using the following expression:
� k dk , (1)
Average delay time =
n
where dk is the virtual elapsed time from inputting
the kth chunk until outputting its translated chunk.
(When a repetition is used, dk is the elapsed time
from inputting the kth chunk until restate its trans-
lated chunk.) The virtual elapsed time increases
by one unit of time whenever a chunk is input, n
is the total number of chunks in all of the test sen-
tences.
The average delay time is effective for evalu-
ating the simultaneity of translation. However, it
is difficult to evaluate whether our system actu-
ally improves the efficiency of a conversation. To
do so, we measured “the speaker’ and the inter-
preter’s utterance time.” “The speaker’ and the in-
terpreter ’utterance time” runs from the start time
of a speaker’s utterance to the end time of its trans-
lation. We cannot actually measure actual “the
</bodyText>
<page confidence="0.998734">
687
</page>
<tableCaption confidence="0.99984">
Table 1: Comparing our method (Y) with two other methods (X, Z)
</tableCaption>
<table confidence="0.997767571428571">
Method Quality Average Speaker and interpreter
delay time utterance time (sec)
A A+B A+B+C
X 7 (1.2%) 48 (8.3%) 92 (15.9%) 0 4.7
Y 40 (6.9%) 358 (61.9%) 413 (71.5%) 2.79 6.0
������ Z ������ 3.79 6.4
������
</table>
<figureCaption confidence="0.98374">
Figure 8: The relation between the speaker’s ut-
</figureCaption>
<bodyText confidence="0.899091545454546">
terance time and the time from the end time of the
speaker’s utterance to the end time of the transla-
tion
speaker’ and the interpreter’ utterance time” be-
cause our system does not include speech recog-
nition and synthesis. Thus, the processing time
of speech recognition and transfer text-to-speech
synthesis is zero, and the speaker’s utterance time
and the interpreter’s utterance time is calculated
virtually by assuming that the speaker’s and inter-
preter’s utterance speed is 125 ms per mora.
</bodyText>
<subsectionHeader confidence="0.998948">
5.3 Experiment Results
</subsectionHeader>
<bodyText confidence="0.995376906976744">
To evaluate the translation quality and simultane-
ity of our system, we compared the translation re-
sults of our method (Y) with two other methods.
One method (X) translates the input chunks with
no delay time. The other method (Z) translates the
input chunks by waiting for the whole sentence to
be input, in as consecutive translation. We could
not evaluate the translation quality of the method
Z because we have not implemented the method Z.
And we virtually compute the delay time and the
utterance time. Table 1 shows the estimation re-
sults of methods X, Y and Z. Note, however, that
we virtually calculated the average delay time and
the speaker’s and interpreter’s utterance times in
method Z without translating the input sentence.
Table 1 indicates that our method Y achieved
a 55.6% improvement over method X in terms
of translation quality and a 1.0 improvement over
method Z for the average delay time.
Figure 8 shows the relation between the
speaker’s utterance time and the time from the end
time of the speaker’s utterance to the end time of
the translation. According to Fig. 8, the longer a
speaker speaks, the more the system reduces the
time from the end time of the speaker’s utterance
to the end time of the translation.
In Section 3, we explained the constant R. Ta-
ble 2 shows increases in R from 0 to 4, with the
results of the estimation of quality, the average de-
lay time, the number of inverted sentences and the
number of sentences with restatement. When we
set the constant to R = 2, the average delay time
improved by a 0.08 over that of method Y, and
the translation quality did not decrease remark-
ably. Note, however, that method Y did not utilize
any predicate inversions.
To ascertain the problem with our method,
we investigated 165 sentences whose translations
were assigned the level D when the system trans-
lated them by utilizing dependency constraints.
According to the investigation, the system gener-
ated grammatically incorrect sentences in the fol-
lowing cases:
</bodyText>
<listItem confidence="0.948540625">
• There is an interrogative word (e.g. “what”
“which”) in the English sentence (64 sen-
tences).
• There are two or more predicates in the En-
glish sentence (25 sentences).
• There is a coordinate conjunction (e.g.
“and” “or”) in the English sentence (21 sen-
tences).
</listItem>
<bodyText confidence="0.998196">
Other cases of decreases in the translation quality
occurred when a English sentence was ill-formed
or when the system fails to parse.
</bodyText>
<sectionHeader confidence="0.999479" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.996796">
In this paper, we have proposed a method for in-
crementally translating English spoken language
into Japanese. To realize simultaneous translation
</bodyText>
<page confidence="0.998966">
688
</page>
<tableCaption confidence="0.9988">
Table 2: The results of each R (0 &lt; R &lt; 4)
</tableCaption>
<table confidence="0.99849975">
R Quality Average Sentences Sentences
delay time with inversion with restatement
A A+B A+B+C
0 8 (1.4%) 152 (26.3%) 363 (62.8%) 2.51 324 27
1 14 (2.4%) 174 (30.1%) 364 (63.0%) 2.53 289 29
2 36 (6.2%) 306 (52.9%) 396 (68.5%) 2.71 73 5
3 39 (6.7%) 344 (59.5%) 412 (71.3%) 2.79 28 2
4 40 (7.0%) 358 (61.9%) 412 (71.3%) 2.79 3 2
</table>
<bodyText confidence="0.999826764705882">
our method utilizes the feature that word order is
flexible in Japanese, and determines the word or-
der of a translation based on dependency struc-
tures and Japanese dependency constraints. More-
over, our method employs predicate inversion and
repetition to resolve the problem that Japanese has
a predicate at the end of a sentence. We imple-
mented a prototype system and conducted an ex-
periment with 578 sentences in the ATIS corpus.
We evaluated the translation results of our sys-
tem in terms of quality and simultaneity, confirm-
ing that our method achieved a 55.6% improve-
ment over the method of translating by retaining
the word order of an original with respect to trans-
lation quality, and a 1.0 improvement over the
method of consecutive translation regarding aver-
age delay time.
</bodyText>
<sectionHeader confidence="0.991915" genericHeader="acknowledgments">
Acknoledgments
</sectionHeader>
<bodyText confidence="0.99996525">
The authors would like to thank Prof. Dr. Toshiki
Sakabe. They also thank Yoshiyuki Watanabe,
Atsushi Mizuno and translator Sachiko Waki for
their contribution to our study.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982840625">
F. Casacuberta, E. Vidal and J. M. Vilar. 2002. Ar-
chitectures for speech-to-speech translation using
finite-state models, Proceedings of Workshop on
Speech-to-Speech Translation: Algorithms and Sys-
tem, pages 39-44.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing, Ph.D. Thesis, University
of Pennsylvania,
R. Frederking, A. Blackk, R. Brow, J. Moody, and
E. Stein-brecher, 2002. Field Testing the Tongues
Speech-to-Speech Machin Translation System, Pro-
ceedings of the 3rd International Conference on
Language Resources and Evaluation(LREC-2002)
pages 160-164.
H. Hoge. 2002. Project Proposal TC-STAR: Make
Speech to Speech Translation Real, Proceedings of
the 3rd International Conference on Language Re-
sources and Evaluation(LREC-2002), pages 136-
141.
R. Isotani, K. Yamada, S. Ando, K. Hanazawa, S.
Ishikawa and K. Iso. 2003. Speech-to-Speech Trans-
lation Software PDAs for Travel Conversation, NEC
Research and Development, 44, No.2 pages 197-
202.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System,
Proceedings of 4th Natural Language Processing
Pacific Rim Symposium, pages 451-456.
F. Liu, Y. Gao, L. Gu and M. Picheny. 2003. Noise Ro-
bustness in Speech to Speech Translation, IBM Tech
Report RC22874.
M. P. Marcus, B. Santorini and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank, Computational Linguis-
tics, 19(2):310-330.
S. Matsubara and Y. Inagaki. 1997. Incremental Trans-
fer in English-Japanese Machine Translation, IE-
ICE Transactions on Information and Systems,
(11):1122-1129.
H. Mima, H. Iida and O. Furuse. 1998. Simultaneous
Interpretation Utilizing Example-based Incremental
Transfer, Proceedings of 17th International Confer-
ence on Computational Linguistics and 36th Annual
Meeting of Association for Computational Linguis-
tics, pages 855-861.
M. Ohara, S. Matsubara, K. Ryu, N. Kawaguchi and Y.
Inagaki. 2003. Temporal Features of Cross-Lingual
Communication Mediated by Simultaneous Inter-
preting: An Analysis of Parallel Translation Cor-
pus in Comparison to Consecutive Interpreting, The
Journal of the Japan Association for Interpretation
Studies pages 35-53.
T. Ohno, S. Matsubrara, N. Kawaguchi and Y. In-
agaki. 2003. Spiral Construction of Syntactically
Annotated Spoken Language Corpus, Proceedings
of 2003 IEEE International Conference on Natural
Language Processing and Knowledge Engineering,
pages 477-483.
T. Takezawa, T. Morimoto, Y. Sagisaka, N. Campbell,
H. Iida, F. Sugaya, A. Yokoo and S. Yamamoto.
1998. A Japanese-to-English Speech Translation
System:ATR-MATRIX, Proceedings of 5th Interna-
tional Conference on Spoken Language Processing,
pages 957-960.
</reference>
<page confidence="0.9989">
689
</page>
<figureCaption confidence="0.999776">
Figure 7: The translation flow for “I want to fly from San Francisco to Denver next Monday.”
</figureCaption>
<page confidence="0.991745">
690
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.468280">
<title confidence="0.998945">Simultaneous English-Japanese Spoken Language Translation Based on Incremental Dependency Parsing and Transfer</title>
<author confidence="0.995347">Koichiro Ryu Shigeki Matsubara YasuyoshiInagaki</author>
<affiliation confidence="0.866154">Graduate School of Information Technology Center, Faculty of Information Science, Nagoya University Information Science Nagoya University Furo-cho, Chikusa-ku, and Technology, Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan Aichi Prefectural University</affiliation>
<address confidence="0.997241">Nagoya, 464-8601, Japan Nagakute-cho, Aichi-gun, 480-1198, Japan</address>
<abstract confidence="0.999200714285714">This paper proposes a method for incrementally translating English spoken language into Japanese. To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem of generating a grammatically incorrect sentence, our method uses dependency structures and Japanese dependency constraints to determine the word order of a translation. Moreover, by considering the fact that the inversion of predicate expressions occurs more frequently in Japanese spoken language, our method takes advantage of a predicate inversion to resolve the problem that Japanese has the predicate at the end of a sentence. Furthermore, our method includes the function of canceling an inversion by restating a predicate when the translation is incomprehensible due to the inversion. We implement a prototype translation system and conduct an experiment with all 578 sentences in the ATIS corpus. The results indicate improvements in comparison to two other methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>E Vidal</author>
<author>J M Vilar</author>
</authors>
<title>Architectures for speech-to-speech translation using finite-state models,</title>
<date>2002</date>
<booktitle>Proceedings of Workshop on Speech-to-Speech Translation: Algorithms and System,</booktitle>
<pages>39--44</pages>
<contexts>
<context position="3235" citStr="Casacuberta et al., 2002" startWordPosition="474" endWordPosition="477"> One effective method of improving these problems is that a translation system begins to translate the words without waiting for the end of the speaker’s utterance, much as a simultaneous interpreter does. This has been verified as possible by a study on comparing simultaneous interpretation with consecutive interpretation from the viewpoint of efficiency and smoothness of cross-language conversations (Ohara et al., 2003). There has also been some research on simultaneous machine interpretation with the aim of developing environments that support multilingual communication (Mima et al., 1998; Casacuberta et al., 2002; Matsubara and Inagaki, 1997). To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem that translation systems generates grammatically dubious sentence, 683 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683–690, Sydney, July 2006. c�2006 Association for Computational Linguistics our method utilizes dependency structures and Japanese dependency constraints to determine the word order of a translation. Mo</context>
</contexts>
<marker>Casacuberta, Vidal, Vilar, 2002</marker>
<rawString>F. Casacuberta, E. Vidal and J. M. Vilar. 2002. Architectures for speech-to-speech translation using finite-state models, Proceedings of Workshop on Speech-to-Speech Translation: Algorithms and System, pages 39-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing,</title>
<date>1999</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="13389" citStr="Collins, 1999" startWordPosition="2088" endWordPosition="2089">ucture of the maximum likelihood at that time by using PCFG (Probabilistic Context-Free Grammar) rules. To resolve the problem of the processing time our system sets a cut-off value. Figure 6: The translation flow for the fragment “I want to fly from San Francisco to Denver” Furthermore, the system transforms the English phrase structure into an English dependency structure (c.f. “English dependency structure” in Fig. 6). The dependency structure for the sentence can be computed from the phrase structure for the input words by defining the category for each rule in CFG, called a ”head child” (Collins, 1999). The head is indicated using an asterisk * in the phrase structure of Fig. 6. In the “English phrase structure,” the chunk in parentheses at each node is the head chunk of the node that is determined by the head information of the syntax rules. If the head chunk (e.g. “from”) of a child node (e.g. PP(from)) differs from that of its parent node (e.g. VP(want-to-fly)), the head chunk (e.g. “from”) of the child node depends on the head chunk (e.g. “want-to-fly”) of the parent node. Some syntax rules are also annotated with subject and object information. Our system uses such information to add J</context>
<context position="16574" citStr="Collins, 1999" startWordPosition="2633" endWordPosition="2634">derlined words indicate that they can be output at that time. 5 Experiment 5.1 Outline of Experiment To evaluate our method, we conducted a translation experiment was made as follows. We implemented the system in Java language on a 1.0-GHz PentiumM PC with 512 MB of RAM. The OS was Windows XP. The experiment used all 578 sentences in the ATIS corpus with a parse tree, in the Penn Treebank (Marcus et al. 1993). In addition, we used 533 syntax rules, which were extracted from the corpus’ parse tree. The position of the head child in the grammatical rule was defined according to Collins’ method (Collins, 1999). 5.2 Evaluation Metric Since an incremental translation system for spoken dialogues is required to realize a quick and informative response to support smooth communication, we evaluated the translation results of our system in terms of both simultaneity and quality. To evaluate the translation quality of our system, each translation result of our system was assigned one of four ranks for translation quality by a human translator: A (Perfect): no problems in either information or grammar B (Fair): easy to understand but some important information is missing or it is grammatically flawed C (Acc</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing, Ph.D. Thesis, University of Pennsylvania,</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frederking</author>
<author>A Blackk</author>
<author>R Brow</author>
<author>J Moody</author>
<author>E Stein-brecher</author>
</authors>
<title>Field Testing the Tongues Speech-to-Speech Machin Translation System,</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd International Conference on Language Resources and Evaluation(LREC-2002)</booktitle>
<pages>160--164</pages>
<contexts>
<context position="2014" citStr="Frederking et al., 2002" startWordPosition="281" endWordPosition="284">s incomprehensible due to the inversion. We implement a prototype translation system and conduct an experiment with all 578 sentences in the ATIS corpus. The results indicate improvements in comparison to two other methods. 1 Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken language translation such as IWSLT have been held. Though some speech translation systems have been developed so far (Frederking et al., 2002; Isotani et al., 2003; Liu et al., 2003; Takezawa et al., 1998), these systems, because of their sentence-by-sentence translation, cannot start to translate a sentence until it has been fully uttered. The following problems may arise in cross-language communication: • The conversation time become long since it takes much time to translate • The listener has to wait for the translation since such systems increase the difference between the beginning time of the speaker’s utterance and the beginning time of its translation These problems are likely to cause some awkwardness in conversations. On</context>
</contexts>
<marker>Frederking, Blackk, Brow, Moody, Stein-brecher, 2002</marker>
<rawString>R. Frederking, A. Blackk, R. Brow, J. Moody, and E. Stein-brecher, 2002. Field Testing the Tongues Speech-to-Speech Machin Translation System, Proceedings of the 3rd International Conference on Language Resources and Evaluation(LREC-2002) pages 160-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoge</author>
</authors>
<title>Project Proposal TC-STAR: Make Speech to Speech Translation Real,</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd International Conference on Language Resources and Evaluation(LREC-2002),</booktitle>
<pages>136--141</pages>
<contexts>
<context position="1808" citStr="Hoge, 2002" startWordPosition="250" endWordPosition="251">olve the problem that Japanese has the predicate at the end of a sentence. Furthermore, our method includes the function of canceling an inversion by restating a predicate when the translation is incomprehensible due to the inversion. We implement a prototype translation system and conduct an experiment with all 578 sentences in the ATIS corpus. The results indicate improvements in comparison to two other methods. 1 Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken language translation such as IWSLT have been held. Though some speech translation systems have been developed so far (Frederking et al., 2002; Isotani et al., 2003; Liu et al., 2003; Takezawa et al., 1998), these systems, because of their sentence-by-sentence translation, cannot start to translate a sentence until it has been fully uttered. The following problems may arise in cross-language communication: • The conversation time become long since it takes much time to translate • The listener has to wait for the translation since</context>
</contexts>
<marker>Hoge, 2002</marker>
<rawString>H. Hoge. 2002. Project Proposal TC-STAR: Make Speech to Speech Translation Real, Proceedings of the 3rd International Conference on Language Resources and Evaluation(LREC-2002), pages 136-141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Isotani</author>
<author>K Yamada</author>
<author>S Ando</author>
<author>K Hanazawa</author>
<author>S Ishikawa</author>
<author>K Iso</author>
</authors>
<title>Speech-to-Speech Translation Software PDAs for Travel Conversation,</title>
<date>2003</date>
<journal>NEC Research and Development,</journal>
<volume>44</volume>
<pages>197--202</pages>
<contexts>
<context position="2036" citStr="Isotani et al., 2003" startWordPosition="285" endWordPosition="288"> the inversion. We implement a prototype translation system and conduct an experiment with all 578 sentences in the ATIS corpus. The results indicate improvements in comparison to two other methods. 1 Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken language translation such as IWSLT have been held. Though some speech translation systems have been developed so far (Frederking et al., 2002; Isotani et al., 2003; Liu et al., 2003; Takezawa et al., 1998), these systems, because of their sentence-by-sentence translation, cannot start to translate a sentence until it has been fully uttered. The following problems may arise in cross-language communication: • The conversation time become long since it takes much time to translate • The listener has to wait for the translation since such systems increase the difference between the beginning time of the speaker’s utterance and the beginning time of its translation These problems are likely to cause some awkwardness in conversations. One effective method of </context>
</contexts>
<marker>Isotani, Yamada, Ando, Hanazawa, Ishikawa, Iso, 2003</marker>
<rawString>R. Isotani, K. Yamada, S. Ando, K. Hanazawa, S. Ishikawa and K. Iso. 2003. Speech-to-Speech Translation Software PDAs for Travel Conversation, NEC Research and Development, 44, No.2 pages 197-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Building a Japanese Parsed Corpus while Improving the Parsing System,</title>
<date>1997</date>
<booktitle>Proceedings of 4th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>451--456</pages>
<contexts>
<context position="9394" citStr="Kurohashi and Nagao, 1997" startWordPosition="1434" endWordPosition="1437">describe how to generate an English-Japanese translation that retains the word order of the input sentence as much as possible while keeping the quality acceptable. 3.1 Dependency Grammar in English and Japanese Dependency grammar illustrates the syntactic structure of a sentence by linking individual words. In each link, modifiers (dependents) are connected to the word that they modify (head). In Japanese, the dependency structure is usually defined in terms of the relation between phrasal units called bunsetsu1. The Japanese dependency relations are satisfied with the following constraints (Kurohashi and Nagao, 1997): • No dependency is directed from right to left. • Dependencies do not cross each other. 1A bunsetsu is one of the linguistic units in Japanese, and roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and more than zero ancillary words. A dependency is a modification relation between two bunsetsus. Figure 3: The dependency structures of translation (J1) Figure 4: The dependency structures of translation (J2) • Each bunsetsu, except the last one, depends on only one bunsetsu. The translation (J1) is satisfied with these constraints as shown in Fig. 3. </context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>S. Kurohashi and M. Nagao. 1997. Building a Japanese Parsed Corpus while Improving the Parsing System, Proceedings of 4th Natural Language Processing Pacific Rim Symposium, pages 451-456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>Y Gao</author>
<author>L Gu</author>
<author>M Picheny</author>
</authors>
<title>Noise Robustness in Speech to Speech Translation,</title>
<date>2003</date>
<tech>IBM Tech Report RC22874.</tech>
<contexts>
<context position="2054" citStr="Liu et al., 2003" startWordPosition="289" endWordPosition="292">lement a prototype translation system and conduct an experiment with all 578 sentences in the ATIS corpus. The results indicate improvements in comparison to two other methods. 1 Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken language translation such as IWSLT have been held. Though some speech translation systems have been developed so far (Frederking et al., 2002; Isotani et al., 2003; Liu et al., 2003; Takezawa et al., 1998), these systems, because of their sentence-by-sentence translation, cannot start to translate a sentence until it has been fully uttered. The following problems may arise in cross-language communication: • The conversation time become long since it takes much time to translate • The listener has to wait for the translation since such systems increase the difference between the beginning time of the speaker’s utterance and the beginning time of its translation These problems are likely to cause some awkwardness in conversations. One effective method of improving these pr</context>
</contexts>
<marker>Liu, Gao, Gu, Picheny, 2003</marker>
<rawString>F. Liu, Y. Gao, L. Gu and M. Picheny. 2003. Noise Robustness in Speech to Speech Translation, IBM Tech Report RC22874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank,</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="16372" citStr="Marcus et al. 1993" startWordPosition="2597" endWordPosition="2600">g the method described in Section 3. 4.4 Example of Translation Process Figure 7 shows the processing flow for the English sentence, “I want to fly from San Francisco to Denver next Monday.” In Fig. 7 the underlined words indicate that they can be output at that time. 5 Experiment 5.1 Outline of Experiment To evaluate our method, we conducted a translation experiment was made as follows. We implemented the system in Java language on a 1.0-GHz PentiumM PC with 512 MB of RAM. The OS was Windows XP. The experiment used all 578 sentences in the ATIS corpus with a parse tree, in the Penn Treebank (Marcus et al. 1993). In addition, we used 533 syntax rules, which were extracted from the corpus’ parse tree. The position of the head child in the grammatical rule was defined according to Collins’ method (Collins, 1999). 5.2 Evaluation Metric Since an incremental translation system for spoken dialogues is required to realize a quick and informative response to support smooth communication, we evaluated the translation results of our system in terms of both simultaneity and quality. To evaluate the translation quality of our system, each translation result of our system was assigned one of four ranks for transl</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank, Computational Linguistics, 19(2):310-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Matsubara</author>
<author>Y Inagaki</author>
</authors>
<date>1997</date>
<booktitle>Incremental Transfer in English-Japanese Machine Translation, IEICE Transactions on Information and Systems,</booktitle>
<pages>11--1122</pages>
<contexts>
<context position="3265" citStr="Matsubara and Inagaki, 1997" startWordPosition="478" endWordPosition="481">mproving these problems is that a translation system begins to translate the words without waiting for the end of the speaker’s utterance, much as a simultaneous interpreter does. This has been verified as possible by a study on comparing simultaneous interpretation with consecutive interpretation from the viewpoint of efficiency and smoothness of cross-language conversations (Ohara et al., 2003). There has also been some research on simultaneous machine interpretation with the aim of developing environments that support multilingual communication (Mima et al., 1998; Casacuberta et al., 2002; Matsubara and Inagaki, 1997). To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem that translation systems generates grammatically dubious sentence, 683 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683–690, Sydney, July 2006. c�2006 Association for Computational Linguistics our method utilizes dependency structures and Japanese dependency constraints to determine the word order of a translation. Moreover, by considering the fac</context>
</contexts>
<marker>Matsubara, Inagaki, 1997</marker>
<rawString>S. Matsubara and Y. Inagaki. 1997. Incremental Transfer in English-Japanese Machine Translation, IEICE Transactions on Information and Systems, (11):1122-1129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mima</author>
<author>H Iida</author>
<author>O Furuse</author>
</authors>
<title>Simultaneous Interpretation Utilizing Example-based Incremental Transfer,</title>
<date>1998</date>
<booktitle>Proceedings of 17th International Conference on Computational Linguistics and 36th Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>855--861</pages>
<contexts>
<context position="3209" citStr="Mima et al., 1998" startWordPosition="470" endWordPosition="473">s in conversations. One effective method of improving these problems is that a translation system begins to translate the words without waiting for the end of the speaker’s utterance, much as a simultaneous interpreter does. This has been verified as possible by a study on comparing simultaneous interpretation with consecutive interpretation from the viewpoint of efficiency and smoothness of cross-language conversations (Ohara et al., 2003). There has also been some research on simultaneous machine interpretation with the aim of developing environments that support multilingual communication (Mima et al., 1998; Casacuberta et al., 2002; Matsubara and Inagaki, 1997). To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem that translation systems generates grammatically dubious sentence, 683 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683–690, Sydney, July 2006. c�2006 Association for Computational Linguistics our method utilizes dependency structures and Japanese dependency constraints to determine the word </context>
</contexts>
<marker>Mima, Iida, Furuse, 1998</marker>
<rawString>H. Mima, H. Iida and O. Furuse. 1998. Simultaneous Interpretation Utilizing Example-based Incremental Transfer, Proceedings of 17th International Conference on Computational Linguistics and 36th Annual Meeting of Association for Computational Linguistics, pages 855-861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ohara</author>
<author>S Matsubara</author>
<author>K Ryu</author>
<author>N Kawaguchi</author>
<author>Y Inagaki</author>
</authors>
<title>Temporal Features of Cross-Lingual Communication Mediated by Simultaneous Interpreting: An Analysis of Parallel Translation Corpus in Comparison to Consecutive Interpreting,</title>
<date>2003</date>
<journal>The Journal of the Japan Association for Interpretation Studies</journal>
<pages>35--53</pages>
<contexts>
<context position="3036" citStr="Ohara et al., 2003" startWordPosition="444" endWordPosition="447">tems increase the difference between the beginning time of the speaker’s utterance and the beginning time of its translation These problems are likely to cause some awkwardness in conversations. One effective method of improving these problems is that a translation system begins to translate the words without waiting for the end of the speaker’s utterance, much as a simultaneous interpreter does. This has been verified as possible by a study on comparing simultaneous interpretation with consecutive interpretation from the viewpoint of efficiency and smoothness of cross-language conversations (Ohara et al., 2003). There has also been some research on simultaneous machine interpretation with the aim of developing environments that support multilingual communication (Mima et al., 1998; Casacuberta et al., 2002; Matsubara and Inagaki, 1997). To realize simultaneous translation between languages with different word order, such as English and Japanese, our method utilizes the feature that the word order of a target language is flexible. To resolve the problem that translation systems generates grammatically dubious sentence, 683 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683–</context>
</contexts>
<marker>Ohara, Matsubara, Ryu, Kawaguchi, Inagaki, 2003</marker>
<rawString>M. Ohara, S. Matsubara, K. Ryu, N. Kawaguchi and Y. Inagaki. 2003. Temporal Features of Cross-Lingual Communication Mediated by Simultaneous Interpreting: An Analysis of Parallel Translation Corpus in Comparison to Consecutive Interpreting, The Journal of the Japan Association for Interpretation Studies pages 35-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ohno</author>
<author>S Matsubrara</author>
<author>N Kawaguchi</author>
<author>Y Inagaki</author>
</authors>
<title>Spiral Construction of Syntactically Annotated Spoken Language Corpus,</title>
<date>2003</date>
<booktitle>Proceedings of 2003 IEEE International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<pages>477--483</pages>
<contexts>
<context position="10707" citStr="Ohno et al., 2003" startWordPosition="1642" endWordPosition="1645">o meet this requirement, our method parses the dependency relations between input chunks and generates a translation satisfying Japanese dependency constraints. 3.2 Inversion In this paper, we call the dependency relations heading from right to left ”inversions”. Inversions occur more frequently in spontaneous speech than in written text in Japanese. That is to say, there are some sentences in Japanese spoken language that do not satisfy the constraint mentioned above. Translation (J2) does not satisfy this constraint, as shown in Fig. 4. We investigated the inversions using the CIAIR corpus (Ohno et al., 2003) and found the following features: Feature 1 92.2% of the inversions are that the head bunsetsu of the dependency relation is a predicate. (predicate inversion) Feature 2 The more the number of dependency relations that depend on a predicate increases, the more the frequency of predicate inversions increases. Feature 3 There are not three or more inversions in a sentence. From Feature 1, our method utilizes a predicate inversion to retain the word order of an input sentence. It also generates a predicate when the number of dependency relations that depend on a predicate exceeds the constant R </context>
</contexts>
<marker>Ohno, Matsubrara, Kawaguchi, Inagaki, 2003</marker>
<rawString>T. Ohno, S. Matsubrara, N. Kawaguchi and Y. Inagaki. 2003. Spiral Construction of Syntactically Annotated Spoken Language Corpus, Proceedings of 2003 IEEE International Conference on Natural Language Processing and Knowledge Engineering, pages 477-483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>T Morimoto</author>
<author>Y Sagisaka</author>
<author>N Campbell</author>
<author>H Iida</author>
<author>F Sugaya</author>
<author>A Yokoo</author>
<author>S Yamamoto</author>
</authors>
<title>A Japanese-to-English Speech Translation System:ATR-MATRIX,</title>
<date>1998</date>
<booktitle>Proceedings of 5th International Conference on Spoken Language Processing,</booktitle>
<pages>957--960</pages>
<contexts>
<context position="2078" citStr="Takezawa et al., 1998" startWordPosition="293" endWordPosition="296"> translation system and conduct an experiment with all 578 sentences in the ATIS corpus. The results indicate improvements in comparison to two other methods. 1 Introduction Recently, speech-to-speech translation has become one of the important research topics in machine translation. Projects concerning speech translation such as TC-STAR (Hoge, 2002) and DARPA Babylon have been executed, and conferences on spoken language translation such as IWSLT have been held. Though some speech translation systems have been developed so far (Frederking et al., 2002; Isotani et al., 2003; Liu et al., 2003; Takezawa et al., 1998), these systems, because of their sentence-by-sentence translation, cannot start to translate a sentence until it has been fully uttered. The following problems may arise in cross-language communication: • The conversation time become long since it takes much time to translate • The listener has to wait for the translation since such systems increase the difference between the beginning time of the speaker’s utterance and the beginning time of its translation These problems are likely to cause some awkwardness in conversations. One effective method of improving these problems is that a transla</context>
</contexts>
<marker>Takezawa, Morimoto, Sagisaka, Campbell, Iida, Sugaya, Yokoo, Yamamoto, 1998</marker>
<rawString>T. Takezawa, T. Morimoto, Y. Sagisaka, N. Campbell, H. Iida, F. Sugaya, A. Yokoo and S. Yamamoto. 1998. A Japanese-to-English Speech Translation System:ATR-MATRIX, Proceedings of 5th International Conference on Spoken Language Processing, pages 957-960.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>