<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000379">
<title confidence="0.993321">
Near-Synonym Choice in an Intelligent Thesaurus
</title>
<author confidence="0.995033">
Diana Inkpen
</author>
<affiliation confidence="0.9985075">
School of Information Technology and Engineering,
University of Ottawa
</affiliation>
<address confidence="0.888277">
800 King Edward, Ottawa, ON, Canada, K1N 6N5
</address>
<email confidence="0.998644">
diana@site.uottawa.ca
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934529411765">
An intelligent thesaurus assists a writer
with alternative choices of words and or-
ders them by their suitability in the writ-
ing context. In this paper we focus on
methods for automatically choosing near-
synonyms by their semantic coherence
with the context. Our statistical method
uses the Web as a corpus to compute
mutual information scores. Evaluation
experiments show that this method per-
forms better than a previous method on
the same task. We also propose and evalu-
ate two more methods, one that uses anti-
collocations, and one that uses supervised
learning. To asses the difficulty of the
task, we present results obtained by hu-
man judges.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968521739131">
When composing a text, a writer can access a the-
saurus to retrieve words that are similar to a given
target word, when there is a need to avoid repeating
the same word, or when the word does not seem to
be the best choice in the context.
Our intelligent thesaurus is an interactive appli-
cation that presents the user with a list of alterna-
tive words (near-synonyms), and, unlike standard
thesauri, it orders the choices by their suitability to
the writing context. We investigate how the collo-
cational properties of near-synonyms can help with
choosing the best words. This problem is difficult
because the near-synonyms have senses that are very
close to each other, and therefore they occur in sim-
ilar contexts; we need to capture the subtle differ-
ences specific to each near-synonym.
Our thesaurus brings up only alternatives that
have the same part-of-speech with the target word.
The choices could come from various inventories
of near-synonyms or similar words, for example the
Roget thesaurus (Roget, 1852), dictionaries of syn-
onyms (Hayakawa, 1994), or clusters acquired from
corpora (Lin, 1998).
In this paper we focus on the task of automat-
ically selecting the best near-synonym that should
be used in a particular context. The natural way to
validate an algorithm for this task would be to ask
human readers to evaluate the quality of the algo-
rithm’s output, but this kind of evaluation would be
very laborious. Instead, we validate the algorithms
by deleting selected words from sample sentences,
to see whether the algorithms can restore the miss-
ing words. That is, we create a lexical gap and eval-
uate the ability of the algorithms to fill the gap. Two
examples are presented in Figure 1. All the near-
synonyms of the original word, including the word
itself, become the choices in the solution set (see the
figure for two examples of solution sets). The task is
to automatically fill the gap with the best choice in
the particular context. We present a method of scor-
ing the choices. The highest scoring near-synonym
will be chosen. In order to evaluate how well our
method works we consider that the only correct so-
lution is the original word. This will cause our eval-
uation scores to underestimate the performance, as
more than one choice will sometimes be a perfect
</bodyText>
<page confidence="0.982882">
356
</page>
<note confidence="0.863968666666667">
Proceedings of NAACL HLT 2007, pages 356–363,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
Sentence: This could be improved by more detailed considera-
</note>
<bodyText confidence="0.574394">
tion of the processes of propagation inherent in digitizing
procedures.
Original near-synonym: error
Solution set: mistake, blooper, blunder, boner, contretemps,
error, faux pas, goof, slip, solecism
Sentence: The day after this raid was the official start of oper-
ation strangle, an attempt to completely destroy the lines
of communication.
</bodyText>
<keyword confidence="0.460158666666667">
Original near-synonym: enemy
Solution set: opponent, adversary, antagonist, competitor, en-
emy, foe, rival
</keyword>
<figureCaption confidence="0.9568695">
Figure 1: Examples of sentences with a lexical gap,
and candidate near-synonyms to fill the gap.
</figureCaption>
<bodyText confidence="0.9970364375">
solution. Moreover, what we consider to be the best
choice is the typical usage in the corpus, but it may
vary from writer to writer. Nonetheless, it is a con-
venient way of producing test data.
The statistical method that we propose here is
based on semantic coherence scores (based on mu-
tual information) of each candidate with the words
in the context. We explore how far such a method
can go when using the Web as a corpus. We estimate
the counts by using the Waterloo MultiText1 system
(Clarke and Terra, 2003b) with a corpus of about
one terabyte of text collected by a Web crawler. We
also propose a method that uses collocations and
anti-collocations, and a supervised method that uses
words and mutual information scores as featured for
machine learning.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.989687583333333">
The idea of using the Web as a corpus of texts
has been exploited by many researchers. Grefen-
stette (1999) used the Web for example-based ma-
chine translation; Kilgarriff (2001) investigated the
type of noise in Web data; Mihalcea and Moldovan
(1999) and Agirre and Martinez (2000) used it as an
additional resource for word sense disambiguation;
Resnik (1999) mined the Web for bilingual texts;
Turney (2001) used Web frequency counts to com-
pute information retrieval-based mutual-information
scores. In a Computational Linguistics special issue
on the Web as a corpus (Kilgarriff and Grefenstette,
</bodyText>
<footnote confidence="0.886629333333333">
1We thank Egidio Terra, Charlie Clarke, and Univ. of Wa-
terloo for allowing us to use MultiText, and to Peter Turney and
IIT/NRC for giving us access to their local copy of the corpus.
</footnote>
<bodyText confidence="0.999746181818182">
2003), Keller and Lapata (2003) show that Web
counts correlate well with counts collected from a
balanced corpus: the size of the Web compensates
for the noise in the data. In this paper we are using a
very large corpus of Web pages to address a problem
that has not been successfully solved before.
In fact, the only work that addresses exactly the
same task is that of Edmonds (1997), as far as we
are aware. Edmonds gives a solution based on a
lexical co-occurrence network that included second-
order co-occurrences. We use a much larger corpus
and a simpler method, and we obtain much better
results.
Our task has similarities to the word sense disam-
biguation task. Our near-synonyms have senses that
are very close to each other. In Senseval, some of
the fine-grained senses are also close to each other,
so they might occur in similar contexts, while the
coarse-grained senses are expected to occur in dis-
tinct contexts. In our case, the near-synonyms are
different words to choose from, not the same word
with different senses.
</bodyText>
<sectionHeader confidence="0.9800555" genericHeader="method">
3 A statistical method for near-synonym
choice
</sectionHeader>
<bodyText confidence="0.999953857142857">
Our method computes a score for each candidate
near-synonym that could fill in the gap. The near-
synonym with the highest score is the proposed so-
lution. The score for each candidate reflects how
well a near-synonym fits in with the context. It is
based on the mutual information scores between a
near-synonym and the content words in the context
(we filter out the stopwords).
The pointwise mutual information (PMI) be-
tween two words x and y compares the probabil-
ity of observing the two words together (their joint
probability) to the probabilities of observing x and y
independently (the probability of occurring together
by chance) (Church and Hanks, 1991): PMI(x, y) =
</bodyText>
<equation confidence="0.86959675">
P (x,y)
lo92 P(x)P(y)
The probabilities can be approximated by:
P(x) = C(x)/N, P(y) = C(y)/N, P(x, y) =
C(x, y)/N, where C denotes frequency counts and
N is the total number of words in the corpus. There-
fore: PMI(x, y) = lo92 C(x)·C(y), where N can be
C(x,y)·N
</equation>
<bodyText confidence="0.9556315">
ignored in comparisons.
We model the context as a window of size 2k
</bodyText>
<page confidence="0.983958">
357
</page>
<figure confidence="0.9979852">
ghastly mistake
*ghastly error
ghastly blunder
*ghastly faux pas
*ghastly blooper
*ghastly solecism
*ghastly goof
*ghastly contretemps
*ghastly boner
*ghastly slip
spelling mistake
spelling error
*spelling blunder
*spelling faux pas
*spelling blooper
*spelling solecism
*spelling goof
*spelling contretemps
*spelling boner
*spelling slip
</figure>
<bodyText confidence="0.999758333333333">
around the gap (the missing word): k words to the
left and k words to the right of the gap. If the sen-
tence is s = · · · w1 · · · wk Gap wk+1 · · · w2k · · ·,
for each near-synonym NSi from the group of can-
didates, the semantic coherence score is computed
by the following formula:
</bodyText>
<equation confidence="0.964185333333333">
Score(NSi, s) = Ekj=1PMI(NSi, wj) +
�2k
j=k+1PMI(NSi, wj).
</equation>
<bodyText confidence="0.999580735294118">
We also experimented with the same formula
when the sum is replaced with maximum to see
whether a particular word in the context has higher
influence than the sum of all contributions (though
the sum worked better).
Because we are using the Waterloo terabyte cor-
pus and we issue queries to its search engine,
we have several possibilities of computing the fre-
quency counts. C(x, y) can be the number of co-
occurrences of x and y when y immediately follows
x, or the distance between x and y can be up to q.
We call q the query frame size. The tool for access-
ing the corpus allows us to use various values for q.
The search engine also allows us to approxi-
mate words counts with document counts. If the
counts C(x), C(y), and C(x, y) are approximated
as the number of document in which they appear,
we obtain the PMI-IR formula (Turney, 2001). The
queries we need to send to the search engine are
the same but they are restricted to document counts:
C(x) is the number of document in which x occurs;
C(x, y) is the number of documents in which x is
followed by y in a frame of size q.
Other statistical association measures, such as
log-likelihood, could be used. We tried only PMI
because it is easy to compute on a Web corpus and
because PMI performed better than other measures
in (Clarke and Terra, 2003a).
We present the results in Section 6.1, where we
compare our method to a baseline algorithm that al-
ways chooses the most frequent near-synonyms and
to Edmonds’s method for the same task, on the same
data set. First, however, we present two other meth-
ods to which we compare our results.
</bodyText>
<sectionHeader confidence="0.988476" genericHeader="method">
4 The anti-collocations method
</sectionHeader>
<bodyText confidence="0.945821571428572">
For the task of near-synonym choice, another
method that we implemented is the anti-collocations
method. By anti-collocation we mean a combina-
Table 1: Examples of collocations and anti-
collocations. The ∗ indicates the anti-collocations.
tion of words that a native speaker would not use
and therefore should not be used when automatically
generating text. This method uses a knowledge-
base of collocational behavior of near-synonyms ac-
quired in previous work (Inkpen and Hirst, 2006). A
fragment of the knowledge-base is presented in Ta-
ble 1, for the near-synonyms of the word error and
two collocate words ghastly and spelling. The lines
marked by ∗ represent anti-collocations and the rest
represent strong collocations.
The anti-collocations method simply ranks the
strong collocations higher than the anti-collocations.
In case of ties it chooses the most frequent near-
synonym. In Section 6.2 we present the results of
comparing this method to the method from the pre-
vious section.
</bodyText>
<sectionHeader confidence="0.968898" genericHeader="method">
5 A supervised learning method
</sectionHeader>
<bodyText confidence="0.999991">
We can also apply supervised learning techniques to
our task. It is easy to obtain labeled training data,
the same way we collected test data for the two un-
supervised methods presented above. We train clas-
sifiers for each group of near-synonyms. The classes
are the near-synonyms in the solution set. The word
that produced the gap is the expected solution, the
class label; this is a convenient way of producing
training data, no need for manual annotation. Each
sentence is converted into a vector of features to be
used for training the supervised classifiers. We used
two types of features. The features of the first type
are the PMI scores of the left and right context with
each class (each near-synonym from the group). The
number of features of this type is twice the number
of classes, one score for the part of the sentence at
the left of the gap, and one for the part at the right
of the gap. The features of the second type are the
</bodyText>
<page confidence="0.984186">
358
</page>
<listItem confidence="0.979988714285714">
1. mistake, error, fault
2. job, task, chore
3. duty, responsibility, obligation
4. difficult, hard
5. material, stuff
6. put up, provide, offer
7. decide, settle, resolve, adjudicate.
</listItem>
<tableCaption confidence="0.994323">
Table 2: The near-synonym groups used in Exp1.
</tableCaption>
<bodyText confidence="0.999914142857143">
words in the context window. For each group of
near-synonyms, we used as features the 500 most-
frequent words situated close to the gaps in a devel-
opment set. The value of a word feature for each
training example is 1 if the word is present in the
sentence (at the left or at the right of the gap), and 0
otherwise. We trained classifiers using several ma-
chine learning algorithms, to see which one is best
at discriminating among the near-synonyms. In Sec-
tion 6.3, we present the results of several classifiers.
A disadvantage of the supervised method is that it
requires training for each group of near-synonyms.
Additional training would be required whenever we
add more near-synonyms to our knowledge-base.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="method">
6 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999636">
6.1 Comparison to Edmonds’s method
</subsectionHeader>
<bodyText confidence="0.986546904761905">
In this section we present results of the statistical
method explained in Section 3. We compare our
results with those of Edmonds’s (1997), whose so-
lution used the texts from the year 1989 of the
Wall Street Journal (WSJ) to build a lexical co-
occurrence network for each of the seven groups
of near-synonyms from Table 2. The network in-
cluded second-order co-occurrences. Edmonds used
the WSJ 1987 texts for testing, and reported accura-
cies only a little higher than the baseline. The near-
synonyms in the seven groups were chosen to have
low polysemy. This means that some sentences with
wrong senses of near-synonyms might be in the
For comparison purposes, in this section we use
the same test data (WSJ 1987) and the same groups
of near-synonyms (we call these sentences the Exp1
data set). Our method is based on mutual informa-
tion, not on co-occurrence counts. Our counts are
collected from a much larger corpus.
Table 3 presents the comparative results for the
seven groups of near-synonyms (we did not repeat
</bodyText>
<table confidence="0.992558583333333">
Accuracy
Set No. of Base- Edmonds Stat. Stat.
cases line method method method
(Docs) (Words)
6,630 41.7% 47.9% 61.0% 59.1%
1,052 30.9% 48.9% 66.4% 61.5%
5,506 70.2% 68.9% 69.7% 73.3%
3,115 38.0% 45.3% 64.1% 66.0%
1,715 59.5% 64.6% 68.6% 72.2%
11,504 36.7% 48.6% 52.0% 52.7%
1,594 37.0% 65.9% 74.5% 76.9%
AVG 31,116 44.8% 55.7% 65.1% 66.0%
</table>
<tableCaption confidence="0.975598">
Table 3: Comparison between the statistical method
from Section 3, baseline algorithm, and Edmonds’s
method (Exp1 data set).
</tableCaption>
<bodyText confidence="0.999936">
them in the first column of the table, only the num-
ber of the group.). The last row averages the ac-
curacies for all the test sentences. The second col-
umn shows how many test sentences we collected
for each near-synonym group. The third column is
for the baseline algorithm that always chooses the
most frequent near-synonym. The fourth column
presents the results reported in (Edmonds, 1997).
column show the results of the supervised learning
classifier described in Section 5. The fifth column
presents the result of our method when using doc-
ument counts in PMI-IR, and the last column is for
the same method when using word counts in PMI.
We show in bold the best accuracy for each data set.
We notice that the automatic choice is more difficult
for some near-synonym groups than for the others.
In this paper, by accuracy we mean the number of
correct choices made by each method (the number of
gaps that were correctly filled). The correct choice is
the near-synonym that was initially replaced by the
gap in the test sentence.
To fine-tune our statistical method, we used the
data set for the near-synonyms of the word difficult
collected from the WSJ 1989 corpus as a develop-
ment set. We varied the context window size k and
the query frame q, and determined good values for
the parameter k and q. The best results were ob-
tained for small window sizes, k = 1 and k = 2
(meaning k words to the left and k words to the right
of the gap). For each k, we varied the query frame
size q. The results are best for a relatively small
query frame, q = 3, 4, 5, when the query frame is
the same or slightly larger then the context window.
</bodyText>
<page confidence="0.983149">
359
</page>
<bodyText confidence="0.8098665">
The results are worse for a very small query frame, 1. benefit, advantage, favor, gain, profit
q = 1, 2 and for larger query frames q = 6, 7,..., 20 2. low, gush, pour, run, spout, spurt, squirt, stream
or unlimited. The results presented in the rest of the 3. deficient, inadequate, poor, unsatisfactory
paper are for k = 2 and q = 5. For all the other data 4. afraid, aghast, alarmed, anxious, apprehensive, fearful,
sets used in this paper (from WSJ 1987 and BNC) frightened, scared, terror-stricken
we use the parameter values as determined on the 5. disapproval, animadversion, aspersion, blame, criticism, rep-
development set. rehension
Table 3 shows that the performance is generally 6. mistake, blooper, blunder, boner, contretemps, error, faux
better for word counts than for document counts. pas, goof, slip, solecism
Therefore, we prefer the method that uses word 7. alcoholic, boozer, drunk, drunkard, lush, sot
counts (which is also faster in our particular set- 8. leave, abandon, desert, forsake
ting). The difference between them is not statis- 9. opponent, adversary, antagonist, competitor, enemy, foe, ri-
tically significant. Our statistical method performs val
significantly better than both Edmond’s method and 10. thin, lean, scrawny, skinny, slender, slim, spare, svelte, wil-
the baseline algorithm. For all the results presented lowy, wiry
in this paper, statistical significance tests were done 11. lie, falsehood, fib, prevarication, rationalization, untruth
using the paired t-test, as described in (Manning and
Sch¨utze, 1999), page 209.
On average, our method performs 22 percentage
points better than the baseline algorithm, and 10
percentage points better than Edmonds’s method.
Its performance is similar to that of the supervised
method (see Section 6.3). An important advan-
tage of our method is that it works on any group
of near-synonyms without training, whereas Ed-
monds’s method required a lexical co-occurrence
network to be built in advance for each group of
near-synonyms and the supervised method required
training for each near-synonym group.
We note that the occasional presence of near-
synonyms with other senses than the ones we need
might make the task somewhat easier. Nonetheless,
the task is still difficult, even for human judges, as
we will see in Section 6.4. On the other hand, be-
cause the solution allows only one correct answer
the accuracies are underestimated.
6.2 Comparison to the anti-collocations
method
In a second experiment we compare the results of
our methods with the anti-collocation method de-
scribed in Section 4. We use the data set from our
previous work, which contain sentences from the
first half of the British National Corpus, with near-
synonyms from the eleven groups from Table 4.
The number of near-synonyms in each group is
higher compared with WordNet synonyms, because
they are taken from (Hayakawa, 1994), a dictionary
360
Table 4: The near-synonym groups used in Exp2.
that explains differences between near-synonyms.
Moreover we retain only the sentences in which at
least one of the context words is in our previously
acquired knowledge-base of near-synonym colloca-
tions. That is, the anti-collocations method works
only if we know how a word in the context collo-
cates with the near-synonyms from a group. For the
sentences that do not contain collocations or anti-
collocations, it will perform no better than the base-
line, because the information needed by the method
is not available in the knowledge-base. Even if we
increase the coverage of the knowledge-base, the
anti-collocation method might still fail too often due
to words that were not included.
Table 5 presents the results of the comparison. We
used two data sets: TestSample, which includes at
most two sentences per collocation (the first two sen-
tences from the corpus); and TestAll, which includes
all the sentences with collocations as they occurred
in the corpus. The reason we chose these two tests is
not to bias the results due to frequent collocations.
The last two columns are the accuracies achieved
by our method. The second last column shows the
results of the method when the word counts are ap-
proximated with document counts. The improve-
ment over the baseline is 16 to 27 percentage points.
The improvement over the anti-collocations method
is 10 to 17 percentage points.
6.3 Comparison to supervised learning
We present the results of the supervised method
from Section 5 on the data sets used in Section 6.1.
</bodyText>
<table confidence="0.981695285714286">
Accuracy
Test set No. Base- Anti- Stat. Stat.
of line collocs method method
cases method (Docs) (Words)
Test 171 57.0% 63.3% 75.6% 73.3%
Sample
TestAll 332 48.5% 58.6% 70.0% 75.6%
</table>
<tableCaption confidence="0.947592333333333">
Table 5: Comparison between the statistical method
from Section 3 and the anti-collocations method
from Section 4. (Exp2 data set from Section 6.2).
</tableCaption>
<table confidence="0.999485545454545">
ML method (Weka) Features Accuracy
Decision Trees PMI scores 65.4%
Decision Rules PMI scores 65.5%
Naive Bayes PMI scores 52.5%
K-Nearest Neighbor PMI scores 64.5%
Kernel Density PMI scores 60.5%
Boosting (Dec. Stumps) PMI scores 67.7%
Naive Bayes 500 words 68.0%
Decision Trees 500 words 67.0%
Naive Bayes PMI + 500 words 66.5%
Boosting (Dec. Stumps) PMI + 500 words 69.2%
</table>
<tableCaption confidence="0.71967225">
Table 6: Comparative results for the supervised
learning method using various ML learning algo-
rithms (Weka), averaged over the seven groups of
near-synonyms from the Exp1 data set.
</tableCaption>
<bodyText confidence="0.999911652173913">
As explained before, the data sets contain sentences
with a lexical gap. For each of the seven groups
of near-synonyms, the class to choose from, in or-
der to fill in the gaps is one of the near-synonyms in
each cluster. We implemented classifiers that use as
features either the PMI scores of the left and right
context with each class, or the words in the con-
text windows, or both types of features combined.
We used as features the 500 most-frequent words for
each group of near-synonyms. We report accuracies
for 10-fold cross-validation.
Table 6 presents the results, averaged for the seven
groups of near-synonyms, of several classifiers from
the Weka package (Witten and Frank, 2000). The
classifiers that use PMI features are Decision Trees,
Decision Rules, Naive Bayes, K-Nearest Neighbor,
Kernel Density, and Boosting a weak classifier (De-
cision Stumps – which are shallow decision trees).
Then, a Naive Bayes classifier that uses only the
word features is presented, and the same type of
classifiers with both types of features. The other
classifiers from the Weka package were also tried,
but the results did not improve and these algorithms
</bodyText>
<table confidence="0.998912416666667">
Accuracy
Test Base- Supervised Supervised Unsuper-
set line Boosting Boosting vised
(PMI) (PMI+words) method
41.7% 55.8% 57.3% 59.1%
30.9% 68.1% 70.8% 61.5%
70.2% 86.5% 86.7% 73.3%
38.0% 66.5% 66.7% 66.0%
59.5% 70.4% 71.0% 72.2%
36.7% 53.0% 56.1% 52.7%
37.0% 74.0% 75.8% 76.9%
AVG 44.8% 67.7% 69.2% 66.0%
</table>
<tableCaption confidence="0.765441333333333">
Table 7: Comparison between the unsupervised sta-
tistical method from Section 3 and the supervised
method described in Section 5, on the Exp1 data set.
</tableCaption>
<bodyText confidence="0.999660821428571">
had difficulties in scaling up. In particular, when
using the 500 word features for each training exam-
ple, only the Naive Bayes algorithm was able to run
in reasonable time. We noticed that the Naive Bayes
classifier performs very poorly on PMI features only
(55% average accuracy), but performs very well on
word features (68% average accuracy). In contrast,
the Decision Tree classifier performs well on PMI
features, especially when using boosting with Deci-
sion Stumps. When using both the PMI scores and
the word features, the results are slightly higher. It
seems that both types of features are sufficient for
training a good classifier, but combining them adds
value.
Table 7 presents the detailed results of two of the
supervised classifiers, and repeats, for easier com-
parison, the results of the unsupervised statistical
method from Section 6.1. The supervised classifier
that uses only PMI scores performs similar to the un-
supervised method. The best supervised classifier,
that uses both types of features, performs slightly
better than the unsupervised statistical method, but
the difference is not statistically significant. We con-
clude that the results of the supervised methods and
the unsupervised statistical method are similar. An
important advantage of the unsupervised method is
that it works on any group of near-synonyms without
training.
</bodyText>
<subsectionHeader confidence="0.969398">
6.4 Results obtained by human judges
</subsectionHeader>
<bodyText confidence="0.999867666666666">
We asked two human judges, native speakers of En-
glish, to guess the missing word in a random sample
of the Exp1 data set (50 sentences for each of the
</bodyText>
<page confidence="0.994914">
361
</page>
<table confidence="0.9999295">
Test set J1-J2 J1 J2 System
Agreement Acc. Acc. Accuracy
72% 70% 76% 53%
82% 84% 84% 68%
86% 92% 92% 78%
76% 82% 76% 66%
76% 82% 74% 64%
78% 68% 70% 52%
80% 80% 90% 77%
AVG 78.5% 79.7% 80.2% 65.4%
</table>
<tableCaption confidence="0.9348205">
Table 8: Results obtained by two human judges on a
random subset of the Exp1 data set.
</tableCaption>
<bodyText confidence="0.9903732">
7 groups of near-synonyms, 350 sentences in total).
The judges were instructed to choose words from the
list of near-synonyms. The choice of a word not in
the list was allowed, but not used by the two judges.
The results in Table 8 show that the agreement be-
tween the two judges is high (78.5%), but not per-
fect. This means the task is difficult, even if some
wrong senses in the test data might have made the
task easier in a few cases.
The human judges were allowed to choose more
than one correct answer when they were convinced
that more than one near-synonym fits well in the
context. They used this option sparingly, only in 5%
of the 350 sentences. Taking the accuracy achieved
of the human judges as an upper limit, the automatic
method has room for improvement (10-15 percent-
age points). In future work, we plan to allow the
system to make more than one choice when appro-
priate (for example when the second choice has a
very close score to the first choice).
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="method">
7 The intelligent thesaurus
</sectionHeader>
<bodyText confidence="0.999886769230769">
Our experiments show that the accuracy of the first
choice being the best choice is 66 to 75%; therefore
there will be cases when the writer will not choose
the first alternative. But the accuracy for the first
two choices is quite high, around 90%, as presented
in Table 9.
If the writer is in the process of writing and selects
a word to be replaced with a near-synonym proposed
by the thesaurus, then only the context on the left of
the word can be used for ordering the alternatives.
Our method can be easily adapted to consider only
the context on the left of the gap. The results of
this case are presented in Table 10, for the data sets
</bodyText>
<table confidence="0.999105">
Test set Accuracy Accuracy
first choice first 2 choices
Exp1, AVG 66.0% 88.5%
Exp2, TestSample 73.3% 94.1%
Exp2, TestAll 75.6% 87.5%
</table>
<tableCaption confidence="0.974422">
Table 9: Accuracies for the first two choices as or-
dered by an interactive intelligent thesaurus.
</tableCaption>
<table confidence="0.9998712">
Test set Accuracy Accuracy
first choice first 2 choices
Exp1, AVG 58.0% 84.8%
Exp2, TestSample 57.4% 75.1%
Exp2, TestAll 56.1% 77.4%
</table>
<tableCaption confidence="0.906208">
Table 10: Results of the statistical method when
only the left context is considered.
</tableCaption>
<bodyText confidence="0.999672470588235">
used in the previous sections. The accuracy values
are lower than in the case when both the left and the
right context are considered (Table 9). This is due
in part to the fact that some sentences in the test sets
have very little left context, or no left context at all.
On the other hand, many times the writer composes
a sentence or paragraph and then she/he goes back
to change a word that does not sound right. In this
case, both the left and right context will be available.
In the intelligent thesaurus, we could combine
the supervised and unsupervised method, by using
a supervised classifier when the confidence in the
classification is high, and by using the unsupervised
method otherwise. Also the unsupervised statisti-
cal method would be used for the groups of near-
synonyms for which a supervised classifier was not
previously trained.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999975307692308">
We presented a statistical method of choosing the
best near-synonym in a context. We compared this
method to a previous method (Edmonds’s method)
and to the anti-collocation method and showed that
the performance improved considerably. We also
show that the unsupervised statistical method per-
forms comparably to a supervised learning method.
Our method based on PMI scores performs well,
despite the well-known limitations of PMI in cor-
pora. PMI tends to have problems mostly on very
small counts, but it works reasonably with larger
counts. Our web corpus is quite large, therefore the
problem of small counts does not appear.
</bodyText>
<page confidence="0.993993">
362
</page>
<bodyText confidence="0.999985181818182">
In the intelligent thesaurus, we do not make the
near-synonym choice automatically, but we let the
user choose. The first choice offered by the the-
saurus is the best one quite often; the first two
choices are correct 90% of the time.
Future work includes a word sense disambigua-
tion module. In case the target word selected by the
writer has multiple senses, they could trigger sev-
eral groups of near-synonyms. The system will de-
cide which group represents the most likely senses
by computing the semantic coherence scores aver-
aged over the near-synonyms from each group.
We plan to explore the question of which inven-
tory of near-synonyms or similar words is the most
suitable for use in the intelligent thesaurus.
Choosing the right near-synonym in context is
also useful in other applications, such as natural lan-
guage generation (NLG) and machine translation.
In fact we already used the near-synonym choice
module in an NLG system, for complementing the
choices made by using the symbolic knowledge in-
corporated into the system.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789708333333">
Eneko Agirre and David Martinez. 2000. Exploring au-
tomatic word sense disambiguation with decision lists
and the Web. In Proceedings of the Workshop on Se-
mantic Annotation And Intelligent Content, COLING
2000, Saarbr¨ucken/Luxembourg/Nancy.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16 (1):22–29.
Charles L. A. Clarke and Egidio Terra. 2003a. Fre-
quency estimates for statistical word similarity mea-
sures. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL 2003), pages 165–172, Edmonton, Canada.
Charles L. A. Clarke and Egidio Terra. 2003b. Passage
retrieval vs. document retrieval for factoid question an-
swering. In Proceedings of the 26th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 427–428,
Toronto, Canada.
Philip Edmonds. 1997. Choosing the word most typical
in context using a lexical co-occurrence network. In
Proceedings of the 35th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 507–509,
Madrid, Spain.
Gregory Grefenstette. 1999. The World Wide Web as a
resource for example-based machine translation tasks.
In Proceedings of the ASLIB Conference on Translat-
ing and Computers, London, UK.
S. I. Hayakawa, editor. 1994. Choose the Right Word.
Second Edition, revised by Eugene Ehrlich. Harper-
Collins Publishers.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge-base of near-synonym dif-
ferences. Computational Linguistics, 32 (2):223–262.
Frank Keller and Mirella Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29 (3):459–484.
Adam Kilgarriff and Gregory Grefenstette. 2003. Intro-
duction to the special issue on the Web as a corpus.
Computational Linguistics, 29 (3):333–347.
Adam Kilgarriff. 2001. Web as corpus. In Proceedings
of the 2001 Corpus Linguistics conference, pages 342–
345, Lancaster, UK.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics joint with 17th International Conference on Com-
putational Linguistics (ACL-COLING’98), pages 768–
774, Montreal, Quebec, Canada.
Christopher Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Natural Language Processing.
The MIT Press, Cambridge, MA.
Rada Mihalcea and Dan Moldovan. 1999. A method for
word sense disambiguation from unrestricted text. In
Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 152–158,
Maryland, MD.
Philip Resnik. 1999. Mining the Web for bilingual text.
In Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 527–534,
Maryland, MD.
Peter Mark Roget, editor. 1852. Roget’s Thesaurus of
English Words and Phrases. Longman Group Ltd.,
Harlow, Essex, UK.
Peter Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML 2001), pages 491–502, Freiburg, Germany.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical machine learning tools with Java implemen-
tations. Morgan Kaufmann, San Francisco, CA.
</reference>
<page confidence="0.999367">
363
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954496">
<title confidence="0.999807">Near-Synonym Choice in an Intelligent Thesaurus</title>
<author confidence="0.994695">Diana</author>
<affiliation confidence="0.9995975">School of Information Technology and University of</affiliation>
<address confidence="0.998561">800 King Edward, Ottawa, ON, Canada, K1N</address>
<email confidence="0.995821">diana@site.uottawa.ca</email>
<abstract confidence="0.998080555555556">An intelligent thesaurus assists a writer with alternative choices of words and orders them by their suitability in the writing context. In this paper we focus on methods for automatically choosing nearsynonyms by their semantic coherence with the context. Our statistical method uses the Web as a corpus to compute mutual information scores. Evaluation experiments show that this method performs better than a previous method on the same task. We also propose and evaluate two more methods, one that uses anticollocations, and one that uses supervised learning. To asses the difficulty of the task, we present results obtained by human judges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Exploring automatic word sense disambiguation with decision lists and the Web.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Semantic Annotation And Intelligent Content, COLING 2000, Saarbr¨ucken/Luxembourg/Nancy.</booktitle>
<contexts>
<context position="4938" citStr="Agirre and Martinez (2000)" startWordPosition="810" endWordPosition="813">timate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a </context>
</contexts>
<marker>Agirre, Martinez, 2000</marker>
<rawString>Eneko Agirre and David Martinez. 2000. Exploring automatic word sense disambiguation with decision lists and the Web. In Proceedings of the Workshop on Semantic Annotation And Intelligent Content, COLING 2000, Saarbr¨ucken/Luxembourg/Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<pages>1--22</pages>
<contexts>
<context position="7181" citStr="Church and Hanks, 1991" startWordPosition="1186" endWordPosition="1189"> candidate near-synonym that could fill in the gap. The nearsynonym with the highest score is the proposed solution. The score for each candidate reflects how well a near-synonym fits in with the context. It is based on the mutual information scores between a near-synonym and the content words in the context (we filter out the stopwords). The pointwise mutual information (PMI) between two words x and y compares the probability of observing the two words together (their joint probability) to the probabilities of observing x and y independently (the probability of occurring together by chance) (Church and Hanks, 1991): PMI(x, y) = P (x,y) lo92 P(x)P(y) The probabilities can be approximated by: P(x) = C(x)/N, P(y) = C(y)/N, P(x, y) = C(x, y)/N, where C denotes frequency counts and N is the total number of words in the corpus. Therefore: PMI(x, y) = lo92 C(x)·C(y), where N can be C(x,y)·N ignored in comparisons. We model the context as a window of size 2k 357 ghastly mistake *ghastly error ghastly blunder *ghastly faux pas *ghastly blooper *ghastly solecism *ghastly goof *ghastly contretemps *ghastly boner *ghastly slip spelling mistake spelling error *spelling blunder *spelling faux pas *spelling blooper *s</context>
</contexts>
<marker>Church, Hanks, 1991</marker>
<rawString>Kenneth Church and Patrick Hanks. 1991. Word association norms, mutual information and lexicography. Computational Linguistics, 16 (1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles L A Clarke</author>
<author>Egidio Terra</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL</booktitle>
<pages>165--172</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4393" citStr="Clarke and Terra, 2003" startWordPosition="720" endWordPosition="723"> foe, rival Figure 1: Examples of sentences with a lexical gap, and candidate near-synonyms to fill the gap. solution. Moreover, what we consider to be the best choice is the typical usage in the corpus, but it may vary from writer to writer. Nonetheless, it is a convenient way of producing test data. The statistical method that we propose here is based on semantic coherence scores (based on mutual information) of each candidate with the words in the context. We explore how far such a method can go when using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disam</context>
<context position="9512" citStr="Clarke and Terra, 2003" startWordPosition="1610" endWordPosition="1613">nt counts. If the counts C(x), C(y), and C(x, y) are approximated as the number of document in which they appear, we obtain the PMI-IR formula (Turney, 2001). The queries we need to send to the search engine are the same but they are restricted to document counts: C(x) is the number of document in which x occurs; C(x, y) is the number of documents in which x is followed by y in a frame of size q. Other statistical association measures, such as log-likelihood, could be used. We tried only PMI because it is easy to compute on a Web corpus and because PMI performed better than other measures in (Clarke and Terra, 2003a). We present the results in Section 6.1, where we compare our method to a baseline algorithm that always chooses the most frequent near-synonyms and to Edmonds’s method for the same task, on the same data set. First, however, we present two other methods to which we compare our results. 4 The anti-collocations method For the task of near-synonym choice, another method that we implemented is the anti-collocations method. By anti-collocation we mean a combinaTable 1: Examples of collocations and anticollocations. The ∗ indicates the anti-collocations. tion of words that a native speaker would </context>
</contexts>
<marker>Clarke, Terra, 2003</marker>
<rawString>Charles L. A. Clarke and Egidio Terra. 2003a. Frequency estimates for statistical word similarity measures. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL 2003), pages 165–172, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles L A Clarke</author>
<author>Egidio Terra</author>
</authors>
<title>Passage retrieval vs. document retrieval for factoid question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>427--428</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="4393" citStr="Clarke and Terra, 2003" startWordPosition="720" endWordPosition="723"> foe, rival Figure 1: Examples of sentences with a lexical gap, and candidate near-synonyms to fill the gap. solution. Moreover, what we consider to be the best choice is the typical usage in the corpus, but it may vary from writer to writer. Nonetheless, it is a convenient way of producing test data. The statistical method that we propose here is based on semantic coherence scores (based on mutual information) of each candidate with the words in the context. We explore how far such a method can go when using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disam</context>
<context position="9512" citStr="Clarke and Terra, 2003" startWordPosition="1610" endWordPosition="1613">nt counts. If the counts C(x), C(y), and C(x, y) are approximated as the number of document in which they appear, we obtain the PMI-IR formula (Turney, 2001). The queries we need to send to the search engine are the same but they are restricted to document counts: C(x) is the number of document in which x occurs; C(x, y) is the number of documents in which x is followed by y in a frame of size q. Other statistical association measures, such as log-likelihood, could be used. We tried only PMI because it is easy to compute on a Web corpus and because PMI performed better than other measures in (Clarke and Terra, 2003a). We present the results in Section 6.1, where we compare our method to a baseline algorithm that always chooses the most frequent near-synonyms and to Edmonds’s method for the same task, on the same data set. First, however, we present two other methods to which we compare our results. 4 The anti-collocations method For the task of near-synonym choice, another method that we implemented is the anti-collocations method. By anti-collocation we mean a combinaTable 1: Examples of collocations and anticollocations. The ∗ indicates the anti-collocations. tion of words that a native speaker would </context>
</contexts>
<marker>Clarke, Terra, 2003</marker>
<rawString>Charles L. A. Clarke and Egidio Terra. 2003b. Passage retrieval vs. document retrieval for factoid question answering. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 427–428, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
</authors>
<title>Choosing the word most typical in context using a lexical co-occurrence network.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>507--509</pages>
<location>Madrid,</location>
<contexts>
<context position="5826" citStr="Edmonds (1997)" startWordPosition="962" endWordPosition="963">eb as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a balanced corpus: the size of the Web compensates for the noise in the data. In this paper we are using a very large corpus of Web pages to address a problem that has not been successfully solved before. In fact, the only work that addresses exactly the same task is that of Edmonds (1997), as far as we are aware. Edmonds gives a solution based on a lexical co-occurrence network that included secondorder co-occurrences. We use a much larger corpus and a simpler method, and we obtain much better results. Our task has similarities to the word sense disambiguation task. Our near-synonyms have senses that are very close to each other. In Senseval, some of the fine-grained senses are also close to each other, so they might occur in similar contexts, while the coarse-grained senses are expected to occur in distinct contexts. In our case, the near-synonyms are different words to choos</context>
<context position="14658" citStr="Edmonds, 1997" startWordPosition="2478" endWordPosition="2479">% 11,504 36.7% 48.6% 52.0% 52.7% 1,594 37.0% 65.9% 74.5% 76.9% AVG 31,116 44.8% 55.7% 65.1% 66.0% Table 3: Comparison between the statistical method from Section 3, baseline algorithm, and Edmonds’s method (Exp1 data set). them in the first column of the table, only the number of the group.). The last row averages the accuracies for all the test sentences. The second column shows how many test sentences we collected for each near-synonym group. The third column is for the baseline algorithm that always chooses the most frequent near-synonym. The fourth column presents the results reported in (Edmonds, 1997). column show the results of the supervised learning classifier described in Section 5. The fifth column presents the result of our method when using document counts in PMI-IR, and the last column is for the same method when using word counts in PMI. We show in bold the best accuracy for each data set. We notice that the automatic choice is more difficult for some near-synonym groups than for the others. In this paper, by accuracy we mean the number of correct choices made by each method (the number of gaps that were correctly filled). The correct choice is the near-synonym that was initially </context>
</contexts>
<marker>Edmonds, 1997</marker>
<rawString>Philip Edmonds. 1997. Choosing the word most typical in context using a lexical co-occurrence network. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 507–509, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>The World Wide Web as a resource for example-based machine translation tasks.</title>
<date>1999</date>
<booktitle>In Proceedings of the ASLIB Conference on Translating and Computers,</booktitle>
<location>London, UK.</location>
<contexts>
<context position="4764" citStr="Grefenstette (1999)" startWordPosition="784" endWordPosition="786">nce scores (based on mutual information) of each candidate with the words in the context. We explore how far such a method can go when using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Pete</context>
</contexts>
<marker>Grefenstette, 1999</marker>
<rawString>Gregory Grefenstette. 1999. The World Wide Web as a resource for example-based machine translation tasks. In Proceedings of the ASLIB Conference on Translating and Computers, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S I Hayakawa</author>
<author>editor</author>
</authors>
<title>Choose the Right Word. Second Edition, revised by Eugene Ehrlich.</title>
<date>1994</date>
<publisher>HarperCollins Publishers.</publisher>
<marker>Hayakawa, editor, 1994</marker>
<rawString>S. I. Hayakawa, editor. 1994. Choose the Right Word. Second Edition, revised by Eugene Ehrlich. HarperCollins Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Inkpen</author>
<author>Graeme Hirst</author>
</authors>
<title>Building and using a lexical knowledge-base of near-synonym differences.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<pages>2--223</pages>
<contexts>
<context position="10315" citStr="Inkpen and Hirst, 2006" startWordPosition="1739" endWordPosition="1742">ame task, on the same data set. First, however, we present two other methods to which we compare our results. 4 The anti-collocations method For the task of near-synonym choice, another method that we implemented is the anti-collocations method. By anti-collocation we mean a combinaTable 1: Examples of collocations and anticollocations. The ∗ indicates the anti-collocations. tion of words that a native speaker would not use and therefore should not be used when automatically generating text. This method uses a knowledgebase of collocational behavior of near-synonyms acquired in previous work (Inkpen and Hirst, 2006). A fragment of the knowledge-base is presented in Table 1, for the near-synonyms of the word error and two collocate words ghastly and spelling. The lines marked by ∗ represent anti-collocations and the rest represent strong collocations. The anti-collocations method simply ranks the strong collocations higher than the anti-collocations. In case of ties it chooses the most frequent nearsynonym. In Section 6.2 we present the results of comparing this method to the method from the previous section. 5 A supervised learning method We can also apply supervised learning techniques to our task. It i</context>
</contexts>
<marker>Inkpen, Hirst, 2006</marker>
<rawString>Diana Inkpen and Graeme Hirst. 2006. Building and using a lexical knowledge-base of near-synonym differences. Computational Linguistics, 32 (2):223–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the Web to obtain frequencies for unseen bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>3--459</pages>
<contexts>
<context position="5472" citStr="Keller and Lapata (2003)" startWordPosition="895" endWordPosition="898">the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a balanced corpus: the size of the Web compensates for the noise in the data. In this paper we are using a very large corpus of Web pages to address a problem that has not been successfully solved before. In fact, the only work that addresses exactly the same task is that of Edmonds (1997), as far as we are aware. Edmonds gives a solution based on a lexical co-occurrence network that included secondorder co-occurrences. We use a much larger corpus and a simpler method, and we obtain much better results. Our task has similarities t</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the Web to obtain frequencies for unseen bigrams. Computational Linguistics, 29 (3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Introduction to the special issue on the Web as a corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>3--333</pages>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue on the Web as a corpus. Computational Linguistics, 29 (3):333–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Web as corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Corpus Linguistics conference,</booktitle>
<pages>342--345</pages>
<location>Lancaster, UK.</location>
<contexts>
<context position="4834" citStr="Kilgarriff (2001)" startWordPosition="795" endWordPosition="796">s in the context. We explore how far such a method can go when using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the c</context>
</contexts>
<marker>Kilgarriff, 2001</marker>
<rawString>Adam Kilgarriff. 2001. Web as corpus. In Proceedings of the 2001 Corpus Linguistics conference, pages 342– 345, Lancaster, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics joint with 17th International Conference on Computational Linguistics (ACL-COLING’98),</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="1971" citStr="Lin, 1998" startWordPosition="317" endWordPosition="318">cational properties of near-synonyms can help with choosing the best words. This problem is difficult because the near-synonyms have senses that are very close to each other, and therefore they occur in similar contexts; we need to capture the subtle differences specific to each near-synonym. Our thesaurus brings up only alternatives that have the same part-of-speech with the target word. The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus (Roget, 1852), dictionaries of synonyms (Hayakawa, 1994), or clusters acquired from corpora (Lin, 1998). In this paper we focus on the task of automatically selecting the best near-synonym that should be used in a particular context. The natural way to validate an algorithm for this task would be to ask human readers to evaluate the quality of the algorithm’s output, but this kind of evaluation would be very laborious. Instead, we validate the algorithms by deleting selected words from sample sentences, to see whether the algorithms can restore the missing words. That is, we create a lexical gap and evaluate the ability of the algorithms to fill the gap. Two examples are presented in Figure 1. </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics joint with 17th International Conference on Computational Linguistics (ACL-COLING’98), pages 768– 774, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>A method for word sense disambiguation from unrestricted text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<location>Maryland, MD.</location>
<contexts>
<context position="4907" citStr="Mihalcea and Moldovan (1999)" startWordPosition="805" endWordPosition="808"> using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 2003), Keller and Lapata (2003) show that Web counts correlate wel</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 1999. A method for word sense disambiguation from unrestricted text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 152–158, Maryland, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Mining the Web for bilingual text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>527--534</pages>
<location>Maryland, MD.</location>
<contexts>
<context position="5017" citStr="Resnik (1999)" startWordPosition="824" endWordPosition="825">orpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a balanced corpus: the size of the Web compensates for the noise in the data. In </context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Mining the Web for bilingual text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 527–534, Maryland, MD.</rawString>
</citation>
<citation valid="false">
<booktitle>Roget’s Thesaurus of English Words and Phrases. Longman Group Ltd.,</booktitle>
<pages>1852</pages>
<editor>Peter Mark Roget, editor.</editor>
<location>Harlow, Essex, UK.</location>
<marker></marker>
<rawString>Peter Mark Roget, editor. 1852. Roget’s Thesaurus of English Words and Phrases. Longman Group Ltd., Harlow, Essex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="5066" citStr="Turney (2001)" startWordPosition="832" endWordPosition="833">a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a balanced corpus: the size of the Web compensates for the noise in the data. In this paper we are using a very large corpus of We</context>
<context position="9047" citStr="Turney, 2001" startWordPosition="1525" endWordPosition="1526">are using the Waterloo terabyte corpus and we issue queries to its search engine, we have several possibilities of computing the frequency counts. C(x, y) can be the number of cooccurrences of x and y when y immediately follows x, or the distance between x and y can be up to q. We call q the query frame size. The tool for accessing the corpus allows us to use various values for q. The search engine also allows us to approximate words counts with document counts. If the counts C(x), C(y), and C(x, y) are approximated as the number of document in which they appear, we obtain the PMI-IR formula (Turney, 2001). The queries we need to send to the search engine are the same but they are restricted to document counts: C(x) is the number of document in which x occurs; C(x, y) is the number of documents in which x is followed by y in a frame of size q. Other statistical association measures, such as log-likelihood, could be used. We tried only PMI because it is easy to compute on a Web corpus and because PMI performed better than other measures in (Clarke and Terra, 2003a). We present the results in Section 6.1, where we compare our method to a baseline algorithm that always chooses the most frequent ne</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML 2001), pages 491–502, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools with Java implementations.</title>
<date>2000</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="21949" citStr="Witten and Frank, 2000" startWordPosition="3691" endWordPosition="3694">l gap. For each of the seven groups of near-synonyms, the class to choose from, in order to fill in the gaps is one of the near-synonyms in each cluster. We implemented classifiers that use as features either the PMI scores of the left and right context with each class, or the words in the context windows, or both types of features combined. We used as features the 500 most-frequent words for each group of near-synonyms. We report accuracies for 10-fold cross-validation. Table 6 presents the results, averaged for the seven groups of near-synonyms, of several classifiers from the Weka package (Witten and Frank, 2000). The classifiers that use PMI features are Decision Trees, Decision Rules, Naive Bayes, K-Nearest Neighbor, Kernel Density, and Boosting a weak classifier (Decision Stumps – which are shallow decision trees). Then, a Naive Bayes classifier that uses only the word features is presented, and the same type of classifiers with both types of features. The other classifiers from the Weka package were also tried, but the results did not improve and these algorithms Accuracy Test Base- Supervised Supervised Unsuperset line Boosting Boosting vised (PMI) (PMI+words) method 41.7% 55.8% 57.3% 59.1% 30.9%</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Ian H. Witten and Eibe Frank. 2000. Data Mining: Practical machine learning tools with Java implementations. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>