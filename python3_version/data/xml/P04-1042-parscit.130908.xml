<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000219">
<title confidence="0.995759">
Deep dependencies from context-free statistical parsers: correcting the
surface dependency approximation
</title>
<author confidence="0.9987">
Roger Levy Christopher D. Manning
</author>
<affiliation confidence="0.999456">
Department of Linguistics Departments of Computer Science and Linguistics
Stanford University Stanford University
</affiliation>
<email confidence="0.999296">
rog@stanford.edu manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999698368421053">
We present a linguistically-motivated algorithm for recon-
structing nonlocal dependency in broad-coverage context-free
parse trees derived from treebanks. We use an algorithm based
on loglinear classifiers to augment and reshape context-free
trees so as to reintroduce underlying nonlocal dependencies
lost in the context-free approximation. We find that our algo-
rithm compares favorably with prior work on English using an
existing evaluation metric, and also introduce and argue for a
new dependency-based evaluation metric. By this new eval-
uation metric our algorithm achieves 60% error reduction on
gold-standard input trees and 5% error reduction on state-of-
the-art machine-parsed input trees, when compared with the
best previous work. We also present the first results on non-
local dependency reconstruction for a language other than En-
glish, comparing performance on English and German. Our
new evaluation metric quantitatively corroborates the intuition
that in a language with freer word order, the surface dependen-
cies in context-free parse trees are a poorer approximation to
underlying dependency structure.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987096774194">
While parsers are been used for other purposes, the
primary motivation for syntactic parsing is as an
aid to semantic interpretation, in pursuit of broader
goals of natural language understanding. Propo-
nents of traditional ‘deep’ or ‘precise’ approaches
to syntax, such as GB, CCG, HPSG, LFG, or TAG,
have argued that sophisticated grammatical for-
malisms are essential to resolving various hidden re-
lationships such as the source phrase of moved wh-
phrases in questions and relativizations, or the con-
troller of clauses without an overt subject. Knowl-
edge of these hidden relationships is in turn es-
sential to semantic interpretation of the kind prac-
ticed in the semantic parsing (Gildea and Jurafsky,
2002) and QA (Pasca and Harabagiu, 2001) litera-
tures. However, work in statistical parsing has for
the most part put these needs aside, being content to
recover surface context-free (CF) phrase structure
trees. This perhaps reflects the fact that context-free
phrase structure grammar (CFG) is in some sense
at the the heart of the majority of both formal and
computational syntactic research. Although, upon
introducing it, Chomsky (1956) rejected CFG as an
adequate framework for natural language descrip-
tion, the majority of work in the last half century
has used context-free structural descriptions and re-
lated methodologies in one form or another as an
important component of syntactic analysis. CFGs
seem adequate to weakly generate almost all com-
mon natural language structures, and also facilitate
a transparent predicate-argument and/or semantic
interpretation for the more basic ones (Gazdar et al.,
1985). Nevertheless, despite their success in pro-
viding surface phrase structure analyses, if statisti-
cal parsers and the representations they produce do
not provide a useful stepping stone to recovering the
hidden relationships, they will ultimately come to
be seen as a dead end, and work will necessarily re-
turn to using richer formalisms.
In this paper we attempt to establish to what de-
gree current statistical parsers are a useful step in
analysis by examining the performance of further
statistical classifiers on non-local dependency re-
covery from CF parse trees. The natural isomor-
phism from CF trees to dependency trees induces
only local dependencies, derived from the head-
sister relation in a CF local tree. However, if the
output of a context-free parser can be algorithmi-
cally augmented to accurately identify and incor-
porate nonlocal dependencies, then we can say that
the context-free parsing model is a safe approxima-
tion to the true task of dependency reconstruction.
We investigate the safeness of this approximation,
devising an algorithm to reconstruct non-local de-
pendencies from context-free parse trees using log-
linear classifiers, tested on treebanks of not only En-
glish but also German, a language with much freer
word order and correspondingly more discontinuity
than English. This algorithm can be used as an in-
termediate step between the surface output trees of
modern statistical parsers and semantic interpreta-
tion systems for a variety of tasks.1
</bodyText>
<footnote confidence="0.92634975">
1Many linguistic and technical intricacies are involved in
the interpretation and use of non-local annotation structure
found in treebanks. A more complete exposition of the work
presented here can be found in Levy (2004).
</footnote>
<figureCaption confidence="0.99600625">
Figure 1: Example of empty and nonlocal annota-
tions from the Penn Treebank of English, including
null complementizers (0), relativization (*T*-1), right-
extraposition (*ICH*-2), and syntactic control (*-3).
</figureCaption>
<subsectionHeader confidence="0.915551">
1.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.999891217391304">
Previous work on nonlocal dependency has focused
entirely on English, despite the disparity in type and
frequency of various non-local dependency con-
structions for varying languages (Kruijff, 2002).
Collins (1999)’s Model 3 investigated GPSG-style
trace threading for resolving nonlocal relative pro-
noun dependencies. Johnson (2002) was the first
post-processing approach to non-local dependency
recovery, using a simple pattern-matching algorithm
on context-free trees. Dienes and Dubey (2003a,b)
and Dienes (2003) approached the problem by pre-
identifying empty categories using an HMM on un-
parsed strings and threaded the identified empties
into the category structure of a context-free parser,
finding that this method compared favorably with
both Collins’ and Johnson’s. Traditional LFG pars-
ing, in both non-stochastic (Kaplan and Maxwell,
1993) and stochastic (Riezler et al., 2002; Kaplan
et al., 2004) incarnations, also divides the labor of
local and nonlocal dependency identification into
two phases, starting with context-free parses and
continuing by augmentation with functional infor-
mation.
</bodyText>
<sectionHeader confidence="0.996041" genericHeader="introduction">
2 Datasets
</sectionHeader>
<bodyText confidence="0.999988271186441">
The datasets used for this study consist of the Wall
Street Journal section of the Penn Treebank of En-
glish (WSJ) and the context-free version of the
NEGRA (version 2) corpus of German (Skut et al.,
1997b). Full-size experiments on WSJ described in
Section 4 used the standard sections 2-21 for train-
ing, 24 for development, and trees whose yield is
under 100 words from section 23 for testing. Ex-
periments described in Section 4.3 used the same
development and test sets but files 200-959 of WSJ
as a smaller training set; for NEGRA we followed
Dubey and Keller (2003) in using the first 18,602
sentences for training, the last 1,000 for develop-
ment, and the previous 1,000 for testing. Consistent
with prior work and with common practice in statis-
tical parsing, we stripped categories of all functional
tags prior to training and testing (though in several
cases this seems to have been a limiting move; see
Section 5).
Nonlocal dependency annotation in Penn Tree-
banks can be divided into three major types: unin-
dexed empty elements, dislocations, and control.
The first type consists primarily of null complemen-
tizers, as exemplified in Figure 1 by the null rela-
tive pronoun 0 (c.f. aspects that it sees), and do not
participate in (though they may mediate) nonlocal
dependency. The second type consists of a dislo-
cated element coindexed with an origin site of se-
mantic interpretation, as in the association in Fig-
ure 1 of WHNP-1 with the direct object position
of sees (a relativization), and the association of S-
2 with the ADJP quick (a right dislocation). This
type encompasses the classic cases of nonlocal de-
pendency: topicalization, relativization, wh- move-
ment, and right dislocation, as well as expletives and
other instances of non-canonical argument position-
ing. The third type involves control loci in syntac-
tic argument positions, sometimes coindexed with
overt controllers, as in the association of the NP
Farmers with the empty subject position of the S-
2 node. (An example of a control locus with no
controller would be [S NP-* [VP Eating ice cream ]]
is fun.) Controllers are to be interpreted as syntac-
tic (and possibly semantic) arguments both in their
overt position and in the position of loci they con-
trol. This type encompasses raising, control, pas-
sivization, and unexpressed subjects of to- infinitive
and gerund verbs, among other constructions.2
NEGRA’s original annotation is as dependency
trees with phrasal nodes, crossing branches, and
no empty elements. However, the distribution in-
cludes a context-free version produced algorithmi-
cally by recursively remapping discontinuous parts
of nodes upward into higher phrases and marking
their sites of origin.3 The resulting “traces” cor-
respond roughly to a subclass of the second class
of Penn Treebank empties discussed above, and in-
clude wh- movement, topicalization, right extrapo-
sitions from NP, expletives, and scrambling of sub-
</bodyText>
<footnote confidence="0.990623333333333">
2Four of the annotation errors in WSJ lead to uninter-
pretable dislocation and sharing patterns, including failure to
annotate dislocations corresponding to marked origin sites, and
mislabelings of control loci as origin sites of dislocation that
lead to cyclic dislocations (which are explicitly prohibited in
WSJ annotation guidelines). We corrected these errors manu-
ally before model testing and training.
3For a detailed description of the algorithm for creating the
context-free version of NEGRA, see Skut et al. (1997a).
</footnote>
<figure confidence="0.998531302325581">
S
NP-3
NNP
Farmers
VP
S
quick*ICH*-2
S-2
NP
*-3
TO
VP
VP
NP
NN
yesterday
NP
NP
DT NN
the problems
to
VB
point
PRT
RP
out
0
S
NP
PRP
it
VBZ
sees
VP
NP
*T*-1
SBAR
WHNP-1
VBD ADJP
was JJ
.
.
“The RMV will not begin to be formed for a long time.”
</figure>
<figureCaption confidence="0.9616678">
Figure 2: Nonlocal dependencies via right-extraposition
(*T1*) and topicalization (*T2*) in the NEGRA cor-
pus of German, before (top) and after (bottom) transfor-
mation to context-free form. Dashed lines show where
nodes go as a result of remapping into context-free form.
</figureCaption>
<bodyText confidence="0.999906416666667">
jects after other complements. The positioning of
NEGRA’s “traces” inside the mother node is com-
pletely algorithmic; a dislocated constituent C has
its trace at the edge of the original mother closest
to C’s overt position. Given a context-free NEGRA
tree shorn of its trace/antecedent notation, however,
it is far from trivial to determine which nodes are
dislocated, and where they come from. Figure 2
shows an annotated sentence from the NEGRA cor-
pus with discontinuities due to right extraposition
(*T1*) and topicalization (*T2*), before and after
transformation into context-free form with traces.
</bodyText>
<sectionHeader confidence="0.996511" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.999897625">
Corresponding to the three types of empty-element
annotation found in the Penn Treebank, our algo-
rithm divides the process of CF tree enhancement
into three phases. Each phase involves the identifi-
cation of a certain subset of tree nodes to be oper-
ated on, followed by the application of the appro-
priate operation to the node. Operations may in-
volve the insertion of a category at some position
among a node’s daughters; the marking of certain
nodes as dislocated; or the relocation of dislocated
nodes to other positions within the tree. The content
and ordering of phases is consistent with the syntac-
tic theory upon which treebank annotation is based.
For example, WSJ annotates relative clauses lacking
overt relative pronouns, such as the SBAR in Fig-
ure 1, with a trace in the relativization site whose
antecedent is an empty relative pronoun. This re-
quires that empty relative pronoun insertion precede
dislocated element identification. Likewise, dislo-
cated elements can serve as controllers of control
loci, based on their originating site, so it is sensible
to return dislocated nodes to their originating sites
before identifying control loci and their controllers.
For WSJ, the three phases are:
</bodyText>
<listItem confidence="0.988442">
1. (a) Determine nodes at which to insert null
COMPlementizers4 (IDENTNULL)
(b) For each COMP insertion node, determine
position of each insertion and insert COMP
(INSERTNULL)
2. (a) Classify each tree node as +/- DISLOCATED
(IDENTMOVED)
(b) For each DISLOCATED node, choose an ORI-
GIN node (RELOCMOVED)
(c) For each pair (DISLOCATED,origin), choose
a position of insertion and insert dislocated
(INSERTRELOC)
3. (a) Classify each node as +/- control LOCUS
(IDENTLOCUS)
(b) For each LOCUS, determine position of inser-
tion and insert LOCUS (INSERTLOCUS)
(c) For each LOCUS, determine CONTROLLER (if
any) (FINDCONTROLLER)
</listItem>
<bodyText confidence="0.975419121212121">
Note in particular that phase 2 involves the classifi-
cation of overt tree nodes as dislocated, followed
by the identification of an origin site (annotated
in the treebank as an empty node) for each dislo-
cated element; whereas phase 3 involves the iden-
tification of (empty) control loci first, and of con-
trollers later. This approach contrasts with John-
son (2002), who treats empty/antecedent identifi-
cation as a joint task, and with Dienes and Dubey
(2003a,b), who always identify empties first and de-
termine antecedents later. Our motivation is that it
should generally be easier to determine whether an
overt element is dislocated than whether a given po-
sition is the origin of some yet unknown dislocated
element (particularly in the absence of a sophisti-
cated model of argument expression); but control
loci are highly predictable from local context, such
as the subjectless non-finite S in Figure 1’s S-2.5 In-
deed this difference seems to be implicit in the non-
local feature templates used by Dienes and Dubey
(2003a,b) in their empty element tagger, in partic-
ular lookback for wh- words preceding a candidate
verb.
As described in Section 2, NEGRA’s nonlocal
annotation schema is much simpler, involving no
4The WSJ contains a number of SBARs headed by empty
complementizers with trace S’s. These SBARs are introduced
in our algorithm as projections of identified empty complemen-
tizers as daughters of non-SBAR categories.
5Additionally, whereas dislocated nodes are always overt,
control loci may be controlled by other (null) control loci,
meaning that identifying controllers before control loci would
still entail looking for nulls.
</bodyText>
<figure confidence="0.9997192">
S
VAFIN VP $, $.
AP wird PP VVPP
.
ADV NP ADJD PROAV begonnen ,VP
Erst ADJA NN sp¨ater damit NP VZ
lange Zeit ART NE PTKZU VVINF
den RMV zu schaffen
VP
VP-1
$,
$.
zu
to
den
the
Zeit
time
lange
long
RMV
RMV
damit
with it
schaffen
form
S
AP-2
VAFIN
NP
ADJD
wird
will
VVPP
,
NP
VZ
.
ADV
*T2* PP
NE
ART
*T1*
NN
PTKZU
PROAV
VVINF
ADJA
begonnen
be begun
Erst
not until
sp¨ater
later
IDENTMOVED S Expletive dislocation
IdentNull
InsertNull
IdentMoved
RelocMoved
InsertReloc
IdentLocus
InsertLocus
FindController
NP(it/there) VP
S/SBAR
</figure>
<figureCaption confidence="0.9988275">
Figure 3: Different classifiers’ specialized tree-matching
fragments and their purposes
</figureCaption>
<bodyText confidence="0.99996105">
uncoindexed empties or control loci. Correspond-
ingly, our NEGRA algorithm includes only phase
2 of the WSJ algorithm, step (c) of which is trivial
for NEGRA due to the deterministic positioning of
trace insertion in the treebank.
In each case we use a loglinear model for node
classification, with a combination of quadratic reg-
ularization and thresholding by individual feature
count to prevent overfitting. In the second and third
parts of phases 2 and 3, when determining an orig-
inating site or controller for a given node N, or
an insertion position for a node N&apos; in N, we use a
competition-based setting, using a binary classifica-
tion (yes/no for association with N) on each node in
the tree, and during testing choosing the node with
the highest score for positive association with N.6
All other phases of classification involve indepen-
dent decisions at each node. In phase 3, we include
a special zero node to indicate a control locus with
no antecedent.
</bodyText>
<subsectionHeader confidence="0.996811">
3.1 Feature templates
</subsectionHeader>
<bodyText confidence="0.9752292">
Each subphase of our dependency reconstruction al-
gorithm involves the training of a separate model
and the development of a separate feature set. We
found that it was important to include both a variety
of general feature templates and a number of manu-
ally designed, specialized features to resolve spe-
cific problems observed for individual classifiers.
We developed all feature templates exclusively on
the training and development sets specified in Sec-
tion 2.
</bodyText>
<tableCaption confidence="0.3762215">
Table 1 shows which general feature templates
we used in each classifier. The features are
6The choice of a unique origin site makes our algorithm un-
able to deal with right-node raising or parasitic gaps. Cases
of right-node raising could be automatically transformed into
single-origin dislocations by making use of a theory of coordi-
nation such as Maxwell and Manning (1996), while parasitic
gaps could be handled with the introduction of a secondary
classifier. Both phenomena are low-frequency, however, and
we ignore them here.
</tableCaption>
<table confidence="0.969015423076923">
Feature typedI
In
dIRIn
Id
InFi
TAG ⊗ ✓ ✓ ✓ ✓
HD ✓ ✓
CAT × MCAT ✓
CAT×MCAT×GCAT
CAT×HD×MCAT×MHD
CAT × TAG × MCAT × MTAG
CAT × TAG ✓ ✓
CAT × HD
(FIRST/LAST)CAT ✓ ✓
(L/RSIS)CAT ✓ ✓
DPOS × CAT ✓
PATH ✓ ✓
CAT × RCAT ✓
TAG × RCAT ✓
CAT × TAG × RCAT ✓
CAT × RCAT × DPO S ✓
HD×RHD
CAT ×HD×RHD ✓
CAT×DCAT ✓ ✓ ✓ ✓
MHD×HD
# Special 9 0 11 0 0 12 0 3
</table>
<tableCaption confidence="0.97868575">
Table 1: Shared feature templates. See text for template
descriptions. # Special is the number of special templates
used for the classifier. ⊗ denotes that all subsets of the
template conjunction were included.
</tableCaption>
<bodyText confidence="0.999972846153846">
coded as follows. The prefixes {∅,M,G,D,R} in-
dicate that the feature value is calculated with re-
spect to the node in question, its mother, grand-
mother, daughter, or relative node respectively.7
{CAT,POS,TAG,WORD} stand for syntactic cate-
gory, position (of daughter) in mother, head tag, and
head word respectively. For example, when deter-
mining whether an infinitival VP is extraposed, such
as S-2 in Figure 1, the plausibility of the VP head
being a deep dependent of the head verb is captured
with the MHD×HD template. (FIRST/LAST)CAT
and (L/RSIS)CAT are templates used for choosing
the position to insert insert relocated nodes, respec-
tively recording whether a node of a given category
is the first/last daughter, and the syntactic category
of a node’s left/right sisters. PATH is the syntac-
tic path between relative and base node, defined as
the list of the syntactic categories on the (inclusive)
node path linking the relative node to the node in
question, paired with whether the step on the path
was upward or downward. For example, in Figure
2 the syntactic path from VP-1 to PP is [↑-VP,↑-
S,↓-VP,↓-PP]. This is a crucial feature for the rel-
ativized classifiers RELOCATEMOVED and FIND-
CONTROLLER; in an abstract sense it mediates the
gap-threading information incorporated into GPSG-
</bodyText>
<footnote confidence="0.8750205">
7The relative node is DISLOCATED in RELOCMOVED and
LOCUS in FINDCONTROLLER.
</footnote>
<figure confidence="0.734274333333333">
IDENTLOCUS S
VP
�
VP-internal context
to determine null
subjecthood
INSERTNULLS S VP Possible null com-
plementizer (records
syntactic path from
</figure>
<table confidence="0.963131875">
every S in sentence)
PCF P AoP JoP D G AoG JoG
91.2 87.6 90.5 90.0 88.3 95.7 99.4 98.5
91.6 89.9 91.4 91.2 89.4 97.9 99.8 99.6
93.3 83.4 91.2 89.9 89.2 89.0 98.0 96.0
91.2 87.3 90.2 89.6 88.0 95.2 99.0 97.7
73.1 72.8 72.9 72.8 72.5 99.7 99.6 98.8
94.4 66.7 89.3 84.9 85.0 72.6 99.4 94.1
70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7
Gold trees Parser output
Jn Pres Jn DD Pres
62.4 75.3 55.6 (69.5) 61.1
85.1 67.6 80.0 (82.0) 63.3
89.3 99.6 77.1 (48.8) 87.0
74.8 74.7 71.0 73.8 71.0
90 93.3 87 84.5 83.6
</table>
<figure confidence="0.97713475">
NP-*
WH-t
0
SBAR
S-t
Overall
NP
S
VP
ADJP
SBAR
ADVP
</figure>
<tableCaption confidence="0.990196666666667">
Table 2: Comparison with previous work using John-
son’s PARSEVAL metric. Jn is Johnson (2002); DD is
Dienes and Dubey (2003b); Pres is the present work.
</tableCaption>
<bodyText confidence="0.999535705882353">
style (Gazdar et al., 1985) parsers, and in concrete
terms it closely matches the information derived
from Johnson (2002)’s connected local tree set pat-
terns. Gildea and Jurafsky (2002) is to our knowl-
edge the first use of such a feature for classification
tasks on syntactic trees; they found it important for
the related task of semantic role identification.
We expressed specialized hand-coded feature
templates as tree-matching patterns that capture a
fragment of the content of the pattern in the fea-
ture value. Representative examples appear in Fig-
ure 3. The italicized node is the node for which
a given feature is recorded; underscores indi-
cate variables that can match any category; and the
angle-bracketed parts of the tree fragment, together
with an index for the pattern, determine the feature
value.8
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999859">
4.1 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.9998945625">
Our algorithm’s performance can be compared with
the work of Johnson (2002) and Dienes and Dubey
(2003a) on WSJ. Valid comparisons exist for the
insertion of uncoindexed empty nodes (COMP and
ARB-SUBJ), identification of control and raising
loci (CONTROLLOCUS), and pairings of dislo-
cated and controller/raised nodes with their origins
(DISLOC,CONTROLLER). In Table 2 we present
comparative results, using the PARSEVAL-based
evaluation metric introduced by Johnson (2002) – a
correct empty category inference requires the string
position of the empty category, combined with the
left and right boundaries plus syntactic category of
the antecedent, if any, for purposes of compari-
son.9,10 Note that this evaluation metric does not re-
quire correct attachment of the empty category into
</bodyText>
<footnote confidence="0.8727187">
8A complete description of feature templates can be found
at http://nlp.stanford.edu/˜rog/acl2004/templates/index.html
9For purposes of comparability with Johnson (2002) we
used Charniak’s 2000 parser as P.
10Our algorithm was evaluated on a more stringent standard
for NP-* than in previous work: control loci-related mappings
were done after dislocated nodes were actually relocated by the
algorithm, so an incorrect dislocation remapping can render in-
correct the indices of a correct NP-* labeled bracketing. Addi-
tionally, our algorithm does not distinguish the syntactic cate-
</footnote>
<tableCaption confidence="0.862361">
Table 3: Typed dependency F1 performance when com-
</tableCaption>
<figureCaption confidence="0.536925">
posed with statistical parser. PCF is parser output eval-
uated by context-free (shallow) dependencies; all oth-
ers are evaluated on deep dependencies. P is parser, G
is string-to-context-free-gold-tree mapping, A is present
remapping algorithm, J is Johnson 2002, D is the COM-
BINED model of Dienes 2003.
</figureCaption>
<bodyText confidence="0.993812571428571">
the parse tree. In Figure 1, for example, WHNP-
1 could be erroneously remapped to the right edge
of any S or VP node in the sentence without result-
ing in error according to this metric. We therefore
abandon this metric in further evaluations as it is
not clear whether it adequately approximates perfor-
mance in predicate-argument structure recovery.11
</bodyText>
<subsectionHeader confidence="0.999651">
4.2 Composition with a context-free parser
</subsectionHeader>
<bodyText confidence="0.957690629629629">
If we think of a statistical parser as a function from
strings to CF trees, and the nonlocal dependency
recovery algorithm A presented in this paper as a
function from trees to trees, we can naturally com-
pose our algorithm with a parser P to form a func-
tion A o P from strings to trees whose dependency
interpretation is, hopefully, an improvement over
the trees from P.
To test this idea quantitatively we evaluate perfor-
mance with respect to recovery of typed dependency
relations between words. A dependency relation,
commonly employed for evaluation in the statistical
parsing literature, is defined at a node N of a lexi-
calized parse tree as a pair (wi, wj) where wi is the
lexical head of N and wj is the lexical head of some
non-head daughter of N. Dependency relations may
further be typed according to information at or near
the relevant tree node; Collins (1999), for exam-
ple, reports dependency scores typed on the syn-
tactic categories of the mother, head daughter, and
dependent daughter, plus on whether the dependent
precedes or follows the head. We present here de-
pendency evaluations where the gold-standard de-
pendency set is defined by the remapped tree, typed
gory of null insertions, whereas previous work has; as a result,
the null complementizer class 0 and WH-t dislocation class are
aggregates of classes used in previous work.
</bodyText>
<footnote confidence="0.940062">
11Collins (1999) reports 93.8%/90.1% precision/recall in his
Model 3 for accurate identification of relativization site in non-
infinitival relative clauses. This figure is difficult to compare
directly with other figures in this section; a tree search indi-
cates that non-infinitival subjects make up at most 85.4% of the
WHNP dislocations in WSJ.
</footnote>
<table confidence="0.940421888888889">
WSJ(full)
WSJ(sm)
NEGRA
Performance on gold trees Performance on parsed trees
ID Rel Combo ID Combo
P R F1 Acc P R F1 P R F1 P R F1
92.0 82.9 87.2 95.0 89.6 80.1 84.6 34.5 47.6 40.0 17.8 24.3 20.5
92.3 79.5 85.5 93.3 90.4 77.2 83.2 38.0 47.3 42.1 19.7 24.3 21.7
73.9 64.6 69.0 85.1 63.3 55.4 59.1 48.3 39.7 43.6 20.9 17.2 18.9
</table>
<tableCaption confidence="0.877723666666667">
Table 4: Cross-linguistic comparison of dislocated node identification and remapping. ID is correct identification
of nodes as +/– dislocated; Rel is relocation of node to correct mother given gold-standard data on which nodes are
dislocated (only applicable for gold trees); Combo is both correct identification and remapping.
</tableCaption>
<bodyText confidence="0.999897733333334">
by syntactic category of the mother node.12 In Fig-
ure 1, for example, to would be an ADJP dependent
of quick rather than a VP dependent of was; and
Farmers would be an S dependent both of to in to
point out ... and of was. We use the head-finding
rules of Collins (1999) to lexicalize trees, and as-
sume that null complementizers do not participate
in dependency relations. To further compare the re-
sults of our algorithm with previous work, we ob-
tained the output trees produced by Johnson (2002)
and Dienes (2003) and evaluated them on typed de-
pendency performance. Table 3 shows the results of
this evaluation. For comparison, we include shal-
low dependency accuracy for Charniak’s parser un-
der PCF.
</bodyText>
<subsectionHeader confidence="0.998041">
4.3 Cross-linguistic comparison
</subsectionHeader>
<bodyText confidence="0.9996518">
In order to compare the results of nonlocal depen-
dency reconstruction between languages, we must
identify equivalence classes of nonlocal dependency
annotation between treebanks. NEGRA’s nonlocal
dependency annotation is quite different from WSJ,
as described in Section 2, ignoring controlled and
arbitrary unexpressed subjects. The natural basis
of comparison is therefore the set of all nonlocal
NEGRA annotations against all WSJ dislocations,
excluding relativizations (defined simply as dislo-
cated wh- constituents under SBAR).13
Table 4 shows the performance comparison be-
tween WSJ and NEGRA of IDENTDISLOC and RE-
LOCMOVED, on sentences of 40 tokens or less.
For this evaluation metric we use syntactic cate-
gory and left &amp; right edges of (1) dislocated nodes
(ID); and (2) originating mother node to which dis-
located node is mapped (Rel). Combo requires both
(1) and (2) to be correct. NEGRA is smaller than
WSJ (∼350,000 words vs. 1 million), so for fair
</bodyText>
<footnote confidence="0.993231">
12Unfortunately, 46 WSJ dislocation annotations in this test-
set involve dislocated nodes dominating their origin sites. It
is not entirely clear how to interpret the intended semantics of
these examples, so we ignore them in evaluation.
13The interpretation of comparative results must be modu-
lated by the fact that more total time was spent on feature en-
gineering for WSJ than for NEGRA, and the first author, who
engineered the NEGRA feature set, is not a native speaker of
German.
</footnote>
<bodyText confidence="0.999833166666667">
comparison we tested WSJ using the smaller train-
ing set described in Section 2, comparable in size
to NEGRA’s. Since the positioning of traces within
NEGRA nodes is trivial, we evaluate remapping and
combination performances requiring only proper se-
lection of the originating mother node; thus we
carry the algorithm out on both treebanks through
step (2b). This is adequate for purposes of our
typed dependency evaluation in Section 4.2, since
typed dependencies do not depend on positional in-
formation. State-of-the-art statistical parsing is far
better on WSJ (Charniak, 2000) than on NEGRA
(Dubey and Keller, 2003), so for comparison of
parser-composed dependency performance we used
vanilla PCFG models for both WSJ and NEGRA
trained on comparably-sized datasets; in addition to
making similar types of independence assumptions,
these models performed relatively comparably on
labeled bracketing measures for our development
sets (73.2% performance for WSJ versus 70.9% for
NEGRA).
Table 5 compares the testset performance of al-
gorithms on the two treebanks on the typed depen-
dency measure introduced in Section 4.2.14
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.949456">
The WSJ results shown in Tables 2 and 3 suggest
that discriminative models incorporating both non-
local and local lexical and syntactic information can
achieve good results on the task of non-local depen-
dency identification. On the PARSEVAL metric,
our algorithm performed particularly well on null
complementizer and control locus insertion, and on
S node relocation. In particular, Johnson noted that
the proper insertion of control loci was a difficult
issue involving lexical as well as structural sensitiv-
ity. We found the loglinear paradigm a good one
in which to model this feature combination; when
run in isolation on gold-standard development trees,
our model reached 96.4% F1 on control locus inser-
tion, reducing error over the Johnson model’s 89.3%
14Many head-dependent relations in NEGRA are explicitly
marked, but for those that are not we used a Collins (1999)-
style head-finding algorithm independently developed for Ger-
man PCFG parsing.
</bodyText>
<table confidence="0.933842">
PCF P A ◦ P G A ◦ G
76.3 75.4 75.7 98.7 99.7
76.3 75.4 75.7 98.7 99.6
62.0 59.3 61.0 90.9 93.6
</table>
<tableCaption confidence="0.982805">
Table 5: Typed dependency F1 performance when com-
posed with statistical parser. Remapped dependencies
involve only non-relativization dislocations and exclude
control loci.
</tableCaption>
<bodyText confidence="0.998324375">
by nearly two-thirds. The performance of our algo-
rithm is also evident in the substantial contribution
to typed dependency accuracy seen in Table 3. For
gold-standard input trees, our algorithm reduces er-
ror by over 80% from the surface-dependency base-
line, and over 60% compared with Johnson’s re-
sults. For parsed input trees, our algorithm reduces
dependency error by 23% over the baseline, and by
5% compared with Johnson’s results. Note that the
dependency figures of Dienes lag behind even the
parsed results for Johnson’s model; this may well
be due to the fact that Dienes built his model as
an extension of Collins (1999), which lags behind
Charniak (2000) by about 1.3-1.5%.
Manual investigation of errors on English gold-
standard data revealed two major issues that suggest
further potential for improvement in performance
without further increase in algorithmic complexity
or training set size. First, we noted that annotation
inconsistency accounted for a large number of er-
rors, particularly false positives. VPs from which an
S has been extracted ([SShut up,] he [VP said t]) are
inconsistently given an empty SBAR daughter, sug-
gesting the cross-model low-70’s performance on
null SBAR insertion models (see Table 2) may be
a ceiling. Control loci were often under-annotated;
the first five development-set false positive control
loci we checked were all due to annotation error.
And why-WHADVPs under SBAR, which are al-
ways dislocations, were not so annotated 20% of the
time. Second, both control locus insertion and dis-
located NP remapping must be sensitive to the pres-
ence of argument NPs under classified nodes. But
temporal NPs, indistinguishable by gross category,
also appear under such nodes, creating a major con-
found. We used customized features to compensate
to some extent, but temporal annotation already ex-
ists in WSJ and could be used. We note that Klein
and Manning (2003) independently found retention
of temporal NP marking useful for PCFG parsing.
As can be seen in Table 3, the absolute improve-
ment in dependency recovery is smaller for both
our and Johnson’s postprocessing algorithms when
applied to parsed input trees than when applied to
gold-standard input trees. It seems that this degra-
dation is not primarily due to noise in parse tree out-
puts reducing recall of nonlocal dependency iden-
tification: precision/recall splits were largely the
same between gold and parsed data, and manual
inspection revealed that incorrect nonlocal depen-
dency choices often arose from syntactically rea-
sonable yet incorrect input from the parser. For
example, the gold-standard parse right-wing whites
... will [VP step up [NP their threats [S [VP * to take
matters into their own hands ]]]] has an unindexed
control locus because Treebank annotation specifies
that infinitival VPs inside NPs are not assigned con-
trollers. Charniak’s parser, however, attaches the in-
finitival VP into the higher step up ... VP. Infinitival
VPs inside VPs generally do receive controllers for
their null subjects, and our algorithm accordingly
yet mistakenly assigns right-wing-whites as the an-
tecedent.
The English/German comparison shown in Ta-
bles 4 and 5 is suggestive, but caution is necessary
in its interpretation due to the fact that differences
in both language structure and treebank annotation
may be involved. Results in the G column of Ta-
ble 5, showing the accuracy of the context-free de-
pendency approximation from gold-standard parse
trees, quantitatively corroborates the intuition that
nonlocal dependency is more prominent in German
than in English.
Manual investigation of errors made on German
gold-standard data revealed two major sources of er-
ror beyond sparsity. The first was a widespread am-
biguity of S and VP nodes within S and VP nodes;
many true dislocations of all sorts are expressed at
the S and VP levels in CFG parse trees, such as VP-
1 of Figure 2, but many adverbial and subordinate
phrases of S or VP category are genuine dependents
of the main clausal verb. We were able to find a
number of features to distinguish some cases, such
as the presence of certain unambiguous relative-
clause introducing complementizers beginning an S
node, but much ambiguity remained. The second
was the ambiguity that some matrix S-initial NPs
are actually dependents of the VP head (in these
cases, NEGRA annotates the finite verb as the head
of S and the non-finite verb as the head of VP). This
is not necessarily a genuine discontinuity per se,
but rather corresponds to identification of the sub-
ject NP in a clause. Obviously, having access to
reliable case marking would improve performance
in this area; such information is in fact included in
NEGRA’s morphological annotation, another argu-
ment for the utility of involving enhanced annota-
tion in CF parsing.
As can be seen in the right half of Table 4, per-
formance falls off considerably on vanilla PCFG-
WSJ(full)
WSJ(sm)
NEGRA
parsed data. This fall-off seems more dramatic than
that seen in Sections 4.1 and 4.2, no doubt partly
due to the poorer performance of the vanilla PCFG,
but likely also because only non-relativization dis-
locations are considered in Section 4.3. These dis-
locations often require non-local information (such
as identity of surface lexical governor) for identifi-
cation and are thus especially susceptible to degra-
dation in parsed data. Nevertheless, seemingly dis-
mal performance here still provided a strong boost
to typed dependency evaluation of parsed data, as
seen in A ◦ P of Table 5. We suspect this indicates
that dislocated terminals are being usefully iden-
tified and mapped back to their proper governors,
even if the syntactic projections of these terminals
and governors are not being correctly identified by
the parser.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="method">
6 Further Work
</sectionHeader>
<bodyText confidence="0.9998954">
Against the background of CFG as the standard
approximation of dependency structure for broad-
coverage parsing, there are essentially three op-
tions for the recovery of nonlocal dependency. The
first option is to postprocess CF parse trees, which
we have closely investigated in this paper. The
second is to incorporate nonlocal dependency in-
formation into the category structure of CF trees.
This was the approach taken by Dienes and Dubey
(2003a,b) and Dienes (2003); it is also practiced
in recent work on broad-coverage CCG parsing
(Hockenmaier, 2003). The third would be to in-
corporate nonlocal dependency information into the
edge structure parse trees, allowing discontinuous
constituency to be explicitly represented in the parse
chart. This approach was tentatively investigated
by Plaehn (2000). As the syntactic diversity of
languages for which treebanks are available grows,
it will become increasingly important to compare
these three approaches.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="conclusions">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9944215">
This work has benefited from feedback from Dan
Jurafsky and three anonymous reviewers, and from
presentation at the Institute of Cognitive Science,
University of Colorado at Boulder. The au-
thors are also grateful to Dan Klein and Jenny
Finkel for use of maximum-entropy software they
wrote. This work was supported in part by
the Advanced Research and Development Activity
(ARDA)’s Advanced Question Answering for Intel-
ligence (AQUAINT) Program.
</bodyText>
<sectionHeader confidence="0.997834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999587224137931">
Charniak, E. (2000). A Maximum-Entropy-inspired parser. In
Proceedings ofNAACL.
Chomsky, N. (1956). Three models for the description of lan-
guage. IRE Transactions on Information Theory, 2(3):113–
124.
Collins, M. (1999). Head-Driven Statistical Models for Natural
Language Parsing. PhD thesis, University of Pennsylvania.
Dienes, P. (2003). Statistical Parsing with Non-local Depen-
dencies. PhD thesis, Saarland University.
Dienes, P. and Dubey, A. (2003a). Antecedent recovery: Ex-
periments with a trace tagger. In Proceedings of EMNLP.
Dienes, P. and Dubey, A. (2003b). Deep processing by com-
bining shallow methods. In Proceedings ofACL.
Dubey, A. and Keller, F. (2003). Parsing German with sister-
head dependencies. In Proceedings ofACL.
Gazdar, G., Klein, E., Pullum, G., and Sag, I. (1985). General-
ized Phrase Structure Grammar. Harvard.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245–288.
Hockenmaier, J. (2003). Data and models for Statistical Pars-
ing with Combinatory Categorial Grammar. PhD thesis,
University of Edinburgh.
Johnson, M. (2002). A simple pattern-matching algorithm for
recovering empty nodes and their antecedents. In Proceed-
ings ofACL, volume 40.
Kaplan, R., Riezler, S., King, T. H., Maxwell, J. T., Vasserman,
A., and Crouch, R. (2004). Speed and accuracy in shallow
and deep stochastic parsing. In Proceedings ofNAACL.
Kaplan, R. M. and Maxwell, J. T. (1993). The interface be-
tween phrasal and functional constraints. Computational
Linguistics, 19(4):571–590.
Klein, D. and Manning, C. D. (2003). Accurate unlexicalized
parsing. In Proceedings ofACL.
Kruijff, G.-J. (2002). Learning linearization rules
from treebanks. Invited talk at the Formal
Grammar’02/COLOGNET-ELSNET Symposium.
Levy, R. (2004). Probabilistic Models of Syntactic Discontinu-
ity. PhD thesis, Stanford University. In progress.
Maxwell, J. T. and Manning, C. D. (1996). A theory of non-
constituent coordination based on finite-state rules. In Butt,
M. and King, T. H., editors, Proceedings ofLFG.
Pasca, M. and Harabagiu, S. M. (2001). High performance
question/answering. In Proceedings of SIGIR.
Plaehn, O. (2000). Computing the most probable parse for a
discontinuous phrase structure grammar. In Proceedings of
IWPT, Trento, Italy.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R. S., Maxwell,
J. T., and Johnson, M. (2002). Parsing the Wall Street Jour-
nal using a Lexical-Functional Grammar and discriminative
estimation techniques. In Proceedings of ACL, pages 271–
278.
Skut, W., Brants, T., Krenn, B., and Uszkoreit, H. (1997a).
Annotating unrestricted German text. In Fachtagung der
Sektion Computerlinguistik der Deutschen Gesellschaft fr
Sprachwissenschaft, Heidelberg, Germany.
Skut, W., Krenn, B., Brants, T., and Uszkoreit, H. (1997b). An
annotation scheme for free word order languages. In Pro-
ceedings ofANLP.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865203">
<title confidence="0.933933">Deep dependencies from context-free statistical parsers: correcting the surface dependency approximation</title>
<author confidence="0.999923">Roger Levy Christopher D Manning</author>
<affiliation confidence="0.999777">Department of Linguistics Departments of Computer Science and Linguistics Stanford University Stanford University</affiliation>
<email confidence="0.999574">rog@stanford.edumanning@cs.stanford.edu</email>
<abstract confidence="0.9992683">We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks. We use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation. We find that our algorithm compares favorably with prior work on English using an existing evaluation metric, and also introduce and argue for a new dependency-based evaluation metric. By this new evaluation metric our algorithm achieves 60% error reduction on gold-standard input trees and 5% error reduction on state-ofthe-art machine-parsed input trees, when compared with the best previous work. We also present the first results on nonlocal dependency reconstruction for a language other than English, comparing performance on English and German. Our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order, the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A Maximum-Entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="27563" citStr="Charniak, 2000" startWordPosition="4434" endWordPosition="4435"> is not a native speaker of German. comparison we tested WSJ using the smaller training set described in Section 2, comparable in size to NEGRA’s. Since the positioning of traces within NEGRA nodes is trivial, we evaluate remapping and combination performances requiring only proper selection of the originating mother node; thus we carry the algorithm out on both treebanks through step (2b). This is adequate for purposes of our typed dependency evaluation in Section 4.2, since typed dependencies do not depend on positional information. State-of-the-art statistical parsing is far better on WSJ (Charniak, 2000) than on NEGRA (Dubey and Keller, 2003), so for comparison of parser-composed dependency performance we used vanilla PCFG models for both WSJ and NEGRA trained on comparably-sized datasets; in addition to making similar types of independence assumptions, these models performed relatively comparably on labeled bracketing measures for our development sets (73.2% performance for WSJ versus 70.9% for NEGRA). Table 5 compares the testset performance of algorithms on the two treebanks on the typed dependency measure introduced in Section 4.2.14 5 Discussion The WSJ results shown in Tables 2 and 3 su</context>
<context position="30008" citStr="Charniak (2000)" startWordPosition="4821" endWordPosition="4822">rithm is also evident in the substantial contribution to typed dependency accuracy seen in Table 3. For gold-standard input trees, our algorithm reduces error by over 80% from the surface-dependency baseline, and over 60% compared with Johnson’s results. For parsed input trees, our algorithm reduces dependency error by 23% over the baseline, and by 5% compared with Johnson’s results. Note that the dependency figures of Dienes lag behind even the parsed results for Johnson’s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English goldstandard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size. First, we noted that annotation inconsistency accounted for a large number of errors, particularly false positives. VPs from which an S has been extracted ([SShut up,] he [VP said t]) are inconsistently given an empty SBAR daughter, suggesting the cross-model low-70’s performance on null SBAR insertion models (see Table 2) may be a ceiling. Control loci wer</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. (2000). A Maximum-Entropy-inspired parser. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Three models for the description of language.</title>
<date>1956</date>
<journal>IRE Transactions on Information Theory,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>124</pages>
<contexts>
<context position="2577" citStr="Chomsky (1956)" startWordPosition="374" endWordPosition="375"> an overt subject. Knowledge of these hidden relationships is in turn essential to semantic interpretation of the kind practiced in the semantic parsing (Gildea and Jurafsky, 2002) and QA (Pasca and Harabagiu, 2001) literatures. However, work in statistical parsing has for the most part put these needs aside, being content to recover surface context-free (CF) phrase structure trees. This perhaps reflects the fact that context-free phrase structure grammar (CFG) is in some sense at the the heart of the majority of both formal and computational syntactic research. Although, upon introducing it, Chomsky (1956) rejected CFG as an adequate framework for natural language description, the majority of work in the last half century has used context-free structural descriptions and related methodologies in one form or another as an important component of syntactic analysis. CFGs seem adequate to weakly generate almost all common natural language structures, and also facilitate a transparent predicate-argument and/or semantic interpretation for the more basic ones (Gazdar et al., 1985). Nevertheless, despite their success in providing surface phrase structure analyses, if statistical parsers and the repres</context>
</contexts>
<marker>Chomsky, 1956</marker>
<rawString>Chomsky, N. (1956). Three models for the description of language. IRE Transactions on Information Theory, 2(3):113– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5188" citStr="Collins (1999)" startWordPosition="774" endWordPosition="775"> involved in the interpretation and use of non-local annotation structure found in treebanks. A more complete exposition of the work presented here can be found in Levy (2004). Figure 1: Example of empty and nonlocal annotations from the Penn Treebank of English, including null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-s</context>
<context position="23326" citStr="Collins (1999)" startWordPosition="3743" endWordPosition="3744">A o P from strings to trees whose dependency interpretation is, hopefully, an improvement over the trees from P. To test this idea quantitatively we evaluate performance with respect to recovery of typed dependency relations between words. A dependency relation, commonly employed for evaluation in the statistical parsing literature, is defined at a node N of a lexicalized parse tree as a pair (wi, wj) where wi is the lexical head of N and wj is the lexical head of some non-head daughter of N. Dependency relations may further be typed according to information at or near the relevant tree node; Collins (1999), for example, reports dependency scores typed on the syntactic categories of the mother, head daughter, and dependent daughter, plus on whether the dependent precedes or follows the head. We present here dependency evaluations where the gold-standard dependency set is defined by the remapped tree, typed gory of null insertions, whereas previous work has; as a result, the null complementizer class 0 and WH-t dislocation class are aggregates of classes used in previous work. 11Collins (1999) reports 93.8%/90.1% precision/recall in his Model 3 for accurate identification of relativization site i</context>
<context position="25076" citStr="Collins (1999)" startWordPosition="4041" endWordPosition="4042"> 20.9 17.2 18.9 Table 4: Cross-linguistic comparison of dislocated node identification and remapping. ID is correct identification of nodes as +/– dislocated; Rel is relocation of node to correct mother given gold-standard data on which nodes are dislocated (only applicable for gold trees); Combo is both correct identification and remapping. by syntactic category of the mother node.12 In Figure 1, for example, to would be an ADJP dependent of quick rather than a VP dependent of was; and Farmers would be an S dependent both of to in to point out ... and of was. We use the head-finding rules of Collins (1999) to lexicalize trees, and assume that null complementizers do not participate in dependency relations. To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. Table 3 shows the results of this evaluation. For comparison, we include shallow dependency accuracy for Charniak’s parser under PCF. 4.3 Cross-linguistic comparison In order to compare the results of nonlocal dependency reconstruction between languages, we must identify equivalence classes of nonlocal</context>
<context position="28996" citStr="Collins (1999)" startWordPosition="4655" endWordPosition="4656"> performed particularly well on null complementizer and control locus insertion, and on S node relocation. In particular, Johnson noted that the proper insertion of control loci was a difficult issue involving lexical as well as structural sensitivity. We found the loglinear paradigm a good one in which to model this feature combination; when run in isolation on gold-standard development trees, our model reached 96.4% F1 on control locus insertion, reducing error over the Johnson model’s 89.3% 14Many head-dependent relations in NEGRA are explicitly marked, but for those that are not we used a Collins (1999)- style head-finding algorithm independently developed for German PCFG parsing. PCF P A ◦ P G A ◦ G 76.3 75.4 75.7 98.7 99.7 76.3 75.4 75.7 98.7 99.6 62.0 59.3 61.0 90.9 93.6 Table 5: Typed dependency F1 performance when composed with statistical parser. Remapped dependencies involve only non-relativization dislocations and exclude control loci. by nearly two-thirds. The performance of our algorithm is also evident in the substantial contribution to typed dependency accuracy seen in Table 3. For gold-standard input trees, our algorithm reduces error by over 80% from the surface-dependency base</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dienes</author>
</authors>
<title>Statistical Parsing with Non-local Dependencies.</title>
<date>2003</date>
<tech>PhD thesis,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="5490" citStr="Dienes (2003)" startWordPosition="813" endWordPosition="814">ativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets</context>
<context position="22058" citStr="Dienes 2003" startWordPosition="3524" endWordPosition="3525">d mappings were done after dislocated nodes were actually relocated by the algorithm, so an incorrect dislocation remapping can render incorrect the indices of a correct NP-* labeled bracketing. Additionally, our algorithm does not distinguish the syntactic cateTable 3: Typed dependency F1 performance when composed with statistical parser. PCF is parser output evaluated by context-free (shallow) dependencies; all others are evaluated on deep dependencies. P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. the parse tree. In Figure 1, for example, WHNP1 could be erroneously remapped to the right edge of any S or VP node in the sentence without resulting in error according to this metric. We therefore abandon this metric in further evaluations as it is not clear whether it adequately approximates performance in predicate-argument structure recovery.11 4.2 Composition with a context-free parser If we think of a statistical parser as a function from strings to CF trees, and the nonlocal dependency recovery algorithm A presented in this paper as a function from trees to trees, we can naturally com</context>
<context position="25320" citStr="Dienes (2003)" startWordPosition="4081" endWordPosition="4082">e dislocated (only applicable for gold trees); Combo is both correct identification and remapping. by syntactic category of the mother node.12 In Figure 1, for example, to would be an ADJP dependent of quick rather than a VP dependent of was; and Farmers would be an S dependent both of to in to point out ... and of was. We use the head-finding rules of Collins (1999) to lexicalize trees, and assume that null complementizers do not participate in dependency relations. To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. Table 3 shows the results of this evaluation. For comparison, we include shallow dependency accuracy for Charniak’s parser under PCF. 4.3 Cross-linguistic comparison In order to compare the results of nonlocal dependency reconstruction between languages, we must identify equivalence classes of nonlocal dependency annotation between treebanks. NEGRA’s nonlocal dependency annotation is quite different from WSJ, as described in Section 2, ignoring controlled and arbitrary unexpressed subjects. The natural basis of comparison is therefore the se</context>
<context position="35560" citStr="Dienes (2003)" startWordPosition="5717" endWordPosition="5718">per governors, even if the syntactic projections of these terminals and governors are not being correctly identified by the parser. 6 Further Work Against the background of CFG as the standard approximation of dependency structure for broadcoverage parsing, there are essentially three options for the recovery of nonlocal dependency. The first option is to postprocess CF parse trees, which we have closely investigated in this paper. The second is to incorporate nonlocal dependency information into the category structure of CF trees. This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003). The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. This approach was tentatively investigated by Plaehn (2000). As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches. 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three anonymous reviewers, an</context>
</contexts>
<marker>Dienes, 2003</marker>
<rawString>Dienes, P. (2003). Statistical Parsing with Non-local Dependencies. PhD thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dienes</author>
<author>A Dubey</author>
</authors>
<title>Antecedent recovery: Experiments with a trace tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5468" citStr="Dienes and Dubey (2003" startWordPosition="808" endWordPosition="811">g null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional i</context>
<context position="12938" citStr="Dienes and Dubey (2003" startWordPosition="2010" endWordPosition="2013">CUS (IDENTLOCUS) (b) For each LOCUS, determine position of insertion and insert LOCUS (INSERTLOCUS) (c) For each LOCUS, determine CONTROLLER (if any) (FINDCONTROLLER) Note in particular that phase 2 involves the classification of overt tree nodes as dislocated, followed by the identification of an origin site (annotated in the treebank as an empty node) for each dislocated element; whereas phase 3 involves the identification of (empty) control loci first, and of controllers later. This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a,b), who always identify empties first and determine antecedents later. Our motivation is that it should generally be easier to determine whether an overt element is dislocated than whether a given position is the origin of some yet unknown dislocated element (particularly in the absence of a sophisticated model of argument expression); but control loci are highly predictable from local context, such as the subjectless non-finite S in Figure 1’s S-2.5 Indeed this difference seems to be implicit in the nonlocal feature templates used by Dienes and Dubey (2003a,b) in their empty element tagger,</context>
<context position="19455" citStr="Dienes and Dubey (2003" startWordPosition="3126" endWordPosition="3129">2 87.6 90.5 90.0 88.3 95.7 99.4 98.5 91.6 89.9 91.4 91.2 89.4 97.9 99.8 99.6 93.3 83.4 91.2 89.9 89.2 89.0 98.0 96.0 91.2 87.3 90.2 89.6 88.0 95.2 99.0 97.7 73.1 72.8 72.9 72.8 72.5 99.7 99.6 98.8 94.4 66.7 89.3 84.9 85.0 72.6 99.4 94.1 70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7 Gold trees Parser output Jn Pres Jn DD Pres 62.4 75.3 55.6 (69.5) 61.1 85.1 67.6 80.0 (82.0) 63.3 89.3 99.6 77.1 (48.8) 87.0 74.8 74.7 71.0 73.8 71.0 90 93.3 87 84.5 83.6 NP-* WH-t 0 SBAR S-t Overall NP S VP ADJP SBAR ADVP Table 2: Comparison with previous work using Johnson’s PARSEVAL metric. Jn is Johnson (2002); DD is Dienes and Dubey (2003b); Pres is the present work. style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)’s connected local tree set patterns. Gildea and Jurafsky (2002) is to our knowledge the first use of such a feature for classification tasks on syntactic trees; they found it important for the related task of semantic role identification. We expressed specialized hand-coded feature templates as tree-matching patterns that capture a fragment of the content of the pattern in the feature value. Representative examples appear in Figure 3. The itali</context>
<context position="35538" citStr="Dienes and Dubey (2003" startWordPosition="5712" endWordPosition="5715">ed and mapped back to their proper governors, even if the syntactic projections of these terminals and governors are not being correctly identified by the parser. 6 Further Work Against the background of CFG as the standard approximation of dependency structure for broadcoverage parsing, there are essentially three options for the recovery of nonlocal dependency. The first option is to postprocess CF parse trees, which we have closely investigated in this paper. The second is to incorporate nonlocal dependency information into the category structure of CF trees. This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003). The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. This approach was tentatively investigated by Plaehn (2000). As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches. 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three a</context>
</contexts>
<marker>Dienes, Dubey, 2003</marker>
<rawString>Dienes, P. and Dubey, A. (2003a). Antecedent recovery: Experiments with a trace tagger. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dienes</author>
<author>A Dubey</author>
</authors>
<title>Deep processing by combining shallow methods.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="5468" citStr="Dienes and Dubey (2003" startWordPosition="808" endWordPosition="811">g null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional i</context>
<context position="12938" citStr="Dienes and Dubey (2003" startWordPosition="2010" endWordPosition="2013">CUS (IDENTLOCUS) (b) For each LOCUS, determine position of insertion and insert LOCUS (INSERTLOCUS) (c) For each LOCUS, determine CONTROLLER (if any) (FINDCONTROLLER) Note in particular that phase 2 involves the classification of overt tree nodes as dislocated, followed by the identification of an origin site (annotated in the treebank as an empty node) for each dislocated element; whereas phase 3 involves the identification of (empty) control loci first, and of controllers later. This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a,b), who always identify empties first and determine antecedents later. Our motivation is that it should generally be easier to determine whether an overt element is dislocated than whether a given position is the origin of some yet unknown dislocated element (particularly in the absence of a sophisticated model of argument expression); but control loci are highly predictable from local context, such as the subjectless non-finite S in Figure 1’s S-2.5 Indeed this difference seems to be implicit in the nonlocal feature templates used by Dienes and Dubey (2003a,b) in their empty element tagger,</context>
<context position="19455" citStr="Dienes and Dubey (2003" startWordPosition="3126" endWordPosition="3129">2 87.6 90.5 90.0 88.3 95.7 99.4 98.5 91.6 89.9 91.4 91.2 89.4 97.9 99.8 99.6 93.3 83.4 91.2 89.9 89.2 89.0 98.0 96.0 91.2 87.3 90.2 89.6 88.0 95.2 99.0 97.7 73.1 72.8 72.9 72.8 72.5 99.7 99.6 98.8 94.4 66.7 89.3 84.9 85.0 72.6 99.4 94.1 70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7 Gold trees Parser output Jn Pres Jn DD Pres 62.4 75.3 55.6 (69.5) 61.1 85.1 67.6 80.0 (82.0) 63.3 89.3 99.6 77.1 (48.8) 87.0 74.8 74.7 71.0 73.8 71.0 90 93.3 87 84.5 83.6 NP-* WH-t 0 SBAR S-t Overall NP S VP ADJP SBAR ADVP Table 2: Comparison with previous work using Johnson’s PARSEVAL metric. Jn is Johnson (2002); DD is Dienes and Dubey (2003b); Pres is the present work. style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)’s connected local tree set patterns. Gildea and Jurafsky (2002) is to our knowledge the first use of such a feature for classification tasks on syntactic trees; they found it important for the related task of semantic role identification. We expressed specialized hand-coded feature templates as tree-matching patterns that capture a fragment of the content of the pattern in the feature value. Representative examples appear in Figure 3. The itali</context>
<context position="35538" citStr="Dienes and Dubey (2003" startWordPosition="5712" endWordPosition="5715">ed and mapped back to their proper governors, even if the syntactic projections of these terminals and governors are not being correctly identified by the parser. 6 Further Work Against the background of CFG as the standard approximation of dependency structure for broadcoverage parsing, there are essentially three options for the recovery of nonlocal dependency. The first option is to postprocess CF parse trees, which we have closely investigated in this paper. The second is to incorporate nonlocal dependency information into the category structure of CF trees. This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003). The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. This approach was tentatively investigated by Plaehn (2000). As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches. 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three a</context>
</contexts>
<marker>Dienes, Dubey, 2003</marker>
<rawString>Dienes, P. and Dubey, A. (2003b). Deep processing by combining shallow methods. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
<author>F Keller</author>
</authors>
<title>Parsing German with sisterhead dependencies.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6660" citStr="Dubey and Keller (2003)" startWordPosition="999" endWordPosition="1002"> augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testing. Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. Consistent with prior work and with common practice in statistical parsing, we stripped categories of all functional tags prior to training and testing (though in several cases this seems to have been a limiting move; see Section 5). Nonlocal dependency annotation in Penn Treebanks can be divided into three major types: unindexed empty elements, dislocations, and control. The first type consists primarily of null complementizers, as exemplified in Figure 1 by the null relative</context>
<context position="27602" citStr="Dubey and Keller, 2003" startWordPosition="4439" endWordPosition="4442">man. comparison we tested WSJ using the smaller training set described in Section 2, comparable in size to NEGRA’s. Since the positioning of traces within NEGRA nodes is trivial, we evaluate remapping and combination performances requiring only proper selection of the originating mother node; thus we carry the algorithm out on both treebanks through step (2b). This is adequate for purposes of our typed dependency evaluation in Section 4.2, since typed dependencies do not depend on positional information. State-of-the-art statistical parsing is far better on WSJ (Charniak, 2000) than on NEGRA (Dubey and Keller, 2003), so for comparison of parser-composed dependency performance we used vanilla PCFG models for both WSJ and NEGRA trained on comparably-sized datasets; in addition to making similar types of independence assumptions, these models performed relatively comparably on labeled bracketing measures for our development sets (73.2% performance for WSJ versus 70.9% for NEGRA). Table 5 compares the testset performance of algorithms on the two treebanks on the typed dependency measure introduced in Section 4.2.14 5 Discussion The WSJ results shown in Tables 2 and 3 suggest that discriminative models incorp</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>Dubey, A. and Keller, F. (2003). Parsing German with sisterhead dependencies. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<location>Harvard.</location>
<contexts>
<context position="3054" citStr="Gazdar et al., 1985" startWordPosition="444" endWordPosition="447"> some sense at the the heart of the majority of both formal and computational syntactic research. Although, upon introducing it, Chomsky (1956) rejected CFG as an adequate framework for natural language description, the majority of work in the last half century has used context-free structural descriptions and related methodologies in one form or another as an important component of syntactic analysis. CFGs seem adequate to weakly generate almost all common natural language structures, and also facilitate a transparent predicate-argument and/or semantic interpretation for the more basic ones (Gazdar et al., 1985). Nevertheless, despite their success in providing surface phrase structure analyses, if statistical parsers and the representations they produce do not provide a useful stepping stone to recovering the hidden relationships, they will ultimately come to be seen as a dead end, and work will necessarily return to using richer formalisms. In this paper we attempt to establish to what degree current statistical parsers are a useful step in analysis by examining the performance of further statistical classifiers on non-local dependency recovery from CF parse trees. The natural isomorphism from CF t</context>
<context position="19512" citStr="Gazdar et al., 1985" startWordPosition="3136" endWordPosition="3139">.4 97.9 99.8 99.6 93.3 83.4 91.2 89.9 89.2 89.0 98.0 96.0 91.2 87.3 90.2 89.6 88.0 95.2 99.0 97.7 73.1 72.8 72.9 72.8 72.5 99.7 99.6 98.8 94.4 66.7 89.3 84.9 85.0 72.6 99.4 94.1 70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7 Gold trees Parser output Jn Pres Jn DD Pres 62.4 75.3 55.6 (69.5) 61.1 85.1 67.6 80.0 (82.0) 63.3 89.3 99.6 77.1 (48.8) 87.0 74.8 74.7 71.0 73.8 71.0 90 93.3 87 84.5 83.6 NP-* WH-t 0 SBAR S-t Overall NP S VP ADJP SBAR ADVP Table 2: Comparison with previous work using Johnson’s PARSEVAL metric. Jn is Johnson (2002); DD is Dienes and Dubey (2003b); Pres is the present work. style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)’s connected local tree set patterns. Gildea and Jurafsky (2002) is to our knowledge the first use of such a feature for classification tasks on syntactic trees; they found it important for the related task of semantic role identification. We expressed specialized hand-coded feature templates as tree-matching patterns that capture a fragment of the content of the pattern in the feature value. Representative examples appear in Figure 3. The italicized node is the node for which a given feature is recor</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., Klein, E., Pullum, G., and Sag, I. (1985). Generalized Phrase Structure Grammar. Harvard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2143" citStr="Gildea and Jurafsky, 2002" startWordPosition="305" endWordPosition="308">ctic parsing is as an aid to semantic interpretation, in pursuit of broader goals of natural language understanding. Proponents of traditional ‘deep’ or ‘precise’ approaches to syntax, such as GB, CCG, HPSG, LFG, or TAG, have argued that sophisticated grammatical formalisms are essential to resolving various hidden relationships such as the source phrase of moved whphrases in questions and relativizations, or the controller of clauses without an overt subject. Knowledge of these hidden relationships is in turn essential to semantic interpretation of the kind practiced in the semantic parsing (Gildea and Jurafsky, 2002) and QA (Pasca and Harabagiu, 2001) literatures. However, work in statistical parsing has for the most part put these needs aside, being content to recover surface context-free (CF) phrase structure trees. This perhaps reflects the fact that context-free phrase structure grammar (CFG) is in some sense at the the heart of the majority of both formal and computational syntactic research. Although, upon introducing it, Chomsky (1956) rejected CFG as an adequate framework for natural language description, the majority of work in the last half century has used context-free structural descriptions a</context>
<context position="19670" citStr="Gildea and Jurafsky (2002)" startWordPosition="3160" endWordPosition="3163"> 84.9 85.0 72.6 99.4 94.1 70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7 Gold trees Parser output Jn Pres Jn DD Pres 62.4 75.3 55.6 (69.5) 61.1 85.1 67.6 80.0 (82.0) 63.3 89.3 99.6 77.1 (48.8) 87.0 74.8 74.7 71.0 73.8 71.0 90 93.3 87 84.5 83.6 NP-* WH-t 0 SBAR S-t Overall NP S VP ADJP SBAR ADVP Table 2: Comparison with previous work using Johnson’s PARSEVAL metric. Jn is Johnson (2002); DD is Dienes and Dubey (2003b); Pres is the present work. style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)’s connected local tree set patterns. Gildea and Jurafsky (2002) is to our knowledge the first use of such a feature for classification tasks on syntactic trees; they found it important for the related task of semantic role identification. We expressed specialized hand-coded feature templates as tree-matching patterns that capture a fragment of the content of the pattern in the feature value. Representative examples appear in Figure 3. The italicized node is the node for which a given feature is recorded; underscores indicate variables that can match any category; and the angle-bracketed parts of the tree fragment, together with an index for the pattern, d</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
</authors>
<title>Data and models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>PhD thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="35647" citStr="Hockenmaier, 2003" startWordPosition="5730" endWordPosition="5731">are not being correctly identified by the parser. 6 Further Work Against the background of CFG as the standard approximation of dependency structure for broadcoverage parsing, there are essentially three options for the recovery of nonlocal dependency. The first option is to postprocess CF parse trees, which we have closely investigated in this paper. The second is to incorporate nonlocal dependency information into the category structure of CF trees. This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003). The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. This approach was tentatively investigated by Plaehn (2000). As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches. 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three anonymous reviewers, and from presentation at the Institute of Cognitive Science, University of Colorado at Bo</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Hockenmaier, J. (2003). Data and models for Statistical Parsing with Combinatory Categorial Grammar. PhD thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>A simple pattern-matching algorithm for recovering empty nodes and their antecedents.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<volume>40</volume>
<contexts>
<context position="5307" citStr="Johnson (2002)" startWordPosition="789" endWordPosition="790">n of the work presented here can be found in Levy (2004). Figure 1: Example of empty and nonlocal annotations from the Penn Treebank of English, including null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divi</context>
<context position="12845" citStr="Johnson (2002)" startWordPosition="1996" endWordPosition="1998">rtion and insert dislocated (INSERTRELOC) 3. (a) Classify each node as +/- control LOCUS (IDENTLOCUS) (b) For each LOCUS, determine position of insertion and insert LOCUS (INSERTLOCUS) (c) For each LOCUS, determine CONTROLLER (if any) (FINDCONTROLLER) Note in particular that phase 2 involves the classification of overt tree nodes as dislocated, followed by the identification of an origin site (annotated in the treebank as an empty node) for each dislocated element; whereas phase 3 involves the identification of (empty) control loci first, and of controllers later. This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a,b), who always identify empties first and determine antecedents later. Our motivation is that it should generally be easier to determine whether an overt element is dislocated than whether a given position is the origin of some yet unknown dislocated element (particularly in the absence of a sophisticated model of argument expression); but control loci are highly predictable from local context, such as the subjectless non-finite S in Figure 1’s S-2.5 Indeed this difference seems to be implicit in the</context>
<context position="19425" citStr="Johnson (2002)" startWordPosition="3122" endWordPosition="3123">oP JoP D G AoG JoG 91.2 87.6 90.5 90.0 88.3 95.7 99.4 98.5 91.6 89.9 91.4 91.2 89.4 97.9 99.8 99.6 93.3 83.4 91.2 89.9 89.2 89.0 98.0 96.0 91.2 87.3 90.2 89.6 88.0 95.2 99.0 97.7 73.1 72.8 72.9 72.8 72.5 99.7 99.6 98.8 94.4 66.7 89.3 84.9 85.0 72.6 99.4 94.1 70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7 Gold trees Parser output Jn Pres Jn DD Pres 62.4 75.3 55.6 (69.5) 61.1 85.1 67.6 80.0 (82.0) 63.3 89.3 99.6 77.1 (48.8) 87.0 74.8 74.7 71.0 73.8 71.0 90 93.3 87 84.5 83.6 NP-* WH-t 0 SBAR S-t Overall NP S VP ADJP SBAR ADVP Table 2: Comparison with previous work using Johnson’s PARSEVAL metric. Jn is Johnson (2002); DD is Dienes and Dubey (2003b); Pres is the present work. style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)’s connected local tree set patterns. Gildea and Jurafsky (2002) is to our knowledge the first use of such a feature for classification tasks on syntactic trees; they found it important for the related task of semantic role identification. We expressed specialized hand-coded feature templates as tree-matching patterns that capture a fragment of the content of the pattern in the feature value. Representative examples</context>
<context position="20817" citStr="Johnson (2002)" startWordPosition="3338" endWordPosition="3339">rts of the tree fragment, together with an index for the pattern, determine the feature value.8 4 Evaluation 4.1 Comparison with previous work Our algorithm’s performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. Valid comparisons exist for the insertion of uncoindexed empty nodes (COMP and ARB-SUBJ), identification of control and raising loci (CONTROLLOCUS), and pairings of dislocated and controller/raised nodes with their origins (DISLOC,CONTROLLER). In Table 2 we present comparative results, using the PARSEVAL-based evaluation metric introduced by Johnson (2002) – a correct empty category inference requires the string position of the empty category, combined with the left and right boundaries plus syntactic category of the antecedent, if any, for purposes of comparison.9,10 Note that this evaluation metric does not require correct attachment of the empty category into 8A complete description of feature templates can be found at http://nlp.stanford.edu/˜rog/acl2004/templates/index.html 9For purposes of comparability with Johnson (2002) we used Charniak’s 2000 parser as P. 10Our algorithm was evaluated on a more stringent standard for NP-* than in prev</context>
<context position="25302" citStr="Johnson (2002)" startWordPosition="4078" endWordPosition="4079">a on which nodes are dislocated (only applicable for gold trees); Combo is both correct identification and remapping. by syntactic category of the mother node.12 In Figure 1, for example, to would be an ADJP dependent of quick rather than a VP dependent of was; and Farmers would be an S dependent both of to in to point out ... and of was. We use the head-finding rules of Collins (1999) to lexicalize trees, and assume that null complementizers do not participate in dependency relations. To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. Table 3 shows the results of this evaluation. For comparison, we include shallow dependency accuracy for Charniak’s parser under PCF. 4.3 Cross-linguistic comparison In order to compare the results of nonlocal dependency reconstruction between languages, we must identify equivalence classes of nonlocal dependency annotation between treebanks. NEGRA’s nonlocal dependency annotation is quite different from WSJ, as described in Section 2, ignoring controlled and arbitrary unexpressed subjects. The natural basis of comparison i</context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>Johnson, M. (2002). A simple pattern-matching algorithm for recovering empty nodes and their antecedents. In Proceedings ofACL, volume 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>S Riezler</author>
<author>T H King</author>
<author>J T Maxwell</author>
<author>A Vasserman</author>
<author>R Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings ofNAACL.</booktitle>
<contexts>
<context position="5883" citStr="Kaplan et al., 2004" startWordPosition="871" endWordPosition="874">elative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testin</context>
</contexts>
<marker>Kaplan, Riezler, King, Maxwell, Vasserman, Crouch, 2004</marker>
<rawString>Kaplan, R., Riezler, S., King, T. H., Maxwell, J. T., Vasserman, A., and Crouch, R. (2004). Speed and accuracy in shallow and deep stochastic parsing. In Proceedings ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>J T Maxwell</author>
</authors>
<title>The interface between phrasal and functional constraints.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="5824" citStr="Kaplan and Maxwell, 1993" startWordPosition="861" endWordPosition="864">investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and tree</context>
</contexts>
<marker>Kaplan, Maxwell, 1993</marker>
<rawString>Kaplan, R. M. and Maxwell, J. T. (1993). The interface between phrasal and functional constraints. Computational Linguistics, 19(4):571–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="31246" citStr="Klein and Manning (2003)" startWordPosition="5015" endWordPosition="5018">annotated; the first five development-set false positive control loci we checked were all due to annotation error. And why-WHADVPs under SBAR, which are always dislocations, were not so annotated 20% of the time. Second, both control locus insertion and dislocated NP remapping must be sensitive to the presence of argument NPs under classified nodes. But temporal NPs, indistinguishable by gross category, also appear under such nodes, creating a major confound. We used customized features to compensate to some extent, but temporal annotation already exists in WSJ and could be used. We note that Klein and Manning (2003) independently found retention of temporal NP marking useful for PCFG parsing. As can be seen in Table 3, the absolute improvement in dependency recovery is smaller for both our and Johnson’s postprocessing algorithms when applied to parsed input trees than when applied to gold-standard input trees. It seems that this degradation is not primarily due to noise in parse tree outputs reducing recall of nonlocal dependency identification: precision/recall splits were largely the same between gold and parsed data, and manual inspection revealed that incorrect nonlocal dependency choices often arose</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-J Kruijff</author>
</authors>
<title>Learning linearization rules from treebanks. Invited talk at the Formal Grammar’02/COLOGNET-ELSNET Symposium.</title>
<date>2002</date>
<contexts>
<context position="5172" citStr="Kruijff, 2002" startWordPosition="772" endWordPosition="773"> intricacies are involved in the interpretation and use of non-local annotation structure found in treebanks. A more complete exposition of the work presented here can be found in Levy (2004). Figure 1: Example of empty and nonlocal annotations from the Penn Treebank of English, including null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsin</context>
</contexts>
<marker>Kruijff, 2002</marker>
<rawString>Kruijff, G.-J. (2002). Learning linearization rules from treebanks. Invited talk at the Formal Grammar’02/COLOGNET-ELSNET Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Probabilistic Models of Syntactic Discontinuity.</title>
<date>2004</date>
<tech>PhD thesis,</tech>
<institution>Stanford University. In progress.</institution>
<contexts>
<context position="4749" citStr="Levy (2004)" startWordPosition="713" endWordPosition="714">dencies from context-free parse trees using loglinear classifiers, tested on treebanks of not only English but also German, a language with much freer word order and correspondingly more discontinuity than English. This algorithm can be used as an intermediate step between the surface output trees of modern statistical parsers and semantic interpretation systems for a variety of tasks.1 1Many linguistic and technical intricacies are involved in the interpretation and use of non-local annotation structure found in treebanks. A more complete exposition of the work presented here can be found in Levy (2004). Figure 1: Example of empty and nonlocal annotations from the Penn Treebank of English, including null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to</context>
</contexts>
<marker>Levy, 2004</marker>
<rawString>Levy, R. (2004). Probabilistic Models of Syntactic Discontinuity. PhD thesis, Stanford University. In progress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Maxwell</author>
<author>C D Manning</author>
</authors>
<title>A theory of nonconstituent coordination based on finite-state rules.</title>
<date>1996</date>
<booktitle>Proceedings ofLFG.</booktitle>
<editor>In Butt, M. and King, T. H., editors,</editor>
<contexts>
<context position="16564" citStr="Maxwell and Manning (1996)" startWordPosition="2604" endWordPosition="2607">l feature templates and a number of manually designed, specialized features to resolve specific problems observed for individual classifiers. We developed all feature templates exclusively on the training and development sets specified in Section 2. Table 1 shows which general feature templates we used in each classifier. The features are 6The choice of a unique origin site makes our algorithm unable to deal with right-node raising or parasitic gaps. Cases of right-node raising could be automatically transformed into single-origin dislocations by making use of a theory of coordination such as Maxwell and Manning (1996), while parasitic gaps could be handled with the introduction of a secondary classifier. Both phenomena are low-frequency, however, and we ignore them here. Feature typedI In dIRIn Id InFi TAG ⊗ ✓ ✓ ✓ ✓ HD ✓ ✓ CAT × MCAT ✓ CAT×MCAT×GCAT CAT×HD×MCAT×MHD CAT × TAG × MCAT × MTAG CAT × TAG ✓ ✓ CAT × HD (FIRST/LAST)CAT ✓ ✓ (L/RSIS)CAT ✓ ✓ DPOS × CAT ✓ PATH ✓ ✓ CAT × RCAT ✓ TAG × RCAT ✓ CAT × TAG × RCAT ✓ CAT × RCAT × DPO S ✓ HD×RHD CAT ×HD×RHD ✓ CAT×DCAT ✓ ✓ ✓ ✓ MHD×HD # Special 9 0 11 0 0 12 0 3 Table 1: Shared feature templates. See text for template descriptions. # Special is the number of speci</context>
</contexts>
<marker>Maxwell, Manning, 1996</marker>
<rawString>Maxwell, J. T. and Manning, C. D. (1996). A theory of nonconstituent coordination based on finite-state rules. In Butt, M. and King, T. H., editors, Proceedings ofLFG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S M Harabagiu</author>
</authors>
<title>High performance question/answering.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="2178" citStr="Pasca and Harabagiu, 2001" startWordPosition="311" endWordPosition="314">ic interpretation, in pursuit of broader goals of natural language understanding. Proponents of traditional ‘deep’ or ‘precise’ approaches to syntax, such as GB, CCG, HPSG, LFG, or TAG, have argued that sophisticated grammatical formalisms are essential to resolving various hidden relationships such as the source phrase of moved whphrases in questions and relativizations, or the controller of clauses without an overt subject. Knowledge of these hidden relationships is in turn essential to semantic interpretation of the kind practiced in the semantic parsing (Gildea and Jurafsky, 2002) and QA (Pasca and Harabagiu, 2001) literatures. However, work in statistical parsing has for the most part put these needs aside, being content to recover surface context-free (CF) phrase structure trees. This perhaps reflects the fact that context-free phrase structure grammar (CFG) is in some sense at the the heart of the majority of both formal and computational syntactic research. Although, upon introducing it, Chomsky (1956) rejected CFG as an adequate framework for natural language description, the majority of work in the last half century has used context-free structural descriptions and related methodologies in one for</context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>Pasca, M. and Harabagiu, S. M. (2001). High performance question/answering. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Plaehn</author>
</authors>
<title>Computing the most probable parse for a discontinuous phrase structure grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="35896" citStr="Plaehn (2000)" startWordPosition="5765" endWordPosition="5766">he first option is to postprocess CF parse trees, which we have closely investigated in this paper. The second is to incorporate nonlocal dependency information into the category structure of CF trees. This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003). The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. This approach was tentatively investigated by Plaehn (2000). As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches. 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three anonymous reviewers, and from presentation at the Institute of Cognitive Science, University of Colorado at Boulder. The authors are also grateful to Dan Klein and Jenny Finkel for use of maximum-entropy software they wrote. This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligenc</context>
</contexts>
<marker>Plaehn, 2000</marker>
<rawString>Plaehn, O. (2000). Computing the most probable parse for a discontinuous phrase structure grammar. In Proceedings of IWPT, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R M Kaplan</author>
<author>R S Crouch</author>
<author>J T Maxwell</author>
<author>M Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="5861" citStr="Riezler et al., 2002" startWordPosition="867" endWordPosition="870">r resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Riezler, S., King, T. H., Kaplan, R. M., Crouch, R. S., Maxwell, J. T., and Johnson, M. (2002). Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of ACL, pages 271– 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Skut</author>
<author>T Brants</author>
<author>B Krenn</author>
<author>H Uszkoreit</author>
</authors>
<title>Annotating unrestricted German text.</title>
<date>1997</date>
<booktitle>In Fachtagung der Sektion Computerlinguistik der Deutschen Gesellschaft fr Sprachwissenschaft,</booktitle>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="6294" citStr="Skut et al., 1997" startWordPosition="936" endWordPosition="939">ng that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testing. Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. Consistent with prior work and with common practice in statistical parsing, we stripped categories of all functiona</context>
<context position="9523" citStr="Skut et al. (1997" startWordPosition="1452" endWordPosition="1455">e wh- movement, topicalization, right extrapositions from NP, expletives, and scrambling of sub2Four of the annotation errors in WSJ lead to uninterpretable dislocation and sharing patterns, including failure to annotate dislocations corresponding to marked origin sites, and mislabelings of control loci as origin sites of dislocation that lead to cyclic dislocations (which are explicitly prohibited in WSJ annotation guidelines). We corrected these errors manually before model testing and training. 3For a detailed description of the algorithm for creating the context-free version of NEGRA, see Skut et al. (1997a). S NP-3 NNP Farmers VP S quick*ICH*-2 S-2 NP *-3 TO VP VP NP NN yesterday NP NP DT NN the problems to VB point PRT RP out 0 S NP PRP it VBZ sees VP NP *T*-1 SBAR WHNP-1 VBD ADJP was JJ . . “The RMV will not begin to be formed for a long time.” Figure 2: Nonlocal dependencies via right-extraposition (*T1*) and topicalization (*T2*) in the NEGRA corpus of German, before (top) and after (bottom) transformation to context-free form. Dashed lines show where nodes go as a result of remapping into context-free form. jects after other complements. The positioning of NEGRA’s “traces” inside the moth</context>
</contexts>
<marker>Skut, Brants, Krenn, Uszkoreit, 1997</marker>
<rawString>Skut, W., Brants, T., Krenn, B., and Uszkoreit, H. (1997a). Annotating unrestricted German text. In Fachtagung der Sektion Computerlinguistik der Deutschen Gesellschaft fr Sprachwissenschaft, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Skut</author>
<author>B Krenn</author>
<author>T Brants</author>
<author>H Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP.</booktitle>
<contexts>
<context position="6294" citStr="Skut et al., 1997" startWordPosition="936" endWordPosition="939">ng that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testing. Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. Consistent with prior work and with common practice in statistical parsing, we stripped categories of all functiona</context>
<context position="9523" citStr="Skut et al. (1997" startWordPosition="1452" endWordPosition="1455">e wh- movement, topicalization, right extrapositions from NP, expletives, and scrambling of sub2Four of the annotation errors in WSJ lead to uninterpretable dislocation and sharing patterns, including failure to annotate dislocations corresponding to marked origin sites, and mislabelings of control loci as origin sites of dislocation that lead to cyclic dislocations (which are explicitly prohibited in WSJ annotation guidelines). We corrected these errors manually before model testing and training. 3For a detailed description of the algorithm for creating the context-free version of NEGRA, see Skut et al. (1997a). S NP-3 NNP Farmers VP S quick*ICH*-2 S-2 NP *-3 TO VP VP NP NN yesterday NP NP DT NN the problems to VB point PRT RP out 0 S NP PRP it VBZ sees VP NP *T*-1 SBAR WHNP-1 VBD ADJP was JJ . . “The RMV will not begin to be formed for a long time.” Figure 2: Nonlocal dependencies via right-extraposition (*T1*) and topicalization (*T2*) in the NEGRA corpus of German, before (top) and after (bottom) transformation to context-free form. Dashed lines show where nodes go as a result of remapping into context-free form. jects after other complements. The positioning of NEGRA’s “traces” inside the moth</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Skut, W., Krenn, B., Brants, T., and Uszkoreit, H. (1997b). An annotation scheme for free word order languages. In Proceedings ofANLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>