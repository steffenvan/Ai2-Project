<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.80112118">
ACL 20 11 June 20
Portland, *regon � �
Web Search Queries as a Corpus
Tutorial at the 44th Annual Meeting of the Association for Computational Linguistics (ACL 2011)
Marius Pa§ca
Google Inc.
mars@google.com
*verview
. Part *ne: Introduction
. Part Two: Queries as a Corpus
. Part Three: Extraction from Queries
Part *ne: Introduction
. *pen-domain information extraction
. Instances, concepts, relations
Unweaving the World Wide Web of Facts
. The Web is a repository of implicitly-encoded human knowledge
- some text fragments contain easier-to-extract knowledge
. More knowledge leads to better answers
- acquire facts from a fraction of the knowledge on the Web
- exploit available facts during search
. *pen-domain information extraction
- extract knowledge (facts, relations) applicable to a wide range,
rather than closed, pre-defined set of domains (e.g., medical,
financial etc.)
- no need to specify set of concepts and relations of interest in
advance
- rely on as little manually-created input data as possible
Instances, Concepts and Relations
. A concept (class) is a placeholder for a set of instances
(objects) that share similar properties
- set of instances
. (matrix, kill bill, ice age, pulp fiction, inception, cidade de deus,...}
- class label
. movies, films
- definition
. a series of pictures projected on a screen in rapid succession with
objects shown in successive positions slightly changed so as to produce
the optical effect of a continuous picture in which the objects move
(Merriam Webster)
. a form of entertainment that enacts a story by sound and a sequence of
images giving the illusion of continuous movement (WordNet)
Instances, Concepts and Relations
. Relations are assertions linking two (binary relation) or more (n-
ary relation) concepts
- actors-act in-movies; cities-capital of-countries
. Facts are instantiations of relations, linking two or more
instances
- leonardo dicaprio-act in-inception; cairo-capital of-egypt
. Attributes correspond to facts capturing quantifiable
properties of a class or an instance
</note>
<bodyText confidence="0.8206235">
- actors --&gt; awards, birth date, height
- movies --&gt; producer, release date, budget
</bodyText>
<figure confidence="0.99366775">
can reduce
risk of
foods
diseases
good sources
of
is a
form of
worth millions
of
used in the
treatment of
decay
product of
chemical elements
currencies
currency
of
depletes the
body of
drugs
countries
brand name
of
</figure>
<bodyText confidence="0.6566045">
paxil, lipitor, ibuprofen,
prednisone, albuterol,
effexor, azithromycin,
fluconazole, advil,...
</bodyText>
<note confidence="0.944704333333333">
yellow fever, influenza,
bipolar disorder, rocky
mountain spotted fever,
anosmia, myxedema,...
*pen-Domain Information
potassium, magnesium,
gold, sulfur, palladium,
argon, carbon, borium,
ruthenium, zinc, lead,...
fish, turkey, rice, milk,
chicken, cheese, eggs,
corn, beans, wheat,
asparagus, grapes,...
australia, south korea,
kenya, greece, sudan,
portugal, argentina,
mexico, cuba, kuwait,...
euro, won, lire, pounds,
rand, us dollars, yen,
pesos, pesetas, kroner,
escudos, shillings,...
*pen-Domain Information
diseases
yellow fever, influenza,
bipolar disorder, rocky
mountain spotted fever,
anosmia, myxedema,...
</note>
<figure confidence="0.978940090909091">
treatment
symptoms
causes
incidence
diagnosis
size
color
taste
allergies
calories
foods
</figure>
<note confidence="0.65607075">
fish, turkey, rice, milk,
chicken, cheese, eggs,
corn, beans, wheat,
asparagus, grapes,...
</note>
<figure confidence="0.962572666666667">
mass
symbol
atomic number
electron configuration
lewis dot diagram
chemical elements
</figure>
<keyword confidence="0.5960205">
potassium, magnesium,
gold, sulfur, palladium,
argon, carbon, borium,
ruthenium, zinc, lead,...
</keyword>
<figure confidence="0.83156175">
currency converter
flag
climate
geography
currency
population density
currencies
euro, won, lire, pounds,
rand, us dollars, yen,
pesos, pesetas, kroner,
escudos, shillings,...
countries
denominations
country
symbol
exchange rate
</figure>
<note confidence="0.7889078">
australia, south korea,
kenya, greece, sudan,
portugal, argentina,
mexico, cuba, kuwait,...
drugs
</note>
<bodyText confidence="0.446084777777778">
paxil, lipitor, ibuprofen,
prednisone, albuterol,
effexor, azithromycin,
fluconazole, advil,...
side effects
dosage
price
generic equivalent
withdrawal symptoms
</bodyText>
<note confidence="0.4546816">
Terminology and Scope
. Terminology
- concept vs. class: used interchangeably
- instance vs. entity: used interchangeably
. Scope
</note>
<bodyText confidence="0.7143434">
- discussing methods using queries to extract open-domain
information
- not discussing methods using queries in other tasks such as Web
search in general (e.g., query suggestion, spelling correction,
improving search results)
</bodyText>
<subsectionHeader confidence="0.572398">
Sources of *pen-Domain Information
</subsectionHeader>
<bodyText confidence="0.224887">
. Human-compiled knowledge resources
- resources created by experts
- resources created collaboratively by non-experts
</bodyText>
<subsectionHeader confidence="0.354354">
. Sources of textual data
</subsectionHeader>
<bodyText confidence="0.5724925">
- text documents (unstructured or semi-structured text)
- (Web) search queries
</bodyText>
<subsectionHeader confidence="0.734324">
Expert Resources
</subsectionHeader>
<bodyText confidence="0.880620153846154">
. WordNet
- [Fel98]: C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press
1998.
- lexical database of English created by experts
- wide-coverage of upper-level conceptual hierarchies
- replicated or extended to other languages
.
Cyc
- [Len95]: D. Lenat. CYC: A Large-Scale Investment in Knowledge
Infrastructure. Communications of the ACM 1995.
- knowledge base of common-sense knowledge created by experts over 100+
person-years
- terms and assertions capturing ground assertions and (inference) rules
</bodyText>
<subsectionHeader confidence="0.385367">
Collaborative, Non-Expert Resources
</subsectionHeader>
<bodyText confidence="0.450846">
. Wikipedia
</bodyText>
<note confidence="0.5235664">
- [Rem02]: M. Remy. Wikipedia: The Free Encyclopedia. Journal of *nline Information
Review 2002.
- free online encyclopedia developed collaboratively by Web volunteers
- among top 20 most po ular Web sites (according to comScore: Top 50 US Web
Properties, Aug 2009f
</note>
<sectionHeader confidence="0.482771" genericHeader="abstract">
. DBpedia
</sectionHeader>
<bodyText confidence="0.8787212">
- [BLK+09] C.Bizer, J. Lehmann, G. Kobilarov, S. Auer et al. DBpedia - A Crystallization
Point for the Web of Data. Journal of Web Semantics 2009.
- community effort to convert Wikipedia articles into structured data
- manually-created ontology, mappings from subset of Wikipedia infoboxes to ontology,
mappings from Wikipedia articles fo WordNet concepts
</bodyText>
<sectionHeader confidence="0.470314" genericHeader="keywords">
. Freebase
</sectionHeader>
<bodyText confidence="0.500577">
- [BEP+08]: K. Bollacker, C. Evans, P. Paritosh et al. Freebase: A Collaboratively Created
Graph Database for Structuring Human Knowledge. SIGM*D-08.
- repository for storing structured data from Wikipedia and other sources, as well as
from user contributions
- collaboratively created, structured and maintained
. *pen Mind
</bodyText>
<note confidence="0.381573333333333">
- [SLM+02]: P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins and W. Zhu. *pen Mind Common
Sense: Knowledge Acquisition from the General Public. Lecture Notes In Computer
Science 2002.
</note>
<table confidence="0.741304970588235">
- collect common-sense knowledge from non-expert Web users
- unlike Cyc, collect and represent knowledge in natural language rather than through
formal assertions
Wikipedia infobox
Wikipedia article
Wikipedia
DBpedia entries
Wikipedia infobox Wikipedia infobox source code
DBpedia, Freebase
&lt;Sears_Tower, previous_building, World_Trade_Center&gt;
&lt;Sears_Tower, construction_period, 1970- 1973&gt;
���
Quantitative Comparison of
Human-Compiled Resources
. Wikipedia
- 3.5+ million articles in English
- articles also available in 200+ other languages
. D8pedia
- 2.5+ million instances, 250+ million relations
. Freebase
- 20+ million instances, 300+ million relations
. Cyc
- ResearchCyc: 300,000+ concepts and 3+ million assertions
- *penCyc 2.0: add mappings from Cyc concepts to Wikipedia articles
. *pen Mind
- 800,000+ facts in English
- facts also available in other languages
Sources of *pen-Domain Information
. Human-compiled knowledge resources
- resources created by experts
- resources created collaboratively by non-experts
. Sources of textual data
- text documents (unstructured or semi-structured text)
- (Web) search queries
</table>
<figure confidence="0.991789685714286">
SEEM
197
Use Off ce, observation, communication
Height
Anten nalSpi re 1,720 057 (007 ml
Root 1,451 01 (442 m)&amp;quot;,
Technical delaia
&apos;id. I 1.11. beett tired°. 0 Hhfllhhtoll E eet-hase
Semi-structured text
Semi-structured text
Documents
Semi-structured text
Preceded by —rad6 CAnter
Surpassed by Petrenas Julio Towers
Information
Location 233S, Wacker Drive
Chicago, Illinois 60696
M United States
Status Lompee
&apos;Floor count 103, I
Floor arca 4 E6 mil0003b ft (3 81 million stti
rentable)
413,064 m. (353,961 rn2 rentable)0€
Elevator count 194, wrth 16 double-decker
elavators. made by Westinghouse,
modernized by Schindler Group
Companies
Architect Skidmore. Owings and Merril
17121311=1
Unstructured text
0 Willis Tower - Wikiperliw the free encyclopedia -0010150 Firefox
Tielv
re.nori C ot SV Mises, secede ...Sears tower
aninle tocuntin vow vortco I I h6Gx
Willis Tower
</figure>
<bodyText confidence="0.957493157894737">
This article&apos;s introduction section may not adequately summarize its
contents_ To comply with Wkipedia&apos;s Mad section guidelines, please
consider expanding the lead to prowle an accessible ovennew of Mosaic.,
key points. f5000enda02009,
Willis Tower,▪ formerly named Sears Tower, Is 108-story I
1,453 feet 042 m) skyscraper in Chicago, Illinois At the time of
Iits completion in 195lit was the tallest building in the world,
surpassing tne worm rade Center towers in New York_ Currently,
Willis Tower is the tallest building in the Hated States and the
tallest
-
tallest freestanding structure in the world.
Although Sears&apos; naming rights expired in 2003, the building
continued to 00 called Sears Tower for several years However, in
March 2009 London-based insurance broker Willa Group Holdings,
LOG, agreed to lease a portion 01 010 building and as part of the
agreement obtained the building&apos;s naming rights On July 16, 2009,
at 10 00 am Central Time, the building was ofticially renamed Willis
Tower
</bodyText>
<figure confidence="0.998055921875">
t
A/ IICIPEDIA,
The One Aryclopediff
dgation
mem page
Conte.
Featured cont.
Pam art.
EiZIT -1
1:1
Coordinates•418701187 6.87/
Frst . I at We free enoolopetlia
tem lone,
Willis Tower
search
wom, I
eradiOn
AWN %Wow.
Conner, portal
Recent chene.
Corned Witereaw
Ocrerne Warw..
Documents
•
•
•
Georgia - Tbilisi
Germany - Berlin
Greece - Athens
Capital
Domestic Domestic Long
Short Name Name
France
French:
French
French: France Republique
Republic
franga,se
• Fame Islands - TOrshavn
• Finland - Helsinki
• France - Paris
Georgian:
1.5.5d0ecn33053eL
Georgian
Transliteration
Sakenvelo
Republic
Georgian] of
Georgia
Tbilisi
Georgian
rtitianTHItio
English English
Short Long
Name Name
Paris
Federal
Republic German
Germany
of Deutschfand
Germany
German:
Bundesrepublik Berlin
Deutschland
</figure>
<subsectionHeader confidence="0.254405">
Alternative to Documents
</subsectionHeader>
<bodyText confidence="0.899249923076923">
. Conventionally: data for textual information extraction is
available as (some sort of) a document collection
- documents capture knowledge, or assertions about the world
- assertions are often &amp;quot;hidden&amp;quot; in expository text
- the goal is to derive some of that knowledge from text
. Alternatively: textual information extraction may be pursued
even without a document collection
- to find new knowledge within a document collection, users formulate
their search queries based on the knowledge that they already
possess at the time of the search
--&gt; query logs collectively capture knowledge, through requests that may
be answered by knowledge asserted in document collections
Next Topic
</bodyText>
<listItem confidence="0.717333">
. Part *ne: Introduction
. Part Two: Queries as a Corpus
. Part Three: Extraction from Queries
</listItem>
<subsectionHeader confidence="0.76929">
Queries as a Corpus
</subsectionHeader>
<bodyText confidence="0.737572">
. Structure of queries
. Comparison with other textual sources
. Usage, demographics and privacy
</bodyText>
<subsectionHeader confidence="0.628953">
Structure of Queries
</subsectionHeader>
<bodyText confidence="0.993343304347826">
. [SW07]: S. Bergsma and Q. Wang. Learning Noun Phrase Query Segmentation.
EMNLP-07.
- identify segments of contiguous query tokens corresponding to semantic concepts, using
manually annotated queries as training data
. [TP08]: B. Tan and F. Peng. Unsupervised Query Segmentation Using Generative
Language Models and Wikipedia. WWW-08.
- identify segments of contiguous query tokens corresponding to semantic concepts, using
evidence from queries and from Wikipedia documents
. [BJR08]: C. Barr, R. Jones and M. Regelson. The Linguistic Structure of English
Web-Search Queries. EMNLP-08.
- identify structural characteristics of queries in the task of part of speech tagging
.
[ML09]: M. Manshadi and X. Li. Semantic Tagging of Web Search Queries. ACL-
IJCNLP-09.
- classify queries into domains, and identify query fragments corresponding to pre-
specified, per-domain schema of tags
. [GXC+09� J. Guo and G. Xu and X. Cheng and H. Li. Named Entity Recognition in
Query. S GIR-09.
- detect instances within queries, and classify instances into coarse-grained classes
. [Li10]: X. Li. Understanding the Semantic Structure of Noun Phrase Queries.
ACL-10.
- represent noun phrase queries as a combination of intent heads and intent modifiers,
and identify those components automatically
</bodyText>
<subsectionHeader confidence="0.708458">
Finding Structure in Queries
</subsectionHeader>
<bodyText confidence="0.711377">
. [BJR08]: C. Barr, R. Jones and M. Regelson. The Linguistic Structure of
English Web-Search Queries. EMNLP-08.
</bodyText>
<subsectionHeader confidence="0.505693">
Part-of-Speech Tags of Query Tokens
</subsectionHeader>
<bodyText confidence="0.806930071428572">
. Task
- investigate the task of part-of-speech (P*S) tagging when applied to queries
. Input data
- set of 3.2K (2.5K unique) Web search queries, after automatic spell checking
and tokenization
. Manual annotation of P*S tags of query tokens is unreliable
- inter-annotator agreement: 0.79 (token-level), 0.65 (query-level)
- main cause of annotation errors (70% of cases): actual query ambiguity (e.g.,
download may be a noun or a verb) rather than human annotation mistakes
. P*S tags have a different distribution in queries than in documents
- in documents (Brown corpus): -90 distinct tags, of which 15 for determiners,
and 35 for verbs
- in queries: -20 distinct tags are sufficient, of which 1 for determiners and 1
for verbs
</bodyText>
<table confidence="0.981437416666667">
Suggested Part-of-Speech Tags
Part-of-Speech Example Percentage of
Tag Token Query Tokens
proper noun texas 40.2%
common noun pictures 30.9%
adjective big 7. 1%
URI ebay.com 5.9%
preposition in 3.7%
unknown y 2.5%
verb get 2.4%
... ... ...
(Courtesy R. Jones)
</table>
<bodyText confidence="0.933295857142857">
. Nouns are predominant in queries
- most frequent tags in documents: 13% of tokens are common nouns
- most frequent tags in queries: 40% of tokens are proper nouns, 7 1% of
tokens are common nouns or proper nouns
. Verbs are infrequent in queries
- in documents: at least one verb in most sentences
- in queries: less than 3% of tokens
</bodyText>
<subsectionHeader confidence="0.600718">
Part-of-Speech Tagging Experiments
</subsectionHeader>
<bodyText confidence="0.99350737037037">
. Use of capitalization in queries is inconsistent
- 17% queries contain capitalization, of which 4% are all-caps
- when a query contains mixed capitalization, first-letter token capitalization
is indicative of an actual proper noun for 73% of cases
- other uses of capitalization in queries: acronyms, capitalization for first
token of query, first-letter capitalization for all tokens
--&gt; cannot rely on capitalization to identify proper nouns in queries
Experimental Setting
Per-Token Tagging
tagger that assigns most frequent tag (over 65.4%
separate training lexicon) of each token
tagger trained on annotated documents 48.2%
tagger trained on annotated queries 69.7%
tagger trained and evaluated on queries with 89.4%
perfect capitalization
tagger trained and evaluated on queries with 70.9%
automatically-induced capitalization
Comparison with *ther Textual Sources
. [CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie. A Statistical
Comparison of Tag and Query Logs. SIGIR-09.
- investigate similarity between vocabularies of tokens from search queries vs. tags
assigned by users to Web documents
. [GNL+ 10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang. A Comparative
Study of Bing Web N-gram Language Models for Web Search and Natural
Language Processing. SIGIR 20 10, Web N-gram Workshop.
- generate a repository of n-grams from Web data, including from queries, and evaluate
it in various text processing tasks
</bodyText>
<figure confidence="0.9137114">
Characteristics of Documents vs. Queries
Characteristic
Data Source
Document Sentences
ueries
Type of medium text text
Purpose convey info. request info.
Available context surrounding text self-contained
Average quality high (varies) low
Grammatical style natural language bag of keywords
Average length 25 words or more 2-3 words
Queries vs. *ther Textual Sources
. [CGC+09]: M. Carman, R. Gwadera, F. Crestani and M. Baillie. A
Statistical Comparison of Tag and Query Logs. SIGIR-09.
Queries vs. Tags
. Task
- investigate the similarity between query logs and user-generated tags
(entered by users to annotate documents)
. Input data
- from query logs containing click-through data, and from Delicious (social
bookmark) tags, select queries and tags associated with a set of 4K Web
documents
- each document clicked at least 50 times, and associated with a tag at least
20 times
- generate respective vocabularies (i.e., sets) of tokens for tags and queries,
after removing stop words and stemming all tokens with the Porter stemmer
Metric
Token *ccurrences Vocabulary Size
Queries
Tags
Queries
Tags
Mean 955.3 1105.8 17.6 139.6
Std deviation 6464.7 1533.4 12.8 137.7
Median 278.0 393.0 15.0 83.0
</figure>
<table confidence="0.896513817391305">
Query vs. Tag vs. Document Vocabulary
. Include vocabulary of Web documents in comparison of relative overlap
. Similarity between query
and document vocabulary
is higher than between
query and tag vocabulary
- since documents are
clicked search results,
they are likely to contain
query tokens
. Similarity is lowest
between tag and
document vocabulary
- users do not necessarily
enter tags that appear in
document content
(Courtesy M. Carman)
. Compute overlap between query tokens and tag tokens
. *ptionally, remove low frequency tokens or keep high frequency tokens
Query vs. Tag Vocabulary
. *ver more than half of
documents, overlap &gt;- 0.5
--&gt; query vocabulary is very
similar to tag vocabulary
(Courtesy M. Carman)
Repositories of Distilled Query Data
. [GNL+ 10]: J. Gao, P. Nguyen, X. Li, C. Thrasher, M. Li and K. Wang. A
Comparative Study of 6ing Web N-gram Language Models for Web
Search and Natural Language Processing. SIGIR 20 10, Web N-gram
Workshop.
Web N-Gram Collection
. Language models of n-grams, from Web documents and search queries
N-gram Length
Documents
Queries
6o
Anchor Text
Title
1-grams 1.26 60.3M 150M 25 1.5M
2-grams 11.76 464. 1M 1. 16 1.36
3-grams 60.06 1.46 3. 16 3. 16
4-grams 148.56 2.36 5. 16 4.66
5-grams 230.06 N/A N/A N/A
. Language models found to be more similar between queries and
document title (and queries and document anchor text) than between
queries and document body
Queries as a Corpus
. Structure of queries
. Comparison with other textual sources
. Usage, demographics and privacy
Usage, Demographics and Privacy
. VAC08]: Q. Mei and K. Church. Entropy of Search Logs: How Hard is Search? With
Personalization?With Backoff? WSDM-08.
- investigate Web search from the perspective of entropy in search logs, and assess the impact of
aggregated data about users (e.g., from IP addresses) on the outcome of Web search
. [JBS08]: B. Jansen and D. Booth and A. Spink. Determining the Informational, Navigational,
and Transactional Intent of Web Queries. Journal of Information Processing and
Management 2008.
- investigate the distribution of queries from the point of view of intent type (and subtypes), and
automatically classify queries accordingly
. [JBS09]: B. Jansen, D. Booth and A. Spink. Patterns of Query Reformulation During Web
Searching. Journal of the American Society for Information Science and Technology 2009.
7 develop models to classify various types of query reformulations and identify the most frequent ones
among Web users
. [WC 10]: Ingmar Weber and Carlos Castillo. The Demographics of Web Search. Sigir-10.
- study the impact of various user demographics factors on the users� choice of queries
. [JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins. &amp;quot;I Know What You did Last Summer&amp;quot;:
Query Logs and User Privacy. CIKM-07.
- study the possibility of uncovering user identity from query logs, despite attempts to remove basic
personally identifiable information from queries
. [GBG+ 10]: S. Goel, A. Broder, E. Gabrilovich and B. Pang. Anatomy of the Long Tail: *rdinary
People with Extraordinary Tastes. WSDM-10.
. [KKM+09]: A. Korolova, K. Kenthapadi, N. Mishra and A. Ntoulas. Releasing Search Queries
and Clicks Privately. WWW-09.
- investigate methods to generate modified query log data that preserves user privacy
. For: ash cloud
Geographical Distribution
(Google Zeitgeist)
Query Usage
. Search zeitgeist
- capture &amp;quot;the general intellectual, moral, and cultural climate of an era&amp;quot;
(Merriam Webster), as reflected in the aggregation of search queries
submitted by Web users
Top Rising Queries (20 10)
Entertainment Consumer
Electronics
Top Global
Events (20 10)
world cup justin bieber ipad
olympics shakira iphone 4
haiti earthquake eminem nokia 5530
oil spill netflix htc evo 4g
ash cloud youtube videos nokia n900
(Google Zeitgeist)
. For: circuit city
Temporal Distribution
(Google Trends)
More queries
submitted later
during the year(s)
(shopping season)
More queries
submitted, due to
unusual event with
high news coverage
Query Demographics
. [WC 10]: Ingmar Weber and Carlos Castillo. The Demographics of Web
Search. Sigir-10.
(Courtesy I. Weber)
. Task
- investigate impact of user demographics on Web search
. Input data
- user profile data (birth year, gender, zip code)
- set of pairs of (query, clicked URL) from query logs
- census demographic data for various zip codes
</table>
<figure confidence="0.92373616">
Per-capita income ($k)
chris jordan
electric candle warmer
www.popsugar.com
ns4w.org
Below poverty (%)
BA degree (%)
www.unitnet.com
slaker
kipasa
www.tokbox.com
spencer stuart executive search
insight venture partners
federal circuit
four seasons jackson hole
Query Demographics
Feature
20%
40%
Query Log Data
60%
80%
Avg.
US
Avg.
</figure>
<table confidence="0.9501463">
Per-capita income ($k) 16.0 18.9 22.4 27.7 22.7 2 1.6
Below poverty (%) 4.5 7.2 10.9 16.5 11.1 12.4
BA degree (%) 12.8 18.1 25.6 37.6 25.5 24.4
White (%) 6 1.9 78.8 88.1 94.4 76.9 75.1
Afric. Amer. (%) 0.9 2.4 5.7 15.5 4.0 12.3
Asian (%) 0.4 1.1 2.3 5.1 4.0 3.6
Non-English (%) 4.5 7.9 14.0 27.3 17.3 17.9
Year of birth 1956 1966 1974 1982 1968 1974
Role of Demographics in Web Search
. Highly-discriminant queries for various user demographics
Feature Query
(Courtesy I. Weber)
pulloff.com
central boiler wood furnace
firewood processors
midwest super cub
White (%)
Afric. Amer. (%)
Asian (%)
trey songz bio
def jam records address
s2s magazine
madinaonline
sina
big bang lyrics
tvb series
jay chou lyrics
www.johnshopkinshealthalerts.com
www.envisionreports.com/vz
yahoo free bridge games
bnymellon.mobular.net/bnymellon/frp
Year of birth, old
Year of birth, young
free teen chatrooms
wet seal
tottaly layouts
photofiltre brushes
Role of Demographics in Web Search
. Highly-discriminant queries for various user demographics
Feature Query
Role of Demographics in Web Search
. Highly-discriminant queries for various user demographics
Feature Query
Queries and User Privacy
. [JKP+07]: R. Jones, R. Kumar, B. Pang and A. Tomkins. &amp;quot;I Know What You
did Last Summer&amp;quot;: Query Logs and User Privacy. CIKM-07.
Queries and User Privacy
. Task
- investigate the vulnerability of narrowing down the identify (demographics)
of users submitting search queries, even after removal of personally
identifiable information (names, numbers) from query logs
. Input data
- from user profile data (anonymized id, birth year, gender, zip code), select
100M profiles
- fromquery logs, select query sessions issued by users with available profile
data, for 744K users
. Assessment of vulnerability
- arrange data into buckets by age, gender, zip code
- arrange buckets into bins, by conjunctions of age, gender, zip code
- smaller bin size makes it easier to identify a particular user from the bin
(especially when additional information, e.g., hobbies, is available about the
user)
- e.g., if input data is arranged into bins that share gender bucket, age bucket,
and first 3 of 5 zi code digits e.g.�, males, ape 25-29, living in zip code
950xx) --&gt; almost 300K of the �44K users fit into a bin of I00 users or less
Deriving Demographics from Queries
. Identifying user gender and age
- classifiers using bag-of-words features
- gender identification: accuracy of 83.8%
. examples of discriminative features: (bridal, makeup, hair, women&apos;s,..} for women;
(nfl, poker, male, compusa,..} for men
- age identification: absolute error of 7 years (predicted vs. actual), better
than always guessing the middle age point
. examples of discriminative features: (myspace, pregnancy, wikipedia, mall,..} for
lower age; (aarp, lottery, amazon.com, senior, repair,..} for higher age
- if personally identifiable information (names and numbers) are removed from
queries, both gender and age classification remain about as accurate
. Identifying location (zip code)
- existing classifier for locations: given query as input, output list of locations
- convert list of locations into zip code buckets of known first 3, 4 or 5 digits
</table>
<figure confidence="0.978167833333333">
First 5
First 4
First 3
Known Digits of Zip Code
Correct at top one 6.2% 3.7% 34.9%
Correct among top three 3. 1% 25 1.% 54. 1%
- if personally identifiable information (names and numbers) are removed from
queries, location classification becomes much less accurate
Deriving Queries from Known Information
. Identifying query sessions submitted by a known user
- use demographics, conversations with, lifestyle changes of user, in order to
guess queries that may have been submitted by user
- as an approximation, manually create a set of guessed queries
Category
Cars
Sports
Food
Books
Common
volkswagen beetle (478)
honda odyssey ( 1504)
toyota prius ( 1070)
skiing (96 18)
football ( 123802)
pizza ( 104888)
italian restaurant (4998)
brie (39325)
harry potter (27838)
danielle steele (238)
freakonomics (574)
Rare
triumph tr23 (23)
e-type jaguar (5)
bassmaster (388)
skulling ( 17)
assam (747)
holly lisle (20)
elizabeth moon (27)
Knowing that a
user submitted
the query e-
type jaguar
narrows down
the identity of
the user to a
bin of 5
possible users
- use combinations of guessed queries (Courtesy R. Jones)
</figure>
<table confidence="0.889605529411765">
Deriving Queries from Known Information
Query Combination Bin Size
harry potter, pizza 4855
football, skiing 2430
italian restaurant, pizza 1441
harry potter, volkswagen beetle 27
��� ���
pizza, triumph tr3 2
brie, holly lisle, pizza 1
danielle steele, volkswagen beetle 1
--&gt; even if individual bits of information are far from unique among users,
putting them together can uniquely identify a user
Next Topic
. Part *ne: Introduction
. Part Two: Queries as a Corpus
. Part Three: Extraction from Queries
Extraction Methods
</table>
<bodyText confidence="0.501713">
. Methods for extraction of:
- instances and concepts
- attributes and relations
</bodyText>
<sectionHeader confidence="0.259184" genericHeader="introduction">
Instances and Concepts
</sectionHeader>
<bodyText confidence="0.868153653846154">
chemical elements
potassium, magnesium,
gold, sulfur, palladium,
argon, carbon, borium,
ruthenium, zinc, lead,...
currencies
euro, won, lire, pounds,
rand, us dollars, yen,
pesos, pesetas, kroner,
escudos, shillings,...
fish, turkey, rice, milk,
chicken, cheese, eggs,
corn, beans, wheat,
asparagus, grapes,...
yellow fever, influenza,
bipolar disorder, rocky
mountain spotted fever,
anosmia, myxedema,...
paxil, lipitor, ibuprofen,
prednisone, albuterol,
effexor, azithromycin,
fluconazole, advil,...
australia, south korea,
kenya, greece, sudan,
portugal, argentina,
mexico, cuba, kuwait,...
</bodyText>
<figure confidence="0.69766275">
foods
diseases
drugs
countries
</figure>
<sectionHeader confidence="0.627728" genericHeader="method">
Instances and Concepts
</sectionHeader>
<bodyText confidence="0.934935444444445">
. [Pas07]: M. Pa�ca. Weakly-Supervised Discovery of Named Entities using Web
Search Queries. CIKM-07.
- expand sets of instances using Web search queries
. [VP08]: B. Van Durme and M. Pagca. Finding Cars, Goddesses and Enzymes:
Parametrizable Acquisition of Labeled Ins lances for *pen-Domain Information
Extraction. AAAI-08.
- extract labeled sets of instances from Web documents, by merging clusters of
distributionally similar phrases with IsA pairs extracted with lexico-syntactic patterns
. [PP09]: M. Pennacchiotti and P. Pantel. Entity Extraction via Ensemble Semantics.
EMNLP-09.
- expand sets of instances using multiple sources of text including queries
. [AHH09]: E. Alfonseca and K. Hall and S. Hartmann. Large-Scale Computation of
Distributional Similarities for Queries. NAACL-HLT-2009.
- apply vector-space model of distributional similarities to queries rather than documents
. EJP 10]: A. Jain and P. Pantel. *pen Entity Extraction from Web Search Query
Logs. C*LING- 10.
- extract clusters of distributionally similar phrases from Web search queries and click-
through data
</bodyText>
<sectionHeader confidence="0.754682" genericHeader="method">
Instances and Concepts
</sectionHeader>
<table confidence="0.986012083333333">
. [VP08]: B. Van Durme and M. Pa�ca. Finding Cars, Goddesses and
Enzymes: Parametrizable Acquisition of Labeled Instances for *pen-
Domain Information Extraction. AAAI-08.
Extraction from Documents and Queries
. Input
- target relation, available as a small set of extraction patterns
. e.g., &lt;C [such asIincluding] I&gt;
. Data sources
- collection of Web documents
- collection of anonymized Web search queries
. *utput
- sets of instances, each set associated with a class label
. e.g., marine animals= (whales, seals, dolphins, turtles, sea lions, fishes,
penguins, squids, pacific walrus, aquatic birds, comb jellies, starfish,
florida manatees, walruses,...)
- each set also associated with lists of attributes
Acquisition of *pen-Domain Classes
. Define a closed vocabulary of potential class instances, as the
set of most frequently-submitted Web search queries
- textual data source: Web query logs
- output: noisy set of potential class instances
. Acquire class labels for potential class instances, via hand-
written extraction patterns
- textual data source: Web documents
- &lt;C [such aslincluding] I&gt;, where Cis a potential class label (e.g.,
zoonotic diseases) and I is a potential instance (e.g., brucellosis)
- output: noisy pairs of an instance and a class label
. *rganize potential class instances into sets of distributionally
similar phrases
- output: noisy sets of distributionally similar instances
Merge into labeled sets of instances
Extraction of Labeled Instances
Input: - pairs of an instance and a class label
- unlabeled sets of distributionally similar instances
*utput: - sets of instances, each set associated with a class label
For each unlabeled set of distributionally-similar instances 5
For each class label L assigned to some instance(s) of set 5
A=set of instances of 5 whose class label is L
B=set of sets that contain some instance(s) whose label is L
If JAI &gt; JxI5I:
If IBI &lt; K:
Collect instances of A, associated with the class label L
. Note: J, K are weighting parameters controlling precision/recall
- J in [0, 1); higher J --&gt; higher precision
- K is non-negative integer; lower K --&gt; higher precision
tf
idf
Patterns and Distributional 5imilarities
</table>
<figure confidence="0.988950064516129">
(Courtesy B. Van Durme)
hillary clinton
abe lincoln
paul revere
Presidents
gm
volvo
carrot
mango
apple
ford
banana
orange
Fruits
rose
Car Companies
j. carter
bill clinton
nixon
george w. bush
schwinn
toyota
ronald reagan
al sharpton
lettuce
broccoli
corn
benjamin franklin
jefferson
john adams
george washington
</figure>
<sectionHeader confidence="0.286017" genericHeader="method">
Instances and Concepts
</sectionHeader>
<table confidence="0.836376420212766">
. [PP09]: M. Pennacchiotti and P. Pantel. Entity Extraction via Ensemble
Semantics. EMNLP-09.
Extraction from Multiple Sources
. Input
- target classes, available as small sets of seed instances
. e.g., (jodie foster, humphrey bogart, anthony hopkins} for Actor
- target classes, also available as small sets of seed relations with other
classes
. e.g., &lt; leonardo dicaprio, inception&gt;, &lt;nicole kidman, eyes wide shut&gt; for Actor
(corresponding to relation Actor-act in-Movie)
. Data sources
- collection of Web documents
- collection of Web search queries
- HTML tables identified within the collection of Web documents
- collection of articles from Wikipedia
. *utput
- ranked lists of instances, one per class
. e.g., [gordon tootoosis, rosalind chao, john hawkes, jeffrey dean morgan,...] for
Actor
Extraction Components
. 5ources (5 1, 52,..., 5k)
- data sources from which instances and their relevant features are
extracted
. Knowledge extractors (KE 1, KE2,..., KEn)
- extract candidate instances from sources, using various algorithms
. Feature generators (FG 1, FG2,..., FGm)
- collect evidence/features relevant to deciding whether candidate
instances are correct or not
. Aggregator
- combine evidence available from multiple sources for candidate
instances
. Ranker
- rank candidate instances extracted by knowledge extractors, based
on features available from feature generators
FEATURE GENERATORS
S,
FGA
FGz
FGm
RANKER
Sz
MODELER
KB
DECODER
SK
KNOWLEDGE EXTRACTORS
AGGREGATOR
KE„
KEZ
KE1
(Courtesy P. Pantel, M. Pennacchiotti)
Ensemble 5emantics
Extraction Results
. Input data = collection of 600 million Web documents; tables identified
within the documents; one year of queries; 2 million Wikipedia articles
. Evaluate lists of instances extracted for 3 classes: Actor, Athlete and
Musician
- create gold standard from samples of 500 instances selected randomly for
each class
- compute precision of extracted lists of instances, relative to and over the
gold standards
. Average precision: 0.860 (Actor), 0.9 15 (Athlete), 0.788 (Musician)
. Precision@100: 0.99 (Athlete)
. Estimated precision@22000: 0.97 (Athlete)
. Collected by feature generators
- 4 feature families: from Web documents, queries, tables, Wikipedia
- 5 feature types: frequency, co-occurrence, distributional, pattern,
termness (i.e., checking whether extracted terms are well-formed)
(Courtesy P. Pantel, M. Pennacchiotti)
Ranking Features
Instances and Concepts
. [JP 10]: A. Jain and P. Pantel. *pen Entity Extraction from Web Search
Query Logs. C*LING- 10.
Extraction from Queries
. Data sources
- anonymized search queries along with frequencies and click-through data
(clicked search results)
- Web documents
. *utput
- clusters of similar instances
. e.g., (basic algebra, numerical analysis, discrete math, lattice theory, nonlinear
physics, ..j, (aaa insurance, roadside assistance, personal liability insurance,
international driving permits, ..j
|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_no bi_xmlSFBIA_new bi_xmlPara_continue
. Steps
- collect set of candidate instances from queries
- cluster instances using context in queries or click-through data or both
Extraction of Instances
. Identify candidate instances
- intuition: in queries composed by copying fragments from Web documents
and pasting them into queries, capitalization of instances is preserved
- from queries containing capitalization, extract contiguous sequences of
capitalized tokens as instances
Queries Candidate Instances
Britney Spears new song --&gt; Britney Spears
travel to Italy Roma --&gt; Italy Roma
restaurant Cascal in Mountain View --&gt; Cascal, Mountain View
. Retain set of best candidate instances
- first criterion: promote candidate instances whose capitalization is frequent
in Web documents
- second criterion: promote candidate instances that occur as full-length
queries
- retain set of candidate instances that score highly (above some thresholds)
according to both criteria
(Courtesy A. Jain)
britney spears
south america cruise
serena williams
bruce springsteen
Other celebrities
Other singers
Other regions
kauai snorkeling
paris hilton
celine dion
guinea
. Contextual space of Web documents
- an instance is represented by the contexts in which it appears in text
documents
- instances are modeled &amp;quot;objectively&apos;, according to descriptions of the world
. Contextual space of Web search queries
- an instance is represented by the contexts in which it appears in a search
queries
- instances are modeled &amp;quot;subjectively&apos;, according to users&apos; perception of the
world
Contextual space of Web documents Contextual space of Web search queries
Similarity in Documents vs. Queries
galapagos islands
tasmania
britney spears galapagos islands
Other island travel topics
Clustering of Instances
. Induce unlabeled classes of instances, by clustering instances using
features collected from queries
- as an alternative to collecting features from unstructured text in documents
- for efficiency, no attempt to parse the queries
. Context features
- vector of elements corresponding to contexts, where a context is the prefix
and postfix around the instance, from queries containing the instance
. Click-through features
- vector of elements corresponding to documents, where a document is one
that is clicked by a user submitting the instance as a full-length query
. Hybrid features
- normalized combination of context and click-through vectors
Impact of Clustering Features
. Given an instance, manually
judge each co-clustered
instance:
- &amp;quot;If you were interested in
instance I, would you also
be interested in instance Ic
in any intent?&amp;quot;
- also, annotate with type of
relation between instance
and co-clustered instance
. Compute precision, over a
set of evaluation instances
- CL-CTX: context
- CL-CLK: click-through
- CL-HYB: hybrid
- CL-Web: context collected
from Web documents
rather than queries
Precision
CL-Web 0.73
CL-CTX 0.46
CL-CLK 0.81
CL-HYB 0.85
Method
Relation
Type
- e - - -
topic 0.27 0.46 0.46 0.40
sibling 0.72 0.43 0.29 0.32
parent - 0.09 0. 13 0.09
child 0.01 - 0.01 0.02
synonym 0.01 0.03 0. 12 0. 16
Method
Extraction Methods
. Methods for extraction of:
- instances and concepts
- attributes and relations
Attributes and Relations
diseases
yellow fever, influenza,
bipolar disorder, rocky
mountain spotted fever,
anosmia, myxedema,...
</table>
<figure confidence="0.912471195121951">
treatment
symptoms
causes
incidence
diagnosis
size
color
taste
allergies
calories
foods
fish, turkey, rice, milk,
chicken, cheese, eggs,
corn, beans, wheat,
asparagus, grapes,...
mass
symbol
atomic number
electron configuration
lewis dot diagram
chemical elements
potassium, magnesium,
gold, sulfur, palladium,
argon, carbon, borium,
ruthenium, zinc, lead,...
currency converter
currencies
euro, won, lire, pounds,
rand, us dollars, yen,
pesos, pesetas, kroner,
escudos, shillings,...
denominations
country
symbol
exchange rate
flag
climate
geography
currency
population density
countries
</figure>
<bodyText confidence="0.787556333333333">
australia, south korea,
kenya, greece, sudan,
portugal, argentina,
mexico, cuba, kuwait,...
drugs
paxil, lipitor, ibuprofen,
prednisone, albuterol,
effexor, azithromycin,
fluconazole, advil,...
</bodyText>
<figure confidence="0.915949855072464">
side effects
dosage
price
generic equivalent
withdrawal symptoms
countries
australia, south korea,
kenya, greece, sudan,
Attributes and Relations
[PV07]: M.
and B. Van Durme. What You Seek is What You Get: Extraction
of Class Attributes from Query Logs. IJCAI-07.
- apply small set of patterns to extract attributes from queries
4VG07]: M. Pa4ca, B. Van Durme and N. Garera. The Role of Documents vs.
i ueries n Extracting Class Attributes from Text. CIKM-07.
- apply patterns to extract attributes from unstructured text in documents vs. queries
M.
and Searching the World Wide Web of Facts -
Step
Harnessing the Wisdom of the Crowds. WWW-07.
- expand sets of seed attributes using queries
[LWA09]: X. Li, Y. Wang and A. Acero. Extracting Structured Information from
User Queries with Semi-Supervised Conditional Random Fields. SIGIR-09.
- detect relevant fields in product-search queries, using click data and document content
[PER+10]: M. Pa4ca, E.
E. Robledo-Arnuncio, R. Martin-Brualla and K.
The Role of Query Sessions in Extracting Instance Attributes from Web
Search Queries.
- extract attributes of instances, from sequences of queries within query sessions
[YTT10]: X. Yin W. Tan and Y. Tu. Automatic Extraction of Clickable Structured
Web Contents
.
Pa4ca
.
.[Pas07]:
Pa4ca.*rganizing
Two:
.
.
Alfonseca,
Hall.
ECIR-10.
.
or Name Entity Queries. WWW- 10.
- given a query containin an instance, extract structured data from click data and
contents of subsequently visited documents
. [SJY 11]: A. Das Sarma, A. Jain and C. Yu. Dynamic Relationship and Event
Discovery. WSDM-11.
- acquire temporal) -anchored relations that apply within a given set of instances, using
queries and (news documents
can reduce
risk of
diseases
good sources
of
is a
form of
used in the
treatment of
decay
product of
chemical elements
currency
of
depletes the
body of
drugs
brand name
of
</figure>
<bodyText confidence="0.988292703703704">
paxil, lipitor, ibuprofen,
prednisone, albuterol,
effexor, azithromycin,
fluconazole, advil,...
yellow fever, influenza,
bipolar disorder, rocky
mountain spotted fever,
anosmia, myxedema,...
Attributes and Relations
potassium, magnesium,
gold, sulfur, palladium,
argon, carbon, borium,
ruthenium, zinc, lead,...
foods
fish, turkey, rice, milk,
chicken, cheese, eggs,
corn, beans, wheat,
asparagus, grapes,...
worth millions
of
currencies
portugal,argentina,
mexico, cuba, kuwait,...
euro, won, lire, pounds,
rand, us dollars, yen,
pesos, pesetas, kroner,
escudos, shillings,...
</bodyText>
<sectionHeader confidence="0.965276" genericHeader="method">
Attributes and Relations
</sectionHeader>
<reference confidence="0.8387501">
. [Pas07]: M. Pa�ca. *rganizing and Searching the World Wide Web of
Facts - Step Two: Harnessing the Wisdom of the Crowds. WWW-07.
Extraction from Queries
. Input
- target classes, available as sets of representative instances
. e.g., {Delphi, Apple Computer, Honda, *racle, Coca Cola, Toyota, Washington
Mutual, Delta, Reuters, Target, ..jfor Company
- small sets of seed attributes, one per class
. e.g., {headquarters, stock price, ceo, location, chairman} for Company
. Data source
</reference>
<bodyText confidence="0.9485832">
- anonymized search queries along with frequencies
. *utput
- ranked (longer) lists of attributes, one per class
. e.g., {headquarters, mission statement, stock price, ceo, code of conduct, stock
symbol, organizational structure, corporate address, cio, ...} for Company
</bodyText>
<sectionHeader confidence="0.820953" genericHeader="method">
. Steps
</sectionHeader>
<bodyText confidence="0.987894">
- select candidate attributes, from queries containing an instance
- create internal representation of candidate attributes, from queries
containing an instance and a candidate attribute
- rank candidate attributes, from similarity between internal representation
of a candidate attribute and combined internal representation of all seed
attributes
</bodyText>
<figure confidence="0.986796907692308">
Class Attribute Extraction
Target classes
Company: {Delphi, Apple Computer, Honda, Oracle, Coca Cola,
Toyota, Washington Mutual, Delta, Reuters, Target,...}
Seed attributes
Company: {headquarters, stock price, ceo, location, chairman}
Search-signature vectors (one per candidate attribute)
Pool of candidate attributes
Company: {installing, stock price, accord,
headquarters, mission statement,...}
Query logs
Reference search-signature vectors (one per class)
Company
Ranked list of extracted class attributes
Company: {headquarters, mission statement, stock price, ceo,
code of conduct, stock symbol, organizational
structure, corporate address, cio,...}
Company: installing
[ ] [ ] [cressida water pump]
prefix infix postfix
[ ] [ ] [8.1-7 on solaris 8]
prefix infix postfix
[new] [ ] [ ]
prefix infix postfix
Company: accord
[ ] [ ] [1989 sei]
prefix infix postfix
Company: stock price
[ ] [company one year] [target]
prefix infix postfix
[ ] [air lines] [history]
prefix infix postfix
Company: headquarters
[where is the world] [for] [corporation]
prefix infix postfix
[ ] [new] [impact]
prefix infix postfix
Company: mission statement
[ ] [for the] [corporation]
prefix infix postfix
[ ] [for] [airlines]
prefix infix postfix
Top Extracted Attributes
Class
Top Extracted Attributes
1 Actor awards, height, age, date of birth, weight, b** ****, birthdate,
birthplace, cause of death, real name
2 AircraftModel weight, length, history, fuel consumption, interior photos,
specifications, photographs, interior pictures, seating
arrangement, flight deck
3 Award recipients, date, winners list, result, gossip, printable ballot,
nominees, winners, location, announcements
4 BasicFood calories, color, size, allergies, taste, carbs, nutritional
information, nutrition facts, nutritional value, nutrition
5 CarModel transmission, top speed, acceleration, transmission problems,
owners manual, gas mileage, towing capacity, stalling, maintenance
schedule, performance parts
6 CartoonChar costume, voice, creator, first appearance, funny pictures, origins,
cartoon images, cartoon pics, color pages
7 CellPhoneModel features, battery life, retail price, mobile review, specification,
price list, functions, ratings, tips, tricks
��� ��� ���
Top Extracted Attributes
Class
... ... ...
</figure>
<table confidence="0.74737225">
34 Stadium location, seating capacity, architect, address, seating map,
dimensions, tours, pics, poster, box office
35 TerroristGroup attacks, leader, goals, meaning, website, leadership, photos,
images, definition, flag
36 Treaty countries, ratification, date, definition, summary, purpose, pros,
cons, members, picture
37 University alumni, mascot, dean, economics department, career center,
graduation 2005, department of psychology, school colors, tuition
costs, campus map
38 VideoGame price, system requirements, creator, official site, official
website, free game download, concept art, download demo, pc
cheat codes, reviews
39 Wine vintage, color, cost, style, taste, vintage chart, pronunciation,
shelf life, wine ratings, wine reviews
40 WorldWarBattle date, location, significance, images, importance, timeline,
summary, pics, maps, photographs
Top Extracted Attributes
. Input data = 50 million anonymized queries
. Evaluate attributes extracted with hand-written patterns vs. based on
seeds
1 Actor 0.85 1.00 0.82 1.00 0.74 0.96
2 AircraftModel 0.80 0.80 0.77 0.85 0.68 0.71
3 Award 0.30 0.95 0. 15 0.77 0.24 0.69
4 BasicFood 1.00 1.00 0.90 0.95 0.65 0.86
... ... ... ... ... ... ... ...
37 University 0.90 0.85 0.82 0.85 0.65 0.74
38 VideoGame 0.70 0.90 0.57 0.90 0.44 0.90
39 Wine 0.40 1.00 0.42 0.87 0.29 0.57
40 WorldWarBattle 0.00 0.85 0.00 0.82 0.00 0.66
Average (40 Classes)
Extraction Results
Class Precision
0.72 0.90 0.64 0.85 0.53 0.76
Patt Seed Patt Seed Patt Seed
@ 10
@20 @50
</table>
<sectionHeader confidence="0.783492" genericHeader="conclusions">
Summary
</sectionHeader>
<bodyText confidence="0.954493538461539">
. Do ask, do tell
- if knowledge is prominent, someone will eventually write about it
- if knowledge is prominent, someone will eventually ask about it
- Web search queries are cursory reflections of knowledge encoded
deeply within unstructured and structured content available in
documents
. Queries are useful in open-domain information extraction
- each user searches for something; collectively, all users search for
many (most?) things
- queries often reflect the relative popularity of people, topics,
events etc.
--&gt; useful in the extraction and ranking of instances, classes and
relations
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.733908">
<note confidence="0.95023975">ACL 20 11 June 20 Portland, *regon � � Web Search Queries as a Corpus Tutorial at the 44th Annual Meeting of the Association for Computational Linguistics (ACL 2011)</note>
<intro confidence="0.820386">verview</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>M Pa�ca</author>
</authors>
<title>rganizing and Searching the World Wide Web of Facts - Step Two: Harnessing the Wisdom of the Crowds.</title>
<pages>07</pages>
<marker>Pa�ca, </marker>
<rawString>. [Pas07]: M. Pa�ca. *rganizing and Searching the World Wide Web of Facts - Step Two: Harnessing the Wisdom of the Crowds. WWW-07.</rawString>
</citation>
<citation valid="false">
<title>Extraction from Queries . Input - target classes, available as sets of representative instances</title>
<marker></marker>
<rawString>Extraction from Queries . Input - target classes, available as sets of representative instances</rawString>
</citation>
<citation valid="false">
<authors>
<author>e g Delphi</author>
</authors>
<title>Apple Computer,</title>
<location>Honda, *racle, Coca Cola, Toyota, Washington Mutual, Delta, Reuters, Target, ..jfor</location>
<marker>Delphi, </marker>
<rawString>. e.g., {Delphi, Apple Computer, Honda, *racle, Coca Cola, Toyota, Washington Mutual, Delta, Reuters, Target, ..jfor Company - small sets of seed attributes, one per class . e.g., {headquarters, stock price, ceo, location, chairman} for Company . Data source</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>