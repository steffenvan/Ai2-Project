<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.827577" genericHeader="method">
ABSTRACTS OF CURRENT LITERATURE
</sectionHeader>
<subsectionHeader confidence="0.830365">
Articles, Word Order, and Resource
Control Hypothesis
</subsectionHeader>
<author confidence="0.269735">
Janusz S. Bien
</author>
<affiliation confidence="0.17436">
Warsaw
</affiliation>
<note confidence="0.882918333333333">
In Mey, Jacob L., Ed., Language and
Discourse: Test and Protest, A Festschrift for
Petr Sgall. (Vol. 19, Linguistic and Literary
Studies in Eastern Europe.) John Benjamins
Publishing Company, Amsterdam/
Philadelphia, 1986.
</note>
<bodyText confidence="0.995127">
The paper elaborates the ideas presented in Bien (1983). The definite and
indefinite distinction is viewed as a manifestation of the variable depth of
nominal phrase processings: indefinite phrases are represented by frame
pointers, while definite ones by frame instances incorporating information
found by memory search. In general, the depth of processing is deter-
mined by the availability of resources. Different word orders cause differ-
ent distributions of the parser&apos;s processing load and therefore influence
also the depth of processing. Articles and word order appear to be only
some of several resource control devices available in natural languages.
For copies of the following papers from Projekt SEMSYN, please write to
</bodyText>
<figure confidence="0.856026166666667">
Frau Martin
c/o Projekt SEMSYN
Institut fuer Informatik
Azenbergstr. 12
D-7000 Stuttgart I
West Germany
or e-mail to: semsyn@ifistg.uucp
The Automated News Agency: SEMTEX —
A Text Generator for German
Dietmar Roesner
GEOTEX — A System for Verbalizing
Geometric Constructions (in German)
</figure>
<subsectionHeader confidence="0.527654">
Walter Kehl
</subsectionHeader>
<bodyText confidence="0.999656857142857">
As a by-product of the Japanese/German machine translation project
SEMSYN the SEMTEX text generator for German has been implemented
(in ZetaLISP for SYMBOLICS lisp machines). SEMTEX&apos;s first application
has been to generate newspaper stories about job market development.
Starting point for the newspaper application is just the data from the
monthly job market report (numbers of unemployed, open jobs, ...). A
rudimentary &amp;quot;text planner&amp;quot; takes these data and those of relevant previous
months, checks for changes and significant developments, simulates possi-
ble argumentations of various political speakers on these developments and
finally creates a representation for the intended text as an ordered list of
frame descriptions. SEMTEX then converts this list into a newspaper story
in German using an extended version of the generator of the SEMSYN
project.
The extensions for SEMTEX include:
</bodyText>
<listItem confidence="0.987657909090909">
• Building up a representation for the context during the utterance of
successive sentences that allows for
— avoiding repetitions in wording
— avoiding re-utterance of information still valid
— pronominalization and other types of references.
• Grammatical tense is dynamically derived by checking the temporal
information from the conceptual representations and relating it to the
time of speech and the time-period focussed by the story.
• When simulating arguments the text planner uses abstract rhetorical
schemata; the generator is enriched with knowledge about various ways
to express such rhetorical structures as German surface texts.
</listItem>
<bodyText confidence="0.845165333333333">
GEOTEX is an application of the SEMTEX text generator for German: The
text generator is combined with a tool for interactively creating geometric
constructions. The latter offers formal commands for manipulating (i.e.
</bodyText>
<note confidence="0.6088275">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 93
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.997171">
creating, naming and — deliberately — deleting) basic objects of Euclidean
geometry. The generator is used to produce descriptive texts — in German
— related to the geometric construction:
</bodyText>
<listItem confidence="0.986380363636364">
• descriptions of the geometric objects involved,
• descriptions of the sequence of steps done during a construction.
SEMTEX&apos;s context-handling mechanisms have been enriched for GEOTEX:
• Elision is no longer restricted to adjuncts. For repetitive operations, verb
and subject will be elided in subsequent sentences.
• The distinction between known information and new one is exploited to
decide on constituent ordering: the constituent referring to the known
object is &amp;quot;topicalized&amp;quot;, i.e. put in front of the sentence.
• The system allows for more ways to refer to objects introduced in the
text: pronouns, textual deixis using demonstrative pronouns, names. The
choice between these variants is done deliberately.
</listItem>
<bodyText confidence="0.912049944444445">
GEOTEX is implemented in ZetaLISP and runs on SYMBOLICS lisp
machines.
The Generation System of the SEMSYN
Project. Towards a Task-Independent
Generator for German
Dietmar Roesner
We report on our experiences from the implementation of the SEMSYN
generator, a system generating German texts from semantic represen-
tations, and its application to a variety of different areas, input structures
and generation tasks. In its initial version the SEMSYN generator was used
within a Japanese/German MT project, where it produced German equiv-
alents to Japanese titles from scientific papers. Being carefully designed in
object-oriented style (and implemented with the FLAVOR system) the
system proved to be easily adaptable to other semantic representations —
e.g. output from CMU&apos;s Universal Parser — and extensible to other gener-
ation tasks: generating German news stories, generating descriptive texts
to geometric constructions.
Copies of the following reports on the joint research project WISBER can be ordered free of charge from
</bodyText>
<figure confidence="0.765005444444444">
Dr. Johannes Arz
Universitat des Saarlandes
FR. 10 Informatik IV
Im Stadtwald 15
D-6600 Saarbrticken 11
Electronic mail address: wisber%sbsvax. uucp@germany.csnet
Neuere Grammatiktheorien und Gramma-
tikf ormalismen
H.-U. Block, M. Gehrke, H. Haugeneder,
R. Hunze
Report No. 1
Entwurf eines Erhebungsschemas fiir
Geldanlage
R. Busche, S. op de Hipt, M.-J. Schacter-Radig
Report No. 2
Generierung von Erklarungen aus formalen
Wissensreprasentationen
H. Rosner
</figure>
<footnote confidence="0.3765995">
in LbV-Forum, Band 4, Nummer 1,
Juni 1986, pp. 3-19
</footnote>
<bodyText confidence="0.99953152631579">
The present paper gives an overview of modern theories of syntax and is
intended to provide insight into current trends in the field of parsing.
The grammar theories treated here are government and binding theory,
generalized phrase structure grammar, and lexical functional grammar, as
these approaches currently appear to be the most promising.
Recent grammar formalisms are virtually all based on unification proce-
dures. Three representatives of this group (functional unification gram-
mar, &amp;patr., and definite clause grammar) are presented.
This report describes the acquisition schema for the knowledge required by
knowledge-based consulting system WISBER, the goal of which consists
in carrying out the process of knowledge acquisition and formalization in a
methodical — i.e., planned and controlled — manner.
The main task involves the design of appropriate acquisition techniques
and their successful application in the domain of investment consulting.
The main topic of this report concerns the generation of natural language
texts. The use of explanation components in expert systems involves
making computer behavior more transparent. This standard can only be
attained if the current stack dump procedure is replaced by procedures in
which user expectations are met with respect to the contents of the systems
</bodyText>
<page confidence="0.965221">
94 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.895960041666667">
The FINITE STRING Newsletter Abstracts of Current Literature
Report No. 3
Incremental Construction of C- and
F-Structure in an LFG-Parser
H.-U. Block, R. Hunze
in Proceedings of the 11th International
Conference on Computational Linguistics,
COLING&apos;86, Bonn, pp. 490-493
Report No. 4
The Treatment of Movement Rules in an
LFG-Parser
H.-U. Block, H. Haugender
in Proceedings of the 11th International
Conference on Computational Linguistics,
COLING&apos;86, Bonn, pp. 482-486
Report No. 5
Morpheme-Based Lexical Analysis
M. Gehrke, H.-U. Block
Report No. 6
Probleme der Wissensreprasentation in
Beratungssystemen
H.-U. Block, M. Gehrke, H. Haugender,
R. Hunze
Report No. 7
</figure>
<bodyText confidence="0.990778592592593">
explanation as well as the acceptability of language structure.
This paper reports on work pertaining to an expanded range of explana-
tion components in the Nixdorf exper system shell TwAIce.
A critical account of the position held by grammatical theory in generat-
ing natural language at the user level is given, whereby the decision for a
certain theory remains first and foremost pragmatical.
Moreover, a stand is taken concerning scientific experimentation on the
transfer of formal knowledge representation. Practical problems concern-
ing technical technology are pointed out that haven&apos;t yet been taken into
account.
In this paper a parser for Lexical Functional Grammar (LFG) which is
characterized by incrementally constructing the c- and f-structure of a
sentence during parsing is presented. Then the possibilities of the earliest
check on consistency, coherence, and completeness are discussed.
Incremental construction of f-structure leads to an early detection and
abortion of incorrect paths and so increases parsing efficiency. Further-
more, those semantic interpretation processes that operate on partial struc-
tures can be triggered at an earlier state. This also leads to a considerable
improvement in parsing time. LFG seems to be well suited for such an
approach because it provides for locality principles by the definition of
coherence and completeness.
In this paper a way of treating long-distance movement phenomena as
exemplified in (1) is proposed within the framework of an LFG-based
parser.
(1) Who do you think Peter tried to meet
&apos;You think Peter tried to meet who&apos;
After a short overview of the treatment of general discontinuous depen-
dencies in the Theory of Government and Binding, Lexical Functional
Grammar, and Generalized Phrase Structure Grammar, the so-called wh-
or long-distance movement are concentrated arguing that a general mech-
anism which is compatible with both the LFG and the GB treatment of
long-distance movement can be found.
Finally, the implementation of such a movement mechanism in an
LFG-parser is presented.
In this paper some aspects of the advantages and disadvantages of a
morpheme-based lexicon with respect to a full lexicon are discussed.
Then a current implementation of an application-independent lexical
access component is presented as well as an implemented formalism for the
inflectional analysis of German.
The present report consists of two main sections. The first part analyzes
individual knowledge sources that require specialization for the consulting
system W1SBER. It should serve as a first approximation to the structural
analysis of all knowledge sources.
In the second part, methods for the representation of knowledge and
languages are examined. Regarding this, KL-ONE, interpreted as an epis-
temic formal structure of language representation for describing structure
objects, is examined. Supplementing this is an examination of other
systems which, in addition, have significant assertive components such as
KRYPTON and KL-TWO at their disposal.
At the other end of the spectrum lies PEARL, a system that cannot
clearly be semantically and epistemically interpreted as a representational
language as such.
Between these two poles lie, on the one hand, FLR, which, without
guaranteeing the semantic clarity of the grammatical constructions used,
</bodyText>
<table confidence="0.950501916666667">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 95
The FINITE STRING Newsletter Abstracts of Current Literature
Beratung und natfirlichsprachlicher Dialog
— eine Evaluation von Systemen der
Kunstlichen Intelligenz
H. Bergmann, M. Gerlach, W. Hoeppner,
H. Marburger
Report No. 8
Form der Ergebnisse der Wissensakqui-
sition in WISBER-XPS4
M. Fliegner, M.-J. Schachter-Radig
Report No. 9
</table>
<bodyText confidence="0.987461633333333">
flexibly combines a large number of the ideas previously suggested and, on
the other hand, KRS, representative for a group of hybrid representation
systems which allow a flexible combination of various formal structures of
representation.
This report contains an evaluation of Artificial Intelligence systems which
provide the research base for the develobment of the natural-language
advisory system WISBER.
First, the reasons for selecting the particular systems considered in the
study are given and a set of evaluation criteria emphasizing in particular
pragmatic factors (e.g., dialog phenomena, handling of speech acts, user
modeling) is presented.
The body of the report consists of descriptions and critical evaluations
of the following systems: ARGOT, AYPA, GRUNDY, GUIDON, HAM-ANS,
KAMP, OSCAR, ROMPER, TRACK, UC, VIE-LANG, WIZARD, WUSOR,
XCALIBUR.
The final chapter summarizes the results, concentrating on the possible
utilization of individual system capabilities in the development of WISBER.
In this paper fundamental questions are discussed concerning the represen-
tation of expert knowledge, exemplified within the area of investment
consulting.
While a written report is appropriate for a general presentation of
results, it neither satisfies the needs of systems development — which of
course must build upon the results of knowledge acquisition — nor can it do
justice to the requirements of knowledge acquisition itself.
On the other hand, epistemologically expressive knowledge represen-
tation tools require that conceptual design decisions must be made quite
early on. The tools LOOPS, OPS5, prolog-based shell, and KL-ONE are
dealt with.
The following abstracts are from COLING &apos;86 PROCEEDINGS, copies of which are available only from
[KS e.V.
</bodyText>
<figure confidence="0.773345833333333">
Poppelsdorfer Allee 47
D-5300 Bonn 1
WEST GERMANY
Telephone: +49/228/735645
EARN/BITNET: UPKOOO@DBNRHRZ I
INTERNET: UPK000TODBNRHRZ1.BITNET@WISCVM.WISC.EDU
The price is 95 DM within Europe and 110 DM for air delivery to non-European countries. Please pay in advance by
check to the address above or by bankers draft to the following account:
Bank far Gemeinwirtschaft Bonn
Account no. 1205 163 900, BLZ 380 101 11
Lexicon—Grammar: The Representation of
Compound Words
Maurice Gross
Universite Paris 7
Laboratoire Documentaire et Linguistique
2, place Jussieu
F-75221 Paris CEDEX 05
COL1NG&apos;86, pp. 1-6
</figure>
<bodyText confidence="0.999752">
The essential feature of a lexicon-grammar is that the elementary unit of
computation and storage is the simple sentence: subject-verb-comple-
ment(s). This type of representation is obviously needed for verbs: limit-
ing a verb to its shape has no meaning other than typographic, since a verb
cannot be separated from its subject and essential complements. We have
shown (1975) that given a verb, or equivalently a simple sentence, the set
of syntactic properties that describes its variations is unique: in general,
no other verb has an identical syntactic paradigm. As a consequence,
the properties of each verbal construction must be represented in a lexi-
con-grammar. The lexicon has no significance taken as an isolated compo-
nent and the grammar component, viewed as independent of the lexicon,
will have to be limited to certain complex sentences.
</bodyText>
<page confidence="0.893514">
96 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.810348">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<figure confidence="0.545712142857143">
User Models: The Problem of Disparity
Sandra Carberry
Department of Computer &amp; Information
Science
University of Delaware
Newark, Delaware 19716
COL1NG&apos;86, pp. 29-34
</figure>
<bodyText confidence="0.999564627450981">
A major problem in machine translation is the semantic description of lexi-
cal units which should be based on a semantic system that is both coherent
and operationalized to the greatest possible degree. This is to guarantee
consistency between lexical units coded by lexicographers. This article
introduces a generating device for achieving well-formed semantic feature
expressions.
This paper discusses the semantic features of nouns classified into catego-
ries in Japanese-to-English translation, and proposes a system for semantic
markers. In our system, syntactic analysis is carried out by checking the
semantic compatibility between verbs and nouns. The semantic structure
of a sentence can be extracted at the same time as its syntactic analysis.
We also use semantic markers to select words in the transfer phase for
translation into English.
The system of the Semantic Markers for Nouns consists of 13 conceptu-
al facets, including one facet for &apos;Others&apos; (discussed later), and is made up
of 49 filial slots (semantic markers) as terminals. We have tested about
3,000 sample abstracts in science and technological fields. Our research
has revealed that our method is extremely effective in determining the
meanings of Wago verbs (basic Japanese verbs) which have broader
concepts like the English verbs make, get, take, put, etc.
Even a superficial meaning representation of a text requires a system of
semantic labels that characterize the relations between the predicates in
the text and their arguments. The semantic interpretation of syntactic
subjects and objects, of prepositions and subordinate conjunctions has
been treated in numerous books and papers with titles including works like
deep case, case roles, semantic roles, and semantic relations.
In this paper we concentrate on the semantic relations established by
predicates: what are they, what are their characteristics, how do they group
the predicates.
OPERA is a natural language question answering system allowing the inter-
rogation of a data base consisting of an extensive listing of operas. The
linguistic front-end of OPERA is a comprehensive grammar of French, and
its semantic component translates the syntactic analysis into logical formu-
las (first order logic formulas).
However, there are quite a few constructions which can be analyzed
syntactically in the grammar but for which we are unable to specify trans-
lations. Foremost among them are anaphoric and elliptic constructions.
Thus this paper describes the extension of OPERA to anaphoric and elliptic
constructions on the basis of the Discourse Segmentation Theory.
A significant component of a user model in an information-seeking
dialogue is the task-related plan motivating the information-seeker&apos;s
queries. A number of researchers have modeled the plan inference process
and used these models to design more robust natural language interfaces.
However, in each case it has been assumed that the system&apos;s context model
and the plan under construction by the information-seeker are never at
variance. This paper addresses the problem of disparate plans. It presents
a four phase approach and argues that handling disparate plans requires an
enriched context model. This model must permit the addition of compo-
nents suggested by the information-seeker but not fully supported by the
system&apos;s domain knowledge, and must differentiate among the components
according to the kind of support accorded each component as a correct
</bodyText>
<figure confidence="0.968460225">
An Empirically Based Approach towards
a System of Semantic Features
Cornelia Zelinsky-Wibbelt
IAI-Eurotra-D
Martin-Luther-StraBe 14
D-6600 Saarbrucken
COL1NG&apos;86, pp. 7-12
Concept and Structure of Semantic Mark-
ers for Machine Translation in Mu-Project
Yoshiyuki Sakamoto
Electrotechnical Laboratory
Sakura-mura. Niihari-gun.
Ibaraki, Japan
Tetsuya Ishikawa
University of Library &amp; Information Science
Yatabe-machi. Tsukuba-gun.
Ibaraki, Japan
Masayuki Satoh
Japan Information Center of Science &amp; Tech-
nology. Nagata-cho, Chiyoda-ku
Tokyo, Japan
COLING 86, pp. 13-20
A Theory of Semantic Relations for Large
Scale Natural Language Processing
Hanne Ruus
Institut for nordisk filologi &amp; Eurotra-DK
Ebbe Spang-Hanssen
Romansk institut &amp; Eurotra-DK
University of Copenhagen
Njalsgade 80
DK-2300 Copenhagen S
COLING&apos;86, pp. 20-22
Extending the Expressive Capacity of the
Semantic Component of the OPERA
System
Celestin Sedogbo
Centre de Recherche Bull
68, Route de Versailles
78430 Louveciennes, France
COL1NG&apos;86, pp. 23-28
</figure>
<page confidence="0.237336">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 97
</page>
<note confidence="0.741034">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.996782666666667">
part of the information-seeker&apos;s overall plan. It is shown how a compo-
nent&apos;s support should affect the system&apos;s hypothesis about the source of
error once plan disparity is suggested.
</bodyText>
<title confidence="0.906884">
Pragmatic Sensitivity in NL Interfaces and
the Structure of Conversation
</title>
<author confidence="0.966139">
Tom Wachtel
</author>
<affiliation confidence="0.633592666666667">
Scicon Ltd., London and
Research Unit for Information Science &amp; Al,
Hamburg University
</affiliation>
<note confidence="0.428664">
COLING&apos;86, pp. 35-41
</note>
<title confidence="0.736307">
A Two-Level Dialogue Representation
</title>
<author confidence="0.835442">
Giacomo Ferrari
</author>
<affiliation confidence="0.8741005">
Department of Linguistics
University of Pisa
</affiliation>
<figure confidence="0.345832461538462">
Ronan Reilly
Educational Research Center
St. Patrick&apos;s College, Dublin 9
COLING&apos;86, pp. 42-45
INTERFACILE: Linguistic Coverage
and Query Reformulation
Yvette Mathieu, Paul Sabatier
CNRS - LADL
Universite Paris 7
Tour Centrale 9 E
2 Place Jussieu
75005 Paris
COL1NG&apos;86, pp. 46-49
</figure>
<subsectionHeader confidence="0.629480666666667">
Category Cooccurrence Restrictions and
the Elimination of Metarules
James Kilbury
</subsectionHeader>
<footnote confidence="0.5401494">
Technical University of Berlin
KIT/NASEV, CIS, Sekr. FR5-8
Franklinstr. 28/29
D-1000 Berlin 10
Germany — West Berlin
</footnote>
<note confidence="0.493169">
COLING 86, pp. 50-55
</note>
<bodyText confidence="0.99932648">
The work reported here is being conducted as part of the LOKI project
(ESPRIT Project 107, &amp;quot;A logic oriented approach to knowledge and data
bases supporting natural user interaction&amp;quot;). The goal of the NL part of the
project is to build a pragmatically sensitive natural language interface to a
knowledge base. By &amp;quot;pragmatically sensitive&amp;quot;, we mean that the system
should not only produce well-formed coherent and cohesive language (a
minimum requirement of any NL system designed to handle discourse) but
should also be sensitive to those aspects of user behaviour that humans are
sensitive to over and above simply providing a good response, including
producing output that is appropriately decorated with those minor and
semantically inconsequential elements of language that make the difference
between natural language and natural natural language.
This paper concentrates on the representation of the structure of
conversation in our systems. we will first outline the representation we use
for dialogue moves, and then outline the nature of the definition of well-
formed dialogue that we are operating with. Finally, we will note a few
extensions to the representation mechanism.
In this paper a two-level dialogue representation system is presented. It is
intended to recognize the structure of a large range of dialogues including
some nonverbal communicative acts which may be involved in an inter-
action. It provides a syntactic description of a dialogue which can be
expressed in terms of re-writing rules. The semantic level of the proposed
representation system is given by the goal and subgoal structure underlying
the dialogue syntactic units. Two types of goals are identified; goals which
relate to the content of the dialogue, and those which relate to communi-
cating the content.
The experience we have gained in designing and using natural language
interfaces has led us to develop a general language system, INTERFACILE,
involving the following principles:
— The linguistic coverage must be elementary but must include pheno-
mena that allow a rapid, concise, and spontaneous interaction, such as
anaphora (ellipsis, pronouns, etc.).
— The linguistic competence and limits of the interface must be easily and
rapidly perceived by the user.
— The interface must be equipped with strategies and procedures for lead-
ing the user to adjust his linguistic competence to the capacities of the
system.
We have illustrated these principles in an application: a natural language
(French) interface for acquiring the formal commands of some operating
system languages. (The examples given here concern DCL of Digital
Equipment Company.)
This paper builds upon and extends certain ideas developed within the
framework of Generalized Phrase Structure Grammar (GPSG). A new
descriptive device, the Category Cooccurrence Restriction (CCR), is intro-
duced in analogy to existing devices of GPSG in order to express con-
straints on the cooccurrence of categories within local trees (i.e., trees of
depth one) which at present are stated with Immediate Dominance &amp;idp.
rules and metarules. In addition to providing a uniform format for the
statement of such constraints, CCRs permit generalizations to be
expressed which presently cannot be captured in GPSG.
</bodyText>
<note confidence="0.7591175">
98 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.998093142857143">
Sections 1.1 and 1.2 introduce CCRs and presuppose only a general
familiarity with GPSG. The ideas do not depend on details of GPSG and
can be applied to other grammatical formalisms.
Sections 1.3-1.5 discuss CCRs in relation to particular principles of
GPSG and assume familiarity with Gazdar et al. (1985) (henceforth abbre-
viated as GKPS). Finally, section 2 contains proposals for using CCRs to
avoid the analyses with metarules given for English in GKPS
</bodyText>
<figure confidence="0.8231325">
Processing Word Order Variation within a
Modified ID/LP Framework
Pradip Dey
University of Alabama at Birmingham
Birmingham, AL 35294
COLING &apos;86 pp. 65-67
Sentence Adverbials in a System of Ques-
tion Answering without a Prearranged Data
Base
Eva Koktova
Hamburg, West Germany
COLING &apos;86 pp. 68-73
</figure>
<bodyText confidence="0.988371186046512">
The empirical validity of the projectivity hypothesis for Bulgarian is tested.
It is shown that the justification of the hypothesis presented for other
languages suffers serious methodological deficiencies. Our automated test-
ing, designed to evade such deficiencies, yielded results falsifying the
hypothesis for Bulgarian: the non-projective constructions studied were in
fact grammatical rather than ungrammatical, as implied by the projectivity
thesis. Despite this, the projectivity/non-projectivity distinction itself has
to be retained in Bulgarian syntax and, with some provisions, in the
systems for automatic processing as well.
The purpose of this contribution is to formulate ways in which the homo-
nymy of so-called &apos;Modal Particles&apos; and the etymons can be handled. Our
aim is to show that not only a strategy for this type of homonymy can be
worked out, but also a formalization of information beyond propositional
content can be introduced with a view to its MT application.
This paper presents an approach for processing incomplete and inconsist-
ent knowledge. Basis for attaching these problems are &apos;structures of
determination&apos;, which are extensions of Scott&apos;s approximation lattices
taking into consideration some requirements from natural language proc-
essing and representation of knowledge. The theory developed is exempli-
fied with processing plural noun phrases referring to objects which have to
be understood as classes or sets. Referential processes are handled by
processes on &apos;Referential Nets&apos;, which are a specific knowledge structure
developed for the representation of object-oriented knowledge. Problems
of determination with respect to cardinality assumptions are emphasized.
From a well represented sample of world languages, Steel (1978) shows
that about 78% of languages exhibit significant word order variation.
Only recently has this wide-spread phenomenon been drawing appropriate
attention. Perhaps ID/LP (Immediate Dominance and Linear Precedence)
framework is the most debated theory in this area. We point out some
difficulties in processing standard ID/LP grammar and present a modified
version of the grammar. In the modified version, the right-hand side of
phrase structure rules is treated as a set or partially-ordered set. An
instance of the framework is implemented.
In the present paper we provide a report on a joint approach to the compu-
tation treatment of sentence adverbials (such as surprisingly, presumably,
or probably) and focussing adverbials (such as only or at least, including
negation (not) and some other adverbial expressions, such as for example
or inter alia) within a system of question answering
without a prearranged data base (TIBAQ).
This approach is based on a joint theoretical account of the expressions
in question in the framework of a functional description of language; we
argue that in the primary case, the expressions in question occupy, in the
underlying topic-focus articulation of a sentence, the focus-initial position,
</bodyText>
<figure confidence="0.956742961538461">
Testing the Projectivity Hypothesis
Vladimir Peridiev
Mathematical Linguistics Dept.
Institute of Mathematics with Comp Centre
1113 Sofia, b1.8, Bulgaria
Harlon Harionov
Mathematics Dept.
Higher Inst of English &amp; Building
Sofia, Bulgaria
COLING&apos;86, pp. 56-58
Particle Homonymy and Machine
Translation
Kdroly Faricz
JATE University of Szeged
Egyetem u. 2.
Hungary H - 6722
COLING&apos;86, pp. 59-61
Plurals, Cardinalities, and Structures of
Determination
Christopher U. Habd
Universitat Hamburg, Fachbereich Informatik
Schltiterstr. 70
D-1000 Hamburg 13
COLING&apos;86, pp. 62-64
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 99
The FINITE STRING Newsletter Abstracts of Current Literature
</figure>
<bodyText confidence="0.9853014">
extending their scope over the focus, or the new information, of a
sentence, thus specifying, in a broad sense of the word, how the next infor-
mation of a sentence holds. On the surface the expressions in question are
usually moved to scope-ambiguous positions, which can be analyzed by
means of several general strategies.
</bodyText>
<sectionHeader confidence="0.890673" genericHeader="method">
D-PATR: A Development Environment
</sectionHeader>
<subsectionHeader confidence="0.914707">
for Unification-Based Grammars
</subsectionHeader>
<figure confidence="0.520285161290322">
Lauri Kartunnen
Artificial Intelligence Center
SRI International
333 Ravenswood Avenue
Menlo Park, CA 94025
and Center for the Study of Language and
Information, Stanford University
COL1NG&apos;86, pp. 74-80
Structural Correspondence Specification
Environment
Yon,gfeng Yan
Groupe d&apos;Etudes pour la Traduction
Automatique (GETA)
B.P. 68
University of Grenoble
38402 Saint Martin d&apos;Heres, France
COLING 86, pp. 81-84
Conditioned Unification for Natural
Language Processing
Ko&apos;iti Hasida
Electrotechnical Laboratory
Umezono 1-1-4, Sakura-Mura, Niibari-Gun
Ibaraki, 305 Japan
COLING 86, pp. 85-87
Methodology and Verifiability in Montague
Grammar
Seiki Akama
Fujitsu Ltd.
2-4-19, Sin-Yokohama
Yokohama, 222, Japan
COL1NG&apos;86, pp. 88-90
</figure>
<subsectionHeader confidence="0.778476">
Towards a Dedicated Database Manage-
ment System for Dictionaries
Marc Domenig, Patrick Shann
</subsectionHeader>
<footnote confidence="0.7316882">
Institut Dalle Molle pour les Etudes
Semantiques et Cognitives &amp;isscop.
Route des Acacias 54
1227 Geneva, Switzerland
COL1NG&apos;86 pp. 91-96
</footnote>
<bodyText confidence="0.99187234883721">
D-PATR is a development environment for unification-based grammars on
Xerox 1100 series work stations. It is based on the PATR formalism devel-
oped at SRI International. This formalism is suitable for encoding a wide
variety of grammars. At one end of this range are simple phrase-structure
grammars with no feature augmentations. The PATR formalism can also be
used to encode grammars that are based on a number of current linguistic
theories, such as lexical-functional grammar (Bresnan and Kaplan), head-
driven phrase structure grammar (Pollard and Sag), and functional unifica-
tion grammar (Kay). At the other end of the range covered by D-PATR are
unification-based categorial grammars (Klein, Steedman, Uszkoreit,
Wittenberg) in which all the syntactic information is incorporated in the
lexicon and the remaining few combinatorial rules that build phrases are
function application and composition. Definite-clause grammars (Pereira
and Warren) can also be encoded in the PATR formalism.
This article presents the Structural Correspondence Specification Environ-
ment (SCSE) being implemented at GETA.
The SCSE is designed to help linguists to develop, consult, and verify
the SCS grammars (SCSG) which specify linguistic models. It integrates
the techniques of data bases, structure editors, and language interpreters.
We argue that formalisms and tools of specification are as important as the
specification itself.
This paper presents what we call a conditional unification, a new method
of unification for processing natural languages. The key idea is to annotate
the patterns with a certain sort of conditions, so that they carry abundant
information. This method transmits information from one pattern to
another more efficiently than procedure attachments, in which information
contained in the procedure is embedded in the program rather than directly
attached to patterns. Coupled with techniques in formal linguistics, more-
over, conditioned unification serves most types of operations for natural
language processing.
Methodological problems in Montague Grammar are discussed. Our
observations show that a mode-theoretic approach to natural language
semantics is inadequate with respect to its verifiability from a logical point
of view. But, the formal attitudes seem to be of use for the development in
computational linguistics.
This paper argues that a lexical data base should be implemented with a
special kind of database management system (DBMS) and outlines the
design of such a system. The major difference between this proposal and a
general purpose DBMS is that its data definition language (DDL) allows
the specification of the entire morphology, which turns the lexical data
base from a mere collection of &apos;static&apos; data into a real-time word-analyzer.
Moreover, the dedication of the system conduces to the feasibility of user
interfaces with very comfortable monitor and manipulation functions.
</bodyText>
<table confidence="0.562151264705882">
100 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
The FINITE STRING Newsletter Abstracts of Current Literature
The Transfer Phase of the Mu Machine
Translation System
Makoto Nagao, Jun-ichi Tsujii
Department of Electrical Engineering
Kyoto University
Kyoto, Japan 606
COLING&apos;86, pp. 97-103
Lexical Transfer: A Missing Element in
Linguistics Theories
Alan K. Melby
Brigham Young University
Department of Linguistics
Provo, Utah 84602
COLING&apos;86, pp. 104-106
Idiosyncratic Gap: A Tough Problem to
Structure-Based Machine Translation
Yoshihiko Nitta
Advanced Research Laboratory
Hitachi Ltd.
Kokubunji, Tokyo 185 Japan
COLING&apos;86, pp. 107-111
Lexical-Functional Transfer: A Transfer
Framework in a Machine-Translation
System Based on LFG
Ikuo Kudo
CSK Research Institute
3-22-17 Higashi-Ikebukuro, Toshima-ku
Tokyo, 170, Japan
Hirosato Nomura
NTT Basic Research Laboratories
Musashino-shi, Tokyo, 180, Japan
COLING&apos;86, pp. 112-114
</table>
<bodyText confidence="0.976317818181818">
The interlingual approach to MT has been repeatedly advocated by
researchers originally interested in natural language understanding who
take machine translation to be one possible application. However, not
only the ambiguity but also the vagueness which every natural language
inevitably has leads this approach into essential difficulties. In contrast,
our project, the Mu-project, adopts the transfer approach as the basic
framework of MT. This paper describes the detailed construction of the
transfer phase of our system from Japanese to English, and gives some
examples of problems which seem difficult to treat in the interlingual
approach.
Some of the design principles relevant to the topic of this paper are:
</bodyText>
<listItem confidence="0.999958">
• Multiple Layer of Grammars
• Multiple Layer Presentation
• Lexicon Driven Processing
• Form-Oriented Dictionary Description
</listItem>
<bodyText confidence="0.999641236842105">
This paper also shows how these principles are realized in the current
system.
One of the necessary tasks of a machine translation system is lexical trans-
fer. In some cases there is a one-to-one mapping from source language
word to target language word. What theoretical model is followed when
there is a one-to-many mapping? Unfortunately, none of the linguistic
models that have been used in machine translation include a lexical trans-
fer component. In the absence of a theoretical model, this paper will
suggest a new way to test lexical transfer systems. This test is being
applied to an MT system under development. One possible conclusion may
be that further effort should be expended developing models of lexical
transfer.
Current practical machine translation systems, which are designed to deal
with a huge amount of documents, are generally structure-based. That is,
the translation process is done based on the analysis and transformation of
the structure of the source sentence, not on the understanding and para-
phrasing of the meaning of that sentence. But each language has its own
syntactic and semantic idiosyncrasy, and on this account, without under-
standing the total meaning of the source.sentence, it is often difficult for
MT to bridge properly the idiosyncratic gap between source and target
language. A somewhat new method call &amp;quot;Cross Translation Test&amp;quot; is
presented that reveals the detail of idiosyncratic gap together with the
so-so satisfiable possibility of MT The usefulness of the sublanguage
approach in reducing the idiosyncratic gap between source and target
languages is also mentioned.
This paper presents a transfer framework called LFT (Lexical-Functional
Transfer) for a machine translation system based on LFG (Lexical-Func-
tional Grammar). The translation process consists of subprocesses of anal-
ysis, transfer, and generation. We adopt the so-called f-structures of LFG
as the intermediate representations or interfaces between those subproc-
esses, thus the transfer process converts a source f-structure into a target
f-structure. Since LFG is a grammatical framework for sentence structure
analysis of one language, for the purpose, we propose a new framework for
specifying transfer rules with LFG schemata, which incorporates corre-
sponding lexical functions of two different languages into an equational
representation. The transfer process, therefore, is to solve equations called
target f-descriptions derived from the transfer rules applied to the source
f-structure and then to produce a target f-structure.
</bodyText>
<table confidence="0.9466695">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 101
The FINITE STRING Newsletter Abstracts of Current Literature
Transfer and MT Modularity
Pierre Isabelle, Elliott Macklovitch
Canadian Workplace Automation Research
Center
1575 Chomedey Boulevard
Laval, Quebec, Canada H7V 2X2
</table>
<tableCaption confidence="0.774561333333333">
COLING&apos;86, pp. 115-117
The Need for MT-Oriented Versions of
Case and Valency in MT
</tableCaption>
<author confidence="0.822879">
Harold L. Somers
</author>
<affiliation confidence="0.979114666666667">
Centre for Computational Linguistics
University of Manchester
Institute of Science and Technology
</affiliation>
<figure confidence="0.608300333333333">
COLING&apos;86, pp. 118-123
A Parametric NL Translator
Randall Sharp
</figure>
<affiliation confidence="0.861651666666667">
Dept. of Computer Science
University of British Columbia
Vancouver, Canada
</affiliation>
<subsubsectionHeader confidence="0.122827">
COLING&apos;86, pp. 124-126
</subsubsectionHeader>
<subsectionHeader confidence="0.969920333333333">
Lexicase Parsing: A Lexicon-Driven
Approach to Syntactic Analysis
Stanley Starosta
</subsectionHeader>
<bodyText confidence="0.419857">
University of Hawaii Social Science Research
Institute and Pacific International Center
for High Technology Research
Honolulu, Hawaii 96822
</bodyText>
<subsectionHeader confidence="0.657443">
Hirasato Nomura
</subsectionHeader>
<bodyText confidence="0.396139666666667">
NTT Basic Research Laboratories
Musashino-shi, Tokyo, 180, Japan
COLING&apos;86, pp. 127-132
</bodyText>
<subsectionHeader confidence="0.6786336">
Solutions for Problems of MT Parser
Methods used in Mu-Machine Translation
Project
Jun-ichi Nakamura, Jun-ichi
Makoto Nagao
</subsectionHeader>
<affiliation confidence="0.948162666666667">
Dept. of Electrical Engineering
Kyoto University
Sakyo, Kyoto 606, Japan
</affiliation>
<subsubsectionHeader confidence="0.419445">
COLING&apos;86, pp. 133-135
</subsubsectionHeader>
<bodyText confidence="0.999891730769231">
The transfer components of typical second generation (G2) MT systems
do not fully conform to the principles of G2 modularity, incorporating
extensive target language information while failing to separate translation
facts from linguistic theory. The exclusion from transfer of all non-con-
trastive information leads us to a system design in which the three major
components operate in parallel rather than in sequence. We also propose
that MT systems be designed to allow translators to express their know-
ledge in natural metalanguage statements.
This paper looks at the use in machine translation systems of the linguistic
models of Case and Valency. It is argued that neither of these models was
originally developed with this use in mind, and both must be adapted
somewhat to meet this purpose. In particular, the traditional Valency
distinction of complements and adjuncts leads to conflicts when valency
frames in different languages are compared: a finer but more flexible
distinction is required. Also, these concepts must be extended beyond the
verb, to include the noun and adjective as valency bearers. As far as Case
is concerned, too narrow an approach has traditionally been taken: work in
this field has been too concerned only with cases for arguments in verb
frames; case label systems for non-valency bound elements and also for
elements in nominal groups must be elaborated. The paper suggests an
integrated approach specifically oriented towards the particular problems
found in MT.
This report outlines a machine translation system whose linguistic compo-
nent is based on principles of Government and Binding. A &amp;quot;universal
grammar&amp;quot; is defined, together with parameters of variation for specific
languages. The system, written in Prolog, parses, generates, and translates
between English and Spanish (both directions).
This paper presents a lexicon-based approach to syntactic analysis, Lexi-
case, and applies it to a lexicon-driven computational parsing system. The
basic descriptive mechanism in a Lexicase grammar is lexical features. The
properties of lexical items are represented by contextual and non-contextu-
al features, and generalizations are expressed as relationships among sets
of these features and among sets of lexical entries. Syntactic tree struc-
tures are represented as networks of pairwise dependency relationships
among the words in a sentence. Possible dependencies are marked as
contextual features on individual lexical items, and Lexicase parsing is a
process of picking out words in a string and attaching dependents to them
in accordance with their contextual features. Lexicase is an appropriate
vehicle for parsing because Lexicase analyses are monostratal, flat, and
relatively non-abstract, and it is well suited to machine translation because
grammatical representations for corresponding sentences in two languages
will be very similar to each other in structure and inter-constituent
relations, and thus far easier to interconvert.
A parser is a key component of a machine translation system. If it fails in
parsing an input sentence, the MT system cannot output a complete trans-
lation. A parser of a practical MT system must solve many problems
caused by the varieties of characteristics of natural languages. Some prob-
lems are caused by the incompleteness of grammatical rules and dictionary
information, and some by the ambiguity of natural languages. Others are
caused by various types of sentence constructions, such as itemization,
insertion by parentheses, and other typographical conventions that cannot
be naturally captured by ordinary linguistic rules.
</bodyText>
<page confidence="0.973547">
102 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.462851">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999507888888889">
The authors of this paper have been developing MT systems between
Japanese and English (in both directions) under the Mu-machine trans-
lation project. In the system&apos;s development, several methods have been
implemented with grammar writing language GRADE to solve the problems
of the MT parser. In this paper, first the characteristics of GRADE and the
Mu-MT parser are briefly described. Then, methods to solve the MT pars-
ing problems that are caused by the varieties of sentence constructions and
the ambiguities of natural languages are discussed from the viewpoint of
efficiency and maintainability.
</bodyText>
<subsectionHeader confidence="0.918308333333333">
Strategies and Heuristics in the Analysis of
a Natural Language in Machine Trans-
lation
</subsectionHeader>
<figure confidence="0.465790714285714">
Zaharin Yusoff
Groupe d&apos;Etudes pour la Traduction
Automatique
BP no. 68
Universite de Grenoble
38402 Saint-Martin-d&apos;Heres, France
COUNG&apos;86, pp. 136-139
</figure>
<subsectionHeader confidence="0.7296736">
Parsing in Parallel
Xiuming Huang, Louise Guthrie
Computing Research Laboratory
New Mexico State University
Las Cruces, NM 88003
</subsectionHeader>
<bodyText confidence="0.469954">
COLING&apos;86, pp. 140-145
</bodyText>
<subsectionHeader confidence="0.88918875">
Computational Comparative Studies on
Romance Languages: A Linguistic
Comparison of Lexicon-Grammars
Annibale Elia
</subsectionHeader>
<bodyText confidence="0.40543">
Istituto di Linguistica Universita di Salerno
</bodyText>
<subsectionHeader confidence="0.575608">
}Wile Mathieu
</subsectionHeader>
<bodyText confidence="0.481503">
Laboratoire d&apos;Automatique Documentaire et
</bodyText>
<equation confidence="0.388364">
Linguistique
C.N.R.S. - Universite de Paris 7
COLING 86, pp. 146-150
</equation>
<bodyText confidence="0.324455">
A Stochastic Approach to Parsing
</bodyText>
<subsectionHeader confidence="0.830053666666667">
Geoffrey Sampson
Department of Linguistics and Phonetics
University of Leeds
</subsectionHeader>
<bodyText confidence="0.45713">
COUNG&apos;86, pp. 151-155
</bodyText>
<subsectionHeader confidence="0.378645">
Parsing Without (Much) Phrase Structure
Michael B. Kac
Department of Linguistics
University of Minnesota
</subsectionHeader>
<bodyText confidence="0.999954230769231">
The analysis phase in an indirect, transfer, and global approach to machine
translation is studied. The analysis conducted can be described as exhaus-
tive (meaning with backtracking), depth-first, and strategically and heuris-
tically driven, while the grammar used is an augmented context free
grammar. The problem areas, being pattern matching, ambiguities,
forward propagation, checking for correctness, and backtracking, are high-
lighted. Established results found in the literature are employed whenever
adaptable, while suggestions are given otherwise.
The paper is a description of a parallel model for natural language parsing,
and a design for its implementation on the Hypercube multiprocessor. The
parallel model is based on the Semantic Definite Clause Grammar formal-
ism and integrates syntax and semantics through the communication of
processes. The main processes, of which there are six, contain either pure-
ly syntactic or purely semantic information, giving the advantage of simple;
transparent algorithms dedicated to only one aspect of parsing. Communi-
cation between processes is used to impose semantic constraints on the
syntactic processes.
What we present here is an application on the basis of the Italian and
French linguistic data bank assembled by the Istituto di Linguistica of
Salerno University (Italy) and the Laboratoire Automatique Documentaire
et Linguistique (C.N.R.S.-France). These two research centers have been
working for years to the constitution of formalized grammars of the
respective languages. The composition of lexicon-grammars is the first
stage of this project.
Simulated annealing is a stochastic computational technique for finding
optimal solutions to combinatorial problems for which the combinatorial
explosion phenomenon rules out the possibility of systematically examining
each alternative. It is currently being applied to the practical problem of
optimizing the physical design of computer circuitry, and to the theoretical
problems of resolving patterns of auditory and visual stimulation into
meaningful arrangements of phonemes and three-dimensional objects.
Grammatical parsing — resolving unanalyzed linear sequences of words into
meaningful grammatical structures — can be regarded as a perception prob-
lem logically analogous to those just cited, and simulated annealing holds
great promise as a parsing technique.
Approaches to NL syntax conform in varying degrees to the older re-
lational/dependency model (essentially that assumed in traditional gram-
mar), which treats a sentence as a group of words united by various re-
lations, and the newer constituent model. ... In computational linguistics
</bodyText>
<table confidence="0.974921916666667">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 103
The FINITE STRING Newsletter Abstracts of Current Literature
Minneapolis, MN 55455
Alexis Manaster-Ramer
Program in Linguistics
University of Michigan
Ann Arbor, MI 48109
COLING 86, pp. 156-158
Requirements for Robust Natural
Language Interfaces: The LanguageCraft
and XCALIBUR Experiences
Jaime G. Carbonell
</table>
<footnote confidence="0.91341">
Carnegie-Mellon University
and Carnegie-Group, Inc.
Pittsburgh, PA 15213
COLING&apos;86, pp. 162-163
</footnote>
<bodyText confidence="0.994783796296296">
there is a strong (if not universal) reliance on phrase structure as the medi-
um via which to represent syntactic structure; call this the consensus view.
... In its strongest form, the consensus view says that the recovery of a
fully specified parse tree is an essential step in computational language
processing, and would, if correct, provide important support for the
constituent model. In this paper, we shall critically examine the rationale
for this view, and will sketch (informally) an alternative view which we
find more defensible. The actual position we shall take for this discussion,
however, is conservative in that we will not argue that there is no place
whatever for constituent analysis in parsing or in syntactic analysis gener-
ally. What we argue is that phrase structure is at least partly redundant in
that a direct leap to the composition of some semantic units is possible
from a relatively underspecified syntactic representation (as opposed to a
complete parse tree).
In this paper we will describe an approach to parsing, one major compo-
nent of which is a strategy called RECONNAISSANCE-ATTACK. Under
this strategy, no structure building is attempted until after completion of a
preliminary phase designed to exploit low-level information to the fullest
possible extent. This first pass then defines a set of constraints that restrict
the set of available options when structure building proper begins. R-A
parsing is in principle compatible with a variety of different views regard-
ing the nature of syntactic representation, though it fits more comfortably
with some than with others.
STATEMENT BY THE CHAIR (abridged) The goal of this panel is to
evaluate three natural language interfaces which were introduced to the
commercial market in 1985 (cf. Carnegie Group 1985, Kamins 1985,
Texas Instruments 1985) and to relate them to current research in compu-
tational linguistics. Each of the commercial systems selected as a starting
point for the discussion (see Wahlster 1986 for a functional comparison)
was developed by a well-known scientist with considerable research expe-
rience in NL processing: LanguageCraft&apos; by Carnegie Group (designed
under the direction of J. Carbonell), NLMenu by Texas Instruments
(designed under the direction of H. Tennant), and Q &amp; A2 by Symantec
(designed under the direction of G. Hendrix).
I Trademark of Carnegie-Group, Inc.
2 Trademark of Symantec Corporation
PANELIST STATEMENT (abridged): Natural Language interfaces to
data bases and expert systems require the investigation of several crucial
capabilities in order to be judged habitable by their end users and produc-
tive by the developers of applications. User habitability is measured in
terms of linguistic coverage, robustness of behavior and speed of response,
whereas implementer activity is measured by the amount of effort required
to connect the interface to a new application, to develop its syntactic and
semantic grammar, and to test and debug the resultant system assuring
a certain level of performance. These latter criteria have not been
addressed directly by natural language researchers in pure laboratory
settings, with the exception of user-defined extensions to an existing inter-
face (e.g., NanoKLAUS, VOX). But, in order to amortize the cost of devel-
oping practical, robust, and efficient interfaces over multiple applications,
the implementer productivity requirements are as important as user habita-
bility. We treat each set of criteria in turn, drawing from our experience in
XCALIBUR, and in LanguageCraft a commercially available environment
and run-time module for rapid development of domain-oriented natural
language interfaces. In our discussion we distill the general lessons accrued
</bodyText>
<figure confidence="0.8723485">
Reconnaissance-Attack Parsing
Michael B. Kac, Tom Rindflesch
Department of Linguistics
University of Minnesota
Minneapolis, MN 55455
Karen L. Ryna
Computer Sciences Center
Honeywell, Inc.
Minneapolis, MN 55427
COLING&apos;86, pp. 159-160
Panel: Natural Language Interfaces —
Ready for Commercial Success?
Wolfgang Wahlster (Chair)
Department of Computer Science
University of SaarbrUcken
D-6600 Saarbrticken 11
Fed. Rep. of Germany
COLING 86 p. 161
</figure>
<page confidence="0.902367">
104 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.429618">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999454418604651">
from several years of experience using these systems, and conducting
several small-scale user studies.
(Responses to moderator&apos;s question based on Q&amp;A.)
PANELIST STATEMENT (abridged): I don&apos;t think that natural language
interfaces are a very good idea. By that I mean conventional natural
language interfaces — the kind where the user types in a question and the
system tries to understand it. Oh sure, when (if?) computers have world
knowledge that is comparable to what humans need to communicate with
each other, natural language interfaces will be easy to build and, depending
on what else is available, might be a good way to communicate with
computers. But today we are s0000 far away from having that much
knowledge in a system, conventional natural language interfaces don&apos;t
make sense.
There is something different that makes more sense — NLMenu. It is a
combination of menu technology with natural language understanding
technology, and it eliminates many of the deficiencies one finds with
conventional natural language interfaces while retaining the important
benefits.
This paper will explore and discuss the less obvious ways syntactic struc-
ture is used to convey information and how this information could be used
by a natural language database system as a heuristic to organize and search
a discourse space.
The primary concern of this paper will be to present a general theory of
processing which capitalizes on the information provided by such non-SVO
word orders as inversion, (wh) clefting, and prepositional phrase (PP)
fronting.
This paper gives a formal theory of presupposition using situation seman-
tics developed by Barwise and Perry. We will slightly modify Barwise and
Perry&apos;s original theory of situation semantics so that we can deal with non-
monotonic reasonings which are very important for the formalization of
presupposition in natural language. This aspect is closely related to the
formulation of incomplete knowledge in artificial intelligence.
The function words of a language provide explicit information about how
propositions are to be related. We have examined a subset of these func-
tion words, namely the subordinating conjunctions which link propositions
within a sentence, using sentences taken from corpora stored on magnetic
tape. On the basis of this analysis, a computer program for Dutch lan-
guage generation and comprehension has been extended to deal with the
subordinating conjunctions. We present an overview of the underlying di-
mensions that were used in describing the semantics and pragmatics of the
Dutch subordinating conjunctions. We propose a Universal set of Linking
Dimensions, sufficient to specify the subordinating conjunctions in any
language. This ULD is a first proposal for the representation required for a
</bodyText>
<figure confidence="0.99328235">
Q&amp;A: Already a Success?
Gary G. Hendrix
Symantec Corporation
Cupertino, CA 95014
COLING&apos;86, pp. 164-166
The Commercial Application of Natural
Language Interfaces
Harry Tennant
Computer Science Center
Texas Instruments
Dallas, Texas
COLING&apos;86 p. 167
...end of panel..
The Role of Inversion and PP-Fronting in
Relating Discourse Elements
Mark Vincent LaPolla
The Artificial Intelligence Laboratory and
The Department of Linguistics
University of Texas at Austin
Austin, Texas
70LING&apos;86, pp. 168-173
Situational Investigation of Presupposition
Seiki Akama
Fujitsu Ltd.
2-4-19 ShinYokohama
Yokohama, Japan
Masahito Kawamori
Sophia University
7 Kioicho, Chiyodaku
Tokyo, Japan
COLING&apos;86, pp. 174-1 76
Linking Propositions
D.S. Brie, R.A. Smit
Rotterdam School of Management
Erasmus University
P.O.B. 1738
NL-3000 DR Rotterdam, The Netherlands
COLING&apos;86, pp. 177-180
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 105
The FINITE STRING Newsletter Abstracts of Current Literature
computer program to understand or translate the subordinating conjunc-
tions of any natural language.
Discourse and Cohesion in Expository
Text
Allen B. Tucker, Sergei Nirenburg
Department of Computer Science
Colgate University
Victor Raskin
Department of English
Purdue University
COLING&apos;86, pp. 181-183
Degrees of Understanding
Eva Hajeovd, Petr Sgall
Faculty of Mathematics and Physics
Charles University
Malostranske n. 25
Prague 1, Czechoslovakia
COLING 86, pp. 184-186
Categorial Unification Grammars
Hans Uszkoreit
</figure>
<bodyText confidence="0.986157535714286">
Artificial Intelligence Center, SRI Interna-
tional and Center for the Study of Lan-
guages and Information, Stanford University
COLING*86, pp. 187-194
This paper discusses the role of discourse in expository text, text which
typically comprises published scholar papers, textbooks, proceedings of
conferences, and other highly stylized documents. Our purpose is to exam-
ine the extent to which those discourse-related phenomena that generally
assist the analysis of dialogue text — where speaker, hearer, and speech-act
information are more actively involved in the identification of plans and
goals — can be used to help with the analysis of expository text. In partic-
ular, we make the optimistic assumption that expository text is strongly
connected, i.e., that all adjacent pairs of clauses in such a text are con-
nected by &amp;quot;cohesion markers&amp;quot;, both explicit and implicit. We investigate
the impact that this assumption may have on the depth of understanding
that can be achieved, the underlying semantic structures, and the support-
ing knowledge base for the analysis. An application of this work in design-
ing the Al-based machine translation model, TRANSLATOR, is discussed in
Nirenburg et al. (page 627 of these Proceedings).
Along with &amp;quot;static&amp;quot; or &amp;quot;declarative&amp;quot; descriptions of language system,
models of language use (the regularities of communicative competence) are
constructed. One of the outstanding aspects of this transfer of attention
consists in the efforts devoted to automatic comprehension of natural
language which, since Winograd&apos;s SHRDLU, are presented in many differ-
ent contexts. One speaks about understanding, or comprehension,
although it may be noticed that the term is used in different, and often
unclear, meanings. In machine translation systems, as the late B.
Vauquois pointed out (see now Vauquois and Boitet, 1985), a flexible
system combining different levels of automatic analysis is necessary (i.e.,
the transfer component should be able to operate at different levels). The
human factor cannot be completely dispensed with; it seems inevitable to
include post-edition, or such a division of labor as that known from the
system METEO. Not only should the semantico-pragmatic items present in
the source language structure be reflected but also certain aspects of factu-
al knowledge (see Slocum 1985: 16). It was pointed out by Kirschner
(1982: 18) that, to a certain degree, this requirement can be met by means
of a system of semantic features. For NL comprehension systems the auto-
matic formulation of a partial image of the world often belongs to the core
of the system; such a task certainly goes far beyond pure linguistic analysis
and description.
Winograd (1976: 269,275) claims that a linguistic description should
handle &amp;quot;the entire complex of the goals of the speaker&amp;quot;. It is then possible
to ask what are the main features relevant for the patterning of this
complex and what are the relationships between understanding all the
goals of the speaker and having internalized the system of a natural
language. It seems to be worthwhile to reexamine the different kinds and
degrees of understanding.
Categorial unification grammars (CUGs) embody the essential properties of
both unification and categorial grammar formalisms. Their efficient and
uniform way of encoding linguistic knowledge in well-understood and
widely-used representations makes them attractive for computational appli-
cations and for linguistic research.
In this paper, the basic concepts of CUGs and simple examples of their
application will be presented. It will be argued that the strategies and
potentials of CUGs justify their further exploration in the wider context of
research on unification grammars. Approaches to selected linguistic
</bodyText>
<page confidence="0.915885">
106 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<bodyText confidence="0.817134333333333">
The FINITE STRING Newsletter Abstracts of Current Literature
phenomena such as long-distance dependencies, adjuncts, word order, and
extraposition are discussed.
</bodyText>
<subsectionHeader confidence="0.748612">
Dependency Unification Grammar
</subsectionHeader>
<figure confidence="0.81375759375">
Peter Hellwig
University of Heidelberg
D-6900 Heidelberg, West Germany
COL1NG&apos;86, pp. 195-198
The Weak Generative Capacity of Paren-
thesis-Free Categorial Grammars
Joyce Friedman, Dani Dai, Weiguo Wang
Computer Science Department
Boston University
111 Cummington Street
Boston, MA 02215
COL1NG&apos;86, pp. 199-201
Tree Adjoining and Head Wrapping
E. Vijay-Shanker, David!. Weir,
Aravind K. Joshi
Department of Computer and Information
Science
University of Pennsylvania
Philadelphia, PA 19104
COLING 86, pp. 202-207
Categorial Grammars for Strata of Non-
CF Languages and their Parsers
Michal P. Chytil
Charles University
Malostranske nam. 25
118 00 Praha 1, Czechoslovakia
Hans Karlgren
KVAL
Sodermalstorg 8
116 45 Stockholm, Sweden
COL1NG&apos;86, pp. 208-210
A Simple Reconstruction of GPSG
</figure>
<subsectionHeader confidence="0.811597">
Stuart M. Shieber
</subsectionHeader>
<bodyText confidence="0.83101475">
Artificial Intelligence Center, SRI Inter-
national and Center for the Study of Lan-
guage and Information, Stanford University
COL1NG&apos;86, pp. 211-215
</bodyText>
<title confidence="0.473703">
Kind Types in Knowledge Representation
</title>
<author confidence="0.656412">
K. Dahlgren
</author>
<affiliation confidence="0.59981">
IBM Los Angeles Scientific Center
11601 Wilshire Blvd.
</affiliation>
<bodyText confidence="0.999856186046512">
This paper describes the analysis component of the language processing
system PLAIN from the viewpoint of unification grammars. The principles
of Dependency Unification Grammar (DUGs) are discussed. The computer
language DRL (Dependency Representation Language) is introduced in
which DUGs can be formulated. A unification-based parsing procedure is
part of the formalism. PLAIN is implemented at the universities of Heidel-
berg, Bonn, Flensburg, Kiel, Zurich, and Cambridge, U.K.
We study the weak generative capacity of a class of parenthesis-free cate-
gorial grammars derived from those of Ades and Steedman by varying the
set of reduction rules. With forward cancellation as the only rule, the
grammars are weakly equivalent to context-free grammars. When a back-
ward combination rule is added, it is no longer possible to obtain all the
context-free languages. With suitable restriction of the forward partial
rule, the languages are still context-free and a push-down automaton can
be used for recognition. Using the unrestricted rule of forward partial
combination, a context-sensitive language is obtained.
In this paper we discuss the formal relationship between the classes of
languages generated by Tree Adjoining Grammars and Head Grammars.
In particular, we show that Head Languages are included in Tree Adjoin-
ing Languages and that Tree Adjoining Grammars are equivalent to a
modification of Head Grammars called Modified Head Grammars. The
inclusion of MHL in HL, and thus the equivalence of HGs and TAGs, in the
most general case remains to be established.
We introduce a generalization of categorial grammar extending its descrip-
tive power, and a simple model of categorial grammar parser. I3oth tools
can be adjusted to particular strata of languages via restricting grammatical
or computational complexity.
Like most linguistic theories, the theory of generalized phrase structure
grammar (GPSG) has described language axiomatically, that is, as a set of
universal and language-specific constraints on the well-formedness of
linguistic elements of some sort. The coverage and detailed analysis of
English grammar in the ambitious recent volume by Gazdar, Klein, Pullum,
and Sag entitled Generalized Phrase Structure Grammar, are impressive, in
part because of the complexity of the axiomatic system developed by the
authors. In this paper, we examine the possibility that simpler descriptions
of the same theory can be achieved through a slightly different, albeit still
axiomatic, method. Rather than characterize the well-formed trees direct-
ly, we progress in two stages by procedurally characterizing the well-
formedness axioms themselves, which in turn characterize the trees.
This paper describes Kind Types (KT), a system which uses commonsense
knowledge to reason about natural language text. KT encodes some of the
knowledge underlying natural language understanding, including category
distinctions and descriptions differentiating real-world objects, states, and
</bodyText>
<figure confidence="0.424819466666667">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 107
The FINITE STRING Newsletter Abstracts of Current Literature
Los Angeles, CA 90025
J. McDowdl
Department of Linguistics
University of Southern California
Los Angeles, CA 90089
COLING&apos;86, pp. 216-221
DCKR — Knowledge Representation in
Prolog and Its Application to Natural
Language Processing
Hozumi Tanaka
Tokyo Institute of Technology
Department of Computer Science
0-okayama, 2-12-1, Megro-ku
Tokyo, Japan
COLING&apos;86, pp. 222-225
Conceptual Lexicon Using an Object-
Oriented Language
Shoichi Yokoyama
Electrotechnical Laboratory
Tsukuba, Ibaraki, Japan
Kenji Hanakata
Universitat Stuttgart
Stuttgart, F.R. Germany
COLING&apos;86, pp. 226-228
Elementary Contracts as a Pragmatic Basis
of Language Interaction
E.L. Petshina
Al Laboratory, Computer Center
</figure>
<figureCaption confidence="0.366052333333333">
Siberian Division of the USSR Ac. Sci.
Novosibirsk 630090, USSR
COLING&apos;86, pp. 229-231
</figureCaption>
<subsectionHeader confidence="0.856039">
Communicative Triad as a Structural
Element of Language Interaction
</subsectionHeader>
<author confidence="0.233264">
F. G. Dinenberg
</author>
<affiliation confidence="0.520148">
Al Laboratory, Computer Center
</affiliation>
<construct confidence="0.677043">
Siberian Division of the USSR Ac. Sci.
Novosibirsk 630090, USSR
COLING&apos;86, pp. 232-234
</construct>
<subsectionHeader confidence="0.39432">
TBMS: Domain Specific Text Manage-
ment and Lexicon Development
</subsectionHeader>
<bodyText confidence="0.999977826923077">
events. It embeds an ontology reflecting the ordinary person&apos;s top-level
cognitive model of real-world distinctions and a data base of prototype
descriptions of real-world entities. KT is transportable, empirically-based
and constrained for efficient reasoning in ways similar to human reasoning
processes.
Semantic processing is one of the important tasks for natural language
processing. Basic to semantic processing is descriptions of lexical items.
The most frequently used form of description of lexical items is probably
Frames or Objects. Therefore in what form Frames or Objects are ex-
pressed is a key issue for natural language processing. A method of the
Object representation in Prolog called DCKR will be introduced. It will be
seen that if part of general knowledge and a dictionary are described in
DCKR, part of context-processing, and the greater part of semantic proc-
essing can be left to the functions built in Prolog.
This paper describe the construction of a lexicon representing abstract
concepts. This lexicon is written by an object-oriented language, CTALK,
and forms a dynamic network system controlled by object-oriented mech-
anisms. The content of the lexicon is constructed using a Japanese diction-
ary. First, entry words and their definition parts are derived from the
dictionary. Second, syntactic and semantic information is analyzed from
these parts. Finally, superconcepts are assigned in the superconcept part in
an object, static parts in the slot values, and dynamic operations to the
message parts, respectively. One word has one object in a world, but
through the superconcept part and slot part this connects to the subconcept
of other words and worlds. When relative concepts are accumulated, the
result will be a model of human thoughts which have conscious and uncon-
scious parts.
Language interaction (LI) as a part of interpersonal communication is
considerably influenced by psychological and social roles of the partners
and their pragmatic goals. These aspects of communication should be
accounted for while elaborating advanced user-computer dialogue systems
and developing formal models of LI. We propose here a formal description
of communicative context of LI-situation, namely, a system of indices of LI
agents&apos; interest in achieving various pragmatic purposes and a system of
contracts which reflect social and psychological roles of the LI agents and
conventionalize their &amp;quot;rights&amp;quot; and &amp;quot;duties&amp;quot; in the LI-process. Different
values of these parameters of communication allow us to state possibility
and/or necessity of certain types of speech acts under certain conditions of
LI-situation.
Researches on dialogue natural-language interaction with intellectual
&amp;quot;human-computer&amp;quot; systems are based on models of language &amp;quot;human-to-
human&amp;quot; interaction, these models representing descriptions of communi-
cation laws. An aspect of developing language interaction models is an
investigation of dialogue structure. In the paper a notion of elementary
communicative triad (SR-triad) is introduced to model the &amp;quot;stimulus-
reaction&amp;quot; relation between utterances in the dialogue. The use of the SR-
triad apparatus allows us to represent a scheme of any dialogue as a triad
structure. SR-triad structure being inherent both to natural and program-
ming language dialogues, SR-system is claimed to be necessary while devel-
oping dialogue processors.
The definition of a Text Base Management System is introduced in terms
of software engineering. That gives a basis for discussing practical text
</bodyText>
<page confidence="0.967559">
108 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.798786">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<author confidence="0.663891">
S. Goeser, E. Mergenthaler
</author>
<affiliation confidence="0.698521">
Universtity of Ulm
</affiliation>
<note confidence="0.739844">
Federal Republic of Germany
COLING&apos;86, pp. 235-240
</note>
<title confidence="0.410311">
Text Analysis and Knowledge Extraction
</title>
<author confidence="0.8602725">
Fujio Nishida, Shinobu Takamatsu,
Tadaaki Tani, Hiroji Kusaka
</author>
<affiliation confidence="0.973378333333333">
Department of Electrical Engineering
Faculty of Engineering
University of Osaka Prefecture
</affiliation>
<address confidence="0.511987">
Sakai, Osaka, 591 Japan
</address>
<table confidence="0.3710254375">
COLING&apos;86, pp. 241-243
Context Analysis System for Japanese
Text
Hitoshi Isahara, Shun Ishizaki
Electrotechnical Laboratory
1-1-4, Umezono, Sakura-mura, Niihari-gun
Ibaraki, Japan 305
COLING &apos;86 pp. 244-246
Disambiguation and Language Acquisition
through the Phrasal Lexicon
Uri Zernik, Michael G. Dyer
Artificial Intelligence Laboratory
Computer Science Department
3531 Boelter Hall
University of California
Los Angeles, CA 90024
</table>
<tableCaption confidence="0.150236">
COLING&apos;86, pp. 247-252
</tableCaption>
<subsectionHeader confidence="0.9701265">
Linguistic Knowledge Extraction from Real
Language Behavior
</subsectionHeader>
<author confidence="0.820993">
K. Shirai, T. Hamada
</author>
<affiliation confidence="0.9695885">
Department of Electrical Engineering
Waseda University
</affiliation>
<page confidence="0.508761">
3-4-1 Ohkubo Shinjuku-ku, Tokyo, Japan
</page>
<subsubsectionHeader confidence="0.35237">
COLING&apos;86, pp. 253-255
</subsubsectionHeader>
<bodyText confidence="0.997616396551724">
administration, including questions on corpus properties and appropriate
retrieval criteria. Finally, strategies for the derivation of a word data base
from an actual TBMS will be discussed.
The study of text understanding and knowledge extraction has been active-
ly done by many researchers. The authors also studied a method of struc-
tured information extraction from texts without a global text analysis. The
method is available for a comparatively short text such as a patent claim
clause and an abstract of a technical paper.
This paper describes the outline of a method of knowledge extraction
from a longer text which needs a global text analysis. The kinds of texts
are expository texts or explanation texts. Expository texts described here
mean those which have various hierarchical headings such as a title, a
heading of each section and sometimes an abstract. In this definition, most
texts, including technical papers, reports, and newspapers, are expository.
Text of this kind disclose the main knowledge in a top-down manner and
show not only the location of an attribute value in a text but also several
key points of the content. This property of expository texts contrasts with
that of novels and stories in which an unexpected development of the plot
is preferred.
This paper pays attention to such characteristics of expository texts and
describes a method of analyzing texts by referring to information
contained in the intersentential relations and the headings of texts and then
extracting requested knowledge such as a summary from texts in an effi-
cient way.
A natural language understanding system is described which extracts con-
textual information from Japanese texts. It integrates syntactic, seman-
tic, and contextual processing serially. The syntactic analyzer obtains
rough syntactic structures from the text. The semantic analyzer treats
modifying relations inside noun phrases and case relations among verbs
and noun phrases. Then, the contextual analyzer obtains contextual infor-
mation from the semantic structure extracted by the semantic analyzer.
Our system understands the context using precoded contextual knowledge
on terrorism and plugs the event information in input sentences into the
contextual structure.
The phrase approach to language processing emphasizes the role of the
lexicon as a knowledge source. Rather than maintaining a single generic
lexical entry for each word, e.g., take, the lexicon contains many phrases,
e.g., take on, take to the streets, take to swimming, take over, etc. Although
this approach proves effective in parsing and in generation, there are two
acute problems which still require solutions. First, due to the huge size of
the phrase lexicon, especially when considering subtle meanings and idio-
syncratic behavior of phrases, encoding of lexical entries cannot be done
manually. Thus phrase acquisition must be employed to construct the lexi-
.con. Second, when a set of phrases is morpho-syntactically equivalent,
disambiguation must be performed by semantic means. These problems
are addressed in the program RNA.
An approach to extract linguistic knowledge from real language behavior is
described. This method depends on the extraction of word relations,
patterns of which are obtained by structuring the dependency relations in
sentences called Kakari-Uke relation in Japanese. As the first step of this
approach, an experiment of a word classification utilizing those patterns
was made on the 4178 sentences of real language data. A system was
made to analyze dependency structure of sentences utilizing the knowledge
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 109
The FINITE STRING Newsletter Abstracts of Current Literature
base obtained through this word classification and the effectiveness of the
knowledge base was evaluated. To develop this approach further, the
relation matrix which captures multiple interaction of words is proposed.
</bodyText>
<subsectionHeader confidence="0.881569">
Tailoring Importance Evaluation to Read-
er&apos;s Goals: A Contribution to Descriptive
Text Summarization
</subsectionHeader>
<figure confidence="0.784761944444444">
Danilo Fum, Giovanni Guido, Carlo Tau°
Istito di Matematica, Informatica e
Sistemistica
Universita di Udine, Italy
COLING&apos;86, pp. 256-259
Domain Dependent Natural Language
Understanding
Klaus Heje Munch
Department of Computer Science
Technical University of Denmark
DK-2800 Lyngby, Denmark
COLING&apos;86, pp. 260-262
Morphological Analysis for a German
Text-to-Speech System
Amanda Pounder, Markus Kommenda
Institut ftir Nachrichtentechnik und
Hochfrequenztechnik
Technische Universitat Wien
</figure>
<figureCaption confidence="0.449647">
Gusshausstrasse 25, A-1040 Wien, Austria
COLING&apos;86, pp. 263-268
</figureCaption>
<subsectionHeader confidence="0.8003306">
Synergy of Syntax and Morphology in
Automatic Parsing of French Language
with a Minimum of Data
Jacques Vergne, Pascale Pages
Inaleo Paris
</subsectionHeader>
<bodyText confidence="0.328918">
COLING&apos;86, pp. 269-271
</bodyText>
<subsectionHeader confidence="0.4045194">
A Morphological Recognizer with Syntac-
tic and Phonologic Rules
John Bear
Artificial Intelligence Center
SRI International
</subsectionHeader>
<footnote confidence="0.545333666666667">
333 Ravenswood Avenue
Menlo Park, CA 94025
COLING&apos;86, pp. 272-276
</footnote>
<bodyText confidence="0.65828">
A Dictionary and Morphological Analyser
for English
</bodyText>
<subsubsectionHeader confidence="0.44547">
G.J. Russell, S.G. Pulman
</subsubsectionHeader>
<bodyText confidence="0.999995906976744">
This paper deals with a new approach to importance evaluation of descrip-
tive texts developed in the framework of SUSY, an experimental system in
the domain of text summarization. The problem of taking into account the
reader&apos;s goals in evaluating importance of different parts of a text is first
analyzed. A solution to the design of a goal interpreter capable of comput-
ing a quantitative measure of the relevance degree of a piece of text ac-
cording to a given goal is then proposed, and an example of goal inter-
preter operation is provided.
A natural language understanding system for a restricted domain of
discourse — thermodynamic exercises at an introductory level — is
presented. The system transforms texts into a formal meaning represen-
tation language based on cases. The semantical interpretation of sentences
and phrases is controlled by case frames formulated around verbs and
surface grammatical roles in noun phrases. During the semantical interpre-
tation of a text, semantic constraints may be imposed on elements of the
text. Each sentence is analyzed with respect to context, making the system
capable of solving anaphoric references such as definite descriptions,
pronouns, and elliptic constructions.
The system has been implemented and successfully tested on a selection
of exercises.
A central problem in speech synthesis with unrestricted vocabulary is the
automatic derivation of correct pronunciation from the graphemic form of
a text. The software module GRAPHON was developed to perform this
conversion for German and is currently being extended by a morphological
analysis component. This analysis is based on a morph lexicon and a set of
rules and structural descriptions for Germany word-forms. It provides
each text input item with an individual characterization such that the
phonological, syntactic, and prosodic components may operate upon it.
This systematic approach thus serves to minimize the number of wrong
transcriptions and at the same time lays the foundation for the generation
of stress and intonation patterns, yielding more intelligible, natural-sound-
ing, and generally acceptable synthetic speech.
We intend to present in this paper a parsing method of French language
whose particularities are: a multi-level approach: syntax and morphology
working simultaneously, the use of string pattern matching and the absence
of dictionary. We want here to evaluate the feasibility of the method
rather than to present an operational system.
This paper describes a morphological analyzer which, when parsing a
word, uses two sets of rules: rules describing the syntax of words, and
and rules describing facts about orthography.
This paper describes the current state of a three-year project aimed at the
development of software for use in handling large quantities of dictionary
information within natural language processing systems. The project ... is
</bodyText>
<page confidence="0.901901">
110 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.475455166666667">
The FINITE STRING Newsletter Abstracts of Current Literature
Computer Laboratory
University of Cambridge
G.D. Ritchie, A. W. Black
Department of Artificial Intelligence
University of Edinburgh
COLING&apos;86, pp. 277-279
A Kana-Kanji Translation System for
Non-Segmented Input Sentences based on
Syntactic and Semantic Analysis
Masahiro Abe, Yoshimitsu Ooshima,
Katsuhiko Yuura, Nobuyuki Takeichi
Central Research Laboratory
Hitachi, Ltd.
Kokubunji, Tokyo, Japan
COLING&apos;86, pp. 280-285
A Compression Technique for Arabic
Dictionaries: The Affix Analysis
Abdelmajid Ben Hamadou
Departement of Computer Science —
FSEG Faculty
B.P. 69 — Route de Faeroport
SFAX Tunisia
COLING&apos;86, pp. 286-288
</figure>
<subsectionHeader confidence="0.937024">
Machine Learning of Morphological Rules
</subsectionHeader>
<bodyText confidence="0.791048">
by Generalization and Analogy
</bodyText>
<subsectionHeader confidence="0.421217">
Klaus Wothke
</subsectionHeader>
<table confidence="0.933191578947368">
Arbeitsstelle Linguistische Datenverarbeitung
Institut fur Deutsche Sprache
Mannheim, West Germany
COLING&apos;86, pp. 289-293
Linguistic Developments in Eurotra since
1983
Lietwn Jaspaert
Katholieke Universiteit Leuven
Belgium
COLING&apos;86, pp. 294-296
The &lt;C,A&gt; Framework in Eurotra: A
Theoretically Committed Notation for MT
D.J. Arnold
University of Essex
Colchester, Essex C04 3SQ, UK
S. Kraunrr, L. des Tombe
University of Utrecht
Trans 14, 3512 JK
Utrecht, The Netherlands
</table>
<bodyText confidence="0.999708791666667">
one of three closely related projects funded under the Alvey IKBS Pro-
gramme (Natural Language Theme); a parser is under development at Edin-
burgh by Henry Thompson and John Phillips), and a sentence grammar
is being devised by Ted Biscoe and Clare Grover at Lancaster and Bran
Boguraev and John Carroll at Cambridge. It is intended that the soft-
ware and rules produced by all three projects will be directly compat-
ible and capable of functioning in an integrated system.
This paper presents a disambiguation approach for translating non-seg-
mented-Kana into Kanji. The method consists of two steps. In the first
step, an input sentence is analyzed morphologically and ambiguous
morphemes are stored in a network form. In the second step, the best
path, which is a string of morphemes, is selected by syntactic and semantic
analysis based on case grammar. In order to avoid the combinatorial
explosion of possible paths, the following heuristic search method is
adopted. First, a path that contains the smallest number of weighted-mor-
phemes is chosen as the quasi-best path by a best-first-search technique.
Next, the restricted range of morphemes near the quasi-best path is
extracted from the morpheme network to construct preferential paths.
An experimental system incorporating large dictionaries has been devel-
oped and evaluated. A translation accuracy of 90.5 was obtained. This
can be improved to about 95% by optimizing the dictionaries.
In every application that concerns the automatic processing of natural
language, the problem of the dictionary size is posed. In this paper we
propose a compression dictionary algorithm based on an affix analysis of
the non-diacritical Arabic.
It consists in decomposing a word into its first elements, taking into
account the different linguistic transformations that can affect the morpho-
logical structures.
This work has been achieved as part of a study of the automatic
detection and correction of spelling-errors in the non-diacritical Arabic
texts.
This paper describes an experimental procedure for the inductive auto-
mated learning of morphological rules from examples. At first an outline
of the problem is given. Then a formalism for the representation of
morphological rules is defined. This formalism is used by the automated
procedure, whose anatomy is subsequently presented. Finally, the
performance of the system is evaluated and the most important unsolved
problems are discussed.
I wish to put the theory and metatheory currently adopted in the Eurotra
project into a historical perspective, indicating where and why changes to
its basic design for a transfer-based MT (TBMT) system have been made.
This paper describes a model for MT, developed within the Eurotra MT
project, based on the idea of compositional translation, by describing a
basic, experimental notation which embodies the idea. The introduction
provides background, section 1 introduces the basic ideas and the notation,
and section 2 discusses some of the theoretical and practical implications of
the model, including some concrete extensions, and some more speculative
discussion.
</bodyText>
<table confidence="0.977624175">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 111
The FINITE STRING Newsletter Abstracts of Current Literature
M. Rosner
ISSCO
54, Route des Acacias
1227 Geneva, Switzerland
G.B. Varile
Commission of the European Communities
L-2928 Luxembourg
COLING&apos;86, pp. 297-303
Generating Semantic Structures in
Eurotra-D
Erich Steiner
!AI — Eurotra — D
Martin-Luther-Strasse 14
D-6600 Saarbrtieken, West Germany
COLING 86, pp. 304 306
Valency Theory in a Stratificational MT
System
Paul Schmidt
1AI Eurotra-D
Martin-Luther-Strasse 14
D-6600 Saarbriicken, West Germany
COLING 86 pp. 307-312
A Compositional Approach to the Trans-
lation of Temporal Expressions in the
Rosetta System
Lisette Appelo
Philips Research Laboratories
Eindhoven, The Netherlands
COLING 86, pp. 313-318
Idioms in the Rosetta Machine Translation
System
André Schenk
Philips Research Laboratories
Eindhoven, The Netherlands
COLING&apos;86, pp. 319-324
NARA: A Two-Way Simultaneous Inter-
pretation System between Korean and
Japanese — A Methodological Study
</table>
<subsectionHeader confidence="0.374102">
Hee Sung Chung, Tosiyasu L. Kunii
</subsectionHeader>
<bodyText confidence="0.9989656">
The following paper is based on work done in the multi-lingual MT project
Eurotra, and MT project of the European Community.
Analysis and generation of clauses within the Eurotra framework
proceeds through the levels of (at least) Eurotra constituent structure
(ECS), Eurotra relation structure (ERS), and interface structure (IS).
At IS, labelling of nodes consists of labellings for time, modality, seman-
tic features, semantic relations, and others. In this paper, we shall be
concerned exclusively with semantic relations (SRs), to which we shall also
refer as &amp;quot;participant roles&amp;quot; (PR).
According to current Eurotra legislation, these SRs are assigned to
dictionary entries of verbs (and other word classes, which will be disre-
garded in this paper) by coders, and through these entries to clauses in a
pattern matching process.
This approach, while certainly valid in principle, leads to the problem of
inter-coder-consistency, at least as long as the means for identifying SRs
are paraphrase tests for SRs. In Eurotra-D, we have for some time now
been experimenting with a set of SRs, or PRs, which are identified with the
help of syntactic criteria. This approach will be outlined in this paper.
This paper tries to investigate valency theory as a linguistic tool in machine
translation. There are three main areas in which major questions arise:
</bodyText>
<listItem confidence="0.989632">
(1) Valency theory itself. I sketch a valency theory in linguistic terms
which includes the discussion of the nature of dependency representation
as an interface for semantic description.
(2) The dependency representation in the translation process. I try to
sketch the different roles of dependency representation in analysis and
generation.
(3) The implementation of valency theory in an MT system. I give a few
examples for how a valency description could be implemented in the Euro-
tra formalism.
</listItem>
<bodyText confidence="0.9996166875">
This paper discusses the translation of temporal expressions, in the frame-
work of the machine translation system Rosetta. The translation method
of Rosetta, the &amp;quot;isomorphic grammar method&amp;quot;, is based on Montague&apos;s
Compositionality Principle. It shows that a compositional approach leads
to a transparent account of the complex aspects of time in natural language
and can be used for the translation of temporal expressions.
This paper discusses one of the problems of machine translation, name-
ly the translation of idioms. The paper describes a solution to this problem
within the theoretical framework of the Rosetta machine translation sys-
tem. Rosetta is an experimental translation system which uses an inter-
mediate language and translates between Dutch, English, and, in the
future, Spanish.
This paper presents a new computing model for constructing a two-way
simultaneous interpretation system between Korean and Japanese. We
also propose several methodological approaches to the construction of a
two-way simultaneous interpretation system, and realize the two-way
</bodyText>
<page confidence="0.753481">
112 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<table confidence="0.915975028571429">
The FINITE STRING Newsletter Abstracts of Current Literature
Department of Information Science
Faculty of Science, University of Tokyo
7-3-1 Hongo, Bunkyo-ku Tokyo, 113 Japan
COLING&apos;86 pp. 325-328
Strategies for Interactive Machine Trans-
lation: The Experience and Implications of
the UMIST Japanese Project
P.J. Whitelock, M. McGee Wood,
B.J. Chandler, N. Holden, H.J. Horsfall
Centre for Computational Linguistics
University of Manchester Institute of Science
and Technology
PO Box 88, Manchester M60 1QD UK
COLING&apos;86 pp. 329-334
Pragmatics in Machine Translation
Annely Rothkegel
Universitat Saarbrticken
Sonderforschungsbereich 100
Elektronische Sprachforschung
D 6600 Saarbriicken, West Germany
COLING&apos;86, pp. 335-337
A Metric for Computational Analysis of
Meaning: Toward an Applied Theory of
Linguistic Semantics
Sergei Nirenburg
Department of Computer Science
Colgate University
Hamilton, NY 13346
SERGEC@IOLGATE
Victor Raskin
Department of English
Purdue University
West Lafayette, IN 47907
JHP@ZURDUE-ASC.CSNET
</table>
<tableCaption confidence="0.232019">
COLING&apos;86, pp. 338-340
</tableCaption>
<subsectionHeader confidence="0.636691">
Collative Semantics
Dan Pass
Computing Research Laboratory
</subsectionHeader>
<bodyText confidence="0.999932854166667">
interpreting process as a model unifying both linguistic competence and
linguistic performance. The model is verified theoretically and through
actual applications.
At the Centre for Computational Linguistics, we are designing and imple-
menting an English-to-Japanese interactive machine translation system.
The project is funded jointly by the Alvey Directorate and International
Computers Limited (ICL). The prototype system runs on the ICL PERQ,
though much of the development work has been done on a VAX 11/750.
It is implemented in Prolog, in the interests of rapid prototyping, but
intended for optimization. The informing principles are those of modern
complex-feature-based linguistic theories, in particular Lexical-Functional
Grammar (Bresnan (ed.) 1982, Kaplan and Bresnan 1982), and General-
ized Phrase Structure Grammar (Gazdar et al. 1985).
For development purposes we are using an existing corpus of 10,000
words of continuous prose from the PERQ&apos;s graphics documentation; in the
long term, the system will be extended for use by technical writers in fields
other than software. At the time of writing, we have well-developed
system development software, user interface, and grammar and dictionary
handling facilities. The English analysis grammar handles most of the
syntactic structures of the corpus, and we have a range of formats for
output of linguistic representations and Japanese text. A transfer grammar
for English-Japanese has been prototyped, but is not yet fully adequate to
handle all constructions in the corpus; a facility for dictionary entry in
Kanji is incorporated. The aspect of the system we will focus on in the
present paper is its interactive nature, discussing the range of different
types of interaction which are provided or permitted for different types of
users.
TEXAN is a system of transfer-oriented text analysis. Its linguistic concept
is based on a communicative approach within the framework of speech act
theory. In this view texts are considered to be the result of linguistic
actions. It is assumed that they control the selection of translation equiv-
alents. The transition of this concept of linguistic actions (text acts) to the
model of computer analysis is performed by a context-free illocution gram-
mar processing categories of actions and a propositional structure of states
of affairs. The grammar which is related to a text lexicon provides the
connection of these categories and the linguistic surface units of a single
language.
A metric for assessing the complexity of semantic (and pragmatic) analysis
in natural language processing is proposed as part of a general applied
theory of linguistic semantics for NLP. The theory is intended as a
complete projection of linguistic semantics onto NLP and is designed as an
exhaustive list of possible choices among strategies of semantic analysis at
each level, from the word to the entire text. The alternatives are summa-
rized in a chart, which can be completed for each existing or projected NLP
system. The remaining components of the applied theory are also outlined.
This paper introduces Collative Semantics (CS), a new domain-indepen-
dent semantics for natural language processing (NLP) which addresses the
problems of lexical ambiguity, metonymy, various semantic relations
</bodyText>
<table confidence="0.9699614">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 113
The FINITE STRING Newsletter Abstracts of Current Literature
New Mexico State University
Las Cruces, NM 88003
COLING&apos;86, pp. 341-343
</table>
<bodyText confidence="0.999767659574468">
(conventional relations, redundant relations, contradictory relations, meta-
phorical relations, and severely anomalous relations) and the introduction
of new information. We explain the two techniques CS uses for matching
together knowledge structures (KSs) and why semantic vectors, which
record the results of such matches, are informative enough to tell apart
semantic relations and be the basis for lexical disambiguation.
Determiners play an important role in conveying the meaning of an utter-
ance, but they have often been disregarded, perhaps because it seemed
more important to devise methods to grasp the global meaning of a
sentence, even if not in a precise way. Another problem with determiners
is their inherent ambiguity.
In this paper we propose a logical formalism, which, among other things,
is suitable for representing determiners without forcing a particular inter-
pretation when their meaning is still not clear.
This paper presents a model-theoretic semantics for directional modifiers
in English. The semantic theory presupposed for the analysis is that of
Montague Grammar (cf. Montague 1970, 1973) which makes it possible to
develop a strongly compositional treatment of directional modifiers. Such
a treatment has significant computational advantages over case-based
treatments of directional modifiers that are advocated in the Al literature.
A calculus is presented which allows an efficient treatment of the following
components: Tenses, temporal conjunctions, temporal adverbials (of
&amp;quot;definite&amp;quot; type), temporal quantifications, and phases. The phases are a
means for structuring the set of time-points t where a certain proposition is
valid. For one proposition, there may exist several &amp;quot;phase&amp;quot;-perspectives.
The calculus has integrative properties, i.e., all five components are repre-
sented by the same formal means. This renders possible a rather easy
combination of all information and conditions coming from the aforesaid
components.
My aim in organizing this panel is to stimulate the discussion between
researchers working on MT and linguists interested in formal syntax and
semantics. I am convinced that a closer cooperation will be fruitful for
both sides. I will be talking about experimental MT or MT as a research
project and not as a development project.
In virtually all current natural-language dialog systems, users can only refer
to objects by using linguistic descriptions. However, in human face-to-face
conversation, participants frequently use various sorts of deictic gestures as
well. In this paper, we will present the referent identification component
of XTRA, a system for a natural-language access to expert systems. XTRA
allows the user to combine NL input together with pointing gestures on the
terminal screen in order to refer to objects on the display. Information
about the location and type of this deictic gesture, as well as about the
linguistic description of the referred object, the case frame, and the dialog
memory are utilized for identifying the object. The system is tolerant in
respect to impreciseness of both the deictic and the natural language input.
The user can thereby refer to objects more easily, avoid referential failures,
and employ vague everyday terms instead of precise technical notions.
</bodyText>
<table confidence="0.992779146341463">
A Logical Formalism for the Represen-
tation of Determiners
Barbara Di Eugenio, Leonardo Lesmo, Paolo
Pogliano, Pietro Torasso, Francesco Urbano
Dipartimento di Informatica —
Universita di Torino
Via Valperga Caluso 37 —
.10125 Torino — Italy
COLING&apos;86, pp. 344-346
A Compositional Semantics for Directional
Modifiers — Locative Case Reopened
Erhard W. Hinrichs
Bolt Beranek &amp; Newman Laboratories
10 Moulton Street
Cambridge, MA 02238
COL1NG&apos;86, pp. 347-349
Temporal Relations in Texts and Time
Logical Inferences
Amen Kunze
Central Institute of Linguistics
Academy of Sciences of GDR
DDR-1100 Berlin
COLING 86, pp. 350-352
Linguistics Bases for Machine Translation
Christian Rohrer
Institut fur Linguistik
Universitat Stuttgart
KeplerstraBe 17
7000 Stuttgart 1
COLING&apos;86, pp. 353-355
Combining Deictic Gestures and Natural
Language for Referent Identification
Alfred Kobsa, Jiirgen Allgayer, Carola Redding,
Norbert Reithinger, Dagmar Schmauks, Karin
Harbusch, Wolfgang Washlster
SFB 314: Al - Knowledge-Based Systems
University of Saarbriicken
D-6600 Saarbriiken 11, West Germany
COLING&apos;86, pp. 356-361
114 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
The FINITE STRING Newsletter Abstracts of Current Literature
</table>
<bodyText confidence="0.999675641509434">
A new Theory of Names and Descriptions that offers a uniform treatment
for many types of non-singular concepts found in natural language
discourse is presented. We introduce a layered model of the language
denotational base (the universe) in which every world object is assigned a
layer (level) reflecting its relative singularity with respect to other objects
in the universe. We define the notion of relative singularity of world
objects as an abstraction class of the layer-membership relation.
This paper describes a system that extracts information from Hungarian
descriptive texts of medical domain. Texts of clinical narratives define a
sublanguage that uses limited syntax but holds the main characteristics of
the language, namely free word order and rich morphology. We offer a
fairly general parsing method for free word order languages and how to
use it for parsing Hungarian clinical texts. The system can handle simple
cases of ellipses, anaphora, unknown words, and typical abbreviations of
clinical practice. The system translates texts of anamneses, patient visits,
laboratory tests, medical examinations, and discharge summaries into an
information format usable for a medical expert system. Similarly to this
expert system, the information formatting program has been written in
MPROLOG language and its experimental version runs on PROPER-16, a
Hungarian-made (IBM-XT compatible) microcomputer.
In this talk I will first give a short overview of the basic Discourse Repre-
sentation Theory system (Kamp 1981), and sketch Kamp&apos;s proposal for
the treatment of definite noun phrases. Then I will indicate how the basic
reference establishing function and the &amp;quot;side-effects&amp;quot; of different types of
definite NPs can be described in more detail. In doing this, I will refer to
the work about anaphora done in the NLP area (especially by Barbara
Grosz, Candy Sidner, and Bonnie Webber), integrating some of their
assumptions into the DRT framework, and critically commenting on some
others.
Several methods to represent meanings of words have been proposed.
However, they are not useful for information retrieval systems, because
they cannot deal with the entities that cannot be universally represented by
symbols.
In this paper we propose a notion of semantic space. Semantic space is
an Euclidean space where words and entities are put. A word is one point
in the space. The meanings of the word are represented as the space
configuration around the word. The entities that cannot be represented by
symbols can be identified in the space by the location the entity should be
settled in. We also give a learning mechanism for the space. We prove the
effectiveness of the proposed method by an experiment on information
retrieval for the study of Japanese literature.
It has been recognized that single words extracted from natural language
texts are not always useful for the representation of information content.
Associated or related terms, and complex content identifiers derived from
thesauruses and knowledge bases, or constructed by automatic word
grouping techniques, have therefore been proposed for text identification
purposes.
The area of associative content analysis and information retrieval is
reviewed in this study. The available experimental evidence shows that
none of the existing or proposed methodologies are guaranteed to improve
retrieval performance in a replicable manner for document collections in
different subject areas. The associative techniques are most valuable for
restricted environments covering narrow subject areas, or in iterative
</bodyText>
<table confidence="0.991823244897959">
An Approach to Non-Singular Terms in
Discourse
Tomek Strudkowski
School of Computing Science
Simon Fraser University
Burnaby, B.C. Canada V5A 1S6
COLING&apos;86, pp. 362-364
Processing Clinical Narratives in
Hungarian
Gdbor Praszeky
National Education Library and Museum
Computer Department
Honved u. 19
H-1055 Budapest, Hungary
COLING&apos;86, pp. 365-367
Definite Noun Phrases and the Semantics
of Discourse
Manfred Pinkal
c/o Fraunhofer-Institute IA0
Holzgartenstrasse 17,
D 7000 Stuttgart 1
and Institut fiir Linguistik
Universitat Stuttgart
COLING&apos;86, pp. 368-373
Learning the Space of Word Meanings for
Information Retrieval Systems
Koichi Hori, Seinosuke Toda, Hisashi Yasunaga
National Institute of Japanese Literature
1-16-10 Yutakacho Shingawaku
Tokyo 142 Japan
COLING&apos;86, pp. 374-379
On the Use of Term Associations in Auto-
matic Information Retrieval
Gerard Salton
Department of Computer Science
Cornet University
Ithaca, NY 14853
COLING&apos;86, pp. 380-386
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 115
The FINITE STRING Newsletter Abstracts of Current Literature
search situations where user inputs are available to refine previously avail-
able query formulations and search output.
Towards the Automatic Acquisition of
Lexical Data
H. Trost, E. Buchberger
Department of Medical Cybernetics and
Artificial Intelligence
University of Vienna, Austria
COLING 86, pp. 387-389
</table>
<bodyText confidence="0.998008232558139">
Creating a knowledge base has always been a bottleneck in the implemen-
tation of AI systems. This is also true for Natural Language Understanding
systems, particularly for data-driven ones. While a perfect system for
automatic acquisition of all sorts of knowledge is still far from being real-
ized, partial solutions are possible. This holds especially for lexical data.
Nevertheless, the task is not trivial, in particular when dealing with
languages rich in inflectional forms like German. Our system is to be used
by persons with no specific linguistic knowledge, thus linguistic expertise
has been put into the system to ascertain correct classification of words.
Classification is done by means of a small rule based system with lexical
knowledge and language-specific heuristics. The key idea is the identifica-
tion of three sorts of knowledge which are processed distinctly and the
optimal use of knowledge already contained in the existing lexicon.
PeriPhrase is a high-level computer language developed by A.L.P. Systems
to facilitate parsing and structural transfer. It is designed to speed the
development of computer-assisted translation systems and grammar check-
ers. We describe the syntax and semantics of this tool, its integrated devel-
opment environment, and some of our experience with it.
Nowadays, MT systems grow to such a size that a first specification step is
necessary if we want to be able to master their development and mainte-
nance, for the software part as well as for the linguistic part (&amp;quot;lingwares&amp;quot;).
Advocating for a clean separation between linguistics tasks and pro-
gramming tasks, we first introduce a specification/implementation/
validation framework for NLP, then SCSL, a language for the specification
of analysis and generation modules.
APE is a workbench to develop ATN grammars based on an active chart
parser. It represents the networks graphically and supports the grammar
writer by window- and menu-based debugging techniques.
To deal with specific alphabets is a necessity in natural language process-
ing. In Grenoble, this problem is solved with the help of transcriptions.
Here we present a language (LT) designed to the rapid writing of passage
from one transcription to another (transducers) and give some examples of
its use.
Toutes les categories grammaticales utilisees dans un modele de traduction
Ariane sont formalisees et codees de f non mnemonique en tant que vari-
ables et valeurs de variables. L&apos;ensemble des variables d&apos;un modele donne
constitute le vocabulaire du metalangage qui permet de decrire la langue
source et la langue cible de ce modele.
La structure de donnees du systeme est une arborescence dont chaque
noeud porte une decoration. Les decorations contiennent les variables
declarees pour le systeme et affectees de certaines valeurs. Les variables
apparaissent egalement dans les grammaires d&apos;analyse, de transfert et de
generation, dans les dictionnaires monolingues d&apos;analyse ou de generation
</bodyText>
<table confidence="0.751257904109589">
PeriPhrase: Lingware for Parsing and
Structural Transfer
Kenneth R. Beesley, David Hefner
A.L.P. Systems
190 West 800 North
Provo, UT 84604
COLING &apos;86, pp. 390-392
SCSL: A Linguistic Specification Language
for MT
Rend Zajac
GETA, BP 68
Universite de Grenoble
38402 Saint-Martin-d&apos;Heres, France
COLING&apos;86, pp. 393-398
A User Friendly ATN Programming Envi-
ronment (APE)
Hans Haugeneder, Manfred Gehrke
Siemens AG, ZT ZTI INF
West Germany
COLING&apos;86, pp. 399-401
A Language for Transcriptions
Yves LePage
GETA, BP 68
Universite Scientifique et Medicate de
Grenoble
38402 Saint-Martin-d&apos;Heres, France
COLING&apos;86, pp. 402-404
Variables et Categories Granunticales
dans un Modele Ariane
Jean-Phillipe Guilbaud
GETA, BP 68
Universite Scientifique et Medicate de
Grenoble
38402 Saint-Martin-d&apos;Heres, France
pp. 405-407
116 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
The FINITE STRING Newsletter Abstracts of Current Literature
et bilingues de transfert lexical, ainsi que dans les specifications de modele
linguistique (grammaires statiques).
Deduction Automatique et Systemes
Transformationnels
J. Chauche
C.E.L.T.A. 23, Boulevard Albert ler
54000 - Nancy, France
COLING&apos;86, pp. 408-411
CRITAC — A Japanese Text Proofreading
System
Koichi Takeda, Tetsunosuke Fujisaki,
Emiko Suzuki
Japan Science Institute
IBM Japan, Ltd.
5-19 Sanban-cho, Chiyoda-ku,
Tokyo 102, Japan
COLING&apos;86, pp. 412-417
Storing Text using Integer Codes
Raja Noor Ainon
Computer Centre
University of Malaya
59100 Kuala Lumpur, Malaysia
COL1NG&apos;86, pp. 418-420
BetaText: An Event Driven Text Process-
ing and Text Analyzing System
Benny Brodda
Department of Linguistics
University of Stockholm
S-106 91 Stockholm, Sweden
COLING 86, pp. 421-422
Toward Integrated Dictionaries for M(a)T
Ch. Boitet, N. Nedobejkine
GETA, BP 68
Universite de Grenoble
38402 Sint-Martin-d&apos;Heres, France
COLING&apos;86, pp. 423-428
</table>
<subsectionHeader confidence="0.902197">
Indexage Lexical au GETA
Jedrzej Bukowski
</subsectionHeader>
<bodyText confidence="0.999802">
Les systemes transformationnels utilisent des processus deductifs d&apos;une
approche differente des systemes utilises en intelligence artificielle. A tray-
ers une comparaison du language Proglog et du language Sygmart, il est
montre comment realiser dans les systemes transformationnels des applica-
tions utilisant des raisonnements et des bases de connaissances.
CRITAC (CRITiquing using ACcumulated knowledge) is an experimental
expert system for proofreading Japanese text. It detects mistypes, Kana-
to-Kanji misconversions, and stylistic errors. This system combines
Prolog-coded heuristic knowledge with conventional Japanese text proc-
essing techniques which involve heavy computation and access to large
language data bases.
Traditionally, text is stored on computers as a stream of characters. The
goal of this research is to store text in a form that facilitates word manipu-
lation whilst reducing storage space. A word list with syntactic linear
ordering is stored and words in a text are given two-byte integer codes that
point to their respective positions in this list. The implementation of the
encoding scheme is described and the performance statistics of this encod-
ing scheme are presented.
BetaText can be described as an event driven production system, in which
(combinations of) text events lead to certain actions, such as the printing
of sentences that exhibit certain, say, syntactic phenomena. The analysis
mechanism used allows for arbitrarily complex parsing, but is particularly
suitable for finite state parsing. A careful investigation of what is actually
needed in linguistically relevant text processing resulted in a rather small
but carefully chosen set of &amp;quot;elementary actions&amp;quot; to be implemented.
In the framework of Machine (aided) Translation systems, two types of
lexical knowledge are used, &amp;quot;natural&amp;quot; and &amp;quot;formal&amp;quot;, in the form of on-line
terminological resources for human translators or revisors and of coded
dictionaries for Machine Translation proper.
A new organization is presented, which allows one to integrate both
types in a unique structure, called &amp;quot;fork&amp;quot; integrated dictionary, or FID. A
given FID is associated with one natural language and may give access to
translations into several other languages.
The FIDs associated with languages Li and L2 contain all information
necessary to generate coded dictionaries of M(a)T systems translating from
Li into L2 or vice-versa. The skeleton of a FID may be viewed as a clas-
sical bilingual dictionary. Each item is a tree structure, constructed by
taking the &amp;quot;natural&amp;quot; information (a tree) and &amp;quot;grafting&amp;quot; onto it some
&amp;quot;formal&amp;quot; information.
Various aspects of this design are refined and illustrated by detailed
examples, several scenarios for the construction of FIDs are presented, and
some problems of organization and implementation are discussed. A
prototype implementation of the FID structure is underway in Grenoble.
L&apos;aspect lexicographique de la traduction assist&amp; par ordinateur est
present&amp; et illustre par des exemples de traduction du russe en frangais
</bodyText>
<table confidence="0.889201166666667">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 117
The FINITE STRING Newsletter Abstracts of Current Literature
GETA, BP 68
Universite Scientifique et Medicate de
Grenoble
38402 Saint-Martin-d&apos;Heres, France
</table>
<figure confidence="0.857133931818182">
COLING&apos;86, pp. 429-431
Experiments with an MT-Directed Lexical
Knowledge Bank
B.C. Papegaaij, V. Sadler, A.P.M. Witkam
BSO/Research
Bureau voor Systeemontwikkeling
P.O. Box 8348
3503 RH Utrecht, The Netherlands
COLING&apos;86, pp. 432-434
A Word Database for Natural Language
Processing
Brigitt Barnett, Hubert Lehmann,
Magdalena Z,oeppritze
IBM Scientific Center
TiergartenstraBe 15
6900 Heidelberg,
Federal Republic of Germany
COLING&apos;86, pp. 435-440
Lexical Database Design: The Shakes-
peare Dictionary Model
H. Joachim Neuhaus
Westfalische Wilhelms-Universitat, FB 12
D-4400 Munster, West Germany
COLING&apos;86, pp. 441-444
An Attempt to Automatic Thesaurus
Construction from an Ordinary Japanese
Language Directory
Hiroaki Tsurumaru
Department of Electronics
Nagasaki University
Nagasaki 852, Japan
Toru Hitaka, She Yoshida
Department of Electronics
Kyushu University 36
Fujuoka 812, Japan
COLING&apos;86, pp. 445-447
Acquisition of Knowledge Data by
Analyzing Natural Language
Yasuhito Tanaka
Himeji College
1-1-12 Shinzaike Honmachi
Himeji City Hyogoken
670 Japan
Sho Yoshida
</figure>
<footnote confidence="0.9136762">
Kyushu University
6-10-1 Hakozaki Higashiku
Fukuoka City Fukuokaken
812 Japan
COLING&apos;86, pp. 448-450
</footnote>
<bodyText confidence="0.998672677419355">
realisee par le GETA a Grenoble.
A crucial test for any MT system is its power to solve lexical ambiguities.
The size of the lexicon, its structural principles, and the availability of
extra-linguistic knowledge are the most important aspects in this respect.
This paper outlines the experimental development of the SWESIL system: a
structured lexicon-based word expert system designed to play a pivotal role in
the process of Distributed Language Translation which is being developed
in the Netherlands. It presents SWESIL&apos;s organizing principles, gives a
short description of the present experimental set-up, and shows how
SWESIL is being tested at this moment.
The paper describes the design of a fair sized lexical data base that is to be
used with a natural language based expert system with German as the
language of interaction. Sources for entries and tools for constructing and
maintaining the database are discussed, as well as the information needed
in the lexicon for the purposes of syntactic and semantic processing.
This paper describes the data and presents some preliminary design consid-
erations along with a sample schema.
How to obtain hierarchical relations (e.g., superordinate-hyponym relation,
synonym relation) is one of the most important problems for thesaurus
construction. A pilot system for extracting these relations automatically
from an ordinary Japanese language dictionary (Shinmeikai Kokugojiten,
published by Sansei-do, in machine readable form) is given. The features
of the definition sentences in the dictionary, the mechanical extraction of
the hierarchical relations, and the estimation of the results are discussed.
Automatic identification of homonyms in kana-to-kanji conversion systems
and of multivocal words in machine translation systems cannot be suffi-
ciently implemented by the mere combination of grammar and word
dictionaries. This calls for a new concept of knowledge data. What the
new knowledge data is and how it can be acquired are mentioned in the
paper. In natural language research, active discussion has been made with-
in the framework of knowledge and samples of knowledge.
</bodyText>
<page confidence="0.93042">
118 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<table confidence="0.923577111111111">
The FINITE STRING Newsletter Abstracts of Current Literature
Model for Lexical Knowledge Base
Michio Isoda, Hideo Aiso
Faculty of Science and Technology
Keio University
Noriyuki Kamibayashi, Yoshifumi Matsunaga
System Technology Laboratory
Fuji Xerox Co., Ltd.
COLING 86, pp. 451-453
</table>
<subsectionHeader confidence="0.922867666666667">
User Specification of Syntactic Case
Frames in TELI, A Transportable, User-
Customized Natural Language Processor
</subsectionHeader>
<figure confidence="0.813914846153846">
Bruce W. Ballard
AT&amp;T Bell Laboratories
600 Mountain Avenue
Murray Hill, NJ 07974
COLING&apos;86, pp. 454-460
Functional Structures for Parsing Depend-
ency Constraints
H. Jiippinen, A. Lehtola, K. Valkonen
SITRA Foundation
P.O. Box 329
Helsinki, Finland
and Helsinki University of Technology
COLING &apos;86, pp. 461-463
</figure>
<footnote confidence="0.30076825">
Controlled Active Procedures as a Tool for
Linguistic Engineering
Heinz-Dirk Luckhardt, Manfred Thiel
Sonderforschungsbereich 100
</footnote>
<bodyText confidence="0.998044169811321">
This paper describes a model for a lexical knowledge base (LKB). An LKB
is a knowledge base management system (KBMS) which stores various
kinds of dictionary knowledge in a uniform framework and provides multi-
ple viewpoints to the stored knowledge.
KBMSs for natural language knowledge will be fundamental components
of knowledgeable environments where non-computer professionals can use
various kinds of support tools for document preparation or translation.
However, basic models for such KBMSs have not been established yet.
Thus, we propose a model for an LKB focusing on dictionary knowledge
such as that obtained from machine-readable dictionaries.
When an LKB is given a key from a user, it accesses the stored know-
ledge associated with that key. In addition to conventional direct retrieval,
the LKB has a more intelligent access capability to retrieve related know-
ledge through relationships among knowledge units. To represent complex
and irregular relationships, we employ the notion of implicit relationships.
In contrast to conventional database models where relationships between
data items are statically defined at data generation time, the LKB extracts
relationships dynamically by interpreting the contents of stored knowledge
at run time. This makes the LKB more flexible; users can add new func-
tions or new knowledge incrementally at any time. The LKB also has the
capability to define and construct new virtual dictionaries from existing
dictionaries. Thus users can define their own customized dictionaries suit-
able for their specific purposes.
The proposed model provides a logical foundation for building flexible
and intelligent LKBs.
In this paper, we present methods that allow the users of a natural
language processor (NLP) to define, inspect, and modify any case frame
information associated with the words and phrases known to the system.
An implementation of this work forms a critical part of the Transportable
English-Language Interface (TELI) system. However, our techniques have
enabled customization capabilities largely independent of the specific NLP
for which information is being acquired.
The primary goal of the syntactic acquisitions of TELI is to redress the
fact that many NL prototypes have failed (1) to make known to users
exactly what inputs are allowed (e.g., what words and phrases are defined)
and (2) to meet the needs of a given user or group of users (e.g., appropri-
ate vocabulary, syntax, and semantics). Experience has shown that neither
users nor system designers can predict in advance all the words, phrases,
and associated meanings that will arise in accessing a given data base (cf.,
Tennant 1977). Thus, we have chosen to make TELI &amp;quot;transportable&amp;quot; in an
extreme sense, where customizations may be performed (1) by end users,
as opposed to computer professionals, and (2) at any time during English
processing.
This paper outlines a high-level language FUNDPL for expressing func-
tional structures for parsing dependency constraints. The goal of the
language is to allow a grammar writer to pin down his or her grammar with
minimal commitment to control. FUNDPL interpreter has been imple-
mented on top of a lower-level language DPL, which we had implemented
earlier.
Controlled active procedures are productions that are grouped under and
activated by units called &amp;quot;scouts&amp;quot;. Scouts are controlled by units called
&amp;quot;mission&amp;quot;, which also select relevant sections from the data structure for
rule application. Following the problem reduction method, the parsing
</bodyText>
<figure confidence="0.8568763">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 119
The FINITE STRING Newsletter Abstracts of Current Literature
&amp;quot;Elektronische Sprachforschung&amp;quot;
Universitat des Saarlandes
D-6600 Saarbrticken 11
Bundesrepublik Deutschland
COLING&apos;86, pp. 464-469
A New Predictive Analyzer of English
Hiroyuki Musha
Department of Information Science
</figure>
<figureCaption confidence="0.501763333333333">
Tokyo Institute of Technology
Ohokayama, Meguro-ku, Tokyo 152, Japan
COLING&apos;86, pp. 470-472
</figureCaption>
<bodyText confidence="0.999914921568628">
problem is subdivided into ever smaller subproblems, each one of which is
represented by a mission. The elementary problems are represented by
scouts. The CAP grammar formalism is based on experience gained with
natural language (NL) analysis and translation by computer in the Sonder-
forschungsbereich at the University of Saarbriicken over the past twelve
years and dictated by the wish to develop an efficient parser for random
NL texts on a sound theoretical basis. The idea has ripened in discussions
with colleagues from the EUROTRA project and is based on what Heinz-
Dieter Maas has developed in the framework of the SUSY-II system.
In the present paper, CAP is introduced as a means of linguistic engi-
neering (cf , Simmons 1985), which covers aspects like rule writing, pars-
ing strategies, syntactic and semantic representation of meaning,
representation of lexical knowledge, etc.
Aspects of syntactic predictions made during the recognition of English
sentences are investigated. We reinforce Kuno&apos;s original predictive analyz-
er by introducing five types of predictions. For each type of prediction, we
discuss and present its necessity, its description method, and recognition
mechanisms. We make use of three kinds of stacks whose behavior is
specified by grammar rules in an extended version of Greibach normal
form. We also investigate other factors that affect the predictive recogni-
tion process, i.e., preferences among syntactic ambiguities and necessary
amount of lookahead. These factors as well as the proposed handling
mechanisms of predictions are tested by analyzing two kinds of articles. In
our experiment, more than seventy percent of sentences are recognized
and looking two words ahead seems to be the critical length for the predic-
tive recognition.
Current (computational) linguistic theories have developed specific formal-
isms for representing linguistic phenomena such as unbounded dependen-
cies, relative, etc. In this contribution we present a model of linguistic
structures storing and accessing, which accounts for the same phenomena
in a procedural way. Such a model has been implemented in the frame of
an ATN parser.
The properties of distributed representations and memory systems are
explored as a potential basis for non-deterministic parsing mechanisms.
The structure of a distributed chart parsing representation is outlined.
Such a representation encodes both immediate-dominance and terminal
projection information on a single composite memory vector. A parsing
architecture is described which uses a permanent store of context-free rule
patterns encoded as split composite vectors, and
two interacting working memory units. These latter two units encode
vectors which correspond to the active and inactive edges of an active
chart parsing scheme. This type of virtual parsing mechanism is compat-
ible with both a macro-level implementation based on standard sequential
processing and a micro-level implementation using a massively parallel
architecture.
The research to be discussed here differs from previous work in that it
explores the properties of distributed representations as a basis for
constructing parallel parsing architectures. Rather than being represented
by localized networks of processing units, the grammar rules are encoded
as patterns which have their effect through simple, yet well-specified,
forms of interaction. The aim of the research is to devise a virtual machine
</bodyText>
<figure confidence="0.733937955555556">
Generalized Memory Manipulating Actions
for Parsing Natural Language
Irina Prodanof
Istituto di Linguistica Computazionale
CNR-Pisa
Giacomo Ferrari
Department of Linguistics
University of Pisa
COLING 86, pp. 473-4 75
Distributed Memory: A Basis for Chart
Parsing
Jon M. Slack
Human Cognition Research Laboratory
Open University
Milton Keynes, MK7 6AA England
COLING 86, pp. 476-481
120 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
The FINITE STRING Newsletter Abstracts of Current Literature
for parsing context-free languages based on the mutual interaction of rela-
tively simple memory components.
The Treatment of Movement Rules in a
LFG Parser
Hans-Ulrich Block, Hans Haugeneder
Siemens AG, Mtinchen
ZT ZTI INF, West Germany
COLING&apos;86, pp. 482-486
A Concept of Derivation for LFG
Jurgen Wedekind
Department of Linguistics
University of Stuttgart
West Germany
COLING&apos;86, pp. 487-489
Incremental Construction of C- and
F-Structure in a LFG Parser
Hans-Ulrich Boock, Rudolf Hunze
ZTI INF 3
Siemens AG
Munchen, West Germany
COLING&apos;86, pp. 490-493
Getting Things Out of Order
Laus Netter
Department of Linguistics
University of Stuttgart
West Germany
COLING&apos;86, pp. 494-496
</figure>
<bodyText confidence="0.98825462745098">
In this paper we propose a way of treating long-distance movement
phenomena as exemplified in (1) in the framework of an LFG-based
parser.
(1) Who do you think Peter tried to meet
&apos;You think Peter tried to meet who&apos;
We therefore concentrate first on the theoretical status of so-called wh-
or long-distance movement in Lexical Functional Grammar (LFG) and in
the Theory of Government and Binding (GB), arguing that a general mech-
anism that is compatible with both LFG and GB treatment of long-distance
movement can be found. Finally, we present the implementation of such a
movement mechanism in a LFG parser.
In this paper a version of LFG will be developed which has only one level
of representation and is equivalent to the modified version of Kaplan,
presented in Bresnan (1982) and Kaplan and Zaenen (1986). The struc-
tures of this monostratal version are f-structures, augmented by additional
information about the derived symbols and their linear order. For these
structures it is possible to define an adequate concept of direct derivability
by which the derivation process becomes more efficient, as the f-descrip-
tion solution algorithm is directly simulated during the derivation of these
structures, instead of being postponed. Apart from this, it follows from
this reducibility that LFG as a theory in its present form does not make use
of the c-structure information that goes beyond the mere linear order of
the derived symbols.
In this paper we present a parser for Lexical Function Grammar (LFG)
which is characterized by incrementally constructing the c- and f-structure
of a sentence during the parse. We then discuss the possibilities of the
earliest check on consistency, coherence, and completeness. Incremental
construction of f-structure leads to an early detection and abortion of
incorrect paths and so increases parsing efficiency. Furthermore those
semantic interpretation processes that operate on partial structures can be
triggered at an earlier state. This also leads to a considerable improvement
in parsing time. LFG seems to be well suited for such an approach because
it provides for locality principles by the definition of coherence and
completeness.
One of the most characteristic features of German word order seems to be
a contrast between fixed ordering rules concerning the order of verbal
elements and a much more variable ordering of their corresponding nomi-
nal arguments. As a consequence, German word order seems to yield a
large number of phenomena that may be classified as &amp;quot;unbounded&amp;quot; or
&amp;quot;long-distance dependencies&amp;quot;, without necessarily involving wh-consti-
tuents or &amp;quot;movement&amp;quot; across sentence boundaries. Whereas in traditional
LFG long-distance dependencies are treated by means of constituent
control, we will follow a recent proposal by Kaplan and Zaenen (1986) to
give up the constraint known as &amp;quot;functional locality&amp;quot; and instead allow
regular expressions to appear as functional schemata annotated to c-struc-
ture rules. Exploiting the principles of completeness and coherence we will
thus be able to cope even with absolutely free word order without the need
of generating empty terminal nodes at all. The empirical assumption,
underlying the proposed analysis in its most radical form, is the hypothesis
that (with very few exceptions) the nominal arguments have to appear on
the left of the verb by which they are assigned case. We will restrict the
</bodyText>
<figure confidence="0.81047844">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 121
The FINITE STRING Newsletter Abstracts of Current Literature
TOPIC Essentials
Udo Hahn, Ulrich Reimer
Universitat Konstanz
Informationswissenschaft
Postfach 5560
D-7750 Konstanz, F.R.G.
COLING&apos;86, pp. 497-503
Two Approaches to Commonsense Infer-
encing for Discourse Analysis
Marc Dymetman
Universite Scientifique et Medicale de
Grenoble
Groupe d&apos;Etudes pour la Traduction
Automatique B.P. 68
38042 Saint Martin d&apos;I-Mres, France
COLING&apos;86, pp. 511-514
Speech Acts of Assertion in Cooperative
Informational Dialogue
J.S. Kononenko
Al Laboratory, Computer Center
Siberian Division of the USSR Ac. Sci
Novosibirsk 630090, USSR
COLING 86, pp. 515-519
</figure>
<bodyText confidence="0.999796760869565">
discussion to sentences with one finite verb as well as to subcategorized
nominal arguments, largely ignoring ADJuncts.
An overview of TOPIC is provided, a knowledge-based text information
system for the analysis of German-language texts. TOPIC supplies text
condensates (summaries) on variable degrees of generality and makes
available facts acquired from the texts. The presentation focuses on the
major methodological principles underlying the design of TOPIC: a frame
representation model that incorporates various integrity constraints, text
parsing with focus on text cohesion and text coherence properties of ex-
pository texts, a lexically distributed semantic text grammar in the format
of word experts, a model of partial text parsing, and text graphs as appro-
priate representation structures for text condensates.
The purpose of this paper is to analyse the phenomenon of nonmonoton-
icity in a natural language and to formulate a number of general principles
which should be taken into consideration while constructing a discourse
oriented nonmonotonic formalism.
A model of Japanese honorific expressions in situation semantics is
proposed. Situation semantics provides considerable power for analyzing
the complicated structure of Japanese honorific expressions. The main
Teature of this model is a set of basic rules for context switching in honorif-
ic sentences. Mizutani&apos;s theory of Japanese honorifics is presented and
incorporated in the model which has been used to develop an experimental
system capable of analyzing honorific context. Some features of this
system are described.
The dominant philosophy regarding the formalization of Commonsense
Inferencing in the physical domain consists in the exploitation of the
&amp;quot;tarskian&amp;quot; scheme axiomatization &lt;—&gt; interpretation borrowed from
mathematical logic. The commonsense postulates constitute the axiomati-
zation, and the real world provides the &amp;quot;model&amp;quot; for this axiomatization.
The observation of the effective activity of linguistic communication and
of the commonsense inferencing processes which are involved in it show
the unacceptability of this scheme.
An alternative is proposed, where the notion of &amp;quot;conceptual category&amp;quot;
plays a principal role, and where the principle of logical adequation of an
axiomatization to a model is replaced by a notion of &amp;quot;projection&amp;quot; of a
conceptual structure onto the observed reality.
Dialogue systems should provide a cooperative informational dialogue
aimed at knowledge sharing. In the paper speech acts of assertion (SAA)
are assumed to be the means of achieving this goal. A typology of SAAs is
proposed which reflects certain cognitive aspects of communicative situ-
ation at different stages of mutual informing process. Information constit-
uents of the type assertions are formally described to represent a current
cognitive state of the speaker&apos;s knowledge base, each proposition in it
being characterized by a subjective verisimilitude evaluation. The general
scheme of information flow in the cooperative dialogue is considered.
With regard to this scheme the dialogue functions of SAAs are discussed.
</bodyText>
<figure confidence="0.884941435897436">
Towards Discourse-Oriented Nonmono-
tonic System
Barbara Dunin-Keplicz, Witold Lukaszewicz
Institute of Informatics
Warsaw University
P.O. Box 1210
00-901 Warszawa, Poland
COLING 86. pp. 504-506
Japanese Honorifics and Situation
Semantics
R. Sugimura
Institute for New Generation Computer
Technology (ICOT) Japan
COLING&apos;86, pp. 507-510
Pragmatic Considerations in Man-Machine This paper presents nothing that has not been noted previously by research
Discourse in Artificial Intelligence but seeks to gather together various ideas that
122 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
The FINITE STRING Newsletter Abstracts of Current Literature
Walther v. Hahn
Research Unit for Information Science and
Artificial Intelligence
University of Hamburg
D-2000 Hamburg 13, West Germany
COLING&apos;86, pp. 520-527
Formal Specification of Natural Language
Syntax using Two-Level Grammar
Barret R. Bryant, Dale Johnson,
Galanjaninath Edupuganty
Department of Computer and Information
Science
The University of Alabama
Birmingham, Alabama 35294
COLING 86, pp. 527-533
On Formalizations of Marcus&apos;s Parser
R. Nozohoor-Farshi
Department of Computing Science
University of Alberta
Edmonton, Canada T6G 2H1
COLING&apos;86, pp. 533-535
</figure>
<table confidence="0.910850083333333">
A Grammar Used for Parsing and Gene-
ration
Jean-Marie Lancd, Nathalie Simonin
CAP Sogeti Innovation
129, rue de PUniversite
75007 Paris, France
Francois Rousselot
University of Strasbourg II
22, rue Descartes
67084, Strasbourg, France
COLING&apos;86, pp. 536-539
BUILDRS: An Implementation of DR
</table>
<sectionHeader confidence="0.530981" genericHeader="method">
Theory and LFG
</sectionHeader>
<subsectionHeader confidence="0.885969">
Hajime Wada
Department of Linguistics
Nicholas Asher
</subsectionHeader>
<affiliation confidence="0.966464666666667">
Department of Philosophy, Center for
Cognitive Science
The University of Texas at Austin
</affiliation>
<footnote confidence="0.314016">
COLING&apos;86, pp. 540-545
</footnote>
<note confidence="0.4472625">
A Prolog Implementation of Government-
Binding Theory
Robert J. Kuhns
Artificial Intelligence Center
</note>
<bodyText confidence="0.999799545454546">
have arisen in the literature. It collects those arguments which are in my
view crucial for further progress and is intended only as a reminder of
insights which might have been forgotten for some time.
The two-level grammar is investigated as a notation for giving formal spec-
ification of the context-free and context-sensitive aspects of natural
language syntax. In this paper, a large class of English declarative
sentences, including post-noun-modification by relative clauses, is formal-
ized using a two-level grammar. The principal advantages of two-level
grammar are: 1) it is very easy to understand and may be used to give a
formal description using a structured form of natural language; 2) it is
formal with many well-known mathematical properties; and 3) it is directly
implementable by interpretation. The significance of the latter fact is that
once we have written a two-level grammar for natural language syntax, we
can derive a parser automatically without writing any additional specialized
computer programs. Because of the ease with which two-level grammars
may express logic and their Turing computability, we expect that they will
also be very suitable for future extensions to semantics and knowledge
representation.
LR(k,t), BCP(m,n), and LRRL(k) grammars, and their relations to Marcus
parsing are discussed.
This text presents the outline of a system using the same grammar for pars-
ing and generating sentences in a given language. This system has been
devised for a &amp;quot;multilingual document generation&amp;quot; project.
The Functional Grammar notation described here allows a full symme-
try between parsing and generating. Such a grammar may be read easily
from the point of view of the parsing and from the point of view of the
generation. This allows one to write only one grammar of a language,
which minimizes the linguistics costs in a multilingual scheme.
This paper examines a particular Prolog implementation of Discourse
Representation theory (DR theory) constructed at the University of Texas.
The implementation also contains a Lexical Functional Grammar parser
that provides f-structures: these f-structures are then translated into the
semantic representations posited by DR theory, structures which are
known as Discourse Representation Structures (DRSs). Our program
handles some linguistically interesting phenomena in English such as (i)
scope ambiguities of singular quantifiers, (ii) functional control phenome-
na, and (iii) long distance dependencies. Finally, we have implemented an
algorithm for anaphora resolution. Our goal is to use purely linguistically
available information in constructing a semantic representation of
discourse as far as is feasible and to forego appeals to world knowledge.
A parser founded on Chomsky&apos;s Government-Binding Theory and imple-
mented in Prolog is described. By focussing on systems of constraints as
proposed by this theory, the system is capable of parsing without an elabo-
rate rule set and subcategorization features on lexical items. In addition to
</bodyText>
<table confidence="0.92279405">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 123
The FINITE STRING Newsletter Abstracts of Current Literature
Arthur D. Little, Inc.
Cambridge, MA 02140
COLING 86, pp. 546-550
A Lexical Functional Grammar System in
Prolog
Andreas Eisele, Jochen Dorre
Department of Linguistics
University of Stuttgart
West Germany
COLING&apos;86, pp. 551-553
Knowledge Structures for Natural
Language Generation
Paul S. Jacobs
Knowledge-Based Systems Branch
General Electric Corporate Research and
Development
Schenectady, NY 12301
COLING&apos;86, pp. 554-559
</table>
<subsectionHeader confidence="0.532953">
Semantic-Based Generation of Japanese
German Translation System
</subsectionHeader>
<figure confidence="0.64625075">
K. Hanakata
Institut f. Informatik
University of Stuttgart
Herdweg 51
D-7000 Stuttgart 1, F.R. Germany
A. Lesniewski
Standard Elektrik Lorenz AG
Ostendstrasse 3
D-7530 Pforzheim, F.R. Germany
S. Yokoyama
Electrotechnical Laboratory
Umezono, Sakuramura, Nihari
Ibaraki 305, Japan
COLING&apos;86 560-562
Synthesizing Weather Forecasts from
Formatted Data
R. Kittredge, A. Polguere
Ddpartement de Linguistique
Universite de Montréal
E. Goldberg
</figure>
<footnote confidence="0.611752666666667">
Atmosphere Environment Services
Environment Canada, Toronto
COLING&apos;86, pp. 563-565
</footnote>
<subsectionHeader confidence="0.853014">
From Structure to Process
Michael Zock, Gerard Sabah
</subsectionHeader>
<bodyText confidence="0.999723075">
the parse, theta, binding, and control relations are determined simultane-
ously.
This paper describes a system in Prolog for the automatic transformation
of a grammar, written in LFG formalism, into a DCG-based parser. It
demonstrates the main principles of the transformation, the representation
of f-structures and constraints, the treatment of long-distance dependen-
cies, and left recursion.
Finally some problem areas of the system and possibilities for overcom-
ing them are discussed.
The development of natural language interfaces to Artificial Intelligence
systems is dependent on the representation of knowledge. A major imped-
iment to building such systems has been the difficulty in adding sufficient
linguistic and conceptual knowledge to extend and adapt their capabilities.
This difficulty has been apparent in systems which perform the task of
language production, i.e., the generation of natural language output to
satisfy the communicative requirements of a system.
The Ace framework applies knowledge representation fundamentals to
the task of encoding knowledge about language. Within this framework,
linguistic and conceptual knowledge are organized into hierarchies, and
structured associations are used to join knowledge structures that are meta-
phorically or referentially related. These structured associations permit
specialized linguistic knowledge to derive partially from more abstract
knowledge, facilitating the use of abstractions in generating specialized
phrases. This organization, used by a generator called KING (Knowledge
INtensive Generator), promotes the extensibility and adaptability of the
generation system.
Project SEMSYN*** achieved a state where a prototype system generates
German texts on the basis of the semantic representation produced from
Japanese texts by ATLAS/II of Fujitsu Laboratory. This paper describes
some problems that are specific to our semantic-based approach and some
results of the evaluation study that has been made by the Germanist group.
This paper describes a system (RAREAS) which synthesizes marine weath-
er forecasts directly from formatted weather data. Such synthesis appears
feasible in certain natural sublanguages with stereotyped text structure.
RAREAS draws on several kinds of linguistic and non-linguistic knowledge
and mirrors a forecaster&apos;s apparent tendency to ascribe less precise tempo-
ral adverbs to more remote meteorological events. The approach can easi-
ly be adapted to synthesize bilingual or multi-lingual texts.
This paper describes an implemented tutoring system designed to help
students to generate clitic-constructions in French. While showing various
</bodyText>
<page confidence="0.905964">
124 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<table confidence="0.865123205128205">
The FINITE STRING Newsletter Abstracts of Current Literature
LIMSI - Langues NatureIles
B.P. 30 - Orsay Cedex / France
Christophe Alviset
INSSEE - 3, ay. P. Larousse
94241 Malakoff - France
COLING&apos;86, pp. 566-569
Generating a Coherent Text Describing a
Traffic Scene
Hans-Joachim Novak
Fachbereich Informatik
Universitat Hamburg
D-2000 Hamburg 13, West Germany
COLING&apos;86, pp. 570-575
Generating Natural Language Text in a
Dialog System
Mare Koit
Department of Programming
2 Juhan Liivi Street
Madis Saluveer
Artificial Intelligence Laboratory
78 Tiigi Street
Tartu State University
202400 Tartu Estonia USSR
COLING&apos;86, pp. 576-580
Generating English Paraphrases from
Formal Relational Calculus Expressions
A.N. De Roeck, B.G.T. Lowden
University of Essex
Wivenhoe Park
Colchester, United Kingdom
COLING&apos;86, pp. 581-583
The Computational Complexity of
Sentence Derivation in Functional
Unification Grammar
Graeme Ritchie
Department of Artificial Intelligence
University of Edingburgh
Edinburgh 3H1 1HN
</table>
<footnote confidence="0.695573333333333">
COLING&apos;86, pp. 584-586
Parsing Spoken Language: a Semantic
Caseframe Approach
</footnote>
<note confidence="0.8174915">
Philip J. Hayes, Alexander G. Hauptmann,
Jaime G. Carbonell, Maseru Tomita
</note>
<subsectionHeader confidence="0.595069">
Computer Science Department
</subsectionHeader>
<bodyText confidence="0.9974295">
ways of converting a given meaning structure into its corresponding
surface expression, the system helps not only to discover what data to proc-
ess but also how this information processing should take place. In other
words, we are concerned with efficiency in verbal planning (performance).
Recognizing that the same result can be obtained by various methods,
the student should find out which one is best suited to the
circumstances (what is known, task demands, etc.). Informational states,
hence the processor&apos;s needs, may vary to a great extent, as may his strate-
gies or cognitive styles. In consequence, in order to become an efficient
processor, the student has to acquire not only STRUCTURAL or RULE
KNOWLEDGE but also PROCEDURAL KNOWLEDGE (skill).
With this in mind we have designed three modules in order to foster a
reflective, experimental attitude in the learner, helping him to discover
insightfully the most efficient strategy.
If a system that embodies a reference semantic for motion verbs and prep-
ositions is to generate a coherent text describing the recognized motions, it
needs a decision procedure to select the events. In NAOS event selection is
done by use of a specialization hierarchy of motion verbs. The strategy of
anticipated visualization is used for the selection of optional deep cases.
The system exhibits low-level strategies which are based on verb inherent
properties that allow the generation of a coherent descriptive text.
The paper deals with generation of natural language text in a dialog
system. The approach is based on principles underlying the dialog system
TARLUS under development at Tartu State University. The main problems
concerned are the architecture of a dialog system and its knowledge base.
Much attention is devoted to problems which arise in answering the user
queries — the problems of planning an answer, the non-linguistic and
linguistic phases of generating an answer.
This paper discusses a system for producing English descriptions (ar
&amp;quot;paraphrases&amp;quot;) of the content of formal relational calculus formulae
expressing a database query. It explains the underlying design motivations
and describes a conceptual model and focus selection mechanism necessary
for delivering coherent paraphrases. The general paraphrasing strategy is
discussed, as are the notions of &amp;quot;desirable&amp;quot; paraphrase and &amp;quot;paraphrasable
query&amp;quot;. Two examples are included. The system was developed and
implemented in Prolog at the University of Essex under a grant from ICL.
Functional unification (FU) grammar is a general linguistic formalism based
on the merging of feature-sets. An informal outline is given of how the
definition of derivation within FU grammar can be used to represent the
satisfiability of an arbitrary logical formula in conjunctive normal form.
This suggests that the generation of a structure from an arbitrary FU gram-
mar is NP-hard, which is an undesirably high level of computational
complexity.
Parsing spoken input introduces serious problems not present in parsing
typed natural language. In particular, indeterminacies and inaccuracies of
acoustic recognition must be handled in an integral manner. Many tech-
niques for parsing typed natural language do not adapt well to these extra
demands. This paper describes an extension of semantic caseframe parsing
</bodyText>
<table confidence="0.724750727272727">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 125
The FINITE STRING Newsletter Abstracts of Current Literature
Carnegie-Mellon University
Pittsburgh, PA 15213
COLING&apos;86, pp. 58 7-5 92
Divided and Valency-Oriented Parsing in
Speech Understanding
Gerh. Th. Niedermair
Zt ZTI INF, Siemens AG
Otto-Hahn-Ring 6
8000 Mtinchen 83
COLING&apos;86, pp. 5 93 -5 95
The Role of Semantic Processing in an
Automatic Speech Understanding System
Astrid Brietzmann, Ute Ehrlich
Lehrstuhl fuer Informatik 5
(Mustererkennung)
Universitaet Erlangen-Nuernberg
Martesstr. 3, 8520 Erlangen, F.R. Germany
COLING&apos;86, pp. 5 96-5 98
Synthesis of Spoken Message from
Semantic Representations
Laurence Danlos, Eric LaPorte
Laboratoire d&apos;Automatique Documentaire et
Linguistique
Universite de Paris 7
2, place Jussieu
75251 Paris Cedex 05
Franoise Emerard
Centre National d&apos;Etudes des
Telecommunications
22301 Lannion Cedex
COLING&apos;86, pp. 599-604
</table>
<subsectionHeader confidence="0.8032875">
The Procedure to Construct a Word
Predictor in a Speech Understanding
System from a Task-Specific Grammar
Defined in a CFG or a DCG
Yasuhisa Niimi, Shigeru Uzuhara,
Yutaka Kobayashi
</subsectionHeader>
<affiliation confidence="0.940686">
Department of Computer Science
Kyoto Institute of Technology
</affiliation>
<construct confidence="0.1156255">
Matsugasaki, Sakyo-ku, Kyoto 606, Japan
COLING&apos;86, pp. 605-607
</construct>
<subsectionHeader confidence="0.808562">
The Role of Phonology in Speech
Processing
Richard Wiese
Seminar fur Allgemeine Sprachwissenschaft
</subsectionHeader>
<bodyText confidence="0.9999865">
to restricted-domain spoken input. The semantic caseframe grammar
representation is the same as that used for earlier work on robust parsing
of typed input. Due to the uncertainty inherent in speech recognition, the
caseframe grammar is applied in a quite different way, emphasizing island
growing from caseframe headers. This radical change in application is
possible due to the high degree of abstraction in the caseframe represen-
tation. The approach presented was tested successfully in a preliminary
implementation.
A parsing scheme for spoken utterances is proposed that deviates from
traditional &amp;quot;one go&amp;quot; left to right sentence parsing in that it divides the
parsing process first into two separate parallel processes. Verbal constitu-
ents and nominal phrases (including prepositional phrases) are treated
separately and only brought together in an utterance parser. This allows
especially the utterance parser to draw on valency information right from
beginning when amalgamating the nominal constituents to the verbal core
by means of binary sentence rules. The paper also discusses problem of
representing the valency information in case-frames arising in a spoken
language environment.
We present the semantics component of a speech understanding and
dialogue system that was developed at our institute. Due to pronunciation
variabilities and vagueness of the word recognition process, semantics in a
speech understanding system has to resolve additional problems. Its main
task is not only to build up a representation structure for the meaning of an
utterance, as in a system for written input, semantic knowledge is also
employed to decide between alternative word hypotheses, to judge the
plausibility of syntactic structures, and to guide the word recognition proc-
ess by expectations resulting from partial analyses.
A semantic-representation-to-speech system communicates orally the
information given in a semantic representation. Such a system must inte-
grate a text generation module, a phonetic conversion module, a prosodic
module, and a speech synthesizer. We will see how the syntactic informa-
tion elaborated by the text generation module is used for both phonetic
conversion and prosody, so as to produce the data that must be supplied to
the speech synthesizer, namely a phonetic chain including prosodic infor-
mation.
This paper describes a method for converting a task-dependent grammar
into a word predictor of a speech understanding system. Since the word
prediction is a top-down operation, left recursive rules induce an infinite
looping. We have solved this problem by applying an algorithm for
bottom-up parsing.
In this paper, I discuss the role of phonology in the modelling of speech
processing. It will be argued that recent models of nonlinear represen-
tation in phonology should be put to use in speech processing systems
(SPS). Models of phonology aim at the reconstruction of the phonological
</bodyText>
<page confidence="0.935035">
126 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.869547285714286">
The FINITE STRING Newsletter Abstracts of Current Literature
Universitat Düsseldorf
D-4000 Dusseldorf, FRG
COLING 86, pp. 608-611
Computational Phonology: Merged, not
Mixed
Egon Berendsen
Department of Phonetics
University of Utrecht
The Netherlands
Simone Langeneg
Phonetics Laboratory
University of Leyden
The Netherlands
Hugo van Leeuwen
Institute of Perception Research
Eindhoven, The Netherlands
COLING&apos;86, pp. 612-614
Phonological Pivot Parsing
Grzegorz Dogil
Universitat Bielefeld
Fakultat fiir Linguistik und
Literaturwissenschaft
D-4800 Bielefeld, West Germany
COLING 86, pp. 615-617
A Description of the VESPRA Speech
Processing System
Rolf Haberbeck
</figure>
<footnote confidence="0.804201666666667">
FU Berlin, FB Germanistik
D-1000 Berlin 33
TU Berlin, FB Informatik
</footnote>
<bodyText confidence="0.999917979591837">
knowledge that speakers possess and utilize in speech processing. The most
important function of phonology in SPS is, therefore, to put constraints on
what can be expected in the speech stream. A second, more specific func-
tion relates to the particular emphasis of the phonological models
mentioned above and outlined in section 4: It has been realized that many
SPSs do not make sufficient use of the suprasegmental aspects of the
speech signal. But it is precisely in the domain of prosody where nonlinear
phonology has made important progress in our insight into the phonologi-
cal component of language.
From the phonetic point of view, phonological knowledge is higher level
knowledge just as syntactic or semantic information. But since phonologi-
cal knowledge is in an obvious way closer to the phonetic domain than
syntax or semantics, it is even more surprising that phonological knowledge
has been rarely applied systematically in SPS.
Research into text-to-speech systems has become a rather important topic
in the areas of linguistics and phonetics. Particularly for English, several
text-to-speech systems have been established (cf., for example, Hertz
1982, Klatt 1976). For Dutch, text-to-speech systems are being developed
at the University of Nijmegen (cf. Wester 1984) and at the Universities of
Utrecht and Leyden and at the Institute of Perception Research Eindhoven
as well. In this paper we will be concerned with the grapheme-to-phoneme
conversion component as part of the Dutch text-to-speech system which is
being developed in Utrecht, Leyden, and Eindhoven.
There are two basic mysteries about natural language: the speed and ease
with which it is acquired by a child, and the speed and ease with which it is
processed. Similarly to language acquisition, language processing faces a
strong input-data-deficiency problem. When we speak we alter a great
deal in the idealized phonological and phonetic representations. We delete
whole phonemes, we radically change allophones, we shift stresses, we
break up intonational patterns, we insert pauses at the most unexpected
places, etc. If to this crippled &amp;quot;phonological string&amp;quot; we add all the noise
from the surroundings which does not help comprehension either, it is
bewildering that the parser is supposed to recognize anything at all.
However, even in the most difficult circumstances (foreign accent, loud
environment, being drunk, etc.), we do comprehend speech quickly and
efficiently. There must be then some signals in the phonetic string which
are particularly easy to grasp and to process. I call these signals pivots and
call the parsers working with these signals pivot parsers.
I present here an idea of what a fast parser which requires the minimum
of phonologically invariant information might look like. This parser works
in a sequentially-looping manner and the decisions it makes are non-deter-
ministic. It is universally applicable, it is faster, and it seems to be no less
efficient than other phonological parsers that have been proposed.
The VESPRA system is designed for the processing of chains of (not
connected utterances of) wordforms. These strings of wordforms corre-
spond to sentences except that they are not realized in connected speech.
VESPRA means: Verarbeitung und Erkennung gesprochener Sprache
(processing and recognition of speech). VESPRA will be used to control
different types of machines by voice input (for instance: non critical
</bodyText>
<table confidence="0.908980615384615">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 127
The FINITE STRING Newsletter Abstracts of Current Literature
D-1000 Berlin 10
COLING 86, pp. 618-620
Translation by Understanding: A Machine
Translation System LUTE
Hirasato Nomura, Shozo Naito,
Yasuhiro Katagiri, Akira Shimazu
NTT Basic Research Laboratories
Musashino-shi
Tokyo, 180, Japan
COL1NG&apos;86, pp. 621-626
On Knowledge-Based Machine Translation
</table>
<figure confidence="0.896300285714286">
Sergei Nirenburg
Colgate University
Victor Raskin
Purdue University
Allen Tucker
Colgate University
COLING&apos;86, pp. 627-632
</figure>
<table confidence="0.663686857142857">
Another Stride towards Knowledge-Based
Machine Translation
Masaru Tomita, Jaime Carbonell
Computer Science Department
Carnegie-Mellon University
Pittsburgh, PA 15213
COLING 86, pp. 633-638
</table>
<subsectionHeader confidence="0.6048584">
English - Malay Translation System:
a Laboratory Prototype
Loon-Cheong Tong
Computer Aided Translation Project
School of Mathematical and Computer
</subsectionHeader>
<bodyText confidence="0.999944074074074">
control functions in cars and in trucks, voice box in digital telephone
systems, text processing systems, different types of office workstations).
This paper presents a linguistic model for language understanding and
describes its application to an experimental machine translation system
called LUTE. The language understanding model is an interactive model
between the memory structure and a text. The memory structure is hierar-
chical and represented in a frame-network. Linguistic and non-linguistic
knowledge is stored and the result of understanding the text is assimilated
into the memory structure. The understanding process is interactive in that
the text invokes knowledge and the understanding procedure interprets the
text by using that knowledge. A linguistic model, called the Extended Case
Structure model, is defined by adopting three kinds of information: struc-
ture, relation, and concept. These three are used recursively and iteratively
as the basis for memory organization. These principles are applied to the
design and implementation of the LUTE which translates Japanese into -
English and vice versa.
This paper describes the design of the knowledge representation medium
used for representing concepts and assertions, respectively, in a subworld
chosen for a knowledge-based machine translation system. This design is
used in the TRANSPORTATION machine translation project. The know-
ledge representation language, or interlingua, has two components, DIL
and TIL. DIL stands for &apos;dictionary of interlingua&apos; and describes the seman-
tics of a subworld. TIL stands for &apos;text of interlingua&apos; and is responsible
for producing an interlingua text, which represents the meaning of an input
text in the terms of the interlingua. We maintain that involved analysis of
various types of linguistic and encyclopedic meaning is necessary for the
task of automatic translation. The mechanisms for extracting and manipu-
lating and reproducing the meaning of texts will be reported in detail else-
where. The linguistic (including the syntactic) knowledge about source
and target languages is used by the mechanisms that translate texts into
and from the interlingua. Since interlingua is an artificial language, we can
(and do, through TIL) control the syntax and semantics of the allowed
interlingua elements. The interlingua suggested for TRANSPORTATION
has a broader coverage than other knowledge representation schemata for
natural language. It involves the knowledge about discourse, speech acts,
focus, time, space, and other facets of the overall meaning of texts.
Building on the well-established premise that reliable machine translation
requires a significant degree of text comprehension, this paper presents a
recent advance in multi-lingual knowledge-based machine translation
(KBMT). Unlike previous approaches, the current method providesf or
separate syntactic and semantic knowledge sources that are integrated
dynamically for parsing and generation. Such a separation enables the
system to have syntactic grammars, language specific but domain general,
and semantic knowledge bases, domain specific but language general.
Subsequently, grammars and domain knowledge are precompiled automat-
ically in any desired combination to produce very efficient and very thor-
ough real-time parsers. A pilot implementation of our KBMT architecture
using functional grammars and entity-oriented semantics demonstrates the
feasibility of the new approach.
This paper presents the results obtained by an English to Malay computer
translation system at the level of a laboratory prototype. The translation
output obtained for a selected text (secondary school chemistry textbook)
is evaluated using a grading scheme based on ease of post-editing. The
effect of a change in area and typology of text is investigated by comparing
</bodyText>
<page confidence="0.881054">
128 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<table confidence="0.873433727272727">
The FINITE STRING Newsletter Abstracts of Current Literature
Science
Universiti Sains Malaysia
11800 Penang, Malaysia
COLING&apos;86, pp. 639-642
A Prototype Machine Translation based on
Extracts from Data Processing
E. Luctkens, Ph. Fermont
Department of Information Science and
Documentation
Free University of Brussels
Belgium
COLING&apos;86, pp. 643-645
A Prototype English-Japanese Machine
Translation System for Translating IBM
Manuals
Taijiro Tsutsumi
Natural Language Processing
Science Institute, IBM Japan, Ltd.
5-19, Sanban-cho, Chiyoda-ku
Tokyo 102, Japan
COLING 86, pp. 646-648
Construction of a Modular and Portable
Translation System
Fujio Nishida, Yoneharu Fujita,
Shinohu Takamatsu
Department of Electrical Engineering
Faculty of Engineering
University of Osaka Prefecture
Sakai, Osaka, Japan 591
COLING&apos;86, pp. 649-651
When Mariko Talks to Siegfried
Dietmar Rosner
</table>
<footnote confidence="0.60051725">
Projekt SEMSYN, Institut fur Informatik
Universitat Stuttgart, Herdweg 51
D-7000 Stuttgart 1 West Germany
COLING&apos;86, pp. 652-654
</footnote>
<bodyText confidence="0.999337125">
with the translation output obtained for a university level computer science
text. An analysis of the problems which give rise to incorrect translations
is discussed. This paper also provides statistical information on the English
to Malay translation system and concludes with an outline of further work
being carried out on this system with the aim of attaining an industrial
prototype.
The following article presents a prototype for the machine translation of
English into French. ... The prototype aims to provide a diagnostic
study that lays the foundations for further development rather than imme-
diately producing an accurate but limited realization.
By way of experiment, the corpus for translation was based on selected
extracts from computer systems manuals. After studying the basic materi-
al, as well as assessing the various decision criteria, it was decided to
construct a prototype made up of three components: analysis, transfer, and
generation.
Although the prototype was designed with multilingual applications in
mind, it appeared preferable at this stage not to set up a system with inter-
lingua since the elaboration of the interlingua alone would have taken up a
disproportionate amount of time, thus handicapping the development of
the prototype itself.
This paper describes a prototype English-Japanese machine-translation
(MT) system developed at the Science Institute of IBM Japan, Ltd. This
MT system currently aims at the translation of IBM computer manuals. It is
based on a transfer approach in which the transfer phase is divided into
two sub-phrases. English transformation and English-Japanese conver-
sion. An outline of the system and a detailed description of the English-
Japanese transfer method are presented.
This paper has two purposes. One of them is to show a method of
constructing an MT system on a library module basis with the aid of a
programming construction system called L-MAPS. The MT system can be
written in any programming language designated by a user if an appropri-
ate data base and the appropriate functions are implemented in advance.
For example, it can be written in a compiler language like C language,
which is preferable for a workstation with a relatively slow running
machine speed.
The other purpose is to give a brief introduction of a program generat-
ing system called Library-Module Aided Program Synthesizing system
(abbreviated to L-MAPS) running on a library module basis. L-MAPS
permits us to write program specifications in a restricted natural language
like Japanese and converts them to formal specifications. It refines the
formal specifications using the library modules and generates a readable
comment of the refined specification written in the above natural language
every refinement in option. The conversion between formal expressions
and natural language expressions is performed efficiently on a case gram-
mar basis.
In this paper we will report on our experiences from a two and a half
year project that designed and implemented a prototypical Japanese to German
translation system for titles of Japanese papers.
</bodyText>
<table confidence="0.419638444444444">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 129
The FINITE STRING Newsletter Abstracts of Current Literature
Future Directions of Machine Translation
Jun-ichi Tsujii
Department of Electrical Engineering
Kyoto University
Sakyo-ku, Kyoto 606, Japan
COLING&apos;86, pp. 655-668
Discourse, Anaphora, and Parsing
</table>
<subsectionHeader confidence="0.475967">
Mark Johnson
</subsectionHeader>
<bodyText confidence="0.46106">
Center for the Study of Language and Infor-
mation and Department of Linguistics
Stanford University
</bodyText>
<author confidence="0.714731">
Ewan Klein
</author>
<affiliation confidence="0.9439635">
Centre for Cognitive Science
Edinburgh University
</affiliation>
<subsubsectionHeader confidence="0.558259">
COLING&apos;86, pp. 669-675
</subsubsectionHeader>
<bodyText confidence="0.999859058823529">
In this paper, we will discuss several problems concerned with &amp;quot;under-
standing and translation&amp;quot;, especially how we can integrate the two lines
of research, with their different histories and different techniques, into
unified frameworks, and the difficulties we might encounter in at-
tempting such an integration. The discussion will reveal some of the rea-
sons why MT researchers are so separated from research in the other
application fields of NLP. We will also list some of the key problems, both
linguistic and computational, which we encountered during the develop-
ment of our MT systems, and whose resolutions we consider to be of
essential importance for future MT research and development.
Discourse Representation Theory, as formulated by Hans Kamp and
others, provides a model of inter- and intra-sentential anaphoric dependen-
cies in natural language. In this paper, we present a reformulation of the
model which, unlike Kamp&apos;s is specified declaratively. Moreover, it uses
the same rule formalism for building both syntactic and semantic struc-
tures. The model has been implemented in an extension of Prolog, and
runs on a VAX 11/750 computer.
</bodyText>
<subsectionHeader confidence="0.965733">
Selected Dissertation Abstracts
</subsectionHeader>
<bodyText confidence="0.919482172413793">
Compiled by:
Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20209
Bob Krovetz, University of Massachusetts, Amherst, MA 01002
The following are citations selected by title and abstract as being related to computational linguistics or knowledge
representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the
Dissertation Abstracts International (DAI) data base produced by University Microfilms International.
Included are the title; author; university, degree, and, if available, number of pages; DAI subject category chosen by
the author of the dissertation; an abstract; and the UM order number and year-month of entry into the data base.
References are sorted first by DAI subject category and second by author. Citations denoted by an MAI reference do
not yet have abstracts in the data base and refer to abstracts in the published Masters Abstracts International.
Unless otherwise specified, paper or microform copies of dissertations may be ordered from
University Microfilms International
Dissertation Copies
Post Office Box 1764
Ann Arbor, MI 48106
telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042
for Canada: 1-800-268-6090.
Price lists and other ordering and shipping information are in the introduction to the published DAI. An alternate
source for copies is sometimes provided at the end of the abstract.
The dissertation titles and abstracts contained here are published with permission of University Microfilms Interna-
tional, publishers of Dissertation Abstracts International (copyright 1986 by University Microfilms International), and
may not be reproduced without their prior permission.
The Effect of Knowledge Representation
and Psychological Type on Human Under-
standing in the Human-Computer Inter-
face
Wallace Irving Castle
The University of Texas at Arlington Ph.D.
1985, 192 pages
</bodyText>
<affiliation confidence="0.204286">
Business Administration, General
University Microfilms International
</affiliation>
<page confidence="0.799851">
ADG86-07478
</page>
<bodyText confidence="0.999844666666667">
People need to understand the logic of an information system because they
must tell the system what to do, how to do it, and determine what the
system did. Because of the limits of human memory, the logic of the
system must be represented in a form that both people and computers can
use.
One hundred thirty graduate and undergraduate business students
participated in an experiment to evaluate the effect on human understand-
ing of representation type and psychological type. A story represented in
English was compared to a story represented in predicate logic. Psycho-
logical type was measured with the Myers-Briggs Type Indicator. Human
understanding was measured with an inference recognition test. The
hypothesis was that sensing psychological types perform better with predi-
</bodyText>
<page confidence="0.921532">
130 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.684622">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999874052631579">
cate logic than with English while intuitive psychological types perform
better with English than with predicate logic.
The results of the experiment supported the hypothesis that represen-
tation type and psychological type interact to effect human understanding;
however, both sensing and intuitive psychological types performed better
with predicate logic. The interaction occurred because the sensing psycho-
logical types performed as well with English as with predicate logic while
the intuitive psychological types performed well with predicate logic but
poorly with English.
English is not the best representation type for helping the intuitive
psychological type to understand the logic of an information system. The
results of this experiment show that it is not correct to assume that English
is always superior to any other representation regardless of the people
using the system.
In designing the human-computer interface, the alternative represent-
ations should be evaluated using an experimental design to determine their
effect on different psychological types of users. The results of an exper-
iment may show that the representation that is easy for the computer may
also be the best for the people using the system.
</bodyText>
<figure confidence="0.9856242">
Towards a Representation of Lisp
Semantics
Nizar Mohammed Awartani
Lehigh University Ph.D. 1986, 89 pages
Computer Science
University Microfilms International
ADG86-16151
Debugging Programs in a Distributed
System Environment
Peter Charles Bates
University of Massachusetts Ph.D. 1986,
239 pages
Computer Science
University Microfilms International
ADG86-12013
</figure>
<bodyText confidence="0.98874132">
In this dissertation we chose to examine the semantics of a subset of RLISP
and give its formal specifications. In this subset we do not include loops or
recursive procedures.
We defined the language elements which are fundamental to the state-
ments within the language such as symbolic expressions, variables, sym-
bolic expressions with quotation marks and a few others. By this we gain
precision and completeness in RLISP specification at the fundamental level.
We described RLISP syntactic constructs in a consistent way on a single
logical level. The dissertation presents a system of formal rules that permit
the establishment of rigorous proofs using only the uninterpreted program
text. The method we used depended on repeated substitutions for occur-
rences of expressions in a given RLISP program.
To explore the subtlety of RLISP we included an informal description of
the rules and provided several examples illustrating them.
Debugging is an activity that attempts to locate the sources of errors in the
specification and coding of a software system and to suggest possible
repairs that might be made to correct the errors. Debugging complex
distributed programs is a frustrating and difficult task. This is due primari-
ly to the predominance of a low-level, computation-unit view of systems.
This extant perspective is necessarily detail intensive and offers little aid in
dealing with the higher level operational characteristics of a system or the
complexities inherent in distributed systems.
In this dissertation we develop a high-level debugging approach in which
debugging is viewed as a process of creating models of actual behavior
from the activity of the system and comparing these to models of expected
system behavior. The differences between the actual and expected models
can be used to characterize errorful behavior. The basis for the approach
is viewing the activity of a system as consisting of a stream of significant,
distinguishable events that may be abstracted into high-level models of
system behavior. An example is presented to demonstrate the use of event
based model building to investigate an error in a distributed program.
Behavior abstraction and system understanding are characterized as
problems in pattern recognition that must operate in a noisy, uncertain
environment. Pattern recognition in support of behavioral abstraction is
thus shown to be more than a simple parsing exercise. A formal model is
developed for event based behavioral abstraction which provides a basis
for rigorous discussions of debugging as behavior modelling and forms a
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 131
The FINITE STRING Newsletter Abstracts of Current Literature
guide for implementing tools to support debugging in terms of events and
higher level abstractions of system behavior.
A prototype distributed behavior recognition system which has been
constructed to demonstrate and evaluate the feasibility of the EBBA
approach is described. The prototype toolset identifies a range of debug-
ging tools useful for distributed systems. Remote debugging, filtered
remote debugging with preset actions, simple cooperative debugging, and
distributed debugging progressively increase the power of debugging
agents at individual nodes by reducing communication requirements,
increasing overall transparency of the debugging tools, and distributing
debugging tool functionality throughout the system.
</bodyText>
<figure confidence="0.967138733333333">
Semantic Query Optimization in Deductive
Data Bases. (Volumes I and II)
Upendranath Sharma Chakravarthy
University of Maryland Ph.D. 1985,
304 pages
Computer Science
University Microfilms International
ADG86-08788
Visual Programming with Icons
Olivier Bernard Clarisse
Illinois Institute of Technology Ph.D. 1985,
256 pages
Computer Science
University Microfilms International
ADG86-06485
</figure>
<bodyText confidence="0.999930577777778">
This thesis addresses the problem of efficient query evaluation over a
deductive data base and proposes several methods to optimize the evalu-
ation of a query. The problems addressed in this thesis and the solutions
proposed, under the central theme of query optimization, can be discussed
under (i) Techniques for interfacing PROLOG with relational data bases,
(ii) A formalism for semantic query optimization using integrity con-
straints, and (iii) Multiple query evaluation in deductive data bases.
We propose several ways in which a PROLOG interpreter can be modi-
fied so that it can be interfaced effectively with a database system. Three
solutions, namely, a simple modification to the PROLOG query evaluation
strategy to accomplish the complied approach, a meta-level interpreter
without any modifications to PROLOG and a set evaluation strategy using
tables, are proposed in this thesis.
A general framework in which domain specific knowledge — in the form
of integrity constraints — is used to transform a query, is proposed in this
thesis and is termed semantic query optimization. The process of semantic
query optimization is carried out in two phases. Initially, the axioms of a
data base are semantically compiled, wherein integrity constraints are inte-
grated into the axioms in a suitable manner. Semantic compilation is
performed only once prior to the submission of any query. Subsequently,
the compiled axioms are utilized for query transformation at the time of
query evaluation. The transformed query has restrictions imposed on it by
the integrity constraints and hence it may be evaluated more efficiently
over the data base than the original query.
Multiple queries arise in several contexts. In the case of deductive data
bases, a single query on an intensional predicate may result in several
disjunctive queries which may have overlapping computations.
We extend the connection graph decomposition algorithm to generate a
single plan for a set of disjunctive queries. A multi-query graph is used as
a non-procedural representation for a set of queries. The algorithm
proposed in this thesis minimizes the number of accesses to the secondary
storage where the relations are physically stored as well as the total
number of joins.
This dissertation describes the design approach of an iconic system on a
modern lisp machine. The proposed system has applications in image
information system, visual programming, computer aided design (CAD),
and multimedia communications. Potential applications include computer
vision systems, visual languages, and iconic expert systems.
A generalized definition of an icon is proposed: a visual representation
of an object (physical or abstract) which has relational dependencies with
other icons. An experimental iconic system has been designed around an
Icon Manager and an Icon Editor. The Icon Manager includes facilities for
icon creation, icon interpretation, icon exploration, icon saving (on files),
and an interactively programmable menu system; it provides basic support
to create icons and relate them to other icons. The Icon Editor supports
</bodyText>
<page confidence="0.878183">
132 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.705423">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999533076923077">
several methods to interactively edit an icon from sketch representations or
icon structure representations. The overall system allows the creation and
organization of flat pictorial objects of any shape in a two and a half
dimensional space.
The image processing tools required by the iconic system include a
general technique for halftone transformation of images, a general region
growing technique, and methods for progressive transmission of images.
Applications of this iconic programming environment to visual program-
ming, to program design and electronic circuit design (from icon selection
and editing), to knowledge systems based on icon graph matching and to
multimedia communications are studied in detail. Finally, a possible hard-
ware structure to support an icon management system is proposed which is
based on a pyramid of microprocessors architecture.
</bodyText>
<figure confidence="0.982743705882353">
Hierarchical Reasoning: Simulating
Complex Processes over Multiple Levels
of Abstraction
Paul Anthony Fishwick
University of Pennsylvania Ph.D. 1986,
196 pages
Computer Science
University Microfilms International
ADG86-I4793
Linguistic Solid Modeling using Graph
Grammars
Patrick Arthur Fitzhorn
Colorado State University Ph.D. 1985,
105 pages
Computer Science
University Microfilms International
ADG86-07641
</figure>
<bodyText confidence="0.990128890909091">
This thesis describes a method for simulating processes over multiple levels
of abstraction. There has been recent work with respect to data, object,
and problem-solving abstraction; however, abstraction in simulation has
not been adequately explored. We define a process as a hierarchy of
distinct production rule sets that interface to each other so that abstraction
levels may be bridged where desired. In this way, the process may be
studied at abstraction levels that are appropriate for the specific task:
notions of qualitative and quantitative simulation are integrated to form a
complete process description. The advantages to such a description are
increased control, computational efficiency, and selective reporting of
simulation results. Within the framework of hierarchical reasoning, we will
concentrate on presenting the primary concept of process abstraction.
A Common Lisp implementation of the hierarchical reasoning theory
called HIRES is presented. HIRES allows the user to reason in a hierarchi-
cal fashion by relating certain facets of the simulation to levels of
abstraction specified in terms of actions, objects, reports, and time. The
user is free to reason about a process over multiple levels by weaving
through the levels either manually or via automatically controlled specifica-
tions. Capabilities exist in HIRES to facilitate the creation of graph-based
abstraction levels. For instance, the analyst can create continuous system
models (CSMP), petri net models, scripts, or generic graph models that
define the process model at a given level. We present a four-level elevator
system and a two-level &amp;quot;dining philosophers&amp;quot; simulation to demonstrate
the efficacy of process abstraction.
The goal of this work is to develop formal relationships between language
theory and topologically correct computer representations of objects in
Euclidean three-space (E3), that is, physical solids. Thus the concern is to
generate grammars whose languages can be interpreted as classes of
representations of possibly proper subsets of physical solids. A methodol-
ogy is then studied for the implementation of the developed grammars.
The grammars of interest are variants of graph grammars whose lan-
guages are sets of directed graphs with node and edge labels, and whose
productions rewrite graphs into other graphs. Graph grammars are of
interest here since they generate structures similar to plane models (topo-
logical representations of the class of 3D solids). Since it can be shown
that plane models are sufficient representations of the topology of E3 poly-
topes, a class of graph grammars that generate all such models should be of
interest. The strings generated by these grammars will then be represent-
ations of physical solids that, although based on formal topological guaran-
tees, can be manipulated with formal language theory.
Computer implementation of the grammars is considered, and it is
shown that a natural method that encompasses storage of the represen-
tation, as well as the grammar itself, is one based on the predicate calculus.
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 133
The FINITE STRING Newsletter Abstracts of Current Literature
In this implementation, the vertices and edges of a representation are
stored as facts in a logic data base, while the grammar that rewrites subsets
of the graph with other graphs becomes a set of relations on graphs. The
programming language PROLOG is used for implementation, since it is
based closely on the first order predicate calculus.
In conclusion, it is shown that the current, major representations of
physical solids have analogs in the developed graph grammars to the same
level of representation validity. That being the case, graph grammars can
replace current, heuristic implementations of physical solid representations
with formal methods from language theory.
</bodyText>
<sectionHeader confidence="0.47131" genericHeader="method">
A Logic Data Model for the Machine
Representation of Knowledge
</sectionHeader>
<figure confidence="0.756809">
Randolph George Goebel
The University of British Columbia (Canada)
Ph.D. 1985
Computer Science
This item is not available from University
Microfilms International.
ADG05-58418
</figure>
<subsectionHeader confidence="0.440359666666667">
A Fully Lazy Higher Order Purely Func-
tional Programming Language with
Reduction Semantics
</subsectionHeader>
<subsubsectionHeader confidence="0.256066">
Kevin John Greene
</subsubsectionHeader>
<affiliation confidence="0.4396675">
Syracuse University Ph.D. 1985, 262 pages
Computer Science
</affiliation>
<subsubsectionHeader confidence="0.5198885">
University Microfilms International
ADG86-03760
</subsubsectionHeader>
<bodyText confidence="0.999715695652174">
DLOG is a logic-based data model developed to show how logic-program-
ming can combine contributions of Data Base Management (DBM) and
Artificial Intelligence (Al). The DLOG data model is based on a logical
formulation that is a superset of the relation data model (Reiter83), and
uses Bowen and Kowalski&apos;s notion of an amalgamated meta and object
language (Bowen82) to describe the relationship between data model
objects. The DLOG specification includes a language syntax, a proof (or
query evaluation) procedure, a description of the language&apos;s semantics, and
a specification of the relationships between assertions, queries, and appli-
cation data bases.
DLOG&apos;s basic data description language is the Horn clause subset of
first order logic (Kowalski79, Kowalski81), together with embedded
descriptive terms and non-Horn integrity constraints. The embedded terms
are motivated by Artificial Intelligence representation language ideas,
specifically, the descriptive terms of the KRL language (Bobrow77). A
similar facility based on logical descriptions is provided in DLOG. The
DLOG language permits the use of definite and indefinite descriptions of
individuals and sets in both queries and assertions.
The meaning of DLOG&apos;s extended language is specified by writing Horn
clauses that describe the relation between the basic language and the
extensions. The experimental implementation is the appropriate Prolog
program derived from that specification.
The DLOG implementation relies on an extension to the standard Prolog
proof procedure. This includes a &amp;quot;unification&amp;quot; procedure that matches
embedded terms by recursively invoking the DLOG proof procedure (cf.,
&amp;loglisp. Robinson82). The experimental system includes logic-based
implementations of traditional database facilities (e.g., transactions, integ-
rity constraints, data dictionaries, data manipulation language facilities),
and an idea for using logic as the basis for heuristic interpretation of
queries. This heuristic uses a notion of partial match or sub-proof to
produce assumptions under which plausible query answers can be derived.
The experimental DLOG database (or &amp;quot;knowledge base&amp;quot;) management
system is exercised by describing an undergraduate degree program. The
example application database is a description of the Bachelor of Computer
Science degree requirements at The University of British Columbia. This
application demonstrates how DLOG&apos;s embedded terms provide a concise
description of degree program knowledge, and how that knowledge is used
to specify student programs, and select program options.
In the first third of this thesis, three well-known reduction calculi — A.
Church&apos;s A-calculus, M. Schonfinkel&apos;s SKI-calculus, and C. P. Wads-
worth&apos;s graph oriented A-calculus — are defined. Schonfinkel&apos;s classic
transformation of A-calculus well-formed formulas (wffs) into variable-free
SKI-calculus wffs is also presented. A new notion, lazy-normal form, a
generalization of the SKI-calculus concept of normal form, is then defined
and compared with Wadsworth&apos;s concept of head-normal form. Head-nor-
mal form is a generalized notion of normal form in the A-calculus. It is
</bodyText>
<page confidence="0.946126">
134 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<note confidence="0.698345">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999822458333333">
demonstrated that a SKI-calculus wff in lazy-normal form is an outline of
the wff&apos;s normal form (if one exists) — i.e., its normal form will have the
same initial atom and the same number of arguments. Other results relat-
ing A-calculus wffs in head-normal form to SKI-calculus wffs in lazy-nor-
mal form are stated and proved.
The ideas behind M. Schonfinkel&apos;s SKI-calculus, C. P. Wadsworth&apos;s
graph oriented A-G-calculus, and D. A. Turner&apos;s SASL implementation are
combined with the concept of lazy-normal form to produce a new deter-
ministic combinator based graph and machine oriented reduction calculus:
the LFN-calculus. The LFN-calculus is equivalent in power to the
A-calculus et al., but is much more directly and efficiently implementable.
This is due primarily to the structure sharing properties of the LFN-calculus
wffs. Both garbage nodes and forwarding arcs (indirection pointers),
concepts that are usually relegated to a calculus&apos;s implementation, are
given formal definitions in this calculus.
The design and experimental Lisp Machine implementation of LFN, a
fully lazy higher order purely functional programming language with
reduction semantics, are discussed. The LFN compiler transforms high
level expressions into representations of LFN-calculus wffs. LFN&apos;s runtime
system, a direct realization of the LFN-calculus&apos;s &amp;quot;is reducible to&amp;quot; relation,
takes as input LFN-calculus wffs and produces irreducible wffs (wffs in
lazy-normal form) as result. The thesis ends with brief discussions of
alternate approaches to functional programming language compilation and
runtime system organization.
</bodyText>
<figure confidence="0.567361">
Learning by Understanding Analogies
Russell Greiner
Stanford University Ph.D. 1985, 417 pages
Computer Science
University Microfilms International
ADG86-02479
</figure>
<subsectionHeader confidence="0.482602">
Pattern-Based and Knowledge-Directed
Query Compilation for Recursive Data
Bases
</subsectionHeader>
<bodyText confidence="0.99997534375">
The phenomenon of learning has intrigued scholars for ages; this fasci-
nation is reflected in Artificial Intelligence, which has always considered
learning to be one of its major challenges. This dissertation provides a
formal account of one mode of learning, learning by analogy. In partic-
ular, it defines the useful analogical inference process (UAI), which uses a
given analogical hint of the form &amp;quot;A is like B&amp;quot; and a particular target prob-
lem to map known facts about B onto proposed conjectures about A. UAI
only considers conjectures which are useful to the target problem; that is,
the conjectures must lead to a plausible solution to that problem.
To construct a procedure that can effectively find these useful analogies,
we use two sets of heuristics to refine the general UAI process. The first set
is based on the intuition that useful analogies often correspond to
&amp;quot;coherent&amp;quot; clusters of facts. This suggests that UAI seeks only the analo-
gies that correspond to common abstractions, where abstractions are
relations that encode solution methods to past problems. The other set of
rules embody the claim that &amp;quot;better analogies impose fewer constraints on
the world&amp;quot;. Basically, these rules prefer the analogies which require the
fewest additional conjectures.
This dissertation also describes a running program, NLAG, which imple-
ments this model of analogy. It is then used in a battery of tests, designed
to empirically validate our claim that UAI is an effective technique for
acquiring new facts. This data also demonstrates that the heuristics are
effective, and suggests why.
In summary, the primary contributions of this research are (1) a formal
definition of UAI, described semantically (using a new variant of Tarskian
semantics), syntactically and operationally; (2) a collection of heuristics
which efficiently guide this process towards useful analogies; and (3) vari-
ous empirical results, which illustrate the source of power underlying this
approach.
Expert database systems (EDSs) comprise an interesting class of computer
systems which represent a confluence of research in artificial intelligence,
logic, and database management systems. They involve knowledge-direct-
</bodyText>
<figure confidence="0.9478664">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 135
The FINITE STRING Newsletter Abstracts of Current Literature
Jiawei Han
The University of Wisconsin - Madison Ph.D.
1985, 216 pages
Computer Science
University Microfilms International
ADG86-01539
A Theory of Scalar Implicature
Julia Bell Hirschberg
University of Pennsylvania Ph.D. 1985,
230 pages
Computer Science
University Microfilms International
ADG86-03648
</figure>
<bodyText confidence="0.999900684210526">
ed processing of large volumes of shared information and constitute a new
generation of knowledge management systems.
Our research is on the deductive augmentation of relational database
systems, especially on the efficient realization of recursion. We study the
compilation and processing of recursive rules in relational database
systems, investigating two related approaches: pattern-based recursive rule
compilation and knowledge-directed recursive rule compilation and plan-
ning.
Pattern-based recursive rule compilation is a method of compiling and
processing recursive rules based on their recursion patterns. We classify
recursive rules according to their processing complexity and develop three
kinds of algorithms for compiling and processing different classes of recur-
sive rules: transitive closure algorithms, SLSR wavefront algorithms, and
stack-directed compilation algorithms. These algorithms, though distinct,
are closely related. The more complex algorithms are generalizations of
the simpler ones, and all apply the heuristics of performing selection first
and utilizing previous processing results (wavefronts) in reducing query
processing costs. The algorithms are formally described and verified, and
important aspects of their behavior are analyzed and experimentally tested.
To further improve search efficiency, a knowledge-directed recursive
rule compilation and planning technique is introduced. We analyze the
issues raised for the compilation of recursive rules and propose to deal with
them by incorporating functional definitions, domain-specific knowledge,
query constants, and a planning technique. A prototype knowledge-direct-
ed relational planner, RELPLAN, which maintains a high level user view
and query interface, has been designed and implemented, and experiments
with the prototype are reported and illustrated.
Speakers may convey many sorts of &apos;meaning&apos; via an utterance. While
each of these contributes to the utterance&apos;s overall communicative effect,
many are not captured by a truth-functional semantics. One class of non-
truth-functional, context-dependent meanings, has been identified by
Grice (1975) as conversational implicatures. This thesis presents a formal
account of one type of conversational implicature, termed here scalar impli-
cature identified from a study of a large corpus of naturally occurring data
collected by the author and others from 1982 through 1985. Scalar impli-
catures rely for their generation and interpretation upon the assumption
that cooperative speakers will say as much as they truthfully can that is
relevant to a conversational exchange. For example, B&apos;s utterance of (la):
</bodyText>
<listItem confidence="0.884119666666667">
(1) A: How was the party last night?
a. B: Some people left early.
b. Not all people left early.
</listItem>
<bodyText confidence="0.9998164">
may convey to A that, as far as B knows, (lb) also holds — even though
the truth of (lb) clearly does not follow from the truth of (la).
Scalar implicatures may be distinguished from other conversational
implicatures in that their generation and interpretation is dependent upon
the identification of some salient relation that orders a concept referred to
in an utterance with other concepts. In 1, for example, the salience of an
inclusion relation between &apos;some people&apos; and &apos;all people&apos; in the discourse is
prerequisite to B&apos;s implicating that (1 b) — and to A&apos;s understanding that
(ib) has in fact been implicated.
To illustrate potential applications of the theory presented, a module of
a natural-language interface, QUASI, is described. QUASI calculates scalar
implicatures that might be licensed by simple direct responses to yes-no
questions. Where licensable implicatures are not consistent with the
system&apos;s knowledge base, QUASI proposes alternative responses. This
system demonstrates how natural language interfaces can use the calcu-
</bodyText>
<page confidence="0.955513">
136 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<bodyText confidence="0.825915666666667">
The FINITE STRING Newsletter Abstracts of Current Literature
lation of implicit meanings to avoid conveying misinformation and to
convey desired information more succinctly.
</bodyText>
<figure confidence="0.9889298125">
A Knowledge-based Approach to
Language Production
Paul Schafran Jacobs
University of California, Berkeley Ph.D.
1985, 278 pages
Computer Science
University Microfilms International
ADG86-10067
A Knowledge-based System for Debugging
Concurrent Software
Carol Helfgott LeDoux
University of California, Los Angeles Ph.D.
1985, 322 pages
Computer Science
University Microfilms International
ADG86-03965
</figure>
<bodyText confidence="0.995286648148148">
The development of natural language interfaces to Artificial Intelligence
systems is dependent on the representation of knowledge. A major imped-
iment to building such systems has been the difficulty in adding sufficient
linguistic and conceptual knowledge to extend and adapt their capabilities.
This difficulty has been apparent in systems which perform the task of
language production, i. e. the generation of natural language output to
satisfy the communicative requirements of a system.
The problem of extending and adapting linguistic capabilities is
rooted in the problem of integrating abstract and specialized knowledge
and applying this knowledge to the language processing task. Three
aspects of a knowledge representation system are highlighted by this prob-
lem: hierarchy, or the ability to represent relationships between abstract
and specific knowledge structures; explicit referential knowledge, or know-
ledge about relationships among concepts used in referring to concepts;
and uniformity, the use of a common framework for linguistic and concep-
tual knowledge. The knowledge-based approach to language production
addresses the language generation task from within the broader context of
the representation and application of conceptual and linguistic knowledge.
This knowledge-based approach has led to the design and implementa-
tion of a knowledge representation framework, called Ace, geared towards
facilitating the interaction of linguistic and conceptual knowledge in
language processing. Ace is a uniform, hierarchical representation system,
which facilitates the use of abstractions in the encoding of specialized
knowledge and the representation of the referential and metaphorical
relationships among concepts.
A general-purpose natural language generator, KING (Knowledge
INtensive Generator), has been implemented to apply knowledge in the
Ace form. The generator is designed for knowledge-intensivity and incre-
mentality, to exploit the power of the Ace knowledge in generation. The
generator works by applying structured associations, or mappings, from
conceptual to linguistic structures, and combining these structures into
grammatical utterances. This has proven to be a simple but powerful
mechanism, easy to adapt and extend, and has provided strong support for
the role of conceptual organization in language generation.
The recent development of high-level concurrent programming languages
has emphasized the problem of limited debugging tools to support the
development of applications using these languages. A new approach is
necessary to improve the efficacy of debugging tools and to adapt them to
the framework of a concurrent software environment.
A knowledge-based debugging approach is presented that aids diagnosis
of a variety of run-time errors that can occur in concurrent programs writ-
ten in the Ada&apos; programming language. In this approach, an event stream
of program activity is captured in an historical database and accessed using
Prolog-based queries constrained by temporal-logic primitives. Diagnosis
is aided by applying rule-based descriptions of some common classes of
software errors and by matching program specifications against the trace
data base.
This approach was used in building a prototype debugger, called Your
Own Debugger for Ada (YODA). The design of YODA is described and
analyses of several sample Ada programs are presented to illustrate diagno-
sis of errors associated with concurrency, including deadness errors and
misuse of shared data.
&apos;Ada is a registered trademark of the U.S. Government — Ada Joint
Program Office.
</bodyText>
<figure confidence="0.937832222222222">
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 137
The FINITE STRING Newsletter Abstracts of Current Literature
Plan Recognition and Discourse Analysis:
an Integrated Approach for Understanding
Dialogues
Diane Judith Litman
The University of Rochester Ph.D. 1986,
197 pages
Computer Science
University Microfilms International
ADG86-10863
Correcting object-related misconceptions
Kathleen Filliben McCoy
University of Pennsylvania Ph.D. 1985,
166 pages
Computer Science
University Microfilms International
ADG86-03674
</figure>
<bodyText confidence="0.971615898305085">
Inferring Domain Plans in Question-
Answering
Martha Elizabeth Pollack
One promising computational approach to understanding dialogues has
involved modeling the goals of the speakers in the domain of discourse. In
general, these models work well as long as the topic follows the goal struc-
ture closely, but they have difficulty accounting for interrupting subdi-
alogues such as clarifications and corrections. Furthermore, such models
are typically unable to use many processing clues provided by the linguistic
phenomena of the dialogues.
This dissertation presents a computational theory and partial implemen-
tation of a discourse level model of dialogue understanding. The theory
extends and integrates plan-based and linguistic-based approaches to
language processing, arguing that such a synthesis is needed to computa-
tionally handle many discourse level phenomena present in naturally occur-
ring dialogues. The simple, fairly syntactic results of discourse analysis
(for example, explanations of phenomena in terms of very local discourse
contexts as well as correlations between syntactic devices and discourse
function) will be input to the plan recognition system, while the more
complex inferential processes relating utterances have been totally refor-
mulated within a plan-based framework. Such an integration has led to a
new model of plan recognition, one that constructs a hierarchy of domain
and meta-plans via the process of constraint satisfaction. Furthermore, the
processing of the plan recognizer is explicitly coordinated with a set of
linguistic clues. The resulting framework handles a wide variety of difficult
linguistic phenomena (for example, interruptions, fragmental and elliptical
utterances, and presence as well as absence of syntactic discourse clues),
while maintaining the computational advantages of the plan-based
approach. The implementation of the plan recognition aspects of this
framework also addresses two difficult issues of knowledge representation
inherent in any plan recognition task.
Analysis of a corpus of naturally occurring data shows that users convers-
ing with a database or expert system are likely to reveal misconceptions
about the objects modelled by the system. Further analysis reveals that
the sort of responses given when such misconceptions are encountered
depends greatly on the discourse context. This work develops a context-
sensitive method for automatically generating responses to object-related
misconceptions with the goal of incorporating a correction module in the
front-end of a database or expert system. The method is demonstrated
through the ROMPER system (Responding to Object-related Miscon-
ceptions using PERspective), which is able to generate responses to two
classes of object-related misconceptions: misclassifications and misattri-
butions.
The transcript analysis reveals a number of specific strategies used by
human experts to correct misconceptions, where each different strategy
refutes a different kind of support for the misconception. In this work
each strategy is paired with a structural specificatiOn of the kind of support
it refutes. ROMPER uses this specification, and a model of the user, to
determine which kind of support is most likely. The corresponding
response strategy is then instantiated.
The above process is made context sensitive by a proposed addition to
standard knowledge-representation systems termed object perspective.
Object perspective is introduced as a method for augmenting a standard
knowledge-representation system to reflect the highlighting affects of
previous discourse. It is shown how this resulting highlighting can be used
to account for the context-sensitive requirements of the correction process.
The importance of plan inference in models of conversation has been
widely noted in the computational-linguistics literature, and its incorpo-
ration in question-answering systems has enabled a range of cooperative
</bodyText>
<page confidence="0.835898">
138 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<figure confidence="0.989519318181818">
The FINITE STRING Newsletter Abstracts of Current Literature
University of Pennsylvania Ph.D. 1986,
191 pages
Computer Science
University Microfilms International
ADG86-14850
Rational Interaction: Cooperation among
Intelligent Agents
Jeffrey Solomon Rosenschein
Stanford University Ph.D. 1986, 145 pages
Computer Science
University Microfilms International
A DG86-08219
Recursions and Rule Selections on a High
Level Relation Processor for Knowledge-
Base Machine
Dongpil Shin
The University of Oklahoma Ph.D. 1986,
132 pages
Computer Science
University Microfilms International
ADG86-13735
</figure>
<bodyText confidence="0.997735652173913">
behaviors. The plan inference process in each of these systems, however,
has assumed that the questioner (Q) whose plan is being inferred and the
respondent (R) who is drawing the inference have identical beliefs about
the actions in the domain. I demonstrate that this assumption is too strong,
and often results in failure not only of the plan inference process but also
of the communicative process that plan inference is meant to support. In
particular, it precludes the principled generation of appropriate responses
to queries that arise from invalid plans. I present a model of plan inference
in conversation that distinguishes between the beliefs of the questioner and
the beliefs of the respondent. This model rests on an account of plans as
mental phenomena: &amp;quot;having a plan&amp;quot; is analyzed as having a particular
configuration of beliefs and intentions. Judgments that a plan is invalid are
associated with particular discrepancies between the beliefs that R ascribes
to Q, when R believes Q has some particular plan, and the beliefs R herself
holds. I define several types of invalidities from which a plan may suffer,
relating each to a particular type of belief discrepancy, and show that the
types of any invalidities judged to be present in the plan underlying a query
can affect the content of a cooperative response. The plan inference
model has been implemented in SPIRIT — a System for Plan Inference that
Reasons about Invalidities Too — which reasons about plans underlying
queries in the domain of computer mail.
The development of intelligent agents presents opportunities to exploit
intelligent cooperation. Before this can occur, however, a framework must
be built for reasoning about interactions. This dissertation describes such a
framework, and explores strategies of interaction among intelligent agents.
The formalism that has been developed removes some serious re-
strictions that underlie previous research in distributed artificial intelli-
gence, particularly the assumption that the interacting agents have identical
or non-conflicting goals. The formalism allows each agent to make various
assumptions about both the goals and the rationality of other agents. A
hierarchy of rationality assumptions is presented, along with an analysis of
the consequences that result when an agent believes a particular level in
the hierarchy describes other agents&apos; rationality.
In addition, the formalism presented allows the modeling of restrictions
on communication and the modeling of binding promises among agents.
Computation on the part of each individual agent can often obviate the
need for inter-agent communication. However, when communication and
promises are allowed, fewer assumptions need be made about the rationali-
ty of other agents when choosing one&apos;s own rational course of action.
For the development of knowledge-based systems, various ways of incor-
porating a relational database system into a PROLOG-based question-
answering system have been investigated. To improve the performance
of a knowledge-based system, a deductive search involving a large set of
facts is performed separately by a relational database subsystem. The cor-
rectness property of an inference performed by such a system is formally
studied.
Problems associated with the scheme, such as determining a termination
in a recursion, incorporating &amp;quot;cut&amp;quot; operations, and the capabilities of
relation processors of performing these operations, are studied. To solve
these problems, first, database queries are classified into six levels based on
their operations so that queries involving recursions and &amp;quot;cut&amp;quot; can be iden-
tified. Next, relation processors are also classified so that corresponding
query expressions can be evaluated. Then, a high-level machine, Data flow
Relation Processor (DFRP), is designed so that all the problems defined
previously can be solved with this machine.
Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 139
The FINITE STRING Newsletter Abstracts of Current Literature
The closed-connection graph is introduced so that a recursion could be
visualized. The conditions for terminating a recursion are defined in terms
of the closed-connection graph. A procedure that synthesizes a level-5
query is developed. Finally, functional definitions of DFRP are studied to
evaluate a recursive query, and its simulation model is built. The major
results of the simulation are (i) a binary join takes a constant time regard-
less of the cardinality of relations; (ii) n-ary join takes 0(n) time, where n
is the number of relations involved; (iii) DFRP is 104 times faster in
performing a binary join operation than A which is the intermediate result
of Japan&apos;s fifth-generation computer project; (iv) a knowledge-based
system with DFRP is 10 to 20 times faster than MPDC in performing
PROLOG queries of 2 to 30 subgoals, each involving 4096 facts.
</bodyText>
<figure confidence="0.989425461538462">
Controlling Inference
David Eugene Smith
Stanford University Ph.D. 1985, 237 pages
Computer Science
University Microfilms International
ADG86-02539
The Essence of Rum: a Theory of the
Intensional and Extensional Aspects of
Lisp-type Computation
Carolyn L. TakW&apos;
Stanford University Ph.D. 1985, 248 pages
Computer Science
University Microfilms International
</figure>
<page confidence="0.930779">
ADG86-02549
</page>
<bodyText confidence="0.999934295454545">
Effective control of inference is a fundamental problem in Artificial Intelli-
gence. Unguided inference leads to a combinatorial explosion of facts or
subgoals for even simple domains. To overcome this problem, expert
systems have used powerful domain-dependent control information in
conjunction with syntactic domain-independent methods like depth-first
backward chaining. While this is possible for some applications, it is not
always feasible or appropriate for problem solvers that must solve a wide
variety of different problems. In this dissertation I argue that a kind of
semi-independent control is essential for problem solvers that must face a
wide variety of different problems.
Semi-independent control is based on the idea that there is underlying
domain-independent rationale behind any good control decision. This
rationale takes the form of simple utility theory applied to the expected
cost and probability of success of different inference steps and strategies.
These basic principles are domain-independent, but their application to any
particular problem relies on global information about the nature and extent
of facts and rules in the problem solver&apos;s data base.
This approach to control is used in the solution of four different control
problems: halting inference when all answers to a query have been found,
halting recursive inference, ordering conjunctive queries when no inference
is involved, and choosing the best inference step for problems where only a
single answer is required. The first two control problems are cases of
recognizing redundant portions of a search space, while the final two cases
involve computing the expected cost for alternative strategies to a problem.
Several novel theorems about control (for specific situations) are devel-
oped in these case studies.
The issue of efficiency is also addressed. Semi-independent control
often involves considerable computation, and may not be cost-effective for
the majority of problems encountered in a particular domain. Interleaving
of inference and control is proposed as a means of making this kind of
control practical.
Rum is a theory of applicative, side-effect free computations over an alge-
braic data structure. It goes beyond a theory of functions computed by
programs, treating both intensional and extensional aspects of computa-
tion. Powerful programming tools such as streams, object-oriented
programming, escape mechanisms, and co-routines can be represented.
Intensional properties include the number of multiplications executed, the
number of the context switches, and the maximum stack depth required in
a computation. Extensional properties include notions of equality for
streams and co-routines and characterization of functionals implementing
strategies for searching tree-structured spaces. Precise definitions of
informal concepts such as stream and co-routine are given and their math-
ematical theory is developed. Operations on programs treated include
program transformations which introduce functional and control
</bodyText>
<page confidence="0.903056">
140 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
<bodyText confidence="0.978072090909091">
The FINITE STRING Newsletter Abstracts of Current Literature
abstractions; a compiling morphism that provides a representation of
control abstractions as functional abstractions; and operations that trans-
form intensional properties to extensional properties. The goal is not only
to account for programming practice in Lisp, but also to improve practice
by providing mathematical tools for developing programs and building
programming systems.
Rum views computation as a process of generating computation struc-
tures — trees for context-independent computations and sequences for
context-dependent computations. The recursion theorem gives a fixed-
point function that computes computationally minimal fixed points. The
context insensitivity theorem says that context-dependent computations
are uniformly parameterized by the calling context and that computations
in which context dependence is localized can be treated like context-inde-
pendent computations. Rum machine structure and morphism are intro-
duced to define and prove properties of compliers. The hierarchy of
comparison relations on programs ranges from intensional equality to
maximum approximation and equivalence relations that are extensional.
The fixed-point function computes the least fixed point with respect to the
maximum approximation. Comparison relations, combined with the inter-
pretation of programs using computation structures, provide operations on
programs both with meanings to preserve and meanings to transform.
</bodyText>
<sectionHeader confidence="0.6972285" genericHeader="method">
INT-AID: The Intelligent Aid for Rela-
tional Database Construction
</sectionHeader>
<reference confidence="0.912939">
Mustafa Mahmoud
Lehigh University Ph.D. 1986, 214 pages
Computer Science
University Microfilms International
ADG86-I6179
The Interactive Effects of Micro- and
Macrostructural Processing during Text
Comprehension
Nicholas Ge/eta
The Catholic University of America Ph.D.
1986, 170 pages
Education, Psychology
University Microfilms International
ADG86-13460
</reference>
<bodyText confidence="0.996483466666666">
In this dissertation we developed a system, INT-AID: The INTelligent AID
for relational database construction. It is an intelligent interactive system
that aids the relational database systems designers in constructing a good
design. We defined a workable methodology by integrating a wide variety
of algorithms, theories, and techniques in one system. The system uses a
set of Functional Dependencies (FDs) to construct relations in Third
Normal Form (3NF) following the Synthetic Approach in relational data-
base design.
We proposed a novel methodology in deriving and generating a set of
functional dependencies. Unlike the standard conventional method which
uses structured analysis techniques to build a set of FDs; our approach is
an unstructured method that does not depend on structured analysis. This
new method suits the evolutionary approach in database design. In our
approach we deal with an incoherent body of data, that contains many
unrelated and unstructured facts. From this body of data we want to let
the natural relationships emerge dynamically, rather than imposing unna-
tural relationships on the data. As a result of this we might uncover some
hidden relationships that were not known before. This new approach is an
attempt towards the establishment of causal effects. We developed a
formula using mathematical induction, to give the total number of what we
called proposed FDs (their validity is yet to be determined).
The Synthetic Approach, which we followed in our system to construct
3NF relations, does not always produce relations that are lossless with
respect to the join operation. To overcome this shortcoming we added an
algorithm to check whether the synthesized relations are lossless with
respect to join or not.
This research investigated the text comprehension model first described in
Kintsch and van Dijk (1978) and later elaborated by van Dijk and Kintsch
(1983). The central research question involved determining if certain
micro- and macropropositions were accessible to the reader/comprehender
in working-memory during on-going reading of prose passages. Sixty
subjects read texts presented one processing cycle at a time on the display
screen of an Apple He microcomputer. While reading they were inter-
rupted by a probe to a specific Micro- (Mi) or Macroproposition (Ma)
that varied in terms of the processing cycle in which it appeared before
Computational Linguistics, Volume 13, Numbers 1-.2, January-June 1987 141
The FINITE STRING Newsletter Abstracts of Current Literature
presentation of the probe (i.e., Next-To-Last Cycle (NTLC) or Prior Cycle
(PC)). The response time, measured in milliseconds, of a subject&apos;s judge-
ment as to whether the probe&apos;s informational content was consistent with
that of the passage was recorded. It was hypothesized that NTLC/Mi&apos;s,
NTLC/Ma&apos;s and PC/Ma&apos;s would be responded to more rapidly than PC/Mi&apos;s
since the propositions were predicted to be resident in working-memory.
The results supported this prediction. It was concluded that good readers
construct micro- and macrostructural representations of the text on-line.
</bodyText>
<figure confidence="0.992322411764706">
Time Course of Activation for High and
Low Centrality Nouns in Scripts
Jacqueline Sullivan Gorski
The Catholic University of America Ph.D.
1986, 206 pages
Education, Psychology
University Microfilms International
ADG86-03279
The Effects of Expertise and Sentence
Form on Reading Rate and Vocalization
Latency
Ann Lamiell Landy
The Pennsylvania State University Ph.D.
1986, 187 pages
Education, Psychology
University Microfilms International
ADG86-152I0
</figure>
<bodyText confidence="0.99985">
This study investigated whether scripts, such as eating in a restaurant, are
prestored or consciously constructed long term memory units and whether
centrality is an organizing dimension. Five models for the activation of
script nouns were proposed. These were differentiated by their predictions
for the time course of activation for high and low centrality script nouns at
each of three intervals.
Sixty subjects generated script nouns. Twenty rated them on centrality.
Forty-eight subjects participated in a computerized lexical decision task.
Primes were script names and neutral XXXs. Targets were high and low
centrality script nouns and nonwords. When the prime was a word and the
target was a word, the prime either named the same script from which the
word was taken or a different script. The interval between prime and
target was varied between 250, 500, and 750 msec. The dependent vari-
able was time to respond word or nonword to the target.
Scores indicating whether same or different script primes facilitated or
inhibited responding were computed by subtracting response times after
script primes from response times after XXX primes. Facilitation was indi-
cated by positive and inhibition by negative values. T-tests were
conducted on mean scores for high and low centrality script nouns at each
interval to determine the type (automatic or conscious) and extent of acti-
vation as indicated by the observed facilitation. Analyses of variance were
conducted separately on scores for same and different script primes to
identify possible effects due to list, time interval, and centrality.
Results supported the Prestorage and Computation model. Same script
primed responses to high centrality nouns were facilitated at all three inter-
vals, while those for low centrality nouns were facilitated only at the long-
est. This suggested that highly important script concepts form a prestored
unit which is automatically activated, while less important concepts must
be consciously activated. An associative network theory of script memory
representation accounted well for the data (cf. Yekovich &amp; Walker 1985,
in press). Suggestions for teachers and computer tutorial designers
included cuing learners to consciously activate low level domain knowledge
and providing adequate time to do.
Two experiments were carried out to test the effects of knowledgeability
and technical vocabulary on processing speed for sentences from familiar
and unfamiliar technical domains. Using a priming paradigm, reading rate
for sentence stems and vocalization latencies for target words that
followed the stems were obtained for technically worded and simplified
sentences.
In the first experiment, biochemists&apos; reading rate and vocalization laten-
cies were compared for familiar (biochemistry) technical and simplified
sentences, unfamiliar (psychopathology) technical and simplified
sentences, and general expository sentences. In the second experiment,
the relationship of distances within semantic networks to processing speed
was explored by obtaining vocalization latencies for target words that
followed related, neutral, and unrelated sentence stems with familiar
(psychopathology) and unfamiliar (biochemistry) content. Clinical
psychologists were subjects.
</bodyText>
<page confidence="0.939546">
142 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992930666666667">ABSTRACTS OF CURRENT LITERATURE Articles, Word Order, and Resource Control Hypothesis</title>
<author confidence="0.999672">Janusz S Bien</author>
<affiliation confidence="0.860937">Warsaw</affiliation>
<address confidence="0.873127">Mey, Jacob L., Ed., and</address>
<title confidence="0.793605">Discourse: Test and Protest, A Festschrift for Sgall. 19, Linguistic and Literary</title>
<author confidence="0.896196">John Benjamins</author>
<affiliation confidence="0.928165">Publishing Company, Amsterdam/</affiliation>
<address confidence="0.929988">Philadelphia, 1986.</address>
<abstract confidence="0.9556053">elaborates the ideas presented in Bien (1983). The definite and indefinite distinction is viewed as a manifestation of the variable depth of nominal phrase processings: indefinite phrases are represented by frame pointers, while definite ones by frame instances incorporating information found by memory search. In general, the depth of processing is determined by the availability of resources. Different word orders cause different distributions of the parser&apos;s processing load and therefore influence also the depth of processing. Articles and word order appear to be only some of several resource control devices available in natural languages. For copies of the following papers from Projekt SEMSYN, please write to</abstract>
<author confidence="0.5044735">Frau Martin co Projekt SEMSYN</author>
<affiliation confidence="0.842589">Institut fuer Informatik</affiliation>
<address confidence="0.841674">Azenbergstr. 12 D-7000 Stuttgart I</address>
<author confidence="0.849703">West Germany</author>
<email confidence="0.700936">ore-mailto:semsyn@ifistg.uucp</email>
<title confidence="0.8548488">The Automated News Agency: SEMTEX — A Text Generator for German Dietmar Roesner — System for Verbalizing Constructions German</title>
<author confidence="0.433693">Walter Kehl</author>
<abstract confidence="0.991688043478261">As a by-product of the Japanese/German machine translation project SEMSYN the SEMTEX text generator for German has been implemented (in ZetaLISP for SYMBOLICS lisp machines). SEMTEX&apos;s first application has been to generate newspaper stories about job market development. Starting point for the newspaper application is just the data from the monthly job market report (numbers of unemployed, open jobs, ...). A rudimentary &amp;quot;text planner&amp;quot; takes these data and those of relevant previous months, checks for changes and significant developments, simulates possible argumentations of various political speakers on these developments and finally creates a representation for the intended text as an ordered list of frame descriptions. SEMTEX then converts this list into a newspaper story in German using an extended version of the generator of the SEMSYN project. The extensions for SEMTEX include: • Building up a representation for the context during the utterance of successive sentences that allows for — avoiding repetitions in wording — avoiding re-utterance of information still valid — pronominalization and other types of references. • Grammatical tense is dynamically derived by checking the temporal information from the conceptual representations and relating it to the time of speech and the time-period focussed by the story. • When simulating arguments the text planner uses abstract rhetorical schemata; the generator is enriched with knowledge about various ways to express such rhetorical structures as German surface texts. GEOTEX is an application of the SEMTEX text generator for German: The text generator is combined with a tool for interactively creating geometric constructions. The latter offers formal commands for manipulating (i.e. Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 93 The FINITE STRING Newsletter Abstracts of Current Literature creating, naming and — deliberately — deleting) basic objects of Euclidean geometry. The generator is used to produce descriptive texts — in German — related to the geometric construction: • descriptions of the geometric objects involved, • descriptions of the sequence of steps done during a construction. mechanisms have been enriched for • Elision is no longer restricted to adjuncts. For repetitive operations, verb and subject will be elided in subsequent sentences. • The distinction between known information and new one is exploited to decide on constituent ordering: the constituent referring to the known is &amp;quot;topicalized&amp;quot;, put in front of the sentence. • The system allows for more ways to refer to objects introduced in the text: pronouns, textual deixis using demonstrative pronouns, names. The choice between these variants is done deliberately. GEOTEX is implemented in ZetaLISP and runs on SYMBOLICS lisp machines.</abstract>
<title confidence="0.970516666666667">The Generation System of the SEMSYN Project. Towards a Task-Independent Generator for German</title>
<author confidence="0.843919">Dietmar Roesner</author>
<abstract confidence="0.997525545454545">We report on our experiences from the implementation of the SEMSYN generator, a system generating German texts from semantic representations, and its application to a variety of different areas, input structures and generation tasks. In its initial version the SEMSYN generator was used within a Japanese/German MT project, where it produced German equivalents to Japanese titles from scientific papers. Being carefully designed in object-oriented style (and implemented with the FLAVOR system) the system proved to be easily adaptable to other semantic representations — from CMU&apos;s Universal Parser — and extensible to other generation tasks: generating German news stories, generating descriptive texts to geometric constructions.</abstract>
<note confidence="0.6898025">Copies of the following reports on the joint research project WISBER can be ordered free of charge from Dr. Johannes Arz Universitat des Saarlandes FR. 10 Informatik IV Im Stadtwald 15 D-6600 Saarbrticken 11</note>
<title confidence="0.509532333333333">Electronic mail address: wisber%sbsvax. uucp@germany.csnet Grammatiktheorien und Grammatikf ormalismen</title>
<author confidence="0.9686765">H-U Block</author>
<author confidence="0.9686765">M Gehrke</author>
<author confidence="0.9686765">H Haugeneder</author>
<author confidence="0.9686765">R Hunze</author>
<pubnum confidence="0.89106">Report No. 1</pubnum>
<title confidence="0.967186">Entwurf eines Erhebungsschemas fiir Geldanlage</title>
<author confidence="0.989239">R Busche</author>
<author confidence="0.989239">S op de_Hipt</author>
<author confidence="0.989239">M-J Schacter-Radig</author>
<pubnum confidence="0.86588">Report No. 2</pubnum>
<title confidence="0.88457">Generierung von Erklarungen aus formalen Wissensreprasentationen</title>
<author confidence="0.970301">H Rosner</author>
<abstract confidence="0.973399818181818">in LbV-Forum, Band 4, Nummer 1, Juni 1986, pp. 3-19 The present paper gives an overview of modern theories of syntax and is intended to provide insight into current trends in the field of parsing. The grammar theories treated here are government and binding theory, generalized phrase structure grammar, and lexical functional grammar, as these approaches currently appear to be the most promising. Recent grammar formalisms are virtually all based on unification procedures. Three representatives of this group (functional unification grammar, &amp;patr., and definite clause grammar) are presented. This report describes the acquisition schema for the knowledge required by consulting system goal of which consists in carrying out the process of knowledge acquisition and formalization in a methodical — i.e., planned and controlled — manner. The main task involves the design of appropriate acquisition techniques and their successful application in the domain of investment consulting. The main topic of this report concerns the generation of natural language texts. The use of explanation components in expert systems involves making computer behavior more transparent. This standard can only be attained if the current stack dump procedure is replaced by procedures in which user expectations are met with respect to the contents of the systems Linguistics, Volume 13, Numbers 1-2, January-June 1987</abstract>
<title confidence="0.961035">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<affiliation confidence="0.781974">Incremental Construction of Cand</affiliation>
<title confidence="0.731528">F-Structure in an LFG-Parser</title>
<author confidence="0.947949">H-U Block</author>
<author confidence="0.947949">R Hunze</author>
<note confidence="0.780568666666667">in Proceedings of the 11th International Conference on Computational Linguistics, pp. 490-493</note>
<pubnum confidence="0.518486">Report No. 4</pubnum>
<title confidence="0.6127595">The Treatment of Movement Rules in an LFG-Parser</title>
<author confidence="0.792058">H-U Block</author>
<author confidence="0.792058">H Haugender</author>
<note confidence="0.8081535">in Proceedings of the 11th International Conference on Computational Linguistics,</note>
<pubnum confidence="0.798959">Report No. 5</pubnum>
<title confidence="0.995898">Lexical Analysis</title>
<author confidence="0.999595">M Gehrke</author>
<author confidence="0.999595">H-U Block</author>
<pubnum confidence="0.929736">Report No. 6</pubnum>
<title confidence="0.8707405">Probleme der Wissensreprasentation in Beratungssystemen</title>
<author confidence="0.9943345">H-U Block</author>
<author confidence="0.9943345">M Gehrke</author>
<author confidence="0.9943345">H Haugender</author>
<author confidence="0.9943345">R Hunze</author>
<pubnum confidence="0.701368">Report No. 7</pubnum>
<abstract confidence="0.999589777777778">explanation as well as the acceptability of language structure. This paper reports on work pertaining to an expanded range of explanation components in the Nixdorf exper system shell TwAIce. A critical account of the position held by grammatical theory in generating natural language at the user level is given, whereby the decision for a certain theory remains first and foremost pragmatical. Moreover, a stand is taken concerning scientific experimentation on the transfer of formal knowledge representation. Practical problems concerning technical technology are pointed out that haven&apos;t yet been taken into account. In this paper a parser for Lexical Functional Grammar (LFG) which is characterized by incrementally constructing the cand f-structure of a sentence during parsing is presented. Then the possibilities of the earliest check on consistency, coherence, and completeness are discussed. Incremental construction of f-structure leads to an early detection and abortion of incorrect paths and so increases parsing efficiency. Furthermore, those semantic interpretation processes that operate on partial structures can be triggered at an earlier state. This also leads to a considerable improvement in parsing time. LFG seems to be well suited for such an approach because it provides for locality principles by the definition of coherence and completeness. In this paper a way of treating long-distance movement phenomena as exemplified in (1) is proposed within the framework of an LFG-based parser. (1) Who do you think Peter tried to meet &apos;You think Peter tried to meet who&apos; After a short overview of the treatment of general discontinuous dependencies in the Theory of Government and Binding, Lexical Functional Grammar, and Generalized Phrase Structure Grammar, the so-called whor long-distance movement are concentrated arguing that a general mechanism which is compatible with both the LFG and the GB treatment of long-distance movement can be found. Finally, the implementation of such a movement mechanism in an LFG-parser is presented. In this paper some aspects of the advantages and disadvantages of a morpheme-based lexicon with respect to a full lexicon are discussed. Then a current implementation of an application-independent lexical access component is presented as well as an implemented formalism for the inflectional analysis of German. The present report consists of two main sections. The first part analyzes individual knowledge sources that require specialization for the consulting system W1SBER. It should serve as a first approximation to the structural analysis of all knowledge sources. In the second part, methods for the representation of knowledge and languages are examined. Regarding this, KL-ONE, interpreted as an epistemic formal structure of language representation for describing structure objects, is examined. Supplementing this is an examination of other systems which, in addition, have significant assertive components such as their disposal. the other end of the spectrum lies system that cannot clearly be semantically and epistemically interpreted as a representational language as such. these two poles lie, on the one hand, without guaranteeing the semantic clarity of the grammatical constructions used,</abstract>
<note confidence="0.337639">Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 95</note>
<title confidence="0.93710225">The FINITE STRING Newsletter Abstracts of Current Literature Beratung und natfirlichsprachlicher Dialog — eine Evaluation von Systemen der Kunstlichen Intelligenz</title>
<author confidence="0.998098">H Bergmann</author>
<author confidence="0.998098">M Gerlach</author>
<author confidence="0.998098">W Hoeppner</author>
<author confidence="0.998098">H Marburger</author>
<pubnum confidence="0.820841">Report No. 8</pubnum>
<title confidence="0.474337">Form der Ergebnisse der Wissensakquiin</title>
<author confidence="0.791598">M Fliegner</author>
<author confidence="0.791598">M-J Schachter-Radig</author>
<pubnum confidence="0.857061">Report No. 9</pubnum>
<abstract confidence="0.997935178571429">flexibly combines a large number of the ideas previously suggested and, on the other hand, KRS, representative for a group of hybrid representation systems which allow a flexible combination of various formal structures of representation. This report contains an evaluation of Artificial Intelligence systems which provide the research base for the develobment of the natural-language advisory system WISBER. First, the reasons for selecting the particular systems considered in the study are given and a set of evaluation criteria emphasizing in particular pragmatic factors (e.g., dialog phenomena, handling of speech acts, user modeling) is presented. The body of the report consists of descriptions and critical evaluations of the following systems: ARGOT, AYPA, GRUNDY, GUIDON, HAM-ANS, KAMP, OSCAR, ROMPER, TRACK, UC, VIE-LANG, WIZARD, WUSOR, XCALIBUR. The final chapter summarizes the results, concentrating on the possible utilization of individual system capabilities in the development of WISBER. In this paper fundamental questions are discussed concerning the representation of expert knowledge, exemplified within the area of investment consulting. While a written report is appropriate for a general presentation of results, it neither satisfies the needs of systems development — which of course must build upon the results of knowledge acquisition — nor can it do justice to the requirements of knowledge acquisition itself. On the other hand, epistemologically expressive knowledge representation tools require that conceptual design decisions must be made quite early on. The tools LOOPS, OPS5, prolog-based shell, and KL-ONE are dealt with.</abstract>
<note confidence="0.294832">following abstracts are from &apos;86 PROCEEDINGS, of which are available only from Poppelsdorfer Allee 47 D-5300 Bonn 1</note>
<author confidence="0.33869">WEST GERMANY</author>
<phone confidence="0.924192">Telephone: +49/228/735645</phone>
<email confidence="0.645554">EARN/BITNET:UPKOOO@DBNRHRZI</email>
<note confidence="0.886706">INTERNET: UPK000TODBNRHRZ1.BITNET@WISCVM.WISC.EDU The price is 95 DM within Europe and 110 DM for air delivery to non-European countries. Please pay in advance by check to the address above or by bankers draft to the following account: Bank far Gemeinwirtschaft Bonn Account no. 1205 163 900, BLZ 380 101 11</note>
<title confidence="0.9850475">Lexicon—Grammar: The Representation of Compound Words</title>
<author confidence="0.985927">Maurice Gross</author>
<affiliation confidence="0.7445975">Universite Paris 7 Laboratoire Documentaire et Linguistique</affiliation>
<abstract confidence="0.933985785714286">2, place Jussieu F-75221 Paris CEDEX 05 The essential feature of a lexicon-grammar is that the elementary unit of computation and storage is the simple sentence: subject-verb-complement(s). This type of representation is obviously needed for verbs: limiting a verb to its shape has no meaning other than typographic, since a verb cannot be separated from its subject and essential complements. We have shown (1975) that given a verb, or equivalently a simple sentence, the set of syntactic properties that describes its variations is unique: in general, no other verb has an identical syntactic paradigm. As a consequence, the properties of each verbal construction must be represented in a lexicon-grammar. The lexicon has no significance taken as an isolated component and the grammar component, viewed as independent of the lexicon, will have to be limited to certain complex sentences.</abstract>
<note confidence="0.723235">Linguistics, Volume 13, Numbers 1-2, January-June 1987</note>
<title confidence="0.9900695">The FINITE STRING Newsletter Abstracts of Current Literature User Models: The Problem of Disparity</title>
<author confidence="0.999934">Sandra Carberry</author>
<affiliation confidence="0.998161">Department of Computer &amp; Information Science University of Delaware</affiliation>
<address confidence="0.978826">Newark, Delaware 19716</address>
<phone confidence="0.343337">COL1NG&apos;86, pp. 29-34</phone>
<abstract confidence="0.996576901960785">A major problem in machine translation is the semantic description of lexical units which should be based on a semantic system that is both coherent and operationalized to the greatest possible degree. This is to guarantee consistency between lexical units coded by lexicographers. This article introduces a generating device for achieving well-formed semantic feature expressions. This paper discusses the semantic features of nouns classified into categories in Japanese-to-English translation, and proposes a system for semantic markers. In our system, syntactic analysis is carried out by checking the semantic compatibility between verbs and nouns. The semantic structure of a sentence can be extracted at the same time as its syntactic analysis. We also use semantic markers to select words in the transfer phase for translation into English. The system of the Semantic Markers for Nouns consists of 13 conceptual facets, including one facet for &apos;Others&apos; (discussed later), and is made up of 49 filial slots (semantic markers) as terminals. We have tested about 3,000 sample abstracts in science and technological fields. Our research has revealed that our method is extremely effective in determining the of (basic Japanese verbs) which have broader like the English verbs get, take, put, Even a superficial meaning representation of a text requires a system of semantic labels that characterize the relations between the predicates in the text and their arguments. The semantic interpretation of syntactic subjects and objects, of prepositions and subordinate conjunctions has been treated in numerous books and papers with titles including works like case, case roles, semantic roles, relations. In this paper we concentrate on the semantic relations established by predicates: what are they, what are their characteristics, how do they group the predicates. OPERA is a natural language question answering system allowing the interrogation of a data base consisting of an extensive listing of operas. The linguistic front-end of OPERA is a comprehensive grammar of French, and its semantic component translates the syntactic analysis into logical formulas (first order logic formulas). However, there are quite a few constructions which can be analyzed syntactically in the grammar but for which we are unable to specify translations. Foremost among them are anaphoric and elliptic constructions. Thus this paper describes the extension of OPERA to anaphoric and elliptic constructions on the basis of the Discourse Segmentation Theory. A significant component of a user model in an information-seeking dialogue is the task-related plan motivating the information-seeker&apos;s queries. A number of researchers have modeled the plan inference process and used these models to design more robust natural language interfaces. However, in each case it has been assumed that the system&apos;s context model and the plan under construction by the information-seeker are never at variance. This paper addresses the problem of disparate plans. It presents a four phase approach and argues that handling disparate plans requires an enriched context model. This model must permit the addition of components suggested by the information-seeker but not fully supported by the system&apos;s domain knowledge, and must differentiate among the components according to the kind of support accorded each component as a correct</abstract>
<title confidence="0.9576725">An Empirically Based Approach towards a System of Semantic Features</title>
<author confidence="0.98385">Cornelia Zelinsky-Wibbelt</author>
<email confidence="0.58568">IAI-Eurotra-D</email>
<note confidence="0.768874">Martin-Luther-StraBe 14 D-6600 Saarbrucken COL1NG&apos;86, pp. 7-12</note>
<title confidence="0.962757">and Structure of Semantic Markers for Machine Translation in Mu-Project</title>
<author confidence="0.997899">Yoshiyuki Sakamoto</author>
<affiliation confidence="0.957459">Electrotechnical Laboratory Sakura-mura. Niihari-gun.</affiliation>
<address confidence="0.654185">Ibaraki, Japan</address>
<author confidence="0.385525">Tetsuya Ishikawa</author>
<affiliation confidence="0.808513">University of Library &amp; Information Science Yatabe-machi. Tsukuba-gun.</affiliation>
<address confidence="0.810057">Ibaraki, Japan</address>
<author confidence="0.935968">Masayuki Satoh</author>
<affiliation confidence="0.875972">Information Center of Science &amp; Tech-</affiliation>
<address confidence="0.722848">nology. Nagata-cho, Chiyoda-ku Tokyo, Japan</address>
<note confidence="0.547985">COLING 86, pp. 13-20</note>
<title confidence="0.8816945">A Theory of Semantic Relations for Large Scale Natural Language Processing Hanne Ruus Institut for nordisk filologi &amp; Eurotra-DK</title>
<author confidence="0.406626">Ebbe Spang-Hanssen</author>
<affiliation confidence="0.805864">Romansk institut &amp; Eurotra-DK University of Copenhagen</affiliation>
<address confidence="0.764718">Njalsgade 80</address>
<note confidence="0.686365">DK-2300 Copenhagen S COLING&apos;86, pp. 20-22</note>
<title confidence="0.835394">Extending the Expressive Capacity of the Semantic Component of the OPERA System</title>
<author confidence="0.983414">Celestin Sedogbo</author>
<affiliation confidence="0.979298">Centre de Recherche Bull</affiliation>
<address confidence="0.965214">68, Route de Versailles 78430 Louveciennes, France</address>
<email confidence="0.778962">pp.</email>
<note confidence="0.635656">Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 97</note>
<title confidence="0.62621">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.975396333333333">part of the information-seeker&apos;s overall plan. It is shown how a component&apos;s support should affect the system&apos;s hypothesis about the source of error once plan disparity is suggested.</abstract>
<title confidence="0.9134625">Pragmatic Sensitivity in NL Interfaces and the Structure of Conversation</title>
<author confidence="0.999874">Tom Wachtel</author>
<affiliation confidence="0.995976">Ltd., London Research Unit for Information Science &amp; Al, Hamburg University</affiliation>
<title confidence="0.987776">A Two-Level Dialogue Representation</title>
<author confidence="0.999987">Giacomo Ferrari</author>
<affiliation confidence="0.999652">Department of Linguistics University of Pisa</affiliation>
<author confidence="0.991385">Ronan Reilly</author>
<affiliation confidence="0.993689">Educational Research Center</affiliation>
<address confidence="0.913318">St. Patrick&apos;s College, Dublin 9</address>
<title confidence="0.9056835">Linguistic and Query Reformulation</title>
<author confidence="0.998215">Yvette Mathieu</author>
<author confidence="0.998215">Paul Sabatier</author>
<affiliation confidence="0.938182">CNRS - LADL</affiliation>
<note confidence="0.2842775">Universite Paris 7 Tour Centrale 9 E 2 Place Jussieu 75005 Paris</note>
<title confidence="0.751269">Category Cooccurrence Restrictions and the Elimination of Metarules</title>
<author confidence="0.999961">James Kilbury</author>
<affiliation confidence="0.999981">Technical University of Berlin</affiliation>
<address confidence="0.975448666666667">KIT/NASEV, CIS, Sekr. FR5-8 Franklinstr. 28/29 D-1000 Berlin 10</address>
<affiliation confidence="0.477037">Germany — West Berlin</affiliation>
<abstract confidence="0.974866482142857">86, The work reported here is being conducted as part of the LOKI project Project logic oriented approach to knowledge and data supporting natural user interaction&amp;quot;). The goal of the of the project is to build a pragmatically sensitive natural language interface to a knowledge base. By &amp;quot;pragmatically sensitive&amp;quot;, we mean that the system should not only produce well-formed coherent and cohesive language (a minimum requirement of any NL system designed to handle discourse) but should also be sensitive to those aspects of user behaviour that humans are sensitive to over and above simply providing a good response, including output is appropriately decorated with those minor and semantically inconsequential elements of language that make the difference between natural language and natural natural language. This paper concentrates on the representation of the structure of conversation in our systems. we will first outline the representation we use for dialogue moves, and then outline the nature of the definition of wellformed dialogue that we are operating with. Finally, we will note a few extensions to the representation mechanism. In this paper a two-level dialogue representation system is presented. It is intended to recognize the structure of a large range of dialogues including some nonverbal communicative acts which may be involved in an interaction. It provides a syntactic description of a dialogue which can be expressed in terms of re-writing rules. The semantic level of the proposed representation system is given by the goal and subgoal structure underlying the dialogue syntactic units. Two types of goals are identified; goals which relate to the content of the dialogue, and those which relate to communicating the content. experience have gained in designing and using natural language has led us to develop a general language system, involving the following principles: — The linguistic coverage must be elementary but must include phenomena that allow a rapid, concise, and spontaneous interaction, such as anaphora (ellipsis, pronouns, etc.). — The linguistic competence and limits of the interface must be easily and rapidly perceived by the user. — The interface must be equipped with strategies and procedures for leading the user to adjust his linguistic competence to the capacities of the system. We have illustrated these principles in an application: a natural language (French) interface for acquiring the formal commands of some operating system languages. (The examples given here concern DCL of Digital Equipment Company.) paper upon and extends certain ideas developed within the framework of Generalized Phrase Structure Grammar (GPSG). A new descriptive device, the Category Cooccurrence Restriction (CCR), is introduced in analogy to existing devices of GPSG in order to express constraints on the cooccurrence of categories within local trees (i.e., trees of depth one) which at present are stated with Immediate Dominance &amp;idp. rules and metarules. In addition to providing a uniform format for the statement of such constraints, CCRs permit generalizations to be expressed which presently cannot be captured in GPSG. 98 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 The FINITE STRING Newsletter Abstracts of Current Literature Sections 1.1 and 1.2 introduce CCRs and presuppose only a general with ideas do not depend on details of GPSG and can be applied to other grammatical formalisms.</abstract>
<note confidence="0.753544666666667">Sections 1.3-1.5 discuss CCRs in relation to particular principles of GPSG and assume familiarity with Gazdar et al. (1985) (henceforth abbreviated as GKPS). Finally, section 2 contains proposals for using CCRs to</note>
<title confidence="0.873839333333333">avoid the analyses with metarules given for English in GKPS Processing Word Order Variation within a Modified ID/LP Framework</title>
<author confidence="0.99571">Pradip Dey</author>
<affiliation confidence="0.999986">University of Alabama at Birmingham</affiliation>
<address confidence="0.999831">Birmingham, AL 35294</address>
<phone confidence="0.931844">apos;86 65-67</phone>
<title confidence="0.990039">Sentence Adverbials in a System of Question Answering without a Prearranged Data</title>
<author confidence="0.674803666666667">Base Eva Koktova Hamburg</author>
<author confidence="0.674803666666667">West Germany</author>
<abstract confidence="0.990322159090909">COLING &apos;86 pp. 68-73 The empirical validity of the projectivity hypothesis for Bulgarian is tested. It is shown that the justification of the hypothesis presented for other languages suffers serious methodological deficiencies. Our automated testing, designed to evade such deficiencies, yielded results falsifying the hypothesis for Bulgarian: the non-projective constructions studied were in fact grammatical rather than ungrammatical, as implied by the projectivity thesis. Despite this, the projectivity/non-projectivity distinction itself has to be retained in Bulgarian syntax and, with some provisions, in the systems for automatic processing as well. The purpose of this contribution is to formulate ways in which the homonymy of so-called &apos;Modal Particles&apos; and the etymons can be handled. Our aim is to show that not only a strategy for this type of homonymy can be worked out, but also a formalization of information beyond propositional content can be introduced with a view to its MT application. This paper presents an approach for processing incomplete and inconsistent knowledge. Basis for attaching these problems are &apos;structures of determination&apos;, which are extensions of Scott&apos;s approximation lattices taking into consideration some requirements from natural language processing and representation of knowledge. The theory developed is exemplified with processing plural noun phrases referring to objects which have to be understood as classes or sets. Referential processes are handled by processes on &apos;Referential Nets&apos;, which are a specific knowledge structure developed for the representation of object-oriented knowledge. Problems of determination with respect to cardinality assumptions are emphasized. From a well represented sample of world languages, Steel (1978) shows that about 78% of languages exhibit significant word order variation. Only recently has this wide-spread phenomenon been drawing appropriate attention. Perhaps ID/LP (Immediate Dominance and Linear Precedence) framework is the most debated theory in this area. We point out some difficulties in processing standard ID/LP grammar and present a modified version of the grammar. In the modified version, the right-hand side of phrase structure rules is treated as a set or partially-ordered set. An instance of the framework is implemented. In the present paper we provide a report on a joint approach to the computreatment of sentence adverbials (such as presumably, focussing adverbials (such as least, some other adverbial expressions, such as example alia) a system of question answering a prearranged data base This approach is based on a joint theoretical account of the expressions in question in the framework of a functional description of language; we argue that in the primary case, the expressions in question occupy, in the underlying topic-focus articulation of a sentence, the focus-initial position,</abstract>
<title confidence="0.959904">Testing the Projectivity Hypothesis</title>
<author confidence="0.994683">Vladimir Peridiev</author>
<affiliation confidence="0.9938335">Mathematical Linguistics Dept. Institute of Mathematics with Comp Centre</affiliation>
<address confidence="0.999326">1113 Sofia, b1.8, Bulgaria</address>
<author confidence="0.843976">Harlon Harionov</author>
<affiliation confidence="0.996463">Mathematics Dept. Inst of English</affiliation>
<address confidence="0.874835">Sofia, Bulgaria</address>
<title confidence="0.915771">Particle Homonymy and Machine Translation</title>
<affiliation confidence="0.805125">JATE University of Szeged</affiliation>
<note confidence="0.5066205">Egyetem u. 2. Hungary H - 6722</note>
<title confidence="0.8463925">Plurals, Cardinalities, and Structures of Determination</title>
<author confidence="0.998951">Christopher U Habd</author>
<affiliation confidence="0.906545">Universitat Hamburg, Fachbereich Informatik</affiliation>
<address confidence="0.78788">Schltiterstr. 70</address>
<note confidence="0.399401333333333">D-1000 Hamburg 13 62-64 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 99</note>
<title confidence="0.4905">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.9969762">extending their scope over the focus, or the new information, of a sentence, thus specifying, in a broad sense of the word, how the next information of a sentence holds. On the surface the expressions in question are usually moved to scope-ambiguous positions, which can be analyzed by means of several general strategies.</abstract>
<title confidence="0.9798255">D-PATR: A Development Environment for Unification-Based Grammars</title>
<author confidence="0.99998">Lauri Kartunnen</author>
<affiliation confidence="0.999751">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.997897">333 Ravenswood Avenue Menlo Park, CA 94025</address>
<degree confidence="0.381236">for the Study of Language and</degree>
<affiliation confidence="0.81306">Information, Stanford University</affiliation>
<address confidence="0.615937">74-80</address>
<title confidence="0.749768">Structural Correspondence Specification Environment</title>
<author confidence="0.727567">gfeng Yan Yon</author>
<affiliation confidence="0.840202">Groupe d&apos;Etudes pour la Traduction University of Grenoble</affiliation>
<address confidence="0.8078185">38402 Saint Martin d&apos;Heres, France 86, 81-84</address>
<title confidence="0.992508">Conditioned Unification for Natural Language Processing</title>
<author confidence="0.992996">Ko&apos;iti Hasida</author>
<affiliation confidence="0.999822">Electrotechnical Laboratory</affiliation>
<address confidence="0.730347">Umezono 1-1-4, Sakura-Mura, Niibari-Gun Ibaraki, 305 Japan</address>
<email confidence="0.631933">86,</email>
<title confidence="0.8714655">Methodology and Verifiability in Montague Grammar</title>
<author confidence="0.8962">Seiki Akama</author>
<affiliation confidence="0.992586">Fujitsu Ltd.</affiliation>
<address confidence="0.9891605">2-4-19, Sin-Yokohama Yokohama, 222, Japan</address>
<title confidence="0.701006">a Dedicated Database Management System for Dictionaries</title>
<author confidence="0.999167">Marc Domenig</author>
<author confidence="0.999167">Patrick Shann</author>
<affiliation confidence="0.8485545">Institut Dalle Molle pour les Etudes Semantiques et Cognitives &amp;isscop.</affiliation>
<address confidence="0.738678666666667">Route des Acacias 54 1227 Geneva, Switzerland 91-96</address>
<abstract confidence="0.996641348837209">a development environment for unification-based grammars on 1100 series work stations. It is based on the developed at SRI International. This formalism is suitable for encoding a wide variety of grammars. At one end of this range are simple phrase-structure grammars with no feature augmentations. The PATR formalism can also be used to encode grammars that are based on a number of current linguistic theories, such as lexical-functional grammar (Bresnan and Kaplan), headdriven phrase structure grammar (Pollard and Sag), and functional unification grammar (Kay). At the other end of the range covered by D-PATR are unification-based categorial grammars (Klein, Steedman, Uszkoreit, Wittenberg) in which all the syntactic information is incorporated in the lexicon and the remaining few combinatorial rules that build phrases are function application and composition. Definite-clause grammars (Pereira and Warren) can also be encoded in the PATR formalism. This article presents the Structural Correspondence Specification Environment (SCSE) being implemented at GETA. The SCSE is designed to help linguists to develop, consult, and verify the SCS grammars (SCSG) which specify linguistic models. It integrates the techniques of data bases, structure editors, and language interpreters. We argue that formalisms and tools of specification are as important as the specification itself. This paper presents what we call a conditional unification, a new method of unification for processing natural languages. The key idea is to annotate the patterns with a certain sort of conditions, so that they carry abundant information. This method transmits information from one pattern to another more efficiently than procedure attachments, in which information contained in the procedure is embedded in the program rather than directly attached to patterns. Coupled with techniques in formal linguistics, moreover, conditioned unification serves most types of operations for natural language processing. Methodological problems in Montague Grammar are discussed. Our observations show that a mode-theoretic approach to natural language semantics is inadequate with respect to its verifiability from a logical point of view. But, the formal attitudes seem to be of use for the development in computational linguistics. This paper argues that a lexical data base should be implemented with a special kind of database management system (DBMS) and outlines the design of such a system. The major difference between this proposal and a general purpose DBMS is that its data definition language (DDL) allows the specification of the entire morphology, which turns the lexical data base from a mere collection of &apos;static&apos; data into a real-time word-analyzer. Moreover, the dedication of the system conduces to the feasibility of user interfaces with very comfortable monitor and manipulation functions.</abstract>
<note confidence="0.280847">100 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987</note>
<title confidence="0.997258">The FINITE STRING Newsletter Abstracts of Current Literature The Transfer Phase of the Mu Machine Translation System</title>
<author confidence="0.999672">Makoto Nagao</author>
<author confidence="0.999672">Jun-ichi Tsujii</author>
<affiliation confidence="0.9996055">Department of Electrical Engineering Kyoto University</affiliation>
<address confidence="0.907968">Kyoto, Japan 606</address>
<title confidence="0.961719">Lexical Transfer: A Missing Element in Linguistics Theories</title>
<author confidence="0.999943">Alan K Melby</author>
<affiliation confidence="0.9718065">Brigham Young University Department of Linguistics</affiliation>
<address confidence="0.989605">Provo, Utah 84602</address>
<title confidence="0.8922125">Idiosyncratic Gap: A Tough Problem to Structure-Based Machine Translation</title>
<author confidence="0.975631">Yoshihiko Nitta</author>
<affiliation confidence="0.9980745">Advanced Research Laboratory Hitachi Ltd.</affiliation>
<address confidence="0.882079">Kokubunji, Tokyo 185 Japan</address>
<title confidence="0.893030333333333">Lexical-Functional Transfer: A Transfer Framework in a Machine-Translation Based on</title>
<author confidence="0.99442">Ikuo Kudo</author>
<affiliation confidence="0.999945">CSK Research Institute</affiliation>
<address confidence="0.9938195">3-22-17 Higashi-Ikebukuro, Toshima-ku Tokyo, 170, Japan</address>
<author confidence="0.971421">Hirosato Nomura</author>
<affiliation confidence="0.998725">NTT Basic Research Laboratories</affiliation>
<address confidence="0.981347">Musashino-shi, Tokyo, 180, Japan</address>
<abstract confidence="0.9844001">The interlingual approach to MT has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application. However, not only the ambiguity but also the vagueness which every natural language inevitably has leads this approach into essential difficulties. In contrast, our project, the Mu-project, adopts the transfer approach as the basic framework of MT. This paper describes the detailed construction of the transfer phase of our system from Japanese to English, and gives some examples of problems which seem difficult to treat in the interlingual approach.</abstract>
<title confidence="0.904291">Some of the design principles relevant to the topic of this paper are: • Multiple Layer of Grammars • Multiple Layer Presentation • Lexicon Driven Processing • Form-Oriented Dictionary Description</title>
<abstract confidence="0.99807347368421">This paper also shows how these principles are realized in the current system. One of the necessary tasks of a machine translation system is lexical transfer. In some cases there is a one-to-one mapping from source language word to target language word. What theoretical model is followed when there is a one-to-many mapping? Unfortunately, none of the linguistic models that have been used in machine translation include a lexical transfer component. In the absence of a theoretical model, this paper will suggest a new way to test lexical transfer systems. This test is being applied to an MT system under development. One possible conclusion may be that further effort should be expended developing models of lexical transfer. Current practical machine translation systems, which are designed to deal with a huge amount of documents, are generally structure-based. That is, the translation process is done based on the analysis and transformation of the structure of the source sentence, not on the understanding and paraphrasing of the meaning of that sentence. But each language has its own syntactic and semantic idiosyncrasy, and on this account, without understanding the total meaning of the source.sentence, it is often difficult for MT to bridge properly the idiosyncratic gap between source and target language. A somewhat new method call &amp;quot;Cross Translation Test&amp;quot; is presented that reveals the detail of idiosyncratic gap together with the so-so satisfiable possibility of MT The usefulness of the sublanguage approach in reducing the idiosyncratic gap between source and target languages is also mentioned. This paper presents a transfer framework called LFT (Lexical-Functional Transfer) for a machine translation system based on LFG (Lexical-Functional Grammar). The translation process consists of subprocesses of analysis, transfer, and generation. We adopt the so-called f-structures of LFG as the intermediate representations or interfaces between those subprocesses, thus the transfer process converts a source f-structure into a target Since a grammatical framework for sentence structure analysis of one language, for the purpose, we propose a new framework for specifying transfer rules with LFG schemata, which incorporates corresponding lexical functions of two different languages into an equational representation. The transfer process, therefore, is to solve equations called target f-descriptions derived from the transfer rules applied to the source f-structure and then to produce a target f-structure.</abstract>
<note confidence="0.663305">Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 101</note>
<title confidence="0.9841565">The FINITE STRING Newsletter Abstracts of Current Literature Transfer and MT Modularity</title>
<author confidence="0.99724">Pierre Isabelle</author>
<author confidence="0.99724">Elliott Macklovitch</author>
<affiliation confidence="0.981429">Canadian Workplace Automation Research Center</affiliation>
<address confidence="0.945235">1575 Chomedey Boulevard Laval, Quebec, Canada H7V 2X2</address>
<title confidence="0.991485">The Need for MT-Oriented Versions of Case and Valency in MT</title>
<author confidence="0.999549">Harold L Somers</author>
<affiliation confidence="0.97463">Centre for Computational Linguistics University of Manchester Institute of Science and Technology</affiliation>
<title confidence="0.956655">A Parametric NL Translator</title>
<author confidence="0.999923">Randall Sharp</author>
<affiliation confidence="0.9997975">Dept. of Computer Science University of British Columbia</affiliation>
<address confidence="0.9495">Vancouver, Canada</address>
<pubnum confidence="0.302932">124-126</pubnum>
<title confidence="0.839811">Lexicase Parsing: A Lexicon-Driven Approach to Syntactic Analysis</title>
<author confidence="0.971684">Stanley Starosta</author>
<affiliation confidence="0.960846333333333">University of Hawaii Social Science Research International Center for High Technology Research</affiliation>
<address confidence="0.998432">Honolulu, Hawaii 96822</address>
<author confidence="0.929636">Hirasato Nomura</author>
<affiliation confidence="0.997634">NTT Basic Research Laboratories</affiliation>
<address confidence="0.987471">Musashino-shi, Tokyo, 180, Japan</address>
<title confidence="0.979492333333333">Solutions for Problems of MT Parser Methods used in Mu-Machine Translation Project</title>
<author confidence="0.961137">Jun-ichi Nakamura</author>
<author confidence="0.961137">Jun-ichi Makoto Nagao</author>
<affiliation confidence="0.999531">Dept. of Electrical Engineering Kyoto University</affiliation>
<address confidence="0.966321">Sakyo, Kyoto 606, Japan</address>
<abstract confidence="0.979611888888889">The transfer components of typical second generation (G2) MT systems do not fully conform to the principles of G2 modularity, incorporating extensive target language information while failing to separate translation facts from linguistic theory. The exclusion from transfer of all non-contrastive information leads us to a system design in which the three major components operate in parallel rather than in sequence. We also propose that MT systems be designed to allow translators to express their knowledge in natural metalanguage statements. This paper looks at the use in machine translation systems of the linguistic models of Case and Valency. It is argued that neither of these models was originally developed with this use in mind, and both must be adapted somewhat to meet this purpose. In particular, the traditional Valency distinction of complements and adjuncts leads to conflicts when valency frames in different languages are compared: a finer but more flexible distinction is required. Also, these concepts must be extended beyond the verb, to include the noun and adjective as valency bearers. As far as Case is concerned, too narrow an approach has traditionally been taken: work in this field has been too concerned only with cases for arguments in verb frames; case label systems for non-valency bound elements and also for elements in nominal groups must be elaborated. The paper suggests an integrated approach specifically oriented towards the particular problems found in MT. This report outlines a machine translation system whose linguistic component is based on principles of Government and Binding. A &amp;quot;universal grammar&amp;quot; is defined, together with parameters of variation for specific languages. The system, written in Prolog, parses, generates, and translates between English and Spanish (both directions). This paper presents a lexicon-based approach to syntactic analysis, Lexicase, and applies it to a lexicon-driven computational parsing system. The basic descriptive mechanism in a Lexicase grammar is lexical features. The properties of lexical items are represented by contextual and non-contextual features, and generalizations are expressed as relationships among sets of these features and among sets of lexical entries. Syntactic tree structures are represented as networks of pairwise dependency relationships among the words in a sentence. Possible dependencies are marked as contextual features on individual lexical items, and Lexicase parsing is a process of picking out words in a string and attaching dependents to them in accordance with their contextual features. Lexicase is an appropriate vehicle for parsing because Lexicase analyses are monostratal, flat, and relatively non-abstract, and it is well suited to machine translation because grammatical representations for corresponding sentences in two languages will be very similar to each other in structure and inter-constituent relations, and thus far easier to interconvert. A parser is a key component of a machine translation system. If it fails in parsing an input sentence, the MT system cannot output a complete translation. A parser of a practical MT system must solve many problems caused by the varieties of characteristics of natural languages. Some problems are caused by the incompleteness of grammatical rules and dictionary information, and some by the ambiguity of natural languages. Others are caused by various types of sentence constructions, such as itemization, insertion by parentheses, and other typographical conventions that cannot be naturally captured by ordinary linguistic rules. 102 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 The FINITE STRING Newsletter Abstracts of Current Literature The authors of this paper have been developing MT systems between Japanese and English (in both directions) under the Mu-machine translation project. In the system&apos;s development, several methods have been with grammar writing language solve the problems of the MT parser. In this paper, first the characteristics of GRADE and the Mu-MT parser are briefly described. Then, methods to solve the MT parsing problems that are caused by the varieties of sentence constructions and the ambiguities of natural languages are discussed from the viewpoint of efficiency and maintainability.</abstract>
<title confidence="0.810138666666667">Strategies and Heuristics in the Analysis of a Natural Language in Machine Translation Zaharin Yusoff Groupe d&apos;Etudes pour la Traduction Automatique</title>
<pubnum confidence="0.267627">68</pubnum>
<affiliation confidence="0.99463">Universite de Grenoble</affiliation>
<address confidence="0.997012">38402 Saint-Martin-d&apos;Heres, France</address>
<email confidence="0.926297">pp.</email>
<title confidence="0.941612">Parsing in Parallel</title>
<author confidence="0.979788">Xiuming Huang</author>
<author confidence="0.979788">Louise Guthrie</author>
<affiliation confidence="0.999809">Computing Research Laboratory New Mexico State University</affiliation>
<address confidence="0.999455">Las Cruces, NM 88003</address>
<email confidence="0.855303">pp.</email>
<title confidence="0.698434">Computational Comparative Studies on Romance Languages: A Linguistic Comparison of Lexicon-Grammars</title>
<author confidence="0.719357333333333">Annibale Elia Istituto di_Linguistica Universita di_Salerno Wile Mathieu</author>
<affiliation confidence="0.850803666666667">Laboratoire d&apos;Automatique Documentaire et Linguistique - Universite de Paris</affiliation>
<address confidence="0.509692">86,</address>
<title confidence="0.995948">A Stochastic Approach to Parsing</title>
<author confidence="0.999738">Geoffrey Sampson</author>
<affiliation confidence="0.9992965">Department of Linguistics and Phonetics University of Leeds</affiliation>
<address confidence="0.576566">COUNG&apos;86, pp. 151-155</address>
<title confidence="0.99842">Parsing Without (Much) Phrase Structure</title>
<author confidence="0.999985">Michael B Kac</author>
<affiliation confidence="0.9991645">Department of Linguistics University of Minnesota</affiliation>
<abstract confidence="0.9821577">The analysis phase in an indirect, transfer, and global approach to machine translation is studied. The analysis conducted can be described as exhaustive (meaning with backtracking), depth-first, and strategically and heuristically driven, while the grammar used is an augmented context free grammar. The problem areas, being pattern matching, ambiguities, forward propagation, checking for correctness, and backtracking, are highlighted. Established results found in the literature are employed whenever adaptable, while suggestions are given otherwise. The paper is a description of a parallel model for natural language parsing, and a design for its implementation on the Hypercube multiprocessor. The parallel model is based on the Semantic Definite Clause Grammar formalism and integrates syntax and semantics through the communication of processes. The main processes, of which there are six, contain either purely syntactic or purely semantic information, giving the advantage of simple; transparent algorithms dedicated to only one aspect of parsing. Communication between processes is used to impose semantic constraints on the syntactic processes. What we present here is an application on the basis of the Italian and French linguistic data bank assembled by the Istituto di Linguistica of Salerno University (Italy) and the Laboratoire Automatique Documentaire et Linguistique (C.N.R.S.-France). These two research centers have been working for years to the constitution of formalized grammars of the respective languages. The composition of lexicon-grammars is the first stage of this project. Simulated annealing is a stochastic computational technique for finding optimal solutions to combinatorial problems for which the combinatorial explosion phenomenon rules out the possibility of systematically examining each alternative. It is currently being applied to the practical problem of optimizing the physical design of computer circuitry, and to the theoretical problems of resolving patterns of auditory and visual stimulation into meaningful arrangements of phonemes and three-dimensional objects. Grammatical parsing — resolving unanalyzed linear sequences of words into meaningful grammatical structures — can be regarded as a perception problem logically analogous to those just cited, and simulated annealing holds great promise as a parsing technique. to conform in varying degrees to the older relational/dependency model (essentially that assumed in traditional grammar), which treats a sentence as a group of words united by various relations, and the newer constituent model. ... In computational linguistics Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 103</abstract>
<affiliation confidence="0.46759">The FINITE STRING Newsletter Abstracts of Current Literature</affiliation>
<address confidence="0.979227">Minneapolis, MN 55455</address>
<author confidence="0.760623">Alexis Manaster-Ramer Program in Linguistics</author>
<affiliation confidence="0.999694">University of Michigan</affiliation>
<address confidence="0.999482">Ann Arbor, MI 48109</address>
<email confidence="0.762656">86,</email>
<title confidence="0.957659">Requirements for Robust Natural Language Interfaces: The LanguageCraft and XCALIBUR Experiences</title>
<author confidence="0.99998">Jaime G Carbonell</author>
<affiliation confidence="0.9877965">Carnegie-Mellon University Inc.</affiliation>
<address confidence="0.999844">Pittsburgh, PA 15213</address>
<phone confidence="0.387227">162-163</phone>
<abstract confidence="0.996860676470588">there is a strong (if not universal) reliance on phrase structure as the medivia which to represent syntactic structure; call this the view. ... In its strongest form, the consensus view says that the recovery of a fully specified parse tree is an essential step in computational language processing, and would, if correct, provide important support for the constituent model. In this paper, we shall critically examine the rationale for this view, and will sketch (informally) an alternative view which we find more defensible. The actual position we shall take for this discussion, however, is conservative in that we will not argue that there is no place whatever for constituent analysis in parsing or in syntactic analysis gener- What we that phrase structure is at least partly redundant in that a direct leap to the composition of some semantic units is possible from a relatively underspecified syntactic representation (as opposed to a complete parse tree). In this paper we will describe an approach to parsing, one major compoof which is a strategy called this strategy, no structure building is attempted until after completion of a preliminary phase designed to exploit low-level information to the fullest possible extent. This first pass then defines a set of constraints that restrict the set of available options when structure building proper begins. R-A parsing is in principle compatible with a variety of different views regarding the nature of syntactic representation, though it fits more comfortably with some than with others. BY THE CHAIR The goal of this panel is to evaluate three natural language interfaces which were introduced to the commercial market in 1985 (cf. Carnegie Group 1985, Kamins 1985, Texas Instruments 1985) and to relate them to current research in computational linguistics. Each of the commercial systems selected as a starting point for the discussion (see Wahlster 1986 for a functional comparison) was developed by a well-known scientist with considerable research experience in NL processing: LanguageCraft&apos; by Carnegie Group (designed the direction of NLMenu by Texas Instruments under the direction of and Q &amp; by Symantec (designed under the direction of G. Hendrix).</abstract>
<affiliation confidence="0.760065">I Trademark of Carnegie-Group, Inc. 2Trademark of Symantec Corporation</affiliation>
<abstract confidence="0.992743555555556">STATEMENT Natural Language interfaces to data bases and expert systems require the investigation of several crucial in order to be judged their end users and producthe developers of applications. User habitability is measured in terms of linguistic coverage, robustness of behavior and speed of response, whereas implementer activity is measured by the amount of effort required to connect the interface to a new application, to develop its syntactic and semantic grammar, and to test and debug the resultant system assuring a certain level of performance. These latter criteria have not been addressed directly by natural language researchers in pure laboratory settings, with the exception of user-defined extensions to an existing inter- (e.g., NanoKLAUS, VOX). order to amortize the cost of developing practical, robust, and efficient interfaces over multiple applications, the implementer productivity requirements are as important as user habitability. We treat each set of criteria in turn, drawing from our experience in XCALIBUR, and in LanguageCraft a commercially available environment and run-time module for rapid development of domain-oriented natural language interfaces. In our discussion we distill the general lessons accrued</abstract>
<title confidence="0.974037">Parsing</title>
<author confidence="0.999864">Michael B Kac</author>
<author confidence="0.999864">Tom Rindflesch</author>
<affiliation confidence="0.999966">Department of Linguistics University of Minnesota</affiliation>
<address confidence="0.999783">Minneapolis, MN 55455</address>
<author confidence="0.996917">Karen L Ryna</author>
<affiliation confidence="0.9574905">Computer Sciences Center Honeywell, Inc.</affiliation>
<address confidence="0.999927">Minneapolis, MN 55427</address>
<title confidence="0.9811625">Panel: Natural Language Interfaces — Ready for Commercial Success?</title>
<author confidence="0.907837">Wolfgang Wahlster</author>
<affiliation confidence="0.9997865">Department of Computer Science University of SaarbrUcken</affiliation>
<address confidence="0.782181">D-6600 Saarbrticken 11</address>
<affiliation confidence="0.467608">Fed. Rep. of Germany</affiliation>
<address confidence="0.316921">86 161 104 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987</address>
<title confidence="0.617105">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.984785568181818">from several years of experience using these systems, and conducting several small-scale user studies. (Responses to moderator&apos;s question based on Q&amp;A.) STATEMENT I don&apos;t think that natural language interfaces are a very good idea. By that I mean conventional natural language interfaces — the kind where the user types in a question and the system tries to understand it. Oh sure, when (if?) computers have world knowledge that is comparable to what humans need to communicate with each other, natural language interfaces will be easy to build and, depending on what else is available, might be a good way to communicate with computers. But today we are s0000 far away from having that much knowledge in a system, conventional natural language interfaces don&apos;t make sense. There is something different that makes more sense — NLMenu. It is a combination of menu technology with natural language understanding technology, and it eliminates many of the deficiencies one finds with conventional natural language interfaces while retaining the important benefits. This paper will explore and discuss the less obvious ways syntactic structure is used to convey information and how this information could be used by a natural language database system as a heuristic to organize and search a discourse space. The primary concern of this paper will be to present a general theory of processing which capitalizes on the information provided by such non-SVO orders as inversion, (wh) clefting, and prepositional phrase fronting. This paper gives a formal theory of presupposition using situation semantics developed by Barwise and Perry. We will slightly modify Barwise and Perry&apos;s original theory of situation semantics so that we can deal with nonmonotonic reasonings which are very important for the formalization of presupposition in natural language. This aspect is closely related to the formulation of incomplete knowledge in artificial intelligence. The function words of a language provide explicit information about how propositions are to be related. We have examined a subset of these function words, namely the subordinating conjunctions which link propositions within a sentence, using sentences taken from corpora stored on magnetic tape. On the basis of this analysis, a computer program for Dutch language generation and comprehension has been extended to deal with the subordinating conjunctions. We present an overview of the underlying dimensions that were used in describing the semantics and pragmatics of the Dutch subordinating conjunctions. We propose a Universal set of Linking Dimensions, sufficient to specify the subordinating conjunctions in any This a first proposal for the representation required for a Q&amp;A: Already a Success?</abstract>
<author confidence="0.999052">Gary G Hendrix</author>
<affiliation confidence="0.999533">Symantec Corporation</affiliation>
<address confidence="0.997741">Cupertino, CA 95014</address>
<title confidence="0.9472235">The Commercial Application of Natural Language Interfaces</title>
<author confidence="0.999612">Harry Tennant</author>
<affiliation confidence="0.909796">Computer Science Center Texas Instruments</affiliation>
<address confidence="0.554554">Dallas, Texas</address>
<affiliation confidence="0.478242">end of panel..</affiliation>
<title confidence="0.9548455">The Role of Inversion and PP-Fronting in Relating Discourse Elements</title>
<author confidence="0.999866">Mark Vincent LaPolla</author>
<affiliation confidence="0.999937666666667">Artificial Intelligence Laboratory The Department of Linguistics University of Texas at Austin</affiliation>
<address confidence="0.986123">Austin, Texas</address>
<phone confidence="0.347653">168-173</phone>
<title confidence="0.861489">Situational Investigation of Presupposition</title>
<author confidence="0.859749">Seiki Akama</author>
<affiliation confidence="0.986643">Fujitsu Ltd.</affiliation>
<address confidence="0.924055">2-4-19 ShinYokohama Yokohama, Japan</address>
<author confidence="0.985227">Masahito Kawamori</author>
<affiliation confidence="0.999984">Sophia University</affiliation>
<address confidence="0.9672255">7 Kioicho, Chiyodaku Tokyo, Japan</address>
<note confidence="0.386551">76</note>
<title confidence="0.996861">Linking Propositions</title>
<author confidence="0.999192">D S Brie</author>
<author confidence="0.999192">R A Smit</author>
<affiliation confidence="0.9992675">Rotterdam School of Management Erasmus University</affiliation>
<address confidence="0.998178">NL-3000 DR Rotterdam, The Netherlands</address>
<note confidence="0.4173185">177-180 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 105</note>
<title confidence="0.746038">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<abstract confidence="0.921025">computer program to understand or translate the subordinating conjunctions of any natural language.</abstract>
<title confidence="0.939802">Discourse and Cohesion in Expository Text</title>
<author confidence="0.999434">Allen B Tucker</author>
<author confidence="0.999434">Sergei Nirenburg</author>
<affiliation confidence="0.999135">Computer Science Colgate University</affiliation>
<author confidence="0.999781">Victor Raskin</author>
<affiliation confidence="0.999957">Department of English Purdue University</affiliation>
<address confidence="0.650689">181-183</address>
<title confidence="0.90233">Degrees of Understanding</title>
<author confidence="0.995295">Eva Hajeovd</author>
<author confidence="0.995295">Petr Sgall</author>
<affiliation confidence="0.993522">Faculty of Mathematics and Physics Charles University</affiliation>
<address confidence="0.680578">Malostranske n. 25 Prague 1, Czechoslovakia 86, 184-186</address>
<title confidence="0.99533">Categorial Unification Grammars</title>
<author confidence="0.986734">Hans Uszkoreit</author>
<affiliation confidence="0.914217666666667">Intelligence Center, Internafor the Study of Languages and Information, Stanford University</affiliation>
<address confidence="0.628068">COLING*86, pp. 187-194</address>
<abstract confidence="0.998863732142857">This paper discusses the role of discourse in expository text, text which typically comprises published scholar papers, textbooks, proceedings of conferences, and other highly stylized documents. Our purpose is to examine the extent to which those discourse-related phenomena that generally assist the analysis of dialogue text — where speaker, hearer, and speech-act information are more actively involved in the identification of plans and goals — can be used to help with the analysis of expository text. In particular, we make the optimistic assumption that expository text is strongly i.e., that pairs of clauses in such a text are connected by &amp;quot;cohesion markers&amp;quot;, both explicit and implicit. We investigate the impact that this assumption may have on the depth of understanding that can be achieved, the underlying semantic structures, and the supporting knowledge base for the analysis. An application of this work in designing the Al-based machine translation model, TRANSLATOR, is discussed in Nirenburg et al. (page 627 of these Proceedings). Along with &amp;quot;static&amp;quot; or &amp;quot;declarative&amp;quot; descriptions of language system, models of language use (the regularities of communicative competence) are constructed. One of the outstanding aspects of this transfer of attention consists in the efforts devoted to automatic comprehension of natural language which, since Winograd&apos;s SHRDLU, are presented in many different contexts. One speaks about understanding, or comprehension, although it may be noticed that the term is used in different, and often meanings. translation systems, as the late B. Vauquois pointed out (see now Vauquois and Boitet, 1985), a flexible system combining different levels of automatic analysis is necessary (i.e., the transfer component should be able to operate at different levels). The human factor cannot be completely dispensed with; it seems inevitable to include post-edition, or such a division of labor as that known from the system METEO. Not only should the semantico-pragmatic items present in the source language structure be reflected but also certain aspects of factual knowledge (see Slocum 1985: 16). It was pointed out by Kirschner (1982: 18) that, to a certain degree, this requirement can be met by means of a system of semantic features. For NL comprehension systems the automatic formulation of a partial image of the world often belongs to the core of the system; such a task certainly goes far beyond pure linguistic analysis and description. Winograd (1976: 269,275) claims that a linguistic description should handle &amp;quot;the entire complex of the goals of the speaker&amp;quot;. It is then possible to ask what are the main features relevant for the patterning of this complex and what are the relationships between understanding all the goals of the speaker and having internalized the system of a natural language. It seems to be worthwhile to reexamine the different kinds and degrees of understanding. Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely-used representations makes them attractive for computational applications and for linguistic research. In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration in the wider context of research on unification grammars. Approaches to selected linguistic 106 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 FINITE STRING Abstracts of Current Literature phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed.</abstract>
<title confidence="0.94539">Dependency Unification Grammar</title>
<author confidence="0.999682">Peter Hellwig</author>
<affiliation confidence="0.999969">University of Heidelberg</affiliation>
<address confidence="0.997783">D-6900 Heidelberg, West Germany</address>
<title confidence="0.9045425">The Weak Generative Capacity of Parenthesis-Free Categorial Grammars</title>
<author confidence="0.99882">Joyce Friedman</author>
<author confidence="0.99882">Dani Dai</author>
<author confidence="0.99882">Weiguo Wang</author>
<affiliation confidence="0.999951">Computer Science Department Boston University</affiliation>
<address confidence="0.9995175">111 Cummington Street Boston, MA 02215</address>
<title confidence="0.95447">Tree Adjoining and Head Wrapping</title>
<author confidence="0.9996905">E Vijay-Shanker</author>
<author confidence="0.9996905">David Weir</author>
<author confidence="0.9996905">Aravind K Joshi</author>
<affiliation confidence="0.998107">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999631">Philadelphia, PA 19104</address>
<email confidence="0.657303">86,</email>
<title confidence="0.944052">Categorial Grammars for Strata of Non- CF Languages and their Parsers</title>
<author confidence="0.999847">Michal P Chytil</author>
<affiliation confidence="0.999887">Charles University</affiliation>
<address confidence="0.925028">Malostranske nam. 25 118 00 Praha 1, Czechoslovakia</address>
<author confidence="0.988818">Hans Karlgren</author>
<email confidence="0.638841">KVAL</email>
<note confidence="0.6265205">Sodermalstorg 8 116 45 Stockholm, Sweden</note>
<title confidence="0.885688">Simple Reconstruction of</title>
<author confidence="0.999785">Stuart M Shieber</author>
<affiliation confidence="0.948802666666667">Artificial Intelligence Center, SRI Interfor the Study of Language and Information, Stanford University</affiliation>
<title confidence="0.970601">Kind Types in Knowledge Representation</title>
<author confidence="0.999945">K Dahlgren</author>
<affiliation confidence="0.999806">IBM Los Angeles Scientific Center</affiliation>
<address confidence="0.815473">11601 Wilshire Blvd.</address>
<abstract confidence="0.994856139534884">This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars. The principles of Dependency Unification Grammar (DUGs) are discussed. The computer language DRL (Dependency Representation Language) is introduced in which DUGs can be formulated. A unification-based parsing procedure is part of the formalism. PLAIN is implemented at the universities of Heidelberg, Bonn, Flensburg, Kiel, Zurich, and Cambridge, U.K. We study the weak generative capacity of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules. With forward cancellation as the only rule, the grammars are weakly equivalent to context-free grammars. When a backward combination rule is added, it is no longer possible to obtain all the context-free languages. With suitable restriction of the forward partial rule, the languages are still context-free and a push-down automaton can be used for recognition. Using the unrestricted rule of forward partial combination, a context-sensitive language is obtained. In this paper we discuss the formal relationship between the classes of languages generated by Tree Adjoining Grammars and Head Grammars. In particular, we show that Head Languages are included in Tree Adjoining Languages and that Tree Adjoining Grammars are equivalent to a modification of Head Grammars called Modified Head Grammars. The inclusion of MHL in HL, and thus the equivalence of HGs and TAGs, in the most general case remains to be established. We introduce a generalization of categorial grammar extending its descriptive power, and a simple model of categorial grammar parser. I3oth tools can be adjusted to particular strata of languages via restricting grammatical or computational complexity. Like most linguistic theories, the theory of generalized phrase structure grammar (GPSG) has described language axiomatically, that is, as a set of universal and language-specific constraints on the well-formedness of linguistic elements of some sort. The coverage and detailed analysis of English grammar in the ambitious recent volume by Gazdar, Klein, Pullum, Sag entitled Phrase Structure Grammar, impressive, in part because of the complexity of the axiomatic system developed by the authors. In this paper, we examine the possibility that simpler descriptions of the same theory can be achieved through a slightly different, albeit still axiomatic, method. Rather than characterize the well-formed trees directly, we progress in two stages by procedurally characterizing the wellformedness axioms themselves, which in turn characterize the trees. This paper describes Kind Types (KT), a system which uses commonsense knowledge to reason about natural language text. KT encodes some of the knowledge underlying natural language understanding, including category distinctions and descriptions differentiating real-world objects, states, and</abstract>
<note confidence="0.642953">Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 107</note>
<title confidence="0.769007">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<address confidence="0.978298">Los Angeles, CA 90025</address>
<author confidence="0.997063">J McDowdl</author>
<affiliation confidence="0.9998865">Department of Linguistics University of Southern California</affiliation>
<address confidence="0.724286">Los Angeles, CA 90089 COLING&apos;86, pp. 216-221</address>
<title confidence="0.972884333333333">DCKR — Knowledge Representation in Prolog and Its Application to Natural Language Processing</title>
<author confidence="0.988019">Hozumi Tanaka</author>
<affiliation confidence="0.9995315">Tokyo Institute of Technology Department of Computer Science</affiliation>
<address confidence="0.9235455">0-okayama, 2-12-1, Megro-ku Tokyo, Japan</address>
<note confidence="0.355005">COLING&apos;86, pp. 222-225</note>
<title confidence="0.9934855">Conceptual Lexicon Using an Object- Oriented Language</title>
<author confidence="0.999568">Shoichi Yokoyama</author>
<affiliation confidence="0.999993">Electrotechnical Laboratory</affiliation>
<address confidence="0.958887">Tsukuba, Ibaraki, Japan</address>
<author confidence="0.923147">Kenji Hanakata</author>
<affiliation confidence="0.999848">Universitat Stuttgart</affiliation>
<address confidence="0.814271">Stuttgart, F.R. Germany</address>
<phone confidence="0.25923">COLING&apos;86, pp. 226-228</phone>
<title confidence="0.982037">Elementary Contracts as a Pragmatic Basis of Language Interaction</title>
<author confidence="0.999773">E L Petshina</author>
<affiliation confidence="0.849463">Computer Center Division of the Ac. USSR</affiliation>
<address confidence="0.196955">COLING&apos;86, pp. 229-231</address>
<title confidence="0.9932535">Communicative Triad as a Structural Element of Language Interaction</title>
<author confidence="0.999355">F G Dinenberg</author>
<affiliation confidence="0.9889165">Computer Center Division of the Ac. Sci.</affiliation>
<address confidence="0.762594">Novosibirsk 630090, USSR</address>
<abstract confidence="0.959280928571429">COLING&apos;86, pp. 232-234 Specific Text Management and Lexicon Development It an ontology reflecting the ordinary person&apos;s top-level cognitive model of real-world distinctions and a data base of prototype descriptions of real-world entities. KT is transportable, empirically-based and constrained for efficient reasoning in ways similar to human reasoning processes. Semantic processing is one of the important tasks for natural language processing. Basic to semantic processing is descriptions of lexical items. The most frequently used form of description of lexical items is probably Frames or Objects. Therefore in what form Frames or Objects are expressed is a key issue for natural language processing. A method of the Object representation in Prolog called DCKR will be introduced. It will be seen that if part of general knowledge and a dictionary are described in of context-processing, and the greater part of semantic processing can be left to the functions built in Prolog. This paper describe the construction of a lexicon representing abstract concepts. This lexicon is written by an object-oriented language, CTALK, and forms a dynamic network system controlled by object-oriented mechanisms. The content of the lexicon is constructed using a Japanese dictionary. First, entry words and their definition parts are derived from the dictionary. Second, syntactic and semantic information is analyzed from these parts. Finally, superconcepts are assigned in the superconcept part in an object, static parts in the slot values, and dynamic operations to the message parts, respectively. One word has one object in a world, but through the superconcept part and slot part this connects to the subconcept of other words and worlds. When relative concepts are accumulated, the result will be a model of human thoughts which have conscious and unconscious parts. Language interaction (LI) as a part of interpersonal communication is considerably influenced by psychological and social roles of the partners and their pragmatic goals. These aspects of communication should be accounted for while elaborating advanced user-computer dialogue systems and developing formal models of LI. We propose here a formal description of communicative context of LI-situation, namely, a system of indices of LI agents&apos; interest in achieving various pragmatic purposes and a system of contracts which reflect social and psychological roles of the LI agents and conventionalize their &amp;quot;rights&amp;quot; and &amp;quot;duties&amp;quot; in the LI-process. Different values of these parameters of communication allow us to state possibility and/or necessity of certain types of speech acts under certain conditions of LI-situation. Researches on dialogue natural-language interaction with intellectual &amp;quot;human-computer&amp;quot; systems are based on models of language &amp;quot;human-tohuman&amp;quot; interaction, these models representing descriptions of communication laws. An aspect of developing language interaction models is an investigation of dialogue structure. In the paper a notion of elementary communicative triad (SR-triad) is introduced to model the &amp;quot;stimulusrelation between utterances in the dialogue. The use of the SRtriad apparatus allows us to represent a scheme of any dialogue as a triad structure. SR-triad structure being inherent both to natural and programming language dialogues, SR-system is claimed to be necessary while developing dialogue processors. The definition of a Text Base Management System is introduced in terms of software engineering. That gives a basis for discussing practical text Linguistics, Volume 13, Numbers 1-2, January-June 1987</abstract>
<title confidence="0.948292">FINITE STRING Abstracts of Current Literature</title>
<author confidence="0.999939">S Goeser</author>
<author confidence="0.999939">E Mergenthaler</author>
<affiliation confidence="0.791579">Universtity of Ulm Federal Republic of Germany</affiliation>
<title confidence="0.862253">Text Analysis and Knowledge Extraction</title>
<author confidence="0.8565395">Fujio Nishida</author>
<author confidence="0.8565395">Shinobu Takamatsu</author>
<author confidence="0.8565395">Tadaaki Tani</author>
<author confidence="0.8565395">Hiroji Kusaka</author>
<affiliation confidence="0.995048333333333">Department of Electrical Engineering Faculty of Engineering University of Osaka Prefecture</affiliation>
<address confidence="0.965309">Sakai, Osaka, 591 Japan</address>
<title confidence="0.9143765">Context Analysis System for Japanese Text</title>
<author confidence="0.915097">Hitoshi Isahara</author>
<author confidence="0.915097">Shun Ishizaki</author>
<affiliation confidence="0.999925">Electrotechnical Laboratory</affiliation>
<address confidence="0.894738">1-1-4, Umezono, Sakura-mura, Niihari-gun Ibaraki, Japan 305</address>
<email confidence="0.322879">'86</email>
<title confidence="0.995282">Disambiguation and Language Acquisition through the Phrasal Lexicon</title>
<author confidence="0.99911">Uri Zernik</author>
<author confidence="0.99911">Michael G Dyer</author>
<affiliation confidence="0.999978">Artificial Intelligence Laboratory Computer Science Department</affiliation>
<address confidence="0.996007">3531 Boelter Hall</address>
<affiliation confidence="0.999918">University of California</affiliation>
<address confidence="0.999634">Los Angeles, CA 90024</address>
<title confidence="0.9426445">Linguistic Knowledge Extraction from Real Language Behavior</title>
<author confidence="0.999697">K Shirai</author>
<author confidence="0.999697">T Hamada</author>
<affiliation confidence="0.9998275">Department of Electrical Engineering Waseda University</affiliation>
<address confidence="0.991568">3-4-1 Ohkubo Shinjuku-ku, Tokyo, Japan</address>
<abstract confidence="0.994534672413793">administration, including questions on corpus properties and appropriate retrieval criteria. Finally, strategies for the derivation of a word data base from an actual TBMS will be discussed. The study of text understanding and knowledge extraction has been actively done by many researchers. The authors also studied a method of structured information extraction from texts without a global text analysis. The method is available for a comparatively short text such as a patent claim clause and an abstract of a technical paper. This paper describes the outline of a method of knowledge extraction from a longer text which needs a global text analysis. The kinds of texts are expository texts or explanation texts. Expository texts described here mean those which have various hierarchical headings such as a title, a heading of each section and sometimes an abstract. In this definition, most texts, including technical papers, reports, and newspapers, are expository. Text of this kind disclose the main knowledge in a top-down manner and show not only the location of an attribute value in a text but also several key points of the content. This property of expository texts contrasts with that of novels and stories in which an unexpected development of the plot is preferred. This paper pays attention to such characteristics of expository texts and describes a method of analyzing texts by referring to information contained in the intersentential relations and the headings of texts and then extracting requested knowledge such as a summary from texts in an efficient way. A natural language understanding system is described which extracts contextual information from Japanese texts. It integrates syntactic, semantic, and contextual processing serially. The syntactic analyzer obtains rough syntactic structures from the text. The semantic analyzer treats modifying relations inside noun phrases and case relations among verbs and noun phrases. Then, the contextual analyzer obtains contextual information from the semantic structure extracted by the semantic analyzer. Our system understands the context using precoded contextual knowledge on terrorism and plugs the event information in input sentences into the contextual structure. The phrase approach to language processing emphasizes the role of the lexicon as a knowledge source. Rather than maintaining a single generic entry for each word, e.g., lexicon contains many phrases, on, take to the streets, take to swimming, take over, Although this approach proves effective in parsing and in generation, there are two acute problems which still require solutions. First, due to the huge size of the phrase lexicon, especially when considering subtle meanings and idiosyncratic behavior of phrases, encoding of lexical entries cannot be done Thus phrase be employed to construct the lexi- .con. Second, when a set of phrases is morpho-syntactically equivalent, disambiguation must be performed by semantic means. These problems are addressed in the program RNA. An approach to extract linguistic knowledge from real language behavior is described. This method depends on the extraction of word relations, patterns of which are obtained by structuring the dependency relations in sentences called Kakari-Uke relation in Japanese. As the first step of this approach, an experiment of a word classification utilizing those patterns was made on the 4178 sentences of real language data. A system was made to analyze dependency structure of sentences utilizing the knowledge Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987 109 The FINITE STRING Newsletter Abstracts of Current Literature base obtained through this word classification and the effectiveness of the knowledge base was evaluated. To develop this approach further, the relation matrix which captures multiple interaction of words is proposed.</abstract>
<title confidence="0.930602666666667">Tailoring Importance Evaluation to Reader&apos;s Goals: A Contribution to Descriptive Text Summarization</title>
<author confidence="0.971271">Danilo Fum</author>
<author confidence="0.971271">Giovanni Guido</author>
<author confidence="0.971271">Carlo Tau°</author>
<affiliation confidence="0.957310666666667">Istito di Matematica, Informatica e Sistemistica Universita di Udine, Italy</affiliation>
<title confidence="0.923449">Domain Dependent Natural Language Understanding</title>
<author confidence="0.998869">Klaus Heje Munch</author>
<affiliation confidence="0.9995335">Department of Computer Science Technical University of Denmark</affiliation>
<address confidence="0.979624">DK-2800 Lyngby, Denmark</address>
<title confidence="0.651405">Morphological Analysis for a German Text-to-Speech System</title>
<author confidence="0.997598">Amanda Pounder</author>
<author confidence="0.997598">Markus Kommenda</author>
<affiliation confidence="0.821021">Institut ftir Nachrichtentechnik und Hochfrequenztechnik Technische Universitat Wien</affiliation>
<address confidence="0.988382">Gusshausstrasse 25, A-1040 Wien, Austria</address>
<email confidence="0.317371">of</email>
<title confidence="0.927815666666667">Automatic Parsing of French Language a Data Jacques Vergne, Pascale Pages Inaleo Paris A Morphological Recognizer with Syntactic and Phonologic Rules</title>
<author confidence="0.999514">John Bear</author>
<affiliation confidence="0.999509">Artificial Intelligence Center SRI International</affiliation>
<address confidence="0.9983425">333 Ravenswood Avenue Menlo Park, CA 94025</address>
<title confidence="0.9064625">A Dictionary and Morphological Analyser for English</title>
<abstract confidence="0.976705244444444">G.J. Russell, S.G. Pulman This paper deals with a new approach to importance evaluation of descriptive texts developed in the framework of SUSY, an experimental system in the domain of text summarization. The problem of taking into account the reader&apos;s goals in evaluating importance of different parts of a text is first analyzed. A solution to the design of a goal interpreter capable of computing a quantitative measure of the relevance degree of a piece of text according to a given goal is then proposed, and an example of goal interpreter operation is provided. A natural language understanding system for a restricted domain of discourse — thermodynamic exercises at an introductory level — is presented. The system transforms texts into a formal meaning representation language based on cases. The semantical interpretation of sentences and phrases is controlled by case frames formulated around verbs and surface grammatical roles in noun phrases. During the semantical interpretation of a text, semantic constraints may be imposed on elements of the text. Each sentence is analyzed with respect to context, making the system capable of solving anaphoric references such as definite descriptions, pronouns, and elliptic constructions. The system has been implemented and successfully tested on a selection of exercises. A central problem in speech synthesis with unrestricted vocabulary is the automatic derivation of correct pronunciation from the graphemic form of a text. The software module GRAPHON was developed to perform this conversion for German and is currently being extended by a morphological analysis component. This analysis is based on a morph lexicon and a set of rules and structural descriptions for Germany word-forms. It provides each text input item with an individual characterization such that the phonological, syntactic, and prosodic components may operate upon it. This systematic approach thus serves to minimize the number of wrong transcriptions and at the same time lays the foundation for the generation of stress and intonation patterns, yielding more intelligible, natural-sounding, and generally acceptable synthetic speech. We intend to present in this paper a parsing method of French language whose particularities are: a multi-level approach: syntax and morphology working simultaneously, the use of string pattern matching and the absence of dictionary. We want here to evaluate the feasibility of the method rather than to present an operational system. This paper describes a morphological analyzer which, when parsing a word, uses two sets of rules: rules describing the syntax of words, and and rules describing facts about orthography. This paper describes the current state of a three-year project aimed at the development of software for use in handling large quantities of dictionary information within natural language processing systems. The project ... is 110 Computational Linguistics, Volume 13, Numbers 1-2, January-June 1987</abstract>
<title confidence="0.559139">The FINITE STRING Newsletter Abstracts of Current Literature</title>
<affiliation confidence="0.999386">Computer Laboratory University of Cambridge</affiliation>
<author confidence="0.997528">G D Ritchie</author>
<author confidence="0.997528">A W Black</author>
<affiliation confidence="0.9979855">Department of Artificial Intelligence University of Edinburgh</affiliation>
<title confidence="0.992056">A Kana-Kanji Translation System for Non-Segmented Input Sentences based on Syntactic and Semantic Analysis</title>
<author confidence="0.8832445">Masahiro Abe</author>
<author confidence="0.8832445">Yoshimitsu Ooshima</author>
<author confidence="0.8832445">Katsuhiko Yuura</author>
<author confidence="0.8832445">Nobuyuki Takeichi</author>
<affiliation confidence="0.933673">Central Research Laboratory Hitachi, Ltd.</affiliation>
<address confidence="0.99776">Kokubunji, Tokyo, Japan</address>
<email confidence="0.788416">pp.</email>
<title confidence="0.994962">A Compression Technique for Arabic Dictionaries: The Affix Analysis</title>
<author confidence="0.999408">Abdelmajid Ben Hamadou</author>
<affiliation confidence="0.994298">Departement of Computer Science —</affiliation>
<title confidence="0.559466">FSEG Faculty</title>
<author confidence="0.370902">Route de_Faeroport</author>
<affiliation confidence="0.521549">SFAX Tunisia</affiliation>
<title confidence="0.8339495">Machine Learning of Morphological Rules by Generalization and Analogy</title>
<author confidence="0.830223">Klaus Wothke Arbeitsstelle Linguistische Datenverarbeitung</author>
<affiliation confidence="0.937162">Institut fur Deutsche Sprache</affiliation>
<address confidence="0.872549">Mannheim, West Germany</address>
<author confidence="0.378966">Linguistic Developments in Eurotra since</author>
<date confidence="0.923944">1983</date>
<title confidence="0.5798714">Lietwn Jaspaert Katholieke Universiteit Leuven Belgium The &lt;C,A&gt; Framework in Eurotra: A Theoretically Committed Notation for MT</title>
<author confidence="0.980929">D J Arnold</author>
<affiliation confidence="0.999962">University of Essex</affiliation>
<address confidence="0.999855">Colchester, Essex C04 3SQ, UK</address>
<author confidence="0.976498">S Kraunrr</author>
<author confidence="0.976498">L des Tombe</author>
<affiliation confidence="0.99948">University of Utrecht</affiliation>
<address confidence="0.8327725">Trans 14, 3512 JK Utrecht, The Netherlands</address>
<abstract confidence="0.987579813953488">one of three closely related projects funded under the Alvey IKBS Programme (Natural Language Theme); a parser is under development at Edinburgh by Henry Thompson and John Phillips), and a sentence grammar is being devised by Ted Biscoe and Clare Grover at Lancaster and Bran Boguraev and John Carroll at Cambridge. It is intended that the software and rules produced by all three projects will be directly compatible and capable of functioning in an integrated system. This paper presents a disambiguation approach for translating non-segmented-Kana into Kanji. The method consists of two steps. In the first step, an input sentence is analyzed morphologically and ambiguous morphemes are stored in a network form. In the second step, the best path, which is a string of morphemes, is selected by syntactic and semantic analysis based on case grammar. In order to avoid the combinatorial explosion of possible paths, the following heuristic search method is adopted. First, a path that contains the smallest number of weighted-morphemes is chosen as the quasi-best path by a best-first-search technique. Next, the restricted range of morphemes near the quasi-best path is extracted from the morpheme network to construct preferential paths. An experimental system incorporating large dictionaries has been developed and evaluated. A translation accuracy of 90.5 was obtained. This can be improved to about 95% by optimizing the dictionaries. In every application that concerns the automatic processing of natural language, the problem of the dictionary size is posed. In this paper we propose a compression dictionary algorithm based on an affix analysis of the non-diacritical Arabic. It consists in decomposing a word into its first elements, taking into account the different linguistic transformations that can affect the morphological structures. This work has been achieved as part of a study of the automatic detection and correction of spelling-errors in the non-diacritical Arabic texts. This paper describes an experimental procedure for the inductive automated learning of morphological rules from examples. At first an outline of the problem is given. Then a formalism for the representation of morphological rules is defined. This formalism is used by the automated procedure, whose anatomy is subsequently presented. Finally, the performance of the system is evaluated and the most important unsolved problems are discussed. I wish to put the theory and metatheory currently adopted in the Eurotra project into a historical perspective, indicating where and why changes to basic design for a transfer-based (TBMT) have been made. paper describes a model for within the Eurotra based on the idea of translation, describing a</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<date>1986</date>
<pages>214</pages>
<institution>Mustafa Mahmoud Lehigh University Ph.D.</institution>
<contexts>
<context position="121379" citStr="(1986)" startWordPosition="17903" endWordPosition="17903">to meet who&apos; We therefore concentrate first on the theoretical status of so-called whor long-distance movement in Lexical Functional Grammar (LFG) and in the Theory of Government and Binding (GB), arguing that a general mechanism that is compatible with both LFG and GB treatment of long-distance movement can be found. Finally, we present the implementation of such a movement mechanism in a LFG parser. In this paper a version of LFG will be developed which has only one level of representation and is equivalent to the modified version of Kaplan, presented in Bresnan (1982) and Kaplan and Zaenen (1986). The structures of this monostratal version are f-structures, augmented by additional information about the derived symbols and their linear order. For these structures it is possible to define an adequate concept of direct derivability by which the derivation process becomes more efficient, as the f-description solution algorithm is directly simulated during the derivation of these structures, instead of being postponed. Apart from this, it follows from this reducibility that LFG as a theory in its present form does not make use of the c-structure information that goes beyond the mere linear</context>
<context position="123381" citStr="(1986)" startWordPosition="18206" endWordPosition="18206">acteristic features of German word order seems to be a contrast between fixed ordering rules concerning the order of verbal elements and a much more variable ordering of their corresponding nominal arguments. As a consequence, German word order seems to yield a large number of phenomena that may be classified as &amp;quot;unbounded&amp;quot; or &amp;quot;long-distance dependencies&amp;quot;, without necessarily involving wh-constituents or &amp;quot;movement&amp;quot; across sentence boundaries. Whereas in traditional LFG long-distance dependencies are treated by means of constituent control, we will follow a recent proposal by Kaplan and Zaenen (1986) to give up the constraint known as &amp;quot;functional locality&amp;quot; and instead allow regular expressions to appear as functional schemata annotated to c-structure rules. Exploiting the principles of completeness and coherence we will thus be able to cope even with absolutely free word order without the need of generating empty terminal nodes at all. The empirical assumption, underlying the proposed analysis in its most radical form, is the hypothesis that (with very few exceptions) the nominal arguments have to appear on the left of the verb by which they are assigned case. We will restrict the Computa</context>
</contexts>
<marker>1986</marker>
<rawString>Mustafa Mahmoud Lehigh University Ph.D. 1986, 214 pages Computer Science University Microfilms International ADG86-I6179</rawString>
</citation>
<citation valid="true">
<title>The Interactive Effects of Micro- and Macrostructural Processing during Text Comprehension Nicholas Ge/eta The Catholic University of America Ph.D.</title>
<date>1986</date>
<pages>170</pages>
<institution>Education, Psychology University Microfilms International</institution>
<contexts>
<context position="121379" citStr="(1986)" startWordPosition="17903" endWordPosition="17903">to meet who&apos; We therefore concentrate first on the theoretical status of so-called whor long-distance movement in Lexical Functional Grammar (LFG) and in the Theory of Government and Binding (GB), arguing that a general mechanism that is compatible with both LFG and GB treatment of long-distance movement can be found. Finally, we present the implementation of such a movement mechanism in a LFG parser. In this paper a version of LFG will be developed which has only one level of representation and is equivalent to the modified version of Kaplan, presented in Bresnan (1982) and Kaplan and Zaenen (1986). The structures of this monostratal version are f-structures, augmented by additional information about the derived symbols and their linear order. For these structures it is possible to define an adequate concept of direct derivability by which the derivation process becomes more efficient, as the f-description solution algorithm is directly simulated during the derivation of these structures, instead of being postponed. Apart from this, it follows from this reducibility that LFG as a theory in its present form does not make use of the c-structure information that goes beyond the mere linear</context>
<context position="123381" citStr="(1986)" startWordPosition="18206" endWordPosition="18206">acteristic features of German word order seems to be a contrast between fixed ordering rules concerning the order of verbal elements and a much more variable ordering of their corresponding nominal arguments. As a consequence, German word order seems to yield a large number of phenomena that may be classified as &amp;quot;unbounded&amp;quot; or &amp;quot;long-distance dependencies&amp;quot;, without necessarily involving wh-constituents or &amp;quot;movement&amp;quot; across sentence boundaries. Whereas in traditional LFG long-distance dependencies are treated by means of constituent control, we will follow a recent proposal by Kaplan and Zaenen (1986) to give up the constraint known as &amp;quot;functional locality&amp;quot; and instead allow regular expressions to appear as functional schemata annotated to c-structure rules. Exploiting the principles of completeness and coherence we will thus be able to cope even with absolutely free word order without the need of generating empty terminal nodes at all. The empirical assumption, underlying the proposed analysis in its most radical form, is the hypothesis that (with very few exceptions) the nominal arguments have to appear on the left of the verb by which they are assigned case. We will restrict the Computa</context>
</contexts>
<marker>1986</marker>
<rawString>The Interactive Effects of Micro- and Macrostructural Processing during Text Comprehension Nicholas Ge/eta The Catholic University of America Ph.D. 1986, 170 pages Education, Psychology University Microfilms International ADG86-13460</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>