<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<note confidence="0.415324">
SYNTACTIC APPROACHES TO AUTOMATIC BOOK INDEXING
</note>
<author confidence="0.902442">
Gerard Salton
</author>
<affiliation confidence="0.9061455">
Department of Computer Science
Cornell University
</affiliation>
<sectionHeader confidence="0.8268435" genericHeader="abstract">
Ithaca, NY 14853
ABSTRACT
</sectionHeader>
<bodyText confidence="0.997229545454546">
Automatic book indexing systems are
based on the generation of phrase struc-
tures capable of reflecting text content.
Some approaches are given for the
automatic construction of back-of-book
indexes using a syntactic analysis of the
available texts, followed by the identifica-
tion of nominal constructions, the assign-
ment of importance weights to the term
phrases, and the choice of phrases as index-
ing units.
</bodyText>
<sectionHeader confidence="0.997665" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999877523809524">
Book indexing is of wide practical
interest to authors, publishers, and readers
of printed materials. For present purposes,
a standard entry in a book index may be
assumed to be a nominal construction listed
in normal phrase order, or appearing in
some permuted form with the principal
term as phrase head. Cross-references
(&amp;quot;see&amp;quot; or &amp;quot;see also&amp;quot; entries) between index
entries are also normally used in the index.
Excerpts from two typical book indexes
appear in Fig. 1.
Attempts have been made over the
years to mechanize the book indexing task,
based in part on the occurrence characteris-
tics of certain content words in the docu-
ment texts (BorkÂ°, 19701, and in part on
more ambitious syntactic methodologies.
[Dillon, 1983] However, as of now, com-
pletely viable automatic book indexing
methods are not available. Two main
</bodyText>
<note confidence="0.752450666666667">
This study was supported in part by a grant from
OCLC Inc., and in part by the National Science Foun-
dation under grant IRI-87-02735.
</note>
<bodyText confidence="0.9996405">
research advances may, however, lead to
the development of improved automatic
book indexing procedures. These include
the generation of advanced syntactic
analysis procedures, capable of analyzing
unrestricted English texts, as well as the
construction of powerful automatic indexing
systems using sophisticated term weighting
systems to assess the importance of the
indexing units. [Salton 1975a, 1975b] By
joining the available linguistic procedures
with the available know-how in automatic
indexing, satisfactory book indexing sys-
tems may be developed.
</bodyText>
<sectionHeader confidence="0.991938" genericHeader="method">
AUTOMATIC PHRASE CONSTRUCTION
</sectionHeader>
<bodyText confidence="0.999326684210526">
Book indexing systems differ from
standard automatic text indexing systems
because complex, multi-word phrases are
normally used for indexing purposes rather
than the single term entries that are pre-
ferred in conventional automatic indexing
systems. The phrase generation system
described in this note is based on an
automatic syntactic analysis of the avail-
able texts followed by a noun-phrase iden-
tification process using parse trees as input
and producing lists of nominal construc-
tions. The parsing system used in this
study is based on an augmented phrase
structure grammar, and was originally
designed for use in the EPISTLE text-
critiquing system.&apos; (Heidorn, 1982, Jensen,
1983)
A typical document abstract is shown
</bodyText>
<page confidence="0.807116">
1
</page>
<note confidence="0.431409">
The writer is indebted to the IBM Corporation and to
</note>
<footnote confidence="0.6897515">
Dr. George Heidorn for making available the PLNLP
parsing system for use at Cornell University.
</footnote>
<page confidence="0.998502">
204
</page>
<bodyText confidence="0.983635878787879">
in Fig. 2, and the output produced by the
syntactic analysis program for sentence 2 of
the document is shown in Fig. 3. It may be
noted that the syntactic output appears in
the form of a standard phrase marker, the
various levels of the syntax tree being listed
in a column format from left to right. Dur-
ing the analysis, a head is identified for
each syntactic constituent, identified by an
asterisk (*) in the output. Thus in Fig. 3,
the VERB is the main head of the sentence;
the head of the noun phrase preceding the
main verb is the NOUN representing the
term &amp;quot;operations&amp;quot;, etc.
The phrase formation system used in
this study builds two-term phrases by com-
bining the head of a constituent with the
head of each constituent that modifies it.
(Fagan 1987a, 1987b) For the sample sen-
tence of Fig. 3, such a strategy produces the
phrases
development exception
dictionary development
negative dictionary
system operations
In the phrase output, the dependent term is
listed first in each case, followed by the
governing term. Note that the phrase gen-
eration system identifies apparently reason-
able constructions such as &amp;quot;dictionary
development&amp;quot; and &amp;quot;system operations&amp;quot;, but
not the unwanted phrases &amp;quot;exception opera-
tions&amp;quot; or &amp;quot;exception systems&amp;quot;.
</bodyText>
<sectionHeader confidence="0.989444" genericHeader="method">
AUTOMATIC PHRASE ASSIGNMENT
</sectionHeader>
<bodyText confidence="0.999896827586207">
An automatic phrase construction sys-
tem generates a large number of phrases for
a given text item. Fig. 4 lists all the
phrases produced for the abstract of Fig. 2.
Phrases occurring in the document title are
identified by the letter T, and phrases
obtained more than once for a given docu-
ment are identified by a frequency marker
(2) in Fig. 4. The output of Fig. 4 could be
used directly in a semi-automatic indexing
environment by letting the user choose
appropriate index entries from the available
list. The standard entries from the figure
might then be manually chosen for indexing
purposes by the document author, or by a
trained indexer.
In a fully automatic indexing system,
additional criteria must be used, leading to
the choice of some of the proposed phrase
constructions, and the rejection of some oth-
ers. The following criteria, among others,
may be useful:
For sentences that produce more than
one acceptable syntactic analysis out-
put, all analyses except the first one
may be eliminated; (in the Heidorn-
Jensen analyzer multiple analyses are
arranged in decreasing order of
presumed correctness).
</bodyText>
<subsectionHeader confidence="0.629543">
Phrases consisting of identical juxta-
</subsectionHeader>
<bodyText confidence="0.949565393939394">
posed words (&amp;quot;computations-
computation&amp;quot; in Fig. 4) may be elim-
inated.
Phrases consisting of more than two
words (e.g. &amp;quot;document-retrieval-
system&amp;quot;) may be given preference in
the phrase assignment process.
Phrases occurring in document titles,
and/or section headings may be given
preference.
Noun-noun constructions might be
given preference over adjective-noun
construction.
A further choice of phrases, as well as
a phrase ordering system in decreasing
order of apparent desirability, can be imple-
mented by assigning a phrase weight to
each phrase and listing the phrases in
decreasing weight order. Two different fre-
quency criteria are important in phrase
weighting:
The frequency of occurrence of a con-
struct in a given document, or docu-
ment section, known as the term fre-
quency (tf)
The number of documents, or docu-
ment sections, in which a given con-
struct occurs, known as the document
frequency (df).2
2 For book indexing purposes, a book can be broken
down into sections, or paragraphs; the term frequency
and document frequency factors are then computed for
the individual book components
</bodyText>
<page confidence="0.995307">
205
</page>
<bodyText confidence="0.999220910447761">
The best constructs for indexing purposes
are those exhibiting a high term frequency,
and a relatively low overall document fre-
quency. Such constructs will distinguish
the documents, or document sections, to
which they are assigned from the remainder
of the collection. The corresponding term
weighting system, known as tfidf is com-
puted by multiplying the term frequency
factor by an inverse document frequency
factor.
Fig. 5 shows selected phrase output
based in part on the use of automatically
derived term weights. The top part of the
figure contains the automatically derived
constructs containing more than two terms.
These might be used for indexing purposes
regardless of term weight. In addition, the
two-term phrases whose term frequency
exceeds 1 in the document might also be
used for indexing purposes. This would add
the 9 phrases listed in the center portion of
Fig. 5.
Some of the phrases with tf &gt; 1 have
either a very high document frequency (125
for &amp;quot;retrieval system&amp;quot;) or a very low docu-
ment frequency of 1, meaning that the
phrase occurs only in the single document
659. In practice, a reasonable indexing pol-
icy consists in choosing phrases for which tf
&gt; k1 and k2 &lt; df &lt; k3 for suitable
parameters k 1,k 2, and k3. When these
parameters are set equal to 1, 1 and 100,
respectively, the 5 phrases identified by
asterisks in Fig. 5 are chosen as indexing
units.
The bottom part of Fig. 5 shows a
ranked phrase list in decreasing order
according to a composite (tf X idf) phrase
weight. Using such an ordered list, a typi-
cal indexing policy consists in choosing the
top n entries from the list, or choosing
entries whose weight exceeds a given thres-
hold T. When T is chosen as 0.1, the 12
phrases listed at the bottom of Fig. 5 are
produced. It may be noted that most of the
terms listed in Fig. 5 appear to be reason-
able indexing units.
In a practical book indexing system, a
phrase classification system capable of
determining relationships between similar,
or identical, phrases becomes useful. Such
a phrase classification then leads to the
choice of canonical representations for each
group of equivalent phrases, and to the
assignment of &amp;quot;see&amp;quot; and &amp;quot;see also&amp;quot; refer-
ences. Phrase relationships can be deter-
mined by using synonym dictionaries and
various kinds of phrase lists. In addition,
attempts have also been made to use the
term definitions contained in machine-
readable dictionaries to construct hierar-
chies of word meanings. (Walker, 1987;
Kucera, 1985; Chodorow, 1985) The
automatic construction of phrase classifica-
tion systems remains to be pursued in
future work.
</bodyText>
<sectionHeader confidence="0.996521" genericHeader="method">
REFERENCES
</sectionHeader>
<bodyText confidence="0.973719916666667">
Borko, H., 1970, Experiments in Book
Indexing by Computer, Information Storage
and Retrieval, 6:1, 5-16.
Chodorow, M.W., Byrd, R.J., and Heidorn,
G.E., 1985, Extracting Semantic Hierar-
chies from a Large On-Line Dictionary,
Proceedings of 23rd Annual Meeting of the
Associations for Computational Linguistics,
Chicago, IL.
Dillon, M. and McDonald, L.K. 1983, Fully
Automatic Book Indexing, Journal of Docu-
mentation, 39:3, 135-154.
</bodyText>
<sectionHeader confidence="0.656438333333333" genericHeader="method">
Fagan, J.L., 1987a, Experiments in
Automatic Phrase Indexing for Document
Retrieval: A Comparison of Syntactic and
</sectionHeader>
<reference confidence="0.909576789473684">
Non-Syntactic Methods, Doctoral Disserta-
tion, Cornell University, Technical Report
87-868, Department of Computer Science,
Cornell University, Ithaca, NY.
Fagan, J.L., 1987b, Automatic Phrase
Indexing for Document Retrieval: An
Examination of Syntactic and Non-
Syntactic Methods, Tenth Annual
ACMISIGIR Conference on Research and
Development in Information Retrieval, New
Orleans, LA, ACM, NY, 1987.
Heidorn, G.E., Jensen, K., Miller, L.A.,
Byrd, R.J., and Chodorow, M.S., 1982, The
EPISTLE Text Critiquing System, IBM Sys-
tems Journal, 21:3â 305-326.
Jensen, K., Heidorn, G.E., Miller, L.A., and
Ravin, Y., 1983, Parse Fitting and Prose
Fixing: Getting Hold on Ill-Formedness,
American Journal of Computational
</reference>
<page confidence="0.996773">
206
</page>
<reference confidence="0.985799">
Linguistics, 9:3-4, 147-160.
Kucera, H., 1985, Uses of On-Line Lexicons,
Proceedings First Conference of the U.W.
Centre for the New Oxford English Diction-
ary: Information in Data, University of
Waterloo, 7-10.
Salton, G., 1975a, A Theory of Indexing,
Regional Conference Series in Applied
Mathematics, No. 18, Society of Industrial
and Applied Mathematics, Philadelphia,
PA.
Salton, G., Yang, C.S., and Yu, C., 1975b, A
Theory of Term Importance in Automatic
Text Analysis, Journal of the ASIS, 26:1,
33-44.
Wa9:er, D.E., 1987, Knowledge Resource
Tools for Analyzing Large Text Files, in
Machine Translation: Theoretical and
Methodological Issues, Sergei Nirenburg,
editor, Cambridge University Press, Cam-
bridge, England, 247-261.
</reference>
<page confidence="0.996891">
207
</page>
<table confidence="0.98423875">
Game tree, 259-270 Data security, 360, 390-394
Garbage collection, 169-178 DBTG (Data Base Task Group), 377-380
Go to statement, 11 Deadlock prevention, 395-396
Graphs, 282-334 Decision support system, 7, 9, 358-359
activity networks, 310-324 Decomposition of relations, 394
adjacency matrix, 287-288 Deductive system, 259, 356, 420
adjacency lists, 288-290 Deep indexing, 55
adjacency multi lists, 290-292 Deep structure of language, 275
bipartite, 329 Default exit, 343
bridge, 334 Delay cost (see Cost analysis)
definitions, 283-287 Density(see Document space density)
Eulerian walk, 282 Dependency (see Functional dependency; Term dependency model)
incidence matrix, 331 Depth-first search, 223
inverse adjacency lists, 290 Descriptive cataloging, 53
orthogonal lista, 291 Deterioration, 225-226, 233
representations, 287-292 DIALOG system, 30-34, 38, 46-48
shortest paths, 301-308 Dice coefficient, 203
spanning trees, 292-301 Dictionary, 56-57, 101-103, 259-263, 285-286
transitive closure, 296, 308-309 Dictionary format, 57
in STAIRS, 36
</table>
<figureCaption confidence="0.84131">
Figure 1. Typical Book Index Entries
</figureCaption>
<sectionHeader confidence="0.60518475" genericHeader="method">
Document 659
.T
A Highly Associative Document Retrieval System
.W
</sectionHeader>
<bodyText confidence="0.997316875">
This paper describes a document retrieval system implemented with a subset of the medi-
cal literature. With the exception of the development of a negative dictionary, all system
operations are completely automatic. Introduced are methods for computation of term-term
association factors, indexing, assignment of term-document relevance values, and computa-
tions for recall and relevance. High weights are provided for low-frequency terms, and
retrieval is performed directly from highly connected term-document files without elaboration.
Recall and relevance are based on quantitative internal system computations, and results are
compared with user evaluations.
</bodyText>
<figureCaption confidence="0.989865">
Figure 2. Typical Document Abstract
</figureCaption>
<page confidence="0.960601">
208
</page>
<table confidence="0.999031928571429">
DEC L PP PREP &amp;quot;with&amp;quot; &amp;quot;the&amp;quot; &amp;quot;the&amp;quot; ,,a,,
DET ADJ* &amp;quot;of&amp;quot; &amp;quot;of&amp;quot; &amp;quot;negative&amp;quot;
NOUN* &amp;quot;exception&amp;quot; ADJ* ADJ*
PP PREP &amp;quot;development&amp;quot; ADJ*
DET PREP &amp;quot;dictionary&amp;quot;
NOUN* DET
PP AJP
NP QUANT ADJ* NOUN*
NP NOUN* PUNC
NOUN* &amp;quot;operations&amp;quot; &amp;quot;all&amp;quot;
VERB* &amp;quot;are&amp;quot; &amp;quot;system&amp;quot;
AJP AVP ADV* &amp;quot;completely&amp;quot;
ADJ* &amp;quot;automatic&amp;quot;
PUNC 11 II
</table>
<figureCaption confidence="0.9997095">
Figure 3. Typical Output of Syntactic Analysis Program for One Sentence
Figure 4. Phrases generated for Document 659
</figureCaption>
<figure confidence="0.987480136363636">
(T = title; 2= occurrence frequency of 2; *= manually selected)
assignment computation
association assignment
association computations
association factors
association indexing
associative retrieval (T)*
associative system (T)
computations computation
computation methods
connected file
development exception
dictionary development
document retrieval (T,2)*
document retrieval system (2)
document system (T,2)
elaboration files
factors computation
indexing computation
internal computation
literature subset
low-frequency terms
medical literature
negative dictionary
quantitative computations
recall computations*
relevance values*
retrieval system (T)
subset implemented
system computations
system implemented
system operations
term-document files
term-document relevance
term-document relevance values
term-document values *
term-term-assingment
term-term association *
term-term association factors
term-term computation
term-term factors
term-term indexing
user evaluation *
values assignment
</figure>
<page confidence="0.883656">
209
</page>
<table confidence="0.718065869565217">
1. Three-Term Phrases document retrieval system
term-term assocaition factor
term-term relevance values
2. Two-Term Phrases (with Term Frequency greater than 1)
Phrase Frequency in Number of Documents for
Document (tf) Phrase (out of 1460) (df)
retrieval system 2 125
*document system 2 25
term-term computation 2 1
term-document 2 1
term-term factors 2 1
*term-term indexing 2 5
*document retrieval 2 28
*term -term association 2 2
*term-term assignment 2 2
3. Two-Term Phrases in Normalized (tf a idf) Weight Order (df &gt; 1)
Phrase Weight Phrase Weight
term-term assignment .2128 association factors .1064
term-term association .2128 associative system .1064
term-term indexing .1832 low frequency terms .1064
document system .1313 associative retrieval .1064
document retrieval .1276 literature subset .1064
indexing computation .1064 term-document files .1064
</table>
<figureCaption confidence="0.990097">
Figure 5. Automatic Phrase Indexing for Document 659
</figureCaption>
<page confidence="0.996664">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957497">
<title confidence="0.999481">SYNTACTIC APPROACHES TO AUTOMATIC BOOK INDEXING</title>
<author confidence="0.999968">Gerard Salton</author>
<affiliation confidence="0.999897">Department of Computer Science Cornell University</affiliation>
<address confidence="0.999945">Ithaca, NY 14853</address>
<abstract confidence="0.996490166666667">Automatic book indexing systems are based on the generation of phrase structures capable of reflecting text content. Some approaches are given for the automatic construction of back-of-book indexes using a syntactic analysis of the available texts, followed by the identification of nominal constructions, the assignment of importance weights to the term phrases, and the choice of phrases as indexing units.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<tech>Technical Report 87-868,</tech>
<institution>Non-Syntactic Methods, Doctoral Dissertation, Cornell University,</institution>
<location>Ithaca, NY.</location>
<marker></marker>
<rawString>Non-Syntactic Methods, Doctoral Dissertation, Cornell University, Technical Report 87-868, Department of Computer Science, Cornell University, Ithaca, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fagan</author>
</authors>
<title>1987b, Automatic Phrase Indexing for Document Retrieval: An Examination of Syntactic and NonSyntactic Methods,</title>
<date>1987</date>
<booktitle>Tenth Annual ACMISIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<location>New Orleans, LA, ACM, NY,</location>
<contexts>
<context position="3737" citStr="Fagan 1987" startWordPosition="595" endWordPosition="596">ntactic output appears in the form of a standard phrase marker, the various levels of the syntax tree being listed in a column format from left to right. During the analysis, a head is identified for each syntactic constituent, identified by an asterisk (*) in the output. Thus in Fig. 3, the VERB is the main head of the sentence; the head of the noun phrase preceding the main verb is the NOUN representing the term &amp;quot;operations&amp;quot;, etc. The phrase formation system used in this study builds two-term phrases by combining the head of a constituent with the head of each constituent that modifies it. (Fagan 1987a, 1987b) For the sample sentence of Fig. 3, such a strategy produces the phrases development exception dictionary development negative dictionary system operations In the phrase output, the dependent term is listed first in each case, followed by the governing term. Note that the phrase generation system identifies apparently reasonable constructions such as &amp;quot;dictionary development&amp;quot; and &amp;quot;system operations&amp;quot;, but not the unwanted phrases &amp;quot;exception operations&amp;quot; or &amp;quot;exception systems&amp;quot;. AUTOMATIC PHRASE ASSIGNMENT An automatic phrase construction system generates a large number of phrases for a gi</context>
</contexts>
<marker>Fagan, 1987</marker>
<rawString>Fagan, J.L., 1987b, Automatic Phrase Indexing for Document Retrieval: An Examination of Syntactic and NonSyntactic Methods, Tenth Annual ACMISIGIR Conference on Research and Development in Information Retrieval, New Orleans, LA, ACM, NY, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
<author>K Jensen</author>
<author>L A Miller</author>
<author>R J Byrd</author>
<author>M S Chodorow</author>
</authors>
<date>1982</date>
<journal>The EPISTLE Text Critiquing System, IBM Systems Journal,</journal>
<volume>21</volume>
<pages>305--326</pages>
<marker>Heidorn, Jensen, Miller, Byrd, Chodorow, 1982</marker>
<rawString>Heidorn, G.E., Jensen, K., Miller, L.A., Byrd, R.J., and Chodorow, M.S., 1982, The EPISTLE Text Critiquing System, IBM Systems Journal, 21:3â 305-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jensen</author>
<author>G E Heidorn</author>
<author>L A Miller</author>
<author>Y Ravin</author>
</authors>
<title>Parse Fitting and Prose Fixing: Getting Hold on Ill-Formedness,</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>9--3</pages>
<marker>Jensen, Heidorn, Miller, Ravin, 1983</marker>
<rawString>Jensen, K., Heidorn, G.E., Miller, L.A., and Ravin, Y., 1983, Parse Fitting and Prose Fixing: Getting Hold on Ill-Formedness, American Journal of Computational Linguistics, 9:3-4, 147-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kucera</author>
</authors>
<title>Uses of On-Line Lexicons,</title>
<date>1985</date>
<booktitle>Proceedings First Conference of the U.W. Centre for the New Oxford English Dictionary: Information in Data,</booktitle>
<pages>7--10</pages>
<institution>University of Waterloo,</institution>
<contexts>
<context position="8989" citStr="Kucera, 1985" startWordPosition="1456" endWordPosition="1457">l book indexing system, a phrase classification system capable of determining relationships between similar, or identical, phrases becomes useful. Such a phrase classification then leads to the choice of canonical representations for each group of equivalent phrases, and to the assignment of &amp;quot;see&amp;quot; and &amp;quot;see also&amp;quot; references. Phrase relationships can be determined by using synonym dictionaries and various kinds of phrase lists. In addition, attempts have also been made to use the term definitions contained in machinereadable dictionaries to construct hierarchies of word meanings. (Walker, 1987; Kucera, 1985; Chodorow, 1985) The automatic construction of phrase classification systems remains to be pursued in future work. REFERENCES Borko, H., 1970, Experiments in Book Indexing by Computer, Information Storage and Retrieval, 6:1, 5-16. Chodorow, M.W., Byrd, R.J., and Heidorn, G.E., 1985, Extracting Semantic Hierarchies from a Large On-Line Dictionary, Proceedings of 23rd Annual Meeting of the Associations for Computational Linguistics, Chicago, IL. Dillon, M. and McDonald, L.K. 1983, Fully Automatic Book Indexing, Journal of Documentation, 39:3, 135-154. Fagan, J.L., 1987a, Experiments in Automati</context>
</contexts>
<marker>Kucera, 1985</marker>
<rawString>Kucera, H., 1985, Uses of On-Line Lexicons, Proceedings First Conference of the U.W. Centre for the New Oxford English Dictionary: Information in Data, University of Waterloo, 7-10.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Salton</author>
</authors>
<title>1975a, A Theory of</title>
<booktitle>Indexing, Regional Conference Series in Applied Mathematics, No. 18, Society of Industrial and Applied Mathematics,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Salton, </marker>
<rawString>Salton, G., 1975a, A Theory of Indexing, Regional Conference Series in Applied Mathematics, No. 18, Society of Industrial and Applied Mathematics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
<author>C Yu</author>
</authors>
<title>A Theory of Term Importance in Automatic Text Analysis,</title>
<date>1975</date>
<journal>Journal of the ASIS,</journal>
<volume>26</volume>
<pages>33--44</pages>
<marker>Salton, Yang, Yu, 1975</marker>
<rawString>Salton, G., Yang, C.S., and Yu, C., 1975b, A Theory of Term Importance in Automatic Text Analysis, Journal of the ASIS, 26:1, 33-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wa9 er</author>
<author>D E</author>
</authors>
<title>Knowledge Resource Tools for Analyzing Large Text Files,</title>
<date>1987</date>
<booktitle>in Machine Translation: Theoretical and Methodological Issues,</booktitle>
<pages>247--261</pages>
<editor>Sergei Nirenburg, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<marker>er, E, 1987</marker>
<rawString>Wa9:er, D.E., 1987, Knowledge Resource Tools for Analyzing Large Text Files, in Machine Translation: Theoretical and Methodological Issues, Sergei Nirenburg, editor, Cambridge University Press, Cambridge, England, 247-261.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>