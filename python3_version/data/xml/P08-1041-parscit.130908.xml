<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002895">
<title confidence="0.995564">
Summarizing Emails with Conversational Cohesion and Subjectivity
</title>
<author confidence="0.998165">
Giuseppe Carenini, Raymond T. Ng and Xiaodong Zhou
</author>
<affiliation confidence="0.895168666666667">
Department of Computer Science
University of British Columbia
Vancouver, BC, Canada
</affiliation>
<email confidence="0.995566">
{carenini, rng, xdzhou}@cs.ubc.ca
</email>
<sectionHeader confidence="0.995564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994064235294118">
In this paper, we study the problem of sum-
marizing email conversations. We first build
a sentence quotation graph that captures the
conversation structure among emails. We
adopt three cohesion measures: clue words,
semantic similarity and cosine similarity as
the weight of the edges. Second, we use
two graph-based summarization approaches,
Generalized ClueWordSummarizer and Page-
Rank, to extract sentences as summaries.
Third, we propose a summarization approach
based on subjective opinions and integrate it
with the graph-based ones. The empirical
evaluation shows that the basic clue words
have the highest accuracy among the three co-
hesion measures. Moreover, subjective words
can significantly improve accuracy.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977895833334">
With the ever increasing popularity of emails, it is
very common nowadays that people discuss spe-
cific issues, events or tasks among a group of peo-
ple by emails(Fisher and Moody, 2002). Those dis-
cussions can be viewed as conversations via emails
and are valuable for the user as a personal infor-
mation repository(Ducheneaut and Bellotti, 2001).
In this paper, we study the problem of summariz-
ing email conversations. Solutions to this problem
can help users access the information embedded in
emails more effectively. For instance, 10 minutes
before a meeting, a user may want to quickly go
through a previous discussion via emails that is go-
ing to be discussed soon. In that case, rather than
reading each individual email one by one, it would
be preferable to read a concise summary of the pre-
vious discussion with the major information summa-
rized. Email summarization is also helpful for mo-
bile email users on a small screen.
Summarizing email conversations is challenging
due to the characteristics of emails, especially the
conversational nature. Most of the existing meth-
ods dealing with email conversations use the email
thread to represent the email conversation struc-
ture, which is not accurate in many cases (Yeh and
Harnly, 2006). Meanwhile, most existing email
summarization approaches use quantitative features
to describe the conversation structure, e.g., number
of recipients and responses, and apply some general
multi-document summarization methods to extract
some sentences as the summary (Rambow et al.,
2004) (Wan and McKeown, 2004). Although such
methods consider the conversation structure some-
how, they simplify the conversation structure into
several features and do not fully utilize it into the
summarization process.
In contrast, in this paper, we propose new summa-
rization approaches by sentence extraction, which
rely on a fine-grain representation of the conversa-
tion structure. We first build a sentence quotation
graph by content analysis. This graph not only cap-
tures the conversation structure more accurately, es-
pecially for selective quotations, but it also repre-
sents the conversation structure at the finer granular-
ity of sentences. As a second contribution of this pa-
per, we study several ways to measure the cohesion
between parent and child sentences in the quotation
graph: clue words (re-occurring words in the reply)
</bodyText>
<page confidence="0.995693">
353
</page>
<note confidence="0.689479">
Proceedings ofACL-08: HLT, pages 353–361,
</note>
<page confidence="0.457802">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.994894185185185">
(Carenini et al., 2007), semantic similarity and co-
sine similarity. Hence, we can directly evaluate the
importance of each sentence in terms of its cohesion
with related ones in the graph. The extractive sum-
marization problem can be viewed as a node ranking
problem. We apply two summarization algorithms,
Generalized ClueWordSummarizer and Page-Rank
to rank nodes in the sentence quotation graph and
to select the corresponding most highly ranked sen-
tences as the summary.
Subjective opinions are often critical in many con-
versations. As a third contribution of this paper, we
study how to make use of the subjective opinions
expressed in emails to support the summarization
task. We integrate our best cohesion measure to-
gether with the subjective opinions. Our empirical
evaluations show that subjective words and phrases
can significantly improve email summarization.
To summarize, this paper is organized as follows.
In Section 2, we discuss related work. After building
a sentence quotation graph to represent the conver-
sation structure in Section 3, we apply two summa-
rization methods in Section 4. In Section 5, we study
summarization approaches with subjective opinions.
Section 6 presents the empirical evaluation of our
methods. We conclude this paper and propose fu-
ture work in Section 7.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999985136363637">
Rambow et al. proposed a sentence extraction sum-
marization approach for email threads (Rambow et
al., 2004). They described each sentence in an email
conversations by a set of features and used machine
learning to classify whether or not a sentence should
be included into the summary. Their experiments
showed that features about emails and the email
thread could significantly improve the accuracy of
summarization.
Wan et al. proposed a summarization approach
for decision-making email discussions (Wan and
McKeown, 2004). They extracted the issue and re-
sponse sentences from an email thread as a sum-
mary. Similar to the issue-response relationship,
Shrestha et al.(Shrestha and McKeown, 2004) pro-
posed methods to identify the question-answer pairs
from an email thread. Once again, their results
showed that including features about the email
thread could greatly improve the accuracy. Simi-
lar results were obtained by Corston-Oliver et al.
They studied how to identify “action” sentences
in email messages and use those sentences as a
summary(Corston-Oliver et al., 2004). All these ap-
proaches used the email thread as a coarse represen-
tation of the underlying conversation structure.
In our recent study (Carenini et al., 2007), we
built a fragment quotation graph to represent an
email conversation and developed a ClueWordSum-
marizer (CWS) based on the concept of clue words.
Our experiments showed that CWS had a higher
accuracy than the email summarization approach
in (Rambow et al., 2004) and the generic multi-
document summarization approach MEAD (Radev
et al., 2004). Though effective, the CWS method
still suffers from the following four substantial limi-
tations. First, we used a fragment quotation graph to
represent the conversation, which has a coarser gran-
ularity than the sentence level. For email summa-
rization by sentence extraction, the fragment granu-
larity may be inadequate. Second, we only adopted
one cohesion measure (clue words that are based on
stemming), and did not consider more sophisticated
ones such as semantically similar words. Third, we
did not consider subjective opinions. Finally, we did
not compared CWS to other possible graph-based
approaches as we propose in this paper.
Other than for email summarization, other docu-
ment summarization methods have adopted graph-
ranking algorithms for summarization, e.g., (Wan et
al., 2007), (Mihalcea and Tarau, 2004) and (Erkan
and Radev, 2004). Those methods built a complete
graph for all sentences in one or multiple documents
and measure the similarity between every pair of
sentences. Graph-ranking algorithms, e.g., Page-
Rank (Brin and Page, 1998), are then applied to rank
those sentences. Our method is different from them.
First, instead of using the complete graph, we build
the graph based on the conversation structure. Sec-
ond, we try various ways to compute the similarity
among sentences and the ranking of the sentences.
Several studies in the NLP literature have ex-
plored the reoccurrence of similar words within one
document due to text cohesion. The idea has been
formalized in the construct of lexical chains (Barzi-
lay and Elhadad, 1997). While our approach and
lexical chains both rely on lexical cohesion, they are
</bodyText>
<page confidence="0.998491">
354
</page>
<bodyText confidence="0.999935857142857">
quite different with respect to the kind of linkages
considered. Lexical chain is only based on similar-
ities between lexical items in contiguous sentences.
In contrast, in our approach, the linkage is based on
the existing conversation structure. In our approach,
the “chain” is not only “lexical” but also “conversa-
tional”, and typically spans over several emails.
</bodyText>
<sectionHeader confidence="0.9455805" genericHeader="method">
3 Extracting Conversations from Multiple
Emails
</sectionHeader>
<bodyText confidence="0.9997624">
In this section, we first review how to build a frag-
ment quotation graph through an example. Then we
extend this structure into a sentence quotation graph,
which can allow us to capture the conversational re-
lationship at the level of sentences.
</bodyText>
<subsectionHeader confidence="0.988969">
3.1 Building the Fragment Quotation Graph
</subsectionHeader>
<figure confidence="0.996774">
E1 E2 E3 E4 E5 E6
a b c d g &gt; g
&gt; a &gt; b e h i
&gt; &gt; a &gt; c &gt; &gt; d &gt; h
&gt; &gt; b &gt; f j
&gt; &gt; &gt; a &gt; &gt; e
(a) Conversation involving 6 Emails
(b) Fragment Quotation Graph
</figure>
<figureCaption confidence="0.999967">
Figure 1: A Real Example
</figureCaption>
<bodyText confidence="0.999774538461539">
Figure 1(a) shows a real example of a conversa-
tion from a benchmark data set involving 6 emails.
For the ease of representation, we do not show the
original content but abbreviate them as a sequence
of fragments. In the first step, all new and quoted
fragments are identified. For instance, email E3 is
decomposed into 3 fragments: new fragment c and
quoted fragments b, which in turn quoted a. E4
is decomposed into de, c, b and a. Then, in the
second step, to identify distinct fragments (nodes),
fragments are compared with each other and over-
laps are identified. Fragments are split if necessary
(e.g., fragment gh in E5 is split into g and h when
matched with E6), and duplicates are removed. At
the end, 10 distinct fragments a,... , j give rise to
10 nodes in the graph shown in Figure 1(b).
As the third step, we create edges, which repre-
sent the replying relationship among fragments. In
general, it is difficult to determine whether one frag-
ment is actually replying to another fragment. We
assume that any new fragment is a potential reply to
neighboring quotations – quotedfragments immedi-
ately preceding or following it. Let us consider E6
in Figure 1(a). there are two edges from node i to g
and h, while there is only a single edge from j to h.
For E3, there are the edges (c, b) and (c, a). Because
of the edge (b, a), the edge (c, a) is not included in
Figure 1(b). Figure 1(b) shows the fragment quota-
tion graph of the conversation shown in Figure 1(a)
with all the redundant edges removed. In contrast,
if threading is done at the coarse granularity of en-
tire emails, as adopted in many studies, the thread-
ing would be a simple chain from E6 to E5, E5 to
E4 and so on. Fragment f reflects a special and im-
portant phenomenon, where the original email of a
quotation does not exist in the user’s folder. We call
this as the hidden email problem. This problem and
its influence on email summarization were studied
in (Carenini et al., 2005) and (Carenini et al., 2007).
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Building the Sentence Quotation Graph
</subsectionHeader>
<bodyText confidence="0.99998752">
A fragment quotation graph can only represent the
conversation in the fragment granularity. We no-
tice that some sentences in a fragment are more rel-
evant to the conversation than the remaining ones.
The fragment quotation graph is not capable of rep-
resenting this difference. Hence, in the following,
we describe how to build a sentence quotation graph
from the fragment quotation graph and introduce
several ways to give weight to the edges.
In a sentence quotation graph G5, each node rep-
resents a distinct sentence in the email conversation,
and each edge (u, v) represents the replying rela-
tionship between node u and v. The algorithm to
create the sentence quotation graph contains the fol-
lowing 3 steps: create nodes, create edges and assign
weight to edges. In the following, we first illustrate
how to create nodes and edges. In Section 3.3, we
discuss different ways to assign weight to edges.
Given a fragment quotation graph GF, we first
split each fragment into a set of sentences. For each
sentence, we create a node in the sentence quotation
graph G5. In this way, each sentence in the email
conversation is represented by a distinct node in G5.
As the second step, we create the edges in G5.
The edges in G5 are based on the edges in GF
</bodyText>
<figure confidence="0.9878595625">
a b c
d
e
f
g i
h
j
355
C1 ... ... Ct
(a) Fragment Quotation Graph
P1
C1 Ck
s1 s2
sn
Pk
(b) Sentence Quotation Graph
</figure>
<figureCaption confidence="0.99221">
Figure 2: Create the Sentence Quotation Graph from the
Fragment Quotation Graph
</figureCaption>
<bodyText confidence="0.994062230769231">
because the edges in GF already reflect the reply-
ing relationship among fragments. For each edge
(u, v) ∈ GF, we create edges from each sentence
of u to each sentence of v in the sentence quotation
graph G5. This is illustrated in Figure 2.
Note that when each distinct sentence in an email
conversation is represented as one node in the sen-
tence quotation graph, the extractive email sum-
marization problem is transformed into a standard
node ranking problem within the sentence quotation
graph. Hence, general node ranking algorithms, e.g.,
Page-Rank, can be used for email summarization as
well.
</bodyText>
<subsectionHeader confidence="0.991663">
3.3 Measuring the Cohesion Between
Sentences
</subsectionHeader>
<bodyText confidence="0.9999755">
After creating the nodes and edges in the sentence
quotation graph, a key technical question is how to
measure the degree that two sentences are related to
each other, e.g., a sentence su is replying to or be-
ing replied by sv. In this paper, we use text cohe-
sion between two sentences su and sv to make this
assessment and assign this as the weight of the cor-
responding edge (su, sv). We explore three types
of cohesion measures: (1) clue words that are based
on stems, (2) semantic distance based on WordNet
</bodyText>
<equation confidence="0.895069333333333">
P1 ... ... Pk
F:
s1, s2,...,sn
</equation>
<bodyText confidence="0.993549666666667">
and (3) cosine similarity that is based on the word
TFIDF vector. In the following, we discuss these
three methods separately in detail.
</bodyText>
<subsectionHeader confidence="0.879346">
3.3.1 Clue Words
</subsectionHeader>
<bodyText confidence="0.9977765">
Clue words were originally defined as re-
occurring words with the same stem between two
adjacent fragments in the fragment quotation graph.
In this section, we re-define clue words based on the
sentence quotation graph as follows. A clue word in
a sentence 5 is a non-stop word that also appears
(modulo stemming) in a parent or a child node (sen-
tence) of 5 in the sentence quotation graph.
The frequency of clue words in the two sentences
measures their cohesion as described in Equation 1.
</bodyText>
<equation confidence="0.9451255">
�weight(su, sv) _ freq(wi, sv) (1)
wi∈su
</equation>
<subsectionHeader confidence="0.772409">
3.3.2 Semantic Similarity Based on WordNet
</subsectionHeader>
<bodyText confidence="0.999821235294118">
Other than stems, when people reply to previous
messages they may also choose some semantically
related words, such as synonyms and antonyms, e.g.,
“talk” vs. “discuss”. Based on this observation, we
propose to use semantic similarity to measure the
cohesion between two sentences. We use the well-
known lexical database WordNet to get the seman-
tic similarity of two words. Specifically, we use the
package by (Pedersen et al., 2004), which includes
several methods to compute the semantic similarity.
Among those methods, we choose “lesk” and “jcn”,
which are considered two of the best methods in (Ju-
rafsky and Martin, 2008). Similar to the clue words,
we measure the semantic similarity of two sentences
by the total semantic similarity of the words in both
sentences. This is described in the following equa-
tion.
</bodyText>
<equation confidence="0.8285995">
�weight(su, sv) _ 11 a(wi, wj), (2)
wi∈su wj∈sv
</equation>
<subsectionHeader confidence="0.545711">
3.3.3 Cosine Similarity
</subsectionHeader>
<bodyText confidence="0.97528625">
Cosine similarity is a popular metric to compute
the similarity of two text units. To do so, each sen-
tence is represented as a word vector of TFIDF val-
ues. Hence, the cosine similarity of two sentences
</bodyText>
<equation confidence="0.890378">
−→sv
||−→su||·||
−→sv ||.
su and sv is then computed as
→− su·
</equation>
<page confidence="0.995219">
356
</page>
<sectionHeader confidence="0.731252" genericHeader="method">
4 Summarization Based on the Sentence
Quotation Graph
</sectionHeader>
<bodyText confidence="0.999934875">
Having built the sentence quotation graph with dif-
ferent measures of cohesion, in this section, we de-
velop two summarization approaches. One is the
generalization of the CWS algorithm in (Carenini
et al., 2007) and one is the well-known Page-
Rank algorithm. Both algorithms compute a score,
SentScore(s), for each sentence (node) s, which is
used to select the top-k% sentences as the summary.
</bodyText>
<subsectionHeader confidence="0.986075">
4.1 Generalized ClueWordSummarizer
</subsectionHeader>
<bodyText confidence="0.999960375">
Given the sentence quotation graph, since the weight
of an edge (s, t) represents the extent that s is related
to t, a natural assumption is that the more relevant a
sentence (node) s is to its parents and children, the
more important s is. Based on this assumption, we
compute the weight of a node s by summing up the
weight of all the outgoing and incoming edges of s.
This is described in the following equation.
</bodyText>
<equation confidence="0.998874333333333">
�SentScore(s) � �weight(s, t) + weight(p, s)
(s,t)EGS (p,s)EGS
(3)
</equation>
<bodyText confidence="0.999985636363636">
The weight of an edge (s, t) can be any of the
three metrics described in the previous section. Par-
ticularly, when the weight of the edge is based on
clue words as in Equation 1, this method is equiva-
lent to Algorithm CWS in (Carenini et al., 2007). In
the rest of this paper, let CWS denote the General-
ized ClueWordSummarizer when the edge weight is
based on clue words, and let CWS-Cosine and CWS-
Semantic denote the summarizer when the edge
weight is cosine similarity and semantic similarity
respectively. Semantic can be either “lesk” or “jcn”.
</bodyText>
<subsectionHeader confidence="0.902356">
4.2 Page-Rank-based Summarization
</subsectionHeader>
<bodyText confidence="0.9994922">
The Generalized ClueWordSummarizer only con-
siders the weight of the edges without considering
the importance (weight) of the nodes. This might
be incorrect in some cases. For example, a sentence
replied by an important sentence should get some of
its importance. This intuition is similar to the one
inspiring the well-known Page-Rank algorithm. The
traditional Page-Rank algorithm only considers the
outgoing edges. In email conversations, what we
want to measure is the cohesion between sentences
no matter which one is being replied to. Hence, we
need to consider both incoming and outgoing edges
and the corresponding sentences.
Given the sentence quotation graph Gs, the Page-
Rank-based algorithm is described in Equation 4.
PR(s) is the Page-Rank score of a node (sentence)
s. d is the dumping factor, which is initialized to
0.85 as suggested in the Page-Rank algorithm. In
this way, the rank of a sentence is evaluated globally
based on the graph.
</bodyText>
<sectionHeader confidence="0.9736705" genericHeader="method">
5 Summarization with Subjective
Opinions
</sectionHeader>
<bodyText confidence="0.999483384615385">
Other than the conversation structure, the measures
of cohesion and the graph-based summarization
methods we have proposed, the importance of a sen-
tence in emails can be captured from other aspects.
In many applications, it has been shown that sen-
tences with subjective meanings are paid more at-
tention than factual ones(Pang and Lee, 2004)(Esuli
and Sebastiani, 2006). We evaluate whether this is
also the case in emails, especially when the conver-
sation is about decision making, giving advice, pro-
viding feedbacks, etc.
A large amount of work has been done on deter-
mining the level of subjectivity of text (Shanahan
et al., 2005). In this paper we follow a very sim-
ple approach that, if successful, could be extended
in future work. More specifically, in order to as-
sess the degree of subjectivity of a sentence s, we
count the frequency of words and phrases in s that
are likely to bear subjective opinions. The assump-
tion is that the more subjective words s contains, the
more likely that s is an important sentence for the
purpose of email summarization. Let SubjScore(s)
denote the number of words with a subjective mean-
ing. Equation 5 illustrates how SubjScore(s) is com-
puted. SubjList is a list of words and phrases that
indicate subjective opinions.
</bodyText>
<equation confidence="0.975732">
SubjScore(s) _ 11 freq(wi) (5)
wiESubjList,wiEs
</equation>
<bodyText confidence="0.999758166666667">
The SubjScore(s) alone can be used to evaluate
the importance of a sentence. In addition, we can
combine SubjScore with any of the sentence scores
based on the sentence quotation graph. In this paper,
we use a simple approach by adding them up as the
final sentence score.
</bodyText>
<page confidence="0.949419">
357
</page>
<equation confidence="0.9987614">
PR(s) _ (1 − d) + d *
E
s;Echild(s)
E
weight(s, si) +
sjEparent(s)
E weight(s, si) * PR(si) + E weight(sj, s) * PR(sj)
s;Echild(s) sjEparent(s)
weight(sj, s)
(4)
</equation>
<bodyText confidence="0.999420333333333">
As to the subjective words and phrases, we
consider the following two lists generated by re-
searchers in this area.
</bodyText>
<listItem confidence="0.975424125">
• OpFind: The list of subjective words in (Wil-
son et al., 2005). The major source of this list is
from (Riloff and Wiebe, 2003) with additional
words from other sources. This list contains
8,220 words or phrases in total.
• OpBear: The list of opinion bearing words
in (Kim and Hovy, 2005). This list contains
27,193 words or phrases in total.
</listItem>
<sectionHeader confidence="0.984215" genericHeader="method">
6 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.982005">
6.1 Dataset Setup
</subsectionHeader>
<bodyText confidence="0.993309695652174">
There are no publicly available annotated corpora to
test email summarization techniques. So, the first
step in our evaluation was to develop our own cor-
pus. We use the Enron email dataset, which is the
largest public email dataset. In the 10 largest in-
box folders in the Enron dataset, there are 296 email
conversations. Since we are studying summarizing
email conversations, we required that each selected
conversation contained at least 4 emails. In total, 39
conversations satisfied this requirement. We use the
MEAD package to segment the text into 1,394 sen-
tences (Radev et al., 2004).
We recruited 50 human summarizers to review
those 39 selected email conversations. Each email
conversation was reviewed by 5 different human
summarizers. For each given email conversation,
human summarizers were asked to generate a sum-
mary by directly selecting important sentences from
the original emails in that conversation. We asked
the human summarizers to select 30% of the total
sentences in their summaries.
Moreover, human summarizers were asked to
classify each selected sentence as either essential
or optional. The essential sentences are crucial to
the email conversation and have to be extracted in
any case. The optional sentences are not critical but
are useful to help readers understand the email con-
versation if the given summary length permits. By
classifying essential and optional sentences, we can
distinguish the core information from the support-
ing ones and find the most convincing sentences that
most human summarizers agree on.
As essential sentences are more important than
the optional ones, we give more weight to the es-
sential selections. We compute a GSValue for each
sentence to evaluate its importance according to the
human summarizers’ selections. The score is de-
signed as follows: for each sentence s, one essen-
tial selection has a score of 3, one optional selec-
tion has a score of 1. Thus, the GSValue of a sen-
tence ranges from 0 to 15 (5 human summarizers x
3). The GSValue of 8 corresponds to 2 essential and
2 optional selections. If a sentence has a GSValue
no less than 8, we take it as an overall essential sen-
tence. In the 39 conversations, we have about 12%
overall essential sentences.
</bodyText>
<subsectionHeader confidence="0.995562">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9985353">
Evaluation of summarization is believed to be a dif-
ficult problem in general. In this paper, we use two
metrics to measure the accuracy of a system gener-
ated summary. One is sentence pyramid precision,
and the other is ROUGE recall. As to the statistical
significance, we use the 2-tail pairwise student t-test
in all the experiments to compare two specific meth-
ods. We also use ANOVA to compare three or more
approaches together.
The sentence pyramid precision is a relative pre-
cision based on the GSValue. Since this idea is
borrowed from the pyramid metric by Nenkova et
al.(Nenkova et al., 2007), we call it the sentence
pyramid precision. In this paper, we simplify it as
the pyramid precision. As we have discussed above,
with the reviewers’ selections, we get a GSValue for
each sentence, which ranges from 0 to 15. With
this GSValue, we rank all sentences in a descendant
order. We also group all sentences with the same
GSValue together as one tier Ti, where i is the corre-
</bodyText>
<page confidence="0.996849">
358
</page>
<bodyText confidence="0.999969961538462">
sponding GSValue; i is called the level of the tier Ti.
In this way, we organize all sentences into a pyra-
mid: a sequence of tiers with a descendant order of
levels. With the pyramid of sentences, the accuracy
of a summary is evaluated over the best summary we
can achieve under the same summary length. The
best summary of k sentences are the top k sentences
in terms of GSValue.
Other than the sentence pyramid precision, we
also adopt the ROUGE recall to evaluate the gen-
erated summary with a finer granularity than sen-
tences, e.g., n-gram and longest common subse-
quence. Unlike the pyramid method which gives
more weight to sentences with a higher GSValue,
ROUGE is not sensitive to the difference between
essential and optional selections (it considers all sen-
tences in one summary equally). Directly applying
ROUGE may not be accurate in our experiments.
Hence, we use the overall essential sentences as the
gold standard summary for each conversation, i.e.,
sentences in tiers no lower than T8. In this way,
the ROUGE metric measures the similarity of a sys-
tem generated summary to a gold standard summary
that is considered important by most human sum-
marizers. Specifically, we choose ROUGE-2 and
ROUGE-L as the evaluation metric.
</bodyText>
<subsectionHeader confidence="0.999328">
6.3 Evaluating the Weight of Edges
</subsectionHeader>
<bodyText confidence="0.99769995">
In Section 3.3, we developed three ways to com-
pute the weight of an edge in the sentence quotation
graph, i.e., clue words, semantic similarity based on
WordNet and cosine similarity. In this section, we
compare them together to see which one is the best.
It is well-known that the accuracy of the summariza-
tion method is affected by the length of the sum-
mary. In the following experiments, we choose the
summary length as 10%, 12%, 15%, 20% and 30%
of the total sentences and use the aggregated average
accuracy to evaluate different algorithms.
Table 1 shows the aggregated pyramid preci-
sion over all five summary lengths of CWS, CWS-
Cosine, two semantic similarities, i.e., CWS-lesk
and CWS-jcn. We first use ANOVA to compare the
four methods. For the pyramid precision, the F ratio
is 50, and the p-value is 2.1E-29. This shows that the
four methods are significantly different in the aver-
age accuracy. In Table 1, by comparing CWS with
the other methods, we can see that CWS obtains the
</bodyText>
<table confidence="0.999671857142857">
CWS CWS-Cosine CWS-lesk CWS-jcn
Pyramid 0.60 0.39 0.57 0.57
p-value &lt;0.0001 0.02 0.005
ROUGE-2 0.46 0.31 0.39 0.35
p-value &lt;0.0001 &lt;0.001 &lt;0.001
ROUGE-L 0.54 0.43 0.49 0.45
p-value &lt;0.0001 &lt;0.001 &lt;0.001
</table>
<tableCaption confidence="0.999955">
Table 1: Generalized CWS with Different Edge Weights
</tableCaption>
<bodyText confidence="0.999960921052631">
highest precision (0.60). The widely used cosine
similarity does not perform well. Its precision (0.39)
is about half of the precision of CWS with a p-value
less than 0.0001. This clearly shows that CWS is
significantly better than CWS-Cosine. Meanwhile,
both semantic similarities have lower accuracy than
CWS, and the differences are also statistically sig-
nificant even with the conservative Bonferroni ad-
justment (i.e., the p-values in Table 1 are multiplied
by three).
The above experiments show that the widely used
cosine similarity and the more sophisticated seman-
tic similarity in WordNet are less accurate than the
basic CWS in the summarization framework. This is
an interesting result and can be viewed at least from
the following two aspects. First, clue words, though
straight forward, are good at capturing the impor-
tant sentences within an email conversation. The
higher accuracy of CWS may suggest that people
tend to use the same words to communicate in email
conversations. Some related words in the previous
emails are adopted exactly or in another similar for-
mat (modulo stemming). This is different from other
documents such as newspaper articles and formal re-
ports. In those cases, the authors are usually profes-
sional in writing and choose their words carefully,
even intentionally avoid repeating the same words
to gain some diversity. However, for email conver-
sation summarization, this does not appear to be the
case.
Moreover, in the previous discussion we only con-
sidered the accuracy in precision without consider-
ing the runtime issue. In order to have an idea of
the runtime of the two methods, we did the follow-
ing comparison. We randomly picked 1000 pairs of
words from the 20 conversations and compute their
semantic distance in “jcn”. It takes about 0.056 sec-
onds to get the semantic similarity for one pair on the
</bodyText>
<page confidence="0.996787">
359
</page>
<bodyText confidence="0.999580666666667">
average. In contrast, when the weight of edges are
computed based on clue words, the average runtime
to compute the SentScore for all sentences in a con-
versation is only 0.05 seconds, which is even a little
less than the time to compute the semantic similar-
ity of one pair of words. In other words, when CWS
has generated the summary of one conversation, we
can only get the semantic distance between one pair
of words. Note that for each edge in the sentence
quotation graph, we need to compute the distance
for every pair of words in each sentence. Hence, the
empirical results do not support the use of semantic
similarity. In addition, we do not discuss the runtime
performance of CWS-cosine here because of its ex-
tremely low accuracy.
</bodyText>
<subsectionHeader confidence="0.998791">
6.4 Comparing Page-Rank and CWS
</subsectionHeader>
<bodyText confidence="0.999892647058824">
Table 2 compares Page-Rank and CWS under differ-
ent edge weights. We compare Page-Rank only with
CWS because CWS is better than the other Gener-
alized CWS methods as shown in the previous sec-
tion. This table shows that Page-Rank has a lower
accuracy than that of CWS and the difference is sig-
nificant in all four cases. Moreover, when we com-
pare Table 1 and 2 together, we can find that, for
each kind of edge weight, Page-Rank has a lower
accuracy than the corresponding Generalized CWS.
Note that Page-Rank computes a node’s rank based
on all the nodes and edges in the graph. In contrast,
CWS only considers the similarity between neigh-
boring nodes. The experimental result indicates that
for email conversation, the local similarity based on
clue words is more consistent with the human sum-
marizers’ selections.
</bodyText>
<subsectionHeader confidence="0.97449">
6.5 Evaluating Subjective Opinions
</subsectionHeader>
<bodyText confidence="0.999847083333333">
Table 3 shows the result of using subjective opinions
described in Section 5. The first 3 columns in this ta-
ble are pyramid precision of CWS and using 2 lists
of subjective words and phrases alone. We can see
that by using subjective words alone, the precision of
each subjective list is lower than that of CWS. How-
ever, when we integrate CWS and subjective words
together, as shown in the remaining 2 columns, the
precisions get improved consistently for both lists.
The increase in precision is at least 0.04 with statisti-
cal significance. A natural question to ask is whether
clue words and subjective words overlap much. Our
</bodyText>
<table confidence="0.9997815">
CWS PR PR-Cosine PR-lesk PR-jcn
-Clue
Pyramid 0.60 0.51 0.37 0.54 0.50
p-value &lt; 0.0001 &lt; 0.0001 &lt; 0.0001 &lt; 0.0001
ROUGE-2 0.46 0.4 0.26 0.36 0.39
p-value 0.05 &lt; 0.0001 0.001 0.02
ROUGE-L 0.54 0.49 0.36 0.44 0.48
p-value 0.06 &lt; 0.0001 0.0005 0.02
</table>
<tableCaption confidence="0.854666">
Table 2: Compare Page-Rank with CWS
</tableCaption>
<table confidence="0.999923285714286">
CWS OpFind OpBear CWS+OpFind CWS+OpBear
Pyramid 0.60 0.52 0.59 0.65 0.64
p-value 0.0003 0.8 &lt;0.0001 0.0007
ROUGE-2 0.46 0.37 0.44 0.50 0.49
p-value 0.0004 0.5 0.004 0.06
ROUGE-L 0.54 0.48 0.56 0.60 0.59
p-value 0.01 0.6 0.0002 0.002
</table>
<tableCaption confidence="0.999909">
Table 3: Accuracy of Using Subjective Opinions
</tableCaption>
<bodyText confidence="0.999801">
analysis shows that the overlap is minimal. For the
list of OpFind, the overlapped words are about 8%
of clue words and 4% of OpFind that appear in the
conversations. This result clearly shows that clue
words and subjective words capture the importance
of sentences from different angles and can be used
together to gain a better accuracy.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999842">
We study how to summarize email conversations
based on the conversational cohesion and the sub-
jective opinions. We first create a sentence quota-
tion graph to represent the conversation structure on
the sentence level. We adopt three cohesion metrics,
clue words, semantic similarity and cosine similar-
ity, to measure the weight of the edges. The Gener-
alized ClueWordSummarizer and Page-Rank are ap-
plied to this graph to produce summaries. Moreover,
we study how to include subjective opinions to help
identify important sentences for summarization.
The empirical evaluation shows the following two
discoveries: (1) The basic CWS (based on clue
words) obtains a higher accuracy and a better run-
time performance than the other cohesion measures.
It also has a significant higher accuracy than the
Page-Rank algorithm. (2) By integrating clue words
and subjective words (phrases), the accuracy of
CWS is improved significantly. This reveals an in-
teresting phenomenon and will be further studied.
</bodyText>
<sectionHeader confidence="0.981953" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.6361535">
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
</bodyText>
<page confidence="0.994819">
360
</page>
<reference confidence="0.998533281553398">
the Intelligent Scalable Text Summarization Workshop
(ISTS’97), ACL, Madrid, Spain.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
Proceedings ofthe seventh international conference on
World Wide Web, pages 107–117.
Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.
2005. Scalable discovery of hidden emails from large
folders. In ACM SIGKDD’05, pages 544–549.
Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.
2007. Summarizing email conversations with clue
words. In WWW ’07: Proceedings of the 16th interna-
tional conference on World Wide Web, pages 91–100.
Simon Corston-Oliver, Eric K. Ringger, Michael Gamon,
and Richard Campbell. 2004. Integration of email
and task lists. In First conference on email and anti-
Spam(CEAS), Mountain View, California, USA, July
30-31.
Nicolas Ducheneaut and Victoria Bellotti. 2001. E-mail
as habitat: an exploration of embedded personal infor-
mation management. Interactions, 8(5):30–38.
G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search(JAIR), 22:457–479.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In Proceedings of the International Confer-
ence on Language Resources and Evaluation, May 24-
26.
Danyel Fisher and Paul Moody. 2002. Studies of au-
tomated collection of email records. In University of
Irvine ISR Technical Report UCI-ISR-02-4.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition (Second Edition). Prentice-Hall.
Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Proceedings of the Second International Joint Con-
ference on Natural Language Processing: Companion
Volume, Jeju Island, Republic of Korea, October 11-
13.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2004), July.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transaction on Speech and Language
Processing, 4(2):4.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL ’04: Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 271–278.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of Fifth Annual
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-04),
pages 38–41, May 3-5.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty´s, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40(6):919–938, November.
Owen Rambow, Lokesh Shrestha, John Chen, and
Chirsty Lauridsen. 2004. Summarizing email threads.
In HLT/NAACL, May 2–7.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2003), pages 105–
112.
James G. Shanahan, Yan Qu, and Janyce Wiebe. 2005.
Computing Attitude and Affect in Text: Theory
and Applications (The Information Retrieval Series).
Springer-Verlag New York, Inc.
Lokesh Shrestha and Kathleen McKeown. 2004. Detec-
tion of question-answer pairs in email conversations.
In Proceedings of COLING’04, pages 889–895, Au-
gust 23–27.
Stephen Wan and Kathleen McKeown. 2004. Generat-
ing overview summaries of ongoing email thread dis-
cussions. In Proceedings of COLING’04, the 20th In-
ternational Conference on Computational Linguistics,
August 23–27.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 552–
559, Prague, Czech Republic, June.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, pages 34–35.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread
reassembly using similarity matching. In Third Con-
ference on Email and Anti-Spam (CEAS), July 27 - 28.
</reference>
<page confidence="0.998717">
361
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.977476">
<title confidence="0.998975">Summarizing Emails with Conversational Cohesion and Subjectivity</title>
<author confidence="0.999459">Giuseppe Carenini</author>
<author confidence="0.999459">Raymond T Ng</author>
<author confidence="0.999459">Xiaodong Zhou</author>
<affiliation confidence="0.999788">Department of Computer Science University of British Columbia</affiliation>
<address confidence="0.99328">Vancouver, BC, Canada</address>
<email confidence="0.9957">rng,</email>
<abstract confidence="0.999423388888889">In this paper, we study the problem of summarizing email conversations. We first build a sentence quotation graph that captures the conversation structure among emails. We adopt three cohesion measures: clue words, semantic similarity and cosine similarity as the weight of the edges. Second, we use two graph-based summarization approaches, Generalized ClueWordSummarizer and Page- Rank, to extract sentences as summaries. Third, we propose a summarization approach based on subjective opinions and integrate it with the graph-based ones. The empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures. Moreover, subjective words can significantly improve accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>the Intelligent Scalable Text Summarization Workshop (ISTS’97), ACL,</title>
<date></date>
<location>Madrid,</location>
<marker></marker>
<rawString>the Intelligent Scalable Text Summarization Workshop (ISTS’97), ACL, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In Proceedings ofthe seventh international conference on World Wide Web,</booktitle>
<pages>107--117</pages>
<contexts>
<context position="7414" citStr="Brin and Page, 1998" startWordPosition="1135" endWordPosition="1138">histicated ones such as semantically similar words. Third, we did not consider subjective opinions. Finally, we did not compared CWS to other possible graph-based approaches as we propose in this paper. Other than for email summarization, other document summarization methods have adopted graphranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004). Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences. Graph-ranking algorithms, e.g., PageRank (Brin and Page, 1998), are then applied to rank those sentences. Our method is different from them. First, instead of using the complete graph, we build the graph based on the conversation structure. Second, we try various ways to compute the similarity among sentences and the ranking of the sentences. Several studies in the NLP literature have explored the reoccurrence of similar words within one document due to text cohesion. The idea has been formalized in the construct of lexical chains (Barzilay and Elhadad, 1997). While our approach and lexical chains both rely on lexical cohesion, they are 354 quite differe</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. In Proceedings ofthe seventh international conference on World Wide Web, pages 107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Xiaodong Zhou</author>
</authors>
<title>Scalable discovery of hidden emails from large folders.</title>
<date>2005</date>
<booktitle>In ACM SIGKDD’05,</booktitle>
<pages>544--549</pages>
<contexts>
<context position="10840" citStr="Carenini et al., 2005" startWordPosition="1759" endWordPosition="1762"> the edge (c, a) is not included in Figure 1(b). Figure 1(b) shows the fragment quotation graph of the conversation shown in Figure 1(a) with all the redundant edges removed. In contrast, if threading is done at the coarse granularity of entire emails, as adopted in many studies, the threading would be a simple chain from E6 to E5, E5 to E4 and so on. Fragment f reflects a special and important phenomenon, where the original email of a quotation does not exist in the user’s folder. We call this as the hidden email problem. This problem and its influence on email summarization were studied in (Carenini et al., 2005) and (Carenini et al., 2007). 3.2 Building the Sentence Quotation Graph A fragment quotation graph can only represent the conversation in the fragment granularity. We notice that some sentences in a fragment are more relevant to the conversation than the remaining ones. The fragment quotation graph is not capable of representing this difference. Hence, in the following, we describe how to build a sentence quotation graph from the fragment quotation graph and introduce several ways to give weight to the edges. In a sentence quotation graph G5, each node represents a distinct sentence in the ema</context>
</contexts>
<marker>Carenini, Ng, Zhou, 2005</marker>
<rawString>Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou. 2005. Scalable discovery of hidden emails from large folders. In ACM SIGKDD’05, pages 544–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Xiaodong Zhou</author>
</authors>
<title>Summarizing email conversations with clue words.</title>
<date>2007</date>
<booktitle>In WWW ’07: Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>91--100</pages>
<contexts>
<context position="3488" citStr="Carenini et al., 2007" startWordPosition="524" endWordPosition="527">ersation structure. We first build a sentence quotation graph by content analysis. This graph not only captures the conversation structure more accurately, especially for selective quotations, but it also represents the conversation structure at the finer granularity of sentences. As a second contribution of this paper, we study several ways to measure the cohesion between parent and child sentences in the quotation graph: clue words (re-occurring words in the reply) 353 Proceedings ofACL-08: HLT, pages 353–361, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics (Carenini et al., 2007), semantic similarity and cosine similarity. Hence, we can directly evaluate the importance of each sentence in terms of its cohesion with related ones in the graph. The extractive summarization problem can be viewed as a node ranking problem. We apply two summarization algorithms, Generalized ClueWordSummarizer and Page-Rank to rank nodes in the sentence quotation graph and to select the corresponding most highly ranked sentences as the summary. Subjective opinions are often critical in many conversations. As a third contribution of this paper, we study how to make use of the subjective opini</context>
<context position="6014" citStr="Carenini et al., 2007" startWordPosition="918" endWordPosition="921">milar to the issue-response relationship, Shrestha et al.(Shrestha and McKeown, 2004) proposed methods to identify the question-answer pairs from an email thread. Once again, their results showed that including features about the email thread could greatly improve the accuracy. Similar results were obtained by Corston-Oliver et al. They studied how to identify “action” sentences in email messages and use those sentences as a summary(Corston-Oliver et al., 2004). All these approaches used the email thread as a coarse representation of the underlying conversation structure. In our recent study (Carenini et al., 2007), we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer (CWS) based on the concept of clue words. Our experiments showed that CWS had a higher accuracy than the email summarization approach in (Rambow et al., 2004) and the generic multidocument summarization approach MEAD (Radev et al., 2004). Though effective, the CWS method still suffers from the following four substantial limitations. First, we used a fragment quotation graph to represent the conversation, which has a coarser granularity than the sentence level. For email summarization by </context>
<context position="10868" citStr="Carenini et al., 2007" startWordPosition="1764" endWordPosition="1767">uded in Figure 1(b). Figure 1(b) shows the fragment quotation graph of the conversation shown in Figure 1(a) with all the redundant edges removed. In contrast, if threading is done at the coarse granularity of entire emails, as adopted in many studies, the threading would be a simple chain from E6 to E5, E5 to E4 and so on. Fragment f reflects a special and important phenomenon, where the original email of a quotation does not exist in the user’s folder. We call this as the hidden email problem. This problem and its influence on email summarization were studied in (Carenini et al., 2005) and (Carenini et al., 2007). 3.2 Building the Sentence Quotation Graph A fragment quotation graph can only represent the conversation in the fragment granularity. We notice that some sentences in a fragment are more relevant to the conversation than the remaining ones. The fragment quotation graph is not capable of representing this difference. Hence, in the following, we describe how to build a sentence quotation graph from the fragment quotation graph and introduce several ways to give weight to the edges. In a sentence quotation graph G5, each node represents a distinct sentence in the email conversation, and each ed</context>
<context position="15694" citStr="Carenini et al., 2007" startWordPosition="2601" endWordPosition="2604">lowing equation. �weight(su, sv) _ 11 a(wi, wj), (2) wi∈su wj∈sv 3.3.3 Cosine Similarity Cosine similarity is a popular metric to compute the similarity of two text units. To do so, each sentence is represented as a word vector of TFIDF values. Hence, the cosine similarity of two sentences −→sv ||−→su||·|| −→sv ||. su and sv is then computed as →− su· 356 4 Summarization Based on the Sentence Quotation Graph Having built the sentence quotation graph with different measures of cohesion, in this section, we develop two summarization approaches. One is the generalization of the CWS algorithm in (Carenini et al., 2007) and one is the well-known PageRank algorithm. Both algorithms compute a score, SentScore(s), for each sentence (node) s, which is used to select the top-k% sentences as the summary. 4.1 Generalized ClueWordSummarizer Given the sentence quotation graph, since the weight of an edge (s, t) represents the extent that s is related to t, a natural assumption is that the more relevant a sentence (node) s is to its parents and children, the more important s is. Based on this assumption, we compute the weight of a node s by summing up the weight of all the outgoing and incoming edges of s. This is des</context>
</contexts>
<marker>Carenini, Ng, Zhou, 2007</marker>
<rawString>Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou. 2007. Summarizing email conversations with clue words. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages 91–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Eric K Ringger</author>
<author>Michael Gamon</author>
<author>Richard Campbell</author>
</authors>
<title>Integration of email and task lists.</title>
<date>2004</date>
<booktitle>In First conference on email and antiSpam(CEAS),</booktitle>
<location>Mountain View, California, USA,</location>
<contexts>
<context position="5857" citStr="Corston-Oliver et al., 2004" startWordPosition="892" endWordPosition="895">ation approach for decision-making email discussions (Wan and McKeown, 2004). They extracted the issue and response sentences from an email thread as a summary. Similar to the issue-response relationship, Shrestha et al.(Shrestha and McKeown, 2004) proposed methods to identify the question-answer pairs from an email thread. Once again, their results showed that including features about the email thread could greatly improve the accuracy. Similar results were obtained by Corston-Oliver et al. They studied how to identify “action” sentences in email messages and use those sentences as a summary(Corston-Oliver et al., 2004). All these approaches used the email thread as a coarse representation of the underlying conversation structure. In our recent study (Carenini et al., 2007), we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer (CWS) based on the concept of clue words. Our experiments showed that CWS had a higher accuracy than the email summarization approach in (Rambow et al., 2004) and the generic multidocument summarization approach MEAD (Radev et al., 2004). Though effective, the CWS method still suffers from the following four substantial limitations. </context>
</contexts>
<marker>Corston-Oliver, Ringger, Gamon, Campbell, 2004</marker>
<rawString>Simon Corston-Oliver, Eric K. Ringger, Michael Gamon, and Richard Campbell. 2004. Integration of email and task lists. In First conference on email and antiSpam(CEAS), Mountain View, California, USA, July 30-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Ducheneaut</author>
<author>Victoria Bellotti</author>
</authors>
<title>E-mail as habitat: an exploration of embedded personal information management.</title>
<date>2001</date>
<journal>Interactions,</journal>
<volume>8</volume>
<issue>5</issue>
<contexts>
<context position="1320" citStr="Ducheneaut and Bellotti, 2001" startWordPosition="189" endWordPosition="192">rization approach based on subjective opinions and integrate it with the graph-based ones. The empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures. Moreover, subjective words can significantly improve accuracy. 1 Introduction With the ever increasing popularity of emails, it is very common nowadays that people discuss specific issues, events or tasks among a group of people by emails(Fisher and Moody, 2002). Those discussions can be viewed as conversations via emails and are valuable for the user as a personal information repository(Ducheneaut and Bellotti, 2001). In this paper, we study the problem of summarizing email conversations. Solutions to this problem can help users access the information embedded in emails more effectively. For instance, 10 minutes before a meeting, a user may want to quickly go through a previous discussion via emails that is going to be discussed soon. In that case, rather than reading each individual email one by one, it would be preferable to read a concise summary of the previous discussion with the major information summarized. Email summarization is also helpful for mobile email users on a small screen. Summarizing em</context>
</contexts>
<marker>Ducheneaut, Bellotti, 2001</marker>
<rawString>Nicolas Ducheneaut and Victoria Bellotti. 2001. E-mail as habitat: an exploration of embedded personal information management. Interactions, 8(5):30–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research(JAIR),</journal>
<pages>22--457</pages>
<contexts>
<context position="7206" citStr="Erkan and Radev, 2004" startWordPosition="1103" endWordPosition="1106">l. For email summarization by sentence extraction, the fragment granularity may be inadequate. Second, we only adopted one cohesion measure (clue words that are based on stemming), and did not consider more sophisticated ones such as semantically similar words. Third, we did not consider subjective opinions. Finally, we did not compared CWS to other possible graph-based approaches as we propose in this paper. Other than for email summarization, other document summarization methods have adopted graphranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004). Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences. Graph-ranking algorithms, e.g., PageRank (Brin and Page, 1998), are then applied to rank those sentences. Our method is different from them. First, instead of using the complete graph, we build the graph based on the conversation structure. Second, we try various ways to compute the similarity among sentences and the ranking of the sentences. Several studies in the NLP literature have explored the reoccurrence of similar words within one document due</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research(JAIR), 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="18342" citStr="Esuli and Sebastiani, 2006" startWordPosition="3039" endWordPosition="3042"> the Page-Rank score of a node (sentence) s. d is the dumping factor, which is initialized to 0.85 as suggested in the Page-Rank algorithm. In this way, the rank of a sentence is evaluated globally based on the graph. 5 Summarization with Subjective Opinions Other than the conversation structure, the measures of cohesion and the graph-based summarization methods we have proposed, the importance of a sentence in emails can be captured from other aspects. In many applications, it has been shown that sentences with subjective meanings are paid more attention than factual ones(Pang and Lee, 2004)(Esuli and Sebastiani, 2006). We evaluate whether this is also the case in emails, especially when the conversation is about decision making, giving advice, providing feedbacks, etc. A large amount of work has been done on determining the level of subjectivity of text (Shanahan et al., 2005). In this paper we follow a very simple approach that, if successful, could be extended in future work. More specifically, in order to assess the degree of subjectivity of a sentence s, we count the frequency of words and phrases in s that are likely to bear subjective opinions. The assumption is that the more subjective words s conta</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In Proceedings of the International Conference on Language Resources and Evaluation, May 24-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danyel Fisher</author>
<author>Paul Moody</author>
</authors>
<title>Studies of automated collection of email records.</title>
<date>2002</date>
<booktitle>In University of Irvine ISR</booktitle>
<tech>Technical Report UCI-ISR-02-4.</tech>
<contexts>
<context position="1161" citStr="Fisher and Moody, 2002" startWordPosition="164" endWordPosition="167"> use two graph-based summarization approaches, Generalized ClueWordSummarizer and PageRank, to extract sentences as summaries. Third, we propose a summarization approach based on subjective opinions and integrate it with the graph-based ones. The empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures. Moreover, subjective words can significantly improve accuracy. 1 Introduction With the ever increasing popularity of emails, it is very common nowadays that people discuss specific issues, events or tasks among a group of people by emails(Fisher and Moody, 2002). Those discussions can be viewed as conversations via emails and are valuable for the user as a personal information repository(Ducheneaut and Bellotti, 2001). In this paper, we study the problem of summarizing email conversations. Solutions to this problem can help users access the information embedded in emails more effectively. For instance, 10 minutes before a meeting, a user may want to quickly go through a previous discussion via emails that is going to be discussed soon. In that case, rather than reading each individual email one by one, it would be preferable to read a concise summary</context>
</contexts>
<marker>Fisher, Moody, 2002</marker>
<rawString>Danyel Fisher and Paul Moody. 2002. Studies of automated collection of email records. In University of Irvine ISR Technical Report UCI-ISR-02-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition (Second Edition).</title>
<date>2008</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="14898" citStr="Jurafsky and Martin, 2008" startWordPosition="2463" endWordPosition="2467"> Other than stems, when people reply to previous messages they may also choose some semantically related words, such as synonyms and antonyms, e.g., “talk” vs. “discuss”. Based on this observation, we propose to use semantic similarity to measure the cohesion between two sentences. We use the wellknown lexical database WordNet to get the semantic similarity of two words. Specifically, we use the package by (Pedersen et al., 2004), which includes several methods to compute the semantic similarity. Among those methods, we choose “lesk” and “jcn”, which are considered two of the best methods in (Jurafsky and Martin, 2008). Similar to the clue words, we measure the semantic similarity of two sentences by the total semantic similarity of the words in both sentences. This is described in the following equation. �weight(su, sv) _ 11 a(wi, wj), (2) wi∈su wj∈sv 3.3.3 Cosine Similarity Cosine similarity is a popular metric to compute the similarity of two text units. To do so, each sentence is represented as a word vector of TFIDF values. Hence, the cosine similarity of two sentences −→sv ||−→su||·|| −→sv ||. su and sv is then computed as →− su· 356 4 Summarization Based on the Sentence Quotation Graph Having built t</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition (Second Edition). Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic detection of opinion bearing words and sentences.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Joint Conference on Natural Language Processing: Companion Volume, Jeju Island,</booktitle>
<pages>11--13</pages>
<location>Republic of</location>
<contexts>
<context position="20125" citStr="Kim and Hovy, 2005" startWordPosition="3353" endWordPosition="3356">h by adding them up as the final sentence score. 357 PR(s) _ (1 − d) + d * E s;Echild(s) E weight(s, si) + sjEparent(s) E weight(s, si) * PR(si) + E weight(sj, s) * PR(sj) s;Echild(s) sjEparent(s) weight(sj, s) (4) As to the subjective words and phrases, we consider the following two lists generated by researchers in this area. • OpFind: The list of subjective words in (Wilson et al., 2005). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. This list contains 8,220 words or phrases in total. • OpBear: The list of opinion bearing words in (Kim and Hovy, 2005). This list contains 27,193 words or phrases in total. 6 Empirical Evaluation 6.1 Dataset Setup There are no publicly available annotated corpora to test email summarization techniques. So, the first step in our evaluation was to develop our own corpus. We use the Enron email dataset, which is the largest public email dataset. In the 10 largest inbox folders in the Enron dataset, there are 296 email conversations. Since we are studying summarizing email conversations, we required that each selected conversation contained at least 4 emails. In total, 39 conversations satisfied this requirement.</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Automatic detection of opinion bearing words and sentences. In Proceedings of the Second International Joint Conference on Natural Language Processing: Companion Volume, Jeju Island, Republic of Korea, October 11-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<contexts>
<context position="7178" citStr="Mihalcea and Tarau, 2004" startWordPosition="1098" endWordPosition="1101">nularity than the sentence level. For email summarization by sentence extraction, the fragment granularity may be inadequate. Second, we only adopted one cohesion measure (clue words that are based on stemming), and did not consider more sophisticated ones such as semantically similar words. Third, we did not consider subjective opinions. Finally, we did not compared CWS to other possible graph-based approaches as we propose in this paper. Other than for email summarization, other document summarization methods have adopted graphranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004). Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences. Graph-ranking algorithms, e.g., PageRank (Brin and Page, 1998), are then applied to rank those sentences. Our method is different from them. First, instead of using the complete graph, we build the graph based on the conversation structure. Second, we try various ways to compute the similarity among sentences and the ranking of the sentences. Several studies in the NLP literature have explored the reoccurrence of similar w</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The pyramid method: incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Transaction on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="23070" citStr="Nenkova et al., 2007" startWordPosition="3842" endWordPosition="3845">.2 Evaluation Metrics Evaluation of summarization is believed to be a difficult problem in general. In this paper, we use two metrics to measure the accuracy of a system generated summary. One is sentence pyramid precision, and the other is ROUGE recall. As to the statistical significance, we use the 2-tail pairwise student t-test in all the experiments to compare two specific methods. We also use ANOVA to compare three or more approaches together. The sentence pyramid precision is a relative precision based on the GSValue. Since this idea is borrowed from the pyramid metric by Nenkova et al.(Nenkova et al., 2007), we call it the sentence pyramid precision. In this paper, we simplify it as the pyramid precision. As we have discussed above, with the reviewers’ selections, we get a GSValue for each sentence, which ranges from 0 to 15. With this GSValue, we rank all sentences in a descendant order. We also group all sentences with the same GSValue together as one tier Ti, where i is the corre358 sponding GSValue; i is called the level of the tier Ti. In this way, we organize all sentences into a pyramid: a sequence of tiers with a descendant order of levels. With the pyramid of sentences, the accuracy of </context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The pyramid method: incorporating human content selection variation in summarization evaluation. ACM Transaction on Speech and Language Processing, 4(2):4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="18314" citStr="Pang and Lee, 2004" startWordPosition="3036" endWordPosition="3039">Equation 4. PR(s) is the Page-Rank score of a node (sentence) s. d is the dumping factor, which is initialized to 0.85 as suggested in the Page-Rank algorithm. In this way, the rank of a sentence is evaluated globally based on the graph. 5 Summarization with Subjective Opinions Other than the conversation structure, the measures of cohesion and the graph-based summarization methods we have proposed, the importance of a sentence in emails can be captured from other aspects. In many applications, it has been shown that sentences with subjective meanings are paid more attention than factual ones(Pang and Lee, 2004)(Esuli and Sebastiani, 2006). We evaluate whether this is also the case in emails, especially when the conversation is about decision making, giving advice, providing feedbacks, etc. A large amount of work has been done on determining the level of subjectivity of text (Shanahan et al., 2005). In this paper we follow a very simple approach that, if successful, could be extended in future work. More specifically, in order to assess the degree of subjectivity of a sentence s, we count the frequency of words and phrases in s that are likely to bear subjective opinions. The assumption is that the m</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04),</booktitle>
<pages>38--41</pages>
<contexts>
<context position="14705" citStr="Pedersen et al., 2004" startWordPosition="2433" endWordPosition="2436">aph. The frequency of clue words in the two sentences measures their cohesion as described in Equation 1. �weight(su, sv) _ freq(wi, sv) (1) wi∈su 3.3.2 Semantic Similarity Based on WordNet Other than stems, when people reply to previous messages they may also choose some semantically related words, such as synonyms and antonyms, e.g., “talk” vs. “discuss”. Based on this observation, we propose to use semantic similarity to measure the cohesion between two sentences. We use the wellknown lexical database WordNet to get the semantic similarity of two words. Specifically, we use the package by (Pedersen et al., 2004), which includes several methods to compute the semantic similarity. Among those methods, we choose “lesk” and “jcn”, which are considered two of the best methods in (Jurafsky and Martin, 2008). Similar to the clue words, we measure the semantic similarity of two sentences by the total semantic similarity of the words in both sentences. This is described in the following equation. �weight(su, sv) _ 11 a(wi, wj), (2) wi∈su wj∈sv 3.3.3 Cosine Similarity Cosine similarity is a popular metric to compute the similarity of two text units. To do so, each sentence is represented as a word vector of TF</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity - measuring the relatedness of concepts. In Proceedings of Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04), pages 38–41, May 3-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing and Management,</journal>
<volume>40</volume>
<issue>6</issue>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Malgorzata Sty´s, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40(6):919–938, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Lokesh Shrestha</author>
<author>John Chen</author>
<author>Chirsty Lauridsen</author>
</authors>
<title>Summarizing email threads.</title>
<date>2004</date>
<booktitle>In HLT/NAACL,</booktitle>
<contexts>
<context position="2505" citStr="Rambow et al., 2004" startWordPosition="375" endWordPosition="378">s on a small screen. Summarizing email conversations is challenging due to the characteristics of emails, especially the conversational nature. Most of the existing methods dealing with email conversations use the email thread to represent the email conversation structure, which is not accurate in many cases (Yeh and Harnly, 2006). Meanwhile, most existing email summarization approaches use quantitative features to describe the conversation structure, e.g., number of recipients and responses, and apply some general multi-document summarization methods to extract some sentences as the summary (Rambow et al., 2004) (Wan and McKeown, 2004). Although such methods consider the conversation structure somehow, they simplify the conversation structure into several features and do not fully utilize it into the summarization process. In contrast, in this paper, we propose new summarization approaches by sentence extraction, which rely on a fine-grain representation of the conversation structure. We first build a sentence quotation graph by content analysis. This graph not only captures the conversation structure more accurately, especially for selective quotations, but it also represents the conversation struct</context>
<context position="4888" citStr="Rambow et al., 2004" startWordPosition="744" endWordPosition="747">ive words and phrases can significantly improve email summarization. To summarize, this paper is organized as follows. In Section 2, we discuss related work. After building a sentence quotation graph to represent the conversation structure in Section 3, we apply two summarization methods in Section 4. In Section 5, we study summarization approaches with subjective opinions. Section 6 presents the empirical evaluation of our methods. We conclude this paper and propose future work in Section 7. 2 Related Work Rambow et al. proposed a sentence extraction summarization approach for email threads (Rambow et al., 2004). They described each sentence in an email conversations by a set of features and used machine learning to classify whether or not a sentence should be included into the summary. Their experiments showed that features about emails and the email thread could significantly improve the accuracy of summarization. Wan et al. proposed a summarization approach for decision-making email discussions (Wan and McKeown, 2004). They extracted the issue and response sentences from an email thread as a summary. Similar to the issue-response relationship, Shrestha et al.(Shrestha and McKeown, 2004) proposed m</context>
<context position="6280" citStr="Rambow et al., 2004" startWordPosition="961" endWordPosition="964"> accuracy. Similar results were obtained by Corston-Oliver et al. They studied how to identify “action” sentences in email messages and use those sentences as a summary(Corston-Oliver et al., 2004). All these approaches used the email thread as a coarse representation of the underlying conversation structure. In our recent study (Carenini et al., 2007), we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer (CWS) based on the concept of clue words. Our experiments showed that CWS had a higher accuracy than the email summarization approach in (Rambow et al., 2004) and the generic multidocument summarization approach MEAD (Radev et al., 2004). Though effective, the CWS method still suffers from the following four substantial limitations. First, we used a fragment quotation graph to represent the conversation, which has a coarser granularity than the sentence level. For email summarization by sentence extraction, the fragment granularity may be inadequate. Second, we only adopted one cohesion measure (clue words that are based on stemming), and did not consider more sophisticated ones such as semantically similar words. Third, we did not consider subject</context>
</contexts>
<marker>Rambow, Shrestha, Chen, Lauridsen, 2004</marker>
<rawString>Owen Rambow, Lokesh Shrestha, John Chen, and Chirsty Lauridsen. 2004. Summarizing email threads. In HLT/NAACL, May 2–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>105--112</pages>
<contexts>
<context position="19963" citStr="Riloff and Wiebe, 2003" startWordPosition="3325" endWordPosition="3328">nce of a sentence. In addition, we can combine SubjScore with any of the sentence scores based on the sentence quotation graph. In this paper, we use a simple approach by adding them up as the final sentence score. 357 PR(s) _ (1 − d) + d * E s;Echild(s) E weight(s, si) + sjEparent(s) E weight(s, si) * PR(si) + E weight(sj, s) * PR(sj) s;Echild(s) sjEparent(s) weight(sj, s) (4) As to the subjective words and phrases, we consider the following two lists generated by researchers in this area. • OpFind: The list of subjective words in (Wilson et al., 2005). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. This list contains 8,220 words or phrases in total. • OpBear: The list of opinion bearing words in (Kim and Hovy, 2005). This list contains 27,193 words or phrases in total. 6 Empirical Evaluation 6.1 Dataset Setup There are no publicly available annotated corpora to test email summarization techniques. So, the first step in our evaluation was to develop our own corpus. We use the Enron email dataset, which is the largest public email dataset. In the 10 largest inbox folders in the Enron dataset, there are 296 email conversations. Since we are studyin</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2003), pages 105– 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James G Shanahan</author>
<author>Yan Qu</author>
<author>Janyce Wiebe</author>
</authors>
<date>2005</date>
<booktitle>Computing Attitude and Affect in Text: Theory and Applications (The Information Retrieval Series).</booktitle>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="18606" citStr="Shanahan et al., 2005" startWordPosition="3085" endWordPosition="3088"> the conversation structure, the measures of cohesion and the graph-based summarization methods we have proposed, the importance of a sentence in emails can be captured from other aspects. In many applications, it has been shown that sentences with subjective meanings are paid more attention than factual ones(Pang and Lee, 2004)(Esuli and Sebastiani, 2006). We evaluate whether this is also the case in emails, especially when the conversation is about decision making, giving advice, providing feedbacks, etc. A large amount of work has been done on determining the level of subjectivity of text (Shanahan et al., 2005). In this paper we follow a very simple approach that, if successful, could be extended in future work. More specifically, in order to assess the degree of subjectivity of a sentence s, we count the frequency of words and phrases in s that are likely to bear subjective opinions. The assumption is that the more subjective words s contains, the more likely that s is an important sentence for the purpose of email summarization. Let SubjScore(s) denote the number of words with a subjective meaning. Equation 5 illustrates how SubjScore(s) is computed. SubjList is a list of words and phrases that in</context>
</contexts>
<marker>Shanahan, Qu, Wiebe, 2005</marker>
<rawString>James G. Shanahan, Yan Qu, and Janyce Wiebe. 2005. Computing Attitude and Affect in Text: Theory and Applications (The Information Retrieval Series). Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lokesh Shrestha</author>
<author>Kathleen McKeown</author>
</authors>
<title>Detection of question-answer pairs in email conversations.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING’04,</booktitle>
<pages>889--895</pages>
<contexts>
<context position="5477" citStr="Shrestha and McKeown, 2004" startWordPosition="834" endWordPosition="837">or email threads (Rambow et al., 2004). They described each sentence in an email conversations by a set of features and used machine learning to classify whether or not a sentence should be included into the summary. Their experiments showed that features about emails and the email thread could significantly improve the accuracy of summarization. Wan et al. proposed a summarization approach for decision-making email discussions (Wan and McKeown, 2004). They extracted the issue and response sentences from an email thread as a summary. Similar to the issue-response relationship, Shrestha et al.(Shrestha and McKeown, 2004) proposed methods to identify the question-answer pairs from an email thread. Once again, their results showed that including features about the email thread could greatly improve the accuracy. Similar results were obtained by Corston-Oliver et al. They studied how to identify “action” sentences in email messages and use those sentences as a summary(Corston-Oliver et al., 2004). All these approaches used the email thread as a coarse representation of the underlying conversation structure. In our recent study (Carenini et al., 2007), we built a fragment quotation graph to represent an email con</context>
</contexts>
<marker>Shrestha, McKeown, 2004</marker>
<rawString>Lokesh Shrestha and Kathleen McKeown. 2004. Detection of question-answer pairs in email conversations. In Proceedings of COLING’04, pages 889–895, August 23–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Generating overview summaries of ongoing email thread discussions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING’04, the 20th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="2529" citStr="Wan and McKeown, 2004" startWordPosition="379" endWordPosition="382">ummarizing email conversations is challenging due to the characteristics of emails, especially the conversational nature. Most of the existing methods dealing with email conversations use the email thread to represent the email conversation structure, which is not accurate in many cases (Yeh and Harnly, 2006). Meanwhile, most existing email summarization approaches use quantitative features to describe the conversation structure, e.g., number of recipients and responses, and apply some general multi-document summarization methods to extract some sentences as the summary (Rambow et al., 2004) (Wan and McKeown, 2004). Although such methods consider the conversation structure somehow, they simplify the conversation structure into several features and do not fully utilize it into the summarization process. In contrast, in this paper, we propose new summarization approaches by sentence extraction, which rely on a fine-grain representation of the conversation structure. We first build a sentence quotation graph by content analysis. This graph not only captures the conversation structure more accurately, especially for selective quotations, but it also represents the conversation structure at the finer granula</context>
<context position="5305" citStr="Wan and McKeown, 2004" startWordPosition="807" endWordPosition="810">ation of our methods. We conclude this paper and propose future work in Section 7. 2 Related Work Rambow et al. proposed a sentence extraction summarization approach for email threads (Rambow et al., 2004). They described each sentence in an email conversations by a set of features and used machine learning to classify whether or not a sentence should be included into the summary. Their experiments showed that features about emails and the email thread could significantly improve the accuracy of summarization. Wan et al. proposed a summarization approach for decision-making email discussions (Wan and McKeown, 2004). They extracted the issue and response sentences from an email thread as a summary. Similar to the issue-response relationship, Shrestha et al.(Shrestha and McKeown, 2004) proposed methods to identify the question-answer pairs from an email thread. Once again, their results showed that including features about the email thread could greatly improve the accuracy. Similar results were obtained by Corston-Oliver et al. They studied how to identify “action” sentences in email messages and use those sentences as a summary(Corston-Oliver et al., 2004). All these approaches used the email thread as </context>
</contexts>
<marker>Wan, McKeown, 2004</marker>
<rawString>Stephen Wan and Kathleen McKeown. 2004. Generating overview summaries of ongoing email thread discussions. In Proceedings of COLING’04, the 20th International Conference on Computational Linguistics, August 23–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>552--559</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7150" citStr="Wan et al., 2007" startWordPosition="1094" endWordPosition="1097">ch has a coarser granularity than the sentence level. For email summarization by sentence extraction, the fragment granularity may be inadequate. Second, we only adopted one cohesion measure (clue words that are based on stemming), and did not consider more sophisticated ones such as semantically similar words. Third, we did not consider subjective opinions. Finally, we did not compared CWS to other possible graph-based approaches as we propose in this paper. Other than for email summarization, other document summarization methods have adopted graphranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004). Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences. Graph-ranking algorithms, e.g., PageRank (Brin and Page, 1998), are then applied to rank those sentences. Our method is different from them. First, instead of using the complete graph, we build the graph based on the conversation structure. Second, we try various ways to compute the similarity among sentences and the ranking of the sentences. Several studies in the NLP literature have explored t</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552– 559, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<contexts>
<context position="19899" citStr="Wilson et al., 2005" startWordPosition="3312" endWordPosition="3316">Es The SubjScore(s) alone can be used to evaluate the importance of a sentence. In addition, we can combine SubjScore with any of the sentence scores based on the sentence quotation graph. In this paper, we use a simple approach by adding them up as the final sentence score. 357 PR(s) _ (1 − d) + d * E s;Echild(s) E weight(s, si) + sjEparent(s) E weight(s, si) * PR(si) + E weight(sj, s) * PR(sj) s;Echild(s) sjEparent(s) weight(sj, s) (4) As to the subjective words and phrases, we consider the following two lists generated by researchers in this area. • OpFind: The list of subjective words in (Wilson et al., 2005). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. This list contains 8,220 words or phrases in total. • OpBear: The list of opinion bearing words in (Kim and Hovy, 2005). This list contains 27,193 words or phrases in total. 6 Empirical Evaluation 6.1 Dataset Setup There are no publicly available annotated corpora to test email summarization techniques. So, the first step in our evaluation was to develop our own corpus. We use the Enron email dataset, which is the largest public email dataset. In the 10 largest inbox folders in the Enron </context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Opinionfinder: a system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, pages 34–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen-Yuan Yeh</author>
<author>Aaron Harnly</author>
</authors>
<title>Email thread reassembly using similarity matching.</title>
<date>2006</date>
<booktitle>In Third Conference on Email and Anti-Spam (CEAS),</booktitle>
<pages>28</pages>
<contexts>
<context position="2217" citStr="Yeh and Harnly, 2006" startWordPosition="337" endWordPosition="340">n via emails that is going to be discussed soon. In that case, rather than reading each individual email one by one, it would be preferable to read a concise summary of the previous discussion with the major information summarized. Email summarization is also helpful for mobile email users on a small screen. Summarizing email conversations is challenging due to the characteristics of emails, especially the conversational nature. Most of the existing methods dealing with email conversations use the email thread to represent the email conversation structure, which is not accurate in many cases (Yeh and Harnly, 2006). Meanwhile, most existing email summarization approaches use quantitative features to describe the conversation structure, e.g., number of recipients and responses, and apply some general multi-document summarization methods to extract some sentences as the summary (Rambow et al., 2004) (Wan and McKeown, 2004). Although such methods consider the conversation structure somehow, they simplify the conversation structure into several features and do not fully utilize it into the summarization process. In contrast, in this paper, we propose new summarization approaches by sentence extraction, whic</context>
</contexts>
<marker>Yeh, Harnly, 2006</marker>
<rawString>Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread reassembly using similarity matching. In Third Conference on Email and Anti-Spam (CEAS), July 27 - 28.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>