<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038580">
<title confidence="0.980747">
Deep Unsupervised Feature Learning for Natural Language Processing
</title>
<author confidence="0.976066">
Stephan Gouws
</author>
<affiliation confidence="0.725772">
MIH Media Lab, Stellenbosch University
Stellenbosch, South Africa
</affiliation>
<email confidence="0.995524">
stephan@ml.sun.ac.za
</email>
<sectionHeader confidence="0.995592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884666666667">
Statistical natural language processing (NLP) builds
models of language based on statistical features ex-
tracted from the input text. We investigate deep
learning methods for unsupervised feature learning
for NLP tasks. Recent results indicate that features
learned using deep learning methods are not a sil-
ver bullet and do not always lead to improved re-
sults. In this work we hypothesise that this is the
result of a disjoint training protocol which results
in mismatched word representations and classifiers.
We also hypothesise that modelling long-range de-
pendencies in the input and (separately) in the out-
put layers would further improve performance. We
suggest methods for overcoming these limitations,
which will form part of our final thesis work.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993255">
Natural language processing (NLP) can be seen as build-
ing models h : X → Y for mapping an input encoding
x E X representing a natural language (NL) fragment, to
an output encoding y E Y representing some construct or
formalism used in the particular NLP task of interest, e.g.
part-of-speech (POS) tags, begin-, inside-, outside (BIO)
tags for information extraction, semantic role labels, etc.
Since the 90s, the predominant approach has been sta-
tistical NLP, where one models the problem as learning a
predictive function h for mapping from h : X → Y using
machine learning techniques. Machine learning consists
of a hypothesis function which learns this mapping based
on latent or explicit features extracted from the input data.
In this framework, h is usually trained in a supervised
setting from labelled training pairs (xi, yi) E X x Y.
Additionally, the discriminant function h typically oper-
ates on a transformed representation of the data to a com-
mon feature space encoded as a feature vector O(x), and
then learns a mapping from feature space to the output
space, h : O(x) → y. In supervised learning, the idea
</bodyText>
<page confidence="0.991096">
48
</page>
<table confidence="0.990756833333333">
x~ E X the cat sits on the mat
�/,
O(~x) Y&apos;(X1) O(X2) φ(x3) φ(x4) φ(x5) φ(x6)
O(X2)
y~ E Y B-NP I-NP B-VP O B-NP I-NP
NE Tags [the cat] NP [sits] yP [on] O [the mat] NP
</table>
<tableCaption confidence="0.943065">
Table 1: Example NLP syntactic chunking task for the sentence
“the cat sits on the mat”. X represents the words in the input
space, Y represents labels in the output space. O(x) is a feature
representation for the input text x and the bottom row represents
the output named entity tags in a more standard form.
</tableCaption>
<bodyText confidence="0.99897832">
is generally that features represent strong discriminating
characteristics of the problem gained through manual en-
gineering and domain-specific insight.
As a concrete example, consider the task of syntactic
chunking, also called “shallow parsing”, (Gildea and Ju-
rafsky, 2002): Given an input string, e.g.
“the cat sits on the mat”,
the chunking problem consists of labelling segments of a
sentence with syntactic constituents such as noun or verb
phrases (NPs or VPs). Each word is assigned one unique
tag often encoded using the BIO encoding1. We repre-
sent the input text as a vector of words xi E x, and each
word’s corresponding label is represented by yi E y� (see
Table 1). Given a feature generating function O(xi) and
a set of labelled training pairs (xi, yi) E X x Y, the task
then reduces to learning a suitable mapping h : O(X) →
Y.
Most previous works have focused on manually en-
gineered features and simpler, linear models, includ-
ing “shallow” model architectures, like the percep-
tron (Rosenblatt, 1957), linear SVM (Cortes and Vap-
nik, 1995) and linear-chain conditional random fields
(CRFs) (Lafferty, 2001). However, a shallow learning ar-
chitecture is only as good as its input features. Due to the
complex nature of NL, deeper architectures may be re-
</bodyText>
<footnote confidence="0.988188">
1E.g. B-NP means “begin NP”, I-NP means “inside NP”, and O
means other/outside.
</footnote>
<note confidence="0.7463035">
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 48–53,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999928048780488">
quired to learn data representations which contain the ap-
propriate level of information for the task at hand. Prior to
2006, it was computationally infeasible to perform infer-
ence in hierarchical (“deep”), non-linear models such as
multi-layer perceptrons with more than one hidden layer.
However, Hinton (2006) proposed an efficient, layer-wise
greedy method for learning the model parameters in these
architectures, which spurred a renewed interest in deep
learning research.
Still, creating annotated training data is labour-
intensive and costly, and manually designing and extract-
ing discriminating features from the data to be used in
the learning process is a costly procedure requiring sig-
nificant levels of domain expertise. Over the last two
decades, the growth of available unlabeled data x E X
and the ubiquity of scalable computing power has shifted
research focus to unsupervised approaches for automat-
ically learning appropriate feature representations O(x)
from large collections of unlabeled text.
Several methods have been proposed for unsuper-
vised feature learning, including simple k-means cluster-
ing (Lloyd, 1982), Brown clustering (Brown et al., 1992),
mutual information (Shannon and Weaver, 1962), princi-
pal components analysis (PCA) (Jolliffe, 2002), and in-
dependent component analysis (ICA) (Hyv¨arinen et al.,
2001).
However, natural language has complex mappings
from text to meaning, arguably involving higher-order
correlations between words which these simpler meth-
ods struggle to model adequately. Advances in the “deep
learning” community allow us to perform efficient unsu-
pervised feature learning in highly complex and high-
dimensional input feature spaces, making it an attrac-
tive method for learning features in e.g. vision or lan-
guage (Bengio, 2009).
The standard deep learning approach is to learn
lower-dimensional embeddings from the raw high-
dimensional2 input space X to lower dimensional (e.g.
50-dimensional) feature spaces in an unsupervised man-
ner, via repeated, layer-wise, non-linear transformation
of the input features, e.g.
</bodyText>
<equation confidence="0.935195">
y = f(k)(··· f(2)(f(1)(x)) ···) �
</equation>
<bodyText confidence="0.8882404">
where f(i)(x) is some non-linear function (typically
tanh) for which the parameters are learned by back prop-
agating error gradients. This configuration is referred to
as a “deep” architecture with k layers (see Figure 1 for an
example).
For feature generation, we present a trained network
with a new vector x� representing the input data on its
2E.g. a “one-hot” 50,000-dimensional vector of input words, with a
‘1’ indicating the presence of the word at that index, and a ‘0’ every-
where else.
</bodyText>
<figureCaption confidence="0.9725568">
Figure 1: Example of a deep model. The input vector x is trans-
formed into the hidden representation, here denoted as h1, using
an affine transformation W and a non-linearity. Each subse-
quent hidden layer hk takes as input the output of its preceding
layer h(k−1) (Bengio, 2009).
</figureCaption>
<bodyText confidence="0.999948789473684">
input layer. After performing one iteration of forward-
propagation through the network, we can then view the
activation values in the hidden layers as dense, so-called
“distributed representations” (features) of the input data.
These features can in turn be passed to an output clas-
sifier layer to produce some tagging task of interest. Re-
cent work in deep learning show state-of-the-art results in
part-of-speech parsing, chunking and named-entity tag-
ging (Collobert, 2011), however performance in more
complex NLP tasks like entity and event disambiguation
and semantic role labelling are still trailing behind.
In this work we focus specifically on extending current
state of the art deep neural models to improve their per-
formance on these more difficult tasks. In the following
section we briefly review and discuss the merits and lim-
itations of three of the current state of the art deep learn-
ing models for NLP. We then identify our primary re-
search questions and introduce our proposed future work
roadmap.
</bodyText>
<sectionHeader confidence="0.468882" genericHeader="method">
2 Current State-of-the-Art and
Limitations
</sectionHeader>
<bodyText confidence="0.9996035">
Most work builds on the idea of a neural probabilistic
language model (NPLM) where words are represented
by learned real-valued embeddings, and a neural network
combines word embeddings to predict the most likely
next word. The first successful NPLM was introduced
by Bengio et al. in 2003 (Bengio et al., 2003).
</bodyText>
<page confidence="0.996746">
49
</page>
<bodyText confidence="0.999955333333333">
Historically, training and testing these models were
slow, scaling linearly in the vocabulary size. However,
several recent approaches have been proposed which
overcome these limitations (Morin and Bengio, 2005),
including the work by Collobert and Weston (2008) and
Mnih and Hinton (2009) discussed next.
</bodyText>
<subsectionHeader confidence="0.996448">
2.1 Collobert &amp; Weston (2008)
</subsectionHeader>
<bodyText confidence="0.999963882352941">
Collobert and Weston (2008) present a discriminative,
non-probabilistic, non-linear neural language model that
can be scaled to train over billions of words since each
training iteration only computes a loss gradient over a
small stochastic sample of the training data.
All K-dimensional word embeddings are initially set
to a random state. During each training iteration, an n-
gram is read from the training data and each word is
mapped to its respective embedding. All embeddings are
then concatenated to form a nK-length positive training
vector. A corrupted n-gram is also created by replac-
ing the n’th (last) word by some word uniformly chosen
from the vocabulary. The training criterion is that the net-
work must predict positive training vectors with a score
at least some margin higher than the score predicted for
corrupted n-grams. Model parameters are trained simul-
taneously with word embeddings via gradient descent.
</bodyText>
<subsectionHeader confidence="0.997746">
2.2 The Hierarchical Log Bilinear (HLBL) Model
</subsectionHeader>
<bodyText confidence="0.999983888888889">
Mnih and Hinton (2007) proposed a simple probabilis-
tic linear neural language model called the log bilin-
ear (LBL) model. For an n-gram context window, the
LBL model concatenates the first (n −1) K-dimensional
word embeddings and then learns a linear mapping from
R(n−1)K to RK for predicting the embedding of the nth
word. For predicting the next word, the model outputs
a probability distribution over the entire vocabulary by
computing the dot product between the predicted embed-
ding and the embedding for each word in the vocabulary
in an output softmax layer. This softmax computation is
linear in the length of the vocabulary for each prediction,
and is therefore the performance bottleneck.
In follow-up work, Mnih and Hinton (2009) speed up
training and testing time by extending the LBL model to
predict the next word by hierarchically decomposing the
search through the vocabulary by traversing a binary tree
constructed over the vocabulary. This speeds up train-
ing and testing exponentially, but initially reduces model
performance as the construction of the binary partition-
ing has a strong effect on the model’s performance. They
introduce a method for bootstrapping the construction of
the tree by initially using a random binary tree to learn
word embeddings, and then rebuilding the tree based on
a clustering of the learned embeddings. Their final results
are superior to the standard LBL in both model perplexity
and model testing time.
</bodyText>
<subsectionHeader confidence="0.996538">
2.3 Recursive Neural Networks (RNNs)
</subsectionHeader>
<bodyText confidence="0.999957541666667">
Socher (2010; 2011) introduces a recursive neural net-
work (RNN) framework for parsing natural language.
Previous approaches dealt with variable-length sentences
by either
i using a window approach (shifting a window of n
words over the input, processing each fixed-size win-
dow at a time), or
ii by using a convolutional layer where each word is
convolved with its neighbours within some sentence-
or window-boundary.
RNNs operate by recursively applying the same neural
network to segments of its input, thereby allowing RNNs
to naturally operate on variable-length inputs. Each pair
of neighbouring words is scored by the network to reflect
how likely these two words are considered to form part of
a phrase. Each such operation takes two K-dimensional
word vectors and outputs another K-dimensional vector
and a score. Socher (2010) proposes several strategies
(ranging from local and greedy to global and optimal) for
choosing which pairs of words to collapse into a new K-
dimensional vector representing the phrase comprised of
the two words. By viewing these collapsing operations
as branch merging decisions, one can construct a binary
parse tree over the words in a bottom-up fashion.
</bodyText>
<subsectionHeader confidence="0.997232">
2.4 Discussion of Limitations
</subsectionHeader>
<bodyText confidence="0.99992488">
Neural language models are appealing since they can
more easily deal with missing data (unknown word com-
binations) due to their inherent continuous-space repre-
sentation, whereas n-gram language models (Manning et
al., 1999) need to employ (sometimes ad hoc) methods
for smoothing unseen and hence zero probability word
combinations.
The original NPLM performs well in terms of model
perplexity on held-out data; however, its training and test-
ing time is very slow. Furthermore, it provides no support
for handling multiple word senses, the property that any
word can have more than one meaning, since each word
is assigned an embedding based on its literal string repre-
sentation (i.e. from a lookup table).
The Collobert &amp; Weston model still provides no mech-
anism for handling word senses, but improves on the
NPLM by adding several non-linear layers which in-
crease its modelling capacity, and a convolutional layer
for modelling longer range dependencies between words.
Recursive neural nets (RNNs) directly address the prob-
lem of longer-range dependencies by allowing neighbour
words to be combined into their phrasal equivalents in a
bottom-up process.
The LBL model, despite its very simple linear struc-
ture, provides very good performance in terms of model
</bodyText>
<page confidence="0.990577">
50
</page>
<bodyText confidence="0.999955166666667">
perplexity, but shares the problem of slow training and
testing times and the inability to handle word senses or
dependencies between words (outside its n-gram con-
text).
In the HLBL model, Mnih and Hinton address the slow
testing performance of the LBL model by using a hierar-
chical search tree over the vocabulary to exponentially
speed up testing time, analogous to the concept of class-
based language models (Brown et al., 1992). The HLBL
model can also handle multiple word senses, but in their
evaluation they show that in practice the model learns
multiple senses (codes) for infrequently observed words
instead of words with more than one meaning (Mnih and
Hinton, 2009). The performance is strongly dependent
on the initialisation of the tree, for which they present an
iterative but non-optimal bootstrap-and-train procedure.
Despite being non-optimal, it is shown to outperform the
standard LBL model in terms of perplexity.
</bodyText>
<sectionHeader confidence="0.988612" genericHeader="method">
3 Mismatched Word Representations and
Classifiers
</sectionHeader>
<bodyText confidence="0.943046333333333">
The deep learning ideal is to train deep, non-linear mod-
els over large collections of unlabeled data, and then use
these models to automatically extract information-rich,
higher-level features3 to integrate into standard NLP or
image processing systems as added features to improve
performance. However, several recent papers report sur-
prising and seemingly contradicting results for this ideal.
In the most direct comparison for NLP, Turian (2010)
compares features extracted using Brown clustering (a
hierarchical clustering technique for clustering words
based on their observed co-occurrence patterns), the hi-
erarchical log-bilinear (HLBL) embeddings (Mnih and
Hinton, 2007) and Collobert and Weston (C+W) em-
beddings (Collobert and Weston, 2008), by integrating
these as additional features in standard supervised condi-
tional random field (CRF) classification systems for NLP.
Somewhat surprisingly, they find that using the more
complex C+W and HLBL features do not improve signif-
icantly over Brown features. Indeed, under several con-
ditions the Brown features give the best results.
These results are important for several reasons (we
highlight these results in Table 2). The goal was to im-
prove classification performance in structured prediction
tasks in natural language by integrating features learned
in a deep, unsupervised approach within a standard lin-
ear classification framework. Yet these complex, deep
methods are outperformed by simpler unsupervised fea-
ture extraction methods.
3“Higher-level” features simply mean combining simpler features
extracted from a text to produce conceptually more abstract indicators,
e.g. combining word-indicators for “attack”, “soldier”, etc. to form an
indicator for WAR, even though “war” is not mentioned anywhere in the
text.
</bodyText>
<table confidence="0.999720166666667">
System Dev Test MUC7
Baseline 90.03 84.39 67.48
HLBL 100-dim 92.00 88.13 75.25
C&amp;W 50-dim 92.27 87.93 75.74
Brown, 1000 clusters 92.32 88.52 78.84
C&amp;W 200-dim 92.46 87.96 75.51
</table>
<tableCaption confidence="0.998538">
Table 2: Final NER F1 results reported by Turian (2010).
</tableCaption>
<bodyText confidence="0.999975315789474">
In a sense, these seem to be negative results for the
utility of deep learning in NLP. However, in this work we
argue that these seemingly anomalous results stem from
a mismatch between the feature learning function and the
classifier that was used in the classification (and hence
evaluation) process.
We consider the learning problem h : X → Y to
decompose into h = h&apos;(O(X)), where O is the feature
learning function and h&apos; is a standard supervised classi-
fier. O reads input from X and outputs encodings in fea-
ture space O(x). h reads input in feature space O(x) and
outputs encodings in the output label space Y.
Note that this easily extends to deep feature
learning models by simply replacing O(X) with
O(k)(··· O(2)(O(1)(X)) · · ·), for a k-layer architecture,
where the first layer reads input in X and each subsequent
layer reads the output of the previous layer.
Within this view of the deep learning process, we can
see that unsupervised feature learning does not happen in
isolation. Instead, the learned features only make sense
within some learning framework, since the output of the
feature learning function O (and each deep layer O(k−1))
maps to a region in feature code space which becomes
in turn the input to the output classifier h&apos; (or subsequent
layer O(k)) . We therefore argue that in a semi-supervised
or unsupervised classification problem, the feature learn-
ing function O should be strongly dependent on the clas-
sifier h&apos; that interprets those features, and vice versa.
This notion ties in with the standard deep-learning
training protocol of unsupervised pre-training followed
by joint supervised fine-tuning (Hinton et al., 2006) of the
top classification layer and the deeper feature extraction
layers. We conjecture that jointly training a deep feature
extraction model with a linear output classifier leads to
better linearly separable feature vectors O(x) than train-
ing both independently. Note that this is in contrast to
how Turian (2010) integrated the unsupervised features
into existing NLP systems via disjoint training.
</bodyText>
<sectionHeader confidence="0.974083" genericHeader="method">
4 Proposed Work and Research Questions
</sectionHeader>
<bodyText confidence="0.99554575">
For simpler sequence tagging tasks such as part-of-
speech tagging and noun phrase chunking, the state-
of-the-art models introduced in Section 2 perform ade-
quately. However, in order to make use of the increased
</bodyText>
<page confidence="0.996587">
51
</page>
<bodyText confidence="0.999929404761905">
modelling capacity of deep neural models, and to suc-
cessfully model more complex semantic tasks such as
anaphora resolution and semantic role labelling, we hy-
pothesise that the model needs to avoid modelling purely
local lexical semantics and needs to efficiently handle
multiple word senses and long-range dependencies be-
tween input words (or phrases) and output labels. We
propose to overcome the limitations of previous models
with regard to these design goals, by focusing on the fol-
lowing key areas:
Input language representation: Neural models rely
on vector representations of their input (as opposed to
discrete representations as in, for instance, HMMs). In
NLP, sentences are therefore encoded as real-valued em-
bedding vectors. These vectors are learned in either a
task-specific setting (as in the C+W model) or as part of
a language model (as in the LBL model), where the goal
is to predict the next word given the learned representa-
tions of the previous words. In order to maximise the
information available to the model, we need to provide
information-rich representations to the model. Current
approaches represent each word in a sentence using a dis-
tinct word vector based on its literal string representation.
However, as noted earlier, in NL the same words can have
different senses based on the context in which it appears
(polysemy). We propose to extend the hierarchical log-
bilinear (HLBL) language model (see Section 2.2) in two
important ways. We choose the HLBL model for its sim-
plicity and good performance compared to more complex
models.
Firstly, we propose to replace the iterative bootstrap-
and-train process for learning the hierarchical tree struc-
ture over the vocabulary with a modified self-balancing
binary tree. The tree rebalances itself from an initial
random tree to leave most frequently accessed words
near the root (for shorter codes and faster access times),
while moving words between clusters to maximise over-
all model perplexity.
Secondly, we propose to add a word sense disambigua-
tion layer capable of modelling long-range dependencies
between input words. For this layer we will compare a
modified RNN layer to a convolutional layer. The modi-
fied RNN will embed each focus word with its n most dis-
criminative neighbour words (in a sentence context win-
dow) into a new K-dimensional, sense-disambiguated
embedding vector for the focus word. We will evaluate
and optimise the final model’s learned representations by
evaluating language model perplexity on held out data.
Model architecture and internal representation:
Deep models derive their modelling power from their hi-
erarchical structure. Each layer transforms the output
representation of its previous layer, allowing the model
to learn more general and abstract feature combinations
in the higher layers which are relevant for the current
task. The representations on the hidden layers serve
as transformed feature representations of the input data
for the output classifier. Enforcing sparsity on the hid-
den layers has been shown to produce stronger features
for certain tasks in vision (Coates et al., 2010). Ad-
ditionally, individual nodes might be highly correlated,
which can also reduce the performance of certain clas-
sifiers which make strong independence assumptions (for
instance naive Bayes). We propose to study the effect that
enforcing sparsity in the learned feature representations
has on task performance in NLP. Additionally, we pro-
pose to evaluate the effect that an even stronger training
objective – one that encourages statistical independence
between hidden nodes by learning factorial code repre-
sentations (Hochreiter and Schmidhuber, 1999) – has on
model performance.
Modelling structure in the output space: Tasks
in NLP mostly involve predicting labels which exhibit
highly regular structure. For instance, in part-of-speech
tagging, two determiners have a very low likelihood of
following directly on one another, e.g. “the the”. In or-
der to successfully model this phenomenon, a model must
take into account previous (and potentially future) predic-
tions when making the current prediction, e.g. as in hid-
den Markov models and conditional random fields. We
propose to include sequential dependencies in the output
labels and to compare this with including a convolutional
layer below the output layer, for predicting output labels
in complex NLP tasks such as coreference resolution and
event structure detection.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9997753">
Deep learning methods offer an attractive unsupervised
approach for extracting higher-level features from large
quantities of text data to be used for NLP tasks. However
current attempts at integrating these features into existing
NLP systems do not produce the desired performance im-
provements. We conjecture that this is due to a mismatch
between the learned word representations and the classi-
fiers used as a result of disjoint training schemes, and our
thesis roadmap suggests three key areas for overcoming
these limitations.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999097625">
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A
neural probabilistic language model. Journal of Machine
Learning Research, 3:1137–1155, March.
Y. Bengio. 2009. Learning deep architectures for ai. Founda-
tions and Trends ® in Machine Learning, 2(1):1–127.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and J.C.
Lai. 1992. Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
</reference>
<page confidence="0.973627">
52
</page>
<reference confidence="0.999896406779661">
A. Coates, H. Lee, and A.Y. Ng. 2010. An analysis of single-
layer networks in unsupervised feature learning. Ann Arbor,
1001:48109.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks with
multitask learning. In Proceedings of the 25th International
Conference on Machine Learning, pages 160–167. ACM.
R. Collobert. 2011. Deep learning for efficient discrimina-
tive parsing. In International Conference on Artificial In-
telligence and Statistics (AISTATS).
C. Cortes and V. Vapnik. 1995. Support-vector networks. Ma-
chine learning, 20(3):273–297.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245–288.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast learn-
ing algorithm for deep belief nets. Neural computation,
18(7):1527–1554.
S. Hochreiter and J. Schmidhuber. 1999. Feature extraction
through lococode. Neural Computation, 11(3):679–714.
A. Hyv¨arinen, J. Karhunen, and E. Oja. 2001. Independent
component analysis, volume 26. Wiley Interscience.
I.T. Jolliffe. 2002. Principal component analysis, volume 2.
Wiley Online Library.
J. Lafferty. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Pro-
ceedings of ICML, 2001.
S. Lloyd. 1982. Least squares quantization in pcm. IEEE
Transactions on Information Theory, 28(2):129–137.
C.D. Manning, H. Sch¨utze, and MITCogNet. 1999. Founda-
tions of statistical natural language processing, volume 999.
MIT Press.
A. Mnih and G. Hinton. 2007. Three new graphical models for
statistical language modelling. In Proceedings of the 24th
international conference on Machine learning, pages 641–
648. ACM.
A. Mnih and G.E. Hinton. 2009. A scalable hierarchical dis-
tributed language model. Advances in neural information
processing systems, 21:1081–1088.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural
network language model. In AISTATS05, pages 246–252.
F. Rosenblatt. 1957. The Perceptron, a Perceiving and Recog-
nizing Automaton Project Para. Cornell Aeronautical Labo-
ratory.
C.E. Shannon and W. Weaver. 1962. The mathematical theory
of communication, volume 19. University of Illinois Press
Urbana.
R. Socher, C.D. Manning, and A.Y. Ng. 2010. Learning contin-
uous phrase representations and syntactic parsing with recur-
sive neural networks. In Proceedings of the NIPS-2010 Deep
Learning and Unsupervised Feature Learning Workshop.
R. Socher, C.C. Lin, A.Y. Ng, and C.D. Manning. 2011. Pars-
ing natural scenes and natural language with recursive neural
networks. In Proceedings of the 26th International Confer-
ence on Machine Learning (ICML), volume 2, page 7.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word represen-
tations: A simple and general method for semi-supervised
learning. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages 384–394.
Association for Computational Linguistics.
</reference>
<page confidence="0.999351">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851898">
<title confidence="0.99995">Deep Unsupervised Feature Learning for Natural Language Processing</title>
<author confidence="0.991685">Stephan</author>
<affiliation confidence="0.918062">MIH Media Lab, Stellenbosch</affiliation>
<address confidence="0.934324">Stellenbosch, South</address>
<email confidence="0.984485">stephan@ml.sun.ac.za</email>
<abstract confidence="0.9992059375">Statistical natural language processing (NLP) builds models of language based on statistical features extracted from the input text. We investigate deep learning methods for unsupervised feature learning for NLP tasks. Recent results indicate that features learned using deep learning methods are not a silver bullet and do not always lead to improved results. In this work we hypothesise that this is the result of a disjoint training protocol which results in mismatched word representations and classifiers. We also hypothesise that modelling long-range dependencies in the input and (separately) in the output layers would further improve performance. We suggest methods for overcoming these limitations, which will form part of our final thesis work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="8304" citStr="Bengio et al., 2003" startWordPosition="1326" endWordPosition="1329">difficult tasks. In the following section we briefly review and discuss the merits and limitations of three of the current state of the art deep learning models for NLP. We then identify our primary research questions and introduce our proposed future work roadmap. 2 Current State-of-the-Art and Limitations Most work builds on the idea of a neural probabilistic language model (NPLM) where words are represented by learned real-valued embeddings, and a neural network combines word embeddings to predict the most likely next word. The first successful NPLM was introduced by Bengio et al. in 2003 (Bengio et al., 2003). 49 Historically, training and testing these models were slow, scaling linearly in the vocabulary size. However, several recent approaches have been proposed which overcome these limitations (Morin and Bengio, 2005), including the work by Collobert and Weston (2008) and Mnih and Hinton (2009) discussed next. 2.1 Collobert &amp; Weston (2008) Collobert and Weston (2008) present a discriminative, non-probabilistic, non-linear neural language model that can be scaled to train over billions of words since each training iteration only computes a loss gradient over a small stochastic sample of the trai</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
</authors>
<title>Learning deep architectures for ai. Foundations and Trends ®</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5840" citStr="Bengio, 2009" startWordPosition="933" endWordPosition="934">, mutual information (Shannon and Weaver, 1962), principal components analysis (PCA) (Jolliffe, 2002), and independent component analysis (ICA) (Hyv¨arinen et al., 2001). However, natural language has complex mappings from text to meaning, arguably involving higher-order correlations between words which these simpler methods struggle to model adequately. Advances in the “deep learning” community allow us to perform efficient unsupervised feature learning in highly complex and highdimensional input feature spaces, making it an attractive method for learning features in e.g. vision or language (Bengio, 2009). The standard deep learning approach is to learn lower-dimensional embeddings from the raw highdimensional2 input space X to lower dimensional (e.g. 50-dimensional) feature spaces in an unsupervised manner, via repeated, layer-wise, non-linear transformation of the input features, e.g. y = f(k)(··· f(2)(f(1)(x)) ···) � where f(i)(x) is some non-linear function (typically tanh) for which the parameters are learned by back propagating error gradients. This configuration is referred to as a “deep” architecture with k layers (see Figure 1 for an example). For feature generation, we present a trai</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Y. Bengio. 2009. Learning deep architectures for ai. Foundations and Trends ® in Machine Learning, 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V Desouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="5227" citStr="Brown et al., 1992" startWordPosition="842" endWordPosition="845"> manually designing and extracting discriminating features from the data to be used in the learning process is a costly procedure requiring significant levels of domain expertise. Over the last two decades, the growth of available unlabeled data x E X and the ubiquity of scalable computing power has shifted research focus to unsupervised approaches for automatically learning appropriate feature representations O(x) from large collections of unlabeled text. Several methods have been proposed for unsupervised feature learning, including simple k-means clustering (Lloyd, 1982), Brown clustering (Brown et al., 1992), mutual information (Shannon and Weaver, 1962), principal components analysis (PCA) (Jolliffe, 2002), and independent component analysis (ICA) (Hyv¨arinen et al., 2001). However, natural language has complex mappings from text to meaning, arguably involving higher-order correlations between words which these simpler methods struggle to model adequately. Advances in the “deep learning” community allow us to perform efficient unsupervised feature learning in highly complex and highdimensional input feature spaces, making it an attractive method for learning features in e.g. vision or language (</context>
<context position="14024" citStr="Brown et al., 1992" startWordPosition="2231" endWordPosition="2234">our words to be combined into their phrasal equivalents in a bottom-up process. The LBL model, despite its very simple linear structure, provides very good performance in terms of model 50 perplexity, but shares the problem of slow training and testing times and the inability to handle word senses or dependencies between words (outside its n-gram context). In the HLBL model, Mnih and Hinton address the slow testing performance of the LBL model by using a hierarchical search tree over the vocabulary to exponentially speed up testing time, analogous to the concept of classbased language models (Brown et al., 1992). The HLBL model can also handle multiple word senses, but in their evaluation they show that in practice the model learns multiple senses (codes) for infrequently observed words instead of words with more than one meaning (Mnih and Hinton, 2009). The performance is strongly dependent on the initialisation of the tree, for which they present an iterative but non-optimal bootstrap-and-train procedure. Despite being non-optimal, it is shown to outperform the standard LBL model in terms of perplexity. 3 Mismatched Word Representations and Classifiers The deep learning ideal is to train deep, non-</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and J.C. Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Coates</author>
<author>H Lee</author>
<author>A Y Ng</author>
</authors>
<title>An analysis of singlelayer networks in unsupervised feature learning.</title>
<date>2010</date>
<journal>Ann Arbor,</journal>
<pages>1001--48109</pages>
<contexts>
<context position="22015" citStr="Coates et al., 2010" startWordPosition="3478" endWordPosition="3481">exity on held out data. Model architecture and internal representation: Deep models derive their modelling power from their hierarchical structure. Each layer transforms the output representation of its previous layer, allowing the model to learn more general and abstract feature combinations in the higher layers which are relevant for the current task. The representations on the hidden layers serve as transformed feature representations of the input data for the output classifier. Enforcing sparsity on the hidden layers has been shown to produce stronger features for certain tasks in vision (Coates et al., 2010). Additionally, individual nodes might be highly correlated, which can also reduce the performance of certain classifiers which make strong independence assumptions (for instance naive Bayes). We propose to study the effect that enforcing sparsity in the learned feature representations has on task performance in NLP. Additionally, we propose to evaluate the effect that an even stronger training objective – one that encourages statistical independence between hidden nodes by learning factorial code representations (Hochreiter and Schmidhuber, 1999) – has on model performance. Modelling structur</context>
</contexts>
<marker>Coates, Lee, Ng, 2010</marker>
<rawString>A. Coates, H. Lee, and A.Y. Ng. 2010. An analysis of singlelayer networks in unsupervised feature learning. Ann Arbor, 1001:48109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8571" citStr="Collobert and Weston (2008)" startWordPosition="1364" endWordPosition="1367">dmap. 2 Current State-of-the-Art and Limitations Most work builds on the idea of a neural probabilistic language model (NPLM) where words are represented by learned real-valued embeddings, and a neural network combines word embeddings to predict the most likely next word. The first successful NPLM was introduced by Bengio et al. in 2003 (Bengio et al., 2003). 49 Historically, training and testing these models were slow, scaling linearly in the vocabulary size. However, several recent approaches have been proposed which overcome these limitations (Morin and Bengio, 2005), including the work by Collobert and Weston (2008) and Mnih and Hinton (2009) discussed next. 2.1 Collobert &amp; Weston (2008) Collobert and Weston (2008) present a discriminative, non-probabilistic, non-linear neural language model that can be scaled to train over billions of words since each training iteration only computes a loss gradient over a small stochastic sample of the training data. All K-dimensional word embeddings are initially set to a random state. During each training iteration, an ngram is read from the training data and each word is mapped to its respective embedding. All embeddings are then concatenated to form a nK-length pos</context>
<context position="15327" citStr="Collobert and Weston, 2008" startWordPosition="2420" endWordPosition="2423">odels to automatically extract information-rich, higher-level features3 to integrate into standard NLP or image processing systems as added features to improve performance. However, several recent papers report surprising and seemingly contradicting results for this ideal. In the most direct comparison for NLP, Turian (2010) compares features extracted using Brown clustering (a hierarchical clustering technique for clustering words based on their observed co-occurrence patterns), the hierarchical log-bilinear (HLBL) embeddings (Mnih and Hinton, 2007) and Collobert and Weston (C+W) embeddings (Collobert and Weston, 2008), by integrating these as additional features in standard supervised conditional random field (CRF) classification systems for NLP. Somewhat surprisingly, they find that using the more complex C+W and HLBL features do not improve significantly over Brown features. Indeed, under several conditions the Brown features give the best results. These results are important for several reasons (we highlight these results in Table 2). The goal was to improve classification performance in structured prediction tasks in natural language by integrating features learned in a deep, unsupervised approach with</context>
<context position="8644" citStr="Collobert &amp; Weston (2008)" startWordPosition="1376" endWordPosition="1379">ea of a neural probabilistic language model (NPLM) where words are represented by learned real-valued embeddings, and a neural network combines word embeddings to predict the most likely next word. The first successful NPLM was introduced by Bengio et al. in 2003 (Bengio et al., 2003). 49 Historically, training and testing these models were slow, scaling linearly in the vocabulary size. However, several recent approaches have been proposed which overcome these limitations (Morin and Bengio, 2005), including the work by Collobert and Weston (2008) and Mnih and Hinton (2009) discussed next. 2.1 Collobert &amp; Weston (2008) Collobert and Weston (2008) present a discriminative, non-probabilistic, non-linear neural language model that can be scaled to train over billions of words since each training iteration only computes a loss gradient over a small stochastic sample of the training data. All K-dimensional word embeddings are initially set to a random state. During each training iteration, an ngram is read from the training data and each word is mapped to its respective embedding. All embeddings are then concatenated to form a nK-length positive training vector. A corrupted n-gram is also created by replacing th</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics (AISTATS).</booktitle>
<contexts>
<context position="7409" citStr="Collobert, 2011" startWordPosition="1182" endWordPosition="1183"> transformation W and a non-linearity. Each subsequent hidden layer hk takes as input the output of its preceding layer h(k−1) (Bengio, 2009). input layer. After performing one iteration of forwardpropagation through the network, we can then view the activation values in the hidden layers as dense, so-called “distributed representations” (features) of the input data. These features can in turn be passed to an output classifier layer to produce some tagging task of interest. Recent work in deep learning show state-of-the-art results in part-of-speech parsing, chunking and named-entity tagging (Collobert, 2011), however performance in more complex NLP tasks like entity and event disambiguation and semantic role labelling are still trailing behind. In this work we focus specifically on extending current state of the art deep neural models to improve their performance on these more difficult tasks. In the following section we briefly review and discuss the merits and limitations of three of the current state of the art deep learning models for NLP. We then identify our primary research questions and introduce our proposed future work roadmap. 2 Current State-of-the-Art and Limitations Most work builds</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>R. Collobert. 2011. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics (AISTATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="3606" citStr="Cortes and Vapnik, 1995" startWordPosition="599" endWordPosition="603">h as noun or verb phrases (NPs or VPs). Each word is assigned one unique tag often encoded using the BIO encoding1. We represent the input text as a vector of words xi E x, and each word’s corresponding label is represented by yi E y� (see Table 1). Given a feature generating function O(xi) and a set of labelled training pairs (xi, yi) E X x Y, the task then reduces to learning a suitable mapping h : O(X) → Y. Most previous works have focused on manually engineered features and simpler, linear models, including “shallow” model architectures, like the perceptron (Rosenblatt, 1957), linear SVM (Cortes and Vapnik, 1995) and linear-chain conditional random fields (CRFs) (Lafferty, 2001). However, a shallow learning architecture is only as good as its input features. Due to the complex nature of NL, deeper architectures may be re1E.g. B-NP means “begin NP”, I-NP means “inside NP”, and O means other/outside. Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 48–53, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics quired to learn data representations which contain the appropriate level of information for the task at hand. Prior to 2006, it was computationally in</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2828" citStr="Gildea and Jurafsky, 2002" startWordPosition="460" endWordPosition="464">e cat] NP [sits] yP [on] O [the mat] NP Table 1: Example NLP syntactic chunking task for the sentence “the cat sits on the mat”. X represents the words in the input space, Y represents labels in the output space. O(x) is a feature representation for the input text x and the bottom row represents the output named entity tags in a more standard form. is generally that features represent strong discriminating characteristics of the problem gained through manual engineering and domain-specific insight. As a concrete example, consider the task of syntactic chunking, also called “shallow parsing”, (Gildea and Jurafsky, 2002): Given an input string, e.g. “the cat sits on the mat”, the chunking problem consists of labelling segments of a sentence with syntactic constituents such as noun or verb phrases (NPs or VPs). Each word is assigned one unique tag often encoded using the BIO encoding1. We represent the input text as a vector of words xi E x, and each word’s corresponding label is represented by yi E y� (see Table 1). Given a feature generating function O(xi) and a set of labelled training pairs (xi, yi) E X x Y, the task then reduces to learning a suitable mapping h : O(X) → Y. Most previous works have focused</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>S Osindero</author>
<author>Y W Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<booktitle>Neural computation,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="18251" citStr="Hinton et al., 2006" startWordPosition="2886" endWordPosition="2889">se within some learning framework, since the output of the feature learning function O (and each deep layer O(k−1)) maps to a region in feature code space which becomes in turn the input to the output classifier h&apos; (or subsequent layer O(k)) . We therefore argue that in a semi-supervised or unsupervised classification problem, the feature learning function O should be strongly dependent on the classifier h&apos; that interprets those features, and vice versa. This notion ties in with the standard deep-learning training protocol of unsupervised pre-training followed by joint supervised fine-tuning (Hinton et al., 2006) of the top classification layer and the deeper feature extraction layers. We conjecture that jointly training a deep feature extraction model with a linear output classifier leads to better linearly separable feature vectors O(x) than training both independently. Note that this is in contrast to how Turian (2010) integrated the unsupervised features into existing NLP systems via disjoint training. 4 Proposed Work and Research Questions For simpler sequence tagging tasks such as part-ofspeech tagging and noun phrase chunking, the stateof-the-art models introduced in Section 2 perform adequatel</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hochreiter</author>
<author>J Schmidhuber</author>
</authors>
<title>Feature extraction through lococode.</title>
<date>1999</date>
<journal>Neural Computation,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="22568" citStr="Hochreiter and Schmidhuber, 1999" startWordPosition="3558" endWordPosition="3561">n to produce stronger features for certain tasks in vision (Coates et al., 2010). Additionally, individual nodes might be highly correlated, which can also reduce the performance of certain classifiers which make strong independence assumptions (for instance naive Bayes). We propose to study the effect that enforcing sparsity in the learned feature representations has on task performance in NLP. Additionally, we propose to evaluate the effect that an even stronger training objective – one that encourages statistical independence between hidden nodes by learning factorial code representations (Hochreiter and Schmidhuber, 1999) – has on model performance. Modelling structure in the output space: Tasks in NLP mostly involve predicting labels which exhibit highly regular structure. For instance, in part-of-speech tagging, two determiners have a very low likelihood of following directly on one another, e.g. “the the”. In order to successfully model this phenomenon, a model must take into account previous (and potentially future) predictions when making the current prediction, e.g. as in hidden Markov models and conditional random fields. We propose to include sequential dependencies in the output labels and to compare </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1999</marker>
<rawString>S. Hochreiter and J. Schmidhuber. 1999. Feature extraction through lococode. Neural Computation, 11(3):679–714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hyv¨arinen</author>
<author>J Karhunen</author>
<author>E Oja</author>
</authors>
<title>Independent component analysis, volume 26. Wiley Interscience.</title>
<date>2001</date>
<marker>Hyv¨arinen, Karhunen, Oja, 2001</marker>
<rawString>A. Hyv¨arinen, J. Karhunen, and E. Oja. 2001. Independent component analysis, volume 26. Wiley Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I T Jolliffe</author>
</authors>
<title>Principal component analysis,</title>
<date>2002</date>
<volume>2</volume>
<publisher>Wiley Online Library.</publisher>
<contexts>
<context position="5328" citStr="Jolliffe, 2002" startWordPosition="857" endWordPosition="858">ss is a costly procedure requiring significant levels of domain expertise. Over the last two decades, the growth of available unlabeled data x E X and the ubiquity of scalable computing power has shifted research focus to unsupervised approaches for automatically learning appropriate feature representations O(x) from large collections of unlabeled text. Several methods have been proposed for unsupervised feature learning, including simple k-means clustering (Lloyd, 1982), Brown clustering (Brown et al., 1992), mutual information (Shannon and Weaver, 1962), principal components analysis (PCA) (Jolliffe, 2002), and independent component analysis (ICA) (Hyv¨arinen et al., 2001). However, natural language has complex mappings from text to meaning, arguably involving higher-order correlations between words which these simpler methods struggle to model adequately. Advances in the “deep learning” community allow us to perform efficient unsupervised feature learning in highly complex and highdimensional input feature spaces, making it an attractive method for learning features in e.g. vision or language (Bengio, 2009). The standard deep learning approach is to learn lower-dimensional embeddings from the </context>
</contexts>
<marker>Jolliffe, 2002</marker>
<rawString>I.T. Jolliffe. 2002. Principal component analysis, volume 2. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<contexts>
<context position="3673" citStr="Lafferty, 2001" startWordPosition="610" endWordPosition="611"> often encoded using the BIO encoding1. We represent the input text as a vector of words xi E x, and each word’s corresponding label is represented by yi E y� (see Table 1). Given a feature generating function O(xi) and a set of labelled training pairs (xi, yi) E X x Y, the task then reduces to learning a suitable mapping h : O(X) → Y. Most previous works have focused on manually engineered features and simpler, linear models, including “shallow” model architectures, like the perceptron (Rosenblatt, 1957), linear SVM (Cortes and Vapnik, 1995) and linear-chain conditional random fields (CRFs) (Lafferty, 2001). However, a shallow learning architecture is only as good as its input features. Due to the complex nature of NL, deeper architectures may be re1E.g. B-NP means “begin NP”, I-NP means “inside NP”, and O means other/outside. Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 48–53, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics quired to learn data representations which contain the appropriate level of information for the task at hand. Prior to 2006, it was computationally infeasible to perform inference in hierarchical (“deep”), non-linear </context>
</contexts>
<marker>Lafferty, 2001</marker>
<rawString>J. Lafferty. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lloyd</author>
</authors>
<title>Least squares quantization in pcm.</title>
<date>1982</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="5188" citStr="Lloyd, 1982" startWordPosition="838" endWordPosition="839"> labourintensive and costly, and manually designing and extracting discriminating features from the data to be used in the learning process is a costly procedure requiring significant levels of domain expertise. Over the last two decades, the growth of available unlabeled data x E X and the ubiquity of scalable computing power has shifted research focus to unsupervised approaches for automatically learning appropriate feature representations O(x) from large collections of unlabeled text. Several methods have been proposed for unsupervised feature learning, including simple k-means clustering (Lloyd, 1982), Brown clustering (Brown et al., 1992), mutual information (Shannon and Weaver, 1962), principal components analysis (PCA) (Jolliffe, 2002), and independent component analysis (ICA) (Hyv¨arinen et al., 2001). However, natural language has complex mappings from text to meaning, arguably involving higher-order correlations between words which these simpler methods struggle to model adequately. Advances in the “deep learning” community allow us to perform efficient unsupervised feature learning in highly complex and highdimensional input feature spaces, making it an attractive method for learnin</context>
</contexts>
<marker>Lloyd, 1982</marker>
<rawString>S. Lloyd. 1982. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2):129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
<author>MITCogNet</author>
</authors>
<title>Foundations of statistical natural language processing, volume 999.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, MITCogNet, 1999</marker>
<rawString>C.D. Manning, H. Sch¨utze, and MITCogNet. 1999. Foundations of statistical natural language processing, volume 999. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9641" citStr="Mnih and Hinton (2007)" startWordPosition="1533" endWordPosition="1536">n ngram is read from the training data and each word is mapped to its respective embedding. All embeddings are then concatenated to form a nK-length positive training vector. A corrupted n-gram is also created by replacing the n’th (last) word by some word uniformly chosen from the vocabulary. The training criterion is that the network must predict positive training vectors with a score at least some margin higher than the score predicted for corrupted n-grams. Model parameters are trained simultaneously with word embeddings via gradient descent. 2.2 The Hierarchical Log Bilinear (HLBL) Model Mnih and Hinton (2007) proposed a simple probabilistic linear neural language model called the log bilinear (LBL) model. For an n-gram context window, the LBL model concatenates the first (n −1) K-dimensional word embeddings and then learns a linear mapping from R(n−1)K to RK for predicting the embedding of the nth word. For predicting the next word, the model outputs a probability distribution over the entire vocabulary by computing the dot product between the predicted embedding and the embedding for each word in the vocabulary in an output softmax layer. This softmax computation is linear in the length of the vo</context>
<context position="15256" citStr="Mnih and Hinton, 2007" startWordPosition="2409" endWordPosition="2412">els over large collections of unlabeled data, and then use these models to automatically extract information-rich, higher-level features3 to integrate into standard NLP or image processing systems as added features to improve performance. However, several recent papers report surprising and seemingly contradicting results for this ideal. In the most direct comparison for NLP, Turian (2010) compares features extracted using Brown clustering (a hierarchical clustering technique for clustering words based on their observed co-occurrence patterns), the hierarchical log-bilinear (HLBL) embeddings (Mnih and Hinton, 2007) and Collobert and Weston (C+W) embeddings (Collobert and Weston, 2008), by integrating these as additional features in standard supervised conditional random field (CRF) classification systems for NLP. Somewhat surprisingly, they find that using the more complex C+W and HLBL features do not improve significantly over Brown features. Indeed, under several conditions the Brown features give the best results. These results are important for several reasons (we highlight these results in Table 2). The goal was to improve classification performance in structured prediction tasks in natural languag</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>A. Mnih and G. Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641– 648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model. Advances in neural information processing systems,</title>
<date>2009</date>
<pages>21--1081</pages>
<contexts>
<context position="8598" citStr="Mnih and Hinton (2009)" startWordPosition="1369" endWordPosition="1372"> and Limitations Most work builds on the idea of a neural probabilistic language model (NPLM) where words are represented by learned real-valued embeddings, and a neural network combines word embeddings to predict the most likely next word. The first successful NPLM was introduced by Bengio et al. in 2003 (Bengio et al., 2003). 49 Historically, training and testing these models were slow, scaling linearly in the vocabulary size. However, several recent approaches have been proposed which overcome these limitations (Morin and Bengio, 2005), including the work by Collobert and Weston (2008) and Mnih and Hinton (2009) discussed next. 2.1 Collobert &amp; Weston (2008) Collobert and Weston (2008) present a discriminative, non-probabilistic, non-linear neural language model that can be scaled to train over billions of words since each training iteration only computes a loss gradient over a small stochastic sample of the training data. All K-dimensional word embeddings are initially set to a random state. During each training iteration, an ngram is read from the training data and each word is mapped to its respective embedding. All embeddings are then concatenated to form a nK-length positive training vector. A co</context>
<context position="10357" citStr="Mnih and Hinton (2009)" startWordPosition="1649" endWordPosition="1652">del. For an n-gram context window, the LBL model concatenates the first (n −1) K-dimensional word embeddings and then learns a linear mapping from R(n−1)K to RK for predicting the embedding of the nth word. For predicting the next word, the model outputs a probability distribution over the entire vocabulary by computing the dot product between the predicted embedding and the embedding for each word in the vocabulary in an output softmax layer. This softmax computation is linear in the length of the vocabulary for each prediction, and is therefore the performance bottleneck. In follow-up work, Mnih and Hinton (2009) speed up training and testing time by extending the LBL model to predict the next word by hierarchically decomposing the search through the vocabulary by traversing a binary tree constructed over the vocabulary. This speeds up training and testing exponentially, but initially reduces model performance as the construction of the binary partitioning has a strong effect on the model’s performance. They introduce a method for bootstrapping the construction of the tree by initially using a random binary tree to learn word embeddings, and then rebuilding the tree based on a clustering of the learne</context>
<context position="14270" citStr="Mnih and Hinton, 2009" startWordPosition="2271" endWordPosition="2274">d testing times and the inability to handle word senses or dependencies between words (outside its n-gram context). In the HLBL model, Mnih and Hinton address the slow testing performance of the LBL model by using a hierarchical search tree over the vocabulary to exponentially speed up testing time, analogous to the concept of classbased language models (Brown et al., 1992). The HLBL model can also handle multiple word senses, but in their evaluation they show that in practice the model learns multiple senses (codes) for infrequently observed words instead of words with more than one meaning (Mnih and Hinton, 2009). The performance is strongly dependent on the initialisation of the tree, for which they present an iterative but non-optimal bootstrap-and-train procedure. Despite being non-optimal, it is shown to outperform the standard LBL model in terms of perplexity. 3 Mismatched Word Representations and Classifiers The deep learning ideal is to train deep, non-linear models over large collections of unlabeled data, and then use these models to automatically extract information-rich, higher-level features3 to integrate into standard NLP or image processing systems as added features to improve performanc</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>A. Mnih and G.E. Hinton. 2009. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morin</author>
<author>Y Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In AISTATS05,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="8520" citStr="Morin and Bengio, 2005" startWordPosition="1356" endWordPosition="1359">ions and introduce our proposed future work roadmap. 2 Current State-of-the-Art and Limitations Most work builds on the idea of a neural probabilistic language model (NPLM) where words are represented by learned real-valued embeddings, and a neural network combines word embeddings to predict the most likely next word. The first successful NPLM was introduced by Bengio et al. in 2003 (Bengio et al., 2003). 49 Historically, training and testing these models were slow, scaling linearly in the vocabulary size. However, several recent approaches have been proposed which overcome these limitations (Morin and Bengio, 2005), including the work by Collobert and Weston (2008) and Mnih and Hinton (2009) discussed next. 2.1 Collobert &amp; Weston (2008) Collobert and Weston (2008) present a discriminative, non-probabilistic, non-linear neural language model that can be scaled to train over billions of words since each training iteration only computes a loss gradient over a small stochastic sample of the training data. All K-dimensional word embeddings are initially set to a random state. During each training iteration, an ngram is read from the training data and each word is mapped to its respective embedding. All embed</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural network language model. In AISTATS05, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosenblatt</author>
</authors>
<title>The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory.</title>
<date>1957</date>
<contexts>
<context position="3568" citStr="Rosenblatt, 1957" startWordPosition="595" endWordPosition="596">with syntactic constituents such as noun or verb phrases (NPs or VPs). Each word is assigned one unique tag often encoded using the BIO encoding1. We represent the input text as a vector of words xi E x, and each word’s corresponding label is represented by yi E y� (see Table 1). Given a feature generating function O(xi) and a set of labelled training pairs (xi, yi) E X x Y, the task then reduces to learning a suitable mapping h : O(X) → Y. Most previous works have focused on manually engineered features and simpler, linear models, including “shallow” model architectures, like the perceptron (Rosenblatt, 1957), linear SVM (Cortes and Vapnik, 1995) and linear-chain conditional random fields (CRFs) (Lafferty, 2001). However, a shallow learning architecture is only as good as its input features. Due to the complex nature of NL, deeper architectures may be re1E.g. B-NP means “begin NP”, I-NP means “inside NP”, and O means other/outside. Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 48–53, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics quired to learn data representations which contain the appropriate level of information for the task at hand. Pr</context>
</contexts>
<marker>Rosenblatt, 1957</marker>
<rawString>F. Rosenblatt. 1957. The Perceptron, a Perceiving and Recognizing Automaton Project Para. Cornell Aeronautical Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
<author>W Weaver</author>
</authors>
<title>The mathematical theory of communication,</title>
<date>1962</date>
<volume>19</volume>
<institution>University of Illinois Press Urbana.</institution>
<contexts>
<context position="5274" citStr="Shannon and Weaver, 1962" startWordPosition="848" endWordPosition="851">minating features from the data to be used in the learning process is a costly procedure requiring significant levels of domain expertise. Over the last two decades, the growth of available unlabeled data x E X and the ubiquity of scalable computing power has shifted research focus to unsupervised approaches for automatically learning appropriate feature representations O(x) from large collections of unlabeled text. Several methods have been proposed for unsupervised feature learning, including simple k-means clustering (Lloyd, 1982), Brown clustering (Brown et al., 1992), mutual information (Shannon and Weaver, 1962), principal components analysis (PCA) (Jolliffe, 2002), and independent component analysis (ICA) (Hyv¨arinen et al., 2001). However, natural language has complex mappings from text to meaning, arguably involving higher-order correlations between words which these simpler methods struggle to model adequately. Advances in the “deep learning” community allow us to perform efficient unsupervised feature learning in highly complex and highdimensional input feature spaces, making it an attractive method for learning features in e.g. vision or language (Bengio, 2009). The standard deep learning appro</context>
</contexts>
<marker>Shannon, Weaver, 1962</marker>
<rawString>C.E. Shannon and W. Weaver. 1962. The mathematical theory of communication, volume 19. University of Illinois Press Urbana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>R. Socher, C.D. Manning, and A.Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C C Lin</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML),</booktitle>
<volume>2</volume>
<pages>7</pages>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>R. Socher, C.C. Lin, A.Y. Ng, and C.D. Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 26th International Conference on Machine Learning (ICML), volume 2, page 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>