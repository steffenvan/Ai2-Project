<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.997615666666667">
Analysis of Unknown Words
through
Morphological Decomposition
</title>
<author confidence="0.993443">
Alan W Black
</author>
<affiliation confidence="0.9967835">
Dept of Artificial Intelligence,
University of Edinburgh
</affiliation>
<address confidence="0.796527">
80 South Bridge,
Edinburgh EH1 1HN
Scotland, UK.
</address>
<bodyText confidence="0.156288333333333">
ewbCed.ac . uk
Joke van de Plassche
MCI,
</bodyText>
<affiliation confidence="0.786966">
University of Nijmegen,
</affiliation>
<address confidence="0.392356">
Montessorilaan 3,
</address>
<note confidence="0.641389666666667">
6525 FIR Nijmegen
The Netherlands
PLASSCHECkunpvl.psych.kun.n1
</note>
<author confidence="0.959692">
Briony Williams
</author>
<affiliation confidence="0.997326">
Centre for Speech Technology
University of Edinburgh
</affiliation>
<address confidence="0.950173666666667">
80 South Bridge,
Edinburgh EH1 1HN
Scotland, UK.
</address>
<email confidence="0.880744">
brionyftstr.ed.ac.uk
</email>
<sectionHeader confidence="0.99441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935">
This paper describes a method of analysing
words through morphological decomposition
when the lexicon is incomplete. The method is
used within a text-to-speech system to help gen-
erate pronunciations of unknown words. The
method is achieved within a general morpho-
logical analyser system using Koskenniemi two-
level rules.
</bodyText>
<keyword confidence="0.723893">
Keywords: Morphology, incomplete lexicon,
text-to-speech systems
</keyword>
<sectionHeader confidence="0.811163" genericHeader="categories and subject descriptors">
Background
</sectionHeader>
<subsectionHeader confidence="0.663372">
When a text-to-speech synthesis system is used,
</subsectionHeader>
<bodyText confidence="0.993124587837839">
it is likely that the text being processed will
contain a few words which do not appear in
the lexicon as entries in their own right. If
the lexicon consists only of whole-word entries,
then the method for producing a pronunciation
for such &amp;quot;unknown&amp;quot; words is simply to pass
them through a set of letter-to-sound rules fol-
lowed by word stress assignment rules and vowel
reduction rules. The resulting pronunciation
may well be inaccurate, particularly in English
(which often shows a poor relationship between
spelling and pronunciation). In addition, the
default set of word classes assigned to the word
(noun, verb, adjective) will be too general to
be of much help to the syntactic parsing mod-
ule. However, if the lexicon contains individual
morphemes (both &amp;quot;bound* and &amp;quot;free&amp;quot;), an un-
known word can be analysed into its constituent
morphemes. Stress assignment rules will then
be more likely to yield the correct pronuncia-
tion, and any characteristic suffix that may be
present will allow for the assignment of a more
accurate word class or classes (eg. +nests de-
notes a noun, +ly an adverb). Morphological
analysis of words will therefore allow a signifi-
cantly larger number of &amp;quot;unknown&amp;quot; words to be
handled. Novel forms such as hamperance, and
thatcheriaation would probably not exist in a
whole-word dictionary, but could be handled by
morphological analysis using existing morpho-
logical entries. Also, the ability to deal with
compound words would allow for significantly
higher accuracy in pronunciation assignment.
A problem arises, however, if one or more
of the word&apos;s constituent morphemes are not
present in the morphological dictionary. In this
case, the morphological analysis will fail, and
the entire word will be passed to the letter-to-
sound rules, with concomitant probable loss of
accuracy in pronunciation assignment and word
class assignment. It is far more likely that the
missing morpheme will be a root morpheme
rather than an affix, since the latter morphemes
form a closed class which may be exhaustively
listed, whereas the former form an open class
which may be added to as the language evolves
(eg. ninja, Chunnel, kluge, yomp). Therefore,
it would be preferable if any closed-class mor-
phemes in a (putatively) polymorphemic un-
- 101 -
known word could be recognised and separated
from the remaining material, which would then
be assumed to be a new root morpheme. Letter-
to-sound rules would then be applied to this pu-
tative new root morpheme (the pronunciation of
the known material would be derived from the
lexicon).
The advantages of this method are that the
pronunciation and word stress assignment are
more likely to be accurate, and also that, if
there is a suitable suffix, the correct word class
may be assigned (eg. in yomping, from yomp
(unknown root) and +ing (known verb or noun
suffix), which will be characterised as a verb
or noun). Thus, in the case of preamble, the
stripping of the prefix pre- will allow for the
correct pronunciation /p r ii ambe 1/: if
the entire word had been passed to the letter-to-
sound rules, the incorrect pronunciation /p r
ii m b C 1/ would have resulted. In addition
to affixes, known root morphemes could also be
stripped to leave the remaining unknown mate-
rial. For example, without morphological anal-
ysis, penthouse may be wrongly pronounced as
/p e n th au s/, with a voiceless dental frica-
tive.
It is known that letter-to-sound rules are
more accurate if they are not allowed to apply
across morpheme boundaries (see (1, Ch. 61),
and this method takes advantage of that fact.
Thus greater accuracy is obtained, for polymor-
phemic unknown words, if known morphs can
be stripped before the application of letter-to-
sound rules. It is this task that the work de-
scribed below attempts to carry out.
The Alvey Natural Language Tools Mor-
phological System ([5],[61), already provides a
comprehensive morphological analyser system.
This system allows morphological analysis of
words into morphemes based on user-defined
rules. The basic system does not offer analysis
of words containing unknown morphemes, nor
does it provide a rank ordering of the output
analyses. Both these latter features have been
added in the work described below.
The system consists of a two tier process:
first a morphological analysis, based on Kosken-
niemi&apos;s two-level morphology ([3[); secondly the
statement of morphosyntactic constraints (not
available in Koskenniemi&apos;s system) based on a
GPSG-like feature grammar.
The morphographemic rules are specified as
a set of high level rules (rather than directly
as finite state transducers) which describe the
relationship between a surface tape (the word)
and a lexical tape (the normalised lexical form).
These rules specify contexts for pairs of lexical
and surface characters. For example a rule
+:e
&lt; s:s h:h &gt; s:s x:x z:z y:i 1
--- 0:8
specifies that a surface character e must match
with a lexical character + when preceded by one
of oh, s, x, z or the pair y: (as in skies to
slcy+s), and succeeded by s. The &amp;quot;---&amp;quot; denotes
where the rule pair fits into the context. For ex-
ample the above rule would admit the following
match
lexical tape: box+s
surface tape:boxes
The exact syntax and interpretation is more
fully described in [5, Sect. 3] and [6, Ch. 2].
In addition to segmentation each lexical en-
try is associated with a syntactic category (rep-
resented as a feature structure). Grammar rules
can be written to specify which conjunctions of
morphemes are valid. Thus valid analyses re-
quire a valid segmentation and a valid morpho-
syntax. In the larger descriptions developed
in the system a “categorial grammar&amp;quot;-like ap-
proach has been used in the specification of af-
fixes. An affix itself will specify what category
it can attach (&amp;quot;apply&amp;quot;) to and what its resulting
category will be.
In the work described here, the basic mor-
phology system has been modified to analyse
words containing morphemes that are not in the
lexicon. The analysis method offers segmenta-
tion and morphological analysis (based on the
word grammar), which results in a list of pos-
sible analyses. An ordering on these possible
analyses has been defined, giving a most likely
analysis, for which the spelling of the unknown
morpheme can then be reconstructed using the
system&apos;s original morphographemic rules. Fi-
nally, the pronunciation of the unknown mor-
pheme can be assigned, using letter-to-sound
rules encoded as two-level rules.
</bodyText>
<subsectionHeader confidence="0.99834">
Analysis Method
</subsectionHeader>
<bodyText confidence="0.993399087719298">
The method used to analyse words containing
unknown substrings proceeds as follows. First,
four new morphemes are added to the lexicon,
one for each major morphologically productive
- 102 -
category (noun, verb, adjective and adverb).
Each has a citation form of **. The intention
is that the unknown part of a word will match
these entries. Thus we get two-level segmenta-
tion as follows
lexical tape: *000*+ing+s
surface tape: Opar0OingOs
The special character 0 represents the null sym-
bol (i.e. the surface form would be parings —
without the nulls). This matching is achieved
by adding two two-level morphological rules.
The first rule allows any character in the sur-
face alphabet to match null on the lexical tape,
but only in the context where the lexical nulls
are flanked by lexical asterisks matching with
surface nulls.
The second rule deals with constraining the
*:0 pairs themselves. It deals with two spe-
cific points. First, it ensures that there is only
one occurrence of ** in an analysis (i.e only one
unknown section). Second, it constrains the
unknown section. This is done in two ways.
Rather than simply allowing the unknown part
to be any arbitrary collection of letters, it is re-
stricted to ensure that if it starts with any of {h
j1mnqr vxy z}, then it is also followed
by a vowel. This (rightly) excludes the possibil-
ity of an unknown section starting with an un-
pronounceable consonant cluster e.g. computer
could not be analysed as co- input +er). Sec-
ond, it ensures that the unknown section is at
least two characters long and contains a vowel.
This excludes the analysis of resting as re-
st +ing.
These restrictions on the unknown section
are weak and more comprehensive restrictions
would help. They are attempts at characteris-
ing English morphemes in terms of the minimal
English syllable. A more complex characteriza-
tion, defining valid consonant clusters, vowels,
etc. would be possible in this formalism, and
the phonotactic constraints of English syllables
are well known. However, the resulting rules
would be clumsy and slow, and it was felt that,
at this stage, any small gain in accuracy would
be offset by a speed penalty.
The rules make use of sets of characters.
Anything is a set consisting of all surface char-
acters, BCDFGKPSTW and HJLMNQRVXYZ are sets
consisting of those letters, V is the set of vowels
and C the consonants. The character $ is used
to mark word boundaries.
</bodyText>
<equation confidence="0.955821818181818">
0:Anything &lt;=&gt;
{ *:0 &lt; *:0 (0:Anything)1+ &gt;
{ *:0 &lt; (0:Anything)1+ *:0 &gt;
*:0 &lt;=.&gt;
{ 0:$ &lt; 0:$ (=:=)1+ &gt; } ---
{ &lt; { 0:BCDFGKPSTW 0:V }
0:Anything &gt;
&lt; 0:HJLMNQRVXYZ 0:V &gt;
or { &lt; 0:C (0:V)1+ &gt;
&lt; 0:V (0:C)1+ &gt; ) - - -
&lt; (=:=)1+ 0:$ &gt; 0:$ 1
</equation>
<bodyText confidence="0.9996546">
The above rules are somewhat clumsily formu-
lated. This is partly due to the particular imple-
mentation used, which allows only one rule for
each surface:lexical pair&apos; and partly due to the
complexity of the phenomena being described.
</bodyText>
<subsectionHeader confidence="0.778733">
Word Grammar
</subsectionHeader>
<bodyText confidence="0.999906368421053">
Using the above two rules and adding the four
new lexical entries to a larger description, it is
now possible to segment words with one un-
known substring. Because the system encodes
constraints for affixes via feature specifications,
only morphosyntactically valid analyses will be
permitted. That is, although ** is ambiguous
in its category, if it is followed by +ed only the
analysis involving the verb will succeed. For ex-
ample, although the segmentation process could
segment bipeds ** +ed +s the word gram-
mar section would exclude this analysis, since
the +s suffix only follows uninflected verbs or
nouns.
However, there are a number of possible mis-
takes that can occur. When an unknown sec-
tion exists it may spuriously contain other mor-
phemes, leading to an incorrect analysis. For
example
</bodyText>
<equation confidence="0.876850333333333">
colour -&gt; co- **
readable -&gt; re- ** +able
cartoons -&gt; car ** +s (compound noun)
</equation>
<bodyText confidence="0.997779375">
In actual fact, when words are analysed by this
technique a large number of analyses is usually
found. The reasons for the large number are
as follows. Firstly, the assumed size of the un-
known part can vary for the same word, as in
the following:
&apos;Ritchie ([4]) shows that this is not a restriction on
the formal power of the rules.
</bodyText>
<equation confidence="0.816522">
- 103 -
entitled -&gt; **
entitled -&gt; ** +ed
entitled -&gt; en- ** +ed
entitled -&gt; en- **
</equation>
<bodyText confidence="0.999915083333333">
Secondly, because ** is four ways ambiguous,
there can be multiple analyses for the same sur-
face form. For example, a word ending in s
could be either a plural noun or a third person
singular verb.
These points can multiply together and of-
ten produce a large number of possible analyses.
Out of the test set of 200 words, based on a lex-
icon consisting of around 3500 morphemes (in-
cluding the ** entries), the average number of
analyses found was 9, with a maximum number
of 71 (for functional).
</bodyText>
<subsectionHeader confidence="0.910344">
Choosing an Analysis
</subsectionHeader>
<bodyText confidence="0.939212195652174">
In order to use these results in a text-to-speech
system, it is necessary to choose one possible
analysis, since a TTS system is deterministic.
To do this, the analyses are rank ordered. A
number of factors are exploited in the rank or-
dering:
- length of unknown root
- structural ordering rules ([1, Ch. 3])
- frequency of affix
Each of these factors will be described in turn.
When analysing a word containing an unknown
part, the best results are usually obtained by us-
ing the analysis with the shortest unknown part
(see [1, Ch. 6]). Thus the analysis of walkers
would be ordered as follows (most likely first):
** +er &gt; **+5 &gt; **
This heuristic will occasionally fail, as in beers
where the shortest unknown analysis is ** +er
+0. But the correct result will be obtained in
most cases.
The second ordering constraint is based on
the ordering rules used in [1]. Some words can
be segmented in many different ways (this is
true even if all parts are known). For example
scarcity -&gt; scar city
scarcity -&gt; scarce +ity
scarcity -&gt; scar cite +y
A simple rule notation has been defined for as-
signing order to analyses in terms of their mor-
phological parse tree. These rules can be sum-
marised as
prefixing &gt; suffixing &gt;
inflection &gt; compounding
The third method used for ordering is affix fre-
quency. The frequencies are based on suffix-as-
tag (word class) frequencies in the LOB corpus
of written English, given in [21. Thus the suffix
+er forming a noun from a verb (as in walker)
was marked in the lexicon as being more likely
than the adjectival comparative +er.
These constraints are applied simultane-
ously. Each rule has an appropriate weight-
ing, such that the length of the unknown part
is a more significant factor than morphological
structure, which in turn is more significant than
affix frequency.
</bodyText>
<sectionHeader confidence="0.916156" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.997920361111111">
The method was subjected to a test procedure.
The test used a basic lexicon of around 3500
morphemes, of which around 150 were affixes.
From a randomly selected Al magazine arti-
cle, the first 200 words were used which could
not be analysed by the basic morphological sys-
tem (i.e. without the unknown root section).
When these 200 words were analysed using the
method described in the previous sections, 133
words (67%) were analysed correctly, 48 words
(24%) were wrong due to segmentation error,
and 19 (9%) were wrong due to word class er-
ror. An analysis was deemed to be correct when
the most preferred analysis had both the correct
morphological structure and the correct word
class.
Segmentation errors were due mainly to spu-
rious words in sub-parts of unknown sections,
e.g. illustrate ill ** ate. Such errors
will increase as the lexicon grows. To prevent
this type of error, it may be necessary to place
restrictions on compounding, such that those
words which can form part of compounds should
be marked as such (though this is a major re-
search problem in itself). Word class errors
occurred where the correct segmentation was
found but an incorrect morphological structure
was assigned.
The definition of error used here may be
over-restrictive, as it may still be the case that
erroneous segmentation and structure errors
still provide analyses with the correct pronun-
ciation. But at this time the remainder of the
text-to-speech system is not advanced enough
for this to be adequately tested.
- 104
</bodyText>
<subsectionHeader confidence="0.997283">
Generating the Spelling of
Unknown Morphemes
</subsectionHeader>
<bodyText confidence="0.999967088235294">
A method has been described for handling a
word which cannot be analysed by the con-
ventional morphological analysis process. This
method may generate a number of analyses, so
an ordering of the results is defined. However,
in a text-to-speech system (or even an interac-
tive spelling corrector), it may be desirable to
add the unknown root to a user lexicon for fu-
ture reference. In such a case, it will be nec-
essary to reconstruct the underlying spelling of
the unknown morpheme.
This can be done in a very similar way to
that in which the system normally generates
surface forms from lexical forms. The problem
is the following: given a surface form and a set
of spelling rules (not including the two special
rules described above), define the set of possi-
ble lexical forms which can match to the surface
form. This, of course, would over-generate lex-
ical forms, but if the permitted lexical form is
further constrained so as to match the one given
from the analysis containing the ** a more sat-
isfactory result will be obtained.
For example, the surface form remoned
would be analysed as re-**+ed. A matching is
carried out character by character between the
lexical and surface forms, checking each match
with respect to the spelling rules (and hypothe-
sizing nulls where appropriate). On encounter-
ing the ** section of the lexical form, the pro-
cess attempts to match all possible lexical char-
acters with the surface form.. This is of course
still constrained by the spelling rules, so only a
few characters will match. What is significant
is that the minor orthographic changes that the
spelling rules describe will be respected. Thus
in this case the ** matches none (rather than
simply mon without an e), as the spelling rules
require there to be an e inserted before the +ed
in this case.
Similarly, given the surface string mogged,
analysed as **+ed, the root form mog is gener-
ated. However, the alternative forms mogg and
mogge are also generated. This is not incorrect,
as in similar cases such analyses are correct (eg.
egged and silhouetted respectively). As yet,
the method has no means of selecting between
these possibilities.
After the generation of possible ortho-
graphic forms, the letter-to-sound rules are ap-
plied. As regards the format of these rules, what
is required is something very similar to Kosken-
niemi two-level rules, relating graphemes to
phonemes in particular contexts. A small set of
grapheme to phoneme rules was written using
this notation. However, there were problems in
writing these rules, as the fuller set of rules from
which they were taken used the concept of rule
ordering, while the Koskenniemi rule interpre-
tation interprets all rules in parallel. The re-
sult was that the rewritten rules were more dif-
ficult both to read and to write. Although it is
possible (and even desirable) to use finite state
transducers in the run-time system, the current
Koskenniemi format may not be the best format
for letter-to-sound rules. Some other notation
which could compile to the same form would
make it easier to extend the ruleset.
</bodyText>
<subsectionHeader confidence="0.461396">
Problems
</subsectionHeader>
<bodyText confidence="0.997952621212121">
The technique described above largely depends
on the existence of an appropriate lexicon and
morphological analyser. The starting-point was
a fairly large lexicon (over 3000 morphemes)
and an analyser description, and the expecta-
tion was that only minor additions would be
needed to the system. However, it seems that
significantly better results will require more sig-
nificant changes.
Firstly, as the description used had a rich
morpho-syntax, words could be analysed in
many ways giving different syntactic markings
(eg. different number and person markings for
verbs) which were not relevant for the rest of
the system. Changes were made to reduce the
number of phonetically similar (though syntac-
tically different) analyses. The end result now
states only the major category of the analysis.
(Naturally, if the, system were to be used within
a more complex syntactic parser, the other anal-
yses may be needed).
Secondly, the number of &amp;quot;stem&amp;quot; entries in
the lexicon is significant. It must be large
enough to analyse most words, though not so
large that it gives too many erroneous anal-
yses of unknown words. Also, while it has
been assumed that the lexicon contains produc-
tive affixes, perhaps it should also contain cer-
tain derivational affixes which are not normally
productive, such as tele-, +ology, +phobia,
+vorous. These would be very useful when
analysing unknown words. The implication is
that there should be a special lexicon used for
- 105 -
analysing unknown words. This lexicon would
have a large number of affixes, together with
constraints on compounds, that would not nor-
mally be used when analysing words.
Another problem is that unknown words
are often place-names, proper names, loanwords
etc. The technique described here would prob-
ably not deal adequately with such words.
So far, this technique has been described
only in terms of English. When considering
other languages, especially those where com-
pounding is common (eg. Dutch and German),
the method would be even more advantageous.
In novel compounds, large sections of the word
could still be analysed. In the above descrip-
tion, only one unknown part is allowed in each
word. This seems to be reasonable for En-
glish, where there will rarely be compounds of
the form ** +suf ** +suf. However, in other
languages (especially those with a more fully-
developed system of inflection) such structures
do exist. An example is the Dutch word be-
jaardentehuizen (old peoples homes), which has
the structure noun +en noun +en. Thus it is
possible for words to contain two (or more)
non-contiguous unknown sections. The method
described here could probably cope with such
cases in principle, but the current implemen-
tation does not do so. Instead, it would find
one unknown part from the start of the first
unknown morpheme to the end of the final un-
known morpheme.
</bodyText>
<sectionHeader confidence="0.973348" genericHeader="conclusions">
Summary
</sectionHeader>
<bodyText confidence="0.999961">
A system has been described which will analyse
any word and assign a pronunciation. The sys-
tem first tries to analyse an input word using
the standard analysis procedure. If this fails,
the modified lexicon and spelling rule set are
used. The output analyses are then ordered.
For each unknown section, the underlying or-
thographic form is constructed, and letter-to-
sound rules are applied. The end result is a
string of phonemic forms, one form for each
morpheme in the original word. These phone-
mic forms are then processed by morphophono-
logical rules, followed by rules for word stress
assignment and vowel reduction.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.954043888888889">
Alan Black is currently funded by an SERC
studentship (number 89313458). During this
project Joke van de Plassche was funded by
the SED and Stichting Nijmeegs Universiteits
Fonds. Briony Williams is employed on the ES-
PRIT &amp;quot;POLYGLOT&amp;quot; project. We should
also like to acknowledge help and ideas from
Gerard Kempen, Fransiska Maier, Helen Pain,
Graeme Ritchie and Alex Zbyslaw.
</reference>
<sectionHeader confidence="0.964092" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98888637037037">
111 J. Allen, M. Hunnicut, and K. Klatt. Text-
to-speech: The MITalk system. Cambridge
University Press, Cambridge, UK., 1987.
[21 S. Johansson and M. Jahr. Grammatical
tagging of the LOB corpus: predicting word
class from word endings. In S. Johansson,
editor, Computer corpora in English lan-
guage research, Norwegian Computing Cen-
tre for the Humanities, Bergen, 1982.
(31 K. Koskenniemi. A general computational
model for word-form recognition and pro-
duction. In Proceedings of the 10th Inter-
national Conference on Computational Lin-
guistics, pages 178-181, Stanford University,
California, 1984.
[41 G. Ritchie. Languages Generated by Two-
level Morphological Rules. Research Pa-
per 496, Dept of AI, University of Edin-
burgh, 1991.
151 G. Ritchie, S. Pulman, A. Black, and
G. Russell. A computational framework for
lexical description. Computational Linguis-
tics, 13(3-4):290-307, 1987.
161 G. Ritchie, G. Russell, A. Black, and S. Pul-
man. Computational Morphology. MIT
Press, Cambrdige, Mass., forthcomming.
- 106 -
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.981145333333333">Analysis of Unknown Words through Morphological Decomposition</title>
<author confidence="0.999881">Alan W Black</author>
<affiliation confidence="0.9995295">Dept of Artificial Intelligence, University of Edinburgh</affiliation>
<address confidence="0.898489333333333">80 South Bridge, EH1 1HN Scotland, UK.</address>
<author confidence="0.508122">uk Joke van_de_Plassche</author>
<affiliation confidence="0.978955">MCI, University of Nijmegen,</affiliation>
<address confidence="0.993142333333333">Montessorilaan 3, 6525 FIR Nijmegen The Netherlands</address>
<email confidence="0.376427">PLASSCHECkunpvl.psych.kun.n1</email>
<affiliation confidence="0.820672333333333">Briony Williams Speech Technology University of Edinburgh</affiliation>
<address confidence="0.941219">80 South Bridge, Edinburgh EH1 1HN Scotland, UK.</address>
<email confidence="0.998725">brionyftstr.ed.ac.uk</email>
<abstract confidence="0.999750222222222">This paper describes a method of analysing words through morphological decomposition when the lexicon is incomplete. The method is used within a text-to-speech system to help generate pronunciations of unknown words. The method is achieved within a general morphological analyser system using Koskenniemi twolevel rules.</abstract>
<keyword confidence="0.828483">Keywords: Morphology, incomplete lexicon,</keyword>
<abstract confidence="0.993329363281251">text-to-speech systems Background a text-to-speech synthesis system used, it is likely that the text being processed will contain a few words which do not appear in the lexicon as entries in their own right. If the lexicon consists only of whole-word entries, then the method for producing a pronunciation for such &amp;quot;unknown&amp;quot; words is simply to pass them through a set of letter-to-sound rules followed by word stress assignment rules and vowel reduction rules. The resulting pronunciation may well be inaccurate, particularly in English (which often shows a poor relationship between spelling and pronunciation). In addition, the default set of word classes assigned to the word (noun, verb, adjective) will be too general to be of much help to the syntactic parsing module. However, if the lexicon contains individual morphemes (both &amp;quot;bound* and &amp;quot;free&amp;quot;), an unknown word can be analysed into its constituent morphemes. Stress assignment rules will then be more likely to yield the correct pronunciation, and any characteristic suffix that may be present will allow for the assignment of a more accurate word class or classes (eg. +nests denotes a noun, +ly an adverb). Morphological analysis of words will therefore allow a significantly larger number of &amp;quot;unknown&amp;quot; words to be handled. Novel forms such as hamperance, and thatcheriaation would probably not exist in a whole-word dictionary, but could be handled by morphological analysis using existing morphological entries. Also, the ability to deal with compound words would allow for significantly higher accuracy in pronunciation assignment. A problem arises, however, if one or more of the word&apos;s constituent morphemes are not present in the morphological dictionary. In this case, the morphological analysis will fail, and the entire word will be passed to the letter-tosound rules, with concomitant probable loss of accuracy in pronunciation assignment and word class assignment. It is far more likely that the missing morpheme will be a root morpheme rather than an affix, since the latter morphemes form a closed class which may be exhaustively listed, whereas the former form an open class which may be added to as the language evolves Chunnel, kluge, yomp). it would be preferable if any closed-class morin a (putatively) polymorphemic un- known word could be recognised and separated from the remaining material, which would then be assumed to be a new root morpheme. Letterto-sound rules would then be applied to this putative new root morpheme (the pronunciation of the known material would be derived from the lexicon). The advantages of this method are that the pronunciation and word stress assignment are more likely to be accurate, and also that, if there is a suitable suffix, the correct word class may be assigned (eg. in yomping, from yomp (unknown root) and +ing (known verb or noun suffix), which will be characterised as a verb or noun). Thus, in the case of preamble, the stripping of the prefix prewill allow for the pronunciation r ii ambe 1/: the entire word had been passed to the letter-torules, the incorrect pronunciation r b C 1/ have resulted. In addition to affixes, known root morphemes could also be stripped to leave the remaining unknown material. For example, without morphological analbe wrongly pronounced as n th au s/, a voiceless dental fricative. It is known that letter-to-sound rules are more accurate if they are not allowed to apply across morpheme boundaries (see (1, Ch. 61), and this method takes advantage of that fact. Thus greater accuracy is obtained, for polymorphemic unknown words, if known morphs can be stripped before the application of letter-tosound rules. It is this task that the work described below attempts to carry out. The Alvey Natural Language Tools Morphological System ([5],[61), already provides a comprehensive morphological analyser system. This system allows morphological analysis of words into morphemes based on user-defined rules. The basic system does not offer analysis of words containing unknown morphemes, nor does it provide a rank ordering of the output analyses. Both these latter features have been added in the work described below. The system consists of a two tier process: first a morphological analysis, based on Koskenniemi&apos;s two-level morphology ([3[); secondly the statement of morphosyntactic constraints (not available in Koskenniemi&apos;s system) based on a GPSG-like feature grammar. The morphographemic rules are specified as a set of high level rules (rather than directly as finite state transducers) which describe the relationship between a surface tape (the word) and a lexical tape (the normalised lexical form). These rules specify contexts for pairs of lexical and surface characters. For example a rule +:e &lt; s:s h:h &gt; s:s x:x z:z y:i 1 --- 0:8 that a surface character match with a lexical character + when preceded by one s, x, z the pair y: (as in to succeeded by s. The &amp;quot;---&amp;quot; denotes where the rule pair fits into the context. For example the above rule would admit the following match tape: The exact syntax and interpretation is more fully described in [5, Sect. 3] and [6, Ch. 2]. In addition to segmentation each lexical entry is associated with a syntactic category (represented as a feature structure). Grammar rules can be written to specify which conjunctions of morphemes are valid. Thus valid analyses require a valid segmentation and a valid morphosyntax. In the larger descriptions developed in the system a “categorial grammar&amp;quot;-like approach has been used in the specification of affixes. An affix itself will specify what category it can attach (&amp;quot;apply&amp;quot;) to and what its resulting category will be. In the work described here, the basic morphology system has been modified to analyse words containing morphemes that are not in the lexicon. The analysis method offers segmentation and morphological analysis (based on the word grammar), which results in a list of possible analyses. An ordering on these possible analyses has been defined, giving a most likely analysis, for which the spelling of the unknown morpheme can then be reconstructed using the system&apos;s original morphographemic rules. Finally, the pronunciation of the unknown morpheme can be assigned, using letter-to-sound rules encoded as two-level rules. Analysis Method The method used to analyse words containing unknown substrings proceeds as follows. First, four new morphemes are added to the lexicon, one for each major morphologically productive - 102 category (noun, verb, adjective and adverb). Each has a citation form of **. The intention is that the unknown part of a word will match these entries. Thus we get two-level segmentation as follows lexical tape: *000*+ing+s surface tape: Opar0OingOs The special character 0 represents the null symbol (i.e. the surface form would be parings — without the nulls). This matching is achieved by adding two two-level morphological rules. The first rule allows any character in the surface alphabet to match null on the lexical tape, but only in the context where the lexical nulls are flanked by lexical asterisks matching with surface nulls. The second rule deals with constraining the *:0 pairs themselves. It deals with two specific points. First, it ensures that there is only one occurrence of ** in an analysis (i.e only one unknown section). Second, it constrains the unknown section. This is done in two ways. Rather than simply allowing the unknown part to be any arbitrary collection of letters, it is reto ensure that if it starts with any of j1mnqr vxy z}, then it is also followed by a vowel. This (rightly) excludes the possibility of an unknown section starting with an unconsonant cluster e.g. not be analysed as co- +er). Second, it ensures that the unknown section is at least two characters long and contains a vowel. excludes the analysis of rest +ing. These restrictions on the unknown section are weak and more comprehensive restrictions would help. They are attempts at characterising English morphemes in terms of the minimal English syllable. A more complex characterization, defining valid consonant clusters, vowels, etc. would be possible in this formalism, and the phonotactic constraints of English syllables are well known. However, the resulting rules would be clumsy and slow, and it was felt that, at this stage, any small gain in accuracy would be offset by a speed penalty. The rules make use of sets of characters. is set consisting of all surface charsets consisting of those letters, V is the set of vowels and C the consonants. The character $ is used to mark word boundaries. 0:Anything &lt;=&gt; { *:0 &lt; *:0 (0:Anything)1+ &gt; { *:0 &lt; (0:Anything)1+ *:0 &gt; &lt;=.&gt; &lt; 0:$ (=:=)1+ &gt; } --- { &lt; { 0:BCDFGKPSTW 0:V } 0:Anything &gt; &lt; 0:HJLMNQRVXYZ 0:V &gt; or { &lt; 0:C (0:V)1+ &gt; 0:V (0:C)1+ &gt; - - - (=:=)1+ 0:$ &gt; 1 The above rules are somewhat clumsily formulated. This is partly due to the particular implementation used, which allows only one rule for each surface:lexical pair&apos; and partly due to the complexity of the phenomena being described. Word Grammar Using the above two rules and adding the four new lexical entries to a larger description, it is now possible to segment words with one unknown substring. Because the system encodes constraints for affixes via feature specifications, only morphosyntactically valid analyses will be permitted. That is, although ** is ambiguous its category, if it is followed by the analysis involving the verb will succeed. For example, although the segmentation process could ** +ed +s word grammar section would exclude this analysis, since the +s suffix only follows uninflected verbs or nouns. However, there are a number of possible mistakes that can occur. When an unknown section exists it may spuriously contain other morphemes, leading to an incorrect analysis. For example colour -&gt; co- ** readable -&gt; re- ** +able cartoons -&gt; car ** +s (compound noun) In actual fact, when words are analysed by this technique a large number of analyses is usually found. The reasons for the large number are as follows. Firstly, the assumed size of the unknown part can vary for the same word, as in the following: &apos;Ritchie ([4]) shows that this is not a restriction on the formal power of the rules. - 103 entitled -&gt; ** entitled -&gt; ** +ed entitled -&gt; en- ** +ed entitled -&gt; en- ** Secondly, because ** is four ways ambiguous, there can be multiple analyses for the same surface form. For example, a word ending in s could be either a plural noun or a third person singular verb. These points can multiply together and often produce a large number of possible analyses. Out of the test set of 200 words, based on a lexicon consisting of around 3500 morphemes (including the ** entries), the average number of analyses found was 9, with a maximum number of 71 (for functional). Choosing an Analysis In order to use these results in a text-to-speech system, it is necessary to choose one possible analysis, since a TTS system is deterministic. To do this, the analyses are rank ordered. A number of factors are exploited in the rank ordering: length of unknown root structural ordering rules ([1, Ch. 3]) frequency of affix Each of these factors will be described in turn. When analysing a word containing an unknown part, the best results are usually obtained by using the analysis with the shortest unknown part [1, Ch. 6]). Thus the analysis of would be ordered as follows (most likely first): +er **+5 &gt; ** heuristic will occasionally fail, as in the shortest unknown analysis is ** the correct result will be obtained in most cases. The second ordering constraint is based on the ordering rules used in [1]. Some words can be segmented in many different ways (this is true even if all parts are known). For example scarcity -&gt; scar city scarcity -&gt; scarce +ity scarcity -&gt; scar cite +y A simple rule notation has been defined for assigning order to analyses in terms of their morphological parse tree. These rules can be summarised as prefixing &gt; suffixing &gt; inflection &gt; compounding The third method used for ordering is affix frequency. The frequencies are based on suffix-astag (word class) frequencies in the LOB corpus of written English, given in [21. Thus the suffix forming a noun from a verb (as in was marked in the lexicon as being more likely the adjectival comparative These constraints are applied simultaneously. Each rule has an appropriate weighting, such that the length of the unknown part is a more significant factor than morphological structure, which in turn is more significant than affix frequency. Results The method was subjected to a test procedure. The test used a basic lexicon of around 3500 morphemes, of which around 150 were affixes. From a randomly selected Al magazine article, the first 200 words were used which could not be analysed by the basic morphological system (i.e. without the unknown root section). When these 200 words were analysed using the method described in the previous sections, 133 words (67%) were analysed correctly, 48 words (24%) were wrong due to segmentation error, and 19 (9%) were wrong due to word class error. An analysis was deemed to be correct when the most preferred analysis had both the correct morphological structure and the correct word class. Segmentation errors were due mainly to spurious words in sub-parts of unknown sections, ill ** ate. errors will increase as the lexicon grows. To prevent this type of error, it may be necessary to place restrictions on compounding, such that those words which can form part of compounds should be marked as such (though this is a major research problem in itself). Word class errors occurred where the correct segmentation was found but an incorrect morphological structure was assigned. The definition of error used here may be over-restrictive, as it may still be the case that erroneous segmentation and structure errors still provide analyses with the correct pronunciation. But at this time the remainder of the text-to-speech system is not advanced enough for this to be adequately tested. - 104 Generating the Spelling of Unknown Morphemes A method has been described for handling a word which cannot be analysed by the conventional morphological analysis process. This method may generate a number of analyses, so an ordering of the results is defined. However, in a text-to-speech system (or even an interactive spelling corrector), it may be desirable to add the unknown root to a user lexicon for future reference. In such a case, it will be necessary to reconstruct the underlying spelling of the unknown morpheme. This can be done in a very similar way to that in which the system normally generates surface forms from lexical forms. The problem is the following: given a surface form and a set of spelling rules (not including the two special rules described above), define the set of possible lexical forms which can match to the surface form. This, of course, would over-generate lexical forms, but if the permitted lexical form is further constrained so as to match the one given from the analysis containing the ** a more satisfactory result will be obtained. For example, the surface form remoned would be analysed as re-**+ed. A matching is carried out character by character between the lexical and surface forms, checking each match with respect to the spelling rules (and hypothesizing nulls where appropriate). On encountering the ** section of the lexical form, the process attempts to match all possible lexical characters with the surface form.. This is of course still constrained by the spelling rules, so only a few characters will match. What is significant is that the minor orthographic changes that the spelling rules describe will be respected. Thus in this case the ** matches none (rather than simply mon without an e), as the spelling rules require there to be an e inserted before the +ed in this case. Similarly, given the surface string mogged, analysed as **+ed, the root form mog is generated. However, the alternative forms mogg and mogge are also generated. This is not incorrect, as in similar cases such analyses are correct (eg. and As yet, the method has no means of selecting between these possibilities. After the generation of possible orthographic forms, the letter-to-sound rules are applied. As regards the format of these rules, what is required is something very similar to Koskenniemi two-level rules, relating graphemes to phonemes in particular contexts. A small set of grapheme to phoneme rules was written using this notation. However, there were problems in writing these rules, as the fuller set of rules from which they were taken used the concept of rule ordering, while the Koskenniemi rule interpretation interprets all rules in parallel. The rewas that the rewritten rules were more difficult both to read and to write. Although it is possible (and even desirable) to use finite state transducers in the run-time system, the current Koskenniemi format may not be the best format for letter-to-sound rules. Some other notation which could compile to the same form would make it easier to extend the ruleset. Problems The technique described above largely depends on the existence of an appropriate lexicon and morphological analyser. The starting-point was a fairly large lexicon (over 3000 morphemes) and an analyser description, and the expectation was that only minor additions would be needed to the system. However, it seems that significantly better results will require more significant changes. Firstly, as the description used had a rich morpho-syntax, words could be analysed in many ways giving different syntactic markings (eg. different number and person markings for verbs) which were not relevant for the rest of the system. Changes were made to reduce the number of phonetically similar (though syntactically different) analyses. The end result now states only the major category of the analysis. (Naturally, if the, system were to be used within a more complex syntactic parser, the other analyses may be needed). Secondly, the number of &amp;quot;stem&amp;quot; entries in the lexicon is significant. It must be large enough to analyse most words, though not so large that it gives too many erroneous analyses of unknown words. Also, while it has assumed that the lexicon contains productive affixes, perhaps it should also contain certain derivational affixes which are not normally productive, such as tele-, +ology, +phobia, +vorous. These would be very useful when analysing unknown words. The implication is that there should be a special lexicon used for - 105 analysing unknown words. This lexicon would have a large number of affixes, together with constraints on compounds, that would not normally be used when analysing words. Another problem is that unknown words are often place-names, proper names, loanwords etc. The technique described here would probably not deal adequately with such words. So far, this technique has been described only in terms of English. When considering other languages, especially those where compounding is common (eg. Dutch and German), the method would be even more advantageous. In novel compounds, large sections of the word could still be analysed. In the above description, only one unknown part is allowed in each word. This seems to be reasonable for English, where there will rarely be compounds of the form ** +suf ** +suf. However, in other languages (especially those with a more fullydeveloped system of inflection) such structures do exist. An example is the Dutch word bejaardentehuizen (old peoples homes), which has the structure noun +en noun +en. Thus it is possible for words to contain two (or more) non-contiguous unknown sections. The method described here could probably cope with such cases in principle, but the current implementation does not do so. Instead, it would find one unknown part from the start of the first unknown morpheme to the end of the final unknown morpheme. Summary A system has been described which will analyse any word and assign a pronunciation. The system first tries to analyse an input word using the standard analysis procedure. If this fails, the modified lexicon and spelling rule set are used. The output analyses are then ordered. For each unknown section, the underlying orthographic form is constructed, and letter-tosound rules are applied. The end result is a string of phonemic forms, one form for each morpheme in the original word. These phonemic forms are then processed by morphophonological rules, followed by rules for word stress assignment and vowel reduction. Acknowledgements Alan Black is currently funded by an SERC studentship (number 89313458). During this project Joke van de Plassche was funded by</abstract>
<title confidence="0.54557075">the SED and Stichting Nijmeegs Universiteits Fonds. Briony Williams is employed on the ES- PRIT &amp;quot;POLYGLOT&amp;quot; project. We should also like to acknowledge help and ideas from</title>
<author confidence="0.837001">Gerard Kempen</author>
<author confidence="0.837001">Fransiska Maier</author>
<author confidence="0.837001">Helen Pain</author>
<author confidence="0.837001">Graeme Ritchie</author>
<author confidence="0.837001">Alex Zbyslaw</author>
<title confidence="0.839578">References</title>
<author confidence="0.902452">Text-</author>
<note confidence="0.615651384615385">The MITalk system. University Press, Cambridge, UK., 1987. [21 S. Johansson and M. Jahr. Grammatical tagging of the LOB corpus: predicting word class from word endings. In S. Johansson, corpora in English lanresearch, Computing Centre for the Humanities, Bergen, 1982. Koskenniemi. A general computational model for word-form recognition and pro- In of the 10th International Conference on Computational Lin- 178-181, Stanford University, California, 1984. Ritchie. Generated by Two- Morphological Rules. Paper 496, Dept of AI, University of Edinburgh, 1991. Ritchie, S. Pulman, A. Black, and G. Russell. A computational framework for description. Linguis- 1987. Ritchie, G. Russell, A. Black, and S. Pul- Computational Morphology. Press, Cambrdige, Mass., forthcomming. - 106 -</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alan</author>
</authors>
<title>Black is currently funded by an SERC studentship (number 89313458). During this project Joke van de Plassche was funded by the SED and Stichting Nijmeegs Universiteits Fonds. Briony Williams is employed on the ESPRIT &amp;quot;POLYGLOT&amp;quot; project. We should also like to acknowledge help and ideas from Gerard Kempen, Fransiska Maier, Helen Pain, Graeme Ritchie and Alex Zbyslaw.</title>
<marker>Alan, </marker>
<rawString>Alan Black is currently funded by an SERC studentship (number 89313458). During this project Joke van de Plassche was funded by the SED and Stichting Nijmeegs Universiteits Fonds. Briony Williams is employed on the ESPRIT &amp;quot;POLYGLOT&amp;quot; project. We should also like to acknowledge help and ideas from Gerard Kempen, Fransiska Maier, Helen Pain, Graeme Ritchie and Alex Zbyslaw.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>M Hunnicut</author>
<author>K Klatt</author>
</authors>
<title>Textto-speech: The MITalk system.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.,</location>
<marker>Allen, Hunnicut, Klatt, 1987</marker>
<rawString>111 J. Allen, M. Hunnicut, and K. Klatt. Textto-speech: The MITalk system. Cambridge University Press, Cambridge, UK., 1987.</rawString>
</citation>
<citation valid="true">
<title>Grammatical tagging of the LOB corpus: predicting word class from word endings.</title>
<date>1982</date>
<booktitle>Computer corpora in English language research, Norwegian Computing Centre for the Humanities,</booktitle>
<editor>In S. Johansson, editor,</editor>
<location>Bergen,</location>
<marker>1982</marker>
<rawString>[21 S. Johansson and M. Jahr. Grammatical tagging of the LOB corpus: predicting word class from word endings. In S. Johansson, editor, Computer corpora in English language research, Norwegian Computing Centre for the Humanities, Bergen, 1982.</rawString>
</citation>
<citation valid="true">
<title>A general computational model for word-form recognition and production.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Linguistics,</booktitle>
<pages>178--181</pages>
<institution>Stanford University,</institution>
<location>California,</location>
<marker>1984</marker>
<rawString>(31 K. Koskenniemi. A general computational model for word-form recognition and production. In Proceedings of the 10th International Conference on Computational Linguistics, pages 178-181, Stanford University, California, 1984.</rawString>
</citation>
<citation valid="true">
<title>Languages Generated by Twolevel Morphological Rules.</title>
<date>1991</date>
<tech>Research Paper 496,</tech>
<institution>Dept of AI, University of Edinburgh,</institution>
<marker>1991</marker>
<rawString>[41 G. Ritchie. Languages Generated by Twolevel Morphological Rules. Research Paper 496, Dept of AI, University of Edinburgh, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ritchie</author>
<author>S Pulman</author>
<author>A Black</author>
<author>G Russell</author>
</authors>
<title>A computational framework for lexical description.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--3</pages>
<marker>Ritchie, Pulman, Black, Russell, 1987</marker>
<rawString>151 G. Ritchie, S. Pulman, A. Black, and G. Russell. A computational framework for lexical description. Computational Linguistics, 13(3-4):290-307, 1987.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Ritchie</author>
<author>G Russell</author>
<author>A Black</author>
<author>S Pulman</author>
</authors>
<title>Computational Morphology.</title>
<publisher>MIT Press,</publisher>
<location>Cambrdige, Mass., forthcomming.</location>
<marker>Ritchie, Russell, Black, Pulman, </marker>
<rawString>161 G. Ritchie, G. Russell, A. Black, and S. Pulman. Computational Morphology. MIT Press, Cambrdige, Mass., forthcomming.</rawString>
</citation>
<citation valid="false">
<pages>106</pages>
<marker></marker>
<rawString>- 106 -</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>