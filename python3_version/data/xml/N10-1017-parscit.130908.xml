<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000088">
<title confidence="0.990103">
Hitting the Right Paraphrases in Good Time
</title>
<author confidence="0.999369">
Stanley Kok
</author>
<affiliation confidence="0.9989825">
Department of Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.582367">
Seattle, WA 98195, USA
</address>
<email confidence="0.995701">
koks@cs.washington.edu
</email>
<author confidence="0.973771">
Chris Brockett
</author>
<affiliation confidence="0.95039">
Microsoft Research
</affiliation>
<address confidence="0.9285565">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.998424">
chrisbkt@microsoft.com
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982547826087">
We present a random-walk-based approach to
learning paraphrases from bilingual parallel
corpora. The corpora are represented as a
graph in which a node corresponds to a phrase,
and an edge exists between two nodes if their
corresponding phrases are aligned in a phrase
table. We sample random walks to compute
the average number of steps it takes to reach
a ranking of paraphrases with better ones be-
ing “closer” to a phrase of interest. This ap-
proach allows “feature” nodes that represent
domain knowledge to be built into the graph,
and incorporates truncation techniques to pre-
vent the graph from growing too large for ef-
ficiency. Current approaches, by contrast, im-
plicitly presuppose the graph to be bipartite,
are limited to finding paraphrases that are of
length two away from a phrase, and do not
generally permit easy incorporation of domain
knowledge. Manual evaluation of generated
output shows that our approach outperforms
the state-of-the-art system of Callison-Burch
(2008).
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836488372093">
Automatically learning paraphrases, or alternative
ways of expressing the same meaning, is an ac-
tive area of NLP research because of its useful-
ness in a variety of applications, e.g., question an-
swering (Lin and Pantel, 2001; Ravichandran and
Hovy, 2002; Reizler et al., 2007), document sum-
marization (Barzilay et al., 1999; McKeown et al.,
2002), natural language generation (Iordanskaja et
al., 1991; Lenke, 1994; Stede, 1999), machine trans-
lation (Kauchak and Barzilay, 2006; Callison-Burch
et al., 2006; Madnani et al., 2007).
Early work on paraphrase acquisition has focused
on using monolingual parallel corpora (Barzilay and
McKeown, 2001; Barzilay and Lee, 2003; Pang et
al., 2003; Quirk et al., 2004). While effective, such
methods are hampered by the scarcity of monolin-
gual parallel corpora, an obstacle that limits both
the quantity and quality of the paraphrases learned.
To address this limitation, Bannard and Callison-
Burch (2005) focused their attention on the abun-
dance of bilingual parallel corpora. The crux of
this system (referred to below as ”BCB”) is to align
phrases in a bilingual parallel corpus and hypothe-
size English phrases as potential paraphrases if they
are aligned to the same phrase in another language
(the “pivot”). Callison-Burch (2008) further refines
BCB with a system that constrains paraphrases to
have the same syntactic structure (Syntactic Bilin-
gual Phrases: SBP).
We take a graphical view of the state-of-the-art
BCB and SBP approaches by representing the bilin-
gual parallel corpora as a graph. A node corresponds
to a phrase, and an edge exists between two nodes if
their corresponding phrases are aligned. This graph-
ical form makes the limitations of the BCB/SBP ap-
proaches more evident. The BCB/SBP graph is lim-
ited to be bipartite with English nodes on one side
and foreign language nodes on the other, and an
edge can only exist between nodes on different sides.
This neglects information between foreign language
nodes that may aid in learning paraphrases. Further,
by only considering English nodes that are linked
via a foreign language node as potential paraphrases,
</bodyText>
<page confidence="0.980712">
145
</page>
<note confidence="0.866">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 145–153,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997432923076923">
these approaches will fail to find paraphrases sepa-
rated by distances greater than length two.
In this paper, we present HTP (Hitting Time
Paraphraser), a paraphrase learning approach that is
based on random walks (Lov´asz, 1996) and hitting
times (Aldous and Fill, 2001). Hitting time mea-
sures the average number of steps one needs to take
in a random traversal of a graph before reaching a
destination node from a source node. Intuitively, the
smaller the hitting time from a phrase E to E&apos; (i.e.,
the closer E&apos; is to E), the more likely it is that E&apos; is
a good paraphrase of E. The advantages of HTP are
as follows:
</bodyText>
<listItem confidence="0.938557307692308">
• By traversing paths of lengths greater than two,
our approach is able to find more paraphrases
of a given phrase.
• We do not require the graph to be bipartite.
Edges can exist between nodes of different for-
eign languages if their corresponding phrases
are aligned. This allows information from for-
eign phrase alignments to be used in finding
English paraphrases.
• We permit domain knowledge to be easily in-
corporated as nodes in the graph. This allows
domain knowledge to favor good paraphrases
over bad ones, thereby improving performance.
</listItem>
<bodyText confidence="0.999359111111111">
In this paper, we focus on learning English para-
phrases. However, our system can be applied to
learning paraphrases in any language.
We begin by reviewing random walks and hitting
times in the next section. Then we describe our para-
phrase learning algorithm (Section 3), and report our
experiments (Section 4). We discuss related work in
Section 5. Finally, we conclude with future work
(Section 6).
</bodyText>
<sectionHeader confidence="0.982041" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999701595238095">
A directed graph consists of a set of nodes V , and a
set of edges E. A directed edge is a pair (i, j) where
i, j E V . Associated with the graph is a |V  |x |V |
adjacency matrix W. Each entry Wij in the matrix
is the weight of edge (i, j), or zero if the edge does
not exist.
In a random walk (Lov´asz, 1996), we traverse
from node to node via the edges. Suppose at time
step t, we are at node i. In the next step, we move
to its neighbor j with probability proportional to
the weight of the edge (i, j), i.e., with probability
Wij/ Ej Wij. This probability is known as the tran-
sition probability from i to j. Note that the transition
probabilities from a node to its neighbors sum to 1.
The hitting time hij (Aldous and Fill, 2001) from
node i to j is defined as the average number of steps
one takes in a random walk starting from i to visit j
for the first time. Hitting time has the property of be-
ing robust to noise. This is a desirable property for
our system which works on bilingual parallel cor-
pora containing numerous spurious alignments be-
tween phrases (i.e., edges between nodes). However,
as observed by Liben-Nowell and Kleinberg (2003),
hitting time has the drawback of being sensitive to
portions of the graph that are far from the start node
because it considers paths of length up to oc.
To circumvent this problem, Sarkar and Moore
(2007) introduced the notion of truncated hitting
time where random walks are limited to have at most
T steps. The truncated hitting time hT ij from node i
to j is defined as the average number of steps one
takes to reach j for the first time starting from i in a
random walk that is limited to at most T steps. hT ij
is defined to be 0 if i = j or T = 0, and to be T if j
is not reach in T steps. As T —4 oc, hT ij—4 hij.
In a recent work, Sarkar et al. (2008) showed that
truncated hitting time can be approximated accu-
rately with high probability by sampling. They run
M independent length-T random walks from node
i. In m of these runs, node j is visited for the first
time at time steps t1j, ... , tmj . The estimated trun-
cated hitting time is given by
</bodyText>
<equation confidence="0.98258325">
Em k=1 tk
�hT M + (1 − m
j
ij = M )T (1)
</equation>
<bodyText confidence="0.99043025">
They also showed that the number of samples of ran-
dom walks M has to be at least 1
2~2 log 2nd in order
for the estimated truncated hitting time to be a good
estimate of the actual truncated hitting time with
high probability, i.e., for P (|�hT ij−hT ij|&lt;eT)&gt;1 − S,
where n is the number of nodes in the graph, e and S
are user-specified parameters, and 0 &lt; e, S &lt; 1.
</bodyText>
<sectionHeader confidence="0.896968" genericHeader="method">
3 Hitting Time Paraphraser (HTP)
</sectionHeader>
<bodyText confidence="0.993407">
HTP takes a query phrase as input, and outputs a list
of paraphrases, with better paraphrases at the top of
</bodyText>
<page confidence="0.998976">
146
</page>
<figureCaption confidence="0.9998295">
Figure 1: Graph created from English-French (E-F),
English-German (E-G), and French-German (F-G) bilin-
gual parallel corpora. Bold edges have large positive
weights (high transition probabilities).
</figureCaption>
<bodyText confidence="0.996465571428572">
the list. HTP also requires as input a set of bilin-
gual parallel corpora that have been processed into
phrase tables of the kind used in statistical machine
translation.
A bilingual parallel corpus is made up of sen-
tences in two languages. Two sentences that are
translations of one another are paired together, and
a phrase in one sentence is aligned with a phrase in
the other with the same meaning. From such align-
ments, we can count for a phrase E both the num-
ber of times it occurs (CountE), and the number of
times it is aligned with a phrase F in the other lan-
guage (CountE,F). With these counts we can es-
timate the probability of F given E as P(F|E) _
</bodyText>
<equation confidence="0.938816">
CountE,F
CountE .
</equation>
<bodyText confidence="0.999270952380952">
HTP represents the aligned phrases as a graph. A
node corresponds to a phrase, and a directed edge
exists from node i to j if their corresponding phrases
are aligned. The weight of edge (i, j) is given by
P(j|i) which is computed as described in the previ-
ous paragraph.
Figure 1 gives an example of a graph created
from English-French, English-German, and French-
German parallel corpora. We use this figure to il-
lustrate the strengths of HTP. First, by using moder-
ately long random walks, HTP is able to find para-
phrases that are separated by long paths. For ex-
ample, there is a high probability path of length 4
(E1, F1, E2, F2, E3) from E1 to E3. Because of the
path’s high probability, it will appear in many of the
random walks starting from E1 that are sampled on
the graph, and thus E3 will be visited in many of
the samples. This causes the truncated hitting time
hE,E3 to be small, allowing HTP to find E3 as a
plausible paraphrase of E1. Second, by allowing
edges between nodes of different foreign languages
</bodyText>
<tableCaption confidence="0.995031">
Table 1: The HTP algorithm.
</tableCaption>
<bodyText confidence="0.97441144">
function HTP(E, C, d, n, m, T, S, l)
input: E, query phrase
C, tables of aligned phrases
d, maximum distance of nodes from E
n, maximum number of nodes in graph
m, number of samples of random walks
T, maximum number of steps taken by a
random walk
S, probability that estimated truncated hitting
time deviates from actual value by a large
margin (see Equation 1)
l, number of top outgoing edges to select at
each node in a random walk
output:(E01,..., E0k), paraphrases of E ranked in
order of increasing hitting times
calls: CreateGraph(E, C, d, n) creates graph G
from C containing at most n nodes that are
at most d steps from E
EstimateHitTimes(E, G, m, T, S), estimates
the truncated hitting times of each node in G
by running m random walks
PruneNodes((E1, ... , Ek), G), removes nodes
from G if their hitting times is equal to T.
AddFeatureNodes(G), adds nodes
representing domain knowledge to G
</bodyText>
<equation confidence="0.9956355">
G +— CreateGraph(E, C, d, n)
(E1, ... , Ek) +— EstimateHitTimes(E, G, m, T, S)
G0+—PruneNodes((E1, ... , Ek), G)
G00 +—AddFeatureNodes(G0)
</equation>
<bodyText confidence="0.958745941176471">
(E01, ... , E0k) +— EstimateHitTimes(E, G00, m, T, S)
return (E01, ... , E0k)
(i.e., by not requiring the graph to be bipartite), HTP
allows strong correlation between foreign language
nodes to aid in finding paraphrases. In the figure,
even though E4 and E5 are not linked via a com-
mon foreign language node, there is a high proba-
bility path linking them (E4, F3, G1, E5). This al-
lows HTP to find E5 as a reasonable paraphrase of
E4. Third, HTP enables domain knowledge to be
incorporated as nodes in the graph. For example,
we could incorporate the domain knowledge that
phrases with lots of unigrams in common are likely
to be paraphrases. In Figure 1, the “feature” node
represents such knowledge, linking E4 and E1 as
possible paraphrases even though they have no for-
eign language nodes in common. Note that such
</bodyText>
<page confidence="0.990989">
147
</page>
<bodyText confidence="0.999979266666667">
domain knowledge nodes can be linked to arbitrary
nodes, not just English ones.
The HTP algorithm is shown in Table 1. It takes
as input a query phrase and a set of bilingual phrase
tables. The algorithm begins by creating a graph
from the phrase tables. Then it estimates the trun-
cated hitting times of each node from the query node
by sampling random walks of length T. Next it
prunes nodes (and their associated edges) if their
truncated hitting times are equal to T. To the result-
ing graph, it then adds nodes representing domain
knowledge and estimates the truncated hitting times
of the nodes by sampling random walks as before.
Finally, it returns the nodes in the same language as
the query phrase in order of increasing hitting times.
</bodyText>
<subsectionHeader confidence="0.997817">
3.1 Graph Creation
</subsectionHeader>
<bodyText confidence="0.999663">
An obvious approach to creating a graph from bilin-
gual parallel corpora is to create a node for every
phrase in the corpora, and two directed edges (i, j)
and (j, i) for every aligned phrase pair i and j. Let
H refer to the graph that is created in this manner.
Such an approach is only tractable for small bilin-
gual parallel corpora that would result in a small
H, but not for large corpora containing millions of
sentences, such as those described in Section 4.1.
Therefore we approximate H with a graph H&apos; that
only contains nodes “near” to the node representing
the query phrase. Specifically, we perform breadth-
first search starting from the query node up to a
depth d, or until the number of nodes visited in the
search has reached a maximum of n nodes. Some
nodes at the periphery of H&apos; have edges to nodes
that are not in H&apos; but are in H. For a periph-
ery node j that has edges to nodes j1, ... , j� out-
side H&apos;, we create a “dummy” node a, and replace
edges (j, j1), ... , (j, j�) with a single edge (j, a)
with weight EX=1 Wj,j.. We also add edges (a, j)
and (a, a) (each with a heuristic weight of 0.5). The
dummy nodes and their edges approximate the tran-
sition probabilities at H&apos;’s periphery. Our empirical
results show that this approximation works well in
practice.
</bodyText>
<subsectionHeader confidence="0.999569">
3.2 Graph Pruning
</subsectionHeader>
<bodyText confidence="0.998069">
After H&apos; is created, we run M independent length-
T random walks on it starting from the query node
to estimate the truncated hitting times of all nodes.
</bodyText>
<figureCaption confidence="0.998623">
Figure 2: Feature nodes representing domain knowledge.
Feature nodes are shaded. The bold node represents a
query phrase. (a) n-gram nodes (b) “syntax” nodes (c)
“not-substring/superstring-of” nodes.
</figureCaption>
<bodyText confidence="0.999976733333333">
A node in H&apos; may have many outgoing edges, most
of which may be due to spurious phrase alignments.
For efficiency, and to reduce the noise due to spuri-
ous edges, we select among a node’s top l outgoing
edges with the highest transition probabilities, when
deciding which node to visit next at each step of a
random walk
For each random walk k, we record the first time
that a node j is visited t� . Using Equation 1, we es-
timate the truncated hitting time of each node. Then
we remove nodes (and their associated edges) that
are far from the query node, i.e., with times equal
to T. Such nodes either are not visited in any of the
random walks, or are always visited for the first time
at step T.
</bodyText>
<subsectionHeader confidence="0.999771">
3.3 Adding Domain Knowledge
</subsectionHeader>
<bodyText confidence="0.999984818181818">
Next we add nodes representing domain knowledge
to the pruned graph. In this version of HTP, we im-
plemented three types of feature nodes.
First, we have n-gram nodes. These nodes cap-
ture the domain knowledge that phrases containing
many words in common are likely to be paraphrases.
For each 1 to 4-gram that appears in English phrases,
we create an n-gram node a. We add directed edges
(a, j) and (j, a) if node j represents an English
phrase containing n-gram a. For example, in Fig-
ure 2(a), “reach the objective” is connected to “ob-
</bodyText>
<page confidence="0.992826">
148
</page>
<bodyText confidence="0.999851214285714">
jective” because it contains that unigram. Note that
such nodes create short paths between nodes with
many n-grams in common, thereby reducing the hit-
ting times between them.
Second, we have “syntax” nodes, which repre-
sent syntactic classes of the start and end words of
English phrases. We created classes such as inter-
rogatives (“whose”, “what”, “where”, etc.), articles
(“the”, “a”, “an”), etc. For each class c, we cre-
ate syntax nodes ac and a&apos; to respectively represent
the conditions that a phrase begins and ends with a
word in class c. Directed edges (ac, j) and (j, ac)
are added if node j starts with a word in class c (sim-
ilarly we add (a&apos;, j) and (j, a&apos;) if it ends with a word
in class c). For example, in Figure 2(b), “the objec-
tive is” is linked to “starts with article” because it
begins with “the”. These syntax nodes allow HTP to
capture broad commonalities about structural distri-
bution, without requiring syntactic equivalence as in
Callison-Burch 2008 (or the use of a parser).
Third, we have “not-substring/superstring-of”
nodes. We observed that many English phrases (e.g.,
“reach the objective” and “reach the”) that are super-
strings or substrings of each other tend to be aligned
to several shared non-English phrases in the bilin-
gual parallel corpora used in our experiments. Most
such English phrase pairs are not paraphrases, but
they are linked by many short paths via their com-
mon aligned foreign phrase, and thus have small
hitting times. To counteract this, we create a “not-
substring/superstring-of” node a. The query node i
is always connected to a via edges (i, a) and (a, i).
We add edges (a, j) and (j, a) if English phrase j
is not a substring or superstring of the query phrase
(see Figure 2(c)).
With the addition of the above, each node rep-
resenting an English phrase can have four kinds
of outgoing edges: edges to foreign phrase nodes,
and edges to the three kinds of feature nodes. Let
fphrase, fngram, fsyntax, fsubstring denote the distri-
bution of transition probabilities among the four
kinds of outgoing edges. Note that fphrase +
fngram + fsyntax + fsubstring = 1.0. These values
are user-specified or can be set with tuning data. An
outgoing edge from English phrase node i that orig-
inally had weight (transition probability) Wil will
now have weight Wig x fphrase. All k edges from i
to n-gram nodes will have weight f&amp;quot;am �. Likewise
for edges to the other two kinds of feature nodes.
Each of the k outgoing edges from a feature node is
simply set to have a weight of 1.
After adding the feature nodes, we again run M
independent length-T random walks to estimate the
truncated hitting times of the nodes, and return the
English phrase nodes in order of increasing hitting
times.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999859666666667">
We conducted experiments to investigate how HTP
compares with the state of the art, and to evaluate
the contributions of its components.
</bodyText>
<subsectionHeader confidence="0.920456">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.998684483870968">
We used the Europarl dataset (Koehn, 2005) for
our experiments. This dataset contains English
transcripts of the proceedings of the European
Parliament, and their translations into 10 other
European languages. In the dataset, there are
about a million sentences per language, and En-
glish sentences are aligned with sentences in the
other languages. Callison-Burch (2008) aligned
English phrases with phrases in each of the
other languages using Giza++ (Och and Ney,
2004). We used his English-foreign phrasal align-
ments which are publicly available on the web at
http://ironman.jhu.edu/emnlp08.tar. In addition, we
paired sentences of different non-English languages
that correspond to the same English sentence, and
aligned the phrases using 5 iterations of IBM model
1 in each direction, followed by 5 iterations of HMM
alignment with paired training using the algorithm
described in Liang et al. (2006). We further used the
technique of Chen et al. (2009) to remove a phrase
alignment F-G (where F and G are phrases in dif-
ferent foreign languages) if it was always aligned
to different phrases in a third “bridge” foreign lan-
guage. As observed by Chen et al., this helped to
remove spurious alignments. We used Finnish as the
bridge language; when either F or G is Finnish, we
used Spanish as the bridge language; when F and
G were Finnish and Spanish, we used English as
the bridge language. In our experiments, we used
phrases of length 1 to 4 of the following six lan-
guages: English, Danish, German, Spanish, Finnish,
</bodyText>
<page confidence="0.99815">
149
</page>
<bodyText confidence="0.9997384">
and Dutch. All the phrasal alignments between each
pair of languages (15 in total) were used as input to
HTP and its comparison systems. A small subset of
the remaining phrase alignments were used for tun-
ing parameters.
</bodyText>
<subsectionHeader confidence="0.984719">
4.2 Systems
</subsectionHeader>
<bodyText confidence="0.999679461538461">
We compared HTP to the state-of-the-art SBP sys-
tem (Callison-Burch, 2008). We also investigated
the contribution of the feature nodes by running HTP
without them. In addition, we ran HTP on a bipartite
graph, i.e., one created from English-foreign phrase
alignments only without any phrase alignments be-
tween foreign languages.
We used Callison-Burch (2008)’s implemen-
tation of SBP that is publicly available at
http://ironman.jhu.edu/emnlp08.tar. SBP is based
on BCB (Bannard and Callison- Burch, 2005) which
computes the probability that English phrase E&apos; is a
paraphrase of E using the following formula:
</bodyText>
<equation confidence="0.9892295">
P(E&apos;|E) � E E P(E&apos;|F)P(F|E) (2)
CEC FEC
</equation>
<bodyText confidence="0.9996478">
where C is set of bilingual parallel corpora, and F is
a foreign language phrase. Representing phrases as
nodes, and viewing P(E&apos;|F) and P(F|E) as tran-
sition probabilities of edges (F, E&apos;) and (E, F), we
see that BCB is summing over the transition prob-
abilities of all length-two paths between E and E&apos;.
All E&apos; paraphrases of E can then be ranked in or-
der of decreasing probability as given by Equation 2.
The SBP system modifies Equation 2 to incorporate
syntactic information, thus:
</bodyText>
<equation confidence="0.945887">
P(E&apos;|F, synE))P(F|E, synE) (3)
</equation>
<bodyText confidence="0.9995555">
where synE is the syntax of phrase E, and
P(E&apos;|F, synE)) = 0 if E&apos; is not of the same syntac-
tic category. From Equation 3, we can see that SBP
constrains E&apos; to have the same syntactic structure
as E. To obtain the syntactic structure of each En-
glish phrase, each English sentence in every parallel
corpus has to be parsed to obtain its parse tree. An
English phrase can have several syntactic structures
because different parse trees can have the phrase as
their leaves, and in each of these, SBP associates the
</bodyText>
<tableCaption confidence="0.59832">
Table 2: Scoring Standards.
0 Clearly wrong; grammatically incorrect, or
does not preserve meaning
</tableCaption>
<bodyText confidence="0.817972454545455">
1 Minor grammatical errors (e.g., subject-verb
disagreement or wrong tense), or meaning is
largely preserved but not completely
2 Totally correct; grammatically correct and
meaning is preserved
phrase with all subtrees that have the phrase as their
leaves. SBP thus offers several ways of choosing
which syntactic structure a phrase should be asso-
ciated with. In our experiments, we used the best
performing method of averaging Equation 3 over all
syntactic structures that E is associated with.
</bodyText>
<subsectionHeader confidence="0.992185">
4.3 Methodology
</subsectionHeader>
<bodyText confidence="0.955112413793103">
To evaluate performance, we used 33,216 En-
glish translations from the Linguistic Data Con-
sortium’s Multiple Translation Chinese (MTC) cor-
pora (Huang et al., 2002). We randomly selected
100 1- to 4-grams that appeared in both Europarl
and MTC sentences (excluding stop words, num-
bers, and phrases containing periods and commas).
For each of those 100 phrases, we randomly se-
lected a MTC sentence containing that phrase. We
then replaced the phrase in the sentence with each
paraphrase output by the systems, and evaluated the
correctness of the paraphrase in the context of the
sentence. We had two volunteers manually score
the paraphrases on a 3-point scale (Table 2), using
a simplified version of the scoring system used by
Callison-Burch (2008). We deemed a paraphrase
to be correct if it was scored 1 or 2, and wrong
if it was scored 0. Evaluation was blind, and the
paraphrases were presented randomly to the volun-
teers. The Kappa measure of inter-annotator agree-
ment was 0.62, which indicates substantial agree-
ment between the evaluators. We took the average
score for each paraphrase.
The parameters used for HTP were as follows
(see Table 1 for parameter descriptions): d =
6, n = 50, 000, m = 1, 000, 000, T = 10, S =
0.05, l = 20, fphrase = 0.1, fngram = 0.1, fsyntax =
0.4, fsubstrzng = 0.4. (e &lt; 0.03 with these values of
n, m, T, and S.)
</bodyText>
<figure confidence="0.645505333333333">
P(E&apos;|E) �
E
FEC
E
A
CEC
</figure>
<page confidence="0.991649">
150
</page>
<tableCaption confidence="0.999413">
Table 3: HTP vs. SBP.
</tableCaption>
<table confidence="0.9997566">
HTP SBP
Correct top-1 paraphrases 71% 53%
Correct top-k paraphrases 54% 39%
Count of correct paraphrases 420 145
Correct paraphrases 43% 39%
</table>
<tableCaption confidence="0.997047">
Table 4: HTP vs. HTP without feature nodes.
</tableCaption>
<table confidence="0.9997845">
HTP HTP-
NoFeatNodes
Correct top-1 paraphrases 61% 41%
Correct top-k paraphrases 43% 29%
Count of correct paraphrases 420 283
Correct paraphrases 43% 29%
</table>
<tableCaption confidence="0.8962875">
Table 5: HTP vs. HTP with bipart
ite graph.
</tableCaption>
<table confidence="0.99911">
HTP HTP-
Bipart
ite
Correct top-1 paraphrases 62% 58%
Correct top-k paraphrases 46% 41%
Count of correct paraphrases 420 361
Correct paraphrases 43% 41%
</table>
<subsectionHeader confidence="0.342959">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.185765">
(420 versus 145), it also has
</bodyText>
<equation confidence="0.3706202">
the
results are over
query
phrases. We see that feature nodes boost
per-
</equation>
<bodyText confidence="0.897820045454545">
formance, allowing HTP to return more correct para-
phrases (420 versus 283), an
“top-1” and“top-k”
all 100
HTP’s
d at higher precision
(43% versus 29%).
performance. HTP-Bipartite refers
to HTP that is given a set consisting only of English-
foreign phrase alignment as input. HTP-Bipartite
does not return paraphrases for 5 query phrases.
Thus, in Table 5, the
and
results are
for the 95 query phrases for which both systems re-
turn paraphrases. From the better performance of
HTP, we see that the foreign phrase alignments help
in finding Engli
HTP’s
“top-1”
“top-k”
sh paraphrases.
</bodyText>
<sectionHeader confidence="0.99986" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.992315636363636">
HTP versus SBP. Comparison between HTP and
SBP is complicated by the fact that the two systems
did not output the same number of paraphrases for
the 100 query phrases. HTP output paraphrases for
all the query phrases, but SBP only did so for 49
query phrases. Of those 49 query phrases, HTP re-
turned at least as many paraphrases as SBP, and for
many it returned more.
To provide a fair comparison, we present results
both for these 49 query phrases, and for all para-
phrases returned by each of the systems. The up-
per half of Table 3 shows results for the 49 query
phrases. The first row of Table 3 reports the per-
centage of top-1 paraphrases from this set that are
correct, while the second row reports the percentage
of correct top-k paraphrases from this set, where k is
the number of queries returned by SBP, and is lim-
ited to at most 10. The value of k may differ for
each query: if SBP and HTP return 3 and 20 para-
phrases respectively, we only consider the top 3. On
the third and fourth rows, we present the number
of correct paraphrases and the percentage of correct
paraphrases among the top 10 paraphrases returned
by HTP for all 100 queries and the corresponding
figures for the 49 queries for SBP. (When a sys-
tem returned fewer than 10 paraphrases for a query,
we consider all the paraphrases for that query.) It
is evident from Table 3 that HTP consistently out-
performs SBP: not only does it return more cor-
rect paraphrases overall
higher precision (43% versus 39%)
HTP and SBP respectively took 48 and 468 sec-
onds per query on a 3 GHz machine. The times are
not directly comparable because the systems are im-
plemented in different languages (HTP in C# and
SBP in Java), and use different data str
uctures.
HTP without Feature Nodes. Both HTP and HTP
minus feature nodes output paraphrases for each of
the 100 query phrases. Table 4 compares perfor-
mance in the same manner as in Table 3, except that
HTP with Bipartite Graph. Lastly, we investi-
gate the contribution of alignments between foreign
phrases to
Random walks and hitting times have been suc-
cessfully applied to a variety of applications.
Brand (2005) has used hitting times for collabora-
tive filtering, in which product recommendations to
users are made based on purchase history. In com-
puter vision, hitting times have been used to de-
termine object shape from silhouettes (Gorelick et
al., 2004), and for image segmentation (Grady and
Schwartz, 2006). In social network analysis, Liben-
Nowell an
d Kleinberg (2003) have investigated the
</bodyText>
<page confidence="0.995288">
151
</page>
<bodyText confidence="0.999975466666667">
use of hitting times for predicting relationships be-
tween entities. Recently, Mei et al. (2008) have used
the hitting times of nodes in a bipartite graph cre-
ated from search engine query logs to find related
queries. They used an iterative algorithm to compute
the hitting time, which converges slowly on large
graphs. In HTP, we have sought to obviate this issue
by using truncated hitting time that can be computed
efficiently by sampling random walks.
Several approaches have been proposed to learn
paraphrases. Barzilay and Mckeown (2001) acquire
paraphrases from a monolingual parallel corpus us-
ing a co-training algorithm. Their co-trained classi-
fier determines whether two phrases are paraphrases
of one another using their surrounding contexts. Lin
and Pantel (2001) learn paraphrases using the dis-
tributional similarity of paths in dependency trees.
Ibrahim et al. (2003) generalize syntactic paths in
aligned monolingual sentence pairs in order to gen-
erate paraphrases. Pang et al. (2003) merge parse
trees of monolingual sentence pairs, and then com-
press the merged tree into a word lattice that can sub-
sequently be used to generate paraphrases. Recently,
Zhao et al. (2008) used dependency parses to learn
paraphrase patterns that include part-of-speech slots.
In other recent work, Das and Smith (2009) use a
generative model for paraphrase detection. Rather
than outputing paraphrases of an input phrase, their
system detects whether two input sentences are para-
phrases of one another.
</bodyText>
<sectionHeader confidence="0.996435" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999896">
We have introduced HTP, a novel approach based
on random walks and hitting times for the learning
of paraphrases from bilingual parallel corpora. HTP
works by converting aligned phrases into a graph,
and finding paraphrases that are “close” to a phrase
of interest. Compared to previous approaches, HTP
is able to find more paraphrases by traversing paths
of lengths longer than 2; utilizes information in the
edges between foreign phrase nodes; and allows do-
main knowledge to be easily incorporated. Empir-
ical results show its effectiveness in learning new
paraphrases.
As future work, we plan to learn the distribution
of weights on edges to phrase nodes and feature
nodes automatically from data, rather than tuning
them manually, and to develop a probabilistic model
supporting HTP. We intend also to apply HTP to
learning paraphrases in languages other than English
and investigate the impact of the learned paraphrases
on resource-sparse machine translation systems.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999941142857143">
This work was done while the first author was an
intern at Microsoft Research. We would like to
thank Xiaodong He, Jianfeng Gao, Chris Quirk,
Kristina Toutanova, Bob Moore, and other mem-
bers of the MSR NLP group, along with Dengyong
Zhou (TMSN) for their insightful comments and as-
sistance in the course of this project.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996112323529412">
David Aldous and Jim Fill. 2001. Reversible
Markov Chains and Random Walks on Graphs.
http://www.stat.berkeley.edu/~aldous/RWG/book.html.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the ACL, pages
597–604.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL,
pages 16–23.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of the 39th Annual Meeting of the ACL, pages 50–57.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
37th Annual Meeting of the ACL, pages 550–557.
Matthew Brand. 2005. A random walks perspective on
maximizing satisfaction and profit. In Proceedings of
the 8th SIAM Conference on Optimization.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT/NAACL,
pages 17–24.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, pages 196–205.
Yu Chen, Martin Kay, and Andreas Eisele. 2009. Inter-
secting multilingual data for faster and better statistical
translations. In Proceedings of HLT/NAACL.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
</reference>
<page confidence="0.978179">
152
</page>
<reference confidence="0.99974200990099">
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing.
Lena Gorelick, Meirav Galun, Eitan Sharon, Ronen
Basri, and Achi Brandt. 2004. Shape representation
and classification using the Poisson equation. In Pro-
ceedings of the Conference on Computer Vision and
Pattern Recognition.
Leo Grady and Eric L. Schwartz. 2006. Isoperimet-
ric graph partitioning for image segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 28:469–475.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-translation Chinese corpus. Linguistic
Data Consortium, Philadelphia.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned monolin-
gual corpora. In Proceedings of the 2nd International
Workshop on Paraphrasing, pages 57–64.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu`ere. 1991. Lexical selection and paraphrase in
a meaning-text generation model. In C´ecile L. Paris,
William R. Swartout, and William C. Mann, editors,
Natural Language Generation in Artificial Intelligence
and Computational Linguistics. Kluwer Academic.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
HLT/NAACL, pages 455–462.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit.
Nils Lenke. 1994. Anticipating the reader’s problems
and the automatic generation of paraphrases. In Pro-
ceedings of the 15th Conference on Computational
Linguistics, pages 319–323.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT/NAACL,
pages 104–111.
David Liben-Nowell and Jon Kleinberg. 2003. The link
prediction problem for social networks. In Proceed-
ings of the 12th International Conference on Informa-
tion and Knowledge, pages 556–559.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Proceedings
of ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining, pages 323–328.
L´aszl´o Lov´asz. 1996. Random walks on graphs: A sur-
vey. In D. Mikl´os, V. T. S´os, and T. Sz˝onyi, editors,
Combinatorics, Paul Erd˝os is Eighty, Vol. 2, pages
353–398.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for param-
eter tuning in statistical machine translation. In Pro-
ceedings of the 2nd Workshop on Statistical Machine
Translation, pages 120–127.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia’s Newsblaster. In Pro-
ceedings of the 2nd International Conference on HLT
Research, pages 280–285.
Qiaozhu Mei, Dengyong Zhou, and Kenneth Church.
2008. Query suggestion using hitting time. In Pro-
ceeding of the 17th ACM Conference on Information
and Knowledge Management, pages 469–478.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417–449.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL, pages 102–109.
Chris Quirk, Chris Brockett, and William B. Dolan.
2004. Monolingual machine translation for paraphrase
generation. In Proceedings of EMNLP, pages 142–
149.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the ACL,
pages 41–47.
Stefan Reizler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the ACL.
Purnamrita Sarkar and Andrew W. Moore. 2007.
A tractable approach to finding closest truncated-
commute-time neighbors in large graphs. In Proceed-
ings of the 23th Conference on Uncertainty in Artificial
Intelligence.
Purnamrita Sarkar, Andrew W. Moore, and Amit Prakash.
2008. Fast incremental proximity search in large
graphs. In Proceedings of the 25th International Con-
ference on Machine Learning.
Manfred Stede. 1999. Lexical Semantics and Knowl-
edge Representation in Multilingual Text Generation.
Kluwer Academic Publishers.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings ofACL.
</reference>
<page confidence="0.99923">
153
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.316275">
<title confidence="0.999952">Hitting the Right Paraphrases in Good Time</title>
<author confidence="0.978521">Stanley</author>
<affiliation confidence="0.999845">Department of Computer Science &amp; University of</affiliation>
<address confidence="0.998661">Seattle, WA 98195,</address>
<email confidence="0.999459">koks@cs.washington.edu</email>
<author confidence="0.866842">Chris</author>
<affiliation confidence="0.888836">Microsoft</affiliation>
<address confidence="0.8800835">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.999668">chrisbkt@microsoft.com</email>
<abstract confidence="0.977466833333334">We present a random-walk-based approach to learning paraphrases from bilingual parallel corpora. The corpora are represented as a graph in which a node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned in a phrase table. We sample random walks to compute the average number of steps it takes to reach a ranking of paraphrases with better ones being “closer” to a phrase of interest. This approach allows “feature” nodes that represent domain knowledge to be built into the graph, and incorporates truncation techniques to prevent the graph from growing too large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Aldous</author>
<author>Jim Fill</author>
</authors>
<title>Reversible Markov Chains and Random Walks on Graphs.</title>
<date>2001</date>
<note>http://www.stat.berkeley.edu/~aldous/RWG/book.html.</note>
<contexts>
<context position="3878" citStr="Aldous and Fill, 2001" startWordPosition="600" endWordPosition="603">at may aid in learning paraphrases. Further, by only considering English nodes that are linked via a foreign language node as potential paraphrases, 145 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 145–153, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics these approaches will fail to find paraphrases separated by distances greater than length two. In this paper, we present HTP (Hitting Time Paraphraser), a paraphrase learning approach that is based on random walks (Lov´asz, 1996) and hitting times (Aldous and Fill, 2001). Hitting time measures the average number of steps one needs to take in a random traversal of a graph before reaching a destination node from a source node. Intuitively, the smaller the hitting time from a phrase E to E&apos; (i.e., the closer E&apos; is to E), the more likely it is that E&apos; is a good paraphrase of E. The advantages of HTP are as follows: • By traversing paths of lengths greater than two, our approach is able to find more paraphrases of a given phrase. • We do not require the graph to be bipartite. Edges can exist between nodes of different foreign languages if their corresponding phras</context>
<context position="5917" citStr="Aldous and Fill, 2001" startWordPosition="979" endWordPosition="982"> j E V . Associated with the graph is a |V |x |V | adjacency matrix W. Each entry Wij in the matrix is the weight of edge (i, j), or zero if the edge does not exist. In a random walk (Lov´asz, 1996), we traverse from node to node via the edges. Suppose at time step t, we are at node i. In the next step, we move to its neighbor j with probability proportional to the weight of the edge (i, j), i.e., with probability Wij/ Ej Wij. This probability is known as the transition probability from i to j. Note that the transition probabilities from a node to its neighbors sum to 1. The hitting time hij (Aldous and Fill, 2001) from node i to j is defined as the average number of steps one takes in a random walk starting from i to visit j for the first time. Hitting time has the property of being robust to noise. This is a desirable property for our system which works on bilingual parallel corpora containing numerous spurious alignments between phrases (i.e., edges between nodes). However, as observed by Liben-Nowell and Kleinberg (2003), hitting time has the drawback of being sensitive to portions of the graph that are far from the start node because it considers paths of length up to oc. To circumvent this problem</context>
</contexts>
<marker>Aldous, Fill, 2001</marker>
<rawString>David Aldous and Jim Fill. 2001. Reversible Markov Chains and Random Walks on Graphs. http://www.stat.berkeley.edu/~aldous/RWG/book.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>597--604</pages>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the ACL, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1952" citStr="Barzilay and Lee, 2003" startWordPosition="294" endWordPosition="297">ive ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiplesequence alignment. In Proceedings of HLT/NAACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="1928" citStr="Barzilay and McKeown, 2001" startWordPosition="290" endWordPosition="293">ing paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “p</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting of the ACL, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="1606" citStr="Barzilay et al., 1999" startWordPosition="244" endWordPosition="247">partite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard an</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>Regina Barzilay, Kathleen McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the ACL, pages 550–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Brand</author>
</authors>
<title>A random walks perspective on maximizing satisfaction and profit.</title>
<date>2005</date>
<booktitle>In Proceedings of the 8th SIAM Conference on Optimization.</booktitle>
<contexts>
<context position="27015" citStr="Brand (2005)" startWordPosition="4722" endWordPosition="4723">and 468 seconds per query on a 3 GHz machine. The times are not directly comparable because the systems are implemented in different languages (HTP in C# and SBP in Java), and use different data str uctures. HTP without Feature Nodes. Both HTP and HTP minus feature nodes output paraphrases for each of the 100 query phrases. Table 4 compares performance in the same manner as in Table 3, except that HTP with Bipartite Graph. Lastly, we investigate the contribution of alignments between foreign phrases to Random walks and hitting times have been successfully applied to a variety of applications. Brand (2005) has used hitting times for collaborative filtering, in which product recommendations to users are made based on purchase history. In computer vision, hitting times have been used to determine object shape from silhouettes (Gorelick et al., 2004), and for image segmentation (Grady and Schwartz, 2006). In social network analysis, LibenNowell an d Kleinberg (2003) have investigated the 151 use of hitting times for predicting relationships between entities. Recently, Mei et al. (2008) have used the hitting times of nodes in a bipartite graph created from search engine query logs to find related q</context>
</contexts>
<marker>Brand, 2005</marker>
<rawString>Matthew Brand. 2005. A random walks perspective on maximizing satisfaction and profit. In Proceedings of the 8th SIAM Conference on Optimization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1789" citStr="Callison-Burch et al., 2006" startWordPosition="270" endWordPosition="273">enerated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual </context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of HLT/NAACL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>196--205</pages>
<contexts>
<context position="1266" citStr="Callison-Burch (2008)" startWordPosition="192" endWordPosition="193">f paraphrases with better ones being “closer” to a phrase of interest. This approach allows “feature” nodes that represent domain knowledge to be built into the graph, and incorporates truncation techniques to prevent the graph from growing too large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on </context>
<context position="2557" citStr="Callison-Burch (2008)" startWordPosition="393" endWordPosition="394">ay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refines BCB with a system that constrains paraphrases to have the same syntactic structure (Syntactic Bilingual Phrases: SBP). We take a graphical view of the state-of-the-art BCB and SBP approaches by representing the bilingual parallel corpora as a graph. A node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned. This graphical form makes the limitations of the BCB/SBP approaches more evident. The BCB/SBP graph is limited to be bipartite with English nodes on one side and foreign language nodes on the other, and an edge can only </context>
<context position="16281" citStr="Callison-Burch 2008" startWordPosition="2882" endWordPosition="2883">e”, etc.), articles (“the”, “a”, “an”), etc. For each class c, we create syntax nodes ac and a&apos; to respectively represent the conditions that a phrase begins and ends with a word in class c. Directed edges (ac, j) and (j, ac) are added if node j starts with a word in class c (similarly we add (a&apos;, j) and (j, a&apos;) if it ends with a word in class c). For example, in Figure 2(b), “the objective is” is linked to “starts with article” because it begins with “the”. These syntax nodes allow HTP to capture broad commonalities about structural distribution, without requiring syntactic equivalence as in Callison-Burch 2008 (or the use of a parser). Third, we have “not-substring/superstring-of” nodes. We observed that many English phrases (e.g., “reach the objective” and “reach the”) that are superstrings or substrings of each other tend to be aligned to several shared non-English phrases in the bilingual parallel corpora used in our experiments. Most such English phrase pairs are not paraphrases, but they are linked by many short paths via their common aligned foreign phrase, and thus have small hitting times. To counteract this, we create a “notsubstring/superstring-of” node a. The query node i is always conne</context>
<context position="18572" citStr="Callison-Burch (2008)" startWordPosition="3270" endWordPosition="3271">of the nodes, and return the English phrase nodes in order of increasing hitting times. 4 Experiments We conducted experiments to investigate how HTP compares with the state of the art, and to evaluate the contributions of its components. 4.1 Dataset We used the Europarl dataset (Koehn, 2005) for our experiments. This dataset contains English transcripts of the proceedings of the European Parliament, and their translations into 10 other European languages. In the dataset, there are about a million sentences per language, and English sentences are aligned with sentences in the other languages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in Liang et al. (2006). We further used the technique of Chen et al. (2009) to remove a</context>
<context position="20035" citStr="Callison-Burch, 2008" startWordPosition="3516" endWordPosition="3517">d Finnish as the bridge language; when either F or G is Finnish, we used Spanish as the bridge language; when F and G were Finnish and Spanish, we used English as the bridge language. In our experiments, we used phrases of length 1 to 4 of the following six languages: English, Danish, German, Spanish, Finnish, 149 and Dutch. All the phrasal alignments between each pair of languages (15 in total) were used as input to HTP and its comparison systems. A small subset of the remaining phrase alignments were used for tuning parameters. 4.2 Systems We compared HTP to the state-of-the-art SBP system (Callison-Burch, 2008). We also investigated the contribution of the feature nodes by running HTP without them. In addition, we ran HTP on a bipartite graph, i.e., one created from English-foreign phrase alignments only without any phrase alignments between foreign languages. We used Callison-Burch (2008)’s implementation of SBP that is publicly available at http://ironman.jhu.edu/emnlp08.tar. SBP is based on BCB (Bannard and Callison- Burch, 2005) which computes the probability that English phrase E&apos; is a paraphrase of E using the following formula: P(E&apos;|E) � E E P(E&apos;|F)P(F|E) (2) CEC FEC where C is set of bilingu</context>
<context position="22997" citStr="Callison-Burch (2008)" startWordPosition="4001" endWordPosition="4002"> (MTC) corpora (Huang et al., 2002). We randomly selected 100 1- to 4-grams that appeared in both Europarl and MTC sentences (excluding stop words, numbers, and phrases containing periods and commas). For each of those 100 phrases, we randomly selected a MTC sentence containing that phrase. We then replaced the phrase in the sentence with each paraphrase output by the systems, and evaluated the correctness of the paraphrase in the context of the sentence. We had two volunteers manually score the paraphrases on a 3-point scale (Table 2), using a simplified version of the scoring system used by Callison-Burch (2008). We deemed a paraphrase to be correct if it was scored 1 or 2, and wrong if it was scored 0. Evaluation was blind, and the paraphrases were presented randomly to the volunteers. The Kappa measure of inter-annotator agreement was 0.62, which indicates substantial agreement between the evaluators. We took the average score for each paraphrase. The parameters used for HTP were as follows (see Table 1 for parameter descriptions): d = 6, n = 50, 000, m = 1, 000, 000, T = 10, S = 0.05, l = 20, fphrase = 0.1, fngram = 0.1, fsyntax = 0.4, fsubstrzng = 0.4. (e &lt; 0.03 with these values of n, m, T, and </context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Proceedings of EMNLP, pages 196–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Chen</author>
<author>Martin Kay</author>
<author>Andreas Eisele</author>
</authors>
<title>Intersecting multilingual data for faster and better statistical translations.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="19160" citStr="Chen et al. (2009)" startWordPosition="3360" endWordPosition="3363">nguages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in Liang et al. (2006). We further used the technique of Chen et al. (2009) to remove a phrase alignment F-G (where F and G are phrases in different foreign languages) if it was always aligned to different phrases in a third “bridge” foreign language. As observed by Chen et al., this helped to remove spurious alignments. We used Finnish as the bridge language; when either F or G is Finnish, we used Spanish as the bridge language; when F and G were Finnish and Spanish, we used English as the bridge language. In our experiments, we used phrases of length 1 to 4 of the following six languages: English, Danish, German, Spanish, Finnish, 149 and Dutch. All the phrasal ali</context>
</contexts>
<marker>Chen, Kay, Eisele, 2009</marker>
<rawString>Yu Chen, Martin Kay, and Andreas Eisele. 2009. Intersecting multilingual data for faster and better statistical translations. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="28721" citStr="Das and Smith (2009)" startWordPosition="4988" endWordPosition="4991">aphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather than outputing paraphrases of an input phrase, their system detects whether two input sentences are paraphrases of one another. 6 Conclusion and Future Work We have introduced HTP, a novel approach based on random walks and hitting times for the learning of paraphrases from bilingual parallel corpora. HTP works by converting aligned phrases into a graph, and finding paraphrases that are “close” to a phrase of interest. Compared to previous approaches, HTP is able to find more paraphrases by traversing paths of lengths longer than 2; util</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lena Gorelick</author>
<author>Meirav Galun</author>
<author>Eitan Sharon</author>
<author>Ronen Basri</author>
<author>Achi Brandt</author>
</authors>
<title>Shape representation and classification using the Poisson equation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="27261" citStr="Gorelick et al., 2004" startWordPosition="4760" endWordPosition="4763">oth HTP and HTP minus feature nodes output paraphrases for each of the 100 query phrases. Table 4 compares performance in the same manner as in Table 3, except that HTP with Bipartite Graph. Lastly, we investigate the contribution of alignments between foreign phrases to Random walks and hitting times have been successfully applied to a variety of applications. Brand (2005) has used hitting times for collaborative filtering, in which product recommendations to users are made based on purchase history. In computer vision, hitting times have been used to determine object shape from silhouettes (Gorelick et al., 2004), and for image segmentation (Grady and Schwartz, 2006). In social network analysis, LibenNowell an d Kleinberg (2003) have investigated the 151 use of hitting times for predicting relationships between entities. Recently, Mei et al. (2008) have used the hitting times of nodes in a bipartite graph created from search engine query logs to find related queries. They used an iterative algorithm to compute the hitting time, which converges slowly on large graphs. In HTP, we have sought to obviate this issue by using truncated hitting time that can be computed efficiently by sampling random walks. </context>
</contexts>
<marker>Gorelick, Galun, Sharon, Basri, Brandt, 2004</marker>
<rawString>Lena Gorelick, Meirav Galun, Eitan Sharon, Ronen Basri, and Achi Brandt. 2004. Shape representation and classification using the Poisson equation. In Proceedings of the Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Grady</author>
<author>Eric L Schwartz</author>
</authors>
<title>Isoperimetric graph partitioning for image segmentation.</title>
<date>2006</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>28--469</pages>
<contexts>
<context position="27316" citStr="Grady and Schwartz, 2006" startWordPosition="4768" endWordPosition="4771">es for each of the 100 query phrases. Table 4 compares performance in the same manner as in Table 3, except that HTP with Bipartite Graph. Lastly, we investigate the contribution of alignments between foreign phrases to Random walks and hitting times have been successfully applied to a variety of applications. Brand (2005) has used hitting times for collaborative filtering, in which product recommendations to users are made based on purchase history. In computer vision, hitting times have been used to determine object shape from silhouettes (Gorelick et al., 2004), and for image segmentation (Grady and Schwartz, 2006). In social network analysis, LibenNowell an d Kleinberg (2003) have investigated the 151 use of hitting times for predicting relationships between entities. Recently, Mei et al. (2008) have used the hitting times of nodes in a bipartite graph created from search engine query logs to find related queries. They used an iterative algorithm to compute the hitting time, which converges slowly on large graphs. In HTP, we have sought to obviate this issue by using truncated hitting time that can be computed efficiently by sampling random walks. Several approaches have been proposed to learn paraphra</context>
</contexts>
<marker>Grady, Schwartz, 2006</marker>
<rawString>Leo Grady and Eric L. Schwartz. 2006. Isoperimetric graph partitioning for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28:469–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shudong Huang</author>
<author>David Graff</author>
<author>George Doddington</author>
</authors>
<title>Multiple-translation Chinese corpus. Linguistic Data Consortium,</title>
<date>2002</date>
<location>Philadelphia.</location>
<contexts>
<context position="22411" citStr="Huang et al., 2002" startWordPosition="3903" endWordPosition="3906">ment or wrong tense), or meaning is largely preserved but not completely 2 Totally correct; grammatically correct and meaning is preserved phrase with all subtrees that have the phrase as their leaves. SBP thus offers several ways of choosing which syntactic structure a phrase should be associated with. In our experiments, we used the best performing method of averaging Equation 3 over all syntactic structures that E is associated with. 4.3 Methodology To evaluate performance, we used 33,216 English translations from the Linguistic Data Consortium’s Multiple Translation Chinese (MTC) corpora (Huang et al., 2002). We randomly selected 100 1- to 4-grams that appeared in both Europarl and MTC sentences (excluding stop words, numbers, and phrases containing periods and commas). For each of those 100 phrases, we randomly selected a MTC sentence containing that phrase. We then replaced the phrase in the sentence with each paraphrase output by the systems, and evaluated the correctness of the paraphrase in the context of the sentence. We had two volunteers manually score the paraphrases on a 3-point scale (Table 2), using a simplified version of the scoring system used by Callison-Burch (2008). We deemed a </context>
</contexts>
<marker>Huang, Graff, Doddington, 2002</marker>
<rawString>Shudong Huang, David Graff, and George Doddington. 2002. Multiple-translation Chinese corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Ibrahim</author>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Extracting structural paraphrases from aligned monolingual corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd International Workshop on Paraphrasing,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="28286" citStr="Ibrahim et al. (2003)" startWordPosition="4919" endWordPosition="4922">e the hitting time, which converges slowly on large graphs. In HTP, we have sought to obviate this issue by using truncated hitting time that can be computed efficiently by sampling random walks. Several approaches have been proposed to learn paraphrases. Barzilay and Mckeown (2001) acquire paraphrases from a monolingual parallel corpus using a co-training algorithm. Their co-trained classifier determines whether two phrases are paraphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather than outputing paraphrases of an input phrase, their system detects whether two input sentences are paraphra</context>
</contexts>
<marker>Ibrahim, Katz, Lin, 2003</marker>
<rawString>Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extracting structural paraphrases from aligned monolingual corpora. In Proceedings of the 2nd International Workshop on Paraphrasing, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidija Iordanskaja</author>
<author>Richard Kittredge</author>
<author>Alain Polgu`ere</author>
</authors>
<title>Lexical selection and paraphrase in a meaning-text generation model.</title>
<date>1991</date>
<booktitle>Natural Language Generation in Artificial Intelligence and Computational Linguistics.</booktitle>
<editor>In C´ecile L. Paris, William R. Swartout, and William C. Mann, editors,</editor>
<publisher>Kluwer Academic.</publisher>
<marker>Iordanskaja, Kittredge, Polgu`ere, 1991</marker>
<rawString>Lidija Iordanskaja, Richard Kittredge, and Alain Polgu`ere. 1991. Lexical selection and paraphrase in a meaning-text generation model. In C´ecile L. Paris, William R. Swartout, and William C. Mann, editors, Natural Language Generation in Artificial Intelligence and Computational Linguistics. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="1760" citStr="Kauchak and Barzilay, 2006" startWordPosition="266" endWordPosition="269">edge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to </context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of HLT/NAACL, pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit.</booktitle>
<contexts>
<context position="18244" citStr="Koehn, 2005" startWordPosition="3222" endWordPosition="3223">ges from i to n-gram nodes will have weight f&amp;quot;am �. Likewise for edges to the other two kinds of feature nodes. Each of the k outgoing edges from a feature node is simply set to have a weight of 1. After adding the feature nodes, we again run M independent length-T random walks to estimate the truncated hitting times of the nodes, and return the English phrase nodes in order of increasing hitting times. 4 Experiments We conducted experiments to investigate how HTP compares with the state of the art, and to evaluate the contributions of its components. 4.1 Dataset We used the Europarl dataset (Koehn, 2005) for our experiments. This dataset contains English transcripts of the proceedings of the European Parliament, and their translations into 10 other European languages. In the dataset, there are about a million sentences per language, and English sentences are aligned with sentences in the other languages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of differe</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils Lenke</author>
</authors>
<title>Anticipating the reader’s problems and the automatic generation of paraphrases.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th Conference on Computational Linguistics,</booktitle>
<pages>319--323</pages>
<contexts>
<context position="1697" citStr="Lenke, 1994" startWordPosition="259" endWordPosition="260">erally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpo</context>
</contexts>
<marker>Lenke, 1994</marker>
<rawString>Nils Lenke. 1994. Anticipating the reader’s problems and the automatic generation of paraphrases. In Proceedings of the 15th Conference on Computational Linguistics, pages 319–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="19107" citStr="Liang et al. (2006)" startWordPosition="3350" endWordPosition="3353">h sentences are aligned with sentences in the other languages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in Liang et al. (2006). We further used the technique of Chen et al. (2009) to remove a phrase alignment F-G (where F and G are phrases in different foreign languages) if it was always aligned to different phrases in a third “bridge” foreign language. As observed by Chen et al., this helped to remove spurious alignments. We used Finnish as the bridge language; when either F or G is Finnish, we used Spanish as the bridge language; when F and G were Finnish and Spanish, we used English as the bridge language. In our experiments, we used phrases of length 1 to 4 of the following six languages: English, Danish, German,</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT/NAACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Liben-Nowell</author>
<author>Jon Kleinberg</author>
</authors>
<title>The link prediction problem for social networks.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th International Conference on Information and Knowledge,</booktitle>
<pages>556--559</pages>
<contexts>
<context position="6335" citStr="Liben-Nowell and Kleinberg (2003)" startWordPosition="1053" endWordPosition="1056">robability Wij/ Ej Wij. This probability is known as the transition probability from i to j. Note that the transition probabilities from a node to its neighbors sum to 1. The hitting time hij (Aldous and Fill, 2001) from node i to j is defined as the average number of steps one takes in a random walk starting from i to visit j for the first time. Hitting time has the property of being robust to noise. This is a desirable property for our system which works on bilingual parallel corpora containing numerous spurious alignments between phrases (i.e., edges between nodes). However, as observed by Liben-Nowell and Kleinberg (2003), hitting time has the drawback of being sensitive to portions of the graph that are far from the start node because it considers paths of length up to oc. To circumvent this problem, Sarkar and Moore (2007) introduced the notion of truncated hitting time where random walks are limited to have at most T steps. The truncated hitting time hT ij from node i to j is defined as the average number of steps one takes to reach j for the first time starting from i in a random walk that is limited to at most T steps. hT ij is defined to be 0 if i = j or T = 0, and to be T if j is not reach in T steps. A</context>
</contexts>
<marker>Liben-Nowell, Kleinberg, 2003</marker>
<rawString>David Liben-Nowell and Jon Kleinberg. 2003. The link prediction problem for social networks. In Proceedings of the 12th International Conference on Information and Knowledge, pages 556–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="1507" citStr="Lin and Pantel, 2001" startWordPosition="229" endWordPosition="232">oo large for efficiency. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limi</context>
<context position="28180" citStr="Lin and Pantel (2001)" startWordPosition="4903" endWordPosition="4906"> created from search engine query logs to find related queries. They used an iterative algorithm to compute the hitting time, which converges slowly on large graphs. In HTP, we have sought to obviate this issue by using truncated hitting time that can be computed efficiently by sampling random walks. Several approaches have been proposed to learn paraphrases. Barzilay and Mckeown (2001) acquire paraphrases from a monolingual parallel corpus using a co-training algorithm. Their co-trained classifier determines whether two phrases are paraphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather th</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´aszl´o Lov´asz</author>
</authors>
<title>Random walks on graphs: A survey. In</title>
<date>1996</date>
<volume>2</volume>
<pages>353--398</pages>
<editor>D. Mikl´os, V. T. S´os, and T. Sz˝onyi, editors, Combinatorics, Paul Erd˝os is Eighty,</editor>
<marker>Lov´asz, 1996</marker>
<rawString>L´aszl´o Lov´asz. 1996. Random walks on graphs: A survey. In D. Mikl´os, V. T. S´os, and T. Sz˝onyi, editors, Combinatorics, Paul Erd˝os is Eighty, Vol. 2, pages 353–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil Ayan</author>
<author>Philip Resnik</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2nd Workshop on Statistical Machine Translation,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="1812" citStr="Madnani et al., 2007" startWordPosition="274" endWordPosition="277">r approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hyp</context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie J. Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In Proceedings of the 2nd Workshop on Statistical Machine Translation, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and summarizing news on a daily basis with Columbia’s Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on HLT Research,</booktitle>
<pages>280--285</pages>
<contexts>
<context position="1629" citStr="McKeown et al., 2002" startWordPosition="248" endWordPosition="251"> finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) </context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with Columbia’s Newsblaster. In Proceedings of the 2nd International Conference on HLT Research, pages 280–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Dengyong Zhou</author>
<author>Kenneth Church</author>
</authors>
<title>Query suggestion using hitting time.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>469--478</pages>
<contexts>
<context position="27501" citStr="Mei et al. (2008)" startWordPosition="4797" endWordPosition="4800">between foreign phrases to Random walks and hitting times have been successfully applied to a variety of applications. Brand (2005) has used hitting times for collaborative filtering, in which product recommendations to users are made based on purchase history. In computer vision, hitting times have been used to determine object shape from silhouettes (Gorelick et al., 2004), and for image segmentation (Grady and Schwartz, 2006). In social network analysis, LibenNowell an d Kleinberg (2003) have investigated the 151 use of hitting times for predicting relationships between entities. Recently, Mei et al. (2008) have used the hitting times of nodes in a bipartite graph created from search engine query logs to find related queries. They used an iterative algorithm to compute the hitting time, which converges slowly on large graphs. In HTP, we have sought to obviate this issue by using truncated hitting time that can be computed efficiently by sampling random walks. Several approaches have been proposed to learn paraphrases. Barzilay and Mckeown (2001) acquire paraphrases from a monolingual parallel corpus using a co-training algorithm. Their co-trained classifier determines whether two phrases are par</context>
</contexts>
<marker>Mei, Zhou, Church, 2008</marker>
<rawString>Qiaozhu Mei, Dengyong Zhou, and Kenneth Church. 2008. Query suggestion using hitting time. In Proceeding of the 17th ACM Conference on Information and Knowledge Management, pages 469–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="18673" citStr="Och and Ney, 2004" startWordPosition="3285" endWordPosition="3288"> conducted experiments to investigate how HTP compares with the state of the art, and to evaluate the contributions of its components. 4.1 Dataset We used the Europarl dataset (Koehn, 2005) for our experiments. This dataset contains English transcripts of the proceedings of the European Parliament, and their translations into 10 other European languages. In the dataset, there are about a million sentences per language, and English sentences are aligned with sentences in the other languages. Callison-Burch (2008) aligned English phrases with phrases in each of the other languages using Giza++ (Och and Ney, 2004). We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar. In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in Liang et al. (2006). We further used the technique of Chen et al. (2009) to remove a phrase alignment F-G (where F and G are phrases in different foreign languages) if it was always ali</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="1971" citStr="Pang et al., 2003" startWordPosition="298" endWordPosition="301">he same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refin</context>
<context position="28404" citStr="Pang et al. (2003)" startWordPosition="4937" endWordPosition="4940">ed hitting time that can be computed efficiently by sampling random walks. Several approaches have been proposed to learn paraphrases. Barzilay and Mckeown (2001) acquire paraphrases from a monolingual parallel corpus using a co-training algorithm. Their co-trained classifier determines whether two phrases are paraphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather than outputing paraphrases of an input phrase, their system detects whether two input sentences are paraphrases of one another. 6 Conclusion and Future Work We have introduced HTP, a novel approach based on random walks and hi</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings of HLT/NAACL, pages 102–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William B Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="1992" citStr="Quirk et al., 2004" startWordPosition="302" endWordPosition="305"> an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux of this system (referred to below as ”BCB”) is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the “pivot”). Callison-Burch (2008) further refines BCB with a system </context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William B. Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of EMNLP, pages 142– 149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="1536" citStr="Ravichandran and Hovy, 2002" startWordPosition="233" endWordPosition="236">y. Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and qual</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the 40th Annual Meeting of the ACL, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Reizler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1559" citStr="Reizler et al., 2007" startWordPosition="237" endWordPosition="240">trast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases </context>
</contexts>
<marker>Reizler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Reizler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings of the 45th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Purnamrita Sarkar</author>
<author>Andrew W Moore</author>
</authors>
<title>A tractable approach to finding closest truncatedcommute-time neighbors in large graphs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 23th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="6542" citStr="Sarkar and Moore (2007)" startWordPosition="1090" endWordPosition="1093">rom node i to j is defined as the average number of steps one takes in a random walk starting from i to visit j for the first time. Hitting time has the property of being robust to noise. This is a desirable property for our system which works on bilingual parallel corpora containing numerous spurious alignments between phrases (i.e., edges between nodes). However, as observed by Liben-Nowell and Kleinberg (2003), hitting time has the drawback of being sensitive to portions of the graph that are far from the start node because it considers paths of length up to oc. To circumvent this problem, Sarkar and Moore (2007) introduced the notion of truncated hitting time where random walks are limited to have at most T steps. The truncated hitting time hT ij from node i to j is defined as the average number of steps one takes to reach j for the first time starting from i in a random walk that is limited to at most T steps. hT ij is defined to be 0 if i = j or T = 0, and to be T if j is not reach in T steps. As T —4 oc, hT ij—4 hij. In a recent work, Sarkar et al. (2008) showed that truncated hitting time can be approximated accurately with high probability by sampling. They run M independent length-T random walk</context>
</contexts>
<marker>Sarkar, Moore, 2007</marker>
<rawString>Purnamrita Sarkar and Andrew W. Moore. 2007. A tractable approach to finding closest truncatedcommute-time neighbors in large graphs. In Proceedings of the 23th Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Purnamrita Sarkar</author>
<author>Andrew W Moore</author>
<author>Amit Prakash</author>
</authors>
<title>Fast incremental proximity search in large graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6997" citStr="Sarkar et al. (2008)" startWordPosition="1193" endWordPosition="1196">ensitive to portions of the graph that are far from the start node because it considers paths of length up to oc. To circumvent this problem, Sarkar and Moore (2007) introduced the notion of truncated hitting time where random walks are limited to have at most T steps. The truncated hitting time hT ij from node i to j is defined as the average number of steps one takes to reach j for the first time starting from i in a random walk that is limited to at most T steps. hT ij is defined to be 0 if i = j or T = 0, and to be T if j is not reach in T steps. As T —4 oc, hT ij—4 hij. In a recent work, Sarkar et al. (2008) showed that truncated hitting time can be approximated accurately with high probability by sampling. They run M independent length-T random walks from node i. In m of these runs, node j is visited for the first time at time steps t1j, ... , tmj . The estimated truncated hitting time is given by Em k=1 tk �hT M + (1 − m j ij = M )T (1) They also showed that the number of samples of random walks M has to be at least 1 2~2 log 2nd in order for the estimated truncated hitting time to be a good estimate of the actual truncated hitting time with high probability, i.e., for P (|�hT ij−hT ij|&lt;eT)&gt;1 −</context>
</contexts>
<marker>Sarkar, Moore, Prakash, 2008</marker>
<rawString>Purnamrita Sarkar, Andrew W. Moore, and Amit Prakash. 2008. Fast incremental proximity search in large graphs. In Proceedings of the 25th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>Lexical Semantics and Knowledge Representation in Multilingual Text Generation.</title>
<date>1999</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1711" citStr="Stede, 1999" startWordPosition="261" endWordPosition="262"> easy incorporation of domain knowledge. Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008). 1 Introduction Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Reizler et al., 2007), document summarization (Barzilay et al., 1999; McKeown et al., 2002), natural language generation (Iordanskaja et al., 1991; Lenke, 1994; Stede, 1999), machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Madnani et al., 2007). Early work on paraphrase acquisition has focused on using monolingual parallel corpora (Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned. To address this limitation, Bannard and CallisonBurch (2005) focused their attention on the abundance of bilingual parallel corpora. The crux o</context>
</contexts>
<marker>Stede, 1999</marker>
<rawString>Manfred Stede. 1999. Lexical Semantics and Knowledge Representation in Multilingual Text Generation. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Pivot approach for extracting paraphrase patterns from bilingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="28591" citStr="Zhao et al. (2008)" startWordPosition="4969" endWordPosition="4972"> a monolingual parallel corpus using a co-training algorithm. Their co-trained classifier determines whether two phrases are paraphrases of one another using their surrounding contexts. Lin and Pantel (2001) learn paraphrases using the distributional similarity of paths in dependency trees. Ibrahim et al. (2003) generalize syntactic paths in aligned monolingual sentence pairs in order to generate paraphrases. Pang et al. (2003) merge parse trees of monolingual sentence pairs, and then compress the merged tree into a word lattice that can subsequently be used to generate paraphrases. Recently, Zhao et al. (2008) used dependency parses to learn paraphrase patterns that include part-of-speech slots. In other recent work, Das and Smith (2009) use a generative model for paraphrase detection. Rather than outputing paraphrases of an input phrase, their system detects whether two input sentences are paraphrases of one another. 6 Conclusion and Future Work We have introduced HTP, a novel approach based on random walks and hitting times for the learning of paraphrases from bilingual parallel corpora. HTP works by converting aligned phrases into a graph, and finding paraphrases that are “close” to a phrase of </context>
</contexts>
<marker>Zhao, Wang, Liu, Li, 2008</marker>
<rawString>Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li. 2008. Pivot approach for extracting paraphrase patterns from bilingual corpora. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>