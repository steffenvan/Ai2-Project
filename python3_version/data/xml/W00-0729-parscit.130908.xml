<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003911">
<note confidence="0.81546">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 139-141, Lisbon, Portugal, 2000.
</note>
<title confidence="0.984239">
Chunking with Maximum Entropy Models
</title>
<author confidence="0.963619">
Rob Koeling
</author>
<affiliation confidence="0.625253">
SRI Cambridge
</affiliation>
<email confidence="0.966342">
koeling@cam.sri.com
</email>
<sectionHeader confidence="0.998071" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999230571428572">
In this paper I discuss a first attempt to create a
text chunker using a Maximum Entropy model.
The first experiments, implementing classifiers
that tag every word in a sentence with a phrase-
tag using very local lexical information, part-
of-speech tags and phrase tags of surrounding
words, give encouraging results.
</bodyText>
<sectionHeader confidence="0.87419" genericHeader="method">
2 Maximum Entropy models
</sectionHeader>
<bodyText confidence="0.999380863636364">
Maximum Entropy (MaxEnt) models (Jaynes,
1957) are exponential models that implement
the intuition that if there is no evidence to
favour one alternative solution above another,
both alternatives should be equally likely. In
order to accomplish this, as much information
as possible about the process you want to model
must be collected. This information consists
of frequencies of events relevant to the process.
The frequencies of relevant events are consid-
ered to be properties of the process. When
building a model we have to constrain our at-
tention to models with these properties. In
most cases the process is only partially de-
scribed. The MaxEnt framework now demands
that from all the models that satisfy these con-
straints, we choose the model with the flattest
probability distribution. This is the model with
the highest entropy (given the fact that the con-
straints are met). When we are looking for a
conditional model P(w1h), the MaxEnt solution
has the form:
</bodyText>
<equation confidence="0.996256">
1
13(wih) = e
Z(h)
</equation>
<bodyText confidence="0.999694444444444">
where fi(h,w) refers to a (binary valued) fea-
ture function that describes a certain event; Ai
is a parameter that indicates how important fea-
ture fi is for the model and Z(h) is a normali-
sation factor.
In the last few years there has been an in-
creasing interest in applying MaxEnt models for
NLP applications (Ratnaparkhi, 1998; Berger et
al., 1996; Rosenfeld, 1994; Ristad, 1998). The
attraction of the framework lies in the ease with
which different information sources used in the
modelling process are combined and the good
results that are reported with the use of these
models. Another strong point of this framework
is the fact that general software can easily be
applied to a wide range of problems. For these
experiments we have used off-the-shelf software
(Maccent) (Dehaspe, 1997).
</bodyText>
<sectionHeader confidence="0.905279" genericHeader="method">
3 An MaxEnt chunker
</sectionHeader>
<subsectionHeader confidence="0.981142">
3.1 Attributes used
</subsectionHeader>
<bodyText confidence="0.9906494">
First need to be decided which information
sources might help to predict the chunk tag. We
need to work with the information that is in-
cluded in the WSJ corpus, so the choice is first
limited to:
</bodyText>
<listItem confidence="0.99964275">
• Current word
• POS tag of current word
• Surrounding words
• POS tags of surrounding words
</listItem>
<bodyText confidence="0.999848428571429">
All these sources will be used, but in case of the
information sources using surrounding words we
will have to decide how much context is taken
into account. I did not perform exhaustive tests
on finding the best configuration, but following
(Tjong Kim Sang and Veenstra, 1999; Ratna-
parkhi, 1997) I only used very local context. In
these experiments I used a left context of three
words and a right context of two words. Exper-
iments described in (Munoz et al., 1999) suc-
cessfully used larger contexts, but the few tests
that I performed to confirm this did not give
evidence that we could benefit significantly by
extending the context. Apart from information
</bodyText>
<equation confidence="0.558783">
Ei Aifi(h,w)
</equation>
<page confidence="0.98354">
139
</page>
<bodyText confidence="0.9791545">
given by the WSJ corpus, information generated
by the model itself will also be used:
</bodyText>
<listItem confidence="0.944108">
• Chunk tags of previous words
</listItem>
<bodyText confidence="0.999990304347826">
It would of course sometimes be desirable to use
the chunk tags of the following words also, but
these are not instantly available and therefore
we will need a cascaded approach. I have exper-
imented with a cascaded chunker, but I did not
improve the results significantly.
In order to use previously predicted chunk
tags, the evaluation part of the Maccent soft-
ware had to be modified. The evaluation pro-
gram needs a previously created file with all
the attributes and the actual class, but the
chunk tag of the previous two words cannot
be provided beforehand as they are produced
in the process of evaluation. A cascaded ap-
proach where after the first run the predicted
tags are added to the file with test data is also
not completely satisfactory as the provided tags
are then predicted on basis of all the other at-
tributes, but not the previous chunk tags. Ide-
ally the information about the tags of the pre-
vious words would be added during evaluation.
This required some modification of the evalua-
tion script.
</bodyText>
<subsectionHeader confidence="0.862113">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999255">
The experiments are evaluated using the follow-
ing standard evaluation measures:
</bodyText>
<listItem confidence="0.846361">
• Tagging accuracy =
</listItem>
<figure confidence="0.7978545">
Number of correct tagged words
Total number of words
• Recall =
Number of correct proposed baseNP&apos;s
Number of correct baseNP&apos;s
• Precision =
Number of correct proposed baseNP&apos;s
Number of proposed baseNP&apos;s
(02-1-1).Recall-Precision
• Fo-score = 02.Recall+Precision
</figure>
<figureCaption confidence="0.346359">
(f3=1 in all experiments.)
</figureCaption>
<bodyText confidence="0.923077285714286">
In all the experiments a left context of 3 words
and a right context of 2 words was used. The
part of speech tags of the surrounding words and
the word itself were all used as atomic features.
The lexical information used consisted of the
previous word, the current word and the next
word. The word W_2 was omitted because it
did not seem to improve the model. Using only
these atomic features, the model scored an tag-
ging accuracy of about 95.5% and a F-score of
about 90.5 %. Well below the reported results in
the literature. Adding features combining POS
tags improved the results significantly to just
below state of the art scores. Finally 2 complex
features involving NP chunk tags predicted for
previous words were added. The most successful
set of features used in our experiments is given
in figure 1. It is not claimed that this is the best
Template Meaning
TAG _3 Base-NP tag of W_3
POS_3 Part of Speech tag of W._3
</bodyText>
<equation confidence="0.861499090909091">
POS_3/POSo
POS-3/POS-2
TAG_31TAG_21TAG_11POS0
TAG _2
POS-2
W-1 Previous word
TAG_i
POS-1
Current word
POSo
TV+1
POS+1
POS_2/POS0
POS-2/POS-1
TAG_21TAG_11POS0
P0S_11TAG_1/POS0
POS_11POS0IPOS+1
POS_21POS_11POS0IPOS+1
POS0IPOS+1
POS+2
POS0IPOS+2
POS0IPOS+11POS+2
</equation>
<figureCaption confidence="0.9281705">
Figure 1: Feature set-up for best scoring exper-
iment
</figureCaption>
<bodyText confidence="0.999819333333333">
set of features possible for this task. Trying new
feature combinations, by adding them manually
and testing the new configuration is a time con-
suming and not very interesting activity. Es-
pecially when the scores are close to the best
published scores, adding new features have little
impact on the behaviour of the model. An al-
gorithm that discovers the interaction between
features and suggests which features could be
combined to improve the model would be very
helpful here. I did not include any complex fea-
tures involving lexical information. It might be
</bodyText>
<page confidence="0.992597">
140
</page>
<bodyText confidence="0.99990205">
useful to include more features with lexical in-
formation if more training data is available (for
example the full R&amp;M data set consisting of sec-
tion 2-21 of WSJ).
For feature selection a simple count cut-off
was used. I experimented with several combi-
nations of thresholds and the number of itera-
tions used to train the model. When the thresh-
old was set to 2, unique contexts (can be prob-
lematic during training of the model; see (Rat-
naparkhi, 1998)) did not occur very frequently
anymore and an upper bound on the number of
iterations did not seem to be necessary. It was
found that (using a threshold of 2 for every sin-
gle feature) after about 100 iterations the model
did not improve very much anymore. Using the
feature setup given in figure 1 a threshold of 2
for all the features and allowing the model to
train over 100 iterations, the scores given in ta-
ble 1 were obtained.
</bodyText>
<sectionHeader confidence="0.945828" genericHeader="method">
4 Concluding remarks
</sectionHeader>
<bodyText confidence="0.9999540625">
The first observation that I would like to make
here, is the fact that it was relatively easy to
get results that are comparable with previously
published results. Even though some improve-
ment is to be expected when more detailed fea-
tures, more context and/or more training data
is used, it seems to be necessary to incorporate
other sources of information to improve signifi-
cantly on these results.
Further, it is not satisfactory to find out what
attribute combinations to use by trying new
combinations and testing them. It might be
worth to examine ways to automatically de-
tect which feature combinations are promis-
ing (Mikheev, forthcoming; Della Pietra et al.,
1997).
</bodyText>
<sectionHeader confidence="0.998074" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981639083333333">
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1).
Luc Dehaspe. 1997. Maximum entropy modeling
with clausal constraints. In Proceedings of the 7th
International Workshop on Inductive Logic Pro-
gramming.
S. Della Pietra, V. Della Pietra, and J. Laf-
ferty. 1997. Inducing features from random fields.
IEEE Transactions on Patterns Analysis and Ma-
chine Intelligence, 19(4).
</reference>
<table confidence="0.999807416666667">
test data precision recall Fo=1
ADJP 65.53 % 75.33 % 70.09
ADVP 78.98 % 78.08 % 78.53
CONJP 55.56 % 45.45 % 50.00
INTJ 50.00 % 100.00 % 66.67
LST 0.00 % 0.00 % 0.00
NP 93.18 % 92.84 % 93.01
PP 97.05 % 93.60 % 95.30
PRT 58.49 % 73.81 % 65.26
SBAR 63.36 % 82.68 % 71.75
VP 93.22 % 92.54 % 92.88
all 92.08 % 91.86 % 91.97
</table>
<tableCaption confidence="0.998103">
Table 1: Results
</tableCaption>
<reference confidence="0.999654642857143">
E.T. Jaynes. 1957. Information theory and statisti-
cal mechanics. Physical Review, 108:171-190.
Andrei Mikheev. forthcoming. Feature lattices and
maximum entropy models. Journal of Machine
Learning.
Marcia Munoz, Vasin Punyakanok, Dan Roth, and
Day Zimak. 1999. A learning approach to shallow
parsing. In Proceedings of EMNLP-WVLC&apos;99.
Adwait Ratnaparkhi. 1997. A linear observed time
statistical parser based on maximum entropy
models. In Proceedings of the Second Conference
on Empirical Methods in Natural Language Pro-
cessing, Brown University, Providence, Rhode Is-
land.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, UPenn.
Sven Eric Ristad. 1998. Maximum entropy mod-
elling toolkit. Technical report.
Ronald Rosenfeld. 1994. Adaptive Statistical Lan-
guage Modelling: A Maximum Entropy Approach.
Ph.D. thesis, Carnegy Mellon University.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999.
Strategy and tactics: a model for language pro-
duction. In Ninth Conference of the European
Chapter of the Association for Computational
Linguistics, University of Bergen, Bergen, Nor-
way.
</reference>
<page confidence="0.998253">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951929">
<note confidence="0.985447">of CoNLL-2000 and LLL-2000, 139-141, Lisbon, Portugal, 2000.</note>
<title confidence="0.981588">Chunking with Maximum Entropy Models</title>
<author confidence="0.995525">Rob</author>
<affiliation confidence="0.997721">SRI Cambridge</affiliation>
<email confidence="0.989065">koeling@cam.sri.com</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="1861" citStr="Berger et al., 1996" startWordPosition="300" endWordPosition="303">onstraints, we choose the model with the flattest probability distribution. This is the model with the highest entropy (given the fact that the constraints are met). When we are looking for a conditional model P(w1h), the MaxEnt solution has the form: 1 13(wih) = e Z(h) where fi(h,w) refers to a (binary valued) feature function that describes a certain event; Ai is a parameter that indicates how important feature fi is for the model and Z(h) is a normalisation factor. In the last few years there has been an increasing interest in applying MaxEnt models for NLP applications (Ratnaparkhi, 1998; Berger et al., 1996; Rosenfeld, 1994; Ristad, 1998). The attraction of the framework lies in the ease with which different information sources used in the modelling process are combined and the good results that are reported with the use of these models. Another strong point of this framework is the fact that general software can easily be applied to a wide range of problems. For these experiments we have used off-the-shelf software (Maccent) (Dehaspe, 1997). 3 An MaxEnt chunker 3.1 Attributes used First need to be decided which information sources might help to predict the chunk tag. We need to work with the in</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Dehaspe</author>
</authors>
<title>Maximum entropy modeling with clausal constraints.</title>
<date>1997</date>
<booktitle>In Proceedings of the 7th International Workshop on Inductive Logic Programming.</booktitle>
<contexts>
<context position="2304" citStr="Dehaspe, 1997" startWordPosition="373" endWordPosition="374"> a normalisation factor. In the last few years there has been an increasing interest in applying MaxEnt models for NLP applications (Ratnaparkhi, 1998; Berger et al., 1996; Rosenfeld, 1994; Ristad, 1998). The attraction of the framework lies in the ease with which different information sources used in the modelling process are combined and the good results that are reported with the use of these models. Another strong point of this framework is the fact that general software can easily be applied to a wide range of problems. For these experiments we have used off-the-shelf software (Maccent) (Dehaspe, 1997). 3 An MaxEnt chunker 3.1 Attributes used First need to be decided which information sources might help to predict the chunk tag. We need to work with the information that is included in the WSJ corpus, so the choice is first limited to: • Current word • POS tag of current word • Surrounding words • POS tags of surrounding words All these sources will be used, but in case of the information sources using surrounding words we will have to decide how much context is taken into account. I did not perform exhaustive tests on finding the best configuration, but following (Tjong Kim Sang and Veenstr</context>
</contexts>
<marker>Dehaspe, 1997</marker>
<rawString>Luc Dehaspe. 1997. Maximum entropy modeling with clausal constraints. In Proceedings of the 7th International Workshop on Inductive Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features from random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Patterns Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Inducing features from random fields. IEEE Transactions on Patterns Analysis and Machine Intelligence, 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Jaynes</author>
</authors>
<title>Information theory and statistical mechanics.</title>
<date>1957</date>
<journal>Physical Review,</journal>
<pages>108--171</pages>
<marker>Jaynes, 1957</marker>
<rawString>E.T. Jaynes. 1957. Information theory and statistical mechanics. Physical Review, 108:171-190.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Feature lattices and maximum entropy models.</title>
<journal>Journal of Machine Learning.</journal>
<marker>forthcoming, </marker>
<rawString>Andrei Mikheev. forthcoming. Feature lattices and maximum entropy models. Journal of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcia Munoz</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Day Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP-WVLC&apos;99.</booktitle>
<contexts>
<context position="3101" citStr="Munoz et al., 1999" startWordPosition="515" endWordPosition="518">luded in the WSJ corpus, so the choice is first limited to: • Current word • POS tag of current word • Surrounding words • POS tags of surrounding words All these sources will be used, but in case of the information sources using surrounding words we will have to decide how much context is taken into account. I did not perform exhaustive tests on finding the best configuration, but following (Tjong Kim Sang and Veenstra, 1999; Ratnaparkhi, 1997) I only used very local context. In these experiments I used a left context of three words and a right context of two words. Experiments described in (Munoz et al., 1999) successfully used larger contexts, but the few tests that I performed to confirm this did not give evidence that we could benefit significantly by extending the context. Apart from information Ei Aifi(h,w) 139 given by the WSJ corpus, information generated by the model itself will also be used: • Chunk tags of previous words It would of course sometimes be desirable to use the chunk tags of the following words also, but these are not instantly available and therefore we will need a cascaded approach. I have experimented with a cascaded chunker, but I did not improve the results significantly.</context>
</contexts>
<marker>Munoz, Punyakanok, Roth, Zimak, 1999</marker>
<rawString>Marcia Munoz, Vasin Punyakanok, Dan Roth, and Day Zimak. 1999. A learning approach to shallow parsing. In Proceedings of EMNLP-WVLC&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>Brown University,</institution>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="2931" citStr="Ratnaparkhi, 1997" startWordPosition="484" endWordPosition="486">xEnt chunker 3.1 Attributes used First need to be decided which information sources might help to predict the chunk tag. We need to work with the information that is included in the WSJ corpus, so the choice is first limited to: • Current word • POS tag of current word • Surrounding words • POS tags of surrounding words All these sources will be used, but in case of the information sources using surrounding words we will have to decide how much context is taken into account. I did not perform exhaustive tests on finding the best configuration, but following (Tjong Kim Sang and Veenstra, 1999; Ratnaparkhi, 1997) I only used very local context. In these experiments I used a left context of three words and a right context of two words. Experiments described in (Munoz et al., 1999) successfully used larger contexts, but the few tests that I performed to confirm this did not give evidence that we could benefit significantly by extending the context. Apart from information Ei Aifi(h,w) 139 given by the WSJ corpus, information generated by the model itself will also be used: • Chunk tags of previous words It would of course sometimes be desirable to use the chunk tags of the following words also, but these</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Brown University, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis, UPenn.</tech>
<contexts>
<context position="1840" citStr="Ratnaparkhi, 1998" startWordPosition="298" endWordPosition="299">hat satisfy these constraints, we choose the model with the flattest probability distribution. This is the model with the highest entropy (given the fact that the constraints are met). When we are looking for a conditional model P(w1h), the MaxEnt solution has the form: 1 13(wih) = e Z(h) where fi(h,w) refers to a (binary valued) feature function that describes a certain event; Ai is a parameter that indicates how important feature fi is for the model and Z(h) is a normalisation factor. In the last few years there has been an increasing interest in applying MaxEnt models for NLP applications (Ratnaparkhi, 1998; Berger et al., 1996; Rosenfeld, 1994; Ristad, 1998). The attraction of the framework lies in the ease with which different information sources used in the modelling process are combined and the good results that are reported with the use of these models. Another strong point of this framework is the fact that general software can easily be applied to a wide range of problems. For these experiments we have used off-the-shelf software (Maccent) (Dehaspe, 1997). 3 An MaxEnt chunker 3.1 Attributes used First need to be decided which information sources might help to predict the chunk tag. We nee</context>
<context position="7102" citStr="Ratnaparkhi, 1998" startWordPosition="1185" endWordPosition="1187"> which features could be combined to improve the model would be very helpful here. I did not include any complex features involving lexical information. It might be 140 useful to include more features with lexical information if more training data is available (for example the full R&amp;M data set consisting of section 2-21 of WSJ). For feature selection a simple count cut-off was used. I experimented with several combinations of thresholds and the number of iterations used to train the model. When the threshold was set to 2, unique contexts (can be problematic during training of the model; see (Ratnaparkhi, 1998)) did not occur very frequently anymore and an upper bound on the number of iterations did not seem to be necessary. It was found that (using a threshold of 2 for every single feature) after about 100 iterations the model did not improve very much anymore. Using the feature setup given in figure 1 a threshold of 2 for all the features and allowing the model to train over 100 iterations, the scores given in table 1 were obtained. 4 Concluding remarks The first observation that I would like to make here, is the fact that it was relatively easy to get results that are comparable with previously p</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, UPenn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Eric Ristad</author>
</authors>
<title>Maximum entropy modelling toolkit.</title>
<date>1998</date>
<tech>Technical report.</tech>
<contexts>
<context position="1893" citStr="Ristad, 1998" startWordPosition="306" endWordPosition="307">he flattest probability distribution. This is the model with the highest entropy (given the fact that the constraints are met). When we are looking for a conditional model P(w1h), the MaxEnt solution has the form: 1 13(wih) = e Z(h) where fi(h,w) refers to a (binary valued) feature function that describes a certain event; Ai is a parameter that indicates how important feature fi is for the model and Z(h) is a normalisation factor. In the last few years there has been an increasing interest in applying MaxEnt models for NLP applications (Ratnaparkhi, 1998; Berger et al., 1996; Rosenfeld, 1994; Ristad, 1998). The attraction of the framework lies in the ease with which different information sources used in the modelling process are combined and the good results that are reported with the use of these models. Another strong point of this framework is the fact that general software can easily be applied to a wide range of problems. For these experiments we have used off-the-shelf software (Maccent) (Dehaspe, 1997). 3 An MaxEnt chunker 3.1 Attributes used First need to be decided which information sources might help to predict the chunk tag. We need to work with the information that is included in th</context>
</contexts>
<marker>Ristad, 1998</marker>
<rawString>Sven Eric Ristad. 1998. Maximum entropy modelling toolkit. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Adaptive Statistical Language Modelling: A Maximum Entropy Approach.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegy Mellon University.</institution>
<contexts>
<context position="1878" citStr="Rosenfeld, 1994" startWordPosition="304" endWordPosition="305"> the model with the flattest probability distribution. This is the model with the highest entropy (given the fact that the constraints are met). When we are looking for a conditional model P(w1h), the MaxEnt solution has the form: 1 13(wih) = e Z(h) where fi(h,w) refers to a (binary valued) feature function that describes a certain event; Ai is a parameter that indicates how important feature fi is for the model and Z(h) is a normalisation factor. In the last few years there has been an increasing interest in applying MaxEnt models for NLP applications (Ratnaparkhi, 1998; Berger et al., 1996; Rosenfeld, 1994; Ristad, 1998). The attraction of the framework lies in the ease with which different information sources used in the modelling process are combined and the good results that are reported with the use of these models. Another strong point of this framework is the fact that general software can easily be applied to a wide range of problems. For these experiments we have used off-the-shelf software (Maccent) (Dehaspe, 1997). 3 An MaxEnt chunker 3.1 Attributes used First need to be decided which information sources might help to predict the chunk tag. We need to work with the information that is</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>Ronald Rosenfeld. 1994. Adaptive Statistical Language Modelling: A Maximum Entropy Approach. Ph.D. thesis, Carnegy Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Jorn Veenstra</author>
</authors>
<title>Strategy and tactics: a model for language production.</title>
<date>1999</date>
<booktitle>In Ninth Conference of the European Chapter of the Association</booktitle>
<institution>for Computational Linguistics, University of Bergen,</institution>
<location>Bergen,</location>
<contexts>
<context position="2911" citStr="Sang and Veenstra, 1999" startWordPosition="480" endWordPosition="483"> (Dehaspe, 1997). 3 An MaxEnt chunker 3.1 Attributes used First need to be decided which information sources might help to predict the chunk tag. We need to work with the information that is included in the WSJ corpus, so the choice is first limited to: • Current word • POS tag of current word • Surrounding words • POS tags of surrounding words All these sources will be used, but in case of the information sources using surrounding words we will have to decide how much context is taken into account. I did not perform exhaustive tests on finding the best configuration, but following (Tjong Kim Sang and Veenstra, 1999; Ratnaparkhi, 1997) I only used very local context. In these experiments I used a left context of three words and a right context of two words. Experiments described in (Munoz et al., 1999) successfully used larger contexts, but the few tests that I performed to confirm this did not give evidence that we could benefit significantly by extending the context. Apart from information Ei Aifi(h,w) 139 given by the WSJ corpus, information generated by the model itself will also be used: • Chunk tags of previous words It would of course sometimes be desirable to use the chunk tags of the following w</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Strategy and tactics: a model for language production. In Ninth Conference of the European Chapter of the Association for Computational Linguistics, University of Bergen, Bergen, Norway.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>