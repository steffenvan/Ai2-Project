<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.997282">
Chinese Segmentation with a Word-Based Perceptron Algorithm
</title>
<author confidence="0.999129">
Yue Zhang and Stephen Clark
</author>
<affiliation confidence="0.998669">
Oxford University Computing Laboratory
</affiliation>
<address confidence="0.991116">
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
</address>
<email confidence="0.99978">
{yue.zhang,stephen.clark}@comlab.ox.ac.uk
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984421052631">
Standard approaches to Chinese word seg-
mentation treat the problem as a tagging
task, assigning labels to the characters in
the sequence indicating whether the char-
acter marks a word boundary. Discrimina-
tively trained models based on local char-
acter features are used to make the tagging
decisions, with Viterbi decoding finding the
highest scoring segmentation. In this paper
we propose an alternative, word-based seg-
mentor, which uses features based on com-
plete words and word sequences. The gener-
alized perceptron algorithm is used for dis-
criminative training, and we use a beam-
search decoder. Closed tests on the first and
second SIGHAN bakeoffs show that our sys-
tem is competitive with the best in the litera-
ture, achieving the highest reported F-scores
for a number of corpora.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994434723404255">
Words are the basic units to process for most NLP
tasks. The problem of Chinese word segmentation
(CWS) is to find these basic units for a given sen-
tence, which is written as a continuous sequence of
characters. It is the initial step for most Chinese pro-
cessing applications.
Chinese character sequences are ambiguous, of-
ten requiring knowledge from a variety of sources
for disambiguation. Out-of-vocabulary (OOV) words
are a major source of ambiguity. For example, a
difficult case occurs when an OOV word consists
,
possible segmentations include “&amp;A (the discus-
sion) &apos;L.: (will) TR (very) MSA (be successful)” and
“&amp;A (the discussion meeting) TR (very) MSA (be
successful)”. The ambiguity can only be resolved
with contextual information outside the sentence.
Human readers often use semantics, contextual in-
formation about the document and world knowledge
to resolve segmentation ambiguities.
There is no fixed standard for Chinese word seg-
mentation. Experiments have shown that there is
only about 75% agreement among native speakers
regarding the correct word segmentation (Sproat et
al., 1996). Also, specific NLP tasks may require dif-
ferent segmentation criteria. For example, “J L;5&apos;,W
f j!” could be treated as a single word (Bank of Bei-
jing) for machine translation, while it is more natu-
rally segmented into “J L� (Beijing) Wf j! (bank)”
for tasks such as text-to-speech synthesis. There-
fore, supervised learning with specifically defined
training data has become the dominant approach.
Following Xue (2003), the standard approach to
of characters which have themselves been seen as
words; here an automatic segmentor may split the
OOV word into individual single-character words.
Typical examples of unseen words include Chinese
names, translated foreign names and idioms.
The segmentation of known words can also be
ambiguous. For example, “iK IITiiI” should be “iK
(here) IITi iI (flour)” in the sentence “iK IITiiI�H*TR
&amp;quot;” (flour and rice are expensive here) or “iK (here)
IITiiI (inside)” in the sentence “iK IITiiITR%�” (it’s
cold inside here). The ambiguity can be resolved
with information about the neighboringn words. In
comparison, for the sentences “&amp; tih {
�”
</bodyText>
<page confidence="0.981717">
840
</page>
<note confidence="0.9260115">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840–847,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99168266">
supervised learning for CWS is to treat it as a tagging beam and the importance of word-based features.
task. Tags are assigned to each character in the sen- We compare the accuracy of our final system to the
tence, indicating whether the character is a single- state-of-the-art CWS systems in the literature using
character word or the start, middle or end of a multi- the first and second SIGHAN bakeoff data. Our sys-
character word. The features are usually confined to tem is competitive with the best systems, obtaining
a five-character window with the current character the highest reported F-scores on a number of the
in the middle. In this way, dynamic programming bakeoff corpora. These results demonstrate the im-
algorithms such as the Viterbi algorithm can be used portance of word-based features for CWS. Further-
for decoding. more, our approach provides an example of the po-
Several discriminatively trained models have re- tential of search-based discriminative training meth-
cently been applied to the CWS problem. Exam- ods for NLP tasks.
ples include Xue (2003), Peng et al. (2004) and Shi 2 The Perceptron Training Algorithm
and Wang (2007); these use maximum entropy (ME) We formulate the CWS problem as finding a mapping
and conditional random field (CRF) models (Ratna- from an input sentence x E X to an output sentence
parkhi, 1998; Lafferty et al., 2001). An advantage y E Y , where X is the set of possible raw sentences
of these models is their flexibility in allowing knowl- and Y is the set of possible segmented sentences.
edge from various sources to be encoded as features. Given an input sentence x, the correct output seg-
Contextual information plays an important role in mentation F(x) satisfies:
word segmentation decisions; especially useful is in- F(x) = arg max Score(y)
formation about surrounding words. Consider the yEGEN(x)
sentence “-Q*�A”, which can be from “-A- where GEN(x) denotes the set of possible segmen-
(among which) Q* (foreign) �A (companies)”, tations for an input sentence x, consistent with nota-
or “-Q (in China) *� (foreign companies) A tion from Collins (2002).
-)� (business)”. Note that the five-character window The score for a segmented sentence is computed
surrounding “*” is the same in both cases, making by first mapping it into a set of features. A feature
the tagging decision for that character difficult given is an indicator of the occurrence of a certain pattern
the local window. However, the correct decision can in a segmented sentence. For example, it can be the
be made by comparison of the two three-word win- occurrence of “%�” as a single word, or the occur-
dows containing this character. rence of “%” separated from “ITii” in two adjacent
In order to explore the potential of word-based words. By defining features, a segmented sentence
models, we adapt the perceptron discriminative is mapped into a global feature vector, in which each
learning algorithm to the CWS problem. Collins dimension represents the count of a particular fea-
(2002) proposed the perceptron as an alternative to ture in the sentence. The term “global” feature vec-
the CRF method for HMM-style taggers. However, tor is used by Collins (2002) to distinguish between
our model does not map the segmentation problem feature count vectors for whole sequences and the
to a tag sequence learning problem, but defines fea- “local” feature vectors in ME tagging models, which
tures on segmented sentences directly. Hence we are Boolean valued vectors containing the indicator
use a beam-search decoder during training and test- features for one element in the sequence.
ing; our idea is similar to that of Collins and Roark Denote the global feature vector for segmented
(2004) who used a beam-search decoder as part of sentence y with 4b(y) E Rd, where d is the total
a perceptron parsing model. Our work can also be number of features in the model; then Score(y) is
seen as part of the recent move towards search-based computed by the dot product of vector 4b(y) and a
learning methods which do not rely on dynamic pro- parameter vector α E Rd, where αz is the weight for
gramming and are thus able to exploit larger parts of the ith feature:
the context for making decisions (Daume III, 2006). Score(y) = -b(y) · α
We study several factors that influence the per-
formance of the perceptron word segmentor, includ-
ing the averaged perceptron method, the size of the
841
Inputs: training examples (xi, yi)
</bodyText>
<equation confidence="0.630232">
Initialization: set α = 0
Algorithm:
for t = 1..T, i = 1..N
calculate zi = argmaxyEGEN(xi) 4&gt;(y) · α
if zi =� yi
α = α + 4&gt;(yi) − 4&gt;(zi)
Outputs: α
</equation>
<figureCaption confidence="0.996903">
Figure 1: the perceptron learning algorithm, adapted
from Collins (2002)
</figureCaption>
<bodyText confidence="0.9993552">
The perceptron training algorithm is used to deter-
mine the weight values α.
The training algorithm initializes the parameter
vector as all zeros, and updates the vector by decod-
ing the training examples. Each training sentence
is turned into the raw input form, and then decoded
with the current parameter vector. The output seg-
mented sentence is compared with the original train-
ing example. If the output is incorrect, the parameter
vector is updated by adding the global feature vector
of the training example and subtracting the global
feature vector of the decoder output. The algorithm
can perform multiple passes over the same training
sentences. Figure 1 gives the algorithm, where N is
the number of training sentences and T is the num-
ber of passes over the data.
Note that the algorithm from Collins (2002) was
designed for discriminatively training an HMM-style
tagger. Features are extracted from an input se-
quence x and its corresponding tag sequence y:
</bodyText>
<equation confidence="0.819915">
Score(x, y) = 4&gt;(x, y) · α
</equation>
<bodyText confidence="0.998343857142857">
Our algorithm is not based on an HMM. For a given
input sequence x, even the length of different candi-
dates y (the number of words) is not fixed. Because
the output sequence y (the segmented sentence) con-
tains all the information from the input sequence x
(the raw sentence), the global feature vector 4&gt;(x, y)
is replaced with 4&gt;(y), which is extracted from the
candidate segmented sentences directly.
Despite the above differences, since the theorems
of convergence and their proof (Collins, 2002) are
only dependent on the feature vectors, and not on
the source of the feature definitions, the perceptron
algorithm is applicable to the training of our CWS
model.
</bodyText>
<subsectionHeader confidence="0.995665">
2.1 The averaged perceptron
</subsectionHeader>
<bodyText confidence="0.999970090909091">
The averaged perceptron algorithm (Collins, 2002)
was proposed as a way of reducing overfitting on
the training data. It was motivated by the voted-
perceptron algorithm (Freund and Schapire, 1999)
and has been shown to give improved accuracy over
the non-averaged perceptron on a number of tasks.
Let N be the number of training sentences, T the
number of training iterations, and αn,t the parame-
ter vector immediately after the nth sentence in the
tth iteration. The averaged parameter vector γ E Rd
is defined as:
</bodyText>
<equation confidence="0.7147045">
� αn,t
n=1..N,t=1..T
</equation>
<bodyText confidence="0.979472222222222">
To compute the averaged parameters γ, the train-
ing algorithm in Figure 1 can be modified by keep-
ing a total parameter vector σn,t = E αn,t, which is
updated using α after each training example. After
the final iteration, γ is computed as σn,t/NT. In the
averaged perceptron algorithm, γ is used instead of
α as the final parameter vector.
With a large number of features, calculating the
total parameter vector σn,t after each training exam-
ple is expensive. Since the number of changed di-
mensions in the parameter vector α after each train-
ing example is a small proportion of the total vec-
tor, we use a lazy update optimization for the train-
ing process.1 Define an update vector τ to record
the number of the training sentence n and iteration
t when each dimension of the averaged parameter
vector was last updated. Then after each training
sentence is processed, only update the dimensions
of the total parameter vector corresponding to the
features in the sentence. (Except for the last exam-
ple in the last iteration, when each dimension of τ
is updated, no matter whether the decoder output is
correct or not).
Denote the sth dimension in each vector before
processing the nth example in the tth iteration as
αn−1,t
s , σn−1,t and τn−1,t
</bodyText>
<subsectionHeader confidence="0.815653">
s
</subsectionHeader>
<bodyText confidence="0.841617">
(nτ,s,tτ,s).
=
</bodyText>
<subsectionHeader confidence="0.447953">
Suppose
</subsectionHeader>
<bodyText confidence="0.92037">
that the decoder output zn,t is different from the
training example yn. Now αn,t
</bodyText>
<equation confidence="0.927044055555556">
s , σn,t
s and τn,t
s can
1Daume III (2006) describes a similar algorithm.
1
γ= NT
842
be updated in the following way:
= an−1,t s+ �n−1,t
s × (tN+n −tτ,sN− nτ,s)
a = an—1,t
8 s
+&apos;b(yn) − `F(zn,t)
s
�n,t
s = �n,t
s + `F(yn) − `F(zn,t)
7n,t s= (n, t)
</equation>
<bodyText confidence="0.998526">
We found that this lazy update method was signif-
icantly faster than the naive method.
</bodyText>
<sectionHeader confidence="0.960053" genericHeader="method">
3 The Beam-Search Decoder
</sectionHeader>
<bodyText confidence="0.999894892857143">
The decoder reads characters from the input sen-
tence one at a time, and generates candidate seg-
mentations incrementally. At each stage, the next in-
coming character is combined with an existing can-
didate in two different ways to generate new candi-
dates: it is either appended to the last word in the
candidate, or taken as the start of a new word. This
method guarantees exhaustive generation of possible
segmentations for any input sentence.
Two agendas are used: the source agenda and the
target agenda. Initially the source agenda contains
an empty sentence and the target agenda is empty.
At each processing stage, the decoder reads in a
character from the input sentence, combines it with
each candidate in the source agenda and puts the
generated candidates onto the target agenda. After
each character is processed, the items in the target
agenda are copied to the source agenda, and then the
target agenda is cleaned, so that the newly generated
candidates can be combined with the next incom-
ing character to generate new candidates. After the
last character is processed, the decoder returns the
candidate with the best score in the source agenda.
Figure 2 gives the decoding algorithm.
For a sentence with length l, there are 2l−1 differ-
ent possible segmentations. To guarantee reasonable
running speed, the size of the target agenda is lim-
ited, keeping only the B best candidates.
</bodyText>
<sectionHeader confidence="0.998182" genericHeader="method">
4 Feature templates
</sectionHeader>
<bodyText confidence="0.9997416">
The feature templates are shown in Table 1. Features
1 and 2 contain only word information, 3 to 5 con-
tain character and length information, 6 and 7 con-
tain only character information, 8 to 12 contain word
and character information, while 13 and 14 contain
</bodyText>
<table confidence="0.550672">
Input: raw sentence sent – a list of characters
Initialization: set agendas src = [[]], tgt = []
Variables: candidate sentence item – a list of words
Algorithm:
for index = 0..sent.length−1:
var char = sent[index]
foreach item in src:
</table>
<equation confidence="0.9547825">
// append as a new word to the candidate
var item1 = item
item1.append(char.toWord())
tgt.insert(item1)
</equation>
<bodyText confidence="0.596782">
// append the character to the last word
</bodyText>
<equation confidence="0.953907666666667">
if item.length &gt; 1:
var item2 = item
item2[item2.length−1].append(char)
tgt.insert(item2)
src = tgt
tgt = []
</equation>
<figureCaption confidence="0.714728">
Outputs: src.best item
Figure 2: The decoding algorithm
</figureCaption>
<bodyText confidence="0.998729461538462">
word and length information. Any segmented sen-
tence is mapped to a global feature vector according
to these templates. There are 356,337 features with
non-zero values after 6 training iterations using the
development data.
For this particular feature set, the longest range
features are word bigrams. Therefore, among partial
candidates ending with the same bigram, the best
one will also be in the best final candidate. The
decoder can be optimized accordingly: when an in-
coming character is combined with candidate items
as a new word, only the best candidate is kept among
those having the same last word.
</bodyText>
<sectionHeader confidence="0.924981" genericHeader="method">
5 Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.9982299">
Among the character-tagging CWS models, Li et al.
(2005) uses an uneven margin alteration of the tradi-
tional perceptron classifier (Li et al., 2002). Each
character is classified independently, using infor-
mation in the neighboring five-character window.
Liang (2005) uses the discriminative perceptron al-
gorithm (Collins, 2002) to score whole character tag
sequences, finding the best candidate by the global
score. It can be seen as an alternative to the ME and
CRF models (Xue, 2003; Peng et al., 2004), which
</bodyText>
<figure confidence="0.819985">
an,t
s
</figure>
<page confidence="0.9929">
843
</page>
<tableCaption confidence="0.998635">
Table 1: feature templates
</tableCaption>
<bodyText confidence="0.99982255">
do not involve word information. Wang et al. (2006)
incorporates an N-gram language model in ME tag-
ging, making use of word information to improve
the character tagging model. The key difference be-
tween our model and the above models is the word-
based nature of our system.
One existing method that is based on sub-word in-
formation, Zhang et al. (2006), combines a CRF and
a rule-based model. Unlike the character-tagging
models, the CRF submodel assigns tags to sub-
words, which include single-character words and
the most frequent multiple-character words from the
training corpus. Thus it can be seen as a step towards
a word-based model. However, sub-words do not
necessarily contain full word information. More-
over, sub-word extraction is performed separately
from feature extraction. Another difference from
our model is the rule-based submodel, which uses a
dictionary-based forward maximum match method
described by Sproat et al. (1996).
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999924410256411">
Two sets of experiments were conducted. The first,
used for development, was based on the part of Chi-
nese Treebank 4 that is not in Chinese Treebank
3 (since CTB3 was used as part of the first bake-
off). This corpus contains 240K characters (150K
words and 4798 sentences). 80% of the sentences
(3813) were randomly chosen for training and the
rest (985 sentences) were used as development test-
ing data. The accuracies and learning curves for the
non-averaged and averaged perceptron were com-
pared. The influence of particular features and the
agenda size were also studied.
The second set of experiments used training and
testing sets from the first and second international
Chinese word segmentation bakeoffs (Sproat and
Emerson, 2003; Emerson, 2005). The accuracies are
compared to other models in the literature.
F-measure is used as the accuracy measure. De-
fine precision p as the percentage of words in the de-
coder output that are segmented correctly, and recall
r as the percentage of gold standard output words
that are correctly segmented by the decoder. The
(balanced) F-measure is 2pr/(p + r).
CWS systems are evaluated by two types of tests.
The closed tests require that the system is trained
only with a designated training corpus. Any extra
knowledge is not allowed, including common sur-
names, Chinese and Arabic numbers, European let-
ters, lexicons, part-of-speech, semantics and so on.
The open tests do not impose such restrictions.
Open tests measure a model’s capability to utilize
extra information and domain knowledge, which can
lead to improved performance, but since this extra
information is not standardized, direct comparison
between open test results is less informative.
In this paper, we focus only on the closed test.
However, the perceptron model allows a wide range
of features, and so future work will consider how to
integrate open resources into our system.
</bodyText>
<subsectionHeader confidence="0.999706">
6.1 Learning curve
</subsectionHeader>
<bodyText confidence="0.9999597">
In this experiment, the agenda size was set to 16, for
both training and testing. Table 2 shows the preci-
sion, recall and F-measure for the development set
after 1 to 10 training iterations, as well as the num-
ber of mistakes made in each iteration. The corre-
sponding learning curves for both the non-averaged
and averaged perceptron are given in Figure 3.
The table shows that the number of mistakes made
in each iteration decreases, reflecting the conver-
gence of the learning algorithm. The averaged per-
</bodyText>
<figure confidence="0.914054303030303">
word w
word bigram w1w2
single-character word w
a word starting with character c and having
length l
a word ending with character c and having
length l
space-separated characters c1 and c2
character bigram c1c2 in any word
the first and last characters c1 and c2 of any
word
word w immediately before character c
character c immediately before word w
the starting characters c1 and c2 of two con-
secutive words
the ending characters c1 and c2 of two con-
secutive words
a word of length l and the previous word w
a word of length l and the next word w
1
2
3
4
5
6
7
8
9
10
11
12
13
14
</figure>
<page confidence="0.994859">
844
</page>
<table confidence="0.999950375">
Iteration 1 2 3 4 5 6 7 8 9 10
P (non-avg) 89.0 91.6 92.0 92.3 92.5 92.5 92.5 92.7 92.6 92.6
R (non-avg) 88.3 91.4 92.2 92.6 92.7 92.8 93.0 93.0 93.1 93.2
F (non-avg) 88.6 91.5 92.1 92.5 92.6 92.6 92.7 92.8 92.8 92.9
P (avg) 91.7 92.8 93.1 92.2 93.1 93.2 93.2 93.2 93.2 93.2
R (avg) 91.6 92.9 93.3 93.4 93.4 93.5 93.5 93.5 93.6 93.6
F (avg) 91.6 92.9 93.2 93.3 93.3 93.4 93.3 93.3 93.4 93.4
#Wrong sentences 3401 1652 945 621 463 288 217 176 151 139
</table>
<tableCaption confidence="0.959379">
Table 2: accuracy using non-averaged and averaged perceptron.
P - precision (%), R - recall (%), F - F-measure.
</tableCaption>
<table confidence="0.99901">
B 2 4 8 16 32 64 128 256 512 1024
Tr 660 610 683 830 1111 1645 2545 4922 9104 15598
Seg 18.65 18.18 28.85 26.52 36.58 56.45 95.45 173.38 325.99 559.87
F 86.90 92.95 93.33 93.38 93.25 93.29 93.19 93.07 93.24 93.34
</table>
<tableCaption confidence="0.819869">
Table 3: the influence of agenda size.
B - agenda size, Tr - training time (seconds), Seg - testing time (seconds), F - F-measure.
</tableCaption>
<figureCaption confidence="0.991288">
Figure 3: learning curves of the averaged and non-
averaged perceptron algorithms
</figureCaption>
<bodyText confidence="0.9981548">
ceptron algorithm improves the segmentation ac-
curacy at each iteration, compared with the non-
averaged perceptron. The learning curve was used
to fix the number of training iterations at 6 for the
remaining experiments.
</bodyText>
<subsectionHeader confidence="0.996121">
6.2 The influence of agenda size
</subsectionHeader>
<bodyText confidence="0.999974933333333">
Reducing the agenda size increases the decoding
speed, but it could cause loss of accuracy by elimi-
nating potentially good candidates. The agenda size
also affects the training time, and resulting model,
since the perceptron training algorithm uses the de-
coder output to adjust the model parameters. Table 3
shows the accuracies with ten different agenda sizes,
each used for both training and testing.
Accuracy does not increase beyond B = 16.
Moreover, the accuracy is quite competitive even
with B as low as 4. This reflects the fact that the best
segmentation is often within the current top few can-
didates in the agenda.2 Since the training and testing
time generally increases as N increases, the agenda
size is fixed to 16 for the remaining experiments.
</bodyText>
<subsectionHeader confidence="0.998142">
6.3 The influence of particular features
</subsectionHeader>
<bodyText confidence="0.999714">
Our CWS model is highly dependent upon word in-
formation. Most of the features in Table 1 are related
to words. Table 4 shows the accuracy with various
features from the model removed.
Among the features, vocabulary words (feature 1)
and length prediction by characters (features 3 to 5)
showed strong influence on the accuracy, while word
bigrams (feature 2) and special characters in them
(features 11 and 12) showed comparatively weak in-
fluence.
</bodyText>
<footnote confidence="0.92023">
2The optimization in Section 4, which has a pruning effect,
was applied to this experiment. Similar observations were made
in separate experiments without such optimization.
</footnote>
<figure confidence="0.997864307692308">
F-measure 0.93
0.92
0.91
0.9
0.89
0.88
0.94
0.87
0.86
non-averaged
averaged
1 2 3 4 5 6 7 8 9 10
number of training iterations
</figure>
<page confidence="0.99276">
845
</page>
<table confidence="0.999862">
Features F Features F
All 93.38 w/o 1 92.88
w/o 2 93.36 w/o 3, 4, 5 92.72
w/o 6 93.13 w/o 7 93.13
w/o 8 93.14 w/o 9, 10 93.31
w/o 11, 12 93.38 w/o 13, 14 93.23
</table>
<tableCaption confidence="0.941399">
Table 4: the influence of features. (F: F-measure.
Feature numbers are from Table 1)
</tableCaption>
<subsectionHeader confidence="0.973804">
6.4 Closed test on the SIGHAN bakeoffs
</subsectionHeader>
<bodyText confidence="0.99983925">
Four training and testing corpora were used in the
first bakeoff (Sproat and Emerson, 2003), including
the Academia Sinica Corpus (AS), the Penn Chinese
Treebank Corpus (CTB), the Hong Kong City Uni-
versity Corpus (CU) and the Peking University Cor-
pus (PU). However, because the testing data from
the Penn Chinese Treebank Corpus is currently un-
available, we excluded this corpus. The corpora are
encoded in GB (PU, CTB) and BIG5 (AS, CU). In
order to test them consistently in our system, they
are all converted to UTF8 without loss of informa-
tion.
The results are shown in Table 5. We follow the
format from Peng et al. (2004). Each row repre-
sents a CWS model. The first eight rows represent
models from Sproat and Emerson (2003) that partic-
ipated in at least one closed test from the table, row
“Peng” represents the CRF model from Peng et al.
(2004), and the last row represents our model. The
first three columns represent tests with the AS, CU
and PU corpora, respectively. The best score in each
column is shown in bold. The last two columns rep-
resent the average accuracy of each model over the
tests it participated in (SAV), and our average over
the same tests (OAV), respectively. For each row the
best average is shown in bold.
We achieved the best accuracy in two of the three
corpora, and better overall accuracy than the major-
ity of the other models. The average score of S10
is 0.7% higher than our model, but S10 only partici-
pated in the HK test.
Four training and testing corpora were used in
the second bakeoff (Emerson, 2005), including the
Academia Sinica corpus (AS), the Hong Kong City
University Corpus (CU), the Peking University Cor-
pus (PK) and the Microsoft Research Corpus (MR) .
</bodyText>
<table confidence="0.999045181818182">
AS CU PU SAV OAV
S01 93.8 90.1 95.1 93.0 95.0
S04 93.9 93.9 94.0
S05 94.2 89.4 91.8 95.3
S06 94.5 92.4 92.4 93.1 95.0
S08 90.4 93.6 92.0 94.3
S09 96.1 94.6 95.4 95.3
S10 94.7 94.7 94.0
S12 95.9 91.6 93.8 95.6
Peng 95.6 92.8 94.1 94.2 95.0
96.5 94.6 94.0
</table>
<tableCaption confidence="0.978212">
Table 5: the accuracies over the first SIGHAN bake-
off data.
</tableCaption>
<table confidence="0.999752142857143">
AS CU PK MR SAV OAV
S14 94.7 94.3 95.0 96.4 95.1 95.4
S15b 95.2 94.1 94.1 95.8 94.8 95.4
S27 94.5 94.0 95.0 96.0 94.9 95.4
Zh-a 94.7 94.6 94.5 96.4 95.1 95.4
Zh-b 95.1 95.1 95.1 97.1 95.6 95.4
94.6 95.1 94.5 97.2
</table>
<tableCaption confidence="0.886491">
Table 6: the accuracies over the second SIGHAN
bakeoff data.
</tableCaption>
<bodyText confidence="0.999368545454546">
Different encodings were provided, and the UTF8
data for all four corpora were used in this experi-
ment.
Following the format of Table 5, the results for
this bakeoff are shown in Table 6. We chose the
three models that achieved at least one best score
in the closed tests from Emerson (2005), as well as
the sub-word-based model of Zhang et al. (2006) for
comparison. Row “Zh-a” and “Zh-b” represent the
pure sub-word CRF model and the confidence-based
combination of the CRF and rule-based models, re-
spectively.
Again, our model achieved better overall accu-
racy than the majority of the other models. One sys-
tem to achieve comparable accuracy with our sys-
tem is Zh-b, which improves upon the sub-word CRF
model (Zh-a) by combining it with an independent
dictionary-based submodel and improving the accu-
racy of known words. In comparison, our system is
based on a single perceptron model.
In summary, closed tests for both the first and the
second bakeoff showed competitive results for our
</bodyText>
<page confidence="0.993834">
846
</page>
<bodyText confidence="0.9997724">
system compared with the best results in the litera-
ture. Our word-based system achieved the best F-
measures over the AS (96.5%) and CU (94.6%) cor-
pora in the first bakeoff, and the CU (95.1%) and
MR (97.2%) corpora in the second bakeoff.
</bodyText>
<sectionHeader confidence="0.995847" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999990142857143">
We proposed a word-based CWS model using the
discriminative perceptron learning algorithm. This
model is an alternative to the existing character-
based tagging models, and allows word information
to be used as features. One attractive feature of the
perceptron training algorithm is its simplicity, con-
sisting of only a decoder and a trivial update process.
We use a beam-search decoder, which places our
work in the context of recent proposals for search-
based discriminative learning algorithms. Closed
tests using the first and second SIGHAN CWS bake-
off data demonstrated our system to be competitive
with the best in the literature.
Open features, such as knowledge of numbers and
European letters, and relationships from semantic
networks (Shi and Wang, 2007), have been reported
to improve accuracy. Therefore, given the flexibility
of the feature-based perceptron model, an obvious
next step is the study of open features in the seg-
mentor.
Also, we wish to explore the possibility of in-
corporating POS tagging and parsing features into
the discriminative model, leading to joint decod-
ing. The advantage is two-fold: higher level syn-
tactic information can be used in word segmenta-
tion, while joint decoding helps to prevent bottom-
up error propagation among the different processing
steps.
</bodyText>
<sectionHeader confidence="0.998021" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999822">
This work is supported by the ORS and Clarendon
Fund. We thank the anonymous reviewers for their
insightful comments.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894344827586">
Michael Collins and Brian Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of ACL’04,
pages 111–118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP, pages 1–8,
Philadelphia, USA, July.
Hal Daume III. 2006. Practical Structured Learning for Natu-
ral Language Processing. Ph.D. thesis, USC.
Thomas Emerson. 2005. The second international Chinese
word segmentation bakeoff. In Proceedings of The Fourth
SIGHAN Workshop, Jeju, Korea.
Y. Freund and R. Schapire. 1999. Large margin classification
using the perceptron algorithm. In Machine Learning, pages
277–296.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of the 18th ICML,
pages 282–289, Massachusetts, USA.
Y. Li, Zaragoza, R. H., Herbrich, J. Shawe-Taylor, and J. Kan-
dola. 2002. The perceptron algorithm with uneven margins.
In Proceedings of the 9th ICML, pages 379–386, Sydney,
Australia.
Yaoyong Li, Chuanjiang Miao, Kalina Bontcheva, and Hamish
Cunningham. 2005. Perceptron learning for Chinese word
segmentation. In Proceedings of the Fourth SIGHAN Work-
shop, Jeju, Korea.
Percy Liang. 2005. Semi-supervised learning for natural lan-
guage. Master’s thesis, MIT.
F. Peng, F. Feng, , and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random fields.
In Proceedings of COLING, Geneva, Switzerland.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Nat-
ural Language Ambiguity Resolution. Ph.D. thesis, UPenn.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF
based joint decoding method for cascade segmentation and
labelling tasks. In Proceedings ofIJCAI, Hyderabad, India.
Richard Sproat and Thomas Emerson. 2003. The first interna-
tional Chinese word segmentation bakeoff. In Proceedings
of The Second SIGHAN Workshop, pages 282–289, Sapporo,
Japan, July.
R. Sproat, C. Shih, W. Gail, and N. Chang. 1996. A stochas-
tic finite-state word-segmentation algorithm for Chinese. In
Computational Linguistics, volume 22(3), pages 377–404.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and Xihong
Wu. 2006. Chinese word segmentation with maximum en-
tropy and n-gram language model. In Proceedings of the
Fifth SIGHAN Workshop, pages 138–141, Sydney, Australia,
July.
N. Xue. 2003. Chinese word segmentation as character tag-
ging. In International Journal of Computational Linguistics
and Chinese Language Processing, volume 8(1).
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006.
Subword-based tagging by conditional random fields for
Chinese word segmentation. In Proceedings of the Human
Language Technology Conference of the NAACL, Compan-
ion, volume Short Papers, pages 193–196, New York City,
USA, June.
</reference>
<page confidence="0.998224">
847
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823566">
<title confidence="0.99699">Chinese Segmentation with a Word-Based Perceptron Algorithm</title>
<author confidence="0.983238">Zhang Clark</author>
<affiliation confidence="0.999998">Oxford University Computing Laboratory</affiliation>
<address confidence="0.9319605">Wolfson Building, Parks Road Oxford OX1 3QD, UK</address>
<abstract confidence="0.9974259">Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary. Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation. In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences. The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder. Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL’04,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL’04, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="5515" citStr="Collins (2002)" startWordPosition="872" endWordPosition="873">nd Y is the set of possible segmented sentences. edge from various sources to be encoded as features. Given an input sentence x, the correct output segContextual information plays an important role in mentation F(x) satisfies: word segmentation decisions; especially useful is in- F(x) = arg max Score(y) formation about surrounding words. Consider the yEGEN(x) sentence “-Q*�A”, which can be from “-A- where GEN(x) denotes the set of possible segmen(among which) Q* (foreign) �A (companies)”, tations for an input sentence x, consistent with notaor “-Q (in China) *� (foreign companies) A tion from Collins (2002). -)� (business)”. Note that the five-character window The score for a segmented sentence is computed surrounding “*” is the same in both cases, making by first mapping it into a set of features. A feature the tagging decision for that character difficult given is an indicator of the occurrence of a certain pattern the local window. However, the correct decision can in a segmented sentence. For example, it can be the be made by comparison of the two three-word win- occurrence of “%�” as a single word, or the occurdows containing this character. rence of “%” separated from “ITii” in two adjacen</context>
<context position="8068" citStr="Collins (2002)" startWordPosition="1310" endWordPosition="1311">namic pro- parameter vector α E Rd, where αz is the weight for gramming and are thus able to exploit larger parts of the ith feature: the context for making decisions (Daume III, 2006). Score(y) = -b(y) · α We study several factors that influence the performance of the perceptron word segmentor, including the averaged perceptron method, the size of the 841 Inputs: training examples (xi, yi) Initialization: set α = 0 Algorithm: for t = 1..T, i = 1..N calculate zi = argmaxyEGEN(xi) 4&gt;(y) · α if zi =� yi α = α + 4&gt;(yi) − 4&gt;(zi) Outputs: α Figure 1: the perceptron learning algorithm, adapted from Collins (2002) The perceptron training algorithm is used to determine the weight values α. The training algorithm initializes the parameter vector as all zeros, and updates the vector by decoding the training examples. Each training sentence is turned into the raw input form, and then decoded with the current parameter vector. The output segmented sentence is compared with the original training example. If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output. The algorithm can perf</context>
<context position="9561" citStr="Collins, 2002" startWordPosition="1559" endWordPosition="1560">. Features are extracted from an input sequence x and its corresponding tag sequence y: Score(x, y) = 4&gt;(x, y) · α Our algorithm is not based on an HMM. For a given input sequence x, even the length of different candidates y (the number of words) is not fixed. Because the output sequence y (the segmented sentence) contains all the information from the input sequence x (the raw sentence), the global feature vector 4&gt;(x, y) is replaced with 4&gt;(y), which is extracted from the candidate segmented sentences directly. Despite the above differences, since the theorems of convergence and their proof (Collins, 2002) are only dependent on the feature vectors, and not on the source of the feature definitions, the perceptron algorithm is applicable to the training of our CWS model. 2.1 The averaged perceptron The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data. It was motivated by the votedperceptron algorithm (Freund and Schapire, 1999) and has been shown to give improved accuracy over the non-averaged perceptron on a number of tasks. Let N be the number of training sentences, T the number of training iterations, and αn,t the parameter vector</context>
<context position="15182" citStr="Collins, 2002" startWordPosition="2511" endWordPosition="2512">ame bigram, the best one will also be in the best final candidate. The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word. 5 Comparison with Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which an,t s 843 Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rul</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Practical Structured Learning for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis, USC.</tech>
<marker>Daume, 2006</marker>
<rawString>Hal Daume III. 2006. Practical Structured Learning for Natural Language Processing. Ph.D. thesis, USC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international Chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of The Fourth SIGHAN Workshop,</booktitle>
<location>Jeju,</location>
<contexts>
<context position="17114" citStr="Emerson, 2005" startWordPosition="2826" endWordPosition="2827">se Treebank 3 (since CTB3 was used as part of the first bakeoff). This corpus contains 240K characters (150K words and 4798 sentences). 80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data. The accuracies and learning curves for the non-averaged and averaged perceptron were compared. The influence of particular features and the agenda size were also studied. The second set of experiments used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson, 2003; Emerson, 2005). The accuracies are compared to other models in the literature. F-measure is used as the accuracy measure. Define precision p as the percentage of words in the decoder output that are segmented correctly, and recall r as the percentage of gold standard output words that are correctly segmented by the decoder. The (balanced) F-measure is 2pr/(p + r). CWS systems are evaluated by two types of tests. The closed tests require that the system is trained only with a designated training corpus. Any extra knowledge is not allowed, including common surnames, Chinese and Arabic numbers, European letter</context>
<context position="23986" citStr="Emerson, 2005" startWordPosition="4047" endWordPosition="4048">ts with the AS, CU and PU corpora, respectively. The best score in each column is shown in bold. The last two columns represent the average accuracy of each model over the tests it participated in (SAV), and our average over the same tests (OAV), respectively. For each row the best average is shown in bold. We achieved the best accuracy in two of the three corpora, and better overall accuracy than the majority of the other models. The average score of S10 is 0.7% higher than our model, but S10 only participated in the HK test. Four training and testing corpora were used in the second bakeoff (Emerson, 2005), including the Academia Sinica corpus (AS), the Hong Kong City University Corpus (CU), the Peking University Corpus (PK) and the Microsoft Research Corpus (MR) . AS CU PU SAV OAV S01 93.8 90.1 95.1 93.0 95.0 S04 93.9 93.9 94.0 S05 94.2 89.4 91.8 95.3 S06 94.5 92.4 92.4 93.1 95.0 S08 90.4 93.6 92.0 94.3 S09 96.1 94.6 95.4 95.3 S10 94.7 94.7 94.0 S12 95.9 91.6 93.8 95.6 Peng 95.6 92.8 94.1 94.2 95.0 96.5 94.6 94.0 Table 5: the accuracies over the first SIGHAN bakeoff data. AS CU PK MR SAV OAV S14 94.7 94.3 95.0 96.4 95.1 95.4 S15b 95.2 94.1 94.1 95.8 94.8 95.4 S27 94.5 94.0 95.0 96.0 94.9 95.4 </context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of The Fourth SIGHAN Workshop, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<pages>277--296</pages>
<contexts>
<context position="9951" citStr="Freund and Schapire, 1999" startWordPosition="1619" endWordPosition="1622">w sentence), the global feature vector 4&gt;(x, y) is replaced with 4&gt;(y), which is extracted from the candidate segmented sentences directly. Despite the above differences, since the theorems of convergence and their proof (Collins, 2002) are only dependent on the feature vectors, and not on the source of the feature definitions, the perceptron algorithm is applicable to the training of our CWS model. 2.1 The averaged perceptron The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data. It was motivated by the votedperceptron algorithm (Freund and Schapire, 1999) and has been shown to give improved accuracy over the non-averaged perceptron on a number of tasks. Let N be the number of training sentences, T the number of training iterations, and αn,t the parameter vector immediately after the nth sentence in the tth iteration. The averaged parameter vector γ E Rd is defined as: � αn,t n=1..N,t=1..T To compute the averaged parameters γ, the training algorithm in Figure 1 can be modified by keeping a total parameter vector σn,t = E αn,t, which is updated using α after each training example. After the final iteration, γ is computed as σn,t/NT. In the avera</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. In Machine Learning, pages 277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th ICML,</booktitle>
<pages>282--289</pages>
<location>Massachusetts, USA.</location>
<contexts>
<context position="4776" citStr="Lafferty et al., 2001" startWordPosition="748" endWordPosition="751"> algorithm can be used portance of word-based features for CWS. Furtherfor decoding. more, our approach provides an example of the poSeveral discriminatively trained models have re- tential of search-based discriminative training methcently been applied to the CWS problem. Exam- ods for NLP tasks. ples include Xue (2003), Peng et al. (2004) and Shi 2 The Perceptron Training Algorithm and Wang (2007); these use maximum entropy (ME) We formulate the CWS problem as finding a mapping and conditional random field (CRF) models (Ratna- from an input sentence x E X to an output sentence parkhi, 1998; Lafferty et al., 2001). An advantage y E Y , where X is the set of possible raw sentences of these models is their flexibility in allowing knowl- and Y is the set of possible segmented sentences. edge from various sources to be encoded as features. Given an input sentence x, the correct output segContextual information plays an important role in mentation F(x) satisfies: word segmentation decisions; especially useful is in- F(x) = arg max Score(y) formation about surrounding words. Consider the yEGEN(x) sentence “-Q*�A”, which can be from “-A- where GEN(x) denotes the set of possible segmen(among which) Q* (foreign</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th ICML, pages 282–289, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>R H Zaragoza</author>
<author>J Shawe-Taylor Herbrich</author>
<author>J Kandola</author>
</authors>
<title>The perceptron algorithm with uneven margins.</title>
<date>2002</date>
<booktitle>In Proceedings of the 9th ICML,</booktitle>
<pages>379--386</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="15003" citStr="Li et al., 2002" startWordPosition="2486" endWordPosition="2489">6 training iterations using the development data. For this particular feature set, the longest range features are word bigrams. Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate. The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word. 5 Comparison with Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which an,t s 843 Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference bet</context>
</contexts>
<marker>Li, Zaragoza, Herbrich, Kandola, 2002</marker>
<rawString>Y. Li, Zaragoza, R. H., Herbrich, J. Shawe-Taylor, and J. Kandola. 2002. The perceptron algorithm with uneven margins. In Proceedings of the 9th ICML, pages 379–386, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaoyong Li</author>
<author>Chuanjiang Miao</author>
<author>Kalina Bontcheva</author>
<author>Hamish Cunningham</author>
</authors>
<title>Perceptron learning for Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop,</booktitle>
<location>Jeju,</location>
<contexts>
<context position="14911" citStr="Li et al. (2005)" startWordPosition="2471" endWordPosition="2474"> vector according to these templates. There are 356,337 features with non-zero values after 6 training iterations using the development data. For this particular feature set, the longest range features are word bigrams. Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate. The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word. 5 Comparison with Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which an,t s 843 Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, m</context>
</contexts>
<marker>Li, Miao, Bontcheva, Cunningham, 2005</marker>
<rawString>Yaoyong Li, Chuanjiang Miao, Kalina Bontcheva, and Hamish Cunningham. 2005. Perceptron learning for Chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<tech>Master’s thesis, MIT.</tech>
<contexts>
<context position="15121" citStr="Liang (2005)" startWordPosition="2503" endWordPosition="2504">rams. Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate. The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word. 5 Comparison with Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which an,t s 843 Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-wo</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>F Feng</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Peng, Feng, 2004</marker>
<rawString>F. Peng, F. Feng, , and A. McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis, UPenn.</tech>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, UPenn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer CRF based joint decoding method for cascade segmentation and labelling tasks.</title>
<date>2007</date>
<booktitle>In Proceedings ofIJCAI,</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="26761" citStr="Shi and Wang, 2007" startWordPosition="4527" endWordPosition="4530">erbased tagging models, and allows word information to be used as features. One attractive feature of the perceptron training algorithm is its simplicity, consisting of only a decoder and a trivial update process. We use a beam-search decoder, which places our work in the context of recent proposals for searchbased discriminative learning algorithms. Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature. Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy. Therefore, given the flexibility of the feature-based perceptron model, an obvious next step is the study of open features in the segmentor. Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding. The advantage is two-fold: higher level syntactic information can be used in word segmentation, while joint decoding helps to prevent bottomup error propagation among the different processing steps. Acknowledgements This work is supported by the ORS and Clarendon Fund</context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Yanxin Shi and Mengqiu Wang. 2007. A dual-layer CRF based joint decoding method for cascade segmentation and labelling tasks. In Proceedings ofIJCAI, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international Chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of The Second SIGHAN Workshop,</booktitle>
<pages>282--289</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="17098" citStr="Sproat and Emerson, 2003" startWordPosition="2822" endWordPosition="2825">ank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff). This corpus contains 240K characters (150K words and 4798 sentences). 80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data. The accuracies and learning curves for the non-averaged and averaged perceptron were compared. The influence of particular features and the agenda size were also studied. The second set of experiments used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson, 2003; Emerson, 2005). The accuracies are compared to other models in the literature. F-measure is used as the accuracy measure. Define precision p as the percentage of words in the decoder output that are segmented correctly, and recall r as the percentage of gold standard output words that are correctly segmented by the decoder. The (balanced) F-measure is 2pr/(p + r). CWS systems are evaluated by two types of tests. The closed tests require that the system is trained only with a designated training corpus. Any extra knowledge is not allowed, including common surnames, Chinese and Arabic numbers,</context>
<context position="22533" citStr="Sproat and Emerson, 2003" startWordPosition="3785" endWordPosition="3788">ect, was applied to this experiment. Similar observations were made in separate experiments without such optimization. F-measure 0.93 0.92 0.91 0.9 0.89 0.88 0.94 0.87 0.86 non-averaged averaged 1 2 3 4 5 6 7 8 9 10 number of training iterations 845 Features F Features F All 93.38 w/o 1 92.88 w/o 2 93.36 w/o 3, 4, 5 92.72 w/o 6 93.13 w/o 7 93.13 w/o 8 93.14 w/o 9, 10 93.31 w/o 11, 12 93.38 w/o 13, 14 93.23 Table 4: the influence of features. (F: F-measure. Feature numbers are from Table 1) 6.4 Closed test on the SIGHAN bakeoffs Four training and testing corpora were used in the first bakeoff (Sproat and Emerson, 2003), including the Academia Sinica Corpus (AS), the Penn Chinese Treebank Corpus (CTB), the Hong Kong City University Corpus (CU) and the Peking University Corpus (PU). However, because the testing data from the Penn Chinese Treebank Corpus is currently unavailable, we excluded this corpus. The corpora are encoded in GB (PU, CTB) and BIG5 (AS, CU). In order to test them consistently in our system, they are all converted to UTF8 without loss of information. The results are shown in Table 5. We follow the format from Peng et al. (2004). Each row represents a CWS model. The first eight rows represen</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international Chinese word segmentation bakeoff. In Proceedings of The Second SIGHAN Workshop, pages 282–289, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>C Shih</author>
<author>W Gail</author>
<author>N Chang</author>
</authors>
<title>A stochastic finite-state word-segmentation algorithm for Chinese.</title>
<date>1996</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>22</volume>
<issue>3</issue>
<pages>377--404</pages>
<contexts>
<context position="2127" citStr="Sproat et al., 1996" startWordPosition="324" endWordPosition="327">se occurs when an OOV word consists , possible segmentations include “&amp;A (the discussion) &apos;L.: (will) TR (very) MSA (be successful)” and “&amp;A (the discussion meeting) TR (very) MSA (be successful)”. The ambiguity can only be resolved with contextual information outside the sentence. Human readers often use semantics, contextual information about the document and world knowledge to resolve segmentation ambiguities. There is no fixed standard for Chinese word segmentation. Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996). Also, specific NLP tasks may require different segmentation criteria. For example, “J L;5&apos;,W f j!” could be treated as a single word (Bank of Beijing) for machine translation, while it is more naturally segmented into “J L� (Beijing) Wf j! (bank)” for tasks such as text-to-speech synthesis. Therefore, supervised learning with specifically defined training data has become the dominant approach. Following Xue (2003), the standard approach to of characters which have themselves been seen as words; here an automatic segmentor may split the OOV word into individual single-character words. Typical</context>
<context position="16347" citStr="Sproat et al. (1996)" startWordPosition="2699" endWordPosition="2702">ormation, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, sub-words do not necessarily contain full word information. Moreover, sub-word extraction is performed separately from feature extraction. Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996). 6 Experiments Two sets of experiments were conducted. The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff). This corpus contains 240K characters (150K words and 4798 sentences). 80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data. The accuracies and learning curves for the non-averaged and averaged perceptron were compared. The influence of particular features and the agenda size were also studied. The second</context>
</contexts>
<marker>Sproat, Shih, Gail, Chang, 1996</marker>
<rawString>R. Sproat, C. Shih, W. Gail, and N. Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. In Computational Linguistics, volume 22(3), pages 377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinhao Wang</author>
<author>Xiaojun Lin</author>
<author>Dianhai Yu</author>
<author>Hao Tian</author>
<author>Xihong Wu</author>
</authors>
<title>Chinese word segmentation with maximum entropy and n-gram language model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop,</booktitle>
<pages>138--141</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="15456" citStr="Wang et al. (2006)" startWordPosition="2559" endWordPosition="2562">th Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which an,t s 843 Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, su</context>
</contexts>
<marker>Wang, Lin, Yu, Tian, Wu, 2006</marker>
<rawString>Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and Xihong Wu. 2006. Chinese word segmentation with maximum entropy and n-gram language model. In Proceedings of the Fifth SIGHAN Workshop, pages 138–141, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>In International Journal of Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2546" citStr="Xue (2003)" startWordPosition="393" endWordPosition="394">fixed standard for Chinese word segmentation. Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996). Also, specific NLP tasks may require different segmentation criteria. For example, “J L;5&apos;,W f j!” could be treated as a single word (Bank of Beijing) for machine translation, while it is more naturally segmented into “J L� (Beijing) Wf j! (bank)” for tasks such as text-to-speech synthesis. Therefore, supervised learning with specifically defined training data has become the dominant approach. Following Xue (2003), the standard approach to of characters which have themselves been seen as words; here an automatic segmentor may split the OOV word into individual single-character words. Typical examples of unseen words include Chinese names, translated foreign names and idioms. The segmentation of known words can also be ambiguous. For example, “iK IITiiI” should be “iK (here) IITi iI (flour)” in the sentence “iK IITiiI�H*TR &amp;quot;” (flour and rice are expensive here) or “iK (here) IITiiI (inside)” in the sentence “iK IITiiITR%�” (it’s cold inside here). The ambiguity can be resolved with information about the</context>
<context position="4476" citStr="Xue (2003)" startWordPosition="698" endWordPosition="699">fined to tem is competitive with the best systems, obtaining a five-character window with the current character the highest reported F-scores on a number of the in the middle. In this way, dynamic programming bakeoff corpora. These results demonstrate the imalgorithms such as the Viterbi algorithm can be used portance of word-based features for CWS. Furtherfor decoding. more, our approach provides an example of the poSeveral discriminatively trained models have re- tential of search-based discriminative training methcently been applied to the CWS problem. Exam- ods for NLP tasks. ples include Xue (2003), Peng et al. (2004) and Shi 2 The Perceptron Training Algorithm and Wang (2007); these use maximum entropy (ME) We formulate the CWS problem as finding a mapping and conditional random field (CRF) models (Ratna- from an input sentence x E X to an output sentence parkhi, 1998; Lafferty et al., 2001). An advantage y E Y , where X is the set of possible raw sentences of these models is their flexibility in allowing knowl- and Y is the set of possible segmented sentences. edge from various sources to be encoded as features. Given an input sentence x, the correct output segContextual information p</context>
<context position="15339" citStr="Xue, 2003" startWordPosition="2540" endWordPosition="2541"> items as a new word, only the best candidate is kept among those having the same last word. 5 Comparison with Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which an,t s 843 Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent mul</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>N. Xue. 2003. Chinese word segmentation as character tagging. In International Journal of Computational Linguistics and Chinese Language Processing, volume 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based tagging by conditional random fields for Chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion, volume Short Papers,</booktitle>
<pages>193--196</pages>
<location>New York City, USA,</location>
<contexts>
<context position="15756" citStr="Zhang et al. (2006)" startWordPosition="2612" endWordPosition="2615">minative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which an,t s 843 Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, sub-words do not necessarily contain full word information. Moreover, sub-word extraction is performed separately from feature extraction. Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996). 6 Exper</context>
<context position="25088" citStr="Zhang et al. (2006)" startWordPosition="4255" endWordPosition="4258">PK MR SAV OAV S14 94.7 94.3 95.0 96.4 95.1 95.4 S15b 95.2 94.1 94.1 95.8 94.8 95.4 S27 94.5 94.0 95.0 96.0 94.9 95.4 Zh-a 94.7 94.6 94.5 96.4 95.1 95.4 Zh-b 95.1 95.1 95.1 97.1 95.6 95.4 94.6 95.1 94.5 97.2 Table 6: the accuracies over the second SIGHAN bakeoff data. Different encodings were provided, and the UTF8 data for all four corpora were used in this experiment. Following the format of Table 5, the results for this bakeoff are shown in Table 6. We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison. Row “Zh-a” and “Zh-b” represent the pure sub-word CRF model and the confidence-based combination of the CRF and rule-based models, respectively. Again, our model achieved better overall accuracy than the majority of the other models. One system to achieve comparable accuracy with our system is Zh-b, which improves upon the sub-word CRF model (Zh-a) by combining it with an independent dictionary-based submodel and improving the accuracy of known words. In comparison, our system is based on a single perceptron model. In summary, closed tests for both the first and the second bak</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by conditional random fields for Chinese word segmentation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion, volume Short Papers, pages 193–196, New York City, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>