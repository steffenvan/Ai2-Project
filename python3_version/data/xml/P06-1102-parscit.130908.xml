<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000320">
<title confidence="0.894713">
Names and Similarities on the Web: Fact Extraction in the Fast Lane
</title>
<author confidence="0.953739">
Marius Pas¸ca Dekang Lin
</author>
<affiliation confidence="0.923162">
Google Inc. Google Inc.
</affiliation>
<address confidence="0.877893">
Mountain View, CA 94043 Mountain View, CA 94043
</address>
<email confidence="0.992059">
mars@google.com lindek@google.com
</email>
<author confidence="0.999517">
Jeffrey Bigham* Andrei Lifchits* Alpa Jain*
</author>
<affiliation confidence="0.999959">
University of Washington University of British Columbia Columbia University
</affiliation>
<address confidence="0.788948">
Seattle, WA 98195 Vancouver, BC V6T 1Z4 New York, NY 10027
</address>
<email confidence="0.998651">
jbigham@cs.washington.edu alifchit@cs.ubc.ca alpa@cs.columbia.edu
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972166666666">
In a new approach to large-scale extrac-
tion of facts from unstructured text, dis-
tributional similarities become an integral
part of both the iterative acquisition of
high-coverage contextual extraction pat-
terns, and the validation and ranking of
candidate facts. The evaluation mea-
sures the quality and coverage of facts
extracted from one hundred million Web
documents, starting from ten seed facts
and using no additional knowledge, lexi-
cons or complex tools.
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.955556">
1.1 Background
</subsectionHeader>
<bodyText confidence="0.996649925925926">
The potential impact of structured fact reposito-
ries containing billions of relations among named
entities on Web search is enormous. They en-
able the pursuit of new search paradigms, the pro-
cessing of database-like queries, and alternative
methods of presenting search results. The prepa-
ration of exhaustive lists of hand-written extrac-
tion rules is impractical given the need for domain-
independent extraction of many types of facts from
unstructured text. In contrast, the idea of boot-
strapping for relation and information extraction
was first proposed in (Riloff and Jones, 1999), and
successfully applied to the construction of seman-
tic lexicons (Thelen and Riloff, 2002), named en-
tity recognition (Collins and Singer, 1999), extrac-
tion of binary relations (Agichtein and Gravano,
2000), and acquisition of structured data for tasks
such as Question Answering (Lita and Carbonell,
2004; Fleischman et al., 2003). In the context of
fact extraction, the resulting iterative acquisition
*Work done during internships at Google Inc.
framework starts from a small set of seed facts,
finds contextual patterns that extract the seed facts
from the underlying text collection, identifies a
larger set of candidate facts that are extracted by
the patterns, and adds the best candidate facts to
the previous seed set.
</bodyText>
<subsectionHeader confidence="0.860423">
1.2 Contributions
</subsectionHeader>
<bodyText confidence="0.999912533333334">
Figure 1 describes an architecture geared towards
large-scale fact extraction. The architecture is sim-
ilar to other instances of bootstrapping for infor-
mation extraction. The main processing stages are
the acquisition of contextual extraction patterns
given the seed facts, acquisition of candidate facts
given the extraction patterns, scoring and ranking
of the patterns, and scoring and ranking of the can-
didate facts, a subset of which is added to the seed
set of the next round.
Within the existing iterative acquisition frame-
work, our first contribution is a method for au-
tomatically generating generalized contextual ex-
traction patterns, based on dynamically-computed
classes of similar words. Traditionally, the ac-
quisition of contextual extraction patterns requires
hundreds or thousands of consecutive iterations
over the entire text collection (Lita and Carbonell,
2004), often using relatively expensive or restric-
tive tools such as shallow syntactic parsers (Riloff
and Jones, 1999; Thelen and Riloff, 2002) or
named entity recognizers (Agichtein and Gravano,
2000). Comparatively, generalized extraction pat-
terns achieve exponentially higher coverage in
early iterations. The extraction of large sets of can-
didate facts opens the possibility of fast-growth it-
erative extraction, as opposed to the de-facto strat-
egy of conservatively growing the seed set by as
few as five items (Thelen and Riloff, 2002) after
each iteration.
</bodyText>
<page confidence="0.981154">
809
</page>
<note confidence="0.787436">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809–816,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Acquisition of contextual extraction patterns
</note>
<figureCaption confidence="0.996086">
Figure 1: Large-scale fact extraction architecture
</figureCaption>
<figure confidence="0.9180978125">
Occurrences of seed facts
Extraction patterns
Generalized extraction patterns
Seed facts
Text collection
Distributional similarities
Acquisition of candidate facts
Validation of candidate facts
Validated candidate facts
Candidate facts
Occurrences of extraction patterns
Scored candidate facts
Scoring and ranking
Validated extraction patterns
Scored extraction patterns
Validation of patterns
</figure>
<bodyText confidence="0.99963085">
The second contribution of the paper is a
method for domain-independent validation and
ranking of candidate facts, based on a similar-
ity measure of each candidate fact relative to the
set of seed facts. Whereas previous studies as-
sume clean text collections such as news cor-
pora (Thelen and Riloff, 2002; Agichtein and Gra-
vano, 2000; Hasegawa et al., 2004), the valida-
tion is essential for low-quality sets of candidate
facts collected from noisy Web documents. With-
out it, the addition of spurious candidate facts to
the seed set would result in a quick divergence of
the iterative acquisition towards irrelevant infor-
mation (Agichtein and Gravano, 2000). Further-
more, the finer-grained ranking induced by simi-
larities is necessary in fast-growth iterative acqui-
sition, whereas previously proposed ranking crite-
ria (Thelen and Riloff, 2002; Lita and Carbonell,
2004) are implicitly designed for slow growth of
the seed set.
</bodyText>
<table confidence="0.956766434782609">
(S1) Infix
Aurelio de la Vega was born November 28 , 1925 , in Havana , Cuba .
FW FW FW NNP VBD VBN NNP CD , CD , IN NNP , NNP .
not found found
(S2) Infix
The poet was born Jan. 13 , several years after the revolution .
DT NN VBD VBN NNP CD , JJ NNS IN DT NN .
not found not found
(S3) Infix
British − native Glenn Cornick of Jethro Tull was born April 23 , 1947 .
NNP : JJ NNP NNP IN NNP NNP VBD VBN NNP CD , CD .
found found
(S4) Infix
The youngest child of three siblings , Mariah Carey was born March 27 ,
DT JJS NN IN CD NNS , NNP NNP VBD VBN NNP CD ,
found
1970 in Huntington , Long Island in New York .
CD IN NNP , JJ NN IN NNP NNP .
found
(S5) Infix
Chester Burton Atkins was born June 20 , 1924, on a farm near Luttrell .
NNP NNP NNP VBD VBN NNP CD , CD , IN DT NN IN NNP .
found found
</table>
<figure confidence="0.7678715">
Candidate facts
(Irving Berlin, 1888)
NNP NNP CD
Prefix Infix Postfix
N/A
CL1 born CL2 00 ,
N/A
Seed fact Infix−only pattern
Matching on sentences
(Jethro Tull, 1947) (Mariah Carey, 1970) (Chester Burton Atkins, 1924)
</figure>
<sectionHeader confidence="0.963605" genericHeader="method">
2 Similarities for Pattern Acquisition
</sectionHeader>
<subsectionHeader confidence="0.965815">
2.1 Generalization via Word Similarities
</subsectionHeader>
<bodyText confidence="0.999827866666667">
The extraction patterns are acquired by matching
the pairs of phrases from the seed set into docu-
ment sentences. The patterns consist of contigu-
ous sequences of sentence terms, but otherwise
differ from the types of patterns proposed in earlier
work in two respects. First, the terms of a pattern
are either regular words or, for higher generality,
any word from a class of similar words. Second,
the amount of textual context encoded in a pat-
tern is limited to the sequence of terms between
(i.e., infix) the pair of phrases from a seed fact that
could be matched in a document sentence, thus ex-
cluding any context to the left (i.e., prefix) and to
the right (i.e., postfix) of the seed.
The pattern shown at the top of Figure 2, which
</bodyText>
<figureCaption confidence="0.998018">
Figure 2: Extraction via infix-only patterns
</figureCaption>
<bodyText confidence="0.999440285714286">
contains the sequence [CL1 born CL2 00 .], illus-
trates the use of classes of distributionally similar
words within extraction patterns. The first word
class in the sequence, CL1, consists of words such
as {was, is, could}, whereas the second class in-
cludes {February, April, June, Aug., November}
and other similar words. The classes of words are
computed on the fly over all sequences of terms
in the extracted patterns, on top of a large set of
pairwise similarities among words (Lin, 1998) ex-
tracted in advance from around 50 million news
articles indexed by the Google search engine over
three years. All digits in both patterns and sen-
tences are replaced with a common marker, such
</bodyText>
<page confidence="0.991745">
810
</page>
<bodyText confidence="0.999920888888889">
that any two numerical values with the same num-
ber of digits will overlap during matching.
Many methods have been proposed to compute
distributional similarity between words, e.g., (Hin-
dle, 1990), (Pereira et al., 1993), (Grefenstette,
1994) and (Lin, 1998). Almost all of the methods
represent a word by a feature vector, where each
feature corresponds to a type of context in which
the word appeared. They differ in how the feature
vectors are constructed and how the similarity be-
tween two feature vectors is computed.
In our approach, we define the features of a
word w to be the set of words that occurred within
a small window of w in a large corpus. The context
window of an instance of w consists of the clos-
est non-stopword on each side of w and the stop-
words in between. The value of a feature w&apos; is de-
fined as the pointwise mutual information between
</bodyText>
<equation confidence="0.5431855">
w&apos; and w: PMI(w&apos;, w) _ − log( P (w,w,)
P (w)P (w,)). The
</equation>
<bodyText confidence="0.999565111111111">
similarity between two different words w1 and w2,
5(w1, w2), is then computed as the cosine of the
angle between their feature vectors.
While the previous approaches to distributional
similarity have only applied to words, we applied
the same technique to proper names as well as
words. The following are some example similar
words and phrases with their similarities, as ob-
tained from the Google News corpus:
</bodyText>
<listItem confidence="0.948623">
• Carey: Higgins 0.39, Lambert 0.39, Payne
</listItem>
<bodyText confidence="0.9518208">
0.38, Kelley 0.38, Hayes 0.38, Goodwin 0.38,
Griffin 0.38, Cummings 0.38, Hansen 0.38,
Williamson 0.38, Peters 0.38, Walsh 0.38, Burke
0.38, Boyd 0.38, Andrews 0.38, Cunningham
0.38, Freeman 0.37, Stephens 0.37, Flynn 0.37,
Ellis 0.37, Bowers 0.37, Bennett 0.37, Matthews
0.37, Johnston 0.37, Richards 0.37, Hoffman
0.37, Schultz 0.37, Steele 0.37, Dunn 0.37, Rowe
0.37, Swanson 0.37, Hawkins 0.37, Wheeler 0.37,
Porter 0.37, Watkins 0.37, Meyer 0.37 [..];
</bodyText>
<listItem confidence="0.986318571428571">
• Mariah Carey: Shania Twain 0.38, Christina
Aguilera 0.35, Sheryl Crow 0.35, Britney Spears
0.33, Celine Dion 0.33, Whitney Houston 0.32,
Justin Timberlake 0.32, Beyonce Knowles 0.32,
Bruce Springsteen 0.30, Faith Hill 0.30, LeAnn
Rimes 0.30, Missy Elliott 0.30, Aretha Franklin
0.29, Jennifer Lopez 0.29, Gloria Estefan 0.29,
Elton John 0.29, Norah Jones 0.29, Missy
Elliot 0.29, Alicia Keys 0.29, Avril Lavigne
0.29, Kid Rock 0.28, Janet Jackson 0.28, Kylie
Minogue 0.28, Beyonce 0.27, Enrique Iglesias
0.27, Michelle Branch 0.27 [..];
• Jethro Tull: Motley Crue 0.28, Black Crowes
0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sab-
</listItem>
<bodyText confidence="0.973326852941176">
bath 0.26, Doobie Brothers 0.26, Judas Priest 0.26,
Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24,
Black Flag 0.24, Godsmack 0.24, Grateful Dead
0.24, Grand Funk Railroad 0.24, Smashing Pump-
kins 0.24, Led Zeppelin 0.24, Aerosmith 0.24,
Limp Bizkit 0.24, Counting Crows 0.24, Echo
And The Bunnymen 0.24, Cold Chisel 0.24, Thin
Lizzy 0.24 [..].
To our knowledge, the only previous study that
embeds similarities into the acquisition of extrac-
tion patterns is (Stevenson and Greenwood, 2005).
The authors present a method for computing pair-
wise similarity scores among large sets of poten-
tial syntactic (subject-verb-object) patterns, to de-
tect centroids of mutually similar patterns. By as-
suming the syntactic parsing of the underlying text
collection to generate the potential patterns in the
first place, the method is impractical on Web-scale
collections. Two patterns, e.g. chairman-resign
and CEO-quit, are similar to each other if their
components are present in an external hand-built
ontology (i.e., WordNet), and the similarity among
the components is high over the ontology. Since
general-purpose ontologies, and WordNet in par-
ticular, contain many classes (e.g., chairman and
CEO) but very few instances such as Osasuna,
Crewe etc., the patterns containing an instance
rather than a class will not be found to be simi-
lar to one another. In comparison, the classes and
instances are equally useful in our method for gen-
eralizing patterns for fact extraction. We merge
basic patterns into generalized patterns, regardless
of whether the similar words belong, as classes or
instances, in any external ontology.
</bodyText>
<subsectionHeader confidence="0.996795">
2.2 Generalization via Infix-Only Patterns
</subsectionHeader>
<bodyText confidence="0.999961846153846">
By giving up the contextual constraints imposed
by the prefix and postfix, infix-only patterns rep-
resent the most aggressive type of extraction pat-
terns that still use contiguous sequences of terms.
In the absence of the prefix and postfix, the outer
boundaries of the fact are computed separately for
the beginning of the first (left) and end of the sec-
ond (right) phrases of the candidate fact. For gen-
erality, the computation relies only on the part-
of-speech tags of the current seed set. Starting
forward from the right extremity of the infix, we
collect a growing sequence of terms whose part-
of-speech tags are [P1+ P2+ .. Pn+], where the
</bodyText>
<page confidence="0.987065">
811
</page>
<bodyText confidence="0.999470038461538">
notation PZ+ represents one or more consecutive
occurrences of the part-of-speech tag PZ. The se-
quence [P1 P2 .. P�] must be exactly the sequence
of part of speech tags from the right side of one of
the seed facts. The point where the sequence can-
not be grown anymore defines the boundary of the
fact. A similar procedure is applied backwards,
starting from the left extremity of the infix. An
infix-only pattern produces a candidate fact from
a sentence only if an acceptable sequence is found
to the left and also to the right of the infix.
Figure 2 illustrates the process on the infix-
only pattern mentioned earlier, and one seed fact.
The part-of-speech tags for the seed fact are [NNP
NNP] and [CD] for the left and right sides respec-
tively. The infix occurs in all sentences. How-
ever, the matching of the part-of-speech tags of the
sentence sequences to the left and right of the in-
fix, against the part-of-speech tags of the seed fact,
only succeeds for the last three sentences. It fails
for the first sentence S1 to the left of the infix, be-
cause [.. NNP] (for Vega) does not match [NNP
NNP]. It also fails for the second sentence S2 to
both the left and the right side of the infix, since [..
NN] (for poet) does not match [NNP NNP], and
[JJ ..] (for several) does not match [CD].
</bodyText>
<sectionHeader confidence="0.995658" genericHeader="method">
3 Similarities for Validation and Ranking
</sectionHeader>
<subsectionHeader confidence="0.99932">
3.1 Revisiting Standard Ranking Criteria
</subsectionHeader>
<bodyText confidence="0.999972878048781">
Because some of the acquired extraction patterns
are too generic or wrong, all approaches to iter-
ative acquisition place a strong emphasis on the
choice of criteria for ranking. Previous literature
quasi-unanimously assesses the quality of each
candidate fact based on the number and qual-
ity of the patterns that extract the candidate fact
(more is better); and the number of seed facts ex-
tracted by the same patterns (again, more is bet-
ter) (Agichtein and Gravano, 2000; Thelen and
Riloff, 2002; Lita and Carbonell, 2004). However,
our experiments using many variations of previ-
ously proposed scoring functions suggest that they
have limited applicability in large-scale fact ex-
traction, for two main reasons. The first is that
it is impractical to perform hundreds of acquisi-
tion iterations on terabytes of text. Instead, one
needs to grow the seed set aggressively in each
iteration. Previous scoring functions were im-
plicitly designed for cautious acquisition strate-
gies (Collins and Singer, 1999), which expand the
seed set very slowly across consecutive iterations.
In that case, it makes sense to single out a small
number of best candidates, among the other avail-
able candidates. Comparatively, when 10,000 can-
didate facts or more need to be added to a seed set
of 10 seeds as early as after the first iteration, it
is difficult to distinguish the quality of extraction
patterns based, for instance, only on the percent-
age of the seed set that they extract. The second
reason is the noisy nature of the Web. A substan-
tial number of factors can and will concur towards
the worst-case extraction scenarios on the Web.
Patterns of apparently high quality turn out to pro-
duce a large quantity of erroneous “facts” such as
(A-League, 1997), but also the more interesting
(Jethro Tull, 1947) as shown earlier in Figure 2, or
(Web Site David, 1960) or (New York, 1831). As
for extraction patterns of average or lower quality,
they will naturally lead to even more spurious ex-
tractions.
</bodyText>
<subsectionHeader confidence="0.9999">
3.2 Ranking of Extraction Patterns
</subsectionHeader>
<bodyText confidence="0.999990789473684">
The intuition behind our criteria for ranking gen-
eralized pattern is that patterns of higher preci-
sion tend to contain words that are indicative of
the relation being mined. Thus, a pattern is more
likely to produce good candidate facts if its in-
fix contains the words language or spoken if ex-
tracting Language-SpokenIn-Country facts, or the
word capital if extracting City-CapitalOf-Country
relations. In each acquisition iteration, the scor-
ing of patterns is a two-pass procedure. The first
pass computes the normalized frequencies of all
words excluding stopwords, over the entire set of
extraction patterns. The computation applies sep-
arately to the prefix, infix and postfix of the pat-
terns. In the second pass, the score of an extraction
pattern is determined by the words with the high-
est frequency score in its prefix, infix and postfix,
as computed in the first pass and adjusted for the
relative distance to the start and end of the infix.
</bodyText>
<subsectionHeader confidence="0.99992">
3.3 Ranking of Candidate Facts
</subsectionHeader>
<bodyText confidence="0.9998548">
Figure 3 introduces a new scheme for assessing the
quality of the candidate facts, based on the compu-
tation of similarity scores for each candidate rela-
tive to the set of seed facts. A candidate fact, e.g.,
(Richard Steele, 1672), is similar to the seed set if
both its phrases, i.e., Richard Steele and 1672, are
similar to the corresponding phrases (John Lennon
or Stephen Foster in the case of Richard Steele)
from the seed facts. For a phrase of a candidate
fact to be assigned a non-default (non-minimum)
</bodyText>
<page confidence="0.96558">
812
</page>
<figure confidence="0.999911594936709">
Seed facts
Similar words
(3)
John
Chris
James
Andrew
Mike
Matt
Brian
Christopher
Similar words
for: John
Stephen
Richard
Stan
Robert
Michael
Peter
Barbara
William
...
Similar words
for: Stephen
(1)
(8)
(5)
(7)
for: Lennon
Lennon
Lambert
McNamara
Bateson
McFadden
Costello
Cronin
Wooley
...
Foster
Hansen
Hawkins
Fisher
Baker
Holloway
Steele
Sweeney
...
Similar words
for: Foster
(1)
(2)
Richard Steele
1672
(3)
(4)
Brian McFadden
1980
(5)
(6)
Robert S. McNamara
1916
Barbara Steele
1937
(8)
(9)
Stan Hansen
1949
Jethro Tull
1947
Candidate facts
John Lennon
Stephen Foster
1940
1826
(7) (2)
(6)
(4)
(9)
(2)
</figure>
<figureCaption confidence="0.982527">
Figure 3: The role of similarities in estimating the
quality of candidate facts
</figureCaption>
<bodyText confidence="0.999400636363636">
similarity score, the words at its extremities must
be similar to one or more words situated at the
same positions in the seed facts. This is the case
for the first five candidate facts in Figure 3. For ex-
ample, the first word Richard from one of the can-
didate facts is similar to the first word John from
one of the seed facts. Concurrently, the last word
Steele from the same phrase is similar to Foster
from another seed fact. Therefore Robert Foster
is similar to the seed facts. The score of a phrase
containing N words is:
</bodyText>
<equation confidence="0.511025">
�C1 + ENi�1 log(1 + Simi) , if Sim1,N &gt; 0
C2 , otherwise.
</equation>
<bodyText confidence="0.999967042553192">
where Simi is the similarity of the component
word at position i in the phrase, and C1 and C2
are scaling constants such that C2«C1. Thus,
the similarity score of a candidate fact aggregates
individual word-to-word similarity scores, for the
left side and then for the right side of a candidate
fact. In turn, the similarity score of a component
word Simi is higher if: a) the computed word-to-
word similarity scores are higher relative to words
at the same position i in the seeds; and b) the com-
ponent word is similar to words from more than
one seed fact.
The similarity scores are one of a linear com-
bination of features that induce a ranking over the
candidate facts. Three other domain-independent
features contribute to the final ranking: a) a phrase
completeness score computed statistically over the
entire set of candidate facts, which demotes candi-
date facts if any of their two sides is likely to be
incomplete (e.g., Mary Lou vs. Mary Lou Retton,
or John F. vs. John F. Kennedy); b) the average
PageRank value over all documents from which
the candidate fact is extracted; and c) the pattern-
based scores of the candidate fact. The latter fea-
ture converts the scores of the patterns extracting
the candidate fact into a score for the candidate
fact. For this purpose, it considers a fixed-length
window of words around each match of a candi-
date fact in some sentence from the text collection.
This is equivalent to analyzing all sentence con-
texts from which a candidate fact can be extracted.
For each window, the word with the highest fre-
quency score, as computed in the first pass of the
procedure for scoring the patterns, determines the
score of the candidate fact in that context. The
overall pattern-based score of a candidate fact is
the sum of the scores over all its contexts of occur-
rence, normalized by the frequency of occurrence
of the candidate over all sentences.
Besides inducing a ranking over the candidate
facts, the similarity scores also serve as a valida-
tion filter over the candidate facts. Indeed, any
candidates that are not similar to the seed set can
be filtered out. For instance, the elimination of
(Jethro Tull, 1947) is a side effect of verifying that
Tull is not similar to any of the last-position words
from phrases in the seed set.
</bodyText>
<sectionHeader confidence="0.998823" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.933078">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999983952380953">
The source text collection consists of three chunks
W1, W2, W3 of approximately 100 million doc-
uments each. The documents are part of a larger
snapshot of the Web taken in 2003 by the Google
search engine. All documents are in English.
The textual portion of the documents is cleaned
of Html, tokenized, split into sentences and part-
of-speech tagged using the TnT tagger (Brants,
2000).
The evaluation involves facts of type Person-
BornIn-Year. The reasons behind the choice of
this particular type are threefold. First, many
Person-BornIn-Year facts are probably available
on the Web (as opposed to, e.g., City-CapitalOf-
Country facts), to allow for a good stress test
for large-scale extraction. Second, either side of
the facts (Person and Year) may be involved in
many other types of facts, such that the extrac-
tion would easily divergence unless it performs
correctly. Third, the phrases from one side (Per-
son) have an utility in their own right, for lexicon
</bodyText>
<page confidence="0.999486">
813
</page>
<tableCaption confidence="0.99981">
Table 1: Set of seed Person-BornIn-Year facts
</tableCaption>
<table confidence="0.920339">
Name Year Name Year
Paul McCartney 1942 John Lennon 1940
Vincenzo Bellini 1801 Stephen Foster 1826
Hoagy Carmichael 1899 Irving Berlin 1888
Johann Sebastian Bach 1685 Bela Bartok 1881
Ludwig van Beethoven 1770 Bob Dylan 1941
construction or detection of person names.
</table>
<bodyText confidence="0.9747825">
The Person-BornIn-Year type is specified
through an initial set of 10 seed facts shown in Ta-
ble 1. Similarly to source documents, the facts are
also part-of-speech tagged.
</bodyText>
<subsectionHeader confidence="0.998428">
4.2 System Settings
</subsectionHeader>
<bodyText confidence="0.9999518125">
In each iteration, the case-insensitive matching of
the current set of seed facts onto the sentences pro-
duces basic patterns. The patterns are converted
into generalized patterns. The length of the infix
may vary between 1 and 6 words. Potential pat-
terns are discarded if the infix contains only stop-
words.
When a pattern is retained, it is used as an
infix-only pattern, and allowed to generate at most
600,000 candidate facts. At the end of an itera-
tion, approximately one third of the validated can-
didate facts are added to the current seed set. Con-
sequently, the acquisition expands the initial seed
set of 10 facts to 100,000 facts (after iteration 1)
and then to one million facts (after iteration 2) us-
ing chunk W1.
</bodyText>
<subsectionHeader confidence="0.997823">
4.3 Precision
</subsectionHeader>
<bodyText confidence="0.999984984848485">
A separate baseline run extracts candidate facts
from the text collection following the traditional
iterative acquisition approach. Pattern general-
ization is disabled, and the ranking of patterns
and facts follows strictly the criteria and scoring
functions from (Thelen and Riloff, 2002), which
are also used in slightly different form in (Lita
and Carbonell, 2004) and (Agichtein and Gravano,
2000). The theoretical option of running thou-
sands of iterations over the text collection is not
viable, since it would imply a non-justifiable ex-
pense of our computational resources. As a more
realistic compromise over overly-cautious acqui-
sition, the baseline run retains as many of the top
candidate facts as the size of the current seed,
whereas (Thelen and Riloff, 2002) only add the
top five candidate facts to the seed set after each it-
eration. The evaluation considers all 80, a sample
of the 320, and another sample of the 10,240 facts
retained after iterations 3, 5 and 10 respectively.
The correctness assessment of each fact consists
in manually finding some Web page that contains
clear evidence that the fact is correct. If no such
page exists, the fact is marked as incorrect. The
corresponding precision values after the three iter-
ations are 91.2%, 83.8% and 72.9%.
For the purpose of evaluating the precision of
our system, we select a sample of facts from
the entire list of one million facts extracted from
chunk W1, ranked in decreasing order of their
computed scores. The sample is generated auto-
matically from the top of the list to the bottom, by
retaining a fact and skipping the following consec-
utive N facts, where N is incremented at each step.
The resulting list, which preserves the relative or-
der of the facts, contains 1414 facts. The 115 facts
for which a Web search engine does not return any
documents, when the name (as a phrase) and the
year are submitted together in a conjunctive query,
are discarded from the sample of 1414 facts. In
those cases, the facts were acquired from the 2003
snapshot of the Web, but queries are submitted to
a search engine with access to current Web doc-
uments, hence the difference when some of the
2003 documents are no longer available or index-
able.
Based on the sample set, the average preci-
sion of the list of one million facts extracted from
chunk W1 is 98.5% over the top 1/100 of the list,
93.1% over the top half of the list, and 88.3% over
the entire list of one million facts. Table 2 shows
examples of erroneous facts extracted from chunk
W1. Causes of errors include incorrect approxima-
tions of the name boundaries (e.g., Alma in Alma
Theresa Rausch is incorrectly tagged as an adjec-
tive), and selection of the wrong year as birth year
(e.g., for Henry Lumbar).
In the case of famous people, the extracted facts
tend to capture the correct birth year for several
variations of the names, as shown in Table 3. Con-
versely, it is not necessary that a fact occur with
high frequency in order for it to be extracted,
which is an advantage over previous approaches
that rely strongly on redundancy (cf. (Cafarella et
al., 2005)). Table 4 illustrates a few of the cor-
rectly extracted facts that occur rarely on the Web.
</bodyText>
<subsectionHeader confidence="0.986345">
4.4 Recall
</subsectionHeader>
<bodyText confidence="0.998625">
In contrast to the assessment of precision, recall
can be evaluated automatically, based on external
</bodyText>
<page confidence="0.998982">
814
</page>
<tableCaption confidence="0.996664">
Table 2: Incorrect facts extracted from the Web
</tableCaption>
<table confidence="0.999694789473684">
Spurious Fact Context in Source Sentence
(Theresa Rausch, Alma Theresa Rausch was born
1912) on 9 March 1912
(Henry Lumbar, Henry Lumbar was born 1861
1937) and died 1937
(Concepcion Paxety, Maria de la Concepcion Paxety
1817) b. 08 Dec. 1817 St. Aug., FL.
(Mae Yaeger, Ella May/Mae Yaeger was born
1872) 20 May 1872 in Mt.
(Charles Whatley, Long, Charles Whatley b. 16
1821) FEB 1821 d. 29 AUG
(HOLT George W. HOLT (new line) George W. Holt
Holt, 1845) was born in Alabama in 1845
(David Morrish David Morrish (new line)
Canadian, 1953) Canadian, b. 1953
(Mary Ann, 1838) had a daughter, Mary Ann, who
was born in Tennessee in 1838
(Mrs. Blackmore, Mrs. Blackmore was born April
1918) 28, 1918, in Labaddiey
</table>
<tableCaption confidence="0.994317">
Table 3: Birth years extracted for both
pseudonyms and corresponding real names
</tableCaption>
<table confidence="0.966872625">
Pseudonym Real Name Year
Gloria Estefan Gloria Fajardo 1957
Nicolas Cage Nicolas Kim Coppola 1964
Ozzy Osbourne John Osbourne 1948
Ringo Starr Richard Starkey 1940
Tina Turner Anna Bullock 1939
Tom Cruise Thomas Cruise Mapother IV 1962
Woody Allen Allen Stewart Konigsberg 1935
</table>
<bodyText confidence="0.999089958333333">
lists of birth dates of various people. We start by
collecting two gold standard sets of facts. The first
set is a random set of 609 actors and their birth
years from a Web compilation (GoldA). The sec-
ond set is derived from the set of questions used
in the Question Answering track (Voorhees and
Tice, 2000) of the Text REtrieval Conference from
1999 through 2002. Each question asking for the
birth date of a person (e.g., “What year was Robert
Frost born?”) results in a pair containing the per-
son’s name and the birth year specified in the an-
swer keys. Thus, the second gold standard set
contains 17 pairs of people and their birth years
(GoldT). Table 5 shows examples of facts in each
of the gold standard sets.
Table 6 shows two types of recall scores com-
puted against the gold standard sets. The recall
scores over nGold take into consideration only the
set of person names from the gold standard with
some extracted year(s). More precisely, given that
some years were extracted for a person name, it
verifies whether they include the year specified in
the gold standard for that person name. Compar-
atively, the recall score denoted AllGold is com-
</bodyText>
<tableCaption confidence="0.998058">
Table 4: Extracted facts that occur infrequently
</tableCaption>
<table confidence="0.999952">
Fact Source Domain
(Irvine J Forcier, 1912) geocities.com
(Marie Louise Azelie Chabert, 1861) vienici.com
(Jacob Shalles, 1750) selfhost.com
(Robert Chester Claggett, 1898) rootsweb.com
(Charoltte Mollett, 1843) rootsweb.com
(Nora Elizabeth Curran, 1979) jimtravis.com
</table>
<tableCaption confidence="0.996602">
Table 5: Composition of gold standard sets
</tableCaption>
<table confidence="0.999179142857143">
Gold Set Composition and Examples of Facts
Golder Actors (Web compilation) Nr. facts: 609
(Andie MacDowell, 1958), (Doris Day,
1924), (Diahann Carroll, 1935)
GoldT People (TREC QA track) Nr. facts: 17
(Davy Crockett, 1786), (Julius Caesar,
100 B.C.), (King Louis XIV, 1638)
</table>
<bodyText confidence="0.999385714285714">
puted over the entire set of names from the gold
standard.
For the GoldA set, the size of the nGold set of
person names changes little when the facts are ex-
tracted from chunk W1 vs. W2 vs. W3. The re-
call scores over nGold exhibit little variation from
one Web chunk to another, whereas the AllGold
score is slightly higher on the W3 chunk, prob-
ably due to a higher number of documents that
are relevant to the extraction task. When the facts
are extracted from a combination of two or three
of the available Web chunks, the recall scores
computed over AllGold are significantly higher as
the size of the nGold set increases. In compar-
ison, the recall scores over the growing nGold
set increases slightly with larger evaluation sets.
The highest value of the recall score for GoldA
is 89.9% over the nGold set, and 70.7% over
AllGold. The smaller size of the second gold stan-
dard set, GoldT, explains the higher variation of
the values shown in the lower portion of Table 6.
</bodyText>
<subsectionHeader confidence="0.996087">
4.5 Comparison to Previous Results
</subsectionHeader>
<bodyText confidence="0.999967583333333">
Another recent approach specifically addresses the
problem of extracting facts from a similarly-sized
collection of Web documents. In (Cafarella et al.,
2005), manually-prepared extraction rules are ap-
plied to a collection of 60 million Web documents
to extract entities of types Company and Country,
as well as facts of type Person-CeoOf-Company
and City-CapitalOf-Country. Based on manual
evaluation of precision and recall, a total of 23,128
company names are extracted at precision of 80%;
the number decreases to 1,116 at precision of 90%.
In addition, 2,402 Person-CeoOf-Company facts
</bodyText>
<page confidence="0.999063">
815
</page>
<tableCaption confidence="0.836898666666667">
Table 6: Automatic evaluation of recall, over two
gold standard sets GoldA (609 person names) and
GoldT (17 person names)
</tableCaption>
<table confidence="0.999132461538461">
Gold Set Input Data Recall (%)
(Web Chunk)
nGold AllGold
GoldA W1 86.4 49.4
W2 85.0 50.5
W3 86.3 54.1
W1+W2 88.5 64.5
W1+W2+W3 89.9 70.7
GoldT W1 81.8 52.9
W2 90.0 52.9
W3 100.0 64.7
W1+W2 81.8 52.9
W1+W2+W3 91.6 64.7
</table>
<bodyText confidence="0.999702444444444">
are extracted at precision 80%. The recall value is
80% at precision 90%. Recall is evaluated against
the set of company names extracted by the system,
rather than an external gold standard with pairs of
a CEO and a company name. As such, the result-
ing metric for evaluating recall used in (Cafarella
et al., 2005) is somewhat similar to, though more
relaxed than, the recall score over the nGold set
introduced in the previous section.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999973761904762">
The combination of generalized extraction pat-
terns and similarity-driven ranking criteria results
in a fast-growth iterative approach for large-scale
fact extraction. From 10 Person-BornIn-Year facts
and no additional knowledge, a set of one million
facts of the same type is extracted from a collec-
tion of 100 million Web documents of arbitrary
quality, with a precision around 90%. This cor-
responds to a growth ratio of 100,000:1 between
the size of the extracted set of facts and the size
of the initial set of seed facts. To our knowledge,
the growth ratio and the number of extracted facts
are several orders of magnitude higher than in any
of the previous studies on fact extraction based on
either hand-written extraction rules (Cafarella et
al., 2005), or bootstrapping for relation and infor-
mation extraction (Agichtein and Gravano, 2000;
Lita and Carbonell, 2004). The next research steps
converge towards the automatic construction of a
searchable repository containing billions of facts
regarding people.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999675940298507">
E. Agichtein and L. Gravano. 2000. Snowball: Extracting
relations from large plaintext collections. In Proceedings
of the 5th ACM International Conference on Digital Li-
braries (DL-00), pages 85–94, San Antonio, Texas.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natural
Language Processing (ANLP-00), pages 224–231, Seattle,
Washington.
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. KnowItNow: Fast, scalable information extrac-
tion from the web. In Proceedings of the Human Lan-
guage Technology Conference (HLT-EMNLP-05), pages
563–570, Vancouver, Canada.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the 1999
Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP/VLC-99),
pages 189–196, College Park, Maryland.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of the 41st
Annual Meeting of the Association for Computational Lin-
guistics (ACL-03), pages 1–7, Sapporo, Japan.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus
Discovery. Kluwer Academic Publishers, Boston, Mas-
sachusetts.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415–
422, Barcelona, Spain.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of the 28th Annual
Meeting of the Association for Computational Linguistics
(ACL-90), pages 268–275, Pittsburgh, Pennsylvania.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of the 17th International Confer-
ence on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Linguistics
(COLING-ACL-98), pages 768–774, Montreal, Quebec.
L. Lita and J. Carbonell. 2004. Instance-based ques-
tion answering: A data driven approach. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-04), pages 396–403,
Barcelona, Spain.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of english words. In Proceedings of the 31st Annual
Meeting of the Association for Computational Linguistics
(ACL-93), pages 183–190, Columbus, Ohio.
E. Riloff and R. Jones. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In Pro-
ceedings of the 16th National Conference on ArtificialIn-
telligence (AAAI-99), pages 474–479, Orlando, Florida.
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to IE pattern induction. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-05), pages 379–386, Ann Arbor, Michigan.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-02),
pages 214–221, Philadelphia, Pennsylvania.
E.M. Voorhees and D.M. Tice. 2000. Building a question-
answering test collection. In Proceedings of the 23rd
International Conference on Research and Development
in Information Retrieval (SIGIR-00), pages 200–207,
Athens, Greece.
</reference>
<page confidence="0.998821">
816
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954370">
<title confidence="0.999157">Names and Similarities on the Web: Fact Extraction in the Fast Lane</title>
<author confidence="0.994178">Dekang Lin</author>
<affiliation confidence="0.999539">Google Inc. Google Inc.</affiliation>
<address confidence="0.999848">Mountain View, CA 94043 Mountain View, CA 94043</address>
<email confidence="0.997019">mars@google.comlindek@google.com</email>
<author confidence="0.989233">Jeffrey</author>
<affiliation confidence="0.999999">University of Washington University of British Columbia Columbia University</affiliation>
<address confidence="0.999951">Seattle, WA 98195 Vancouver, BC V6T 1Z4 New York, NY 10027</address>
<email confidence="0.991672">jbigham@cs.washington.edualifchit@cs.ubc.caalpa@cs.columbia.edu</email>
<abstract confidence="0.998638692307692">In a new approach to large-scale extraction of facts from unstructured text, distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns, and the validation and ranking of candidate facts. The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents, starting from ten seed facts and using no additional knowledge, lexicons or complex tools.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plaintext collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th ACM International Conference on Digital Libraries (DL-00),</booktitle>
<pages>85--94</pages>
<location>San Antonio, Texas.</location>
<contexts>
<context position="1735" citStr="Agichtein and Gravano, 2000" startWordPosition="252" endWordPosition="255">ew search paradigms, the processing of database-like queries, and alternative methods of presenting search results. The preparation of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition *Work done during internships at Google Inc. framework starts from a small set of seed facts, finds contextual patterns that extract the seed facts from the underlying text collection, identifies a larger set of candidate facts that are extracted by the patterns, and adds the best candidate facts to the previous seed set. 1.2 Contributions Figure 1 describes an architecture geared towards large-scal</context>
<context position="3352" citStr="Agichtein and Gravano, 2000" startWordPosition="494" endWordPosition="497">o the seed set of the next round. Within the existing iterative acquisition framework, our first contribution is a method for automatically generating generalized contextual extraction patterns, based on dynamically-computed classes of similar words. Traditionally, the acquisition of contextual extraction patterns requires hundreds or thousands of consecutive iterations over the entire text collection (Lita and Carbonell, 2004), often using relatively expensive or restrictive tools such as shallow syntactic parsers (Riloff and Jones, 1999; Thelen and Riloff, 2002) or named entity recognizers (Agichtein and Gravano, 2000). Comparatively, generalized extraction patterns achieve exponentially higher coverage in early iterations. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. 809 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809–816, Sydney, July 2006. c�2006 Association for Computational Linguistics Acquisition of contextual extraction </context>
<context position="4738" citStr="Agichtein and Gravano, 2000" startWordPosition="689" endWordPosition="693">ction Distributional similarities Acquisition of candidate facts Validation of candidate facts Validated candidate facts Candidate facts Occurrences of extraction patterns Scored candidate facts Scoring and ranking Validated extraction patterns Scored extraction patterns Validation of patterns The second contribution of the paper is a method for domain-independent validation and ranking of candidate facts, based on a similarity measure of each candidate fact relative to the set of seed facts. Whereas previous studies assume clean text collections such as news corpora (Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Hasegawa et al., 2004), the validation is essential for low-quality sets of candidate facts collected from noisy Web documents. Without it, the addition of spurious candidate facts to the seed set would result in a quick divergence of the iterative acquisition towards irrelevant information (Agichtein and Gravano, 2000). Furthermore, the finer-grained ranking induced by similarities is necessary in fast-growth iterative acquisition, whereas previously proposed ranking criteria (Thelen and Riloff, 2002; Lita and Carbonell, 2004) are implicitly designed for slow growth of the seed set. (S1) In</context>
<context position="14516" citStr="Agichtein and Gravano, 2000" startWordPosition="2376" endWordPosition="2379"> does not match [NNP NNP], and [JJ ..] (for several) does not match [CD]. 3 Similarities for Validation and Ranking 3.1 Revisiting Standard Ranking Criteria Because some of the acquired extraction patterns are too generic or wrong, all approaches to iterative acquisition place a strong emphasis on the choice of criteria for ranking. Previous literature quasi-unanimously assesses the quality of each candidate fact based on the number and quality of the patterns that extract the candidate fact (more is better); and the number of seed facts extracted by the same patterns (again, more is better) (Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Lita and Carbonell, 2004). However, our experiments using many variations of previously proposed scoring functions suggest that they have limited applicability in large-scale fact extraction, for two main reasons. The first is that it is impractical to perform hundreds of acquisition iterations on terabytes of text. Instead, one needs to grow the seed set aggressively in each iteration. Previous scoring functions were implicitly designed for cautious acquisition strategies (Collins and Singer, 1999), which expand the seed set very slowly across consecutive iterations</context>
<context position="23745" citStr="Agichtein and Gravano, 2000" startWordPosition="3939" endWordPosition="3942">acts are added to the current seed set. Consequently, the acquisition expands the initial seed set of 10 facts to 100,000 facts (after iteration 1) and then to one million facts (after iteration 2) using chunk W1. 4.3 Precision A separate baseline run extracts candidate facts from the text collection following the traditional iterative acquisition approach. Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano, 2000). The theoretical option of running thousands of iterations over the text collection is not viable, since it would imply a non-justifiable expense of our computational resources. As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. The evaluation considers all 80, a sample of the 320, and another sample of the 10,240 facts retained after iterations 3, 5 and 10 respectively. The corr</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: Extracting relations from large plaintext collections. In Proceedings of the 5th ACM International Conference on Digital Libraries (DL-00), pages 85–94, San Antonio, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT - a statistical part of speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00),</booktitle>
<pages>224--231</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="21521" citStr="Brants, 2000" startWordPosition="3580" endWordPosition="3581">the seed set can be filtered out. For instance, the elimination of (Jethro Tull, 1947) is a side effect of verifying that Tull is not similar to any of the last-position words from phrases in the seed set. 4 Evaluation 4.1 Data The source text collection consists of three chunks W1, W2, W3 of approximately 100 million documents each. The documents are part of a larger snapshot of the Web taken in 2003 by the Google search engine. All documents are in English. The textual portion of the documents is cleaned of Html, tokenized, split into sentences and partof-speech tagged using the TnT tagger (Brants, 2000). The evaluation involves facts of type PersonBornIn-Year. The reasons behind the choice of this particular type are threefold. First, many Person-BornIn-Year facts are probably available on the Web (as opposed to, e.g., City-CapitalOfCountry facts), to allow for a good stress test for large-scale extraction. Second, either side of the facts (Person and Year) may be involved in many other types of facts, such that the extraction would easily divergence unless it performs correctly. Third, the phrases from one side (Person) have an utility in their own right, for lexicon 813 Table 1: Set of see</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT - a statistical part of speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing (ANLP-00), pages 224–231, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cafarella</author>
<author>D Downey</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>KnowItNow: Fast, scalable information extraction from the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT-EMNLP-05),</booktitle>
<pages>563--570</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="26440" citStr="Cafarella et al., 2005" startWordPosition="4413" endWordPosition="4416">of erroneous facts extracted from chunk W1. Causes of errors include incorrect approximations of the name boundaries (e.g., Alma in Alma Theresa Rausch is incorrectly tagged as an adjective), and selection of the wrong year as birth year (e.g., for Henry Lumbar). In the case of famous people, the extracted facts tend to capture the correct birth year for several variations of the names, as shown in Table 3. Conversely, it is not necessary that a fact occur with high frequency in order for it to be extracted, which is an advantage over previous approaches that rely strongly on redundancy (cf. (Cafarella et al., 2005)). Table 4 illustrates a few of the correctly extracted facts that occur rarely on the Web. 4.4 Recall In contrast to the assessment of precision, recall can be evaluated automatically, based on external 814 Table 2: Incorrect facts extracted from the Web Spurious Fact Context in Source Sentence (Theresa Rausch, Alma Theresa Rausch was born 1912) on 9 March 1912 (Henry Lumbar, Henry Lumbar was born 1861 1937) and died 1937 (Concepcion Paxety, Maria de la Concepcion Paxety 1817) b. 08 Dec. 1817 St. Aug., FL. (Mae Yaeger, Ella May/Mae Yaeger was born 1872) 20 May 1872 in Mt. (Charles Whatley, Lo</context>
<context position="30720" citStr="Cafarella et al., 2005" startWordPosition="5133" endWordPosition="5136">ed over AllGold are significantly higher as the size of the nGold set increases. In comparison, the recall scores over the growing nGold set increases slightly with larger evaluation sets. The highest value of the recall score for GoldA is 89.9% over the nGold set, and 70.7% over AllGold. The smaller size of the second gold standard set, GoldT, explains the higher variation of the values shown in the lower portion of Table 6. 4.5 Comparison to Previous Results Another recent approach specifically addresses the problem of extracting facts from a similarly-sized collection of Web documents. In (Cafarella et al., 2005), manually-prepared extraction rules are applied to a collection of 60 million Web documents to extract entities of types Company and Country, as well as facts of type Person-CeoOf-Company and City-CapitalOf-Country. Based on manual evaluation of precision and recall, a total of 23,128 company names are extracted at precision of 80%; the number decreases to 1,116 at precision of 90%. In addition, 2,402 Person-CeoOf-Company facts 815 Table 6: Automatic evaluation of recall, over two gold standard sets GoldA (609 person names) and GoldT (17 person names) Gold Set Input Data Recall (%) (Web Chunk</context>
</contexts>
<marker>Cafarella, Downey, Soderland, Etzioni, 2005</marker>
<rawString>M. Cafarella, D. Downey, S. Soderland, and O. Etzioni. 2005. KnowItNow: Fast, scalable information extraction from the web. In Proceedings of the Human Language Technology Conference (HLT-EMNLP-05), pages 563–570, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99),</booktitle>
<pages>189--196</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="1673" citStr="Collins and Singer, 1999" startWordPosition="243" endWordPosition="246">ies on Web search is enormous. They enable the pursuit of new search paradigms, the processing of database-like queries, and alternative methods of presenting search results. The preparation of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition *Work done during internships at Google Inc. framework starts from a small set of seed facts, finds contextual patterns that extract the seed facts from the underlying text collection, identifies a larger set of candidate facts that are extracted by the patterns, and adds the best candidate facts to the previous seed set. 1.2 Contribution</context>
<context position="15047" citStr="Collins and Singer, 1999" startWordPosition="2457" endWordPosition="2460">seed facts extracted by the same patterns (again, more is better) (Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Lita and Carbonell, 2004). However, our experiments using many variations of previously proposed scoring functions suggest that they have limited applicability in large-scale fact extraction, for two main reasons. The first is that it is impractical to perform hundreds of acquisition iterations on terabytes of text. Instead, one needs to grow the seed set aggressively in each iteration. Previous scoring functions were implicitly designed for cautious acquisition strategies (Collins and Singer, 1999), which expand the seed set very slowly across consecutive iterations. In that case, it makes sense to single out a small number of best candidates, among the other available candidates. Comparatively, when 10,000 candidate facts or more need to be added to a seed set of 10 seeds as early as after the first iteration, it is difficult to distinguish the quality of extraction patterns based, for instance, only on the percentage of the seed set that they extract. The second reason is the noisy nature of the Web. A substantial number of factors can and will concur towards the worst-case extraction</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the 1999 Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 189–196, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>E Hovy</author>
<author>A Echihabi</author>
</authors>
<title>Offline strategies for online question answering: Answering questions before they are asked.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>1--7</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1860" citStr="Fleischman et al., 2003" startWordPosition="271" endWordPosition="274">on of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition *Work done during internships at Google Inc. framework starts from a small set of seed facts, finds contextual patterns that extract the seed facts from the underlying text collection, identifies a larger set of candidate facts that are extracted by the patterns, and adds the best candidate facts to the previous seed set. 1.2 Contributions Figure 1 describes an architecture geared towards large-scale fact extraction. The architecture is similar to other instances of bootstrapping for information extraction. The main proce</context>
</contexts>
<marker>Fleischman, Hovy, Echihabi, 2003</marker>
<rawString>M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline strategies for online question answering: Answering questions before they are asked. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 1–7, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="8139" citStr="Grefenstette, 1994" startWordPosition="1311" endWordPosition="1312">r words. The classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words (Lin, 1998) extracted in advance from around 50 million news articles indexed by the Google search engine over three years. All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al., 1993), (Grefenstette, 1994) and (Lin, 1998). Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach, we define the features of a word w to be the set of words that occurred within a small window of w in a large corpus. The context window of an instance of w consists of the closest non-stopword on each side of w and the stopwords in between. The value of a feature w&apos; is defined as the pointwise mutual</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04),</booktitle>
<pages>415--422</pages>
<location>Barcelona,</location>
<contexts>
<context position="4762" citStr="Hasegawa et al., 2004" startWordPosition="694" endWordPosition="697">ties Acquisition of candidate facts Validation of candidate facts Validated candidate facts Candidate facts Occurrences of extraction patterns Scored candidate facts Scoring and ranking Validated extraction patterns Scored extraction patterns Validation of patterns The second contribution of the paper is a method for domain-independent validation and ranking of candidate facts, based on a similarity measure of each candidate fact relative to the set of seed facts. Whereas previous studies assume clean text collections such as news corpora (Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Hasegawa et al., 2004), the validation is essential for low-quality sets of candidate facts collected from noisy Web documents. Without it, the addition of spurious candidate facts to the seed set would result in a quick divergence of the iterative acquisition towards irrelevant information (Agichtein and Gravano, 2000). Furthermore, the finer-grained ranking induced by similarities is necessary in fast-growth iterative acquisition, whereas previously proposed ranking criteria (Thelen and Riloff, 2002; Lita and Carbonell, 2004) are implicitly designed for slow growth of the seed set. (S1) Infix Aurelio de la Vega w</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 415– 422, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-90),</booktitle>
<pages>268--275</pages>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="8093" citStr="Hindle, 1990" startWordPosition="1304" endWordPosition="1306">, June, Aug., November} and other similar words. The classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words (Lin, 1998) extracted in advance from around 50 million news articles indexed by the Google search engine over three years. All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al., 1993), (Grefenstette, 1994) and (Lin, 1998). Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach, we define the features of a word w to be the set of words that occurred within a small window of w in a large corpus. The context window of an instance of w consists of the closest non-stopword on each side of w and the stopwords in between. The value of a</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structures. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-90), pages 268–275, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-98),</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec.</location>
<contexts>
<context position="7699" citStr="Lin, 1998" startWordPosition="1239" endWordPosition="1240">., postfix) of the seed. The pattern shown at the top of Figure 2, which Figure 2: Extraction via infix-only patterns contains the sequence [CL1 born CL2 00 .], illustrates the use of classes of distributionally similar words within extraction patterns. The first word class in the sequence, CL1, consists of words such as {was, is, could}, whereas the second class includes {February, April, June, Aug., November} and other similar words. The classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words (Lin, 1998) extracted in advance from around 50 million news articles indexed by the Google search engine over three years. All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al., 1993), (Grefenstette, 1994) and (Lin, 1998). Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL-98), pages 768–774, Montreal, Quebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lita</author>
<author>J Carbonell</author>
</authors>
<title>Instance-based question answering: A data driven approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-04),</booktitle>
<pages>396--403</pages>
<location>Barcelona,</location>
<contexts>
<context position="1834" citStr="Lita and Carbonell, 2004" startWordPosition="267" endWordPosition="270">rch results. The preparation of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition *Work done during internships at Google Inc. framework starts from a small set of seed facts, finds contextual patterns that extract the seed facts from the underlying text collection, identifies a larger set of candidate facts that are extracted by the patterns, and adds the best candidate facts to the previous seed set. 1.2 Contributions Figure 1 describes an architecture geared towards large-scale fact extraction. The architecture is similar to other instances of bootstrapping for information </context>
<context position="3155" citStr="Lita and Carbonell, 2004" startWordPosition="465" endWordPosition="468">en the seed facts, acquisition of candidate facts given the extraction patterns, scoring and ranking of the patterns, and scoring and ranking of the candidate facts, a subset of which is added to the seed set of the next round. Within the existing iterative acquisition framework, our first contribution is a method for automatically generating generalized contextual extraction patterns, based on dynamically-computed classes of similar words. Traditionally, the acquisition of contextual extraction patterns requires hundreds or thousands of consecutive iterations over the entire text collection (Lita and Carbonell, 2004), often using relatively expensive or restrictive tools such as shallow syntactic parsers (Riloff and Jones, 1999; Thelen and Riloff, 2002) or named entity recognizers (Agichtein and Gravano, 2000). Comparatively, generalized extraction patterns achieve exponentially higher coverage in early iterations. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. 809 Proceedings of the 21st International</context>
<context position="5273" citStr="Lita and Carbonell, 2004" startWordPosition="771" endWordPosition="774">ext collections such as news corpora (Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Hasegawa et al., 2004), the validation is essential for low-quality sets of candidate facts collected from noisy Web documents. Without it, the addition of spurious candidate facts to the seed set would result in a quick divergence of the iterative acquisition towards irrelevant information (Agichtein and Gravano, 2000). Furthermore, the finer-grained ranking induced by similarities is necessary in fast-growth iterative acquisition, whereas previously proposed ranking criteria (Thelen and Riloff, 2002; Lita and Carbonell, 2004) are implicitly designed for slow growth of the seed set. (S1) Infix Aurelio de la Vega was born November 28 , 1925 , in Havana , Cuba . FW FW FW NNP VBD VBN NNP CD , CD , IN NNP , NNP . not found found (S2) Infix The poet was born Jan. 13 , several years after the revolution . DT NN VBD VBN NNP CD , JJ NNS IN DT NN . not found not found (S3) Infix British − native Glenn Cornick of Jethro Tull was born April 23 , 1947 . NNP : JJ NNP NNP IN NNP NNP VBD VBN NNP CD , CD . found found (S4) Infix The youngest child of three siblings , Mariah Carey was born March 27 , DT JJS NN IN CD NNS , NNP NNP V</context>
<context position="14568" citStr="Lita and Carbonell, 2004" startWordPosition="2384" endWordPosition="2387">oes not match [CD]. 3 Similarities for Validation and Ranking 3.1 Revisiting Standard Ranking Criteria Because some of the acquired extraction patterns are too generic or wrong, all approaches to iterative acquisition place a strong emphasis on the choice of criteria for ranking. Previous literature quasi-unanimously assesses the quality of each candidate fact based on the number and quality of the patterns that extract the candidate fact (more is better); and the number of seed facts extracted by the same patterns (again, more is better) (Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Lita and Carbonell, 2004). However, our experiments using many variations of previously proposed scoring functions suggest that they have limited applicability in large-scale fact extraction, for two main reasons. The first is that it is impractical to perform hundreds of acquisition iterations on terabytes of text. Instead, one needs to grow the seed set aggressively in each iteration. Previous scoring functions were implicitly designed for cautious acquisition strategies (Collins and Singer, 1999), which expand the seed set very slowly across consecutive iterations. In that case, it makes sense to single out a small</context>
<context position="23711" citStr="Lita and Carbonell, 2004" startWordPosition="3934" endWordPosition="3937">rd of the validated candidate facts are added to the current seed set. Consequently, the acquisition expands the initial seed set of 10 facts to 100,000 facts (after iteration 1) and then to one million facts (after iteration 2) using chunk W1. 4.3 Precision A separate baseline run extracts candidate facts from the text collection following the traditional iterative acquisition approach. Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano, 2000). The theoretical option of running thousands of iterations over the text collection is not viable, since it would imply a non-justifiable expense of our computational resources. As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. The evaluation considers all 80, a sample of the 320, and another sample of the 10,240 facts retained after iterations </context>
</contexts>
<marker>Lita, Carbonell, 2004</marker>
<rawString>L. Lita and J. Carbonell. 2004. Instance-based question answering: A data driven approach. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-04), pages 396–403, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL-93),</booktitle>
<pages>183--190</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="8117" citStr="Pereira et al., 1993" startWordPosition="1307" endWordPosition="1310">vember} and other similar words. The classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words (Lin, 1998) extracted in advance from around 50 million news articles indexed by the Google search engine over three years. All digits in both patterns and sentences are replaced with a common marker, such 810 that any two numerical values with the same number of digits will overlap during matching. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al., 1993), (Grefenstette, 1994) and (Lin, 1998). Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach, we define the features of a word w to be the set of words that occurred within a small window of w in a large corpus. The context window of an instance of w consists of the closest non-stopword on each side of w and the stopwords in between. The value of a feature w&apos; is defined a</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of english words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics (ACL-93), pages 183–190, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on ArtificialIntelligence (AAAI-99),</booktitle>
<pages>474--479</pages>
<location>Orlando, Florida.</location>
<contexts>
<context position="1527" citStr="Riloff and Jones, 1999" startWordPosition="221" endWordPosition="224">lex tools. 1 Introduction 1.1 Background The potential impact of structured fact repositories containing billions of relations among named entities on Web search is enormous. They enable the pursuit of new search paradigms, the processing of database-like queries, and alternative methods of presenting search results. The preparation of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition *Work done during internships at Google Inc. framework starts from a small set of seed facts, finds contextual patterns that extract the seed facts from the underlying text collection, identifie</context>
<context position="3268" citStr="Riloff and Jones, 1999" startWordPosition="482" endWordPosition="485">, and scoring and ranking of the candidate facts, a subset of which is added to the seed set of the next round. Within the existing iterative acquisition framework, our first contribution is a method for automatically generating generalized contextual extraction patterns, based on dynamically-computed classes of similar words. Traditionally, the acquisition of contextual extraction patterns requires hundreds or thousands of consecutive iterations over the entire text collection (Lita and Carbonell, 2004), often using relatively expensive or restrictive tools such as shallow syntactic parsers (Riloff and Jones, 1999; Thelen and Riloff, 2002) or named entity recognizers (Agichtein and Gravano, 2000). Comparatively, generalized extraction patterns achieve exponentially higher coverage in early iterations. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. 809 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809–816, Sydney, July 2006. c�</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the 16th National Conference on ArtificialIntelligence (AAAI-99), pages 474–479, Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>M Greenwood</author>
</authors>
<title>A semantic approach to IE pattern induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05),</booktitle>
<pages>379--386</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="10852" citStr="Stevenson and Greenwood, 2005" startWordPosition="1758" endWordPosition="1761">e Iglesias 0.27, Michelle Branch 0.27 [..]; • Jethro Tull: Motley Crue 0.28, Black Crowes 0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sabbath 0.26, Doobie Brothers 0.26, Judas Priest 0.26, Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24, Black Flag 0.24, Godsmack 0.24, Grateful Dead 0.24, Grand Funk Railroad 0.24, Smashing Pumpkins 0.24, Led Zeppelin 0.24, Aerosmith 0.24, Limp Bizkit 0.24, Counting Crows 0.24, Echo And The Bunnymen 0.24, Cold Chisel 0.24, Thin Lizzy 0.24 [..]. To our knowledge, the only previous study that embeds similarities into the acquisition of extraction patterns is (Stevenson and Greenwood, 2005). The authors present a method for computing pairwise similarity scores among large sets of potential syntactic (subject-verb-object) patterns, to detect centroids of mutually similar patterns. By assuming the syntactic parsing of the underlying text collection to generate the potential patterns in the first place, the method is impractical on Web-scale collections. Two patterns, e.g. chairman-resign and CEO-quit, are similar to each other if their components are present in an external hand-built ontology (i.e., WordNet), and the similarity among the components is high over the ontology. Since</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>M. Stevenson and M. Greenwood. 2005. A semantic approach to IE pattern induction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05), pages 379–386, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-02),</booktitle>
<pages>214--221</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1620" citStr="Thelen and Riloff, 2002" startWordPosition="235" endWordPosition="238">s containing billions of relations among named entities on Web search is enormous. They enable the pursuit of new search paradigms, the processing of database-like queries, and alternative methods of presenting search results. The preparation of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition *Work done during internships at Google Inc. framework starts from a small set of seed facts, finds contextual patterns that extract the seed facts from the underlying text collection, identifies a larger set of candidate facts that are extracted by the patterns, and adds the best candi</context>
<context position="3294" citStr="Thelen and Riloff, 2002" startWordPosition="486" endWordPosition="489">g of the candidate facts, a subset of which is added to the seed set of the next round. Within the existing iterative acquisition framework, our first contribution is a method for automatically generating generalized contextual extraction patterns, based on dynamically-computed classes of similar words. Traditionally, the acquisition of contextual extraction patterns requires hundreds or thousands of consecutive iterations over the entire text collection (Lita and Carbonell, 2004), often using relatively expensive or restrictive tools such as shallow syntactic parsers (Riloff and Jones, 1999; Thelen and Riloff, 2002) or named entity recognizers (Agichtein and Gravano, 2000). Comparatively, generalized extraction patterns achieve exponentially higher coverage in early iterations. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. 809 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809–816, Sydney, July 2006. c�2006 Association for Compu</context>
<context position="4709" citStr="Thelen and Riloff, 2002" startWordPosition="685" endWordPosition="688">rns Seed facts Text collection Distributional similarities Acquisition of candidate facts Validation of candidate facts Validated candidate facts Candidate facts Occurrences of extraction patterns Scored candidate facts Scoring and ranking Validated extraction patterns Scored extraction patterns Validation of patterns The second contribution of the paper is a method for domain-independent validation and ranking of candidate facts, based on a similarity measure of each candidate fact relative to the set of seed facts. Whereas previous studies assume clean text collections such as news corpora (Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Hasegawa et al., 2004), the validation is essential for low-quality sets of candidate facts collected from noisy Web documents. Without it, the addition of spurious candidate facts to the seed set would result in a quick divergence of the iterative acquisition towards irrelevant information (Agichtein and Gravano, 2000). Furthermore, the finer-grained ranking induced by similarities is necessary in fast-growth iterative acquisition, whereas previously proposed ranking criteria (Thelen and Riloff, 2002; Lita and Carbonell, 2004) are implicitly designed for slow gr</context>
<context position="14541" citStr="Thelen and Riloff, 2002" startWordPosition="2380" endWordPosition="2383">d [JJ ..] (for several) does not match [CD]. 3 Similarities for Validation and Ranking 3.1 Revisiting Standard Ranking Criteria Because some of the acquired extraction patterns are too generic or wrong, all approaches to iterative acquisition place a strong emphasis on the choice of criteria for ranking. Previous literature quasi-unanimously assesses the quality of each candidate fact based on the number and quality of the patterns that extract the candidate fact (more is better); and the number of seed facts extracted by the same patterns (again, more is better) (Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Lita and Carbonell, 2004). However, our experiments using many variations of previously proposed scoring functions suggest that they have limited applicability in large-scale fact extraction, for two main reasons. The first is that it is impractical to perform hundreds of acquisition iterations on terabytes of text. Instead, one needs to grow the seed set aggressively in each iteration. Previous scoring functions were implicitly designed for cautious acquisition strategies (Collins and Singer, 1999), which expand the seed set very slowly across consecutive iterations. In that case, it makes </context>
<context position="23633" citStr="Thelen and Riloff, 2002" startWordPosition="3921" endWordPosition="3924">st 600,000 candidate facts. At the end of an iteration, approximately one third of the validated candidate facts are added to the current seed set. Consequently, the acquisition expands the initial seed set of 10 facts to 100,000 facts (after iteration 1) and then to one million facts (after iteration 2) using chunk W1. 4.3 Precision A separate baseline run extracts candidate facts from the text collection following the traditional iterative acquisition approach. Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano, 2000). The theoretical option of running thousands of iterations over the text collection is not viable, since it would imply a non-justifiable expense of our computational resources. As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. The evaluation considers all 80, a sample</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-02), pages 214–221, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>D M Tice</author>
</authors>
<title>Building a questionanswering test collection.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd International Conference on Research and Development in Information Retrieval (SIGIR-00),</booktitle>
<pages>200--207</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="28071" citStr="Voorhees and Tice, 2000" startWordPosition="4693" endWordPosition="4696">eudonyms and corresponding real names Pseudonym Real Name Year Gloria Estefan Gloria Fajardo 1957 Nicolas Cage Nicolas Kim Coppola 1964 Ozzy Osbourne John Osbourne 1948 Ringo Starr Richard Starkey 1940 Tina Turner Anna Bullock 1939 Tom Cruise Thomas Cruise Mapother IV 1962 Woody Allen Allen Stewart Konigsberg 1935 lists of birth dates of various people. We start by collecting two gold standard sets of facts. The first set is a random set of 609 actors and their birth years from a Web compilation (GoldA). The second set is derived from the set of questions used in the Question Answering track (Voorhees and Tice, 2000) of the Text REtrieval Conference from 1999 through 2002. Each question asking for the birth date of a person (e.g., “What year was Robert Frost born?”) results in a pair containing the person’s name and the birth year specified in the answer keys. Thus, the second gold standard set contains 17 pairs of people and their birth years (GoldT). Table 5 shows examples of facts in each of the gold standard sets. Table 6 shows two types of recall scores computed against the gold standard sets. The recall scores over nGold take into consideration only the set of person names from the gold standard wit</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>E.M. Voorhees and D.M. Tice. 2000. Building a questionanswering test collection. In Proceedings of the 23rd International Conference on Research and Development in Information Retrieval (SIGIR-00), pages 200–207, Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>