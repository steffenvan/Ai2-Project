<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000119">
<title confidence="0.9984825">
Information Retrieval Oriented Word Segmentation based on Character
Associative Strength Ranking
</title>
<author confidence="0.999168">
Yixuan Liu, Bin Wang, Fan Ding, Sheng Xu
</author>
<affiliation confidence="0.98835825">
Information Retrieval Group
Center for Advanced Computing Research
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.786004">
Beijing, 100190, P.R.China
</address>
<email confidence="0.973053">
{liuyixuan, wangbin, dingfan, xusheng}@ict.ac.cn
</email>
<sectionHeader confidence="0.995477" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995425">
This paper presents a novel, ranking-style
word segmentation approach, called RSVM-
Seg, which is well tailored to Chinese informa-
tion retrieval(CIR). This strategy makes seg-
mentation decision based on the ranking of the
internal associative strength between each pair
of adjacent characters of the sentence. On the
training corpus composed of query items, a
ranking model is learned by a widely-used tool
Ranking SVM, with some useful statistical
features, such as mutual information, differ-
ence of t-test, frequency and dictionary infor-
mation. Experimental results show that, this
method is able to eliminate overlapping am-
biguity much more effectively, compared to
the current word segmentation methods. Fur-
thermore, as this strategy naturally generates
segmentation results with different granular-
ity, the performance of CIR systems is im-
proved and achieves the state of the art.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944152173913">
To improve information retrieval systems’ perfor-
mance, it is important to comprehend both queries
and corpus precisely. Unlike English and other
western languages, Chinese does not delimit words
by white-space. Word segmentation is therefore a
key preprocessor for Chinese information retrieval
to comprehend sentences.
Due to the characteristics of Chinese, two main
problems remain unresolved in word segmentation:
segmentation ambiguity and unknown words, which
are also demonstrated to affect the performance of
Chinese information retrieval (Foo and Li, 2004).
Overlapping ambiguity and combinatory ambiguity
are two forms of segmentation ambiguity. The first
one refers to that ABC can be segmented into AB
C or A BC. The second one refers to that string
AB can be a word, or A can be a word and B can
be a word. In CIR, the combinatory ambiguity is
also called segmentation granularity problem (Fan
et al., 2007). There are many researches on the
relationship between word segmentation and Chi-
nese information retrieval (Foo and Li, 2004; Peng
et al., 2002a; Peng et al., 2002b; Jin and Wong,
2002). Their studies show that the segmentation
accuracy does not monotonically influence subse-
quent retrieval performance. Especially the overlap-
ping ambiguity, as shown in experiments of (Wang,
2006), will cause more performance decrement of
CIR. Thus a CIR system with a word segmenter bet-
ter solving the overlapping ambiguity, may achieve
better performance. Besides, it also showed that the
precision of new word identification was more im-
portant than the recall.
There are some researches show that when com-
pound words are split into smaller constituents, bet-
ter retrieval results can be achieved (Peng et al.,
2002a). On the other hand, it is reasonable that the
longer the word which co-exists in query and cor-
pus, the more similarity they may have. A hypothe-
sis, therefore, comes to our mind, that different seg-
mentation granularity can be incorporated to obtain
better CIR performance.
In this paper we present a novel word segmenta-
tion approach for CIR, which can not only obviously
reduce the overlapping ambiguity, but also introduce
different segmentation granularity for the first time.
</bodyText>
<page confidence="0.909704">
1061
</page>
<note confidence="0.879186">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061–1069,
Honolulu, October 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999908238095238">
In our method, we first predict the ranking result of
all internal association strength (IAS) between each
pair of adjacent characters in a sentence using Rank-
ing SVM model, and then, we segment the sentence
into sub-sentences with smaller and smaller granu-
larity by cutting adjacent character pairs according
to this rank. Other machine-learning based segmen-
tation algorithms (Zhang et al., 2003; Lafferty et al.,
2001; Ng and Low, 2004) treat segmentation prob-
lem as a character sequence tagging problem based
on classification. However, these methods cannot di-
rectly obtain different segmentation granularity. Ex-
periments show that our method can actually im-
prove information retrieval performance.
This paper is structured as follows. It starts with
a brief introduction of the related work on the word
segmentation approaches. Then in Section 3, we in-
troduce our segmentation method. Section 4 evalu-
ates the method based on experimental results. Fi-
nally, Section 5 makes summary of this whole paper
and proposes the future research orientation.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999826733333334">
Various methods have been proposed to address
the word segmentation problem in previous studies.
They fall into two main categories, rule-based ap-
proaches that make use of linguistic knowledge and
statistical approaches that train on corpus with ma-
chine learning methods. In rule-based approaches,
algorithms of string matching based on dictionary
are the most commonly used, such as maximum
matching. They firstly segment sentences accord-
ing to a dictionary and then resort to some rules
to resolve ambiguities (Liu, 2002; Luo and Song,
2001). These rule-based methods are fast, how-
ever, their performances depend on the dictionary
which cannot include all words, and also on the rules
which cost a lot of time to make and must be up-
dated frequently. Recent years statistical approaches
became more popular. These methods take advan-
tage of various probability information gained from
large corpus to segment sentences. Among them,
Wang’s work (Wang, 2006) is the most similar to
our method, since both of us apply statistics infor-
mation of each gap in the sentence to eliminate over-
lapping ambiguity in methods. However, when com-
bining different statistics, Wang decided the weight
by a heuristic way which was too simply to be suit-
able for all sentences. In our method, we employ a
machine-learning method to train features’ weights.
Many machine-learning methods, such as
HMM (Zhang et al., 2003), CRF (Lafferty et al.,
2001), Maximum Entropy (Ng and Low, 2004),
have been exploited in segmentation task. To our
knowledge, machine-learning methods used in seg-
mentation treated word segmentation as a character
tagging problem. According to the model trained
from training corpus and features extracted from the
context in the sentence, these methods assign each
character a positional tag, indicating its relative po-
sition in the word. These methods are difficult to get
different granularity segmentation results directly.
Our method has two main differences with them.
Firstly, we tag the gap between characters rather
than characters themselves. Secondly, our method
is based on ranking rather than classification.
Then, we will present our ranking-based segmen-
tation method, RSVM-Seg.
</bodyText>
<sectionHeader confidence="0.891508" genericHeader="method">
3 Ranking based Segmentation
</sectionHeader>
<bodyText confidence="0.9880065">
Traditional segmentation methods always take the
segmentation problem as classification problem and
give a definite segmentation result. In our approach,
we try to solve word segmentation problem from the
view of ranking. For easy understanding, let’s rep-
resent a Chinese sentence S as a character sequence:
</bodyText>
<equation confidence="0.939017">
C1:n = C1C2 ... Cn
</equation>
<bodyText confidence="0.994865">
We also explicitly show the gap Gi(i = 1... n − 1)
between every two adjacent characters Ci and Ci+1:
</bodyText>
<equation confidence="0.987482">
C1:n|G1:n−1 = C1G1C2G2 ... Gn−1Cn
IASi(i = 1... n) is corresponding to Gi(i =
</equation>
<bodyText confidence="0.985481636363636">
1... n), reflecting the internal association strength
between Ci and Ci+1. The higher the IAS value is,
the stronger the associative between the two charac-
ters is. If the association between two characters is
weak, then they can be segmented. Otherwise, they
should be unsegmented. That is to say we could
make segmentation based on the ranking of IAS
value. In our ranking-style segmentation method,
Ranking SVM is exploited to predict IAS ranking.
In next subsections, we will introduce how to
take advantage of Ranking SVM model to solve our
</bodyText>
<page confidence="0.986989">
1062
</page>
<bodyText confidence="0.9998735">
problem. Then, we will describe features used for
training the Ranking SVM model. Finally, we will
give a scheme how to get segmentation result from
predicted ranking result of Ranking SVM.
</bodyText>
<subsectionHeader confidence="0.999362">
3.1 Segmentation based on Ranking SVM
</subsectionHeader>
<bodyText confidence="0.999922363636364">
Ranking SVM is a classical algorithm for ranking,
which formalizes learning to rank as learning for
classification on pairs of instances and tackles the
classification issue by using SVM (Joachims, 2002).
Suppose that XERd is the feature space, where d is
the number of features, and Y = r1, r2, ... , rK is
the set of labels representing ranks. And there exists
a total order between ranks r1 &gt; r2 &gt; ... &gt; rK,
where &gt; denotes the order relationship. The actual
task of learning is formalized as a Quadratic Pro-
gramming problem as shown below:
</bodyText>
<equation confidence="0.996141666666667">
2IlwIl2 + Cr�13
1
S.t.(w, x1 − x3) &gt; 1 − X13, bx1 &gt;- x3, X13 &gt; 0
</equation>
<bodyText confidence="0.999888511627907">
where IlwIl denotes 12 norm measuring the margin
of the hyperplane and �ij denotes a slack variable.
xi &gt;- xj means the rank class of xi has an order
prior to that of xj, i.e. Y (xi) &gt; Y (xj). Suppose
that the solution to (1) is w*, then we can make the
ranking function as f(x) = (w*, x).
When applying Ranking SVM model to our prob-
lems, an instance (feature vector x) is created from
all bigrams (namely CiCi+1, i = 1... n − 1) of
a sentence in the training corpus. Each feature
is defined as a function of bigrams (we will de-
scribe features in detail in next subsection). The
instances from all sentences are then combined for
training. And Y refers to the class label of the
IAS degree. As we mentioned above, segmenta-
tion decision is based on IAS value. Therefore,
the number of IAS degree’s class label is also cor-
respondent to the number of segmentation class la-
bel. In traditional segmentation algorithms, they al-
ways label segmentation as two classes, segmented
and unsegmented. However, for some phrases, it is
a dilemma to make a segmentation decision based
on this two-class scheme. For example, Chinese
phrase ”笔记本电脑(Notepad)” can be segmented
as ”笔记本(Note)” and ”电脑(computer)” or can
be viewed as one word. We cannot easily classify
the gap between ”本” and ”脑” as segmented or un-
segmented. Therefore, beside these two class la-
bels, we define another class label, semisegmented,
which means that the gap between two characters
could be segmented or unsegmented, either will be
right. Correspondingly, IAS degree is also divided
into three classes, definitely inseparable (marked as
3), partially inseparable (marked as 2), and sepa-
rable (marked as 1). ”Separable” corresponds to
be segmented”; ”partially inseparable” corresponds
to semisegmented; ”definitely inseparable” corre-
sponds to be unsegmented. Obviously, there exists
orders between these labels’ IAS values, namely
IAS(1) &lt; IAS(2) &lt; IAS(3), IAS(*) represents
the IAS value of different labels. Next, we will
describe the features used to train Ranking SVM
model.
</bodyText>
<subsectionHeader confidence="0.874553">
3.2 Features for IAS computation
</subsectionHeader>
<bodyText confidence="0.999790571428571">
Mutual Information: Mutual information, mea-
suring the relationship between two variables, has
been extensively used in computational language re-
search. Given a Chinese character string ’xy’ (as
mentioned above, in our method, ’xy’ refers to bi-
gram in a sentence), mutual information between
characters x and y is defined as follows:
</bodyText>
<equation confidence="0.9131105">
mi(x, y) = 1o92 p(x, y) (2)
p(x)p(y)
</equation>
<bodyText confidence="0.999795235294118">
where p(x, y) is the co-occurrence probability of x
and y, namely the probability that bigram ’xy’ oc-
curs in the training corpus, and p(x), p(y) are the
independent probabilities of x and y respectively.
From (2), we conclude that mi(x, y) » 0 means
that IAS is strong; mi(x, y) Pz� 0 means that it
is indefinite for IAS between characters x and y;
mi(x, y) « 0 means that there is no association
been characters x and y. However, mutual infor-
mation has no consideration of context, so it can-
not solve the overlapping ambiguity effectively (Sili
Wang 2006). To remedy this defect, we introduce
another statistics measure, difference of t-test.
Difference of t-score (DTS): Difference of t-
score is proposed on the basis of t-score. Given
a Chinese character string ’xyz’, the t-score of the
character y relevant to character x and z is defined
</bodyText>
<equation confidence="0.970864142857143">
min,,��.,
(1)
1063
as:
p(z|y) − p(y|x)
tx,z(y) = (3)
VIσ2(p(z|y)) + σ2(p(y|x))
</equation>
<bodyText confidence="0.96115425">
where p(y|x) is the conditional probability of y
given x, and p(z|y), of z given y, and σ2(p(y|x)),
σ2(p(z|y)) are variances of p(y|x) and of p(z|y) re-
spectively. Sun et al. gave the derivation formula of
</bodyText>
<equation confidence="0.887072333333333">
σ2(p(y|x)),σ2(p(z|y)) (Sun et al., 1997) as
r(y, σ2(p(z|y)) ≈ r2 (y))
σ2(p(y|x)) ≈ r2(x )) (4)
</equation>
<bodyText confidence="0.992545090909091">
where r(x, y), r(y, z), r(y), r(z) are the frequency
of string xy, yz, y, and z respectively. Thus formula
(3) is deducted as
tx,z(y) indicates the binding tendency of y in the
context of x and z: if tx,z(y) &gt; 0 then y tends to
be bound with z rather than with x; if tx,z(y) &lt; 0,
they y tends to be bound with x rather than with z.
To measure the binding tendency between two ad-
jacent characters ’xy’ (also, it refers to bigram in a
sentence in our method), we use difference of t-score
(DTS) (Sun et al., 1998) which is defined as
</bodyText>
<equation confidence="0.994988">
dts(x, y) = tv,y(x) − tx,w(y) (6)
</equation>
<bodyText confidence="0.998812">
Higher dts(x, y) indicates stronger IAS between
adjacent characters x and y.
Dictionary Information: Both statistics mea-
sures mentioned above cannot avoid sparse data
problem. Then Dictionary Information is used to
compensate for the shortage of statistics informa-
tion. The dictionary we used includes 75784 terms.
We use binary value to denote the dictionary feature.
If a bigram is in the dictionary or a part of dictionary
term, we label it as ”1”, otherwise, we label is as ”0”.
Frequency: An important characteristic of new
word is its repeatability. Thus, we also use fre-
quency as another feature to train Ranking SVM
model. Here, the frequency is referred to the number
of times that a bigram occurs in the training corpus.
We give a training sentence for a better under-
standing of features mentioned above. The sentence
</bodyText>
<listItem confidence="0.924486769230769">
Algorithm 1 : Generate various granularity terms
1: Input: A Chinese sentence S = C1 : Cn
IAS = IAS1:n_1 LB = 1; RB = n
2: Iterative(S, IAS):
3: while length(S) ≥ 3 do
4: MB = FindMinIAS(IAS)
5: SL = CLB:MB
6: SR = CMB+1:RB
7: IASL = IASLB:MB
8: IASR = IASMB+1:RB
9: Iterative(SL, IASL)
10: Iterative(SR, IASR)
11: end while
</listItem>
<bodyText confidence="0.929378">
is ”+Q,Vfi�tWITM(China Construction Bank net-
work)” We extract all bigrams in this sentence, com-
pute the four above features and give the IAS a la-
bel for each bigram. The feature vectors of all these
bigrams for training are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999588">
3.3 Segmentation scheme
</subsectionHeader>
<bodyText confidence="0.999908653846154">
In order to compare with other segmentation meth-
ods, which give a segmentation result based on two
class labels, segmented and unsegmented, it is nec-
essary to convert real numbers result given by Rank-
ing SVM to these two labels. Here, we make a
heuristic scheme to segment the sentence based on
IAS ranking result predicted by Ranking SVM. The
scheme is described in Algorithm 1. In each itera-
tion we cut the sentence at the gap with minimum
IAS value. Nie et.al. pointed out that the average
length of words in usage is 1.59 (Nie et al., 2000).
Therefore, we stop the segmentation iterative when
the length of sub sentence is 2 or less than 2. By
this method, we could represent the segmentation re-
sult as a binary tree. Figure 1 shows an example of
this tree. With this tree, we can obtain various gran-
ularity segmentations easily, which could be used
in CIR. This segmentation scheme may cause some
combinatory ambiguity. However, Nie et.al. (Nie
et al., 2000) also pointed out that there is no accu-
rate word definition, thus whether combinatory am-
biguity occurs is uncertain. What’s more, compared
to overlapping ambiguity, combinatory ambiguity is
not the fatal factor for information retrieval perfor-
mance as mentioned in introduction. Therefore, this
scheme is reasonable for Chinese information re-
</bodyText>
<equation confidence="0.995314142857143">
r(y,z) r(x,y)
−
r(y) r(x)
r(y,z) r(x,y)
r2(y) + r2(x)
tx,z(y) =
(5)
</equation>
<page confidence="0.838601">
1064
</page>
<table confidence="0.999827">
Bigram MI DTS Dictionary Frequency IAS
中国(China) 6.67 1985.26 1 1064561 3
国建 2.59 -1447.6 0 14325 1
建设(Construction) 8.67 822.64 1 200129 3
设银 5.94 -844.05 0 16098 2
银行(Bank) 9.22 931.25 1 236976 3
行网 2.29 -471.24 0 15282 1
</table>
<tableCaption confidence="0.999902">
Table 1: Example of feature vector
</tableCaption>
<figure confidence="0.9826925">
∳㽓ⳕѸ䗮ഄ೒
(Traffic map of JiangXi Province)
∳㽓ⳕ Ѹ䗮ഄ೒
(JiangXi Province) (Traffic map)
∳㽓 ⳕ Ѹ䗮 ഄ೒
(JiangXi) (Province) (Traffic) (Map)
</figure>
<figureCaption confidence="0.999669">
Figure 1: Example 1
</figureCaption>
<bodyText confidence="0.846309">
trieval.
</bodyText>
<sectionHeader confidence="0.988893" genericHeader="evaluation">
4 Experiments and analysis
</sectionHeader>
<subsectionHeader confidence="0.933139">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999985421052632">
Since the label scheme and evaluation measure (de-
scribed in next subsection) of our segmentation
method are both different from the traditional seg-
mentation methods, we did not carry out experi-
ments on SIGHAN. Instead, we used two query logs
(QueryLog1 and QueryLog2) as our experiment cor-
pus, which are from two Chinese search engine com-
panies. 900 queries randomly from QueryLog1 were
chosen as training corpus. 110 Chinese queries from
PKU Tianwang1 , randomly selected 150 queries
from QueryLog1 and 100 queries from QueryLog2
were used as test corpus. The train and test cor-
pus have been tagged by three people. They were
given written information need statements, and were
asked to judge the IAS of every two adjacent char-
acters in a sentence on a three level scale as men-
tioned above, separable, partially inseparable, and
definitely inseparable. The assessors agreed in 84%
of the sentences, the other sentences were checked
</bodyText>
<footnote confidence="0.859316">
1Title field of SEWM2006 and SEWM2007 web retrieval
TD task topics. See http://www.cwirf.org/
</footnote>
<bodyText confidence="0.935535666666667">
by all assessors, and a more plausible alternative was
selected. We exploited SV MUght2 as the toolkit to
implement Ranking SVM model.
</bodyText>
<subsectionHeader confidence="0.964459">
4.2 Evaluation Measure
</subsectionHeader>
<bodyText confidence="0.999819444444444">
Since our approach is based on the ranking of IAS
values, it is inappropriate to evaluate our method by
the traditional method used in other segmentation
algorithms. Here, we proposed an evaluation mea-
sure RankPrecision based on Kendall’s τ (Joachims,
2002), which compared the similarity between the
predicted ranking of IAS values and the rankings
of these tags as descending order. RankPrecision
formula is as follows:
</bodyText>
<equation confidence="0.999593666666667">
RankPrecision =
1 − EZ1CompInverseCount(si)
E�i�1InverseCount(si) (7)
</equation>
<bodyText confidence="0.999926142857143">
where si represents the ith sentence (unsegmented
string), InverseCount(si) represents the number
of discordant pairs inversions in the ranking of the
predicted IAS value compared to the correct labeled
ranking. CompInverseCount(si) represents the
number of discordant pairs inversions when the la-
bels totally inverse.
</bodyText>
<subsectionHeader confidence="0.997076">
4.3 Experiments Results
</subsectionHeader>
<bodyText confidence="0.999978125">
Contributions of the Features: We investi-
gated the contribution of each feature by gen-
erating many versions of Ranking SVM model.
RankPrecision as described above was used for
evaluations in these and following experiments.
We used Mutual Information(MI); Difference
of T-Score(DTS); Frequency(F); mutual informa-
tion and difference of t-score(MI+DTS); mu-
</bodyText>
<footnote confidence="0.970336">
2http://svmlight.joachims.org/
</footnote>
<page confidence="0.908097">
1065
</page>
<table confidence="0.999564583333333">
Feature Corpus Size of Corpus
Train
Corpus
Train Query Query Tian Train Query Query Tian
Log1 Log2 Wang Log1 Log2 Wang
MI 0.882 0.8719 0.8891 0.9444 100 0.9149 0.9070 0.9209 0.9630
DTS 0.9054 0.8954 0.9086 0.9444 200 0.9325 0.9304 0.9446 0.9907
F 0.8499 0.8416 0.8563 0.9583 400 0.9169 0.9057 0.9230 0.9630
MI+DTS 0.9077 0.9117 0.923 0.9769 500 0.9320 0.9300 0.9374 0.9954
MI+DTS+F 0.8896 0.8857 0.9209 0.9815 600 0.9106 0.9050 0.9312 0.9907
MI+DTS+D 0.933 0.916 0.9384 0.9954 700 0.9330 0.9284 0.9353 0.9954
MI+DTS+F+D 0.932 0.93 0.9374 0.9954 900 0.9217 0.9104 0.9240 0.9907
</table>
<tableCaption confidence="0.982473">
Table 2: The segmentation performance with different
features
Table 3: The segmentation performance with different
size training corpus
</tableCaption>
<figure confidence="0.983049566666667">
MI DTS F MI+DTS MI+DTS+F MI+DTS+D MI+DTS+F+D
Features
0 200 400 600 800 1000
Number of Train Query
TrainCorpus
QueryLog1
QueryLog2
TianWang
1.02
1.00
.98
.96
.94
.92
.90
.88
.86
.84
.82
1.10
1.05
1.00
.95
.90
.85
.80
TrainCorpus
QueryLog1
QueryLog2
TianWang
</figure>
<figureCaption confidence="0.999983">
Figure 2: Effects of features Figure 3: Effects of Corpus Size
</figureCaption>
<bodyText confidence="0.880715666666667">
tual information, difference of t-score and Fre-
quency(MI+DTS+F); mutual information, differ-
ence of t-score and dictionary(MI+DTS+D); mutual
information, difference of t-score, frequency and
Dictionary(MI+DTS+F+D) as features respectively.
The results are shown in Table 2 and Figure 2.
From the results, we can see that:
• Using all described features together, the Rank-
ing SVM achieved a good performance. And
when we added MI, DT 5, frequency, dictio-
nary as features one by one, the RankPrecision
improved step by step. It demonstrates that the
features we selected are useful for segmenta-
tion.
• The lowest RankPrecision is above 85%, which
suggests that the predicted rank result by our
approach is very close to the right rank. It is
shown that our method is effective.
</bodyText>
<listItem confidence="0.9885715">
• When we used each feature alone, difference
of t-score achieved highest RankPrecise, fre-
quency was worst on most of test corpus (ex-
cept TianWang). It is induced that difference
of t-test is the most effective feature for seg-
mentation. It is explained that because dts is
combined with the context information, which
eliminates overlapping ambiguity errors.
• It is surprising that when mutual information
and difference of t-score was combined with
</listItem>
<page confidence="0.914613">
1066
</page>
<bodyText confidence="0.985803214285714">
io c s re
re lc s io
frequency, the RankPrecision was hurt on three
test corpus, even worse than dts feature. The
reason is supposed that some non-meaning but
common strings, such as ”的人” would be took
for a word with high IAS values. To correct
this error, we could build a stop word list, and
when we meet a character in this list, we treat
them as a white-space.
Effects of corpus size:We trained different Rank-
ing SVM models with different corpus size to in-
vestigate the effects of training corpus size to our
method performance. The results are shown in Ta-
ble 3 and Figure 3. From the results, we can see that
the effect of corpus size to the performance of our
approach is minors. Our segmentation approach can
achieve good performance even with small training
corpus, which indicates that Ranking SVM has gen-
eralization ability. Therefore we can use a relative
small corpus to train Ranking SVM, saving labeling
effort.
Effects on Finding Boundary: In algorithm
1, we could get different granularity segmentation
words when we chose different length as stop
condition. Figure 4 shows the ”boundary precision”
at each stop condition. Here, ”boundary precision”
is defined as
</bodyText>
<subsectionHeader confidence="0.255665">
No.of right cut boundaries (8)
</subsectionHeader>
<bodyText confidence="0.963822789473684">
No.of all cut boundaries
From the result shown in figure 4, we can see
that as the segmentation granularity gets smaller, the
boundary precision gets lower. The reason is obvi-
ous, that we may segment a whole word into smaller
parts. However, as we analyzed in introduction, in
CIR, we should judge words boundaries correctly to
avoid overlapping ambiguity. As for combinatory
ambiguity, through setting different stop length con-
dition, we can obtain different granularity segmen-
tation result.
Effects on Overlapping Ambiguity: Due to the
inconsistency of train and test corpus, it is difficult to
keep fair for Chinese word segmentation evaluation.
Since ICTCLAS is considered as the best Chinese
word segmentation systems. We chose ICTCLAS
as the comparison object. Moreover, we chose
Maximum Match segmentation algorithm, which is
rule-based segmentation method, as the baseline.
</bodyText>
<figure confidence="0.963404090909091">
.96
.95
.94
.93
.92
.91
.90
.89
.88
2~3 4~5 6~7
Stop length
</figure>
<figureCaption confidence="0.931865">
Figure 4: Precision of boundary with different stop word
length conditions
</figureCaption>
<table confidence="0.99928075">
Corpus NOA NOA NOA
(RSVM Seg) (ICTCLAS) (MM)
Query 7 10 21
Log1
Query 2 6 16
Log2
Tian 0 0 1
Wang
</table>
<tableCaption confidence="0.999683">
Table 4: Number of Overlapping Ambiguity
</tableCaption>
<bodyText confidence="0.972548526315789">
We compared the number of overlapping ambigu-
ity(NOA) among these three approaches on test cor-
pus QueryLog1, QueryLog2 and TianWang. The re-
sult is shown in Table 4. On these three test cor-
pus, the NOA of our approach is smallest, which
indicates our method resolve overlapping ambiguity
more effectively. For example, the sentence ”基础
课件(basic notes)”, the segmentation result of ICT-
CLAS is ”基础课(basic class)/件(article)”, the word
”课件(notes)” is segmented, overlapping ambiguity
occurring. However, with our method, the predicted
IAS value rank of positions between every two ad-
jacent characters in this sentence is ”基3础1课2件”,
which indicates that the character ”课” has stronger
internal associative strength with the character ”件”
than with the character ”础”, eliminating overlap-
ping ambiguity according to this ISA rank results.
Effects on Recognition Boundaries of new
word: According to the rank result of all IAS values
</bodyText>
<page confidence="0.966752">
1067
</page>
<figure confidence="0.994267">
⍋फЁ᢯ᔩপ
(Hainan High School’s Entry Recruitme)
⍋फ Ё᢯ᔩপ
(Hainan) (High School’s Entry Recruitment)
Ё᢯ ᔩপ
(High School’s Entry)(Recruitment)
</figure>
<figureCaption confidence="0.99999">
Figure 5: Example of New Word boundary
</figureCaption>
<bodyText confidence="0.9989225">
in a sentence, our method can recognize the bound-
aries of new words precisely, avoiding the overlap-
ping ambiguity caused by new words. For example,
the phrase ”海南中招录取(Hainan High School’s
Entry Recruitment)”, the ICTCLAS segmentation
result is ”海南/中/招录/取”, because the new word
”中招” cannot be recognized accurately, thus the
character ”招” is combined with its latter charac-
ter ”录”, causing overlapping ambiguity. By our
method, the segmentation result is shown as figure
5, in which no overlapping ambiguity occurs.
Performance of Chinese Information Re-
trieval: To evaluate the effectiveness of RSVM-Seg
method on CIR, we compared it with the FMM seg-
mentation. Our retrieval system combines differ-
ent query representations obtained by our segmen-
tation method, RSVM-Seg. In previous TREC Tere-
byte Track, Markov Random Field(MRF) (Metzler
and Croft, 2005) model has displayed better perfor-
mance than other information retrieval models, and
it can much more easily include dependence fea-
tures. There are three variants of MRF model, full
independence(FI), sequential dependence(SD), and
full dependence(FD). We chose SD as our retrieval
model, since Chinese words are composed by char-
acters and the adjacent characters have strong de-
pendence relationship. We evaluated the CIR per-
formance on the Chinese Web Corpora CWT200g
provided by Tianwang 3, which, as we know, is
the largest publicly available Chinese web corpus
till now. It consists of 37, 482, 913 web pages
with total size of 197GB. We used the topic set
</bodyText>
<footnote confidence="0.938771">
3http://www.cwirf.org/
</footnote>
<table confidence="0.99948">
Segmentation
Method MAP R-P GMAP
FMM 0.0548 0.0656 0.0095
RSVM-Seg 0.0623 0.0681 0.0196
</table>
<tableCaption confidence="0.999888">
Table 5: Evaluation of CIR performance
</tableCaption>
<bodyText confidence="0.999854777777778">
for SEWM2007 and SEWM2006 Topic Distillation
(TD) task which contains 121 topics. MAP, R-
Precision and GMAP (Robertson, 2006) were as
main evaluation metrics. GMAP is the geometric
mean of AP(Average Precision) through different
queries, which was introduced to concentrate on dif-
ficult queries. The result is shown in 5. From the
table, we can see that our segmentation method im-
prove the CIR performance compared to FMM.
</bodyText>
<sectionHeader confidence="0.993987" genericHeader="conclusions">
5 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.99997745">
From what we have discussed above, we can safely
draw the conclusion that our work includes several
main contributions. Firstly, to our best known, this
is the first time to take the Chinese word segmenta-
tion problem as ranking problem, which provides a
new view for Chinese word segmentation. This ap-
proach has been proved to be able to eliminate over-
lapping ambiguity and also be able to obtain various
segmentation granularities. Furthermore, our seg-
mentation method can improve Chinese information
retrieval performance to some extent.
As future work, we would search another more
encouraging method to make a segmentation deci-
sion from the ranking result. Moreover, we will try
to relabel SIGHAN corpus on our three labels, and
do experiments on them, which will be more con-
venient to compare with other segmentation meth-
ods. Besides, we will carry out more experiments to
search the effectiveness of our segmentation method
to CIR.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999637428571429">
This paper is supported by China Natural Science
Founding under No. 60603094 and China National
863 key project under No. 2006AA010105. We ap-
preciate Wenbin Jiang’s precious modification ad-
vices. Finally, we would like to thank the three
anonymous EMNLP reviewers for their helpful and
constructive comments.
</bodyText>
<page confidence="0.986613">
1068
</page>
<sectionHeader confidence="0.995849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999842262295082">
D. Fan, W. Bin, and W. Sili. 2007. A Heuristic Approach
for Segmentation Granularity Problem in Chinese In-
formation Retrieval. Advanced Language Processing
and Web Information Technology, 2007. ALPIT 2007.
Sixth International Conference on, pages 87–91.
S. Foo and H. Li. 2004. Chinese word segmentation and
its effect on information retrieval. volume 40, pages
161–190. Elsevier.
H. Jin and K.F. Wong. 2002. A Chinese dictionary
construction algorithm for information retrieval. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 1(4):281–296.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 133–142.
J.D. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. Proceed-
ings of the Eighteenth International Conference on
Machine Learning table of contents, pages 282–289.
Q. Liu. 2002. Review of Chinese lexical and syntactic
technology.
Z.Y. Luo and R. Song. 2001. Proper noun recognition in
Chinese word segmentation research. Conference of
international Chinese computer, 328:2001–323.
D. Metzler and W.B. Croft. 2005. A Markov random
field model for term dependencies. Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 472–479.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: one-at-a-time or all-at-once? word-based or
character-based. Proc of EMNLP.
J.Y. Nie, J. Gao, J. Zhang, and M. Zhou. 2000. On
the use of words and n-grams for Chinese informa-
tion retrieval. Proceedings of the fifth international
workshop on on Information retrieval with Asian lan-
guages, pages 141–148.
F. Peng, X. Huang, D. Schuurmans, and N. Cercone.
2002a. Investigating the relationship between word
segmentation performance and retrieval performance
in Chinese IR. Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1–7.
F. Peng, X. Huang, D. Schuurmans, N. Cercone, and S.E.
Robertson. 2002b. Using self-supervised word seg-
mentation in Chinese information retrieval. Proceed-
ings of the 25th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 349–350.
S. Robertson. 2006. On GMAP: and other transforma-
tions. Proceedings of the 15th ACM international con-
ference on Information and knowledge management,
pages 78–83.
Sili Wang. 2006. Research on chinese word segmenta-
tion for large scale information retrieval.
H.P. Zhang, H.K. Yu, D.Y. Xiong, and Q. Liu. 2003.
HHMM-based Chinese Lexical Analyzer ICTCLAS.
Proceedings of Second SIGHAN Workshop on Chinese
Language Processing, pages 184–187.
</reference>
<page confidence="0.985317">
1069
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382121">
<title confidence="0.9965565">Information Retrieval Oriented Word Segmentation based on Associative Strength Ranking</title>
<author confidence="0.997212">Yixuan Liu</author>
<author confidence="0.997212">Bin Wang</author>
<author confidence="0.997212">Fan Ding</author>
<author confidence="0.997212">Sheng</author>
<affiliation confidence="0.996614">Information Retrieval Center for Advanced Computing Institute of Computing</affiliation>
<address confidence="0.7508045">Chinese Academy of Beijing, 100190,</address>
<email confidence="0.966405">wangbin,dingfan,</email>
<abstract confidence="0.989621095238095">This paper presents a novel, ranking-style segmentation approach, called RSVMwhich is well tailored to Chinese information retrieval(CIR). This strategy makes segmentation decision based on the ranking of the internal associative strength between each pair of adjacent characters of the sentence. On the training corpus composed of query items, a ranking model is learned by a widely-used tool Ranking SVM, with some useful statistical features, such as mutual information, difference of t-test, frequency and dictionary information. Experimental results show that, this method is able to eliminate overlapping ambiguity much more effectively, compared to the current word segmentation methods. Furthermore, as this strategy naturally generates segmentation results with different granularity, the performance of CIR systems is improved and achieves the state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Fan</author>
<author>W Bin</author>
<author>W Sili</author>
</authors>
<title>A Heuristic Approach for Segmentation Granularity Problem</title>
<date>2007</date>
<booktitle>in Chinese Information Retrieval. Advanced Language Processing and Web Information Technology,</booktitle>
<pages>87--91</pages>
<contexts>
<context position="2168" citStr="Fan et al., 2007" startWordPosition="319" endWordPosition="322">rehend sentences. Due to the characteristics of Chinese, two main problems remain unresolved in word segmentation: segmentation ambiguity and unknown words, which are also demonstrated to affect the performance of Chinese information retrieval (Foo and Li, 2004). Overlapping ambiguity and combinatory ambiguity are two forms of segmentation ambiguity. The first one refers to that ABC can be segmented into AB C or A BC. The second one refers to that string AB can be a word, or A can be a word and B can be a word. In CIR, the combinatory ambiguity is also called segmentation granularity problem (Fan et al., 2007). There are many researches on the relationship between word segmentation and Chinese information retrieval (Foo and Li, 2004; Peng et al., 2002a; Peng et al., 2002b; Jin and Wong, 2002). Their studies show that the segmentation accuracy does not monotonically influence subsequent retrieval performance. Especially the overlapping ambiguity, as shown in experiments of (Wang, 2006), will cause more performance decrement of CIR. Thus a CIR system with a word segmenter better solving the overlapping ambiguity, may achieve better performance. Besides, it also showed that the precision of new word i</context>
</contexts>
<marker>Fan, Bin, Sili, 2007</marker>
<rawString>D. Fan, W. Bin, and W. Sili. 2007. A Heuristic Approach for Segmentation Granularity Problem in Chinese Information Retrieval. Advanced Language Processing and Web Information Technology, 2007. ALPIT 2007. Sixth International Conference on, pages 87–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Foo</author>
<author>H Li</author>
</authors>
<title>Chinese word segmentation and its effect on information retrieval.</title>
<date>2004</date>
<volume>40</volume>
<pages>161--190</pages>
<publisher>Elsevier.</publisher>
<contexts>
<context position="1813" citStr="Foo and Li, 2004" startWordPosition="253" endWordPosition="256">ed and achieves the state of the art. 1 Introduction To improve information retrieval systems’ performance, it is important to comprehend both queries and corpus precisely. Unlike English and other western languages, Chinese does not delimit words by white-space. Word segmentation is therefore a key preprocessor for Chinese information retrieval to comprehend sentences. Due to the characteristics of Chinese, two main problems remain unresolved in word segmentation: segmentation ambiguity and unknown words, which are also demonstrated to affect the performance of Chinese information retrieval (Foo and Li, 2004). Overlapping ambiguity and combinatory ambiguity are two forms of segmentation ambiguity. The first one refers to that ABC can be segmented into AB C or A BC. The second one refers to that string AB can be a word, or A can be a word and B can be a word. In CIR, the combinatory ambiguity is also called segmentation granularity problem (Fan et al., 2007). There are many researches on the relationship between word segmentation and Chinese information retrieval (Foo and Li, 2004; Peng et al., 2002a; Peng et al., 2002b; Jin and Wong, 2002). Their studies show that the segmentation accuracy does no</context>
</contexts>
<marker>Foo, Li, 2004</marker>
<rawString>S. Foo and H. Li. 2004. Chinese word segmentation and its effect on information retrieval. volume 40, pages 161–190. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jin</author>
<author>K F Wong</author>
</authors>
<title>A Chinese dictionary construction algorithm for information retrieval.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="2354" citStr="Jin and Wong, 2002" startWordPosition="350" endWordPosition="353"> to affect the performance of Chinese information retrieval (Foo and Li, 2004). Overlapping ambiguity and combinatory ambiguity are two forms of segmentation ambiguity. The first one refers to that ABC can be segmented into AB C or A BC. The second one refers to that string AB can be a word, or A can be a word and B can be a word. In CIR, the combinatory ambiguity is also called segmentation granularity problem (Fan et al., 2007). There are many researches on the relationship between word segmentation and Chinese information retrieval (Foo and Li, 2004; Peng et al., 2002a; Peng et al., 2002b; Jin and Wong, 2002). Their studies show that the segmentation accuracy does not monotonically influence subsequent retrieval performance. Especially the overlapping ambiguity, as shown in experiments of (Wang, 2006), will cause more performance decrement of CIR. Thus a CIR system with a word segmenter better solving the overlapping ambiguity, may achieve better performance. Besides, it also showed that the precision of new word identification was more important than the recall. There are some researches show that when compound words are split into smaller constituents, better retrieval results can be achieved (P</context>
</contexts>
<marker>Jin, Wong, 2002</marker>
<rawString>H. Jin and K.F. Wong. 2002. A Chinese dictionary construction algorithm for information retrieval. ACM Transactions on Asian Language Information Processing (TALIP), 1(4):281–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="8408" citStr="Joachims, 2002" startWordPosition="1305" endWordPosition="1306">ranking-style segmentation method, Ranking SVM is exploited to predict IAS ranking. In next subsections, we will introduce how to take advantage of Ranking SVM model to solve our 1062 problem. Then, we will describe features used for training the Ranking SVM model. Finally, we will give a scheme how to get segmentation result from predicted ranking result of Ranking SVM. 3.1 Segmentation based on Ranking SVM Ranking SVM is a classical algorithm for ranking, which formalizes learning to rank as learning for classification on pairs of instances and tackles the classification issue by using SVM (Joachims, 2002). Suppose that XERd is the feature space, where d is the number of features, and Y = r1, r2, ... , rK is the set of labels representing ranks. And there exists a total order between ranks r1 &gt; r2 &gt; ... &gt; rK, where &gt; denotes the order relationship. The actual task of learning is formalized as a Quadratic Programming problem as shown below: 2IlwIl2 + Cr�13 1 S.t.(w, x1 − x3) &gt; 1 − X13, bx1 &gt;- x3, X13 &gt; 0 where IlwIl denotes 12 norm measuring the margin of the hyperplane and �ij denotes a slack variable. xi &gt;- xj means the rank class of xi has an order prior to that of xj, i.e. Y (xi) &gt; Y (xj). S</context>
<context position="17737" citStr="Joachims, 2002" startWordPosition="2912" endWordPosition="2913">nd definitely inseparable. The assessors agreed in 84% of the sentences, the other sentences were checked 1Title field of SEWM2006 and SEWM2007 web retrieval TD task topics. See http://www.cwirf.org/ by all assessors, and a more plausible alternative was selected. We exploited SV MUght2 as the toolkit to implement Ranking SVM model. 4.2 Evaluation Measure Since our approach is based on the ranking of IAS values, it is inappropriate to evaluate our method by the traditional method used in other segmentation algorithms. Here, we proposed an evaluation measure RankPrecision based on Kendall’s τ (Joachims, 2002), which compared the similarity between the predicted ranking of IAS values and the rankings of these tags as descending order. RankPrecision formula is as follows: RankPrecision = 1 − EZ1CompInverseCount(si) E�i�1InverseCount(si) (7) where si represents the ith sentence (unsegmented string), InverseCount(si) represents the number of discordant pairs inversions in the ranking of the predicted IAS value compared to the correct labeled ranking. CompInverseCount(si) represents the number of discordant pairs inversions when the labels totally inverse. 4.3 Experiments Results Contributions of the F</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing search engines using clickthrough data. Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Lafferty</author>
<author>A McCallum</author>
<author>F C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the Eighteenth International Conference on Machine Learning table of contents,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4053" citStr="Lafferty et al., 2001" startWordPosition="613" endWordPosition="616">or the first time. 1061 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061–1069, Honolulu, October 2008. c�2008 Association for Computational Linguistics In our method, we first predict the ranking result of all internal association strength (IAS) between each pair of adjacent characters in a sentence using Ranking SVM model, and then, we segment the sentence into sub-sentences with smaller and smaller granularity by cutting adjacent character pairs according to this rank. Other machine-learning based segmentation algorithms (Zhang et al., 2003; Lafferty et al., 2001; Ng and Low, 2004) treat segmentation problem as a character sequence tagging problem based on classification. However, these methods cannot directly obtain different segmentation granularity. Experiments show that our method can actually improve information retrieval performance. This paper is structured as follows. It starts with a brief introduction of the related work on the word segmentation approaches. Then in Section 3, we introduce our segmentation method. Section 4 evaluates the method based on experimental results. Finally, Section 5 makes summary of this whole paper and proposes th</context>
<context position="6127" citStr="Lafferty et al., 2001" startWordPosition="943" endWordPosition="946">r. These methods take advantage of various probability information gained from large corpus to segment sentences. Among them, Wang’s work (Wang, 2006) is the most similar to our method, since both of us apply statistics information of each gap in the sentence to eliminate overlapping ambiguity in methods. However, when combining different statistics, Wang decided the weight by a heuristic way which was too simply to be suitable for all sentences. In our method, we employ a machine-learning method to train features’ weights. Many machine-learning methods, such as HMM (Zhang et al., 2003), CRF (Lafferty et al., 2001), Maximum Entropy (Ng and Low, 2004), have been exploited in segmentation task. To our knowledge, machine-learning methods used in segmentation treated word segmentation as a character tagging problem. According to the model trained from training corpus and features extracted from the context in the sentence, these methods assign each character a positional tag, indicating its relative position in the word. These methods are difficult to get different granularity segmentation results directly. Our method has two main differences with them. Firstly, we tag the gap between characters rather than</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J.D. Lafferty, A. McCallum, and F.C.N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the Eighteenth International Conference on Machine Learning table of contents, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Liu</author>
</authors>
<title>Review of Chinese lexical and syntactic technology.</title>
<date>2002</date>
<contexts>
<context position="5221" citStr="Liu, 2002" startWordPosition="795" endWordPosition="796">mmary of this whole paper and proposes the future research orientation. 2 Related Work Various methods have been proposed to address the word segmentation problem in previous studies. They fall into two main categories, rule-based approaches that make use of linguistic knowledge and statistical approaches that train on corpus with machine learning methods. In rule-based approaches, algorithms of string matching based on dictionary are the most commonly used, such as maximum matching. They firstly segment sentences according to a dictionary and then resort to some rules to resolve ambiguities (Liu, 2002; Luo and Song, 2001). These rule-based methods are fast, however, their performances depend on the dictionary which cannot include all words, and also on the rules which cost a lot of time to make and must be updated frequently. Recent years statistical approaches became more popular. These methods take advantage of various probability information gained from large corpus to segment sentences. Among them, Wang’s work (Wang, 2006) is the most similar to our method, since both of us apply statistics information of each gap in the sentence to eliminate overlapping ambiguity in methods. However, </context>
</contexts>
<marker>Liu, 2002</marker>
<rawString>Q. Liu. 2002. Review of Chinese lexical and syntactic technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Y Luo</author>
<author>R Song</author>
</authors>
<date>2001</date>
<booktitle>Proper noun recognition in Chinese word segmentation research. Conference of international Chinese computer,</booktitle>
<pages>328--2001</pages>
<contexts>
<context position="5242" citStr="Luo and Song, 2001" startWordPosition="797" endWordPosition="800">is whole paper and proposes the future research orientation. 2 Related Work Various methods have been proposed to address the word segmentation problem in previous studies. They fall into two main categories, rule-based approaches that make use of linguistic knowledge and statistical approaches that train on corpus with machine learning methods. In rule-based approaches, algorithms of string matching based on dictionary are the most commonly used, such as maximum matching. They firstly segment sentences according to a dictionary and then resort to some rules to resolve ambiguities (Liu, 2002; Luo and Song, 2001). These rule-based methods are fast, however, their performances depend on the dictionary which cannot include all words, and also on the rules which cost a lot of time to make and must be updated frequently. Recent years statistical approaches became more popular. These methods take advantage of various probability information gained from large corpus to segment sentences. Among them, Wang’s work (Wang, 2006) is the most similar to our method, since both of us apply statistics information of each gap in the sentence to eliminate overlapping ambiguity in methods. However, when combining differ</context>
</contexts>
<marker>Luo, Song, 2001</marker>
<rawString>Z.Y. Luo and R. Song. 2001. Proper noun recognition in Chinese word segmentation research. Conference of international Chinese computer, 328:2001–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>W B Croft</author>
</authors>
<title>A Markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>472--479</pages>
<contexts>
<context position="25297" citStr="Metzler and Croft, 2005" startWordPosition="4096" endWordPosition="4099">tion result is ”海南/中/招录/取”, because the new word ”中招” cannot be recognized accurately, thus the character ”招” is combined with its latter character ”录”, causing overlapping ambiguity. By our method, the segmentation result is shown as figure 5, in which no overlapping ambiguity occurs. Performance of Chinese Information Retrieval: To evaluate the effectiveness of RSVM-Seg method on CIR, we compared it with the FMM segmentation. Our retrieval system combines different query representations obtained by our segmentation method, RSVM-Seg. In previous TREC Terebyte Track, Markov Random Field(MRF) (Metzler and Croft, 2005) model has displayed better performance than other information retrieval models, and it can much more easily include dependence features. There are three variants of MRF model, full independence(FI), sequential dependence(SD), and full dependence(FD). We chose SD as our retrieval model, since Chinese words are composed by characters and the adjacent characters have strong dependence relationship. We evaluated the CIR performance on the Chinese Web Corpora CWT200g provided by Tianwang 3, which, as we know, is the largest publicly available Chinese web corpus till now. It consists of 37, 482, 91</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>D. Metzler and W.B. Croft. 2005. A Markov random field model for term dependencies. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 472–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>J K Low</author>
</authors>
<title>Chinese part-of-speech tagging: one-at-a-time or all-at-once? word-based or character-based.</title>
<date>2004</date>
<booktitle>Proc of EMNLP.</booktitle>
<contexts>
<context position="4072" citStr="Ng and Low, 2004" startWordPosition="617" endWordPosition="620"> Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061–1069, Honolulu, October 2008. c�2008 Association for Computational Linguistics In our method, we first predict the ranking result of all internal association strength (IAS) between each pair of adjacent characters in a sentence using Ranking SVM model, and then, we segment the sentence into sub-sentences with smaller and smaller granularity by cutting adjacent character pairs according to this rank. Other machine-learning based segmentation algorithms (Zhang et al., 2003; Lafferty et al., 2001; Ng and Low, 2004) treat segmentation problem as a character sequence tagging problem based on classification. However, these methods cannot directly obtain different segmentation granularity. Experiments show that our method can actually improve information retrieval performance. This paper is structured as follows. It starts with a brief introduction of the related work on the word segmentation approaches. Then in Section 3, we introduce our segmentation method. Section 4 evaluates the method based on experimental results. Finally, Section 5 makes summary of this whole paper and proposes the future research o</context>
<context position="6163" citStr="Ng and Low, 2004" startWordPosition="949" endWordPosition="952">s probability information gained from large corpus to segment sentences. Among them, Wang’s work (Wang, 2006) is the most similar to our method, since both of us apply statistics information of each gap in the sentence to eliminate overlapping ambiguity in methods. However, when combining different statistics, Wang decided the weight by a heuristic way which was too simply to be suitable for all sentences. In our method, we employ a machine-learning method to train features’ weights. Many machine-learning methods, such as HMM (Zhang et al., 2003), CRF (Lafferty et al., 2001), Maximum Entropy (Ng and Low, 2004), have been exploited in segmentation task. To our knowledge, machine-learning methods used in segmentation treated word segmentation as a character tagging problem. According to the model trained from training corpus and features extracted from the context in the sentence, these methods assign each character a positional tag, indicating its relative position in the word. These methods are difficult to get different granularity segmentation results directly. Our method has two main differences with them. Firstly, we tag the gap between characters rather than characters themselves. Secondly, ou</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>H.T. Ng and J.K. Low. 2004. Chinese part-of-speech tagging: one-at-a-time or all-at-once? word-based or character-based. Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Y Nie</author>
<author>J Gao</author>
<author>J Zhang</author>
<author>M Zhou</author>
</authors>
<title>On the use of words and n-grams for Chinese information retrieval.</title>
<date>2000</date>
<booktitle>Proceedings of the fifth international workshop on on Information retrieval with Asian languages,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="15005" citStr="Nie et al., 2000" startWordPosition="2463" endWordPosition="2466">these bigrams for training are shown in Table 1. 3.3 Segmentation scheme In order to compare with other segmentation methods, which give a segmentation result based on two class labels, segmented and unsegmented, it is necessary to convert real numbers result given by Ranking SVM to these two labels. Here, we make a heuristic scheme to segment the sentence based on IAS ranking result predicted by Ranking SVM. The scheme is described in Algorithm 1. In each iteration we cut the sentence at the gap with minimum IAS value. Nie et.al. pointed out that the average length of words in usage is 1.59 (Nie et al., 2000). Therefore, we stop the segmentation iterative when the length of sub sentence is 2 or less than 2. By this method, we could represent the segmentation result as a binary tree. Figure 1 shows an example of this tree. With this tree, we can obtain various granularity segmentations easily, which could be used in CIR. This segmentation scheme may cause some combinatory ambiguity. However, Nie et.al. (Nie et al., 2000) also pointed out that there is no accurate word definition, thus whether combinatory ambiguity occurs is uncertain. What’s more, compared to overlapping ambiguity, combinatory ambi</context>
</contexts>
<marker>Nie, Gao, Zhang, Zhou, 2000</marker>
<rawString>J.Y. Nie, J. Gao, J. Zhang, and M. Zhou. 2000. On the use of words and n-grams for Chinese information retrieval. Proceedings of the fifth international workshop on on Information retrieval with Asian languages, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>X Huang</author>
<author>D Schuurmans</author>
<author>N Cercone</author>
</authors>
<title>Investigating the relationship between word segmentation performance and retrieval performance in Chinese IR.</title>
<date>2002</date>
<booktitle>Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="2312" citStr="Peng et al., 2002" startWordPosition="342" endWordPosition="345">known words, which are also demonstrated to affect the performance of Chinese information retrieval (Foo and Li, 2004). Overlapping ambiguity and combinatory ambiguity are two forms of segmentation ambiguity. The first one refers to that ABC can be segmented into AB C or A BC. The second one refers to that string AB can be a word, or A can be a word and B can be a word. In CIR, the combinatory ambiguity is also called segmentation granularity problem (Fan et al., 2007). There are many researches on the relationship between word segmentation and Chinese information retrieval (Foo and Li, 2004; Peng et al., 2002a; Peng et al., 2002b; Jin and Wong, 2002). Their studies show that the segmentation accuracy does not monotonically influence subsequent retrieval performance. Especially the overlapping ambiguity, as shown in experiments of (Wang, 2006), will cause more performance decrement of CIR. Thus a CIR system with a word segmenter better solving the overlapping ambiguity, may achieve better performance. Besides, it also showed that the precision of new word identification was more important than the recall. There are some researches show that when compound words are split into smaller constituents, b</context>
</contexts>
<marker>Peng, Huang, Schuurmans, Cercone, 2002</marker>
<rawString>F. Peng, X. Huang, D. Schuurmans, and N. Cercone. 2002a. Investigating the relationship between word segmentation performance and retrieval performance in Chinese IR. Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>X Huang</author>
<author>D Schuurmans</author>
<author>N Cercone</author>
<author>S E Robertson</author>
</authors>
<title>Using self-supervised word segmentation in Chinese information retrieval.</title>
<date>2002</date>
<booktitle>Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>349--350</pages>
<contexts>
<context position="2312" citStr="Peng et al., 2002" startWordPosition="342" endWordPosition="345">known words, which are also demonstrated to affect the performance of Chinese information retrieval (Foo and Li, 2004). Overlapping ambiguity and combinatory ambiguity are two forms of segmentation ambiguity. The first one refers to that ABC can be segmented into AB C or A BC. The second one refers to that string AB can be a word, or A can be a word and B can be a word. In CIR, the combinatory ambiguity is also called segmentation granularity problem (Fan et al., 2007). There are many researches on the relationship between word segmentation and Chinese information retrieval (Foo and Li, 2004; Peng et al., 2002a; Peng et al., 2002b; Jin and Wong, 2002). Their studies show that the segmentation accuracy does not monotonically influence subsequent retrieval performance. Especially the overlapping ambiguity, as shown in experiments of (Wang, 2006), will cause more performance decrement of CIR. Thus a CIR system with a word segmenter better solving the overlapping ambiguity, may achieve better performance. Besides, it also showed that the precision of new word identification was more important than the recall. There are some researches show that when compound words are split into smaller constituents, b</context>
</contexts>
<marker>Peng, Huang, Schuurmans, Cercone, Robertson, 2002</marker>
<rawString>F. Peng, X. Huang, D. Schuurmans, N. Cercone, and S.E. Robertson. 2002b. Using self-supervised word segmentation in Chinese information retrieval. Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 349–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
</authors>
<title>On GMAP: and other transformations.</title>
<date>2006</date>
<booktitle>Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>78--83</pages>
<contexts>
<context position="26231" citStr="Robertson, 2006" startWordPosition="4244" endWordPosition="4245">ed by characters and the adjacent characters have strong dependence relationship. We evaluated the CIR performance on the Chinese Web Corpora CWT200g provided by Tianwang 3, which, as we know, is the largest publicly available Chinese web corpus till now. It consists of 37, 482, 913 web pages with total size of 197GB. We used the topic set 3http://www.cwirf.org/ Segmentation Method MAP R-P GMAP FMM 0.0548 0.0656 0.0095 RSVM-Seg 0.0623 0.0681 0.0196 Table 5: Evaluation of CIR performance for SEWM2007 and SEWM2006 Topic Distillation (TD) task which contains 121 topics. MAP, RPrecision and GMAP (Robertson, 2006) were as main evaluation metrics. GMAP is the geometric mean of AP(Average Precision) through different queries, which was introduced to concentrate on difficult queries. The result is shown in 5. From the table, we can see that our segmentation method improve the CIR performance compared to FMM. 5 Conclusion and Future work From what we have discussed above, we can safely draw the conclusion that our work includes several main contributions. Firstly, to our best known, this is the first time to take the Chinese word segmentation problem as ranking problem, which provides a new view for Chines</context>
</contexts>
<marker>Robertson, 2006</marker>
<rawString>S. Robertson. 2006. On GMAP: and other transformations. Proceedings of the 15th ACM international conference on Information and knowledge management, pages 78–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sili Wang</author>
</authors>
<title>Research on chinese word segmentation for large scale information retrieval.</title>
<date>2006</date>
<contexts>
<context position="2550" citStr="Wang, 2006" startWordPosition="379" endWordPosition="380">n be segmented into AB C or A BC. The second one refers to that string AB can be a word, or A can be a word and B can be a word. In CIR, the combinatory ambiguity is also called segmentation granularity problem (Fan et al., 2007). There are many researches on the relationship between word segmentation and Chinese information retrieval (Foo and Li, 2004; Peng et al., 2002a; Peng et al., 2002b; Jin and Wong, 2002). Their studies show that the segmentation accuracy does not monotonically influence subsequent retrieval performance. Especially the overlapping ambiguity, as shown in experiments of (Wang, 2006), will cause more performance decrement of CIR. Thus a CIR system with a word segmenter better solving the overlapping ambiguity, may achieve better performance. Besides, it also showed that the precision of new word identification was more important than the recall. There are some researches show that when compound words are split into smaller constituents, better retrieval results can be achieved (Peng et al., 2002a). On the other hand, it is reasonable that the longer the word which co-exists in query and corpus, the more similarity they may have. A hypothesis, therefore, comes to our mind,</context>
<context position="5655" citStr="Wang, 2006" startWordPosition="865" endWordPosition="866">ry are the most commonly used, such as maximum matching. They firstly segment sentences according to a dictionary and then resort to some rules to resolve ambiguities (Liu, 2002; Luo and Song, 2001). These rule-based methods are fast, however, their performances depend on the dictionary which cannot include all words, and also on the rules which cost a lot of time to make and must be updated frequently. Recent years statistical approaches became more popular. These methods take advantage of various probability information gained from large corpus to segment sentences. Among them, Wang’s work (Wang, 2006) is the most similar to our method, since both of us apply statistics information of each gap in the sentence to eliminate overlapping ambiguity in methods. However, when combining different statistics, Wang decided the weight by a heuristic way which was too simply to be suitable for all sentences. In our method, we employ a machine-learning method to train features’ weights. Many machine-learning methods, such as HMM (Zhang et al., 2003), CRF (Lafferty et al., 2001), Maximum Entropy (Ng and Low, 2004), have been exploited in segmentation task. To our knowledge, machine-learning methods used </context>
<context position="11810" citStr="Wang 2006" startWordPosition="1894" endWordPosition="1895">ined as follows: mi(x, y) = 1o92 p(x, y) (2) p(x)p(y) where p(x, y) is the co-occurrence probability of x and y, namely the probability that bigram ’xy’ occurs in the training corpus, and p(x), p(y) are the independent probabilities of x and y respectively. From (2), we conclude that mi(x, y) » 0 means that IAS is strong; mi(x, y) Pz� 0 means that it is indefinite for IAS between characters x and y; mi(x, y) « 0 means that there is no association been characters x and y. However, mutual information has no consideration of context, so it cannot solve the overlapping ambiguity effectively (Sili Wang 2006). To remedy this defect, we introduce another statistics measure, difference of t-test. Difference of t-score (DTS): Difference of tscore is proposed on the basis of t-score. Given a Chinese character string ’xyz’, the t-score of the character y relevant to character x and z is defined min,,��., (1) 1063 as: p(z|y) − p(y|x) tx,z(y) = (3) VIσ2(p(z|y)) + σ2(p(y|x)) where p(y|x) is the conditional probability of y given x, and p(z|y), of z given y, and σ2(p(y|x)), σ2(p(z|y)) are variances of p(y|x) and of p(z|y) respectively. Sun et al. gave the derivation formula of σ2(p(y|x)),σ2(p(z|y)) (Sun et</context>
</contexts>
<marker>Wang, 2006</marker>
<rawString>Sili Wang. 2006. Research on chinese word segmentation for large scale information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Zhang</author>
<author>H K Yu</author>
<author>D Y Xiong</author>
<author>Q Liu</author>
</authors>
<title>HHMM-based Chinese Lexical Analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>Proceedings of Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>184--187</pages>
<contexts>
<context position="4030" citStr="Zhang et al., 2003" startWordPosition="609" endWordPosition="612">tation granularity for the first time. 1061 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061–1069, Honolulu, October 2008. c�2008 Association for Computational Linguistics In our method, we first predict the ranking result of all internal association strength (IAS) between each pair of adjacent characters in a sentence using Ranking SVM model, and then, we segment the sentence into sub-sentences with smaller and smaller granularity by cutting adjacent character pairs according to this rank. Other machine-learning based segmentation algorithms (Zhang et al., 2003; Lafferty et al., 2001; Ng and Low, 2004) treat segmentation problem as a character sequence tagging problem based on classification. However, these methods cannot directly obtain different segmentation granularity. Experiments show that our method can actually improve information retrieval performance. This paper is structured as follows. It starts with a brief introduction of the related work on the word segmentation approaches. Then in Section 3, we introduce our segmentation method. Section 4 evaluates the method based on experimental results. Finally, Section 5 makes summary of this whol</context>
<context position="6098" citStr="Zhang et al., 2003" startWordPosition="938" endWordPosition="941">roaches became more popular. These methods take advantage of various probability information gained from large corpus to segment sentences. Among them, Wang’s work (Wang, 2006) is the most similar to our method, since both of us apply statistics information of each gap in the sentence to eliminate overlapping ambiguity in methods. However, when combining different statistics, Wang decided the weight by a heuristic way which was too simply to be suitable for all sentences. In our method, we employ a machine-learning method to train features’ weights. Many machine-learning methods, such as HMM (Zhang et al., 2003), CRF (Lafferty et al., 2001), Maximum Entropy (Ng and Low, 2004), have been exploited in segmentation task. To our knowledge, machine-learning methods used in segmentation treated word segmentation as a character tagging problem. According to the model trained from training corpus and features extracted from the context in the sentence, these methods assign each character a positional tag, indicating its relative position in the word. These methods are difficult to get different granularity segmentation results directly. Our method has two main differences with them. Firstly, we tag the gap b</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>H.P. Zhang, H.K. Yu, D.Y. Xiong, and Q. Liu. 2003. HHMM-based Chinese Lexical Analyzer ICTCLAS. Proceedings of Second SIGHAN Workshop on Chinese Language Processing, pages 184–187.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>