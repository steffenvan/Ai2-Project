<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002321">
<title confidence="0.9921595">
Dialog Navigator: A Spoken Dialog Q-A System
based on Large Text Knowledge Base
</title>
<author confidence="0.88584">
Yoji Kiyota, Sadao Kurohashi (The University of Tokyo)
</author>
<email confidence="0.937912">
{kiyota,kuro}@kc.t.u-tokyo.ac.jp
</email>
<author confidence="0.725602">
Teruhisa Misu, Kazunori Komatani, Tatsuya Kawahara (Kyoto University)
</author>
<email confidence="0.973724">
{misu,komatani,kawahara}@kuis.kyoto-u.ac.jp
</email>
<author confidence="0.911017">
Fuyuko Kido (Microsoft Co., Ltd.)
</author>
<email confidence="0.965191">
fkido@microsoft.com
</email>
<sectionHeader confidence="0.993185" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998565">
This paper describes a spoken dialog Q-
A system as a substitution for call centers.
The system is capable of making dialogs
for both fixing speech recognition errors
and for clarifying vague questions, based
on only large text knowledge base. We in-
troduce two measures to make dialogs for
fixing recognition errors. An experimental
evaluation shows the advantages of these
measures.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896090909091">
When we use personal computers, we often en-
counter troubles. We usually consult large manu-
als, experts, or call centers to solve such troubles.
However, these solutions have problems: it is diffi-
cult for beginners to retrieve a proper item in large
manuals; experts are not always near us; and call
centers are not always available. Furthermore, op-
eration cost of call centers is a big problem for en-
terprises. Therefore, we proposed a spoken dialog
Q-A system which substitute for call centers, based
on only large text knowledge base.
If we consult a call center, an operator will help
us through a dialog. The substitutable system also
needs to make a dialog. First, asking backs for fixing
speech recognition errors are needed. Note that too
many asking backs make the dialog inefficient. Sec-
ondly, asking backs for clarifying users’ problems
are also needed, because they often do not know
their own problems so clearly.
To realize such asking backs, we developed a sys-
tem as shown in Figure 1. The features of our system
are as follows:
</bodyText>
<listItem confidence="0.983473">
• Precise text retrieval.
</listItem>
<bodyText confidence="0.758747">
The system precisely retrieves texts from large
</bodyText>
<figureCaption confidence="0.998466">
Figure 1: Architecture.
</figureCaption>
<bodyText confidence="0.9998806">
text knowledge base provided by Microsoft
Corporation (Table 1), using question types,
products, synonymous expressions, and syntac-
tic information. Dialog cards which can cope
with very vague questions are also retrieved.
</bodyText>
<listItem confidence="0.93186">
• Dialog for fixing speech recognition errors.
</listItem>
<bodyText confidence="0.999857416666667">
When accepting speech input, recognition er-
rors are inevitable. However, it is not obvi-
ous which portions of the utterance the sys-
tem should confirm by asking back to the user.
A great number of spoken dialog systems for
particular task domains, such as (Levin et al.,
2000), solved this problem by defining slots,
but it is not applicable to large text knowledge
base. Therefore, we introduce two measures
of confidence in recognition and significance
for retrieval to make dialogs for fixing speech
recognition errors.
</bodyText>
<listItem confidence="0.971613">
• Dialog for clarifying vague questions.
</listItem>
<bodyText confidence="0.942706">
When a user asks a vague question such as
“An error has occurred”, the system navigates
him/her to the desired answer, asking him/her
back using both dialog cards and extraction of
</bodyText>
<figure confidence="0.999117771428571">
user system
user’s selection
N-best candidates
(or reject all)
user’s selection
confirmation for
significant parts
choices in
dialog cards
user’s selection
final result
speech input
automatic speech recognizer
(Julius)
confirmation using
confidence in recognition
confirmation using
significance for retrieval
description extraction
asking back(s)
with dialog cards
retrieval result
N-best candidates of speech recognition
dialog for fixing
speech recognition
errors
dialog for clarifying
vague questions
text
retrieval
text
knowledge
base
dialog
cards
</figure>
<tableCaption confidence="0.998807">
Table 1: Text collections.
</tableCaption>
<table confidence="0.95412">
# of # of matching
text collection texts characters target
Glossary 4,707 700,000 entries
Help texts 11,306 6,000,000 titles
Support KB 23,323 22,000,000 entire texts
</table>
<bodyText confidence="0.999206125">
summaries that makes differences between re-
trieved texts more clear.
Our system makes asking backs by showing them
on a display, and users respond them by selecting
the displayed buttons by mouses.
Initially, we developed the system as a keyboard
based Q-A system, and started its service in April
2002 at the web site of Microsoft Corporation. The
extension for speech input was done based on the
one-year operation. Our system uses Julius (Lee et
al., 2001) as a Japanese speech recognizer, and it
uses language model acquired from the text knowl-
edge base of Microsoft Corporation.
In this paper, we describe the above three features
in Section 2, 3, and 4. After that, we show experi-
mental evaluation, and then conclude this paper.
</bodyText>
<sectionHeader confidence="0.902256" genericHeader="method">
2 Precise Text Retrieval
</sectionHeader>
<bodyText confidence="0.999878944444445">
It is critical for a Q-A system to retrieve relevant
texts for a question precisely. In this section, we
describe the score calculation method, giving large
points to modifier-head relations between bunsetsu1
based on the parse results of KNP (Kurohashi and
Nagao, 1994), to improve precision of text retrieval.
Our system also uses question types, product names,
and synonymous expression dictionary as described
in (Kiyota et al., 2002).
First, scores of all sentences in each text are calcu-
lated as shown in Figure 2. Sentence score is the to-
tal points of matching keywords and modifier-head
relations. We give 1 point to a matching of a key-
word, and 2 points to a matching of a modifier-head
relation (these parameters were set experimentally).
Then sentence score is normalized by the maximum
matching score (MMS) of both sentences as follows
(the MMS is the sentence score with itself):
</bodyText>
<footnote confidence="0.712733166666667">
sentence score
(the MMS of al (the MMS of al
user question) X text sentence J
1Bunsetsu is a commonly used linguistic unit in Japanese,
consisting of one or more adjoining content words and zero or
more following functional words.
</footnote>
<figure confidence="0.973293">
user question text sentence
normalized score = 52
8��� = 0.31
</figure>
<figureCaption confidence="0.999991">
Figure 2: Score calculation.
Figure 3: User navigation.
</figureCaption>
<bodyText confidence="0.9995195">
Finally, the sentence that has the largest score in
each text is selected as the representative sentence of
the text. Then, the score of the sentence is regarded
as the score of the text.
</bodyText>
<sectionHeader confidence="0.970382" genericHeader="method">
3 Dialog Strategy for Clarifying Questions
</sectionHeader>
<bodyText confidence="0.999954">
In most cases, users’ questions are vague. To cope
with such vagueness, our system uses the following
two methods: asking backs using dialog cards and
extraction of summaries that makes difference be-
tween retrieved texts more clear (Figure 3).
</bodyText>
<subsectionHeader confidence="0.992459">
3.1 Dialog cards
</subsectionHeader>
<bodyText confidence="0.9998873">
If a question is very vague, it matches many texts,
so users have to pay their labor on finding a rele-
vant one. Our system navigates users to the desired
answer using dialog cards as shown in Figure 3.
We made about three hundred of dialog cards
to throw questions back to users. Figure 4 shows
two dialog cards. &lt;UQ&gt; (User Question) is fol-
lowed by a typical vague user question. If a user
question matches it, the dialog manager asks the
back question after &lt;SYS&gt;, showing choices be-
</bodyText>
<figure confidence="0.99018485915493">
Outlook wo tsukatte Outlook de meru wo jushin
meru wo jushin dekinai. suru sai no error.
‘I cannot receive mails using Outlook.’ ‘An error while receiving mails
using Outlook.’
8 MMS 10
Outlook
‘Outlook’
tsukau
‘use’
meru
‘mail’
sentence score
= 5
jushin
‘receive’
+1
+1
+2
+1
Outlook
‘Outlook’
meru
‘mail’
jushin
‘receive’
error
‘error’
Windows 98 de kidouji ni
error ga hassei shita.
‘An error has occurred
while booting Windows 98.’
Komatte imasu.
‘I have a problem.’
user
questions
text knowledge base
concrete
vague
Error ga hassei shita.
‘An error has occurred.’
text retrieval &amp;
description extraction
clarifying questions
using dialog cards
)2
[Error]
&lt;UQ&gt; Errorgahasseisuru
‘An error occurs’
&lt;SYS&gt;Error wa itsu hassei shimasuka?
‘When does the error occurs?’
&lt;SELECT&gt;
Windows kidou ji goto [Error/Booting Windows]
‘while booting Windows’
in’satsuji goto [Error/Printing Out]
‘while printing out’
application kidouji goto [Error/Launching Applications]
‘while launching applications’
&lt;/SELECT&gt;
[Error/Booting Windows]
&lt;UQ&gt; Windows wo kidou ji ni error ga hassei suru
‘An error occurs while booting Windows’
&lt;SYS&gt;Anata ga otsukai no Windows wo erande kudasai.
‘Choose your Windows.’
&lt;SELECT&gt;
Windows 95 retrieve Windows 95 wo kidouji ni error ga hassei suru
‘An error occurs while booting Windows 95’
Windows 98 retrieve Windows 98 wo kidou ji ni error ga hassei suru
‘An error occurs while booting Windows 98’
Windows ME retrieve Windows ME wo kidou ji ni error ga hassei suru
‘An error occurs while booting Windows ME’
&lt;/SELECT&gt;
</figure>
<figureCaption confidence="0.999995">
Figure 4: Dialog cards.
</figureCaption>
<bodyText confidence="0.999364">
tween &lt;SELECT&gt; and &lt;/SELECT&gt;. Every choice is
followed by goto or retrieve. goto means that the
system follow the another dialog cards if this choice
is selected. retrieve means that the system retrieve
texts using the query specified there.
</bodyText>
<subsectionHeader confidence="0.999326">
3.2 Description extraction from retrieved texts
</subsectionHeader>
<bodyText confidence="0.999965166666667">
In most cases, the neighborhood of the part that
matches the user question describes specific symp-
toms and conditions of the problem users encounter.
Our system extracts such descriptions from the re-
trieved texts as the summaries of them. The algo-
rithm is described in (Kiyota et al., 2002).
</bodyText>
<sectionHeader confidence="0.955936" genericHeader="method">
4 Dialog Strategy for Speech Input
</sectionHeader>
<bodyText confidence="0.999935266666667">
It is necessary for a spoken dialog system to deter-
mine which portions of the speech input should be
confirmed. Moreover, criteria for judging whether
it should make confirmation or not are needed, be-
cause too many confirmations make the dialog inef-
ficient. Therefore, we introduce two criteria of con-
fidence in recognition and significance for retrieval.
Our system makes two types of asking backs for
fixing recognition errors (Figure 1). First, Julius out-
puts N-best candidates of speech recognition. Then,
the system makes confirmation for significant parts
based on confidence in recognition. After that, the
system retrieves relevant texts in the text knowledge
base using each candidate, and makes confirmation
based on significance for retrieval.
</bodyText>
<subsectionHeader confidence="0.997551">
4.1 Confidence in recognition
</subsectionHeader>
<bodyText confidence="0.998976818181818">
We define the confidence in recognition for each
phrase in order to reject partial recognition errors. It
is calculated based on word perplexity, which is of-
ten used in order to evaluate suitability of language
models for test-set sentences. We adopt word per-
plexity because of the following reasons: incorrectly
recognized parts are often unnatural in context, and
words that are unnatural in context have high per-
plexity values.
As Julius uses trigram as its language model, the
word perplexity PP is calculated as follows:
</bodyText>
<equation confidence="0.996134666666667">
log PP = — 1
n
k
</equation>
<bodyText confidence="0.999823">
PPs are summed up in each bunsetsu (phrases).
As a result, the system assigned the sum of PPs
to each bunsetsu as the criterion for confidence in
recognition.
We preliminarily defined the set of product names
as significant phrases2. If the sums of PPs for any
significant phrases are beyond the threshold (now,
we set it 50), the system makes confirmation for
these phrases.
</bodyText>
<subsectionHeader confidence="0.942378">
4.2 Significance for retrieval
</subsectionHeader>
<bodyText confidence="0.999230947368421">
The system calculates significance for retrieval us-
ing N-best candidates of speech recognition. Be-
cause slight speech recognition errors are not harm-
ful for retrieval results, we regard a difference that
affects its retrieval result as significant. Namely,
when the difference between retrieval results for
each recognition candidate is large, we regard that
the difference is significant.
Significance for retrieval is defined as a rate
of disagreement of five high-scored retrieved texts
among N recognition candidates. For example, if
there is a substituted part in two recognition candi-
dates, and only one text is commonly retrieved out
of five high-scored texts by both candidates, the sig-
nificance for retrieval for the substituted part is 0.8
(= 1 — 1/5).
The system makes confirmation which candidate
should be used, if significance for retrieval is beyond
the threshold (now, we set it 0.5).
</bodyText>
<footnote confidence="0.9261925">
2We are now developing a method to define the set of sig-
nificant phrases semi-automatically.
</footnote>
<table confidence="0.583912">
log P(wklwk-1, wk-2)-
</table>
<tableCaption confidence="0.962826">
Table 2: Number of successful retrieval for each speaker.
</tableCaption>
<table confidence="0.945371066666667">
speaker # of ASR transcription speech recognition with confidence with significance with both
ID utterances corr. (1) results (2) in recognition (3) for retrieval (4) measures (5)
A 13 87.8% 10/13 8/13 8/13 10/13 10/13
B 14 55.0% 6/14 3/14 5/14 3/14 5/14
C 13 61.7% 5/13 5/13 5/13 5/13 5/13
D 13 83.1% 11/13 9/13 9/13 9/13 9/13
total 53 68.5% 32/53 25/53 27/53 27/53 29/53
U: OS ME
‘I am using (Windows) ME as OS(operating system), then ...’
ASR: OS IME
(“ME” was incorrectly recognized as “IME”.)
S: The underlined product name seems to be uncorrectly recognized. May I
remove it, or leave it?
U: (selected “remove”)
S: The following texts are retrieved.
</table>
<figure confidence="0.881559928571429">
1. [AC97] [ ]
2. [ ] [ ]
3. [WinMe] TrueType (a correct answer)
U: Word 2002
“Please tell me the way to input formulas in Word 2002.”
ASR: 1. Word 2002 (numbers)
2. Word 2002 (formulas)
3. Word 2002 (values)
S: Please select the most correct recognition result from the above candidates.
U: (selected No. 2)
S: The following texts are retrieved.
1. Word (a correct answer)
2. Word
|XML |xmlLoc_3 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_continue bi_xmlPara_new
</figure>
<figureCaption confidence="0.9881575">
Figure 5: Dialogs for fixing speech recognition er-
rors.
</figureCaption>
<bodyText confidence="0.440818">
(U: user, S: system, ASR: automatic speech recognition)
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999927">
We evaluated the system performance experimen-
tally. For the experiments, we had 4 subjects, who
were accustomed to using computers. They made
utterances by following given 10 scenarios and also
made several utterances freely. In total, 53 utter-
ances were recorded. Figure 5 shows two successful
dialogs by confirmation using confidence in recog-
nition and by that using significance for retrieval.
We experimented on the system using the 53
recorded utterances by the following methods:
</bodyText>
<listItem confidence="0.998047090909091">
(1) Using correct transcription of recorded utter-
ance, including fillers.
(2) Using speech recognition results from which
only fillers were removed.
(3) Using speech recognition results and making
confirmation by confidence in recognition.
(4) Using N-best candidates of speech recognition
and making confirmation by significance for re-
trieval. Here, N = 3.
(5) Using N-best candidates of speech recognition
and both measures in (3) and (4).
</listItem>
<bodyText confidence="0.999870363636364">
In these experiments, we assumed that users al-
ways correctly answer system’s asking backs. We
regarded a retrieval as a successful one if a relevant
text was contained in ten high-scored retrieval texts.
Table 2 shows the result. It indicates that our
confirmation methods for fixing speech recognition
errors improve the success rate. Furthermore, the
success rate with both measures gets close to that
with the transcriptions. Considering that the speech
recognition correctness is about 70%, the proposed
dialog strategy is effective.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999997">
We proposed a spoken dialog Q-A system in which
asking backs for fixing speech recognition errors and
those for clarifying vague questions are integrated.
To realize dialog for fixing recognition errors based
on large text knowledge base, we introduced two
measures of confidence in recognition and signif-
icance for retrieval. The experimental evaluation
shows the advantages of these measures.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995804222222222">
Yoji Kiyota, Sadao Kurohashi, and Fuyuko Kido. 2002.
“Dialog Navigator” : A Question Answering System
based on Large Text Knowledge Base. In Proceedings
of COLING 2002, pages 460–466.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
A. Lee, T. Kawahara, and K. Shikano. 2001. Julius – an
open source real-time large vocabulary recognition en-
gine. In Proceedings of European Conf. Speech Com-
mun. &amp; Tech. (EUROSPEECH), pages 1691–1694.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The AT&amp;T-DARPA communicator mixed-
initiative spoken dialogue system. In Proceedings of
Int’l Conf. Spoken Language Processing (ICSLP).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.107208">
<title confidence="0.967222">Dialog Navigator: A Spoken Dialog Q-A System</title>
<author confidence="0.581744">based on Large Text Knowledge Base</author>
<affiliation confidence="0.735646">Kiyota, Sadao Kurohashi University of Tokyo</affiliation>
<email confidence="0.914062">kiyota@kc.t.u-tokyo.ac.jp</email>
<email confidence="0.914062">kuro@kc.t.u-tokyo.ac.jp</email>
<affiliation confidence="0.636005">Misu, Kazunori Komatani, Tatsuya Kawahara University</affiliation>
<email confidence="0.874488">misu@kuis.kyoto-u.ac.jp</email>
<email confidence="0.874488">komatani@kuis.kyoto-u.ac.jp</email>
<email confidence="0.874488">kawahara@kuis.kyoto-u.ac.jp</email>
<note confidence="0.37975">Kido Co., Ltd.</note>
<email confidence="0.999354">fkido@microsoft.com</email>
<abstract confidence="0.997411363636364">This paper describes a spoken dialog Q- A system as a substitution for call centers. The system is capable of making dialogs for both fixing speech recognition errors and for clarifying vague questions, based on only large text knowledge base. We introduce two measures to make dialogs for fixing recognition errors. An experimental evaluation shows the advantages of these measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoji Kiyota</author>
<author>Sadao Kurohashi</author>
<author>Fuyuko Kido</author>
</authors>
<title>Dialog Navigator” : A Question Answering System based on Large Text Knowledge Base.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>460--466</pages>
<contexts>
<context position="4840" citStr="Kiyota et al., 2002" startWordPosition="745" endWordPosition="748">tion. In this paper, we describe the above three features in Section 2, 3, and 4. After that, we show experimental evaluation, and then conclude this paper. 2 Precise Text Retrieval It is critical for a Q-A system to retrieve relevant texts for a question precisely. In this section, we describe the score calculation method, giving large points to modifier-head relations between bunsetsu1 based on the parse results of KNP (Kurohashi and Nagao, 1994), to improve precision of text retrieval. Our system also uses question types, product names, and synonymous expression dictionary as described in (Kiyota et al., 2002). First, scores of all sentences in each text are calculated as shown in Figure 2. Sentence score is the total points of matching keywords and modifier-head relations. We give 1 point to a matching of a keyword, and 2 points to a matching of a modifier-head relation (these parameters were set experimentally). Then sentence score is normalized by the maximum matching score (MMS) of both sentences as follows (the MMS is the sentence score with itself): sentence score (the MMS of al (the MMS of al user question) X text sentence J 1Bunsetsu is a commonly used linguistic unit in Japanese, consistin</context>
<context position="8762" citStr="Kiyota et al., 2002" startWordPosition="1389" endWordPosition="1392">s ME’ &lt;/SELECT&gt; Figure 4: Dialog cards. tween &lt;SELECT&gt; and &lt;/SELECT&gt;. Every choice is followed by goto or retrieve. goto means that the system follow the another dialog cards if this choice is selected. retrieve means that the system retrieve texts using the query specified there. 3.2 Description extraction from retrieved texts In most cases, the neighborhood of the part that matches the user question describes specific symptoms and conditions of the problem users encounter. Our system extracts such descriptions from the retrieved texts as the summaries of them. The algorithm is described in (Kiyota et al., 2002). 4 Dialog Strategy for Speech Input It is necessary for a spoken dialog system to determine which portions of the speech input should be confirmed. Moreover, criteria for judging whether it should make confirmation or not are needed, because too many confirmations make the dialog inefficient. Therefore, we introduce two criteria of confidence in recognition and significance for retrieval. Our system makes two types of asking backs for fixing recognition errors (Figure 1). First, Julius outputs N-best candidates of speech recognition. Then, the system makes confirmation for significant parts b</context>
</contexts>
<marker>Kiyota, Kurohashi, Kido, 2002</marker>
<rawString>Yoji Kiyota, Sadao Kurohashi, and Fuyuko Kido. 2002. “Dialog Navigator” : A Question Answering System based on Large Text Knowledge Base. In Proceedings of COLING 2002, pages 460–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="4672" citStr="Kurohashi and Nagao, 1994" startWordPosition="720" endWordPosition="723">ar operation. Our system uses Julius (Lee et al., 2001) as a Japanese speech recognizer, and it uses language model acquired from the text knowledge base of Microsoft Corporation. In this paper, we describe the above three features in Section 2, 3, and 4. After that, we show experimental evaluation, and then conclude this paper. 2 Precise Text Retrieval It is critical for a Q-A system to retrieve relevant texts for a question precisely. In this section, we describe the score calculation method, giving large points to modifier-head relations between bunsetsu1 based on the parse results of KNP (Kurohashi and Nagao, 1994), to improve precision of text retrieval. Our system also uses question types, product names, and synonymous expression dictionary as described in (Kiyota et al., 2002). First, scores of all sentences in each text are calculated as shown in Figure 2. Sentence score is the total points of matching keywords and modifier-head relations. We give 1 point to a matching of a keyword, and 2 points to a matching of a modifier-head relation (these parameters were set experimentally). Then sentence score is normalized by the maximum matching score (MMS) of both sentences as follows (the MMS is the senten</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lee</author>
<author>T Kawahara</author>
<author>K Shikano</author>
</authors>
<title>Julius – an open source real-time large vocabulary recognition engine.</title>
<date>2001</date>
<booktitle>In Proceedings of European Conf. Speech Commun. &amp; Tech. (EUROSPEECH),</booktitle>
<pages>1691--1694</pages>
<contexts>
<context position="4101" citStr="Lee et al., 2001" startWordPosition="626" endWordPosition="629"> matching text collection texts characters target Glossary 4,707 700,000 entries Help texts 11,306 6,000,000 titles Support KB 23,323 22,000,000 entire texts summaries that makes differences between retrieved texts more clear. Our system makes asking backs by showing them on a display, and users respond them by selecting the displayed buttons by mouses. Initially, we developed the system as a keyboard based Q-A system, and started its service in April 2002 at the web site of Microsoft Corporation. The extension for speech input was done based on the one-year operation. Our system uses Julius (Lee et al., 2001) as a Japanese speech recognizer, and it uses language model acquired from the text knowledge base of Microsoft Corporation. In this paper, we describe the above three features in Section 2, 3, and 4. After that, we show experimental evaluation, and then conclude this paper. 2 Precise Text Retrieval It is critical for a Q-A system to retrieve relevant texts for a question precisely. In this section, we describe the score calculation method, giving large points to modifier-head relations between bunsetsu1 based on the parse results of KNP (Kurohashi and Nagao, 1994), to improve precision of tex</context>
</contexts>
<marker>Lee, Kawahara, Shikano, 2001</marker>
<rawString>A. Lee, T. Kawahara, and K. Shikano. 2001. Julius – an open source real-time large vocabulary recognition engine. In Proceedings of European Conf. Speech Commun. &amp; Tech. (EUROSPEECH), pages 1691–1694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
<author>K Biatov</author>
<author>E Bocchieri</author>
<author>G Di Fabbrizio</author>
<author>W Eckert</author>
<author>S Lee</author>
<author>A Pokrovsky</author>
<author>M Rahim</author>
<author>P Ruscitti</author>
<author>M Walker</author>
</authors>
<title>The AT&amp;T-DARPA communicator mixedinitiative spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings of Int’l Conf. Spoken Language Processing (ICSLP).</booktitle>
<marker>Levin, Narayanan, Pieraccini, Biatov, Bocchieri, Di Fabbrizio, Eckert, Lee, Pokrovsky, Rahim, Ruscitti, Walker, 2000</marker>
<rawString>E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker. 2000. The AT&amp;T-DARPA communicator mixedinitiative spoken dialogue system. In Proceedings of Int’l Conf. Spoken Language Processing (ICSLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>