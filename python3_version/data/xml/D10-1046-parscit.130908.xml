<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015516">
<title confidence="0.998142">
Fusing Eye Gaze with Speech Recognition Hypotheses to
Resolve Exophoric References in Situated Dialogue
</title>
<author confidence="0.997239">
Zahar Prasov and Joyce Y. Chai
</author>
<affiliation confidence="0.996457">
Department of Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.96634">
East Lansing, MI 48824, USA
</address>
<email confidence="0.999826">
{prasovza,jchai}@cse.msu.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999799458333334">
In situated dialogue humans often utter lin-
guistic expressions that refer to extralinguistic
entities in the environment. Correctly resolv-
ing these references is critical yet challeng-
ing for artificial agents partly due to their lim-
ited speech recognition and language under-
standing capabilities. Motivated by psycholin-
guistic studies demonstrating a tight link be-
tween language production and human eye
gaze, we have developed approaches that in-
tegrate naturally occurring human eye gaze
with speech recognition hypotheses to resolve
exophoric references in situated dialogue in
a virtual world. In addition to incorporat-
ing eye gaze with the best recognized spo-
ken hypothesis, we developed an algorithm to
also handle multiple hypotheses modeled as
word confusion networks. Our empirical re-
sults demonstrate that incorporating eye gaze
with recognition hypotheses consistently out-
performs the results obtained from processing
recognition hypotheses alone. Incorporating
eye gaze with word confusion networks fur-
ther improves performance.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964523809524">
Given a rapid growth in virtual world applications
for tutoring and training, video games and simu-
lations, and assistive technology, enabling situated
dialogue in virtual worlds has become increasingly
important. Situated dialogue allows human users to
navigate in a spatially rich environment and carry
a conversation with artificial agents to achieve spe-
cific tasks pertinent to the environment. Different
from traditional telephony-based spoken dialogue
systems and multimodal conversational interfaces,
situated dialogue supports immersion and mobility
in a visually rich environment and encourages so-
cial and collaborative language use (Byron et al.,
2005; Gorniak et al., 2006). In situated dialogue, hu-
man users often need to make linguistic references,
known as exophoric referring expressions (e.g., the
book to the right), to extralinguistic entities
in the environment. Reliably resolving these ref-
erences is critical for dialogue success. However,
reference resolution remains a challenging problem,
partly due to limited speech and language process-
ing capabilities caused by poor speech recognition
(ASR), ambiguous language, and insufficient prag-
matic knowledge.
To address this problem, motivated by psycholin-
guistic studies demonstrating a close relationship
between language production and eye gaze, our
previous work has incorporated naturally occurring
eye gaze in reference resolution (Prasov and Chai,
2008). Our findings have shown that eye gaze can
partially compensate for limited language process-
ing and domain modeling. However, this work was
conducted in a setting where users only spoke to a
static visual interface. In situated dialogue, human
speech and eye gaze patterns are much more com-
plex. The dynamic nature of the environment and
the complexity of spatially rich tasks have a massive
influence on what the user will look at and say. It is
not clear to what degree prior findings can generalize
to situated dialogue. Therefore, this paper explores
new studies on incorporating eye gaze for exophoric
reference resolution in a fully situated virtual envi-
</bodyText>
<page confidence="0.982777">
471
</page>
<note confidence="0.8177255">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 471–481,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999871736842105">
ronment — a more realistic approximation of real
world interaction. In addition to incorporating eye
gaze with the best recognized spoken hypothesis, we
developed an algorithm to also handle multiple hy-
potheses modeled as word confusion networks.
Our empirical results have demonstrated the util-
ity of eye gaze for reference resolution in situ-
ated dialogue. Although eye gaze is much more
noisy given the mobility of the user, our results
have shown that incorporating eye gaze with recog-
nition hypotheses consistently outperform the re-
sults obtained from processing recognition hypothe-
ses alone. In addition, incorporating eye gaze with
word confusion networks further improves perfor-
mance. Our analysis also indicates that, although a
word confusion network appears to be more compli-
cated, the time complexity of its integration with eye
gaze is well within the acceptable range for real-time
applications.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999949825396826">
Prior work in reference resolution within situated di-
alogue has focused on using visual context to assist
reference resolution during interaction. In (Kelleher
and van Genabith, 2004) and (Byron et al., 2005), vi-
sual features of objects are used to model the focus
of attention. This attention modeling is subsequently
used to resolve references. In contrast to this line of
research, here we explore the use of human eye gaze
during real-time interaction to model attention and
facilitate reference resolution. Eye gaze provides a
richer medium for attentional information, but re-
quires processing of a potentially noisy signal.
Eye gaze has been used to facilitate human ma-
chine conversation and automated language process-
ing. For example, eye gaze has been studied in
embodied conversational discourse as a mechanism
to gather visual information, aid in thinking, or fa-
cilitate turn taking and engagement (Nakano et al.,
2003; Bickmore and Cassell, 2004; Sidner et al.,
2004; Morency et al., 2006; Bee et al., 2009).
Recent work has explored incorporating eye gaze
into automated language understanding such as au-
tomated speech recognition (Qu and Chai, 2007;
Cooke and Russell, 2008), automated vocabulary ac-
quisition (Liu et al., 2007; Qu and Chai, 2010), at-
tention prediction (Qvarfordt and Zhai, 2005; Fang
et al., 2009).
Motivated by previous psycholinguistic findings
that eye gaze is tightly linked with language pro-
cessing (Just and Carpenter, 1976; Tanenhous et al.,
1995; Meyer and Levelt, 1998; Griffin and Bock,
2000), our prior work incorporates eye gaze into
reference resolution. Our results demonstrate that
such use of eye gaze can potentially compensate
for a conversational systems limited language pro-
cessing and domain modeling capability (Prasov and
Chai, 2008). However, this work is conducted in a
static visual environment and evaluated only on tran-
scribed spoken utterances. In situated dialogue, eye
gaze behavior is much more complex. Here, gaze
fixations may be made for the purpose of naviga-
tion or scanning the environment rather than refer-
ring to a particular object. Referring expressions can
be made to objects that are not in the user’s field of
view, but were previously visible on the interface.
Additionally, users may make egocentric spatial ref-
erences (e.g. “the chair on the left”) which require
contextual knowledge (e.g. the users position in the
environment) in order to resolve. Therefore, the fo-
cus of our work here is on exploring these complex
user behaviors in situated dialogue and examining
how to combine eye gaze with ASR hypotheses for
improved reference resolution.
Alternative ASR hypotheses have been used in
many different ways in speech driven systems. Par-
ticularly, in (Mangu et al., 2000) multiple lattice
alignment is used for construction of word confusion
networks and in (Hakkani-T¨ur et al., 2006) word
confusion networks are used for named entity de-
tection. In the study presented here, we apply word
confusion networks (to represent ASR hypotheses)
along with eye gaze to the problem of reference res-
olution.
</bodyText>
<sectionHeader confidence="0.99095" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.999929142857143">
In this investigation, we created a 3D virtual world
(using the Irrlicht game engine1) to support situated
dialogue. We conducted a Wizard of Oz study in
which the user must collaborate with a remote arti-
ficial agent cohort (controlled by a human) to solve
a treasure hunting task. The cohort is an “expert”
in treasure hunting and has some knowledge regard-
</bodyText>
<footnote confidence="0.993738">
1http://irrlicht.sourceforge.net/
</footnote>
<page confidence="0.9982">
472
</page>
<bodyText confidence="0.999749066666667">
ing the locations of the treasure items, but cannot
see the virtual environment. The user, immersed in
the virtual world, must navigate the environment and
conduct a mixed-initiative dialogue with the agent to
find the hidden treasures. During the experiments,
a noise-canceling microphone was used to record
user speech and the Tobii 1750 display-mounted eye
tracker was used to record user eye movements.
A snapshot of user interaction with the treasure
hunting environment is shown in Figure 1. Here,
the user’s eye fixation is represented by the white
dot and saccades (eye movements) are represented
by white lines. The virtual world contains 10 rooms
with a total of 155 unique objects that encompass 74
different object types (e.g. chair or plant).
</bodyText>
<figureCaption confidence="0.9881825">
Figure 1: Snapshot of the situated treasure hunting envi-
ronment
</figureCaption>
<bodyText confidence="0.998584444444444">
Table 1 shows a portion of a sample dialogue be-
tween a user and the expert. Each Si represents a
system utterance and each Ui represents a user ut-
terance. We focus on resolving exophoric referring
expressions, which are enclosed in brackets here. In
our dataset, an exophoric referring expression is a
non-pronominal noun phrase that refers to an en-
tity in the extralinguistic environment. It may be
an evoking reference that initially refers to a new
object in the virtual world (e.g. an axe in utter-
ance U2) or a subsequent reference to an entity in the
virtual world which has previously been mentioned
in the dialogue (e.g. an axe in utterance U3). In
our study we focus on resolving exophoric referring
expressions because they are tightly coupled with a
user’s eye gaze behavior.
From this study, we constructed a parallel spoken
utterance and eye gaze corpus. Utterances, which
</bodyText>
<note confidence="0.840416142857143">
S1 Describe what you’re doing.
U1 I just came out from the room that
U2 I started and i see [one long sword]
[one short sword] and [an axe]
S2 Compare these objects.
U3 one of them is long and one of them
is really short, and i see [an axe]
</note>
<tableCaption confidence="0.9884555">
Table 1: A conversational fragment demonstrating inter-
action with exophoric referring expressions.
</tableCaption>
<table confidence="0.9746572">
Utterance: i just came out from the room that
i started and i see [one long sword]
Ht : ... i 5210 see 5410 [one 5630
long 6080 sword 6460]
. . . ... icy 5210 winds 5630 along 6080
H25: so 6460 words 68000
. . . ... icy 5210 [wine 5630] along 6080
so 6460 words 6800
... icy 5210 winds 5630
[long 6080 sword 6460]
</table>
<tableCaption confidence="0.999669">
Table 2: Sample n-best list of recognition hypotheses
</tableCaption>
<bodyText confidence="0.9994104">
are separated by a long pause (500 ms) in speech,
are automatically recognized using the Microsoft
Speech SDK. Gaze fixations are characterized by
objects in the virtual world that are fixated via a
user’s eye gaze. When a fixation points to multi-
ple spatially overlapping objects, only the one in the
forefront is deemed to be fixated. The data corpus
was transcribed and annotated with 2204 exophoric
referring expressions amongst 2052 utterances from
15 users.
</bodyText>
<sectionHeader confidence="0.993578" genericHeader="method">
4 Word Confusion Networks
</sectionHeader>
<bodyText confidence="0.999235909090909">
For each user utterance in our dataset, an n-best list
(with n = 100) of recognition hypotheses ranked
in order of likelihood is produced by the Microsoft
Speech SDK. One way to use the speech recogni-
tion results (as in most speech applications) is to use
the top ranked recognition hypothesis. This may not
be the best solution because a large amount of infor-
mation is being ignored. Table 2 demonstrates this
problem. Here, the number after the underscore de-
notes a timestamp associated with each recognized
spoken word. The strings enclosed in brackets de-
</bodyText>
<page confidence="0.996445">
473
</page>
<bodyText confidence="0.999906921568628">
note recognized referring expressions. In this exam-
ple, the manual transcription of the original utter-
ance is shown by Ht. In this case, the system must
first identify one long sword as a referring ex-
pression and then resolve it to the correct set of en-
tities in the virtual world. However, not until the
twenty fifth ranked recognition hypothesis H25, do
we see a referring expression closest to the actual ut-
tered referring expression. Moreover, in utterances
with multiple referring expressions, there may not
be a single recognition hypothesis that contains all
referring expressions, but each referring expression
may be contained in some recognition hypothesis.
Thus, it is desirable to consider the entire n-best list
of hypotheses.
To address this issue, we adopted the word con-
fusion network (WCN): a compact representation
of a word lattice or n-best list (Mangu et al.,
2000). A WCN captures alternative word hypothe-
ses and their corresponding posterior probabilities
in time-ordered sets. In addition to being compact,
an important feature for efficient post-processing
of recognition hypotheses for real-time systems,
WCNs are capable of representing more competing
hypotheses than either n-best lists or word lattices.
Figure 2 shows an example of a WCN for the utter-
ance “... I see one long sword” along with a timeline
(in milliseconds) depicting the eye gaze fixations to
potential referent objects that correspond to the ut-
terance. The confusion network shows competing
word hypotheses along with corresponding proba-
bilities in log scale.
Using our data set, we can show that word con-
fusion networks contain significantly more words
that can compose a referring expression than the top
recognition hypothesis. The confusion network key-
word error rate (KWER) is 0.192 compared to a 1-
best list KWER of 0.318, where a keyword is a word
that can be contained in a referring expression. The
overall WER for word confusion networks and 1-
best lists are 0.315 and 0.460, respectively. The re-
ported WCN word error rates are all oracle word er-
ror rates reflecting the best WER that can be attained
using any path in the confusion network. One more
important feature of word confusion networks is that
they provide time alignment for words that occur at
approximately the same time interval in competing
hypotheses. This is not only useful for efficient syn-
tactic parsing, which is necessary for identifying re-
ferring expressions, but also critical for integration
with time aligned gaze streams.
</bodyText>
<sectionHeader confidence="0.974319" genericHeader="method">
5 Reference Resolution Algorithm
</sectionHeader>
<bodyText confidence="0.9992718">
We have developed an algorithm that combines an
n-best list of speech recognition hypotheses with di-
alogue, domain, and eye-gaze information to resolve
exophoric referring expressions. There are three in-
puts to the multimodal reference resolution algo-
rithm for each utterance: (1) an n-best list of alter-
native speech recognition hypotheses (n = 100 for a
WCN and n = 1 for the top recognized hypothesis),
(2) a list of fixated objects (by eye gaze) that tempo-
rally correspond to the spoken utterance and (3) a set
of potential referent objects. Since during the trea-
sure hunting task people typically only speak about
objects that are visible or have recently been visible
on the screen, an object is considered to be a poten-
tial referent if it is present within a close proximity
(in the same room) of the user while an utterance is
spoken.
The multimodal reference resolution algorithm
proceeds with the following four steps:
Step 1: construct word confusion network A
word confusion network is constructed out of the in-
put n-best list of alternative recognition hypotheses
with the SRI Language Modeling (SRILM) toolkit
(Stolcke, 2002) using the procedure described in
(Mangu et al., 2000). This procedure aligns words
from the n-best list into equivalence classes. First,
instances of the same word containing approxi-
mately the same starting and ending timestamps are
clustered. Then, equivalence classes with common
time ranges are merged. For each competing word
hypothesis its probability is computed by summing
the posteriors of all utterance hypotheses containing
this word. In our work, instead of using the actual
posterior probability of each utterance hypothesis
(which was not available), we assigned each utter-
ance hypothesis a probability based on its position
in the ranked list. Figure 2 depicts a portion of the
resulting word confusion network (showing compet-
ing word hypotheses and their probabilities in log
scale) constructed from the n-best list in Table 2.
</bodyText>
<page confidence="0.998587">
474
</page>
<figureCaption confidence="0.999273">
Figure 2: Sample parallel speech and eye gaze data streams, including a portion of the sample WCN
</figureCaption>
<bodyText confidence="0.999706425">
Step 2: extract referring expressions from WCN
The word confusion network is syntactically parsed
using a modified version of the CYK (Cooke and
Schwartz, 1970; Kasami, 1965; Younger, 1967)
parsing algorithm that is capable of taking a word
confusion network as input rather than a single
string. We call this the CYK-WCN algorithm. To
do the parsing, we applied a set of grammar rules
largely derived from a different domain in our pre-
vious work (Prasov and Chai, 2008). A parse chart
of the sample word confusion network is shown in
Table 3. Here, just as in the CYK algorithm the
chart is filled in from left to right then bottom to
top. The difference is that the chart has an added
dimension for competing word hypotheses. This
is demonstrated in position 15 of the WCN, where
one and wine are two nouns that constitute com-
peting words. Note that some words from the con-
fusion network are not in the chart (e.g. winds)
because they are out of vocabulary. The result of
the syntactic parsing is that the parts of speech of
all sub-phrases in the confusion network are identi-
fied. Next, a set of all exophoric referring expres-
sions (i.e. non-pronominal noun phrases) found in
the word confusion network are extracted. Each re-
ferring expression has a corresponding confidence
score, which can be computed in many many dif-
ferent ways. Currently, we simply take the mean of
the probability scores of the expression’s constituent
words. The sample WCN has four such phrases
(shown in bold in Table 3): wine at position 15 with
length 1, one long sword at position 15 with
length 3, long sword at position 16 with length
2, and sword at position 17 with length 1.
Step 3: resolve referring expressions Each re-
ferring expression rj is resolved to the top k po-
tential referent objects according to the probabil-
ity P(oi|rj), where k is determined by information
from the linguistic expressions. P(oi|rj) is deter-
mined using the following expression:
</bodyText>
<equation confidence="0.994424666666667">
Po�r�
( Z |�) — AS(oi)α x Compat(oi, rj)1−α
—
EAS(oi)α x Compat(oi, rj)1−α
i
(1)
</equation>
<bodyText confidence="0.961179">
In this equation,
</bodyText>
<listItem confidence="0.998304">
• AS: Attentional salience score of a particu-
</listItem>
<page confidence="0.994113">
475
</page>
<figure confidence="0.993443666666667">
length
(1) N → wine, NP → N
(2) NUM → one
15
Adj-NP → ADJ N,
NP → Adj-NP
ADJ → long
16
N → sword,
NP → N
17
...
5
4 NP → NUM Adj-NP
3
2
1 14
...
...
18
WCN position
</figure>
<tableCaption confidence="0.997731">
Table 3: Syntactic parsing of word confusion network
</tableCaption>
<bodyText confidence="0.996644">
lar object oi, which is determined based on
the gaze fixation intensity of an object at the
start time of referring expression rj. The fix-
ation intensity of an object is defined as the
amount of time that the object is fixated during
a predefined time window W (Prasov and Chai,
2008). As in (Prasov and Chai, 2008), we set
W = [−1500..0] ms relative to the beginning
of referring expression rj.
</bodyText>
<listItem confidence="0.963891571428572">
• Compat: Compatibility score, which specifies
whether the object oi is compatible with the in-
formation specified by the referring expression
rj. Currently, the compatibility score is set to 1
if referring expression rj and object oi have the
same object type (e.g. chair), and 0 otherwise.
• a: Importance weight, in the range [0..1], of
</listItem>
<bodyText confidence="0.98668180952381">
attentional salience relative to compatibility.
A high a value indicates that the attentional
salience score based on eye gaze carries more
weight in deciding referents, while a low a
value indicates that compatibility carries more
weight. In this work, we set a = 0.5 to indicate
equal weighting between attentional salience
and compatibility. If we do not want to inte-
grate eye gaze in reference resolution, we can
set a = 0.0. In this case, reference resolution
will be purely based on compatibility between
visual objects and information specified via lin-
guistic expressions.
Once all probabilities are calculated, each refer-
ring expression is resolved to a set of referent ob-
jects. Finally, this results in a set of (referring ex-
pression, referent object set) pairs with confidence
scores, which are determined by two components.
The first component is the confidence score of the
referring expression, which is explained in the Step
1 of the algorithm. The second component is the
probability that the referent object set is indeed the
referent of this expression (which is determined by
Equation 1). There are various ways to combine
these two components together to form an overall
confidence score for the pair. Here we simply mul-
tiply the two components. The confidence score for
the pair is used in the following step to prune un-
likely referring expressions.
Step 4: post-prune The resulting set of (referring
expression, referent object set) pairs is pruned to re-
move pairs that fall under one of the following two
conditions: (1) the pair has a confidence score equal
to or below a predefined threshold c (currently, the
threshold is set to 0 and thus keeps all resolved pairs)
and (2) the pair temporally overlaps with a higher
confidence pair. For example, in Table 3, the re-
ferring expressions one long sword and wine
overlap in position 15. Finally, the resulting (refer-
ring expression, referent object set) pairs are sorted
in ascending order according to their constituent re-
ferring expression timestamps.
</bodyText>
<sectionHeader confidence="0.996883" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999975555555556">
Using our data, described in Section 3, we applied
the multimodal reference resolution algorithm de-
scribed in Section 5. All of the data is used to
report the experimental results. Reference resolu-
tion model parameters are set based on our prior
work in a different domain (Prasov and Chai, 2008).
For each utterance we compare the reference reso-
lution performance with and without the integration
of eye gaze information. We also evaluate using a
</bodyText>
<page confidence="0.998318">
476
</page>
<bodyText confidence="0.9940115">
word confusion network compared to a 1-best list to this case, a referring expression from the sys-
model speech recognition hypotheses. For perspec- tem output needs to exactly match the corre-
tive, reference resolution with recognized speech in- sponding expression from the gold standard.
put is compared with transcribed speech.
</bodyText>
<subsectionHeader confidence="0.991833">
6.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999715">
The reference resolution algorithm outputs a list of
(referring expression, referent object set) pairs for
each utterance. We evaluate the algorithm by com-
paring the generated pairs to the annotated “gold
standard” pairs using F-measure. We perform the
following two types of evaluation:
</bodyText>
<listItem confidence="0.978968142857143">
• Lenient Evaluation: Due to speech recognition
errors, there are many cases in which the al-
gorithm may not return a referring expression
that exactly matches the gold standard refer-
ring expression. It may only match based on
the object type. For example, the expressions
one long sword and sword are different,
but they match in terms of the intended object
type. For applications in which it is critical to
identify the objects referred to by the user, pre-
cisely identifying uttered referring expressions
may be unnecessary. Thus, we evaluate the ref-
erence resolution algorithm with a lenient com-
parison of (referring expression, referent object
set) pairs. In this case, two pairs are considered
a match if at least the object types specified via
the referring expressions match each other and
the referent object sets are identical.
• Strict Evaluation: For some applications it may
be important to identify exact referring ex-
pressions in addition to the objects they re-
</listItem>
<bodyText confidence="0.769609307692308">
fer to. This is important for applications that
attempt to learn a relationship between refer-
ring expressions and referenced objects. For
example, in automated vocabulary acquisition,
words other than object types must be identi-
fied so the system can learn to associate these
words with referenced objects. Similarly, in
systems that apply priming for language gen-
eration, identification of the exact referring ex-
pressions from human users could be impor-
tant. Thus, we also evaluate the reference reso-
lution algorithm with a strict comparison of (re-
ferring expression, referent object set) pairs. In
</bodyText>
<subsectionHeader confidence="0.999355">
6.2 Role of Eye Gaze
</subsectionHeader>
<bodyText confidence="0.999243352941176">
We evaluate the effect of incorporating eye gaze
information into the reference resolution algorithm
using the top best recognition hypothesis (1-best),
the word confusion network (WCN), and the man-
ual speech transcription (Transcription). Speech
transcription, which contains no recognition errors,
demonstrates the upper bound performance of our
approach. When no gaze information is used, ref-
erence resolution solely depends on linguistic and
semantic processing of referring expressions. Table
4 shows the lenient reference resolution evaluation
using F-measure. This table demonstrates that le-
nient reference resolution is improved by incorpo-
rating eye gaze information. This effect is statisti-
cally significant in the case of transcription and 1-
best (p &lt; 0.0001 and p &lt; 0.009, respectively) and
marginal (p &lt; 0.07) in the case of WCN.
</bodyText>
<table confidence="0.999136">
Configuration Without Gaze With Gaze
Transcription 0.619 0.676
WCN 0.524 0.552
1-best 0.471 0.514
</table>
<tableCaption confidence="0.972727">
Table 4: Lenient F-measure Evaluation
</tableCaption>
<table confidence="0.9997795">
Configuration Without Gaze With Gaze
Transcription 0.584 0.627
WCN 0.309 0.333
1-best 0.039 0.035
</table>
<tableCaption confidence="0.99919">
Table 5: Strict F-measure Evaluation
</tableCaption>
<bodyText confidence="0.999256090909091">
Table 5 shows the strict reference resolution eval-
uation using F-measure. As can be seen in the ta-
ble, incorporating eye gaze information significantly
(p &lt; 0.0024) improves reference resolution per-
formance when using transcription and marginally
(p &lt; 0.113) in the case of WCN optimized for strict
evaluation. However there is no difference for the 1-
best hypotheses which result in extremely low per-
formance. This observation is not surprising since 1-
best hypotheses are quite error prone and less likely
to produce the exact expressions.
</bodyText>
<page confidence="0.996925">
477
</page>
<bodyText confidence="0.999623272727273">
Since eye gaze can be used to direct navigation in
a mobile environment as in situated dialogue, there
could be situations where eye gaze does not reflect
the content of the corresponding speech. In such
situations, integrating eye gaze in reference reso-
lution could be detrimental. To further understand
the role of eye gaze in reference resolution, we ap-
plied our reference resolution algorithm only to ut-
terances where speech and eye gaze are considered
closely coupled (i.e., eye gaze reflects the content of
speech). More specifically, following the previous
work (Qu and Chai, 2010), we define a closely cou-
pled utterance as one in which at least one noun or
adjective describes an object that has been fixated by
the corresponding gaze stream.
Table 6 and Table 7 show the performance based
on closely coupled utterances using lenient and strict
evaluation, respectively. In the lenient evaluation,
reference resolution performance is significantly im-
proved for all input configurations when eye gaze
information is incorporated (p &lt; 0.0001 for tran-
scription, p &lt; 0.015 for WCN, and p &lt; 0.0022 for
1-best). In each case the closely coupled utterances
achieve higher performance than the entire set of ut-
terances evaluated in Table 5. Aside from the 1-best
case, the same is true when using strict evaluation
(p &lt; 0.0006 for transcription and p &lt; 0.046 for
WCN optimized for strict evaluation). This observa-
tion indicates that in situated dialogue, some mech-
anism to predict whether a gaze stream is closely
coupled with the corresponding speech content can
be beneficial in further improving reference resolu-
tion performance.
</bodyText>
<table confidence="0.99954725">
Configuration Without Gaze With Gaze
Transcription 0.616 0.700
WCN 0.523 0.570
1-best 0.473 0.537
</table>
<tableCaption confidence="0.992624">
Table 6: Lenient F-measure Evaluation for Closely Cou-
pled Utterances
</tableCaption>
<subsectionHeader confidence="0.999465">
6.3 Role of Word Confusion Network
</subsectionHeader>
<bodyText confidence="0.9993384">
The effect of incorporating eye gaze with WCNs
rather than 1-best recognition hypotheses into ref-
erence resolution can also be seen in Tables 4 and
5. Table 4 shows a significant improvement when
using WCNs rather than 1-best hypotheses for both
</bodyText>
<table confidence="0.99967875">
Configuration Without Gaze With Gaze
Transcription 0.579 0.644
WCN 0.307 0.345
1-best 0.045 0.038
</table>
<tableCaption confidence="0.9176925">
Table 7: Strict F-measure Evaluation for Closely Coupled
Utterances
</tableCaption>
<bodyText confidence="0.999916676470588">
with (p &lt; 0.015) and without (p &lt; 0.0012) eye
gaze configurations. Similarly, Table 5 shows a sig-
nificant improvement in strict evaluation when us-
ing WCNs rather than 1-best hypotheses for both
with (p &lt; 0.0001) and without (p &lt; 0.0001) eye
gaze configurations. These results indicate that us-
ing word confusion networks improves both lenient
and strict reference resolution. This observation is
not surprising since identifying correct linguistic ex-
pressions will enable better search for semantically
matching referent objects.
Although WCNs lead to better performance, uti-
lizing WCNs is more computationally expensive
compared to 1-best recognition hypotheses. Never-
theless, in practice, WCN depth, which specifies the
maximum number of competing word hypotheses in
any position of the word confusion network, can be
limited to a certain value |d|. For example, in Figure
2 the depth of the shown WCN is 8 (there are 8 com-
peting word hypotheses in position 17 of the WCN).
The WCN depth can be limited by pruning word al-
ternatives with low probabilities until, at most, the
top Idl words remain in each position of the WCN.
It is interesting to observe how limiting WCN depth
can affect reference resolution performance. Figure
3 demonstrates this observation. In this figure the
resolution performance (in terms of lenient evalua-
tion) for WCNs of varying depth is shown as dashed
lines for with and without eye gaze configurations.
As a reference point, the performance when utiliz-
ing 1-best recognition hypotheses is shown as solid
lines. It can be seen that as the depth increases, the
performance also increases until the depth reaches 8.
After that, there is no performance improvement.
</bodyText>
<sectionHeader confidence="0.998517" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999661666666667">
In Section 6.2 we have shown that incorporating
eye gaze information improves reference resolu-
tion performance. Eye-gaze information is particu-
</bodyText>
<page confidence="0.99817">
478
</page>
<figureCaption confidence="0.99956">
Figure 3: Lenient F-measure at each WCN Depth
</figureCaption>
<bodyText confidence="0.999981972972973">
larly helpful for resolving referring expressions that
are ambiguous from the perspective of the artificial
agent. Consider a scenario where the user utters a
referring expression that has an equivalent seman-
tic compatibility with multiple potential referent ob-
jects. For example, in a room with multiple books,
the user utters “the open book to the right”, but only
the phrase “the book” is recognized by the ASR. If
a particular book is fixated during interaction, there
is a high probability that it is indeed being referred
to by the user. Without eye gaze information, the se-
mantic compatibility alone could be insufficient to
resolve this referring expression. Thus, when eye
gaze information is incorporated, the main source of
performance improvement comes from better iden-
tification of potential referent objects.
In Section 6.3 we have shown that incorporating
multiple speech recognition hypotheses in the form
of a word confusion network further improves ref-
erence resolution performance. This is especially
true when exact referring expression identification
is required (F-measure of 0.309 from WCNs com-
pared to F-measure of 0.039 from 1-best hypothe-
ses). Using a WCN improves identification of low-
probability referring expressions. Consider a sce-
nario where the top recognition hypothesis of an ut-
terance contains no referring expressions or an in-
correct referring expression that has no semantically
compatible potential referent objects. If a referring
expression with a high compatibility value to some
potential referent object is present in a lower proba-
bility hypothesis, this referring expression can only
be identified when a WCN rather than a 1-best hy-
pothesis is utilized. Thus, when word confusion net-
works are incorporated, the main source of perfor-
mance improvement comes from better referring ex-
pression identification.
</bodyText>
<subsectionHeader confidence="0.991728">
7.1 Computational Complexity
</subsectionHeader>
<bodyText confidence="0.999930953488372">
One potential concern of using word confusion net-
works rather than 1-best hypotheses is that they are
more computationally expensive to process. The
asymptotic computational complexity for resolving
the referring expressions using the algorithm pre-
sented in this work with a WCN is the summa-
tion of three components: (1) O(|G |· |d|2 · |w|3)
for confusion network construction and parsing, (2)
O(|r|·|O|·log(|O|)) for reference resolution, and (3)
O(|r|2) for selection of (referring expression, ref-
erent object set) pairs. Here, |w |is the number of
words in the input speech signal (or, more precisely,
the number of words in the longest ASR hypothesis
for a given utterance); |G |is the size of the parsing
grammar; |d |is the depth of the constructed word
confusion network; |O |is the number of potential
referent objects for each utterance; and |r |is the
number of referring expressions that are extracted
from the word confusion network.
The complexity is dominated by the word confu-
sion network construction and parsing. Also, both
the number of words in an input utterance ASR hy-
pothesis |w |and the number of referring expressions
in a word confusion network |r |are dependent on ut-
terance length. In our study, interactive dialogue is
encouraged and, thus, utterances are typically short;
with a mean length of 6.41 words and standard de-
viation of 4.35 words. The longest utterances in our
data set has 31 words. WCN depth |d |has a mean of
10.1, a standard deviation of 8.1, and a maximum 89
words. In practice, as shown in Section 6.3, limiting
|d |to 8 words achieves comparable reference resolu-
tion results as using a full word confusion network.
To demonstrate the applicability of our reference
resolution algorithm for real-time processing, we ap-
plied it on the data corpus presented in Section 3.
This corpus contains utterances with a mean input
time of 2927.5 ms and standard deviation of 1903.8
ms. On a 2.4 GHz AMD Athlon(tm) 64 X2 Dual
Core Processor, the runtimes resulted in a real time
factor of 0.0153 on average. Thus, on average, an
utterance from this corpus can be processed in just
under 45 ms, which is well within the range of ac-
</bodyText>
<page confidence="0.997723">
479
</page>
<bodyText confidence="0.679387">
ceptable real-time performance.
</bodyText>
<subsectionHeader confidence="0.839182">
7.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99994893220339">
As can be seen in Section 6, even when using tran-
scribed data, reference resolution performance still
has room for improvement (achieving the highest le-
nient F-measure of 0.700 when eye gaze is utilized
for resolving closely coupled utterances). In this
section, we elaborate on the potential error sources.
Specifically, we discuss two types of error: (1) a re-
ferring expression is incorrectly recognized or (2) a
recognized referring expression is not resolved to a
correct referent object set.
Given transcribed data, which simulates per-
fectly recognized utterances, all referring expression
recognition errors arise due to incorrect language
processing. Most of these errors occur because an
incorrect part of speech (POS) tag is assigned to a
word, or an out-of-vocabulary (OOV) word is en-
countered, or the parsing grammar has insufficient
coverage. A particularly interesting parsing prob-
lem occurs due to the nature of spoken language.
Since punctuation is sometimes unavailable, given
an utterance with several consecutive nouns, it is un-
clear which of these nouns should be treated as head
nouns and which should be treated as noun modi-
fiers. For example, in the utterance “there is a desk
lamp table and two chairs” it is unclear if the itali-
cized expression should be parsed as a single phrase
or as a list of (two) phrases a desk and lamp.
Thus, some timing information should be used for
disambiguation.
Object set identification errors are more prevalent
than referring expression recognition errors. The
majority of these errors occur because a referring
expression is ambiguous from the perspective of the
conversational system and there is not enough in-
formation to choose amongst multiple potential ref-
erent objects due to limited speech recognition and
domain modeling. One reason for this is that a re-
ferring expression may be resolved to an incorrect
number of referent objects. Another reason is that a
pertinent object attribute or a distinguishing spatial
relationship between objects specified by the user
cannot be established by the system. For example,
during the utterance “I see a vase left of the table”
there are two vases visible on the screen creating an
ambiguity if the phrase left of is not processed
correctly. This is caused by an inadequate repre-
sentation of spatial relationships and processing of
spatial language. One more reason for potential am-
biguity is the lack of pragmatic knowledge that can
support adequate inference. For example, when the
user refers to two sofa objects using the phrase “an
armchair and a sofa”, the system lacks pragmatic
knowledge to indicate that arm chair refers to
the smaller of the two objects. Some of these errors
can be avoided when eye gaze information is avail-
able to the system. However, due to the noisy nature
of eye gaze data, many such referring expressions
remain ambiguous even when eye gaze information
is considered.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999958357142857">
In this work, we have examined the utility of eye
gaze and word confusion networks for reference res-
olution in situated dialogue within a virtual world.
Our empirical results indicate that incorporating
eye gaze information with recognition hypotheses
is beneficial for the reference resolution task com-
pared to only using recognition hypotheses. Further-
more, using a word confusion network rather than
the top best recognition hypothesis further improves
reference resolution performance. Our findings also
demonstrate that the processing speed necessary to
integrate word confusion networks with eye gaze
information is well within the acceptable range for
real-time applications.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99984425">
This work was supported by IIS-0347548 and IIS-
0535112 from the National Science Foundation. We
would like to thank anonymous reviewers for their
valuable comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998762875">
N. Bee, E. Andr´e, and S. Tober. 2009. Breaking the
ice in human-agent communication: Eye-gaze based
initiation of contact with an embodied conversational
agent. In Proceedings of the 9th International Con-
ference on Intelligent Virtual Agents (IVA’09), pages
229–242. Springer.
T. Bickmore and J. Cassell, 2004. Social Dialogue with
Embodied Conversational Agents, chapter Natural, In-
</reference>
<page confidence="0.975724">
480
</page>
<reference confidence="0.999926075268817">
telligent and Effective Interaction with Multimodal Di-
alogue Systems. Kluwer Academic.
D. K. Byron, T. Mampilly, and T. Sharma, V.and Xu.
2005. Utilizing visual attention for cross-modal coref-
erence interpretation. In Spring Lecture Notes in Com-
puter Science: Proceedings of CONTEXT-05, pages
83–96.
N. J. Cooke and M. Russell. 2008. Gaze-contingent au-
tomatic speech recognition. IET Signal Processing,
2(4):369–380, December.
J. Cooke and J. T. Schwartz. 1970. Programming lan-
guages and their compilers: Preliminary notes. Tech-
nical report, Courant Institute of Mathematical Sci-
ence.
R. Fang, J. Y. Chai, and F. Ferreira. 2009. Between lin-
guistic attention and gaze fixations in multimodal con-
versational interfaces. In The 11th International Con-
ference on Multimodal Interfaces (ICMI).
P. Gorniak, J. Orkin, and D. Roy. 2006. Speech, space
and purpose: Situated language understanding in com-
puter games. In Twenty-eighth Annual Meeting of
the Cognitive Science Society Workshop on Computer
Games.
Z. M. Griffin and K. Bock. 2000. What the eyes say
about speaking. In Psychological Science, volume 11,
pages 274–279.
D. Hakkani-T¨ur, F. B´echet, G. Riccardi, and G. Tur.
2006. Beyond asr 1-best: Using word confusion net-
works in spoken language understanding. Computer
Speech and Language, 20(4):495–514.
M. A. Just and P. A. Carpenter. 1976. Eye fixations and
cognitive processes. In Cognitive Psychology, vol-
ume 8, pages 441–480.
T. Kasami. 1965. An efficient recognition and syntax-
analysis algorithm for context-free languages. Scien-
tific report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory, Bedford, Massachusetts.
J. Kelleher and J. van Genabith. 2004. Visual salience
and reference resolution in simulated 3-d environ-
ments. Artificial Intelligence Review, 21(3).
Y. Liu, J. Y. Chai, and R. Jin. 2007. Automated vo-
cabulary acquisition and interpretation in multimodal
conversational systems. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL).
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding con-
sensus in speech recognition: word error minimization
and other applications of confusion networks. Com-
puter Speech and Language, 14(4):373–400.
A. S. Meyer and W. J. M. Levelt. 1998. Viewing and
naming objects: Eye movements during noun phrase
production. In Cognition, volume 66, pages B25–B33.
L.-P. Morency, C. M. Christoudias, and T. Darrell. 2006.
Recognizing gaze aversion gestures in embodied con-
versational discourse. In International Conference on
Multimodal Interfaces (ICMI).
Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell.
2003. Towards a model of face-to-face grounding. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL’03), pages
553–561.
Z. Prasov and J. Y. Chai. 2008. What’s in a gaze? the
role of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of 13th In-
ternational Conference on Intelligent User interfaces
(IUI), pages 20–29.
S. Qu and J. Y. Chai. 2007. An exploration of eye gaze in
spoken language processing for multimodal conversa-
tional interfaces. In Proceedings of the Conference of
the North America Chapter of the Association of Com-
putational Linguistics (NAACL).
S. Qu and J. Y. Chai. 2010. Context-based word acquisi-
tion for situated dialogue in a virtual world. Journal of
Artificial Intelligence Research, 37:347–377, March.
P. Qvarfordt and S. Zhai. 2005. Conversing with the
user based on eye-gaze patterns. In Proceedings Of the
Conference on Human Factors in Computing Systems.
ACM.
C. L. Sidner, C. D. Kidd, C. Lee, and N. Lesh. 2004.
Where to look: A study of human-robot engagement.
In Proceedings of the 9th international conference
on Intelligent User Interfaces (IUI’04), pages 78–84.
ACM Press.
A. Stolcke. 2002. SRILM an extensible language model-
ing toolkit, confusion network. In International Con-
ference on Spoken Language Processing.
M. K. Tanenhous, M. Spivey-Knowlton, E. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguistic
information during spoken language comprehension.
In Science, volume 268, pages 1632–1634.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.
</reference>
<page confidence="0.998707">
481
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892539">
<title confidence="0.9960415">Fusing Eye Gaze with Speech Recognition Hypotheses Resolve Exophoric References in Situated Dialogue</title>
<author confidence="0.991992">Zahar Prasov</author>
<author confidence="0.991992">Y Joyce</author>
<affiliation confidence="0.9705405">Department of Computer Science and Michigan State</affiliation>
<address confidence="0.977596">East Lansing, MI 48824,</address>
<abstract confidence="0.99894684">In situated dialogue humans often utter linguistic expressions that refer to extralinguistic entities in the environment. Correctly resolving these references is critical yet challenging for artificial agents partly due to their limited speech recognition and language understanding capabilities. Motivated by psycholinguistic studies demonstrating a tight link between language production and human eye gaze, we have developed approaches that integrate naturally occurring human eye gaze with speech recognition hypotheses to resolve exophoric references in situated dialogue in a virtual world. In addition to incorporating eye gaze with the best recognized spoken hypothesis, we developed an algorithm to also handle multiple hypotheses modeled as word confusion networks. Our empirical results demonstrate that incorporating eye gaze with recognition hypotheses consistently outperforms the results obtained from processing recognition hypotheses alone. Incorporating eye gaze with word confusion networks further improves performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bee</author>
<author>E Andr´e</author>
<author>S Tober</author>
</authors>
<title>Breaking the ice in human-agent communication: Eye-gaze based initiation of contact with an embodied conversational agent.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Virtual Agents (IVA’09),</booktitle>
<pages>229--242</pages>
<publisher>Springer.</publisher>
<marker>Bee, Andr´e, Tober, 2009</marker>
<rawString>N. Bee, E. Andr´e, and S. Tober. 2009. Breaking the ice in human-agent communication: Eye-gaze based initiation of contact with an embodied conversational agent. In Proceedings of the 9th International Conference on Intelligent Virtual Agents (IVA’09), pages 229–242. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bickmore</author>
<author>J Cassell</author>
</authors>
<title>Social Dialogue with Embodied Conversational Agents, chapter Natural, Intelligent and Effective Interaction with Multimodal Dialogue Systems.</title>
<date>2004</date>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="5498" citStr="Bickmore and Cassell, 2004" startWordPosition="810" endWordPosition="813">olve references. In contrast to this line of research, here we explore the use of human eye gaze during real-time interaction to model attention and facilitate reference resolution. Eye gaze provides a richer medium for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorpo</context>
</contexts>
<marker>Bickmore, Cassell, 2004</marker>
<rawString>T. Bickmore and J. Cassell, 2004. Social Dialogue with Embodied Conversational Agents, chapter Natural, Intelligent and Effective Interaction with Multimodal Dialogue Systems. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Byron</author>
<author>T Mampilly</author>
<author>T Sharma</author>
<author>V and Xu</author>
</authors>
<title>Utilizing visual attention for cross-modal coreference interpretation.</title>
<date>2005</date>
<booktitle>In Spring Lecture Notes in Computer Science: Proceedings of CONTEXT-05,</booktitle>
<pages>83--96</pages>
<contexts>
<context position="1989" citStr="Byron et al., 2005" startWordPosition="275" endWordPosition="278">ications for tutoring and training, video games and simulations, and assistive technology, enabling situated dialogue in virtual worlds has become increasingly important. Situated dialogue allows human users to navigate in a spatially rich environment and carry a conversation with artificial agents to achieve specific tasks pertinent to the environment. Different from traditional telephony-based spoken dialogue systems and multimodal conversational interfaces, situated dialogue supports immersion and mobility in a visually rich environment and encourages social and collaborative language use (Byron et al., 2005; Gorniak et al., 2006). In situated dialogue, human users often need to make linguistic references, known as exophoric referring expressions (e.g., the book to the right), to extralinguistic entities in the environment. Reliably resolving these references is critical for dialogue success. However, reference resolution remains a challenging problem, partly due to limited speech and language processing capabilities caused by poor speech recognition (ASR), ambiguous language, and insufficient pragmatic knowledge. To address this problem, motivated by psycholinguistic studies demonstrating a clos</context>
<context position="4750" citStr="Byron et al., 2005" startWordPosition="692" endWordPosition="695">ently outperform the results obtained from processing recognition hypotheses alone. In addition, incorporating eye gaze with word confusion networks further improves performance. Our analysis also indicates that, although a word confusion network appears to be more complicated, the time complexity of its integration with eye gaze is well within the acceptable range for real-time applications. 2 Related Work Prior work in reference resolution within situated dialogue has focused on using visual context to assist reference resolution during interaction. In (Kelleher and van Genabith, 2004) and (Byron et al., 2005), visual features of objects are used to model the focus of attention. This attention modeling is subsequently used to resolve references. In contrast to this line of research, here we explore the use of human eye gaze during real-time interaction to model attention and facilitate reference resolution. Eye gaze provides a richer medium for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as </context>
</contexts>
<marker>Byron, Mampilly, Sharma, Xu, 2005</marker>
<rawString>D. K. Byron, T. Mampilly, and T. Sharma, V.and Xu. 2005. Utilizing visual attention for cross-modal coreference interpretation. In Spring Lecture Notes in Computer Science: Proceedings of CONTEXT-05, pages 83–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Cooke</author>
<author>M Russell</author>
</authors>
<title>Gaze-contingent automatic speech recognition.</title>
<date>2008</date>
<journal>IET Signal Processing,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="5729" citStr="Cooke and Russell, 2008" startWordPosition="847" endWordPosition="850">nformation, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008).</context>
</contexts>
<marker>Cooke, Russell, 2008</marker>
<rawString>N. J. Cooke and M. Russell. 2008. Gaze-contingent automatic speech recognition. IET Signal Processing, 2(4):369–380, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cooke</author>
<author>J T Schwartz</author>
</authors>
<title>Programming languages and their compilers: Preliminary notes.</title>
<date>1970</date>
<tech>Technical report,</tech>
<institution>Courant Institute of Mathematical Science.</institution>
<contexts>
<context position="16285" citStr="Cooke and Schwartz, 1970" startWordPosition="2578" endWordPosition="2581">ctual posterior probability of each utterance hypothesis (which was not available), we assigned each utterance hypothesis a probability based on its position in the ranked list. Figure 2 depicts a portion of the resulting word confusion network (showing competing word hypotheses and their probabilities in log scale) constructed from the n-best list in Table 2. 474 Figure 2: Sample parallel speech and eye gaze data streams, including a portion of the sample WCN Step 2: extract referring expressions from WCN The word confusion network is syntactically parsed using a modified version of the CYK (Cooke and Schwartz, 1970; Kasami, 1965; Younger, 1967) parsing algorithm that is capable of taking a word confusion network as input rather than a single string. We call this the CYK-WCN algorithm. To do the parsing, we applied a set of grammar rules largely derived from a different domain in our previous work (Prasov and Chai, 2008). A parse chart of the sample word confusion network is shown in Table 3. Here, just as in the CYK algorithm the chart is filled in from left to right then bottom to top. The difference is that the chart has an added dimension for competing word hypotheses. This is demonstrated in positio</context>
</contexts>
<marker>Cooke, Schwartz, 1970</marker>
<rawString>J. Cooke and J. T. Schwartz. 1970. Programming languages and their compilers: Preliminary notes. Technical report, Courant Institute of Mathematical Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fang</author>
<author>J Y Chai</author>
<author>F Ferreira</author>
</authors>
<title>Between linguistic attention and gaze fixations in multimodal conversational interfaces.</title>
<date>2009</date>
<booktitle>In The 11th International Conference on Multimodal Interfaces (ICMI).</booktitle>
<contexts>
<context position="5869" citStr="Fang et al., 2009" startWordPosition="870" endWordPosition="873">anguage processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and evaluated only on transcribed spoken utterances. In situated dialogue, e</context>
</contexts>
<marker>Fang, Chai, Ferreira, 2009</marker>
<rawString>R. Fang, J. Y. Chai, and F. Ferreira. 2009. Between linguistic attention and gaze fixations in multimodal conversational interfaces. In The 11th International Conference on Multimodal Interfaces (ICMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gorniak</author>
<author>J Orkin</author>
<author>D Roy</author>
</authors>
<title>Speech, space and purpose: Situated language understanding in computer games.</title>
<date>2006</date>
<booktitle>In Twenty-eighth Annual Meeting of the Cognitive Science Society Workshop on Computer Games.</booktitle>
<contexts>
<context position="2012" citStr="Gorniak et al., 2006" startWordPosition="279" endWordPosition="282">g and training, video games and simulations, and assistive technology, enabling situated dialogue in virtual worlds has become increasingly important. Situated dialogue allows human users to navigate in a spatially rich environment and carry a conversation with artificial agents to achieve specific tasks pertinent to the environment. Different from traditional telephony-based spoken dialogue systems and multimodal conversational interfaces, situated dialogue supports immersion and mobility in a visually rich environment and encourages social and collaborative language use (Byron et al., 2005; Gorniak et al., 2006). In situated dialogue, human users often need to make linguistic references, known as exophoric referring expressions (e.g., the book to the right), to extralinguistic entities in the environment. Reliably resolving these references is critical for dialogue success. However, reference resolution remains a challenging problem, partly due to limited speech and language processing capabilities caused by poor speech recognition (ASR), ambiguous language, and insufficient pragmatic knowledge. To address this problem, motivated by psycholinguistic studies demonstrating a close relationship between </context>
</contexts>
<marker>Gorniak, Orkin, Roy, 2006</marker>
<rawString>P. Gorniak, J. Orkin, and D. Roy. 2006. Speech, space and purpose: Situated language understanding in computer games. In Twenty-eighth Annual Meeting of the Cognitive Science Society Workshop on Computer Games.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z M Griffin</author>
<author>K Bock</author>
</authors>
<title>What the eyes say about speaking.</title>
<date>2000</date>
<booktitle>In Psychological Science,</booktitle>
<volume>11</volume>
<pages>274--279</pages>
<contexts>
<context position="6074" citStr="Griffin and Bock, 2000" startWordPosition="901" endWordPosition="904">Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and evaluated only on transcribed spoken utterances. In situated dialogue, eye gaze behavior is much more complex. Here, gaze fixations may be made for the purpose of navigation or scanning the environment rather than referring to a particular object. Referring expressions can be </context>
</contexts>
<marker>Griffin, Bock, 2000</marker>
<rawString>Z. M. Griffin and K. Bock. 2000. What the eyes say about speaking. In Psychological Science, volume 11, pages 274–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hakkani-T¨ur</author>
<author>F B´echet</author>
<author>G Riccardi</author>
<author>G Tur</author>
</authors>
<title>Beyond asr 1-best: Using word confusion networks in spoken language understanding.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Hakkani-T¨ur, B´echet, Riccardi, Tur, 2006</marker>
<rawString>D. Hakkani-T¨ur, F. B´echet, G. Riccardi, and G. Tur. 2006. Beyond asr 1-best: Using word confusion networks in spoken language understanding. Computer Speech and Language, 20(4):495–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Just</author>
<author>P A Carpenter</author>
</authors>
<title>Eye fixations and cognitive processes.</title>
<date>1976</date>
<booktitle>In Cognitive Psychology,</booktitle>
<volume>8</volume>
<pages>441--480</pages>
<contexts>
<context position="6001" citStr="Just and Carpenter, 1976" startWordPosition="889" endWordPosition="892">l information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and evaluated only on transcribed spoken utterances. In situated dialogue, eye gaze behavior is much more complex. Here, gaze fixations may be made for the purpose of navigation or scanning the environment ra</context>
</contexts>
<marker>Just, Carpenter, 1976</marker>
<rawString>M. A. Just and P. A. Carpenter. 1976. Eye fixations and cognitive processes. In Cognitive Psychology, volume 8, pages 441–480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntaxanalysis algorithm for context-free languages. Scientific report AFCRL-65-758, Air Force Cambridge Research Laboratory,</title>
<date>1965</date>
<location>Bedford, Massachusetts.</location>
<contexts>
<context position="16299" citStr="Kasami, 1965" startWordPosition="2582" endWordPosition="2583">y of each utterance hypothesis (which was not available), we assigned each utterance hypothesis a probability based on its position in the ranked list. Figure 2 depicts a portion of the resulting word confusion network (showing competing word hypotheses and their probabilities in log scale) constructed from the n-best list in Table 2. 474 Figure 2: Sample parallel speech and eye gaze data streams, including a portion of the sample WCN Step 2: extract referring expressions from WCN The word confusion network is syntactically parsed using a modified version of the CYK (Cooke and Schwartz, 1970; Kasami, 1965; Younger, 1967) parsing algorithm that is capable of taking a word confusion network as input rather than a single string. We call this the CYK-WCN algorithm. To do the parsing, we applied a set of grammar rules largely derived from a different domain in our previous work (Prasov and Chai, 2008). A parse chart of the sample word confusion network is shown in Table 3. Here, just as in the CYK algorithm the chart is filled in from left to right then bottom to top. The difference is that the chart has an added dimension for competing word hypotheses. This is demonstrated in position 15 of the WC</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>T. Kasami. 1965. An efficient recognition and syntaxanalysis algorithm for context-free languages. Scientific report AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kelleher</author>
<author>J van Genabith</author>
</authors>
<title>Visual salience and reference resolution in simulated 3-d environments.</title>
<date>2004</date>
<journal>Artificial Intelligence Review,</journal>
<volume>21</volume>
<issue>3</issue>
<marker>Kelleher, van Genabith, 2004</marker>
<rawString>J. Kelleher and J. van Genabith. 2004. Visual salience and reference resolution in simulated 3-d environments. Artificial Intelligence Review, 21(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>J Y Chai</author>
<author>R Jin</author>
</authors>
<title>Automated vocabulary acquisition and interpretation in multimodal conversational systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5781" citStr="Liu et al., 2007" startWordPosition="855" endWordPosition="858">ignal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual </context>
</contexts>
<marker>Liu, Chai, Jin, 2007</marker>
<rawString>Y. Liu, J. Y. Chai, and R. Jin. 2007. Automated vocabulary acquisition and interpretation in multimodal conversational systems. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
<author>A Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="7294" citStr="Mangu et al., 2000" startWordPosition="1095" endWordPosition="1098">ade to objects that are not in the user’s field of view, but were previously visible on the interface. Additionally, users may make egocentric spatial references (e.g. “the chair on the left”) which require contextual knowledge (e.g. the users position in the environment) in order to resolve. Therefore, the focus of our work here is on exploring these complex user behaviors in situated dialogue and examining how to combine eye gaze with ASR hypotheses for improved reference resolution. Alternative ASR hypotheses have been used in many different ways in speech driven systems. Particularly, in (Mangu et al., 2000) multiple lattice alignment is used for construction of word confusion networks and in (Hakkani-T¨ur et al., 2006) word confusion networks are used for named entity detection. In the study presented here, we apply word confusion networks (to represent ASR hypotheses) along with eye gaze to the problem of reference resolution. 3 Data Collection In this investigation, we created a 3D virtual world (using the Irrlicht game engine1) to support situated dialogue. We conducted a Wizard of Oz study in which the user must collaborate with a remote artificial agent cohort (controlled by a human) to sol</context>
<context position="12386" citStr="Mangu et al., 2000" startWordPosition="1952" endWordPosition="1955">virtual world. However, not until the twenty fifth ranked recognition hypothesis H25, do we see a referring expression closest to the actual uttered referring expression. Moreover, in utterances with multiple referring expressions, there may not be a single recognition hypothesis that contains all referring expressions, but each referring expression may be contained in some recognition hypothesis. Thus, it is desirable to consider the entire n-best list of hypotheses. To address this issue, we adopted the word confusion network (WCN): a compact representation of a word lattice or n-best list (Mangu et al., 2000). A WCN captures alternative word hypotheses and their corresponding posterior probabilities in time-ordered sets. In addition to being compact, an important feature for efficient post-processing of recognition hypotheses for real-time systems, WCNs are capable of representing more competing hypotheses than either n-best lists or word lattices. Figure 2 shows an example of a WCN for the utterance “... I see one long sword” along with a timeline (in milliseconds) depicting the eye gaze fixations to potential referent objects that correspond to the utterance. The confusion network shows competin</context>
<context position="15234" citStr="Mangu et al., 2000" startWordPosition="2414" endWordPosition="2417">nting task people typically only speak about objects that are visible or have recently been visible on the screen, an object is considered to be a potential referent if it is present within a close proximity (in the same room) of the user while an utterance is spoken. The multimodal reference resolution algorithm proceeds with the following four steps: Step 1: construct word confusion network A word confusion network is constructed out of the input n-best list of alternative recognition hypotheses with the SRI Language Modeling (SRILM) toolkit (Stolcke, 2002) using the procedure described in (Mangu et al., 2000). This procedure aligns words from the n-best list into equivalence classes. First, instances of the same word containing approximately the same starting and ending timestamps are clustered. Then, equivalence classes with common time ranges are merged. For each competing word hypothesis its probability is computed by summing the posteriors of all utterance hypotheses containing this word. In our work, instead of using the actual posterior probability of each utterance hypothesis (which was not available), we assigned each utterance hypothesis a probability based on its position in the ranked l</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus in speech recognition: word error minimization and other applications of confusion networks. Computer Speech and Language, 14(4):373–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Meyer</author>
<author>W J M Levelt</author>
</authors>
<title>Viewing and naming objects: Eye movements during noun phrase production.</title>
<date>1998</date>
<booktitle>In Cognition,</booktitle>
<volume>66</volume>
<pages>25--33</pages>
<contexts>
<context position="6049" citStr="Meyer and Levelt, 1998" startWordPosition="897" endWordPosition="900"> taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and evaluated only on transcribed spoken utterances. In situated dialogue, eye gaze behavior is much more complex. Here, gaze fixations may be made for the purpose of navigation or scanning the environment rather than referring to a particular object. Refe</context>
</contexts>
<marker>Meyer, Levelt, 1998</marker>
<rawString>A. S. Meyer and W. J. M. Levelt. 1998. Viewing and naming objects: Eye movements during noun phrase production. In Cognition, volume 66, pages B25–B33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L-P Morency</author>
<author>C M Christoudias</author>
<author>T Darrell</author>
</authors>
<title>Recognizing gaze aversion gestures in embodied conversational discourse.</title>
<date>2006</date>
<booktitle>In International Conference on Multimodal Interfaces (ICMI).</booktitle>
<contexts>
<context position="5541" citStr="Morency et al., 2006" startWordPosition="818" endWordPosition="821">arch, here we explore the use of human eye gaze during real-time interaction to model attention and facilitate reference resolution. Eye gaze provides a richer medium for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. O</context>
</contexts>
<marker>Morency, Christoudias, Darrell, 2006</marker>
<rawString>L.-P. Morency, C. M. Christoudias, and T. Darrell. 2006. Recognizing gaze aversion gestures in embodied conversational discourse. In International Conference on Multimodal Interfaces (ICMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y I Nakano</author>
<author>G Reinstein</author>
<author>T Stocky</author>
<author>J Cassell</author>
</authors>
<title>Towards a model of face-to-face grounding.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL’03),</booktitle>
<pages>553--561</pages>
<contexts>
<context position="5470" citStr="Nakano et al., 2003" startWordPosition="806" endWordPosition="809">sequently used to resolve references. In contrast to this line of research, here we explore the use of human eye gaze during real-time interaction to model attention and facilitate reference resolution. Eye gaze provides a richer medium for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2</context>
</contexts>
<marker>Nakano, Reinstein, Stocky, Cassell, 2003</marker>
<rawString>Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell. 2003. Towards a model of face-to-face grounding. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL’03), pages 553–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Prasov</author>
<author>J Y Chai</author>
</authors>
<title>What’s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces.</title>
<date>2008</date>
<booktitle>In Proceedings of 13th International Conference on Intelligent User interfaces (IUI),</booktitle>
<pages>20--29</pages>
<contexts>
<context position="2757" citStr="Prasov and Chai, 2008" startWordPosition="383" endWordPosition="386">., the book to the right), to extralinguistic entities in the environment. Reliably resolving these references is critical for dialogue success. However, reference resolution remains a challenging problem, partly due to limited speech and language processing capabilities caused by poor speech recognition (ASR), ambiguous language, and insufficient pragmatic knowledge. To address this problem, motivated by psycholinguistic studies demonstrating a close relationship between language production and eye gaze, our previous work has incorporated naturally occurring eye gaze in reference resolution (Prasov and Chai, 2008). Our findings have shown that eye gaze can partially compensate for limited language processing and domain modeling. However, this work was conducted in a setting where users only spoke to a static visual interface. In situated dialogue, human speech and eye gaze patterns are much more complex. The dynamic nature of the environment and the complexity of spatially rich tasks have a massive influence on what the user will look at and say. It is not clear to what degree prior findings can generalize to situated dialogue. Therefore, this paper explores new studies on incorporating eye gaze for ex</context>
<context position="6328" citStr="Prasov and Chai, 2008" startWordPosition="938" endWordPosition="941">ooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and evaluated only on transcribed spoken utterances. In situated dialogue, eye gaze behavior is much more complex. Here, gaze fixations may be made for the purpose of navigation or scanning the environment rather than referring to a particular object. Referring expressions can be made to objects that are not in the user’s field of view, but were previously visible on the interface. Additionally, users may make egocentric spatial references (e.g. “the chair on the left”) which require contextual knowledge (e.g. the users position </context>
<context position="16596" citStr="Prasov and Chai, 2008" startWordPosition="2632" endWordPosition="2635">ale) constructed from the n-best list in Table 2. 474 Figure 2: Sample parallel speech and eye gaze data streams, including a portion of the sample WCN Step 2: extract referring expressions from WCN The word confusion network is syntactically parsed using a modified version of the CYK (Cooke and Schwartz, 1970; Kasami, 1965; Younger, 1967) parsing algorithm that is capable of taking a word confusion network as input rather than a single string. We call this the CYK-WCN algorithm. To do the parsing, we applied a set of grammar rules largely derived from a different domain in our previous work (Prasov and Chai, 2008). A parse chart of the sample word confusion network is shown in Table 3. Here, just as in the CYK algorithm the chart is filled in from left to right then bottom to top. The difference is that the chart has an added dimension for competing word hypotheses. This is demonstrated in position 15 of the WCN, where one and wine are two nouns that constitute competing words. Note that some words from the confusion network are not in the chart (e.g. winds) because they are out of vocabulary. The result of the syntactic parsing is that the parts of speech of all sub-phrases in the confusion network ar</context>
<context position="18721" citStr="Prasov and Chai, 2008" startWordPosition="3024" endWordPosition="3027">mpat(oi, rj)1−α — EAS(oi)α x Compat(oi, rj)1−α i (1) In this equation, • AS: Attentional salience score of a particu475 length (1) N → wine, NP → N (2) NUM → one 15 Adj-NP → ADJ N, NP → Adj-NP ADJ → long 16 N → sword, NP → N 17 ... 5 4 NP → NUM Adj-NP 3 2 1 14 ... ... 18 WCN position Table 3: Syntactic parsing of word confusion network lar object oi, which is determined based on the gaze fixation intensity of an object at the start time of referring expression rj. The fixation intensity of an object is defined as the amount of time that the object is fixated during a predefined time window W (Prasov and Chai, 2008). As in (Prasov and Chai, 2008), we set W = [−1500..0] ms relative to the beginning of referring expression rj. • Compat: Compatibility score, which specifies whether the object oi is compatible with the information specified by the referring expression rj. Currently, the compatibility score is set to 1 if referring expression rj and object oi have the same object type (e.g. chair), and 0 otherwise. • a: Importance weight, in the range [0..1], of attentional salience relative to compatibility. A high a value indicates that the attentional salience score based on eye gaze carries more weight in</context>
<context position="21516" citStr="Prasov and Chai, 2008" startWordPosition="3485" endWordPosition="3488">rlaps with a higher confidence pair. For example, in Table 3, the referring expressions one long sword and wine overlap in position 15. Finally, the resulting (referring expression, referent object set) pairs are sorted in ascending order according to their constituent referring expression timestamps. 6 Experimental Results Using our data, described in Section 3, we applied the multimodal reference resolution algorithm described in Section 5. All of the data is used to report the experimental results. Reference resolution model parameters are set based on our prior work in a different domain (Prasov and Chai, 2008). For each utterance we compare the reference resolution performance with and without the integration of eye gaze information. We also evaluate using a 476 word confusion network compared to a 1-best list to this case, a referring expression from the sysmodel speech recognition hypotheses. For perspec- tem output needs to exactly match the corretive, reference resolution with recognized speech in- sponding expression from the gold standard. put is compared with transcribed speech. 6.1 Evaluation Metrics The reference resolution algorithm outputs a list of (referring expression, referent object</context>
</contexts>
<marker>Prasov, Chai, 2008</marker>
<rawString>Z. Prasov and J. Y. Chai. 2008. What’s in a gaze? the role of eye-gaze in reference resolution in multimodal conversational interfaces. In Proceedings of 13th International Conference on Intelligent User interfaces (IUI), pages 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Qu</author>
<author>J Y Chai</author>
</authors>
<title>An exploration of eye gaze in spoken language processing for multimodal conversational interfaces.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the North America Chapter of the Association of Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="5703" citStr="Qu and Chai, 2007" startWordPosition="843" endWordPosition="846">m for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capabilit</context>
</contexts>
<marker>Qu, Chai, 2007</marker>
<rawString>S. Qu and J. Y. Chai. 2007. An exploration of eye gaze in spoken language processing for multimodal conversational interfaces. In Proceedings of the Conference of the North America Chapter of the Association of Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Qu</author>
<author>J Y Chai</author>
</authors>
<title>Context-based word acquisition for situated dialogue in a virtual world.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<contexts>
<context position="5801" citStr="Qu and Chai, 2010" startWordPosition="859" endWordPosition="862">s been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and eval</context>
<context position="26158" citStr="Qu and Chai, 2010" startWordPosition="4210" endWordPosition="4213">ce the exact expressions. 477 Since eye gaze can be used to direct navigation in a mobile environment as in situated dialogue, there could be situations where eye gaze does not reflect the content of the corresponding speech. In such situations, integrating eye gaze in reference resolution could be detrimental. To further understand the role of eye gaze in reference resolution, we applied our reference resolution algorithm only to utterances where speech and eye gaze are considered closely coupled (i.e., eye gaze reflects the content of speech). More specifically, following the previous work (Qu and Chai, 2010), we define a closely coupled utterance as one in which at least one noun or adjective describes an object that has been fixated by the corresponding gaze stream. Table 6 and Table 7 show the performance based on closely coupled utterances using lenient and strict evaluation, respectively. In the lenient evaluation, reference resolution performance is significantly improved for all input configurations when eye gaze information is incorporated (p &lt; 0.0001 for transcription, p &lt; 0.015 for WCN, and p &lt; 0.0022 for 1-best). In each case the closely coupled utterances achieve higher performance tha</context>
</contexts>
<marker>Qu, Chai, 2010</marker>
<rawString>S. Qu and J. Y. Chai. 2010. Context-based word acquisition for situated dialogue in a virtual world. Journal of Artificial Intelligence Research, 37:347–377, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Qvarfordt</author>
<author>S Zhai</author>
</authors>
<title>Conversing with the user based on eye-gaze patterns.</title>
<date>2005</date>
<booktitle>In Proceedings Of the Conference on Human Factors in Computing Systems.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="5849" citStr="Qvarfordt and Zhai, 2005" startWordPosition="866" endWordPosition="869">nversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and evaluated only on transcribed spoken utterances. In </context>
</contexts>
<marker>Qvarfordt, Zhai, 2005</marker>
<rawString>P. Qvarfordt and S. Zhai. 2005. Conversing with the user based on eye-gaze patterns. In Proceedings Of the Conference on Human Factors in Computing Systems. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Sidner</author>
<author>C D Kidd</author>
<author>C Lee</author>
<author>N Lesh</author>
</authors>
<title>Where to look: A study of human-robot engagement.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th international conference on Intelligent User Interfaces (IUI’04),</booktitle>
<pages>78--84</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="5519" citStr="Sidner et al., 2004" startWordPosition="814" endWordPosition="817"> to this line of research, here we explore the use of human eye gaze during real-time interaction to model attention and facilitate reference resolution. Eye gaze provides a richer medium for attentional information, but requires processing of a potentially noisy signal. Eye gaze has been used to facilitate human machine conversation and automated language processing. For example, eye gaze has been studied in embodied conversational discourse as a mechanism to gather visual information, aid in thinking, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into r</context>
</contexts>
<marker>Sidner, Kidd, Lee, Lesh, 2004</marker>
<rawString>C. L. Sidner, C. D. Kidd, C. Lee, and N. Lesh. 2004. Where to look: A study of human-robot engagement. In Proceedings of the 9th international conference on Intelligent User Interfaces (IUI’04), pages 78–84. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM an extensible language modeling toolkit, confusion network.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="15180" citStr="Stolcke, 2002" startWordPosition="2407" endWordPosition="2408">al referent objects. Since during the treasure hunting task people typically only speak about objects that are visible or have recently been visible on the screen, an object is considered to be a potential referent if it is present within a close proximity (in the same room) of the user while an utterance is spoken. The multimodal reference resolution algorithm proceeds with the following four steps: Step 1: construct word confusion network A word confusion network is constructed out of the input n-best list of alternative recognition hypotheses with the SRI Language Modeling (SRILM) toolkit (Stolcke, 2002) using the procedure described in (Mangu et al., 2000). This procedure aligns words from the n-best list into equivalence classes. First, instances of the same word containing approximately the same starting and ending timestamps are clustered. Then, equivalence classes with common time ranges are merged. For each competing word hypothesis its probability is computed by summing the posteriors of all utterance hypotheses containing this word. In our work, instead of using the actual posterior probability of each utterance hypothesis (which was not available), we assigned each utterance hypothes</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM an extensible language modeling toolkit, confusion network. In International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Tanenhous</author>
<author>M Spivey-Knowlton</author>
<author>E Eberhard</author>
<author>J Sedivy</author>
</authors>
<title>Integration of visual and linguistic information during spoken language comprehension.</title>
<date>1995</date>
<journal>In Science,</journal>
<volume>268</volume>
<pages>1632--1634</pages>
<contexts>
<context position="6025" citStr="Tanenhous et al., 1995" startWordPosition="893" endWordPosition="896">king, or facilitate turn taking and engagement (Nakano et al., 2003; Bickmore and Cassell, 2004; Sidner et al., 2004; Morency et al., 2006; Bee et al., 2009). Recent work has explored incorporating eye gaze into automated language understanding such as automated speech recognition (Qu and Chai, 2007; Cooke and Russell, 2008), automated vocabulary acquisition (Liu et al., 2007; Qu and Chai, 2010), attention prediction (Qvarfordt and Zhai, 2005; Fang et al., 2009). Motivated by previous psycholinguistic findings that eye gaze is tightly linked with language processing (Just and Carpenter, 1976; Tanenhous et al., 1995; Meyer and Levelt, 1998; Griffin and Bock, 2000), our prior work incorporates eye gaze into reference resolution. Our results demonstrate that such use of eye gaze can potentially compensate for a conversational systems limited language processing and domain modeling capability (Prasov and Chai, 2008). However, this work is conducted in a static visual environment and evaluated only on transcribed spoken utterances. In situated dialogue, eye gaze behavior is much more complex. Here, gaze fixations may be made for the purpose of navigation or scanning the environment rather than referring to a</context>
</contexts>
<marker>Tanenhous, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>M. K. Tanenhous, M. Spivey-Knowlton, E. Eberhard, and J. Sedivy. 1995. Integration of visual and linguistic information during spoken language comprehension. In Science, volume 268, pages 1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="16315" citStr="Younger, 1967" startWordPosition="2584" endWordPosition="2585">rance hypothesis (which was not available), we assigned each utterance hypothesis a probability based on its position in the ranked list. Figure 2 depicts a portion of the resulting word confusion network (showing competing word hypotheses and their probabilities in log scale) constructed from the n-best list in Table 2. 474 Figure 2: Sample parallel speech and eye gaze data streams, including a portion of the sample WCN Step 2: extract referring expressions from WCN The word confusion network is syntactically parsed using a modified version of the CYK (Cooke and Schwartz, 1970; Kasami, 1965; Younger, 1967) parsing algorithm that is capable of taking a word confusion network as input rather than a single string. We call this the CYK-WCN algorithm. To do the parsing, we applied a set of grammar rules largely derived from a different domain in our previous work (Prasov and Chai, 2008). A parse chart of the sample word confusion network is shown in Table 3. Here, just as in the CYK algorithm the chart is filled in from left to right then bottom to top. The difference is that the chart has an added dimension for competing word hypotheses. This is demonstrated in position 15 of the WCN, where one and</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>