<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.061506">
<title confidence="0.9981605">
Recent Improvements in the
CMU Large Scale Chinese-English SMT System
</title>
<author confidence="0.9584305">
Almut Silja Hildebrand, Kay Rottmann, Mohamed Noamany, Qin Gao,
Sanjika Hewavitharana, Nguyen Bach and Stephan Vogel
</author>
<affiliation confidence="0.927667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.979798">
silja, kayrm, mfn, qing, sanjika, nbach, vogel+@cs.cmu.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.977198875">
In this paper we describe recent improvements
to components and methods used in our statis-
tical machine translation system for Chinese-
English used in the January 2008 GALE eval-
uation. Main improvements are results of
consistent data processing, larger statistical
models and a POS-based word reordering ap-
proach.
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999841421052632">
Building a full scale Statistical Machine Transla-
tion (SMT) system involves many preparation and
training steps and it consists of several components,
each of which contribute to the overall system per-
formance. Between 2007 and 2008 our system im-
proved by 5 points in BLEU from 26.60 to 31.85
for the unseen MT06 test set, which can be mainly
attributed to two major points.
The fast growth of computing resources over
the years make it possible to use larger and larger
amounts of data in training. In Section 3 we show
how parallelizing model training can reduce training
time by an order of magnitude and how using larger
training data as well as more extensive models im-
prove translation quality.
Word reordering is still a difficult problem in
SMT. In Section 4 we apply a Part Of Speech (POS)
based syntactic reordering model successfully to our
large Chinese system.
</bodyText>
<subsectionHeader confidence="0.863844">
1.1 Decoder
</subsectionHeader>
<bodyText confidence="0.998469">
Our translation system is based on the CMU
SMT decoder as described in (Hewavitharana et
</bodyText>
<page confidence="0.978082">
77
</page>
<bodyText confidence="0.999762625">
al., 2005). Our decoder is a phrase-based beam
search decoder, which combines multiple models
e.g. phrase tables, several language models, a dis-
tortion model ect. in a log-linear fashion. In order
to find an optimal set of weights, we use MER train-
ing as described in (Venugopal et al., 2005), which
uses rescoring of the top n hypotheses to maximize
an evaluation metric like BLEU or TER.
</bodyText>
<subsectionHeader confidence="0.959841">
1.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999272714285714">
In this paper we report results using the BLEU met-
ric (Papineni et al., 2002), however as the evaluation
criterion in GALE is HTER (Snover et al., 2006), we
also report in TER (Snover et al., 2005).
We used the test sets from the NIST MT evalua-
tions from the years 2003 and 2006 as development
and unseen test data.
</bodyText>
<subsectionHeader confidence="0.998914">
1.3 Training Data
</subsectionHeader>
<bodyText confidence="0.999959615384615">
In translation model training we used the Chinese-
English bilingual corpora relevant to GALE avail-
able through the LDC1. After sentence alignment
these sources add up to 10.7 million sentences with
301 million running words on the English side. Our
preprocessing steps include tokenization on the En-
glish side and for Chinese: automatic word segmen-
tation using the revised version of the Stanford Chi-
nese Word Segmenter2 (Tseng et al., 2005) from
2007, replacement of traditional by simplified Chi-
nese characters and 2-byte to 1-byte ASCII charac-
ter normalization. After data cleaning steps like e.g.
removal of sentence pairs with very unbalanced sen-
</bodyText>
<footnote confidence="0.999952">
1http://projects.ldc.upenn.edu/gale/data/catalog.html
2http://nlp.stanford.edu/software/segmenter.shtml
</footnote>
<note confidence="0.543134">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 77–80,
</note>
<page confidence="0.502451">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.971693666666667">
tence length etc., we used the remaining 10 million
sentences with 260 million words (English) in trans-
lation model training (260M system).
</bodyText>
<sectionHeader confidence="0.919963" genericHeader="method">
2 Number Tagging
</sectionHeader>
<bodyText confidence="0.9992274">
Systematic tagging and pre-translation of numbers
had shown significant improvements for our Arabic-
English system, so we investigated this for Chinese-
English. The baseline for these experiments was a
smaller system with 67 million words (67M) bilin-
gual training data (English) and a 500 million word
3-gram LM with a BLEU score of 27.61 on MT06.
First we pre-translated all numbers in the testdata
only, thus forcing the decoder to treat the numbers as
unknown words. Probably because the system could
not match longer phrases across the pre-translated
numbers, the overall translation quality degraded by
1.6 BLEU to 26.05 (see Table 1).
We then tagged all numbers in the training corpus,
replaced them with a placeholder tag and re-trained
the translation model. This reduced the vocabu-
lary and enabled the decoder to generalize longer
phrases across numbers. This strategy did not lead to
the expected result, the BLEU score for MT06 only
reached 25.97 BLEU.
</bodyText>
<table confidence="0.991024">
System MT03 MT06
67M baseline 31.45/60.93 27.61/62.18
test data tagged – 26.06/63.36
training data tagged 29.07/62.52 25.97/63.39
</table>
<tableCaption confidence="0.999627">
Table 1: Number tagging experiments, BLEU/TER
</tableCaption>
<bodyText confidence="0.9998909">
Analysing this in more detail, we found, the rea-
son for this degradation in translation quality could
be the unbalanced occurrence of number tags in the
training data. From the bilingual sentence pairs,
which contain number tags, 66.52% do not contain
the same number of tags on the Chinese and the En-
glish side. As a consequence 52% of the phrase pairs
in the phrase table, which contain number tags had
to be removed, because the tags were unbalanced.
This hurts system performance considerably.
</bodyText>
<sectionHeader confidence="0.82377" genericHeader="method">
3 Scaling up to Large Data
</sectionHeader>
<subsectionHeader confidence="0.960886">
3.1 Language Model
</subsectionHeader>
<bodyText confidence="0.995964857142857">
Due to the availability of more computing resources,
we were able to extend the language model history
from 4- to 5-gram, which improved translation qual-
ity from 29.49 BLEU to 30.22 BLEU for our large
scale 260M system (see Table 2). This shows, that
longer LM histories help if we are able to use enough
data in model training.
</bodyText>
<table confidence="0.908502">
System MT03 MT06
260M, 4gram 31.20/61.00 29.49/61.00
260M, 5gram 32.20/60.59 30.22/60.81
</table>
<tableCaption confidence="0.995944">
Table 2: 4- and 5-gram LM,260M system, BLEU/TER
</tableCaption>
<bodyText confidence="0.999970368421052">
The language model was trained on the sources
from the English Gigaword Corpus V3, which con-
tains several newspapers for the years between 1994
to 2006. We also included the English side of the
bilingual training data, resulting in a total of 2.7 bil-
lion running words after tokenization.
We trained separate open vocabulary language
models for each source and interpolated them using
the SRI Language Modeling Toolkit (Stolcke, 2002).
Table 3 shows the interpolation weights for the dif-
ferent sources. Apart from the English part of the
bilingual data, the newswire data from the Chinese
Xinhua News Agency and the Agence France Press
have the largest weights. This reflects the makeup of
the test data, which comes in large parts from these
sources. Other sources, as for example the UN par-
lamentary speeches or the New York Times, differ
significantly in style and vocabulary from the test
data and therefore get small weights.
</bodyText>
<table confidence="0.956481333333333">
xin 0.30 cna 0.06 nyt 0.03
bil 0.26 un 0.07 ltw 0.01
afp 0.21 apw 0.05
</table>
<tableCaption confidence="0.999205">
Table 3: LM interpolation weights per source
</tableCaption>
<subsectionHeader confidence="0.999384">
3.2 Speeding up Model Training
</subsectionHeader>
<bodyText confidence="0.9998626">
To accelerate the training of word alignment
models we implemented a distributed version of
GIZA++ (Och and Ney, 2003), based on the latest
version of GIZA++ and a parallel version developed
at Peking University (Lin et al., 2006). We divide the
bilingual training data in equal parts and distribute it
over several processing nodes, which perform align-
ment independently. In each iteration the nodes read
the model from the previous step and output all nec-
essary counts from the data for the models, e.g. the
</bodyText>
<page confidence="0.991375">
78
</page>
<bodyText confidence="0.999945">
co-occurrence or fertility model. A master process
collects the counts from the nodes, normalizes them
and outputs the intermediate model for each itera-
tion.
This distributed GIZA++ version finished training
the word alignment up to IBM Model 4 for both lan-
guage directions on the full bilingual corpus (260
million words, English) in 39 hours. On average
about 11 CPUs were running concurrently. In com-
parison the standard GIZA++ implementation fin-
ished the same training in 169 hours running on 2
CPUs, one for each language direction.
We used the Pharaoh/Moses package (Koehn et
al., 2007) to extract and score phrase pairs using the
grow-diag-final extraction method.
</bodyText>
<subsectionHeader confidence="0.944834">
3.3 Translation Model
</subsectionHeader>
<bodyText confidence="0.999982285714286">
We trained two systems, one on the full data and one
without the out-of-domain corpora: UN parlament,
HK hansard and HK law parallel texts. These parla-
mentary sessions and law texts are very different in
genre and style from the MT test data, which con-
sists mainly of newspaper texts and in recent years
also of weblogs, broadcast news and broadcast con-
versation. The in-domain training data had 3.8 mil-
lion sentences and 67 million words (English). The
67 million word system reached a BLEU score of
29.65 on the unseeen MT06 testset. Even though the
full 260M system was trained on almost four times
as many running words, the baseline score for MT06
only increased by 0.6 to 30.22 BLEU (see Table 4).
</bodyText>
<table confidence="0.996047666666667">
System MT03 MT06
67M in-domain 32.42/60.26 29.65/61.22
260M full 32.20/60.59 30.22/60.81
</table>
<tableCaption confidence="0.999766">
Table 4: In-domain only or all training data, BLEU/TER
</tableCaption>
<bodyText confidence="0.999940875">
The 67M system could not translate 752 Chinese
words out of 38937, the number of unknown words
decreased to 564 for the 260M system. To increase
the unigram coverage of the phrase table, we added
the lexicon entries that were not in the phrase table
as one-word translations. This lowered the number
of unknown words further to 410, but did not effect
the translation score.
</bodyText>
<sectionHeader confidence="0.993505" genericHeader="method">
4 POS-based Reordering
</sectionHeader>
<bodyText confidence="0.999987538461539">
As Chinese and English have very different word
order, reordering over a rather limited distance dur-
ing decoding is not sufficient. Also using a simple
distance based distortion probability leaves it essen-
tially to the language model to select among dif-
ferent reorderings. An alternative is to apply auto-
matically learned reordering rules to the test sen-
tences before decoding (Crego and Marino, 2006).
We create a word lattice, which encodes many re-
orderings and allows long distance reordering. This
keeps the translation process in the decoder mono-
tone and makes it significantly faster compared to
allowing long distance reordering at decoding time.
</bodyText>
<subsectionHeader confidence="0.999753">
4.1 Learning Reordering Rules
</subsectionHeader>
<bodyText confidence="0.999953625">
We tag both language sides of the bilingual corpus
with POS information using the Stanford Parser3
and extract POS based reordering patterns from
word alignment information. We use the context in
which a reordering pattern is seen in the training data
as an additional feature. Context refers to the words
or tags to the left or to the right of the sequence for
which a reordering pattern is extracted.
Relative frequencies are computed for every rule
that has been seen more than n times in the training
corpus (we observed good results for n &gt; 5).
For the Chinese system we used only 350k bilin-
gual sentence pairs to extract rules with length of
up to 15. We did not reorder the training corpus
to retrain the translation model on modified Chinese
word order.
</bodyText>
<subsectionHeader confidence="0.999789">
4.2 Applying Reordering Rules
</subsectionHeader>
<bodyText confidence="0.999931545454546">
To avoid hard decisions, we build a lattice struc-
ture for each source sentence as input for our de-
coder, which contains reordering alternatives consis-
tent with the previously extracted rules.
Longer reordering patterns are applied first.
Thereby shorter patterns can match along new paths,
creating short distance reordering on top of long dis-
tance reordering. Every outgoing edge of a node is
scored with the relative frequency of the pattern used
on the following sub path (For details see (Rottmann
and Vogel, 2007)). These model scores give this re-
</bodyText>
<footnote confidence="0.952284">
3http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.998658">
79
</page>
<bodyText confidence="0.996498">
ordering approach an advantage over a simple jump
model with a sliding window.
</bodyText>
<table confidence="0.957048333333333">
System MT03 MT06
260M, standard 32.20/60.59 30.22/60.81
260M, lattice 33.53/59.74 31.74/59.59
</table>
<tableCaption confidence="0.999651">
Table 5: Reordering lattice decoding in BLEU/TER
</tableCaption>
<bodyText confidence="0.998302333333333">
The system with reordering lattice input outper-
forms the system with a reordering window of 4
words by 1.5 BLEU (see Table 5).
</bodyText>
<sectionHeader confidence="0.998559" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999828818181818">
The recent improvements to our Chinese-English
SMT system (see Fig. 1) can be mainly attributed to
a POS based word reordering method and the possi-
bility to work with larger statistical models.
We used the lattice translation functionality of our
decoder to translate reordering lattices. They are
built using reordering rules extracted from tagged
and aligned parallel data. There is further potential
for improvement in this approach, as we did not yet
reorder the training corpus and retrain the translation
model on modified Chinese word order.
</bodyText>
<table confidence="0.818222636363636">
33 Improvements in BLEU
32
31
30
29
28
27
26
25
24
2007 67M+3gr 260M+3gr 260M+4gr 260M+5gr 260M+RO
</table>
<figureCaption confidence="0.995799">
Figure 1: Improvements for MT06 in BLEU
</figureCaption>
<bodyText confidence="0.999042666666667">
We modified GIZA++ to run in parallel, which en-
abled us to include especially longer sentences into
translation model training. We also extended our de-
coder to use 5-gram language models and were able
to train an interpolated LM from all sources of the
English GigaWord Corpus.
</bodyText>
<sectionHeader confidence="0.998272" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999265333333333">
This work was partly funded by DARPA under
the project GALE (Grant number #HR0011-06-2-
0001).
</bodyText>
<sectionHeader confidence="0.989279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99955622">
Josep M. Crego and Jose B. Marino. 2006. Reordering
Experiments for N-Gram-Based SMT. Spoken Lan-
guage Technology Workshop, Palm Beach, Aruba.
Sanjika Hewavitharana, Bing Zhao, Almut Silja Hilde-
brand, Matthias Eck, Chiori Hori, Stephan Vogel and
Alex Waibel. 2005. The CMU Statistical Machine
Translation System for IWSLT 2005. IWSLT 2005,
Pittsburgh, PA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL 2007,
Demonstration Session, Prague, Czech Republic.
Xiaojun Lin, Xinhao Wang, and Xihong Wu. 2006.
NLMP System Description for the 2006 NIST MT
Evaluation. NIST 2006 MT Evaluation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Poukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. ACL 2002, Philadel-
phia, USA.
Kay Rottmann and Stephan Vogel. 2007. Word Reorder-
ing in Statistical Machine Translation with a POS-
based Distortion Model. TMI-2007: 11th Interna-
tional Conference on Theoretical and Methodological
Issues in MT, Skvde, Sweden.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciula and Ralph Weischedel.
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. LAMP-TR-126, University
of Maryland, College Park and BBN Technologies.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. 7th Conference of AMTA, Cambridge, Mas-
sachusetts, USA.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. ICSLP, Denver, Colorado.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky and Christopher Manning. 2005. A Con-
ditional Random Field Word Segmenter. Fourth
SIGHAN Workshop on Chinese Language Processing.
Ashish Venugopal, Andreas Zollman and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. ACL 2005,
WPT-05, Ann Arbor, MI
</reference>
<page confidence="0.998241">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.513803">
<title confidence="0.991828">Recent Improvements in the CMU Large Scale Chinese-English SMT System</title>
<author confidence="0.8839285">Almut Silja Hildebrand</author>
<author confidence="0.8839285">Kay Rottmann</author>
<author confidence="0.8839285">Mohamed Noamany</author>
<author confidence="0.8839285">Qin Gao</author>
<author confidence="0.8839285">Nguyen Bach Vogel Hewavitharana</author>
<affiliation confidence="0.998403">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.9997">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999705">silja,kayrm,mfn,qing,sanjika,nbach,vogel+@cs.cmu.edu</email>
<abstract confidence="0.963351444444445">In this paper we describe recent improvements to components and methods used in our statistical machine translation system for Chinese- English used in the January 2008 GALE evaluation. Main improvements are results of consistent data processing, larger statistical models and a POS-based word reordering approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jose B Marino</author>
</authors>
<title>Reordering Experiments for N-Gram-Based SMT. Spoken Language Technology Workshop,</title>
<date>2006</date>
<location>Palm Beach, Aruba.</location>
<contexts>
<context position="9531" citStr="Crego and Marino, 2006" startWordPosition="1542" endWordPosition="1545">ase table, we added the lexicon entries that were not in the phrase table as one-word translations. This lowered the number of unknown words further to 410, but did not effect the translation score. 4 POS-based Reordering As Chinese and English have very different word order, reordering over a rather limited distance during decoding is not sufficient. Also using a simple distance based distortion probability leaves it essentially to the language model to select among different reorderings. An alternative is to apply automatically learned reordering rules to the test sentences before decoding (Crego and Marino, 2006). We create a word lattice, which encodes many reorderings and allows long distance reordering. This keeps the translation process in the decoder monotone and makes it significantly faster compared to allowing long distance reordering at decoding time. 4.1 Learning Reordering Rules We tag both language sides of the bilingual corpus with POS information using the Stanford Parser3 and extract POS based reordering patterns from word alignment information. We use the context in which a reordering pattern is seen in the training data as an additional feature. Context refers to the words or tags to </context>
</contexts>
<marker>Crego, Marino, 2006</marker>
<rawString>Josep M. Crego and Jose B. Marino. 2006. Reordering Experiments for N-Gram-Based SMT. Spoken Language Technology Workshop, Palm Beach, Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjika Hewavitharana</author>
<author>Bing Zhao</author>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Chiori Hori</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<date>2005</date>
<booktitle>The CMU Statistical Machine Translation System for IWSLT 2005. IWSLT</booktitle>
<location>Pittsburgh, PA.</location>
<marker>Hewavitharana, Zhao, Hildebrand, Eck, Hori, Vogel, Waibel, 2005</marker>
<rawString>Sanjika Hewavitharana, Bing Zhao, Almut Silja Hildebrand, Matthias Eck, Chiori Hori, Stephan Vogel and Alex Waibel. 2005. The CMU Statistical Machine Translation System for IWSLT 2005. IWSLT 2005, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. ACL 2007, Demonstration Session,</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.</location>
<contexts>
<context position="7782" citStr="Koehn et al., 2007" startWordPosition="1252" endWordPosition="1255">or the models, e.g. the 78 co-occurrence or fertility model. A master process collects the counts from the nodes, normalizes them and outputs the intermediate model for each iteration. This distributed GIZA++ version finished training the word alignment up to IBM Model 4 for both language directions on the full bilingual corpus (260 million words, English) in 39 hours. On average about 11 CPUs were running concurrently. In comparison the standard GIZA++ implementation finished the same training in 169 hours running on 2 CPUs, one for each language direction. We used the Pharaoh/Moses package (Koehn et al., 2007) to extract and score phrase pairs using the grow-diag-final extraction method. 3.3 Translation Model We trained two systems, one on the full data and one without the out-of-domain corpora: UN parlament, HK hansard and HK law parallel texts. These parlamentary sessions and law texts are very different in genre and style from the MT test data, which consists mainly of newspaper texts and in recent years also of weblogs, broadcast news and broadcast conversation. The in-domain training data had 3.8 million sentences and 67 million words (English). The 67 million word system reached a BLEU score </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL 2007, Demonstration Session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Lin</author>
<author>Xinhao Wang</author>
<author>Xihong Wu</author>
</authors>
<date>2006</date>
<journal>MT Evaluation.</journal>
<booktitle>NLMP System Description for the 2006 NIST MT Evaluation. NIST</booktitle>
<contexts>
<context position="6907" citStr="Lin et al., 2006" startWordPosition="1108" endWordPosition="1111">he test data, which comes in large parts from these sources. Other sources, as for example the UN parlamentary speeches or the New York Times, differ significantly in style and vocabulary from the test data and therefore get small weights. xin 0.30 cna 0.06 nyt 0.03 bil 0.26 un 0.07 ltw 0.01 afp 0.21 apw 0.05 Table 3: LM interpolation weights per source 3.2 Speeding up Model Training To accelerate the training of word alignment models we implemented a distributed version of GIZA++ (Och and Ney, 2003), based on the latest version of GIZA++ and a parallel version developed at Peking University (Lin et al., 2006). We divide the bilingual training data in equal parts and distribute it over several processing nodes, which perform alignment independently. In each iteration the nodes read the model from the previous step and output all necessary counts from the data for the models, e.g. the 78 co-occurrence or fertility model. A master process collects the counts from the nodes, normalizes them and outputs the intermediate model for each iteration. This distributed GIZA++ version finished training the word alignment up to IBM Model 4 for both language directions on the full bilingual corpus (260 million w</context>
</contexts>
<marker>Lin, Wang, Wu, 2006</marker>
<rawString>Xiaojun Lin, Xinhao Wang, and Xihong Wu. 2006. NLMP System Description for the 2006 NIST MT Evaluation. NIST 2006 MT Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6795" citStr="Och and Ney, 2003" startWordPosition="1089" endWordPosition="1092">he Chinese Xinhua News Agency and the Agence France Press have the largest weights. This reflects the makeup of the test data, which comes in large parts from these sources. Other sources, as for example the UN parlamentary speeches or the New York Times, differ significantly in style and vocabulary from the test data and therefore get small weights. xin 0.30 cna 0.06 nyt 0.03 bil 0.26 un 0.07 ltw 0.01 afp 0.21 apw 0.05 Table 3: LM interpolation weights per source 3.2 Speeding up Model Training To accelerate the training of word alignment models we implemented a distributed version of GIZA++ (Och and Ney, 2003), based on the latest version of GIZA++ and a parallel version developed at Peking University (Lin et al., 2006). We divide the bilingual training data in equal parts and distribute it over several processing nodes, which perform alignment independently. In each iteration the nodes read the model from the previous step and output all necessary counts from the data for the models, e.g. the 78 co-occurrence or fertility model. A master process collects the counts from the nodes, normalizes them and outputs the intermediate model for each iteration. This distributed GIZA++ version finished traini</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Poukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<publisher>ACL</publisher>
<location>Philadelphia, USA.</location>
<contexts>
<context position="2128" citStr="Papineni et al., 2002" startWordPosition="341" endWordPosition="344">sfully to our large Chinese system. 1.1 Decoder Our translation system is based on the CMU SMT decoder as described in (Hewavitharana et 77 al., 2005). Our decoder is a phrase-based beam search decoder, which combines multiple models e.g. phrase tables, several language models, a distortion model ect. in a log-linear fashion. In order to find an optimal set of weights, we use MER training as described in (Venugopal et al., 2005), which uses rescoring of the top n hypotheses to maximize an evaluation metric like BLEU or TER. 1.2 Evaluation In this paper we report results using the BLEU metric (Papineni et al., 2002), however as the evaluation criterion in GALE is HTER (Snover et al., 2006), we also report in TER (Snover et al., 2005). We used the test sets from the NIST MT evaluations from the years 2003 and 2006 as development and unseen test data. 1.3 Training Data In translation model training we used the ChineseEnglish bilingual corpora relevant to GALE available through the LDC1. After sentence alignment these sources add up to 10.7 million sentences with 301 million running words on the English side. Our preprocessing steps include tokenization on the English side and for Chinese: automatic word se</context>
</contexts>
<marker>Papineni, Poukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Poukos, Todd Ward and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. ACL 2002, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word Reordering in Statistical Machine Translation with a POSbased Distortion Model.</title>
<date>2007</date>
<booktitle>TMI-2007: 11th International Conference on Theoretical and Methodological Issues in MT, Skvde,</booktitle>
<contexts>
<context position="11124" citStr="Rottmann and Vogel, 2007" startWordPosition="1807" endWordPosition="1810">ning corpus to retrain the translation model on modified Chinese word order. 4.2 Applying Reordering Rules To avoid hard decisions, we build a lattice structure for each source sentence as input for our decoder, which contains reordering alternatives consistent with the previously extracted rules. Longer reordering patterns are applied first. Thereby shorter patterns can match along new paths, creating short distance reordering on top of long distance reordering. Every outgoing edge of a node is scored with the relative frequency of the pattern used on the following sub path (For details see (Rottmann and Vogel, 2007)). These model scores give this re3http://nlp.stanford.edu/software/lex-parser.shtml 79 ordering approach an advantage over a simple jump model with a sliding window. System MT03 MT06 260M, standard 32.20/60.59 30.22/60.81 260M, lattice 33.53/59.74 31.74/59.59 Table 5: Reordering lattice decoding in BLEU/TER The system with reordering lattice input outperforms the system with a reordering window of 4 words by 1.5 BLEU (see Table 5). 5 Summary The recent improvements to our Chinese-English SMT system (see Fig. 1) can be mainly attributed to a POS based word reordering method and the possibility</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word Reordering in Statistical Machine Translation with a POSbased Distortion Model. TMI-2007: 11th International Conference on Theoretical and Methodological Issues in MT, Skvde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Linnea Micciula</author>
<author>Ralph Weischedel</author>
</authors>
<title>A Study of Translation Error Rate with Targeted Human Annotation.</title>
<date>2005</date>
<tech>LAMP-TR-126,</tech>
<institution>University of Maryland, College Park and BBN Technologies.</institution>
<contexts>
<context position="2248" citStr="Snover et al., 2005" startWordPosition="363" endWordPosition="366">wavitharana et 77 al., 2005). Our decoder is a phrase-based beam search decoder, which combines multiple models e.g. phrase tables, several language models, a distortion model ect. in a log-linear fashion. In order to find an optimal set of weights, we use MER training as described in (Venugopal et al., 2005), which uses rescoring of the top n hypotheses to maximize an evaluation metric like BLEU or TER. 1.2 Evaluation In this paper we report results using the BLEU metric (Papineni et al., 2002), however as the evaluation criterion in GALE is HTER (Snover et al., 2006), we also report in TER (Snover et al., 2005). We used the test sets from the NIST MT evaluations from the years 2003 and 2006 as development and unseen test data. 1.3 Training Data In translation model training we used the ChineseEnglish bilingual corpora relevant to GALE available through the LDC1. After sentence alignment these sources add up to 10.7 million sentences with 301 million running words on the English side. Our preprocessing steps include tokenization on the English side and for Chinese: automatic word segmentation using the revised version of the Stanford Chinese Word Segmenter2 (Tseng et al., 2005) from 2007, replacement</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Makhoul, Micciula, Weischedel, 2005</marker>
<rawString>Mathew Snover, Bonnie Dorr, Richard Schwartz, John Makhoul, Linnea Micciula and Ralph Weischedel. 2005. A Study of Translation Error Rate with Targeted Human Annotation. LAMP-TR-126, University of Maryland, College Park and BBN Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<date>2006</date>
<booktitle>A Study of Translation Edit Rate with Targeted Human Annotation. 7th Conference of AMTA,</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="2203" citStr="Snover et al., 2006" startWordPosition="354" endWordPosition="357">ed on the CMU SMT decoder as described in (Hewavitharana et 77 al., 2005). Our decoder is a phrase-based beam search decoder, which combines multiple models e.g. phrase tables, several language models, a distortion model ect. in a log-linear fashion. In order to find an optimal set of weights, we use MER training as described in (Venugopal et al., 2005), which uses rescoring of the top n hypotheses to maximize an evaluation metric like BLEU or TER. 1.2 Evaluation In this paper we report results using the BLEU metric (Papineni et al., 2002), however as the evaluation criterion in GALE is HTER (Snover et al., 2006), we also report in TER (Snover et al., 2005). We used the test sets from the NIST MT evaluations from the years 2003 and 2006 as development and unseen test data. 1.3 Training Data In translation model training we used the ChineseEnglish bilingual corpora relevant to GALE available through the LDC1. After sentence alignment these sources add up to 10.7 million sentences with 301 million running words on the English side. Our preprocessing steps include tokenization on the English side and for Chinese: automatic word segmentation using the revised version of the Stanford Chinese Word Segmenter</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. 7th Conference of AMTA, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<location>ICSLP, Denver, Colorado.</location>
<contexts>
<context position="6033" citStr="Stolcke, 2002" startWordPosition="960" endWordPosition="961">able to use enough data in model training. System MT03 MT06 260M, 4gram 31.20/61.00 29.49/61.00 260M, 5gram 32.20/60.59 30.22/60.81 Table 2: 4- and 5-gram LM,260M system, BLEU/TER The language model was trained on the sources from the English Gigaword Corpus V3, which contains several newspapers for the years between 1994 to 2006. We also included the English side of the bilingual training data, resulting in a total of 2.7 billion running words after tokenization. We trained separate open vocabulary language models for each source and interpolated them using the SRI Language Modeling Toolkit (Stolcke, 2002). Table 3 shows the interpolation weights for the different sources. Apart from the English part of the bilingual data, the newswire data from the Chinese Xinhua News Agency and the Agence France Press have the largest weights. This reflects the makeup of the test data, which comes in large parts from these sources. Other sources, as for example the UN parlamentary speeches or the New York Times, differ significantly in style and vocabulary from the test data and therefore get small weights. xin 0.30 cna 0.06 nyt 0.03 bil 0.26 un 0.07 ltw 0.01 afp 0.21 apw 0.05 Table 3: LM interpolation weight</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. ICSLP, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A Conditional Random Field Word Segmenter.</title>
<date>2005</date>
<booktitle>Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="2825" citStr="Tseng et al., 2005" startWordPosition="460" endWordPosition="463">e also report in TER (Snover et al., 2005). We used the test sets from the NIST MT evaluations from the years 2003 and 2006 as development and unseen test data. 1.3 Training Data In translation model training we used the ChineseEnglish bilingual corpora relevant to GALE available through the LDC1. After sentence alignment these sources add up to 10.7 million sentences with 301 million running words on the English side. Our preprocessing steps include tokenization on the English side and for Chinese: automatic word segmentation using the revised version of the Stanford Chinese Word Segmenter2 (Tseng et al., 2005) from 2007, replacement of traditional by simplified Chinese characters and 2-byte to 1-byte ASCII character normalization. After data cleaning steps like e.g. removal of sentence pairs with very unbalanced sen1http://projects.ldc.upenn.edu/gale/data/catalog.html 2http://nlp.stanford.edu/software/segmenter.shtml Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 77–80, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics tence length etc., we used the remaining 10 million sentences with 260 million words (English) in translation model training (260M </context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky and Christopher Manning. 2005. A Conditional Random Field Word Segmenter. Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollman</author>
<author>Alex Waibel</author>
</authors>
<title>Training and Evaluation Error Minimization Rules for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>ACL 2005, WPT-05,</booktitle>
<location>Ann Arbor, MI</location>
<contexts>
<context position="1938" citStr="Venugopal et al., 2005" startWordPosition="307" endWordPosition="310">as more extensive models improve translation quality. Word reordering is still a difficult problem in SMT. In Section 4 we apply a Part Of Speech (POS) based syntactic reordering model successfully to our large Chinese system. 1.1 Decoder Our translation system is based on the CMU SMT decoder as described in (Hewavitharana et 77 al., 2005). Our decoder is a phrase-based beam search decoder, which combines multiple models e.g. phrase tables, several language models, a distortion model ect. in a log-linear fashion. In order to find an optimal set of weights, we use MER training as described in (Venugopal et al., 2005), which uses rescoring of the top n hypotheses to maximize an evaluation metric like BLEU or TER. 1.2 Evaluation In this paper we report results using the BLEU metric (Papineni et al., 2002), however as the evaluation criterion in GALE is HTER (Snover et al., 2006), we also report in TER (Snover et al., 2005). We used the test sets from the NIST MT evaluations from the years 2003 and 2006 as development and unseen test data. 1.3 Training Data In translation model training we used the ChineseEnglish bilingual corpora relevant to GALE available through the LDC1. After sentence alignment these so</context>
</contexts>
<marker>Venugopal, Zollman, Waibel, 2005</marker>
<rawString>Ashish Venugopal, Andreas Zollman and Alex Waibel. 2005. Training and Evaluation Error Minimization Rules for Statistical Machine Translation. ACL 2005, WPT-05, Ann Arbor, MI</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>