<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.995862">
A language−independent shallow−parser Compiler
</title>
<author confidence="0.765198">
Alexandra Kinyon
</author>
<affiliation confidence="0.7664485">
CIS Dpt. .
University of Pennsylvania
</affiliation>
<email confidence="0.9247345">
kinyon@linc.cis.upenn.edu
http://www.cis.upenn.edu/˜kinyon
</email>
<sectionHeader confidence="0.995334" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947933333333">
We present a rule−based shallow−
parser compiler, which allows to
generate a robust shallow−parser for
any language, even in the absence of
training data, by resorting to a very
limited number of rules which aim at
identifying constituent boundaries.
We contrast our approach to other
approaches used for shallow−parsing
(i.e. finite−state and probabilistic
methods). We present an evaluation
of our tool for English (Penn
Treebank) and for French (newspaper
corpus &amp;quot;LeMonde&amp;quot;) for several tasks
(NP−chunking &amp; &amp;quot;deeper&amp;quot; parsing) .
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.965776064516129">
Full syntactic parsers of unrestricted text are
costly to develop, costly to run and often yield
errors, because of lack of robustness of wide−
coverage grammars and problems of attachment.
This has led, as early as 1958 (Joshi &amp; Hopely
97), to the development of shallow−parsers,
which aim at identifying as quickly and
accurately as possible, main constituents (and
possibly syntactic functions) in an input,
without dealing with the most difficult problems
encountered with &amp;quot;full−parsing&amp;quot;. Hence,
shallow−parsers are very practical tools. There
are two main techniques used to develop
shallow−parsers:
1− Probabilistic techniques (e.g. Magerman
94, Ratnaparkhi 97, Daelmans &amp; al. 99)
2− Finite−state techniques (e.g. Grefenstette 96)
Probabilistic techniques require large amounts
of syntactically−annotated training data1, which
makes them very unsuitable for languages for
which no such data is available (i.e. most
languages except English) and also, they are not
domain−independent nor &amp;quot;style−independent&amp;quot;
(e.g. they do not allow to successfully shallow−
parse speech, if no annotated data is available
for that “style”). Finally, a shallow−parser
developed using these techniques will have to
mirror the information contained in the training
data. For instance, if one trains such a tool on
data were only non recursive NP chunks are
marked2, then one will not be able to obtain
richer information such as chunks of other
categories, embeddings, syntactic functions...
On the other hand, finite−state techniques rely
on the development of a large set of rules (often
based on regular expressions) to capture all the
ways a constituent can expend. So for example
for detecting English NPs, one could write the
following rules :
NP → Det adj* noun adj*
NP → Det adj (for noun ellipsis)
NP → ProperNoun etc ....
But this is time consuming and difficult since
one needs to foresee all possible rewriting cases,
and if some rule is forgotten, or if too many
POS errors are left, robustness and/or accuracy
will suffer.
Then these regular expressions have to be
manipulated i.e. transformed into automata,
which will be determinized and minimized
(both being costly operations). And even though
determinization and minimization must be done
only once (in theory) for a given set of rules, it
is still costly to port such tools to a new set of
rules (e.g. for a new language, a new domain) or
to change some existing rules.
In this paper, we argue that in order to
accomplish the same task, it is unnecessary to
develop full sets of regular expression : instead
1 We are leaving aside unsupervised learning techniques
here, since to our knowledge they have not proved a
successful for developing practical shallow−parsers.
</bodyText>
<page confidence="0.765157">
2 See (Abney 91) for the definition of a chunk.
</page>
<bodyText confidence="0.985189527777778">
of specifying all the ways a constituent can be
rewritten, it is sufficient to express how it
begins and/or ends. This allows to achieve
similar results but with far fewer rules, and
without a need for determinization or
minimization because rules which are written
that way are de−facto deterministic. So in a
sense, our approach bears some similarities with
the constraint−based formalism because we
resort to “local rules” (Karlsson &amp; al. 95), but
we focus on identifying constituent boundaries
(and not syntactic functions), and allow any
level of embedding thanks to the use of a stack.
In the first part of this paper, we present our
tool: a shallow−parser compiler. In a second
part, we present output samples as well as
several evaluations for French and for English,
where the tool has been used to develop both an
NP−chunker and a richer shallow−parser. We
also explain why our approach is more tolerant
to POS−tagging errors. Finally, we discuss
some other practical uses which are made of this
shallow−parser compiler.
2 Presentation of the compiler
Our tool has been developed using JavaCC (a
compiler compiler similar to Lex &amp; Yacc, but
for java). The program takes as input a file
containing rules. These rules aim at identifying
constituent boundaries for a given language.
(For example for English, one such rule could
say &amp;quot;When encountering a preposition, start a
PP&amp;quot;), either by relying on function words, or on
morphological information (e.g. gender) if it is
appropriate for the language which is being
considered.
These rule files specify :
</bodyText>
<listItem confidence="0.999711571428571">
• A mapping between the &amp;quot;abstract&amp;quot; morpho−
syntactic tags, used in the rules, and &amp;quot;real&amp;quot;
morpho−syntactic tags as they will appear
in the input.
• A declaration of the syntactic constituents
which will be detected (e.g. NP, VP, PP ...)
• A set of unordered rules
</listItem>
<bodyText confidence="0.970269266666667">
From this rule file, the compiler generates a java
program, which is a shallow−parser based on
the rule file. One can then run this shallow−
parser on an input to obtain a shallow−parsed
text3.
The compiler itself is quite simple, but we have
decided to compile the rules rather than interpret
them essentially for efficiency reasons. Also, it
3 The input is generally POS−tagged, although this is not
an intrinsic requirement of the compiler.
is language independent since a rule file may be
written for any given language, and compiled
into a shallow−parser for that language.
Each rule is of the form:
{Preamble} disjunction of patterns then actions
</bodyText>
<subsectionHeader confidence="0.59421">
2.1 A concrete example : compiling a
simple NP−chunker for English
</subsectionHeader>
<bodyText confidence="0.999834916666666">
In this section we present a very simple &amp;quot;toy&amp;quot;
example which aims at identifying some NPs in
the Penn Treebank4 (Marcus &amp; al 93).
In order to do so, we write a rule file, shown on
figure 1. The top of the file declares a mapping
between the abstract tagset we use in our rules,
and the tagset of the PennTreebank. For
example commonN corresponds to the 3 tags NN,
NNS, NNPS in the PennTreebank. It then
declares the labels of the constituents which will
be detected (here there is only one: NP). Finally,
it declares 3 rules.
</bodyText>
<equation confidence="0.973199111111111">
%% A small NP−chunker for the Penn−treebank
tagmap &lt;QuantityAdv:any,some,many&gt;;
tagmap&lt;ProperN:NNP&gt;;
tagmap&lt;det:DT,PDT&gt;;
tagmap&lt;commonN:NN,NNS,NNPS&gt;;
tagmap&lt;DemPro:D*&gt;;
tagmap&lt;Adj:JJ*&gt;;
tagmap&lt;OtherTags:V*,P,C*,RB*.,:,,&gt;;
label NP;
%% rule 1
{} (:$det)  |($QuantityAdv:)  |(:$DemPro) then
close(),open(NP);
%% rule 2
{!NP} (:$commonN)  |(:$Adj)  |(:$ProperN) then
close(),open(NP);
%% rule 3
{} (:$OtherTags) then close();
FIGURE 1 : An example of a rule−file
</equation>
<bodyText confidence="0.951727764705882">
Rule 1 says that when a determiner, a quantity
adverb or a demonstrative pronoun is
encountered, the current constituent must be
closed, and an NP must be opened. Rule 2 says
that, when not inside an NP, if a common noun,
an adjective or a proper noun is encountered,
then the current constituent should be closed and
an NP should be opened. Finally, Rule 3 says
that when some other tag is encountered (i.e. a
verb, a preposition, a punctuation, a conjunction
4 This example is kept very simple for sake of clarity. It
does not aim at yielding a very accurate result.
or an adverb) then the current constituent
should be closed.
This rule file is then compiled into an NP−
chunker. If one inputs (a) to the NP−chunker, it
will then output (b)
</bodyText>
<figure confidence="0.57537375">
(a) The/DT cat/NNS eats/VBZ the/DT
mouse/NNS ./.
(b) &lt;NP&gt; The/DT cat/NNS &lt;/NP&gt; eats/VBZ &lt;NP&gt;
the/DT mouse/NNS &lt;/NP&gt; ./.
</figure>
<bodyText confidence="0.619401">
In our compiler, rules access a limited context :
</bodyText>
<listItem confidence="0.99521575">
• The constituent(s) being built
• The previous form and POS
• The current form and POS
• The next form and POS
</listItem>
<bodyText confidence="0.999445166666667">
So contrary to standard finite−state techniques,
only constituent boundaries are explicited, and
it is not necessary (or even possible) to specify
all the possible ways a constituent may be
realized .
As shown in section 3, this reduces greatly the
number of rules in the system (from several
dozens to less than 60 for a wide−coverage
shallow−parser). Also, focussing only on
constituent boundaries ensures determinism :
there is no need for determinizing nor
minimizing the automata we obtain from our
rules.
Our tool is robust : it never fails to provide an
output and can be used to create a parser for any
text from any domain in any language.
It is also important to note that the parsing is
done incrementally : the input is scanned strictly
from left to right, in one single pass. And for
each pattern matched, the associated actions are
taken (i.e. constituent boundaries are added).
Since there is no backtracking, this allows an
output in linear time. If several patterns match,
the longest one is applied. Hence our rules are
declarative and unordered. Although in theory
conflicts could appear between 2 patterns of
same length (as shown in (c1) and (c2)), this has
never happened in practice. Of course the case is
nonetheless dealt with in the implementation,
and a warning is then issued to the user.
</bodyText>
<equation confidence="0.2630085">
(c1) 1} (:a) (:b) then close();
(c2) 1} (:a) (:b) then open(X);
</equation>
<bodyText confidence="0.997431538461538">
As is seen on figure 1, one can write
disjunctions of patterns for a given rule.
In this very simple example, only non recursive
NP−chunks are marked, by choice. But it is not
an intrinsic limitation of the tool, since any
amount of embedding can be obtained (as
shown in section 3 below), through the use of a
Stack. From a formal point of view, our tool has
the power of a deterministic push−down
automaton.
When there is a match between the input and the
pattern in a rule, the following actions may be
taken :
</bodyText>
<listItem confidence="0.998136230769231">
• close(): closes the constituent last opened by
inserting &lt;/X&gt; in the output, were X is the
syntactic label at the top of the Stack.
• open(X): opens a new constituent by
inserting label &lt;X&gt; in the output
• closeWhenOpen(X,Y): delays the closing
of constituent labeled X, until constituent
labeled Y is opened.
• closeWhenClose(X,Y): delays the closing of
constituent labeled X, until constituent
labeled Y is closed.
• doNothing(): used to &amp;quot;neutralize&amp;quot; a shorter
match.
</listItem>
<bodyText confidence="0.993877461538462">
Examples for the actions open() and close() were
provided on figure 1. The actions
closeWhenOpen(X,Y) and closeWhenClose(X,Y)
allow to perform some attachments. For
example a rule for English could say :
1NP} (:$conjCoord) then close(), open(NPcoord),
closeWhenClose(NPcoord,NP);
This rule says that when inside an NP, a
coordinating conjunction is encountered, a
NPcoord should be opened, and should be
closed only when the next NP to the right will
be closed. This allows, for example, to obtain
output (d) for a coordination of NPs5.
</bodyText>
<figure confidence="0.897635555555556">
(d) John likes
&lt;NP&gt;Apples&lt;/NP&gt;
&lt;NPcoord&gt; and
&lt;NP&gt; green beans &lt;/NP&gt;
&lt;/NPcoord&gt;
An example of the action doNothing() for
English could be:
1} (:$prep) then open(PP);
1} P(:$prep) (:$prep) then doNothing() ;
</figure>
<bodyText confidence="0.988882625">
The first rule says that when a preposition is
encountered, a PP should be opened. The second
rule says that when a preposition is encountered,
if the previous tag was also a preposition,
nothing should be done. Since the pattern for
5 This is shown as an example as to how this action can be
used, it does not aim at imposing this structure to
coordinations, which could be dealt with differently using
other rules.
rule 2 is longer than the pattern for rule 1, it will
apply when the second preposition in a row is
encountered, hence &amp;quot;neutralizing&amp;quot; rule 1. This
allows to obtain &amp;quot;flatter&amp;quot; structures for PPs,
such as the one in (e1). Without this rule, one
would obtain the structure in (e2) for the same
input.
</bodyText>
<figure confidence="0.92196">
(e1) This costs &lt;PP&gt; up to 1000 $ &lt;/PP&gt;
(e2) This costs
&lt;PP&gt; up
&lt;PP&gt; to 1000 $ &lt;/PP&gt;
&lt;/PP&gt;
3. Some &amp;quot;real world&amp;quot; applications
</figure>
<bodyText confidence="0.999910462962963">
In this section, we present some uses which
have been made of this Shallow−Parser
compiler. First we explain how the tool has been
used to develop a 1 million word Treebank for
French, along with an evaluation. Then we
present an evaluation for English.
It is well known that evaluating a Parser is a
difficult task, and this is even more true for
Shallow−Parsers, because there is no real
standard task (some Shallow−parsers have
embedded constituents, some encode syntactic
functions, some encode constituent information,
some others dependencies or even a mixture of
the 2) There also isn’t standard evaluation
measures for such tools. To perform evaluation,
one can compare the output of the parser to a
well−established Treebank developed
independently (assuming one is available for the
language considered), but the result is unfair to
the parser because generally in Treebanks all
constituents are attached. One can also compare
the output of the parser to a piece of text which
has been manually annotated just for the
purpose of the evaluation. But then it is difficult
to ensure an objective measure (esp. if the
person developing the parser and the person
doing the annotation are the same). Finally, one
can automatically extract, from a well−
established Treebank, information that is
relevant to a given , widely agreed on, non
ambiguous task such as identifying bare non−
recursive NP−chunks, and compare the output
of the parser for that task to the extracted
information. But this yields an evaluation that is
valid only for this particular task and may not
well reflect the overall performance of the
parser. In what follows, in order to be as
objective as possible, we use these 3 types of
evaluation, both for French and for English6, and
use standard measures of recall and precision.
Please bear in mind though that these metric
measures, although very fashionable, have their
limits7. Our goal is not to show that our tool is
the one which provides the best results when
compared to other shallow−parsers, but rather to
show that it obtains similar results, although in a
much simpler way, with a limited number of
rules compared to finite−state techniques and
more tolerance to POS errors, and even in the
absence of available training data (i.e. cases
were probabilistic techniques could not be
used). To achieve this goal, we also present
samples of parsed outputs we obtain, so that the
reader may judge for himself/herself.
</bodyText>
<subsectionHeader confidence="0.990284">
3.1. A shallow−parser for French
</subsectionHeader>
<bodyText confidence="0.9995525">
We used our compiler to create a shallow−parser
for French. Contrary to English, very few
shallow−parsers exist for French, and no
Treebank actually exist to train a probabilistic
parser (although one is currently being built
using our tool c.f. (Abeillé &amp; al. 00)).
Concerning shallow−parsers, one can mention
(Bourigault 92) who aims at isolating NPs
representing technical terms, whereas we wish
to have information on other constituents as
well, and (Ait−Moktar &amp; Chanod 97) whose
tool is not publicly available. One can also
mention (Vergne 99), who developed a parser
for French which also successfully relies on
function words to identify constituent
boundaries. But contrary to us, his tool does not
embed constituents8. And it is also not publicly
available.
In order to develop a set of rules for French, we
had to examine the linguistic characteristics of
this language. It turns out that although French
has a richer morphology than English (e.g.
gender for nouns, marked tense for verbs), most
constituents are nonetheless triggered by the
occurrence of a function word. Following the
linguistic tradition, we consider as function
words all words associated to a POS which
labels a closed−class i.e. : determiners,
prepositions, clitics, auxiliaries, pronouns
(relative, demonstrative), conjunctions
</bodyText>
<footnote confidence="0.896995333333333">
6 Of course, manual annotation was done by a different
person than the one who developed the rules.
7 For instance in a rule−based system, performance may
often be increased by adding more rules.
8 Instead, it identifies chunks and then assigns some
syntactic functions to these chunks.
</footnote>
<bodyText confidence="0.925683868421053">
(subordinating, coordinating), auxiliaries, (g2) * &lt;SENT&gt; &lt;NP&gt; Jean &lt;/NP&gt;
punctutation marks and adverbs belonging to a &lt;VN&gt; pense&lt;/VN&gt;
closed class (e.g. negation adverbs &amp;quot;ne&amp;quot; &amp;quot;pas&amp;quot;)9. &lt;SUB&gt; que
The presence of function words makes the &lt;NP&gt; la femme &lt;/NP&gt;
detection of the beginning of a constituent rather &lt;REL&gt; que
easy. For instance, contrary to English, &lt;NP&gt; Pierre &lt;/NP&gt;
subordinating conjunctions (que/that) are never &lt;VN&gt; voit&lt;/VN&gt;
omitted when a subordinating clause starts. &lt;VN&gt; aime &lt;/VN&gt;
Similarly, determiners are rarely omitted at the &lt;NP&gt; Donald &lt;/NP&gt;
beginning of an NP. &lt;/REL&gt;
Our aim was to develop a shallow−parser which &lt;/SUB&gt; . &lt;/SENT&gt;
dealt with some embedding, but did not commit (*John thinks that the woman [REL that Peter sees
to attach potentially ambiguous phrases such as likes Donald])
PPs and verb complements. We wanted to Concerning clitics, we have decided to group
identify the following constituents : NP, PP, VN them with the verb (h1) even when dealing with
(verbal nucleus), VNinf (infinitivals introduced subject clitics (h2). One motivation is the
by a preposition), COORD (for coordination), possible inversion of the subject clitic (h3).
SUB (sentential complements), REL (relative (h1) &lt;SENT&gt;&lt;NP&gt; JEAN &lt;/NP&gt;
clauses), SENT (sentence boundaries), INC (for &lt;VN&gt; le lui donne&lt;/VN&gt; . &lt;/SENT&gt;
constituents of unknown category), AdvP (J. gives it to him).
(adverbial phrases). (h2) &lt;SENT&gt; &lt;VN&gt; Il le voit &lt;/VN&gt; . &lt;/SENT&gt;
We wanted NPs to include all adjectives but not (He sees it)
other postnominal modifiers (i.e. postposed (h3) &lt;SENT&gt;&lt;VN&gt; L’as tu vu &lt;/VN&gt; ? &lt;/SENT&gt;
relative clauses and PPs), in order to obtain a (Him did you see ?).
structure similar to (f). Sentences are given a flat structure, that is
(f) &lt;NP&gt; Le beau livre bleu &lt;/NP&gt; complements are not included in a verbal phrase10
&lt;PP&gt; de &lt;NP&gt;ma cousine&lt;/NP&gt; &lt;/PP&gt; ... (i). From a practical point of view this eases our
(my cousin’s beautiful blue book) task. From a theoretical point of view, the
Relative clauses also proved easy to identify traditional VP (with complements) is subject to
since they begin when a relative pronoun is much linguistic debate and is often
encountered. The ending of clauses occurs discontinuous in French as is shown in (j1) and
essentially when a punctuation mark or a (j2): In (j1) the NP subject (IBM) is postverbal
conjunction of coordination is encountered or and precedes the locative complement (sur le
when another clause begins, or when a sentence marché). In (j2), the adverb certainement is also
ends (g1) . These rules for closing clauses work postverbal and precedes the NP object (une
fairly well in practice (see evaluation below) but augmentation de capital).
could be further refined, since they will yield a (i) &lt;SENT&gt;&lt;NP&gt; JEAN &lt;/NP&gt;
wrong closing boundary for the relative in a &lt;VN&gt; donne&lt;/VN&gt;
</bodyText>
<figure confidence="0.822868107142857">
sentence like (g2) &lt;NP&gt;une pomme&lt;/NP&gt;
(g1) &lt;SENT&gt; &lt;NP&gt; Jean &lt;/NP&gt; &lt;PP&gt; à &lt;NP&gt; Marie &lt;/NP&gt; &lt;/PP&gt; . &lt;/SENT&gt;
&lt;VN&gt; voit&lt;/VN&gt; (John gives an apple to Mary)
&lt;NP&gt;la femme &lt;/NP&gt; (j1) les actions qu’a mises IBM sur le marché
&lt;REL&gt; qui (the shares that IBM put on the market)
&lt;VN&gt; pense &lt;/VN&gt; (j2) Les actionnaires décideront certainement une
&lt;SUB&gt; que augmentation de capital (the stock holders will
&lt;NP&gt; Paul &lt;/NP&gt; certainly decide on a raise of capital)
&lt;VN&gt; viendra &lt;/VN&gt;
&lt;/SUB&gt;
&lt;/REL&gt; . &lt;/SENT&gt;
(John sees the woman who thinks that Paul will
come)
3.1.1 Evaluation for French
9 Considering punctuation marks as function words may
be &amp;quot;extending&amp;quot; the linguistic tradition. Nonetheless, it is a
closed class, since there is a small finite number of
punctuation marks.
10 Hence the use of VN(for verbal nucleus) instead of VP.
&lt;SENT&gt; &lt;NP&gt; La:Dfs proportion:NC &lt;/NP&gt;
&lt;PP&gt; d’:P &lt;NP&gt; 6tudiants:NC &lt;/NP&gt; &lt;/PP&gt;
&lt;PP&gt; par_rapport_à:P
&lt;NP&gt; la:Ddef population:NC&lt;/NP&gt; &lt;/PP&gt;
&lt;PONCT&gt; ,:PONCT &lt;/PONCT&gt;
&lt;PP&gt; dans:P &lt;NP&gt; notre:Dposs
pays:NC&lt;/NP&gt; &lt;/PP&gt;
&lt;PONCT&gt; ,:PONCT&lt;/PONCT&gt;
&lt;VN&gt; est:VP inf6rieure:Aqual &lt;/VN&gt;
&lt;PP&gt; à:P &lt;NP&gt; ce:PROdem&lt;/NP&gt; &lt;/PP&gt;
&lt;REL&gt; qu:PROR3ms
&lt;VN&gt; elle:CL est:VP &lt;/VN&gt;
&lt;COORD&gt; &lt;PP&gt; à:P &lt;NP&gt; les:Ddef Etats−
Unis:NP &lt;/NP&gt; &lt;/PP&gt; ou:CC
&lt;PP&gt; à:P &lt;NP&gt; le:Ddef
Japon:NP&lt;/NP&gt;&lt;/PP&gt; &lt;/COORD&gt;
&lt;/REL&gt; &lt;PONCT&gt; .:PONCT&lt;/PONCT&gt;
&lt;/SENT&gt;
&lt;SENT&gt; &lt;NP&gt; Les:Dmp pays:NC&lt;/NP&gt;
&lt;NP&gt; les:Ddef plus:ADV efficaces:Aqual
6conomiquement:ADV&lt;/NP&gt;
&lt;VN&gt; sont:VP&lt;/VN&gt;
&lt;NP&gt; ceux:PROdem&lt;/NP&gt;
&lt;REL&gt; qui:PROR
&lt;VN&gt; ont:VP&lt;/VN&gt;
&lt;NP&gt; les:Ddef travailleurs:NC les:Ddef
mieux:ADV&lt;/NP&gt;
&lt;VN&gt; form6s:VK&lt;/VN&gt;
&lt;/REL&gt; &lt;PONCT&gt; .:PONCT&lt;/PONCT&gt;
&lt;/SENT&gt;
&lt;SENT&gt; &lt;ADVP&gt; D’autre_part:ADV&lt;/ADVP&gt;
&lt;PONCT&gt; ,:PONCT &lt;/PONCT&gt;
&lt;SUB&gt; si:CS
&lt;VN&gt; nous:CL voulons:VP demeurer:VW
&lt;/VN&gt;
&lt;NP&gt; une:Dind grande_puissance:NC&lt;/NP&gt;
&lt;/SUB&gt; &lt;PONCT&gt; ,:PONCT&lt;/PONCT&gt;
&lt;VN&gt; nous:CL devons:VP rester:VW&lt;/VN&gt;
&lt;NP&gt; un:Dind pays:NC&lt;/NP&gt;
&lt;REL&gt; qui:PROR
&lt;VN&gt; cr6e:VP&lt;/VN&gt;
&lt;NP&gt; le:Ddef savoir:NC&lt;/NP&gt;
&lt;/REL&gt;&lt;PONCT&gt; .:PONCT&lt;/PONCT&gt;
&lt;/SENT&gt;
&lt;SENT&gt; &lt;COORD&gt; Et:CC
&lt;PP&gt; pour:P &lt;NP&gt; cela:PROdem&lt;/NP&gt;
&lt;/PP&gt; &lt;/COORD&gt;
&lt;PONCT&gt; ,:PONCT &lt;/PONCT&gt;
&lt;VN&gt; il:CL faut:VP&lt;/VN&gt;
&lt;NP&gt; un:Dind
enseignement_sup6rieur:NC fort:Aqual&lt;/NP&gt;
&lt;PONCT&gt; .:PONCT&lt;/PONCT&gt; &lt;/SENT&gt;
&lt;SENT&gt; &lt;COORD&gt; Mais:CC
&lt;PP&gt; en_dehors_de:P
&lt;NP&gt; ces:Ddem raisons:NC
6conomiques:Aqual ou:CC
philosophiques:Aqual &lt;/NP&gt; &lt;/PP&gt;
&lt;/COORD&gt;
&lt;PONCT&gt; ,:PONCT &lt;/PONCT&gt;
&lt;VN&gt; il:CL y:CL a:VP &lt;/VN&gt;
&lt;NP&gt; la:Ddef r6alit6:NC &lt;/NP&gt;
&lt;NP&gt; les:Ddef 6tudiants:NC&lt;/NP&gt;
&lt;VN&gt; sont:VP&lt;/VN&gt;
&lt;PP&gt; à:P &lt;NP&gt; notre:Dposs porte:NC&lt;/NP&gt;
&lt;/PP&gt; &lt;PONCT&gt; .:PONCT&lt;/PONCT&gt; &lt;/SENT&gt;
</figure>
<figureCaption confidence="0.479906">
FIGURE 2 : Sample output for French
</figureCaption>
<bodyText confidence="0.997308631578948">
When we began our task, we had at our disposal
a 1 million word POS tagged and hand−
corrected corpus (Abeill6 &amp; Cl6ment 98). The
corpus was meant to be syntactically annotated
for constituency. To achieve this, precise
annotation guidelines for constituency had been
written and a portion of the corpus had been
hand−annotated (independently of the
development of the shallow−parser) to test the
guidelines (approx. 25 000 words) .
To evaluate the shallow parser, we
performed as described at the beginning of
section 3 : We parsed the 1 million words. We
set aside 500 sentences (approx. 15 000 words)
for quickly tuning our rules. We also set aside
the 25 000 words that had been independently
annotated in order to compare the output of the
parser to a portion of the final Treebank. In
addition, an annotator hand−corrected the output
of the shallow−parser on 1000 new randomly
chosen sentences (approx. 30 000 words).
Contrary to the 25 000 words which constituted
the beginning of the Treebank, for these 30 000
words verb arguments, PPs and modifiers were
not attached. Finally, we extracted bare non−
recursive NPs from the 25 000 words, in order
to evaluate how the parser did on this particular
task.
When compared to the hand−corrected
parser’s output, for opening brackets we obtain
a recall of 94.3 % and a precision of 95.2%. For
closing brackets, we obtain a precision of 92.2
% and a recall of 91.4 %. Moreover, 95.6 % of
the correctly placed brackets are labeled
correctly, the remaining 4.4% are not strictly
speaking labeled incorrectly, since they are
labeled INC (i.e. unknown) These unknown
constituents, rather then errors, constitute a
mechanism of underspecification (the idea being
to assign as little wrong information as
possible)11.
When compared to the 25 000 words of the
Treebank, For opening brackets, the recall is
92.9% and the precision is 94%. For closing
brackets, the recall is 62,8% and the precision is
65%. These lower results are normal, since the
Treebank contains attachments that the parser is
not supposed to make.
Finally, on the specific task of identifying non−
recursive NP−chunks, we obtain a recall of 96.6
% and a precision of 95.8 %. for opening
11 These underspecified label can be removed at a deeper
parsing stage, or one can add a guesser .
brackets, and a recall and precision of resp.
94.3% and 92.9 % for closing brackets.
To give an idea about the coverage of the
parser, sentences are on average 30 words long
and comprise 20.6 opening brackets (and thus as
many closing brackets). Errors difficult to
correct with access to a limited context involve
mainly &amp;quot;missing&amp;quot; brackets (e.g. &amp;quot;comptez vous
* ne pas le traiter&amp;quot; (do you expect not to treat
him) appears as single constituent, while there
should be 2) , while &amp;quot;spurious&amp;quot; brackets can
often be eliminated by adding more rules (e.g.
for multiple prepositions : &amp;quot;de chez&amp;quot;). Most
errors for closing brackets are due to clause
boundaries(i.e. SUB, COORD and REL).
To obtain these results, we had to write only
48 rules.
Concerning speed, as argued in (Tapanainen
&amp; Järvinen, 94), we found that rule−based
systems are not necessarily slow, since the 1
million words are parsed in 3mn 8 seconds.
One can compare this to (Ait−Moktar &amp;
Chanod 97), who, in order to shallow−parse
French resort to 14 networks and parse
150words /sec (Which amounts to approx. 111
minutes for one million words)12. It is difficult to
compare our result to other results, since most
Shallow−parsers pursue different tasks, and use
different evaluation metrics. However to give an
idea, standard techniques typically produce an
output for one million words in 20 mn and
report a precision and a recall ranging from 70%
to 95% depending on the language, kind of text
and task. Again, we are not saying that our
technique obtains best results, but simply that it
is fast and easy to use for unrestricted text for
any language. To give a better idea to the reader,
we provide an output of the Shallow−parser for
French on figure 2.
In order to improve our tool and our rules, a
demo is available online on the author’s
homepage.
</bodyText>
<subsectionHeader confidence="0.999421">
3.2 A Shallow−Parser for English
</subsectionHeader>
<bodyText confidence="0.993716728813559">
We wanted to evaluate our compiler on more
than one language, to make sure that our results
were easily replicable. So we wrote a new set of
rules for English using the PennTreebank tagset,
both for POS and for constituent labels.
12 They report a recall ranging from 82.6% and 92.6%
depending on the type of texts, and a precision of 98% for
subject recognition, but their results are not directly
comparable to ours, since the task is different.
We sat aside sections 00 and 01 of the WSJ for
evaluation (i.e. approx. 3900 sentences), and
used other sections of the WSJ for tuning our
rules.
Contrary to the French Treebank, the Penn
Treebank contains non−surfastic constructions
such as empty nodes, and constituents that are
not triggered by a lexical items.
Therefore, before evaluating our new shallow−
parser, we automatically removed from the test
sentences all opening brackets that were not
immediately followed by a lexical item, with
their corresponding closing brackets, as well as
all the constituents which contained an empty
element. We also removed all information on
pseudo−attachment. We then evaluated the
output of the shallow−parser to the test
sentences. For bare NPs, we compared our
output to the POS tagged version of the test
sentences (since bare−NPs are marked there).
For the shallow−parsing task, we obtain a
precision of 90.8% and a recall of 91% for
opening brackets, a precision of 65.7% and
recall of 66.1% for closing brackets. For the
NP−chunking task, we obtain a precision of
91% and recall of 93.2%, using an “exact
match” measure (i.e. both the opening and
closing boundaries of an NP must match to be
counted as correct).
The results, were as satisfactory as for French.
Concerning linguistic choices when writing the
rules, we didn’t really make any, and simply
followed closely those of the Penn Treebank
syntactic annotation guidelines (modulo the
embeddings, the empty categories and pseudo−
attachments mentioned above).
Concerning the number of rules, we used 54 of
them in order to detect all constituents, and 27
rules for NP−chunks identification. . In sections
00 and 01 of the wsj there were 24553 NPs,
realized as 1200 different POS patterns (ex : CD
NN, DT $ JJ NN, DT NN...). Even though
these 1200 patterns corresponded to a lower
number of regular expressions, a standard
finite−state approach would have to resort to
more than 27 rules. One can also compare this
result to the one reported in (Ramshaw &amp;
Marcus 95) who, obtain up to 93.5% recall and
93.1% precision on the same task, but using
between 500 and 2000 rules.
</bodyText>
<subsectionHeader confidence="0.994979">
3.3 Tolerance to POS errors
</subsectionHeader>
<bodyText confidence="0.999980238095238">
To test the tolerance to POS tagging errors,
we have extracted the raw text from the English
corpus from section 3.2., and retagged it using
the publicly available tagger TreeTagger
(Schmid, 94). without retraining it. The authors
of the tagger advertise an error−rate between 3
and 4%. We then ran the NP−chunker on the
output of the tagger, and still obtain a precision
of 90.2% and a recall of 92% on the “exact
match” NP identification task: the fact that our
tool does not rely on regular expressions
describing &amp;quot;full constituent patterns&amp;quot; allows to
ignore some POS errors since mistagged words
which do not appear at constituent boundaries
(i.e. essentially lexical words) have no
influence on the output. This improves accuracy
and robustness. For example, if &amp;quot;first&amp;quot; has been
mistagged noun instead of adjective in [NP the
first man J on the moon ..., it won’t prevent
detecting the NP, as long as the determiner has
been tagged correctly.
</bodyText>
<sectionHeader confidence="0.930447" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999756121212121">
We have presented a tool which allows to
generate a shallow−parser for unrestricted text
in any language. This tool is based on the use of
a imited number of rules which aim at
identifying constituent boundaries. We then
presented evaluations on French and on English,
and concluded that our tools obtains results
similar to other shallow−parsing techniques, but
in a much simpler and economical way.
We are interested in developing new sets of
rules for new languages (e.g. Portuguese and
German) and new style (e.g. French oral texts).
It would also be interesting to test the tool on
inflectional languages.
The shallow−parser for French is also being
used in the SynSem project which aims at
syntactically and semantically annotating
several millions words of French texts
distributed by ELRA13. Future improvements of
the tool will consist in adding a module to
annotate syntactic functions, and complete
valency information for verbs, with the help of a
lexicon (Kinyon, 00).
Finally, from a theoretical point of view, it may
be interesting to see if our rules could be
acquired automatically from raw text (although
this might not be worth it in practice,
considering the small number of rules we use,
and the fact that acquiring the rules in such a
way would most likely introduce errors).
Acknowledgement We especially thank F.
Toussenel, who has performed most of the
evaluation for French presented in section 3.1.1.
</bodyText>
<sectionHeader confidence="0.998455" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999122442307692">
Abeillé A. Clément L. 1999 : A tagged reference corpus
for French. Proc. LINC−EACL’99. Bergen
Abeillé A., Clément L., Kinyon A., Toussenel F. 2001
Building a Treebank for French. In Treebanks (A
Abeillé ed.). Kluwer academic publishers.
Abney S. 1991. Parsing by chunks. In Principle−based
Parsing. (R. Berwick, S. Abney and C. Tenny eds),
Kluwer academic publishers.
Aït−Mokhtar S. &amp; Chanod J.P. 1997. Incremental Finite−
State Parsing. Proc. ANLP’97, Washington,
Bourigault 1992 : Surface Grammatical analysis for the
extraction of terminological noun phrases. Proc.
COLING’92. Vol 3, pp. 977−981
Brants T., Skut W., Uszkoreit H., 1999. Syntactic
Annotation of a German Newspaper Corpus. Proc.
ATALA Treebank Workshop. Paris, France.
Daelemans W., Buchholz S., Veenstra J.. Memory−Based
Shallow Parsing.Proc.CoNLL−EACL’99
Grefenstette G.. 1996. Light Parsing as Finite−State
Filtering. Proc. ECAI ’96 workshop on &amp;quot;Extended
finite state models of language&amp;quot;.
Joshi A.K. Hopely P. 1997. A parser from antiquity. In
Extended Finite State Models of Language. (A. Kornai
ed.). University Press.
Karlsson F., Voutilainen A., Heikkil J., Antilla A. (eds.)
1995. Constraint Grammar: a language−independent
system for parsing unrestricted text. Mouton de
Gruyer.
Kinyon A. 2000. Hypertags. Proc. COLING’00.
Sarrebrucken.
Magerman D.M., 1994 Natural language parsing as
statistical pattern recognition. PhD Dissertation,
Stanford University.
Marcus M., Santorini B., and Marcinkiewicz M.A. 1993.
Building a large annotated corpus of english: The penn
treebank. Computational Linguistics, 19:313−−330.
Ramshaw, L.A. &amp; Marcus, M.P., 1995. Text Chunking
using Transformation−Based Learning, ACL Third
Workshop on Very Large Corpora, pp.82−94, 1995.
Ratnaparkhi A. 1997. linear observed time statistical
parser based on maximum entropy models. Technical
Report cmp-lg/9706014.
Tapanainen P. and Järvinen T., 1994, Syntactic Analysis
of a Natural Language Using Linguistic Rules and
Corpus−Based Patterns. Proc. COLING’94. Vol I, pp
629−634. Kyoto.
Schmid H. 1994 Probabilistic Part−Of−Speech Tagging
Using Decision Trees. Proc. NEMLAP’94.
Vergne J. 1999. Etude et modélisation de la syntaxe des
langues à l’aide de l’ordinateur. Analyse syntaxique
automatique non combinatoire. Dossier d’habilitation
à diriger des recherches. Univ. de Caen.
</reference>
<page confidence="0.886115">
13 European Language Ressources Association
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871136">
<title confidence="0.998077">A language−independent shallow−parser Compiler</title>
<author confidence="0.999932">Alexandra Kinyon</author>
<affiliation confidence="0.9941395">CIS Dpt. . University of Pennsylvania</affiliation>
<email confidence="0.999844">kinyon@linc.cis.upenn.edu</email>
<abstract confidence="0.9924703125">We present a rule−based shallow− parser compiler, which allows to generate a robust shallow−parser for any language, even in the absence of training data, by resorting to a very limited number of rules which aim at identifying constituent boundaries. We contrast our approach to other approaches used for shallow−parsing (i.e. finite−state and probabilistic methods). We present an evaluation of our tool for English (Penn Treebank) and for French (newspaper corpus &amp;quot;LeMonde&amp;quot;) for several tasks (NP−chunking &amp; &amp;quot;deeper&amp;quot; parsing) .</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abeillé A Clément L</author>
</authors>
<title>A tagged reference corpus for French.</title>
<date>1999</date>
<booktitle>Proc. LINC−EACL’99.</booktitle>
<location>Bergen</location>
<marker>L, 1999</marker>
<rawString>Abeillé A. Clément L. 1999 : A tagged reference corpus for French. Proc. LINC−EACL’99. Bergen</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abeillé</author>
<author>L Clément</author>
<author>A Kinyon</author>
<author>F Toussenel</author>
</authors>
<title>Building a Treebank for French. In Treebanks</title>
<date>2001</date>
<editor>(A Abeillé ed.).</editor>
<publisher>Kluwer academic publishers.</publisher>
<marker>Abeillé, Clément, Kinyon, Toussenel, 2001</marker>
<rawString>Abeillé A., Clément L., Kinyon A., Toussenel F. 2001 Building a Treebank for French. In Treebanks (A Abeillé ed.). Kluwer academic publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>In Principle−based</booktitle>
<publisher>Kluwer academic publishers.</publisher>
<marker>Abney, 1991</marker>
<rawString>Abney S. 1991. Parsing by chunks. In Principle−based Parsing. (R. Berwick, S. Abney and C. Tenny eds), Kluwer academic publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Aït−Mokhtar</author>
<author>J P Chanod</author>
</authors>
<date>1997</date>
<booktitle>Incremental Finite− State Parsing. Proc. ANLP’97,</booktitle>
<location>Washington,</location>
<marker>Aït−Mokhtar, Chanod, 1997</marker>
<rawString>Aït−Mokhtar S. &amp; Chanod J.P. 1997. Incremental Finite− State Parsing. Proc. ANLP’97, Washington,</rawString>
</citation>
<citation valid="true">
<title>Surface Grammatical analysis for the extraction of terminological noun phrases.</title>
<date>1992</date>
<booktitle>Proc. COLING’92.</booktitle>
<volume>3</volume>
<pages>977--981</pages>
<institution>Bourigault</institution>
<marker>1992</marker>
<rawString>Bourigault 1992 : Surface Grammatical analysis for the extraction of terminological noun phrases. Proc. COLING’92. Vol 3, pp. 977−981</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>W Skut</author>
<author>H Uszkoreit</author>
</authors>
<title>Syntactic Annotation of a German Newspaper Corpus.</title>
<date>1999</date>
<booktitle>Proc. ATALA Treebank Workshop.</booktitle>
<location>Paris, France.</location>
<marker>Brants, Skut, Uszkoreit, 1999</marker>
<rawString>Brants T., Skut W., Uszkoreit H., 1999. Syntactic Annotation of a German Newspaper Corpus. Proc. ATALA Treebank Workshop. Paris, France.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Daelemans</author>
<author>S Buchholz</author>
<author>J Veenstra</author>
</authors>
<note>Memory−Based Shallow Parsing.Proc.CoNLL−EACL’99</note>
<marker>Daelemans, Buchholz, Veenstra, </marker>
<rawString>Daelemans W., Buchholz S., Veenstra J.. Memory−Based Shallow Parsing.Proc.CoNLL−EACL’99</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Light Parsing as Finite−State Filtering.</title>
<date>1996</date>
<booktitle>Proc. ECAI ’96</booktitle>
<marker>Grefenstette, 1996</marker>
<rawString>Grefenstette G.. 1996. Light Parsing as Finite−State Filtering. Proc. ECAI ’96 workshop on &amp;quot;Extended finite state models of language&amp;quot;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshi A K Hopely P</author>
</authors>
<title>A parser from antiquity.</title>
<date>1997</date>
<booktitle>In Extended Finite State Models of Language.</booktitle>
<editor>(A. Kornai ed.).</editor>
<publisher>University Press.</publisher>
<marker>P, 1997</marker>
<rawString>Joshi A.K. Hopely P. 1997. A parser from antiquity. In Extended Finite State Models of Language. (A. Kornai ed.). University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
<author>A Voutilainen</author>
<author>J Heikkil</author>
<author>A Antilla</author>
</authors>
<title>Constraint Grammar: a language−independent system for parsing unrestricted text. Mouton de Gruyer.</title>
<date>1995</date>
<marker>Karlsson, Voutilainen, Heikkil, Antilla, 1995</marker>
<rawString>Karlsson F., Voutilainen A., Heikkil J., Antilla A. (eds.) 1995. Constraint Grammar: a language−independent system for parsing unrestricted text. Mouton de Gruyer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kinyon</author>
</authors>
<date>2000</date>
<booktitle>Hypertags. Proc. COLING’00. Sarrebrucken.</booktitle>
<marker>Kinyon, 2000</marker>
<rawString>Kinyon A. 2000. Hypertags. Proc. COLING’00. Sarrebrucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Natural language parsing as statistical pattern recognition.</title>
<date>1994</date>
<tech>PhD</tech>
<institution>Dissertation, Stanford University.</institution>
<marker>Magerman, 1994</marker>
<rawString>Magerman D.M., 1994 Natural language parsing as statistical pattern recognition. PhD Dissertation, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics,</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus M., Santorini B., and Marcinkiewicz M.A. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19:313−−330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text Chunking using Transformation−Based Learning,</title>
<date>1995</date>
<booktitle>ACL Third Workshop on Very Large Corpora, pp.82−94,</booktitle>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Ramshaw, L.A. &amp; Marcus, M.P., 1995. Text Chunking using Transformation−Based Learning, ACL Third Workshop on Very Large Corpora, pp.82−94, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<tech>Technical Report cmp-lg/9706014.</tech>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Ratnaparkhi A. 1997. linear observed time statistical parser based on maximum entropy models. Technical Report cmp-lg/9706014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>T Järvinen</author>
</authors>
<title>Syntactic Analysis of a Natural Language Using Linguistic Rules and Corpus−Based Patterns.</title>
<date>1994</date>
<booktitle>Proc. COLING’94. Vol I,</booktitle>
<pages>629--634</pages>
<marker>Tapanainen, Järvinen, 1994</marker>
<rawString>Tapanainen P. and Järvinen T., 1994, Syntactic Analysis of a Natural Language Using Linguistic Rules and Corpus−Based Patterns. Proc. COLING’94. Vol I, pp 629−634. Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic Part−Of−Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>Proc. NEMLAP’94.</booktitle>
<marker>Schmid, 1994</marker>
<rawString>Schmid H. 1994 Probabilistic Part−Of−Speech Tagging Using Decision Trees. Proc. NEMLAP’94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Vergne</author>
</authors>
<title>Etude et modélisation de la syntaxe des langues à l’aide de l’ordinateur. Analyse syntaxique automatique non combinatoire. Dossier d’habilitation à diriger des recherches.</title>
<date>1999</date>
<institution>Univ. de Caen.</institution>
<marker>Vergne, 1999</marker>
<rawString>Vergne J. 1999. Etude et modélisation de la syntaxe des langues à l’aide de l’ordinateur. Analyse syntaxique automatique non combinatoire. Dossier d’habilitation à diriger des recherches. Univ. de Caen.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>