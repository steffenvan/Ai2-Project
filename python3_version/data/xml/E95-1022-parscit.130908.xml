<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.98198">
A syntax-based part-of-speech analyser
</title>
<author confidence="0.494178">
Atro Voutilainen
</author>
<affiliation confidence="0.542308333333333">
Research Unit for Multilingual Language Technology
P.O. Box 4
FIN-00014 University of Helsinki
</affiliation>
<address confidence="0.695025">
Finland
</address>
<email confidence="0.994149">
Atro.Voutilainen@Helsinki.FI
</email>
<sectionHeader confidence="0.998533" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999283043478261">
There are two main methodologies for
constructing the knowledge base of a
natural language analyser: the linguis-
tic and the data-driven. Recent state-of-
the-art part-of-speech taggers are based
on the data-driven approach. Because
of the known feasibility of the linguis-
tic rule-based approach at related levels
of description, the success of the data-
driven approach in part-of-speech analy-
sis may appear surprising. In this paper,
a case is made for the syntactic nature
of part-of-speech tagging. A new tagger
of English that uses only linguistic dis-
tributional rules is outlined and empiri-
cally evaluated. Tested against a bench-
mark corpus of 38,000 words of previ-
ously unseen text, this syntax-based sys-
tem reaches an accuracy of above 99%.
Compared to the 95-97% accuracy of its
best competitors, this result suggests the
feasibility of the linguistic approach also
in part-of-speech analysis.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99914175">
Part-of-speech analysis usually consists of (i) in-
troduction of ambiguity (lexical analysis) and (ii)
disambiguation (elimination of illegitimate alter-
natives). While introducing ambiguity is regarded
as relatively straightforward, disambiguation is
known to be a difficult and controversial problem.
There are two main methodologies: the linguistic
and the data-driven.
</bodyText>
<listItem confidence="0.957935">
• In the linguistic approach, the generalisa-
tions are based on the linguist&apos;s (poten-
tially corpus-based) abstractions about the
paradigms and syntagms of the language.
Distributional generalisations are manually
coded as a grammar, a system of constraint
rules used for discarding contextually illegit-
imate analyses. The linguistic approach is
labour-intensive: skill and effort is needed for
writing an exhaustive grammar.
• In the data-driven approach, frequency-based
information is automatically derived from
corpora. The learning corpus can consist of
plain text, but the best results seem achiev-
able with annotated corpora (Merialdo 1994;
Elworthy 1994). This corpus-based informa-
tion typically concerns sequences of 1-3 tags
or words (with some well-known exceptions,
e.g. Cutting et al. 1992). Corpus-based infor-
mation can be represented e.g. as neural net-
works (Eineborg and Gamback 1994; Schmid
1994), local rules (Brill 1992), or collocational
matrices (Garside 1987). In the data-driven
approach, no human effort is needed for rule-
writing. However, considerable effort may be
needed for determining a workable tag set (cf.
Cutting 1994) and annotating the training
corpus.
</listItem>
<bodyText confidence="0.993505">
At the first flush, the linguistic approach may
seem an obvious choice. A part-of-speech tagger&apos;s
task is often illustrated with a noun—verb ambigu-
ous word directly preceded by an unambiguous de-
terminer (e.g. table in the table). This ambiguity
can reliably be resolved with a simple and obvious
grammar rule that disallows verbs after determin-
ers.
Indeed, few contest the fact that reliable linguis-
tic rules can be written for resolving some part-
of-speech ambiguities. The main problem with
this approach seems to be that resolving part-of-
speech ambiguities on a large scale, without intro-
ducing a considerable error margin, is very diffi-
cult at best. At least, no rule-based system with
a convincing accuracy has been reported so far.&apos;
As a rule, data-driven systems rely on statisti-
cal generalisations about short sequences of words
or tags. Though these systems do not usually
employ information about long-distance phenom-
</bodyText>
<footnote confidence="0.959311285714286">
&apos;There is one potential exception: the rule-based
morphological disambiguator used in the English
Constraint Grammar Parser ENGCG (Voutilainen,
lieikld15. and Anttila 1992). Its recall is very high
(99.7% of all words receive the correct morphologi-
cal analysis), but this system leaves 3-7% of all words
ambiguous, trading precision for recall.
</footnote>
<page confidence="0.995841">
157
</page>
<bodyText confidence="0.998854931034483">
ena or the linguist&apos;s abstraction capabilities (e.g.
knowledge about what is relevant in the context),
they tend to reach a 95-97% accuracy in the anal-
ysis of several languages, in particular English
(Marshall 1983; Black et al. 1992; Church 1988;
Cutting et al. 1992; de Marcken 1990; DeRose
1988; Hindle 1989; Merialdo 1994; Weischedel et
al. 1993; Brill 1992; Samuelsson 1994; Eineborg
and Gamback 1994, etc.). Interestingly, no sig-
nificant improvement beyond the 97% &amp;quot;barrier&amp;quot;
by means of purely data-driven systems has been
reported so far.
In terms of the accuracy of known systems,
the data-driven approach seems then to pro-
vide the best model of part-of-speech distribu-
tion. This should appear a little curious because
very competitive results have been achieved us-
ing the linguistic approach at related levels of de-
scription. With respect to computational mor-
phology, witness for instance the success of the
Two-Level paradigm introduced by Koskenniemi
(1983): extensive morphological descriptions have
been made of more than 15 typologically dif-
ferent languages (Kimmo Koskenniemi, personal
communication). With regard to computational
syntax, see for instance (Giingordii and Oflazer
1994; Hindle 1983; Jensen, Heidorn and Richard-
son (eds.) 1993; McCord 1990; Sleator and Tem-
perley 1991; Alshawi (ed.) 1992; Strzalkowski
1992). The present success of the statistical ap-
proach in part-of-speech analysis seems then to
form an exception to the general feasibility of the
rule-based linguistic approach. Is the level of parts
of speech somehow different, perhaps less rule-
governed, than related levels?2
We do not need to assume this idiosyncratic sta-
tus entirely. The rest of this paper argues that also
parts of speech can be viewed as a rule-governed
phenomenon, possible to model using the linguis-
tic approach. However, it will also be argued that
though the distribution of parts of speech can to
some extent be described with rules specific to this
level of representation, a more natural account
could be given using rules overtly about the form
and function of essentially syntactic categories. A
syntactic grammar appears to predict the distri-
bution of parts of speech as a &amp;quot;side effect&amp;quot;. In this
sense parts of speech seem to differ from morphol-
ogy and syntax: their status as an independent
level of linguistic description appears doubtful.
Before proceeding further with the main argu-
ment, consider three very recent hybrids — sys-
tems that employ linguistic rules for resolving
some of the ambiguities before using automati-
cally generated corpus-based information: collo-
cation matrices (Leech, Garside and Bryant 1994),
Hidden Markov Models (Tapanainen and Voutilai-
nen 1994), or syntactic patterns (Tapanainen and
</bodyText>
<footnote confidence="0.992231">
2For related discussion, cf. Sampson (1987) and
Church (1992).
</footnote>
<bodyText confidence="0.998331166666667">
Jarvinen 1994). What is interesting in these hy-
brids is that they, unlike purely data-driven tag-
gers, seem capable of exceeding the 97% barrier:
all three report an accuracy of about 98.5%.3 The
success of these hybrids could be regarded as evi-
dence for the syntactic aspects of parts of speech.
However, the above hybrids still contain a data-
driven component, i.e. it remains an open question
whether a tagger entirely based on the linguistic
approach can compare with a data-driven system.
Next, a new system with the following properties
is outlined and evaluated:
</bodyText>
<listItem confidence="0.9167055">
• The tagger uses only linguistic distributional
rules.
• Tested against a 38,000-word corpus of previ-
ously unseen text, the tagger reaches a better
accuracy than previous systems (over 99%).
• At the level of linguistic abstraction, the
grammar rules are essentially syntactic. Ide-
ally, part-of-speech disambiguation should
fall out as a &amp;quot;side effect&amp;quot; of syntactic anal-
ysis.
</listItem>
<bodyText confidence="0.9997968">
Section 2 outlines a rule-based system consist-
ing of the ENGCG tagger followed by a finite-
state syntactic parser (Voutilainen and Tapanai-
nen 1993; Voutilainen 1994) that resolves remain-
ing part-of-speech ambiguities as a side effect.
In Section 3, this rule-based system is tested
against a 38,000-word corpus of previously unseen
text. Currently tagger evaluation is only becom-
ing standardised; the evaluation method is accord-
ingly reported in detail.
</bodyText>
<sectionHeader confidence="0.917388" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.990114">
The tagger consists of the following sequential
components:
</bodyText>
<listItem confidence="0.999915857142857">
• Tokeniser
• ENGCG morphological analyser
— Lexicon
— Morphological heuristics
• ENGCG morphological disambiguator
• Lookup of alternative syntactic tags
• Finite state syntactic disambiguator
</listItem>
<subsectionHeader confidence="0.999903">
2.1 Morphological analysis
</subsectionHeader>
<bodyText confidence="0.999878">
The tokeniser is a rule-based system for identify-
ing words, punctuation marks, document markers,
and fixed syntagms (multiword prepositions, cer-
tain compounds etc.).
The morphological description consists of two
rule components: (i) the lexicon and (ii) heuristic
rules for analysing unrecognised words.
The English Koskenniemi-style lexicon contains
over 80,000 lexical entries, each of which repre-
sents all inflected and some derived surface forms.
</bodyText>
<footnote confidence="0.979306">
&apos;However, CLAWS4 (Leech, Garside and Bryant
1994) leaves some ambiguities unresolved; it uses port-
manteau tags for representing them.
</footnote>
<page confidence="0.995914">
158
</page>
<bodyText confidence="0.999746">
The lexicon employs 139 tags mainly for part of
speech, inflection and derivation; for instance:
</bodyText>
<equation confidence="0.989949833333334">
&amp;quot;&lt;that&gt;&amp;quot;
&amp;quot;that&amp;quot; &lt;**CLB&gt; CS
&amp;quot;that&amp;quot; DET CENTRAL DEM SG
&amp;quot;that&amp;quot; ADV
&amp;quot;that&amp;quot; PRON DEM SG
&amp;quot;that&amp;quot; &lt;Rel&gt; PRON SG/PL
</equation>
<bodyText confidence="0.999922090909091">
The morphological analyser produces about
180 different tag combinations. To contrast
the ENGCG morphological description with
the well-known Brown Corpus tags: ENGCG
is more distinctive in that a part-of-speech
distinction is spelled out in the description
of (i) determiner—pronoun, (ii) preposition—
conjunction, (iii) determiner—adverb—pronoun,
and (iv) subjunctive—imperative—infinitive—pres-
ent tense homographs. On the other hand,
ENGCG does not spell out part-of-speech ambi-
guity in the description of (i) -ing and nonfinite
-ed forms, (ii) noun—adjective homographs with
similar core meanings, or (iii) abbreviation—proper
noun—common noun homographs.
&amp;quot;Morphological heuristics&amp;quot; is a rule-based mod-
ule for the analysis of those 1-5% of input words .
not represented in the lexicon. This module em-
ploys ordered hand-grafted rules that base their
analyses on word shape. If none of the pattern
rules apply, a nominal reading is assigned as a de-
fault.
</bodyText>
<subsectionHeader confidence="0.997755">
2.2 ENGCG disarnbiguator
</subsectionHeader>
<bodyText confidence="0.999938170212766">
A Constraint Grammar can be viewed as a
collection4 of pattern—action rules, no more than
one for each ambiguity-forming tag. Each rule
specifies one or more context patterns, or &amp;quot;con-
straints&amp;quot;, where the tag is illegitimate. If any of
these context patterns are satisfied during disam-
biguation, the tag is deleted; otherwise it is left in-
tact. The context patterns can be local or global,
and they can refer to ambiguous or unambiguous
analyses. During disambiguation, the context can
become less ambiguous. To help a pattern defining
an unambiguous context match, several passes are
made over the sentence during disambiguation.
The current English grammar contains 1,185
linguistic constraints on the linear order of mor-
phological tags. Of these, 844 specify a context
that extends beyond the neighboring word; in this
limited sense, 71% of the constraints are global.
Interestingly, the constraints are partial and of-
ten negative paraphrases of 23 general, essentially
syntactic generalisations about the form of the
noun phrase, the prepositional phrase, the finite
verb chain etc. (Voutilainen 1994).
&apos;Actually, it is possible to define additional heuris-
tic rule collections that can optionally be applied af-
ter the more reliable ones for resolving remaining
ambiguities.
The grammar avoids risky&apos;predictions, therefore
3-7% of all words remain ambiguous (an average
1.04-1.08 alternative analyses per output word).
On the other hand, at least 99.7% of all words
retain the correct morphological analysis. Note in
passing that the ratio 1.04-1.08/99.7% compares
very favourably with other systems; c.f. 3.0/99.3%
by POST (Weischedel et al. 1993) and 1.04/97.6%
or 1.09/98.6% by de Marcken (1990).
There is an additional collection of 200 option-
ally applicable heuristic constraints that are based
on simplified linguistic generalisations. They re-
solve about half of the remaining ambiguities, in-
creasing the overall error rate to about 0.5%.
Most of even the remaining ambiguities are
structurally resolvable. ENGCG leaves them
pending mainly because it is prohibitively diffi-
cult to express certain kinds of structural gener-
alisation using the available rule formalism and
grammatical representation.
</bodyText>
<subsectionHeader confidence="0.999689">
2.3 Syntactic analysis
</subsectionHeader>
<subsubsectionHeader confidence="0.640879">
2.3.1 Finite-State Intersection Grammar
</subsubsectionHeader>
<bodyText confidence="0.995598166666667">
Syntactic analysis is carried out in another re-
ductionistic parsing framework known as Finite-
State Intersection Grammar (Koskenniemi 1990;
Koskenniemi, Tapanainen and Voutilainen 1992;
Tapanainen 1992; Voutilainen and Tapanainen
1993; Voutilainen 1994). A short introduction:
</bodyText>
<listItem confidence="0.9470184">
• Also here syntactic analysis means resolu-
tion of structural ambiguities. Morphologi-
cal, syntactic and clause boundary descrip-
tors are introduced as ambiguities with sim-
ple mappings; these ambiguities are then re-
solved in parallel.
• The formalism does not distinguish between
various types of ambiguity; nor are ambiguity
class specific rule sets needed. A single rule
often resolves all types of ambiguity, though
superficially it may look e.g. like a rule about
syntactic functions.
• The grammarian can define constants and
predicates using regular expressions. For in-
stance, the constants &amp;quot;.&amp;quot; and &amp;quot;..&amp;quot; accept any
features within a morphological reading and
a finite clause (that may even contain centre-
embedded clauses), respectively. Constants
and predicates can be used in rules, e.g. im-
plication rules that are of the form
</listItem>
<equation confidence="0.9954995">
X =&gt;
LC1 _ RC1,
LC2 _ RC2,
LCn _ RCn;
</equation>
<bodyText confidence="0.9939396">
Here X, LCI, RC1, LC2 etc. are regular ex-
pressions. The rule reads: &amp;quot;X is legitimate
only if it occurs in context LC1 _ RC1 or in
context LC2 _ RC2 ... or in context LCn
RCn&amp;quot; .
</bodyText>
<page confidence="0.989301">
159
</page>
<listItem confidence="0.993883">
• Also the ambiguous sentences are represented
as regular expressions.
• Before parsing, rules and sentences are com-
piled into deterministic finite-state automata.
• Parsing means intersecting the (ambiguous)
sentence automaton with each rule automa-
ton. Those sentence readings accepted by all
rule automata are proposed as parses.
• In addition, heuristic rules can be used for
ranking alternative analyses accepted by the
strict rules.
</listItem>
<subsubsectionHeader confidence="0.692793">
2.3.2 Grammatical representation
</subsubsectionHeader>
<figureCaption confidence="0.9370308">
The grammatical representation used in the
Finite State framework is an extension of the
ENGCG syntax. Surface-syntactic grammatical
relations are encoded with dependency-oriented
functional tags. Functional representation of
phrases and clauses has been introduced to fa-
cilitate expressing syntactic generalisations. The
representation is introduced in (Voutilainen and
Tapanainen 1993; Voutilainen 1994); here, only
the main characteristics are given:
</figureCaption>
<listItem confidence="0.942414777777778">
• Each word boundary is explicitly represented
as one of five alternatives:
— the sentence boundary &amp;quot;@@&amp;quot;
— the boundary separating juxtaposed fi-
nite clauses &amp;quot;@/&amp;quot;
— centre-embedded (sequences of) finite
clauses are flanked with &amp;quot;@&lt;&amp;quot; and &amp;quot;@&gt;&amp;quot;
— the plain word boundary &amp;quot;A&amp;quot;
• Each word is furnished with a tag indicating a
</listItem>
<bodyText confidence="0.9036348">
surface-syntactic function (subject, premodi-
fier, auxiliary, main verb, adverbial, etc.). All
main verbs are furnished with two syntactic
tags, one indicating its main verb status, the
other indicating the function of the clause.
</bodyText>
<listItem confidence="0.987576625">
• An explicit difference is made between finite
and nonfinite clauses. Members in nonfinite
clauses are indicated with lower case tags; the
rest with upper case.
• In addition to syntactic tags, also morpholog-
ical, e.g. part-of-speech tags are provided for
each word. Let us illustrate with a simplified
example.
</listItem>
<table confidence="0.954888388888889">
ee
Mary N OSUBJ
told V CMV MCC C
the DET G&gt;N
fat A G&gt;N
butcher&apos;s N G&gt;N
wife N GIOBJ
and CC GCC
daughters N GIOBJ Cl
that CS GCS
she PRON GSUBJ
remembers V CMV OBJG G
seeing V Gmv OBJC
a DET G&gt;N
dream fobj
last DET G&gt;N
night GADVL
Gfullstop
</table>
<bodyText confidence="0.970835034482758">
Here Mary is a subject in a finite clause
(hence the upper case); told is a main verb in
a main clause; the, fat and butcher&apos;s are pre-
modifiers; wife and daughters are indirect ob-
jects; that is a subordinating conjunction; re-
members is a main verb in a finite clause that
serves the Object role in a finite clause (the
regent being told); seeing is a main verb in a
nonfinite clause (hence the lower case) that
also serves the Object role in a finite clause;
dream is an object in a nonfinite clause; night
is an adverbial. Because only boundaries sep-
arating finite clauses are indicated, there is
only one sentence-internal clause boundary,
&amp;quot;@/&amp;quot; between daughters and that.
This kind of representation seeks to be (i) suf-
ficiently expressive for stating grammatical gener-
alisations in an economical and transparent fash-
ion and (ii) sufficiently underspecific to make for
a structurally resolvable grammatical representa-
tion. For example, the present way of functionally
accounting for clauses enables the grammarian to
express rules about the coordination of formally
different but functionally similar entities. Regard-
ing the resolvability requirement, certain kinds of
structurally unresolvable distinctions are never in-
troduced. For instance, the premodifier tag @&gt;N
only indicates that its head is a nominal in the
right hand context.
</bodyText>
<subsectionHeader confidence="0.760103">
2.3.3 - A sample rule
</subsectionHeader>
<bodyText confidence="0.923429">
Here is a realistic implication rule that partially
defines the form of prepositional phrases:
</bodyText>
<table confidence="0.518866666666667">
PREP =&gt;
_ . G Coord,
_ ..PrepComp,
PassVChain.. &lt;Deferred&gt;
PostModiCl.. &lt;Deferred&gt;
WH-Question.. &lt;Deferred&gt; -;
</table>
<bodyText confidence="0.99712125">
A preposition is followed by a coordination or a
preposition complement (here hidden in the con-
stant ..PrepComp that accepts e.g. noun phrases,
nonfinite clauses and nominal clauses), or it (as
a &apos;deferred&apos; preposition) is preceded by a pas-
sive verb chain PassVChain.. or a postmodifying
clause PostModiCl.. (the main verb in a postmod-
dying clause is furnished with the postmodifier
tag N&lt;@) or of a WH-question (i.e. in the same
clause, there is a WH-word). If the tag PREP oc-
curs in none of the specified contexts, the sentence
reading containing it is discarded.
A comprehensive parsing grammar is under de-
velopment. Currently it accounts for all major
syntactic structures of English, but in a somewhat
underspecific fashion. Though the accuracy of the
</bodyText>
<page confidence="0.992385">
160
</page>
<bodyText confidence="0.99990475">
grammar at the level of syntactic analysis can still
be considerably improved, the syntactic grammar
is already capable of resolving morphological am-
biguities left pending by ENGCG.
</bodyText>
<sectionHeader confidence="0.829211" genericHeader="method">
3 An experiment with
part-of-speech disambiguation
</sectionHeader>
<bodyText confidence="0.99935865">
The system was tested against a 38,202-word test
corpus consisting of previously unseen journalis-
tic, scientific and manual texts.
The finite-state parser, the last module in the
system, can in principle be &amp;quot;forced&amp;quot; to produce
an unambiguous analysis for each input sentence,
even for ungrammatical ones. In practice, the
present implementation sometimes fails to give an
analysis to heavily ambiguous inputs, regardless
of their grammaticality.&apos; Therefore two kinds of
output were accepted for the evaluation: (0 the
unambiguous analyses actually proposed by the
finite-state parser, and (ii) the ENGCG analysis
of those sentences for which the finite-state parser
gave no analyses. From this nearly unambiguous
combined output, the success of the hybrid was
measured, by automatically comparing it with a
benchmark version of the test corpus at the level.
of morphological (including part-of-speech) anal-
ysis (i.e. the syntax tags were ignored).
</bodyText>
<subsectionHeader confidence="0.999865">
3.1 Creation of benchmark corpus
</subsectionHeader>
<bodyText confidence="0.999992176470588">
The benchmark corpus was created by first apply-
ing the preprocessor and morphological analyser
to the test text. This morphologically analysed
ambiguous text was then independently disam-
biguated by two experts whose task also was to de-
tect any errors potentially produced by the previ-
ously applied components. They worked indepen-
dently, consulting written documentation of the
grammatical representation when necessary. Then
these manually disambiguated versions were au-
tomatically compared. At this stage, slightly over
99% of all analyses were identical. When the dif-
ferences were collectively examined, it was agreed
that virtually all were due to inattention.6 One
of these two corpus versions was modified to rep-
resent the consensus, and this &apos;consensus corpus&apos;
was used as the benchmark in the evaluation.7
</bodyText>
<subsectionHeader confidence="0.868803">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999966333333333">
The results are given in Figure 1 (next page).
Let us examine the results. ENGCG accuracy
was close to normal, except that the heuristic con-
</bodyText>
<footnote confidence="0.959715875">
5During the intersection, the sentence automaton
sometimes becomes prohibitively large.
60nly in the analysis of a few headings, different
(meaning-level) interpretations arose, and even here
it was agreed by both judges that this ambiguity was
genuine.
7If this high consensus level appears surprising, see
Voutilainen and Jarvinen (this volume).
</footnote>
<bodyText confidence="0.998395294117647">
straints (tagger D2) performed somewhat poorer
than usual.
The finite-state parser gave an analysis to about
80% of all words. Overall, 0.6% of all words re-
mained ambiguous (due to the failure of the Finite
State parser; c.f. Section 3). Parsing speed varied
greatly (0.1-150 words/sec.) — refinement of the
Finite State software is still underway.
The overall success of the system is very encour-
aging — 99.26% of all words retained the correct
morphological analysis. Compared to the 95-97%
accuracy of the best competing probabilistic part-
of-speech taggers, this accuracy, achieved with an
entirely rule-based description, suggests that part-
of-speech disambiguation is a syntactic problem.
The misanalyses have not been studied in detail,
but some general observations can be made:
</bodyText>
<listItem confidence="0.9978357">
• Many misanalyses made by the Finite State
parser were due to ENGCG misanalyses (the
&amp;quot;domino effect&amp;quot;).
• The choice between adverbs and other cate-
gories was sometimes difficult. The distribu-
tions of adverbs and certain other categories
overlaps; this may explain this error type.
Lexeme-oriented constraints could be formu-
lated for some of these cases.
• Some ambiguities, e.g. noun—verb and
</listItem>
<bodyText confidence="0.998095727272727">
participle—past tense, were problematic. This
is probably due to the fact that while the
parsing grammar always requires a regent for
a dependent, it is much more permissive on
dependentless regents. Clause boundaries,
and hence the internal structure of clauses,
could probably be determined more accu-
rately if the heuristic part of the grammar
also contained rules for preferring e.g. verbs
with typical complements over verbs without
complements.
</bodyText>
<sectionHeader confidence="0.999167" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999899473684211">
Part-of-speech disambiguation has recently been
tackled best with data-driven techniques. Lin-
guistic techniques have done well at related lev-
els (morphology, syntax) but not here. Is there
something in parts of speech that makes them less
accessible to the rule-based linguistic approach?
This paper outlines and evaluates a new part-
of-speech tagger. It uses only linguistic distribu-
tional rules, yet reaches an accuracy clearly better
than any competing system. This suggests that
also parts of speech are a rule-governed distribu-
tional phenomenon.
The tagger has two rule components. One is
a grammar specifically developed for resolution
of part-of-speech ambiguities. Though much ef-
fort was given to its development, it leaves many
ambiguities unresolved. These rules, superficially
about parts of speech, actually express essentially
syntactic generalisations, though indirectly and
</bodyText>
<page confidence="0.996195">
161
</page>
<bodyText confidence="0.511924">
ambiguous words readings readings/word errors error rate
</bodyText>
<table confidence="0.95109725">
DO (Morph. analysis) 39.0% 67,737 1.77 31 0.08%
D1 (DO + ENGCG) 6.2% 40,450 1.06 124 0.32%
D2 (D1 + ENGCG heur.) 3.2% 38,949 1.02 226 0.59%
D3 (D2 ± FS parser) 0.6% 38,342 1.00 281 0.74%
</table>
<figureCaption confidence="0.989411">
Figure 1: Results from a tagging test on a 38,202-word corpus.
</figureCaption>
<bodyText confidence="0.999865777777778">
partially. The other rule component is a syntactic
grammar. This syntactic grammar is able to re-
solve the pending part-of-speech ambiguities as a
side effect.
In short: like morphology and syntax, parts of
speech seem to be a rule-governed phenomenon.
However, the best distributional account of parts
of speech appears achievable by means of a syn-
tactic grammar.8
</bodyText>
<sectionHeader confidence="0.989381" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99991725">
I would like to thank Timo Jarvinen, Jussi Piitu-
lainen, Path Tapanainen and two EACL referees
for useful comments on an earlier version of this
paper. The usual disclaimers hold.
</bodyText>
<sectionHeader confidence="0.975886" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.630740038461538">
Hiyan Alshawi (ed.) 1992. The Core Language
Engine. Cambridge, Mass.: The MIT Press.
Ezra Black, Fred Jelinek, John Lafferty, Robert
Mercer and Salim Roukos 1992. Decision-tree
models applied to the labeling of text with parts-
of-speech. Proceedings of the Workshop on Speech
and natural Language. Defence Advanced Re-
search Projects Agency, U.S. Govt.
Eric Brill 1992. A simple rule-based part of
speech tagger. Proceedings of the Third Con-
ference on Applied Natural Language Processing,
ACL.
Kenneth Church 1988. A Stochastic Parts Pro-
gram and Noun Phrase Parser for Unrestricted
Text. Proceedings of the Second Conference on
Applied Natural Language Processing, ACL.
- 1992. Current Practice in Part of Speech
Tagging and Suggestions for the Future. In Sim-
mons (ed.), Sbornik praci: In Honor of Henry
Kueera. Michigan Slavic Studies.
Douglass Cutting 1994. Porting a stochastic
part-of-speech tagger to Swedish. In Eklund (ed.).
65-70.
Douglass Cutting, Julian Kupiec, Jan Pedersen
and Penelope Sibun 1992. A Practical Part-of-
Speech Tagger. Proceedings of ANLP-92.
</reference>
<footnote confidence="0.920934571428571">
&apos;However, the parsing description would also ben-
efit from a large corpus-based lexicon extension of
compound nouns and other useful collocations for re-
solving some even syntactically genuine part-of-speech
ambiguities. Collocations can be extracted from cor-
pora using ENGCG-style corpus tools, e.g. NPtoot
(Voutilainen 1993).
</footnote>
<reference confidence="0.985704137254902">
Stephen DeRose 1988. Grammatical category
disambiguation by statistical optimization. Com-
putational Linguistics.
Robert Eklund (ed.) Proceedings of &apos;9:e
Nordiska Datalingvistikdagarna&apos;, Stockholm 3-5
June 1993. Department of Linguistics, Computa-
tional Linguistics, Stockholm University. Stock-
holm.
Martin Eineborg and Bjorn Gamback 1994.
Tagging experiment using neural networks. In Ek-
lund (ed.). 71-81.
David Elworthy 1994. Does Baum-Welch re-
estimation help taggers? In Proceedings of the 4th
Conference on Applied Natural Language Process-
ing, ACL. Stuttgart.
Elizabeth Eyes and Geoffrey Leech 1993. Syn-
tactic Annotation: Linguistic Aspects of Gram-
matical Tagging and Skeleton Parsing. In Black
et al. (eds.), Statistically-Driven Computer Gram-
mars of English: The IBM/Lancaster Approach.
Amsterdam: Rodopi.
Roger Garside 1987. The CLAWS word-tagging
system. In Garside, Leech and Sampson (eds.),
The Computational Analysis of English. London
and New York: Longman.
Zelal Giingordii and Kemal Oflazer 1994. Pars-
ing Turkish text: the lexical functional grammar
approach. Proceedings of ACL-94.
Donald Hindle 1983. &amp;quot;User manual for Fid-
ditch&amp;quot;. Technical memorandum 7590-142, Naval
Research Lab. USA.
- 1989. Acquiring disambiguation rules from
text. Proceedings of ACL-89.
Karen Jensen, George Heidorn and Stephen
Richardson (eds.) 1993. Natural language pro-
cessing: the PLNLP approach. Kluver Academic
Publishers: Boston.
Fred Karlsson, Atro Voutilainen, Juha Heikkila
and Arto Anttila (eds.) 1995. Constraint Gram-
mar. A Language-Independent System for Pars-
ing Unrestricted Text. Berlin and New York:
Mouton de Gruyter.
Kimmo Koskenniemi 1983. Two-level Morphol-
ogy. A General Computational Model for Word-
form Production and Generation. Publications
11, Department of General Linguistics, University
of Helsinki.
- 1990. Finite-state parsing and disambigua-
tion. Proceedings of the fourteenth Interna-
tional Conference on Computational Linguistics.
COLING-90. Helsinki, Finland.
</reference>
<page confidence="0.988802">
162
</page>
<reference confidence="0.916096835294118">
Kimmo Koskenniemi, Pasi Tapanainen and
Atro Voutilainen 1992. Compiling and using
finite-state syntactic rules. In Proceedings of the
fifteenth International Conference on Computa-
tional Linguistics. COLING-92. Vol. I, pp 156-
162, Nantes, France.
Geoffrey Leech, Roger Garside and Michael
Bryant 1994. CLAWS4: The tagging of the
British National Corpus. In Proceedings of
COLING-94. Kyoto, Japan.
Carl de Marcken 1990. Parsing the LOB Cor-
pus. Proceedings of the 28th Annual Meeting of
the ACL.
Mitchell Marcus, Beatrice Santorini and Mary
Ann Marcinkiewicz 1993. Building a Large An-
notated Corpus of English: The Penn Treebank.
Computational Linguistics, Vol. 19, Number 2.
313-330.
Ian Marshall 1983. Choice of grammatical
word-class without global syntactic analysis: tag-
ging words in the LOB Corpus. Computers in the
Humanities.
Michael McCord 1990. A System for Sim-
pler Construction of Practical Natural Language
Grammars. In R. Studer (ed.), Natural Language
and Logic. Lecture Notes in Artificial Intelligence&apos;
459. Berlin: Springer Verlag.
Bernard Merialdo 1994. Tagging English text
with a probabilistic model. Computational Lin-
guistics, Vol. 20.
Geoffrey Sampson 1987. Probabilistic Mod-
els of Analysis. In Garside, Leech and Sampson
(eds.).
Christer Samuelsson 1994. Morphological tag-
ging based entirely on Bayesian inference. In Ek-
lund (ed.). 225-237.
Helmut Schmid 1994. Part-of-speech tagging
with neural networks. In Proceedings of COLING-
94. Kyoto, Japan.
Daniel Sleator and Davy Temperley 1991.
&amp;quot;Parsing English with a Link Grammar&amp;quot;. CMU-
CS-91-196. School of Computer Science, Carnegie
Mellon University, Pittsburgh, PA 15213.
Tomek Strzalkowski 1992. TTP: a fast and ro-
bust parser for natural language. Proceedings of
the fifteenth International Conference on Com-
putational Linguistics. COLING-92. Nantes,
France.
Pasi Tapanainen 1992. &amp;quot;Aarellisiin automaat-
teihin perustuva luonnollisen kielen jasennin&amp;quot; (A
finite state parser of natural language). Licentiate
(pre-doctoral) thesis. Department of Computer
Science, University of Helsinki.
Pasi Tapanainen and Timo Jarvinen 1994. Syn-
tactic analysis of natural language using linguistic
rules and corpus-based patterns. Proceedings of
CO LING- 94. Kyoto, Japan.
Pasi Tapanainen and Atro Voutilainen 1994.
Tagging accurately - Don&apos;t guess if you know.
Proceedings of the 4th Conference on Applied Nat-
ural Language Processing, ACL. Stuttgart.
Atro Voutilainen 1993. NPtool, a Detector of
English Noun Phrases. In Proceedings of the
Workshop on Very Large Corpora. Ohio State
University, Ohio. 42-51.
- 1994. Three studies of grammar-based sur-
face parsing of unrestricted English text. (Doc-
toral dissertation.). Publications 24, Department
of General Linguistics, University of Helsinki.
Atro Voutilainen, Juha Heikkila and Arto
Anttila 1992. Constraint Grammar of English. A
Performance-Oriented Introduction. Publications
21, Department of General Linguistics, University
of Helsinki.
Atro Voutilainen and Pasi Tapanainen 1993.
Ambiguity Resolution in a Reductionistic Parser.
Proceedings of the Sixth Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics. Association for Computational
Linguistics. Utrecht. 394-403.
Ralph Weischedel, Marie Meteer, Richard
Schwartz, Lance Ramshaw and Jeff Palmuzzi
1993. Coping with ambiguity and unknown words
through probabilistic models. Computational Lin-
guistics, Vol. 19, Number 2.
</reference>
<sectionHeader confidence="0.984489" genericHeader="references">
Appendix
</sectionHeader>
<bodyText confidence="0.99988225">
Enclosed is a sample output of the system. Syntax
tags have been retained; base forms and some tags
have been removed for better readability. The
syntactic tags used here are the following:
</bodyText>
<listItem confidence="0.9943854">
• 0&gt;A premodifier of adjective, adverb or quan-
tifier,
• 0&gt;N noun premodifier,
•ON&lt; noun postmodifier,
• OADVL adverbial,
• @ADVL/N&lt; adverbial or noun postmodifier,
• ©OBJ object in a finite clause,
• OIOBJ indirect object in a finite clause,
• @SUBJ subject in a finite clause,
• @obj object in a nonfinite clause,
• OP&lt;&lt; preposition complement,
• @nh nominal head,
• ©CC coordinating conjunction,
• @CS subordinating conjunction,
• @MV main verb in a finite clause,
• ©aux auxiliary in a nonfinite clause,
• @my main verb in a nonfinite clause,
• ADVLO adverbial clause,
• MC© finite main clause,
• OBJ@ clause as an object in a finite clause.
</listItem>
<reference confidence="0.5540105">
00 On PREP @ADVL @
completion N NOM SG @P&lt;&lt; @
</reference>
<page confidence="0.99087">
163
</page>
<figure confidence="0.963455888888888">
©comma @
check V IMP ©MV MC© @
the DET CENTRAL SG/PL O&gt;N 0
engine N NOM SG O&gt;N 0
oil N NOM SG O&gt;N 0
level N NOM SG ©OBJ 0/
©comma @
start V IMP ©MV MC© 0
the DET CENTRAL SG/PL O&gt;N @
engine N NOM SG ©OBJ @/
then ADV ADVL ©ADVL
check V IMP ©MV MC© @
for PREP ©ADVL @
oil N NOM SG O&gt;N
leaks N NOM PL OP« 0
Ofullstop @0
@@ Screw V IMP ©MV MC© 0
a DET CENTRAL SG O&gt;N 0
self-tapping PCP1 O&gt;N
screw N NOM SG ©OBJ @
of PREP ON&lt; @
appropriate A ABS O&gt;N
diameter N NOM SG OP« 0
into PREP ©ADVL/N&lt; @
this DET CENTRAL DEM SG @&gt;N
hole N NOM SG OP&lt;&lt; 0/
©comma 0
then ADV ADVL ©ADVL
lever V IMP @MV MC@ 0
against PREP ©ADVL
the DET CENTRAL SG/PL O&gt;N 0
screw N NOM SG OP« 0
to INFMARK&gt; ©aux @
extract V INF @my ADVL©
the DET CENTRAL SG/PL @&gt;N 0
plug N NOM SG @obj
as CS ©CS 0
shown PCP2 amv ADVL© 0
in PREP ©ADVL 0
FIG ABBR NOM SG @&gt;N 0
1.26 NUM CARD @P« @
Ofullstop 00
00 This PRON DEM SG Onh 0
done PCP2 @NG 0
©comma @
push V IMP ©MV MC© 0
the DET CENTRAL SG/PL @&gt;N @
crankshaft N NOM SG ©OBJ 0
fully ADV @&gt;A @
rearwards ADV ©ADVL 0/
©comma
then ADV ADVL ©ADVL 0
slowly ADV ©ADVL 0
but CC ©CC @
positively ADV ©ADVL
push V IMP ©MV MC© 0
it PRON ACC SG3 ©OBJ @
forwards ADV ADVL ©ADVL 0
to PREP ©ADVL @
its PRON GEN SG3 0&gt;N 0
stop N NOM SG @P&lt;&lt;
@fullstop @@
00 Lightly ADV ©ADVL 0
moisten V IMP WV MC© 0
the DET CENTRAL SG/PL @&gt;N
lips N NOM PL ©OBJ @
of PREP ON&lt; 0
a DET CENTRAL SG 0&gt;N 0
new A ABS O&gt;N @
rear N NOM SG @&gt;N 0
oil N NOM SG 0&gt;N 0
seal N NOM SG OP« 0
with PREP ©ADVL/N&lt; @
engine N NOM SG O&gt;N @
oil N NOM SG OP« @/
©comma 0
then ADV ADVL ©ADVL 0
drive V IMP ©MV MC© 0
it PRON ACC SG3 ©OBJ @
squarely ADV ©ADVL @
into PREP ©ADVL
position N NOM SG @P&lt;&lt; 0/
until CS ©CS 0
it PRON NOM SG3 SUBJ @SUBJ 0
rests V PRES SG3 ©MV ADVL© 0
against PREP ©ADVL 0
its PRON GEN SG3 4&gt;N @
abutment N NOM SG OP« 0
©comma 0
preferably ADV ©ADVL
using PCP1 @mv ADVL© @
the DET CENTRAL SG/PL O&gt;N 0
appropriate A ABS @&gt;N 0
service N NOM SG @&gt;N
tool N NOM SG @obj
for PREP ©ADVL/N&lt; 0
this DET CENTRAL DEM SG @&gt;N 0
operation N NOM SG OP« 0
@fullstop 0@
</figure>
<page confidence="0.984991">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.197032">
<title confidence="0.983719">A syntax-based part-of-speech analyser</title>
<author confidence="0.995331">Atro Voutilainen</author>
<affiliation confidence="0.935648">Research Unit for Multilingual Language Technology</affiliation>
<address confidence="0.786399">4</address>
<affiliation confidence="0.8603445">FIN-00014 University of Helsinki Finland</affiliation>
<email confidence="0.456452">Atro.Voutilainen@Helsinki.FI</email>
<abstract confidence="0.993092708333333">There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven. Recent state-ofthe-art part-of-speech taggers are based on the data-driven approach. Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the datadriven approach in part-of-speech analysis may appear surprising. In this paper, a case is made for the syntactic nature of part-of-speech tagging. A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated. Tested against a benchmark corpus of 38,000 words of previously unseen text, this syntax-based system reaches an accuracy of above 99%. Compared to the 95-97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Core Language Engine.</title>
<date>1992</date>
<editor>Hiyan Alshawi (ed.)</editor>
<publisher>The MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="6736" citStr="(1992)" startWordPosition="1017" endWordPosition="1017">a &amp;quot;side effect&amp;quot;. In this sense parts of speech seem to differ from morphology and syntax: their status as an independent level of linguistic description appears doubtful. Before proceeding further with the main argument, consider three very recent hybrids — systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices (Leech, Garside and Bryant 1994), Hidden Markov Models (Tapanainen and Voutilainen 1994), or syntactic patterns (Tapanainen and 2For related discussion, cf. Sampson (1987) and Church (1992). Jarvinen 1994). What is interesting in these hybrids is that they, unlike purely data-driven taggers, seem capable of exceeding the 97% barrier: all three report an accuracy of about 98.5%.3 The success of these hybrids could be regarded as evidence for the syntactic aspects of parts of speech. However, the above hybrids still contain a datadriven component, i.e. it remains an open question whether a tagger entirely based on the linguistic approach can compare with a data-driven system. Next, a new system with the following properties is outlined and evaluated: • The tagger uses only linguis</context>
</contexts>
<marker>1992</marker>
<rawString>Hiyan Alshawi (ed.) 1992. The Core Language Engine. Cambridge, Mass.: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Fred Jelinek</author>
<author>John Lafferty</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Decision-tree models applied to the labeling of text with partsof-speech.</title>
<date>1992</date>
<booktitle>Proceedings of the Workshop on Speech and natural Language. Defence Advanced Research Projects Agency, U.S. Govt.</booktitle>
<contexts>
<context position="4174" citStr="Black et al. 1992" startWordPosition="618" endWordPosition="621">ormation about long-distance phenom&apos;There is one potential exception: the rule-based morphological disambiguator used in the English Constraint Grammar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Mercer, Roukos, 1992</marker>
<rawString>Ezra Black, Fred Jelinek, John Lafferty, Robert Mercer and Salim Roukos 1992. Decision-tree models applied to the labeling of text with partsof-speech. Proceedings of the Workshop on Speech and natural Language. Defence Advanced Research Projects Agency, U.S. Govt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="2417" citStr="Brill 1992" startWordPosition="346" endWordPosition="347">uistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar. • In the data-driven approach, frequency-based information is automatically derived from corpora. The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. Cutting 1994) and annotating the training corpus. At the first flush, the linguistic approach may seem an obvious choice. A part-of-speech tagger&apos;s task is often illustrated with a noun—verb ambiguous word directly preceded by an unambiguous determiner (e.g. table in the table). This ambiguity can reliably be resolved with a simple and obvious grammar rule that disallows verbs after determiners. In</context>
<context position="4302" citStr="Brill 1992" startWordPosition="641" endWordPosition="642">nstraint Grammar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Eric Brill 1992. A simple rule-based part of speech tagger. Proceedings of the Third Conference on Applied Natural Language Processing, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.</title>
<date>1988</date>
<booktitle>Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="4187" citStr="Church 1988" startWordPosition="622" endWordPosition="623">-distance phenom&apos;There is one potential exception: the rule-based morphological disambiguator used in the English Constraint Grammar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respec</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth Church 1988. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. Proceedings of the Second Conference on Applied Natural Language Processing, ACL.</rawString>
</citation>
<citation valid="true">
<title>Current Practice in Part of Speech Tagging and Suggestions for the Future.</title>
<date>1992</date>
<booktitle>Sbornik praci: In Honor of Henry Kueera. Michigan Slavic Studies.</booktitle>
<editor>In Simmons (ed.),</editor>
<contexts>
<context position="6736" citStr="(1992)" startWordPosition="1017" endWordPosition="1017">a &amp;quot;side effect&amp;quot;. In this sense parts of speech seem to differ from morphology and syntax: their status as an independent level of linguistic description appears doubtful. Before proceeding further with the main argument, consider three very recent hybrids — systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices (Leech, Garside and Bryant 1994), Hidden Markov Models (Tapanainen and Voutilainen 1994), or syntactic patterns (Tapanainen and 2For related discussion, cf. Sampson (1987) and Church (1992). Jarvinen 1994). What is interesting in these hybrids is that they, unlike purely data-driven taggers, seem capable of exceeding the 97% barrier: all three report an accuracy of about 98.5%.3 The success of these hybrids could be regarded as evidence for the syntactic aspects of parts of speech. However, the above hybrids still contain a datadriven component, i.e. it remains an open question whether a tagger entirely based on the linguistic approach can compare with a data-driven system. Next, a new system with the following properties is outlined and evaluated: • The tagger uses only linguis</context>
</contexts>
<marker>1992</marker>
<rawString>- 1992. Current Practice in Part of Speech Tagging and Suggestions for the Future. In Simmons (ed.), Sbornik praci: In Honor of Henry Kueera. Michigan Slavic Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglass Cutting</author>
</authors>
<title>Porting a stochastic part-of-speech tagger to Swedish.</title>
<date>1994</date>
<pages>65--70</pages>
<editor>In Eklund (ed.).</editor>
<contexts>
<context position="2629" citStr="Cutting 1994" startWordPosition="378" endWordPosition="379">corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. Cutting 1994) and annotating the training corpus. At the first flush, the linguistic approach may seem an obvious choice. A part-of-speech tagger&apos;s task is often illustrated with a noun—verb ambiguous word directly preceded by an unambiguous determiner (e.g. table in the table). This ambiguity can reliably be resolved with a simple and obvious grammar rule that disallows verbs after determiners. Indeed, few contest the fact that reliable linguistic rules can be written for resolving some partof-speech ambiguities. The main problem with this approach seems to be that resolving part-ofspeech ambiguities on a</context>
</contexts>
<marker>Cutting, 1994</marker>
<rawString>Douglass Cutting 1994. Porting a stochastic part-of-speech tagger to Swedish. In Eklund (ed.). 65-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglass Cutting</author>
<author>Julian Kupiec</author>
</authors>
<title>Pedersen and Penelope Sibun</title>
<date></date>
<booktitle>Proceedings of ANLP-92.</booktitle>
<marker>Cutting, Kupiec, </marker>
<rawString>Douglass Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun 1992. A Practical Part-ofSpeech Tagger. Proceedings of ANLP-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization. Computational Linguistics.</title>
<date>1988</date>
<contexts>
<context position="4238" citStr="DeRose 1988" startWordPosition="631" endWordPosition="632">the rule-based morphological disambiguator used in the English Constraint Grammar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>Stephen DeRose 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>Proceedings of &apos;9:e Nordiska Datalingvistikdagarna&apos;, Stockholm 3-5</booktitle>
<editor>Robert Eklund (ed.)</editor>
<location>Stockholm University. Stockholm.</location>
<marker>1993</marker>
<rawString>Robert Eklund (ed.) Proceedings of &apos;9:e Nordiska Datalingvistikdagarna&apos;, Stockholm 3-5 June 1993. Department of Linguistics, Computational Linguistics, Stockholm University. Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Eineborg</author>
<author>Bjorn Gamback</author>
</authors>
<title>Tagging experiment using neural networks.</title>
<date>1994</date>
<pages>71--81</pages>
<editor>In Eklund (ed.).</editor>
<contexts>
<context position="2377" citStr="Eineborg and Gamback 1994" startWordPosition="338" endWordPosition="341">iscarding contextually illegitimate analyses. The linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar. • In the data-driven approach, frequency-based information is automatically derived from corpora. The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. Cutting 1994) and annotating the training corpus. At the first flush, the linguistic approach may seem an obvious choice. A part-of-speech tagger&apos;s task is often illustrated with a noun—verb ambiguous word directly preceded by an unambiguous determiner (e.g. table in the table). This ambiguity can reliably be resolved with a simple and obvious grammar rule th</context>
<context position="4346" citStr="Eineborg and Gamback 1994" startWordPosition="645" endWordPosition="648"> (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological description</context>
</contexts>
<marker>Eineborg, Gamback, 1994</marker>
<rawString>Martin Eineborg and Bjorn Gamback 1994. Tagging experiment using neural networks. In Eklund (ed.). 71-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elworthy</author>
</authors>
<title>Does Baum-Welch reestimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied Natural Language Processing, ACL.</booktitle>
<location>Stuttgart.</location>
<contexts>
<context position="2140" citStr="Elworthy 1994" startWordPosition="304" endWordPosition="305">ions are based on the linguist&apos;s (potentially corpus-based) abstractions about the paradigms and syntagms of the language. Distributional generalisations are manually coded as a grammar, a system of constraint rules used for discarding contextually illegitimate analyses. The linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar. • In the data-driven approach, frequency-based information is automatically derived from corpora. The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. Cutting 1994) and annotating the training corpus. At the first flush, the linguistic approach may seem an obvious choice. A </context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>David Elworthy 1994. Does Baum-Welch reestimation help taggers? In Proceedings of the 4th Conference on Applied Natural Language Processing, ACL. Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Eyes</author>
<author>Geoffrey Leech</author>
</authors>
<title>Syntactic Annotation: Linguistic Aspects of Grammatical Tagging and Skeleton Parsing.</title>
<date>1993</date>
<booktitle>Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach.</booktitle>
<editor>In Black et al. (eds.),</editor>
<location>Amsterdam: Rodopi.</location>
<marker>Eyes, Leech, 1993</marker>
<rawString>Elizabeth Eyes and Geoffrey Leech 1993. Syntactic Annotation: Linguistic Aspects of Grammatical Tagging and Skeleton Parsing. In Black et al. (eds.), Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Amsterdam: Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Garside</author>
</authors>
<title>The CLAWS word-tagging system.</title>
<date>1987</date>
<booktitle>The Computational Analysis of English. London and</booktitle>
<editor>In Garside, Leech and Sampson (eds.),</editor>
<publisher>Longman.</publisher>
<location>New York:</location>
<contexts>
<context position="2459" citStr="Garside 1987" startWordPosition="351" endWordPosition="352">ll and effort is needed for writing an exhaustive grammar. • In the data-driven approach, frequency-based information is automatically derived from corpora. The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. Cutting 1994) and annotating the training corpus. At the first flush, the linguistic approach may seem an obvious choice. A part-of-speech tagger&apos;s task is often illustrated with a noun—verb ambiguous word directly preceded by an unambiguous determiner (e.g. table in the table). This ambiguity can reliably be resolved with a simple and obvious grammar rule that disallows verbs after determiners. Indeed, few contest the fact that reliable l</context>
</contexts>
<marker>Garside, 1987</marker>
<rawString>Roger Garside 1987. The CLAWS word-tagging system. In Garside, Leech and Sampson (eds.), The Computational Analysis of English. London and New York: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelal Giingordii</author>
<author>Kemal Oflazer</author>
</authors>
<title>Parsing Turkish text: the lexical functional grammar approach.</title>
<date>1994</date>
<booktitle>Proceedings of ACL-94.</booktitle>
<contexts>
<context position="5140" citStr="Giingordii and Oflazer 1994" startWordPosition="762" endWordPosition="765">f known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typologically different languages (Kimmo Koskenniemi, personal communication). With regard to computational syntax, see for instance (Giingordii and Oflazer 1994; Hindle 1983; Jensen, Heidorn and Richardson (eds.) 1993; McCord 1990; Sleator and Temperley 1991; Alshawi (ed.) 1992; Strzalkowski 1992). The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach. Is the level of parts of speech somehow different, perhaps less rulegoverned, than related levels?2 We do not need to assume this idiosyncratic status entirely. The rest of this paper argues that also parts of speech can be viewed as a rule-governed phenomenon, possible to model using t</context>
</contexts>
<marker>Giingordii, Oflazer, 1994</marker>
<rawString>Zelal Giingordii and Kemal Oflazer 1994. Parsing Turkish text: the lexical functional grammar approach. Proceedings of ACL-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>User manual for Fidditch&amp;quot;. Technical memorandum 7590-142, Naval Research Lab.</title>
<date>1983</date>
<publisher>USA.</publisher>
<contexts>
<context position="5153" citStr="Hindle 1983" startWordPosition="766" endWordPosition="767">ven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typologically different languages (Kimmo Koskenniemi, personal communication). With regard to computational syntax, see for instance (Giingordii and Oflazer 1994; Hindle 1983; Jensen, Heidorn and Richardson (eds.) 1993; McCord 1990; Sleator and Temperley 1991; Alshawi (ed.) 1992; Strzalkowski 1992). The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach. Is the level of parts of speech somehow different, perhaps less rulegoverned, than related levels?2 We do not need to assume this idiosyncratic status entirely. The rest of this paper argues that also parts of speech can be viewed as a rule-governed phenomenon, possible to model using the linguistic</context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Donald Hindle 1983. &amp;quot;User manual for Fidditch&amp;quot;. Technical memorandum 7590-142, Naval Research Lab. USA.</rawString>
</citation>
<citation valid="true">
<title>Acquiring disambiguation rules from text.</title>
<date>1989</date>
<booktitle>Proceedings of ACL-89.</booktitle>
<marker>1989</marker>
<rawString>- 1989. Acquiring disambiguation rules from text. Proceedings of ACL-89.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>Natural language processing: the PLNLP approach.</booktitle>
<editor>Karen Jensen, George Heidorn and Stephen Richardson (eds.)</editor>
<publisher>Kluver Academic Publishers: Boston.</publisher>
<marker>1993</marker>
<rawString>Karen Jensen, George Heidorn and Stephen Richardson (eds.) 1993. Natural language processing: the PLNLP approach. Kluver Academic Publishers: Boston.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.) 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text.</title>
<location>Berlin and New York: Mouton</location>
<note>de Gruyter.</note>
<marker>Karlsson, </marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.) 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. Berlin and New York: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level Morphology. A General Computational Model for Wordform Production and Generation.</title>
<date>1983</date>
<journal>Publications</journal>
<volume>11</volume>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<contexts>
<context position="4909" citStr="Koskenniemi (1983)" startWordPosition="734" endWordPosition="735"> Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typologically different languages (Kimmo Koskenniemi, personal communication). With regard to computational syntax, see for instance (Giingordii and Oflazer 1994; Hindle 1983; Jensen, Heidorn and Richardson (eds.) 1993; McCord 1990; Sleator and Temperley 1991; Alshawi (ed.) 1992; Strzalkowski 1992). The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach. Is the level of parts of speech somehow different, perha</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi 1983. Two-level Morphology. A General Computational Model for Wordform Production and Generation. Publications 11, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<title>Finite-state parsing and disambiguation.</title>
<date>1990</date>
<booktitle>Proceedings of the fourteenth International Conference on Computational Linguistics. COLING-90.</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="11912" citStr="(1990)" startWordPosition="1795" endWordPosition="1795">inen 1994). &apos;Actually, it is possible to define additional heuristic rule collections that can optionally be applied after the more reliable ones for resolving remaining ambiguities. The grammar avoids risky&apos;predictions, therefore 3-7% of all words remain ambiguous (an average 1.04-1.08 alternative analyses per output word). On the other hand, at least 99.7% of all words retain the correct morphological analysis. Note in passing that the ratio 1.04-1.08/99.7% compares very favourably with other systems; c.f. 3.0/99.3% by POST (Weischedel et al. 1993) and 1.04/97.6% or 1.09/98.6% by de Marcken (1990). There is an additional collection of 200 optionally applicable heuristic constraints that are based on simplified linguistic generalisations. They resolve about half of the remaining ambiguities, increasing the overall error rate to about 0.5%. Most of even the remaining ambiguities are structurally resolvable. ENGCG leaves them pending mainly because it is prohibitively difficult to express certain kinds of structural generalisation using the available rule formalism and grammatical representation. 2.3 Syntactic analysis 2.3.1 Finite-State Intersection Grammar Syntactic analysis is carried </context>
</contexts>
<marker>1990</marker>
<rawString>- 1990. Finite-state parsing and disambiguation. Proceedings of the fourteenth International Conference on Computational Linguistics. COLING-90. Helsinki, Finland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Pasi Tapanainen and Atro Voutilainen 1992. Compiling and using finite-state syntactic rules.</title>
<booktitle>In Proceedings of the fifteenth International Conference on Computational Linguistics. COLING-92.</booktitle>
<volume>Vol. I,</volume>
<pages>156--162</pages>
<location>Nantes, France.</location>
<marker>Koskenniemi, </marker>
<rawString>Kimmo Koskenniemi, Pasi Tapanainen and Atro Voutilainen 1992. Compiling and using finite-state syntactic rules. In Proceedings of the fifteenth International Conference on Computational Linguistics. COLING-92. Vol. I, pp 156-162, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Roger Garside</author>
<author>Michael Bryant</author>
</authors>
<title>CLAWS4: The tagging of the British National Corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94. Kyoto,</booktitle>
<marker>Leech, Garside, Bryant, 1994</marker>
<rawString>Geoffrey Leech, Roger Garside and Michael Bryant 1994. CLAWS4: The tagging of the British National Corpus. In Proceedings of COLING-94. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Parsing the LOB Corpus.</title>
<date>1990</date>
<booktitle>Proceedings of the 28th Annual Meeting of the ACL.</booktitle>
<marker>de Marcken, 1990</marker>
<rawString>Carl de Marcken 1990. Parsing the LOB Corpus. Proceedings of the 28th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mitchell Marcus</author>
</authors>
<institution>Beatrice Santorini and Mary</institution>
<marker>Marcus, </marker>
<rawString>Mitchell Marcus, Beatrice Santorini and Mary</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>313--330</pages>
<marker>Marcinkiewicz, 1993</marker>
<rawString>Ann Marcinkiewicz 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, Vol. 19, Number 2. 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Marshall</author>
</authors>
<title>Choice of grammatical word-class without global syntactic analysis: tagging words in the LOB Corpus. Computers in the Humanities.</title>
<date>1983</date>
<contexts>
<context position="4155" citStr="Marshall 1983" startWordPosition="616" endWordPosition="617">ally employ information about long-distance phenom&apos;There is one potential exception: the rule-based morphological disambiguator used in the English Constraint Grammar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related le</context>
</contexts>
<marker>Marshall, 1983</marker>
<rawString>Ian Marshall 1983. Choice of grammatical word-class without global syntactic analysis: tagging words in the LOB Corpus. Computers in the Humanities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McCord</author>
</authors>
<title>A System for Simpler Construction of Practical Natural Language Grammars. In</title>
<date>1990</date>
<booktitle>Natural Language and Logic. Lecture Notes in Artificial Intelligence&apos; 459.</booktitle>
<editor>R. Studer (ed.),</editor>
<publisher>Springer Verlag.</publisher>
<location>Berlin:</location>
<contexts>
<context position="5210" citStr="McCord 1990" startWordPosition="775" endWordPosition="776">-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typologically different languages (Kimmo Koskenniemi, personal communication). With regard to computational syntax, see for instance (Giingordii and Oflazer 1994; Hindle 1983; Jensen, Heidorn and Richardson (eds.) 1993; McCord 1990; Sleator and Temperley 1991; Alshawi (ed.) 1992; Strzalkowski 1992). The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach. Is the level of parts of speech somehow different, perhaps less rulegoverned, than related levels?2 We do not need to assume this idiosyncratic status entirely. The rest of this paper argues that also parts of speech can be viewed as a rule-governed phenomenon, possible to model using the linguistic approach. However, it will also be argued that though th</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>Michael McCord 1990. A System for Simpler Construction of Practical Natural Language Grammars. In R. Studer (ed.), Natural Language and Logic. Lecture Notes in Artificial Intelligence&apos; 459. Berlin: Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<contexts>
<context position="2124" citStr="Merialdo 1994" startWordPosition="302" endWordPosition="303">the generalisations are based on the linguist&apos;s (potentially corpus-based) abstractions about the paradigms and syntagms of the language. Distributional generalisations are manually coded as a grammar, a system of constraint rules used for discarding contextually illegitimate analyses. The linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar. • In the data-driven approach, frequency-based information is automatically derived from corpora. The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. Cutting 1994) and annotating the training corpus. At the first flush, the linguistic approach may seem an ob</context>
<context position="4266" citStr="Merialdo 1994" startWordPosition="635" endWordPosition="636">al disambiguator used in the English Constraint Grammar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Leve</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo 1994. Tagging English text with a probabilistic model. Computational Linguistics, Vol. 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>Probabilistic Models of Analysis.</title>
<date>1987</date>
<editor>In Garside, Leech and Sampson (eds.).</editor>
<contexts>
<context position="6718" citStr="Sampson (1987)" startWordPosition="1013" endWordPosition="1014">ion of parts of speech as a &amp;quot;side effect&amp;quot;. In this sense parts of speech seem to differ from morphology and syntax: their status as an independent level of linguistic description appears doubtful. Before proceeding further with the main argument, consider three very recent hybrids — systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices (Leech, Garside and Bryant 1994), Hidden Markov Models (Tapanainen and Voutilainen 1994), or syntactic patterns (Tapanainen and 2For related discussion, cf. Sampson (1987) and Church (1992). Jarvinen 1994). What is interesting in these hybrids is that they, unlike purely data-driven taggers, seem capable of exceeding the 97% barrier: all three report an accuracy of about 98.5%.3 The success of these hybrids could be regarded as evidence for the syntactic aspects of parts of speech. However, the above hybrids still contain a datadriven component, i.e. it remains an open question whether a tagger entirely based on the linguistic approach can compare with a data-driven system. Next, a new system with the following properties is outlined and evaluated: • The tagger</context>
</contexts>
<marker>Sampson, 1987</marker>
<rawString>Geoffrey Sampson 1987. Probabilistic Models of Analysis. In Garside, Leech and Sampson (eds.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Morphological tagging based entirely on Bayesian inference.</title>
<date>1994</date>
<pages>225--237</pages>
<editor>In Eklund (ed.).</editor>
<contexts>
<context position="4319" citStr="Samuelsson 1994" startWordPosition="643" endWordPosition="644">mmar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensiv</context>
</contexts>
<marker>Samuelsson, 1994</marker>
<rawString>Christer Samuelsson 1994. Morphological tagging based entirely on Bayesian inference. In Eklund (ed.). 225-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Part-of-speech tagging with neural networks.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING94. Kyoto,</booktitle>
<contexts>
<context position="2391" citStr="Schmid 1994" startWordPosition="342" endWordPosition="343">gitimate analyses. The linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar. • In the data-driven approach, frequency-based information is automatically derived from corpora. The learning corpus can consist of plain text, but the best results seem achievable with annotated corpora (Merialdo 1994; Elworthy 1994). This corpus-based information typically concerns sequences of 1-3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. Cutting 1994) and annotating the training corpus. At the first flush, the linguistic approach may seem an obvious choice. A part-of-speech tagger&apos;s task is often illustrated with a noun—verb ambiguous word directly preceded by an unambiguous determiner (e.g. table in the table). This ambiguity can reliably be resolved with a simple and obvious grammar rule that disallows v</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid 1994. Part-of-speech tagging with neural networks. In Proceedings of COLING94. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a Link Grammar&amp;quot;.</title>
<date>1991</date>
<tech>CMUCS-91-196.</tech>
<pages>15213</pages>
<institution>School of Computer Science, Carnegie Mellon University,</institution>
<location>Pittsburgh, PA</location>
<contexts>
<context position="5238" citStr="Sleator and Temperley 1991" startWordPosition="777" endWordPosition="781">stribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typologically different languages (Kimmo Koskenniemi, personal communication). With regard to computational syntax, see for instance (Giingordii and Oflazer 1994; Hindle 1983; Jensen, Heidorn and Richardson (eds.) 1993; McCord 1990; Sleator and Temperley 1991; Alshawi (ed.) 1992; Strzalkowski 1992). The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach. Is the level of parts of speech somehow different, perhaps less rulegoverned, than related levels?2 We do not need to assume this idiosyncratic status entirely. The rest of this paper argues that also parts of speech can be viewed as a rule-governed phenomenon, possible to model using the linguistic approach. However, it will also be argued that though the distribution of parts of s</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel Sleator and Davy Temperley 1991. &amp;quot;Parsing English with a Link Grammar&amp;quot;. CMUCS-91-196. School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
</authors>
<title>TTP: a fast and robust parser for natural language.</title>
<date>1992</date>
<booktitle>Proceedings of the fifteenth International Conference on Computational Linguistics. COLING-92.</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="5278" citStr="Strzalkowski 1992" startWordPosition="785" endWordPosition="786">because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by Koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typologically different languages (Kimmo Koskenniemi, personal communication). With regard to computational syntax, see for instance (Giingordii and Oflazer 1994; Hindle 1983; Jensen, Heidorn and Richardson (eds.) 1993; McCord 1990; Sleator and Temperley 1991; Alshawi (ed.) 1992; Strzalkowski 1992). The present success of the statistical approach in part-of-speech analysis seems then to form an exception to the general feasibility of the rule-based linguistic approach. Is the level of parts of speech somehow different, perhaps less rulegoverned, than related levels?2 We do not need to assume this idiosyncratic status entirely. The rest of this paper argues that also parts of speech can be viewed as a rule-governed phenomenon, possible to model using the linguistic approach. However, it will also be argued that though the distribution of parts of speech can to some extent be described wi</context>
</contexts>
<marker>Strzalkowski, 1992</marker>
<rawString>Tomek Strzalkowski 1992. TTP: a fast and robust parser for natural language. Proceedings of the fifteenth International Conference on Computational Linguistics. COLING-92. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
</authors>
<title>Aarellisiin automaatteihin perustuva luonnollisen kielen jasennin&amp;quot; (A finite state parser of natural language). Licentiate (pre-doctoral) thesis.</title>
<date>1992</date>
<institution>Department of Computer Science, University of Helsinki.</institution>
<contexts>
<context position="12682" citStr="Tapanainen 1992" startWordPosition="1900" endWordPosition="1901">e about half of the remaining ambiguities, increasing the overall error rate to about 0.5%. Most of even the remaining ambiguities are structurally resolvable. ENGCG leaves them pending mainly because it is prohibitively difficult to express certain kinds of structural generalisation using the available rule formalism and grammatical representation. 2.3 Syntactic analysis 2.3.1 Finite-State Intersection Grammar Syntactic analysis is carried out in another reductionistic parsing framework known as FiniteState Intersection Grammar (Koskenniemi 1990; Koskenniemi, Tapanainen and Voutilainen 1992; Tapanainen 1992; Voutilainen and Tapanainen 1993; Voutilainen 1994). A short introduction: • Also here syntactic analysis means resolution of structural ambiguities. Morphological, syntactic and clause boundary descriptors are introduced as ambiguities with simple mappings; these ambiguities are then resolved in parallel. • The formalism does not distinguish between various types of ambiguity; nor are ambiguity class specific rule sets needed. A single rule often resolves all types of ambiguity, though superficially it may look e.g. like a rule about syntactic functions. • The grammarian can define constants</context>
</contexts>
<marker>Tapanainen, 1992</marker>
<rawString>Pasi Tapanainen 1992. &amp;quot;Aarellisiin automaatteihin perustuva luonnollisen kielen jasennin&amp;quot; (A finite state parser of natural language). Licentiate (pre-doctoral) thesis. Department of Computer Science, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Jarvinen</author>
</authors>
<title>Syntactic analysis of natural language using linguistic rules and corpus-based patterns.</title>
<date>1994</date>
<booktitle>Proceedings of CO LING- 94.</booktitle>
<location>Kyoto, Japan.</location>
<marker>Tapanainen, Jarvinen, 1994</marker>
<rawString>Pasi Tapanainen and Timo Jarvinen 1994. Syntactic analysis of natural language using linguistic rules and corpus-based patterns. Proceedings of CO LING- 94. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Atro Voutilainen</author>
</authors>
<title>Tagging accurately - Don&apos;t guess if you know.</title>
<date>1994</date>
<booktitle>Proceedings of the 4th Conference on Applied Natural Language Processing, ACL.</booktitle>
<location>Stuttgart.</location>
<contexts>
<context position="6635" citStr="Tapanainen and Voutilainen 1994" startWordPosition="999" endWordPosition="1003">nd function of essentially syntactic categories. A syntactic grammar appears to predict the distribution of parts of speech as a &amp;quot;side effect&amp;quot;. In this sense parts of speech seem to differ from morphology and syntax: their status as an independent level of linguistic description appears doubtful. Before proceeding further with the main argument, consider three very recent hybrids — systems that employ linguistic rules for resolving some of the ambiguities before using automatically generated corpus-based information: collocation matrices (Leech, Garside and Bryant 1994), Hidden Markov Models (Tapanainen and Voutilainen 1994), or syntactic patterns (Tapanainen and 2For related discussion, cf. Sampson (1987) and Church (1992). Jarvinen 1994). What is interesting in these hybrids is that they, unlike purely data-driven taggers, seem capable of exceeding the 97% barrier: all three report an accuracy of about 98.5%.3 The success of these hybrids could be regarded as evidence for the syntactic aspects of parts of speech. However, the above hybrids still contain a datadriven component, i.e. it remains an open question whether a tagger entirely based on the linguistic approach can compare with a data-driven system. Next,</context>
</contexts>
<marker>Tapanainen, Voutilainen, 1994</marker>
<rawString>Pasi Tapanainen and Atro Voutilainen 1994. Tagging accurately - Don&apos;t guess if you know. Proceedings of the 4th Conference on Applied Natural Language Processing, ACL. Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>NPtool, a Detector of English Noun Phrases.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora. Ohio</booktitle>
<pages>42--51</pages>
<institution>State University,</institution>
<location>Ohio.</location>
<marker>Voutilainen, 1993</marker>
<rawString>Atro Voutilainen 1993. NPtool, a Detector of English Noun Phrases. In Proceedings of the Workshop on Very Large Corpora. Ohio State University, Ohio. 42-51.</rawString>
</citation>
<citation valid="true">
<title>Three studies of grammar-based surface parsing of unrestricted English text.</title>
<date>1994</date>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<note>(Doctoral dissertation.). Publications 24,</note>
<marker>1994</marker>
<rawString>- 1994. Three studies of grammar-based surface parsing of unrestricted English text. (Doctoral dissertation.). Publications 24, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>Juha Heikkila and Arto Anttila 1992. Constraint Grammar of English. A Performance-Oriented Introduction.</title>
<journal>Publications</journal>
<volume>21</volume>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<marker>Voutilainen, </marker>
<rawString>Atro Voutilainen, Juha Heikkila and Arto Anttila 1992. Constraint Grammar of English. A Performance-Oriented Introduction. Publications 21, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
<author>Pasi Tapanainen</author>
</authors>
<title>Ambiguity Resolution in a Reductionistic Parser.</title>
<date>1993</date>
<booktitle>Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics. Utrecht.</booktitle>
<pages>394--403</pages>
<contexts>
<context position="7827" citStr="Voutilainen and Tapanainen 1993" startWordPosition="1189" endWordPosition="1193">ompare with a data-driven system. Next, a new system with the following properties is outlined and evaluated: • The tagger uses only linguistic distributional rules. • Tested against a 38,000-word corpus of previously unseen text, the tagger reaches a better accuracy than previous systems (over 99%). • At the level of linguistic abstraction, the grammar rules are essentially syntactic. Ideally, part-of-speech disambiguation should fall out as a &amp;quot;side effect&amp;quot; of syntactic analysis. Section 2 outlines a rule-based system consisting of the ENGCG tagger followed by a finitestate syntactic parser (Voutilainen and Tapanainen 1993; Voutilainen 1994) that resolves remaining part-of-speech ambiguities as a side effect. In Section 3, this rule-based system is tested against a 38,000-word corpus of previously unseen text. Currently tagger evaluation is only becoming standardised; the evaluation method is accordingly reported in detail. 2 System description The tagger consists of the following sequential components: • Tokeniser • ENGCG morphological analyser — Lexicon — Morphological heuristics • ENGCG morphological disambiguator • Lookup of alternative syntactic tags • Finite state syntactic disambiguator 2.1 Morphological</context>
<context position="12715" citStr="Voutilainen and Tapanainen 1993" startWordPosition="1902" endWordPosition="1905">he remaining ambiguities, increasing the overall error rate to about 0.5%. Most of even the remaining ambiguities are structurally resolvable. ENGCG leaves them pending mainly because it is prohibitively difficult to express certain kinds of structural generalisation using the available rule formalism and grammatical representation. 2.3 Syntactic analysis 2.3.1 Finite-State Intersection Grammar Syntactic analysis is carried out in another reductionistic parsing framework known as FiniteState Intersection Grammar (Koskenniemi 1990; Koskenniemi, Tapanainen and Voutilainen 1992; Tapanainen 1992; Voutilainen and Tapanainen 1993; Voutilainen 1994). A short introduction: • Also here syntactic analysis means resolution of structural ambiguities. Morphological, syntactic and clause boundary descriptors are introduced as ambiguities with simple mappings; these ambiguities are then resolved in parallel. • The formalism does not distinguish between various types of ambiguity; nor are ambiguity class specific rule sets needed. A single rule often resolves all types of ambiguity, though superficially it may look e.g. like a rule about syntactic functions. • The grammarian can define constants and predicates using regular exp</context>
<context position="14666" citStr="Voutilainen and Tapanainen 1993" startWordPosition="2204" endWordPosition="2207">n with each rule automaton. Those sentence readings accepted by all rule automata are proposed as parses. • In addition, heuristic rules can be used for ranking alternative analyses accepted by the strict rules. 2.3.2 Grammatical representation The grammatical representation used in the Finite State framework is an extension of the ENGCG syntax. Surface-syntactic grammatical relations are encoded with dependency-oriented functional tags. Functional representation of phrases and clauses has been introduced to facilitate expressing syntactic generalisations. The representation is introduced in (Voutilainen and Tapanainen 1993; Voutilainen 1994); here, only the main characteristics are given: • Each word boundary is explicitly represented as one of five alternatives: — the sentence boundary &amp;quot;@@&amp;quot; — the boundary separating juxtaposed finite clauses &amp;quot;@/&amp;quot; — centre-embedded (sequences of) finite clauses are flanked with &amp;quot;@&lt;&amp;quot; and &amp;quot;@&gt;&amp;quot; — the plain word boundary &amp;quot;A&amp;quot; • Each word is furnished with a tag indicating a surface-syntactic function (subject, premodifier, auxiliary, main verb, adverbial, etc.). All main verbs are furnished with two syntactic tags, one indicating its main verb status, the other indicating the functi</context>
</contexts>
<marker>Voutilainen, Tapanainen, 1993</marker>
<rawString>Atro Voutilainen and Pasi Tapanainen 1993. Ambiguity Resolution in a Reductionistic Parser. Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics. Utrecht. 394-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Marie Meteer</author>
<author>Richard Schwartz</author>
<author>Lance Ramshaw</author>
<author>Jeff Palmuzzi</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="4290" citStr="Weischedel et al. 1993" startWordPosition="637" endWordPosition="640">r used in the English Constraint Grammar Parser ENGCG (Voutilainen, lieikld15. and Anttila 1992). Its recall is very high (99.7% of all words receive the correct morphological analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall. 157 ena or the linguist&apos;s abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach a 95-97% accuracy in the analysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the accuracy of known systems, the data-driven approach seems then to provide the best model of part-of-speech distribution. This should appear a little curious because very competitive results have been achieved using the linguistic approach at related levels of description. With respect to computational morphology, witness for instance the success of the Two-Level paradigm introduced by</context>
<context position="11862" citStr="Weischedel et al. 1993" startWordPosition="1784" endWordPosition="1787">rase, the prepositional phrase, the finite verb chain etc. (Voutilainen 1994). &apos;Actually, it is possible to define additional heuristic rule collections that can optionally be applied after the more reliable ones for resolving remaining ambiguities. The grammar avoids risky&apos;predictions, therefore 3-7% of all words remain ambiguous (an average 1.04-1.08 alternative analyses per output word). On the other hand, at least 99.7% of all words retain the correct morphological analysis. Note in passing that the ratio 1.04-1.08/99.7% compares very favourably with other systems; c.f. 3.0/99.3% by POST (Weischedel et al. 1993) and 1.04/97.6% or 1.09/98.6% by de Marcken (1990). There is an additional collection of 200 optionally applicable heuristic constraints that are based on simplified linguistic generalisations. They resolve about half of the remaining ambiguities, increasing the overall error rate to about 0.5%. Most of even the remaining ambiguities are structurally resolvable. ENGCG leaves them pending mainly because it is prohibitively difficult to express certain kinds of structural generalisation using the available rule formalism and grammatical representation. 2.3 Syntactic analysis 2.3.1 Finite-State I</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmuzzi, 1993</marker>
<rawString>Ralph Weischedel, Marie Meteer, Richard Schwartz, Lance Ramshaw and Jeff Palmuzzi 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, Vol. 19, Number 2.</rawString>
</citation>
<citation valid="false">
<journal>00 On PREP @ADVL @ completion N NOM SG @P&lt;&lt; @</journal>
<marker></marker>
<rawString>00 On PREP @ADVL @ completion N NOM SG @P&lt;&lt; @</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>