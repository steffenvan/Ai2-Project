<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998148">
Automated Alignment and Extraction of Bilingual Domain Ontology for
Medical Domain Web Search
</title>
<author confidence="0.999405">
Jui-Feng Yeh&amp;quot;﹢, Chung-Hsien Wu&amp;quot;, Ming-Jun Chen&amp;quot; and Liang-Chih Yu&amp;quot;
</author>
<affiliation confidence="0.996443">
*Department of Computer Science and Information Engineering
National Cheng Kung University, Taiwan, R.O.C.
</affiliation>
<email confidence="0.76273">
{chwu, jfyeh, mjchen,lcyu}@csie.ncku.edu.tw
</email>
<affiliation confidence="0.9639245">
﹢Department of Computer Application Engineering
Far East College, Taiwan, R.O.C.
</affiliation>
<sectionHeader confidence="0.979916" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946115384615">
This paper proposes an approach to automated
ontology alignment and domain ontology
extraction from two knowledge bases. First,
WordNet and HowNet knowledge bases are
aligned to construct a bilingual universal
ontology based on the co-occurrence of the
words in a parallel corpus. The bilingual
universal ontology has the merit that it
contains more structural and semantic
information coverage from two
complementary knowledge bases, WordNet
and HowNet. For domain-specific applications,
a medical domain ontology is further extracted
from the universal ontology using the island-
driven algorithm and a medical domain corpus.
Finally, the domain-dependent terms and some
axioms between medical terms based on a
medical encyclopaedia are added into the
ontology. For ontology evaluation,
experiments on web search were conducted
using the constructed ontology. The
experimental results show that the proposed
approach can automatically align and extract
the domain-specific ontology. In addition, the
extracted ontology also shows its promising
ability for medical web search.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996965470588235">
In intelligent mining, in order to obviate the
unnecessary keyword expansion, some knowledge
base should be involved in the intelligent
information system. In recent years, considerable
progress has been invested in developing the
conceptual bases for building technology that
allows knowledge reuse and sharing. As
information exchangeability and communication
becomes increasingly global, multiple-language
lexical resources that can provide transnational
services are becoming increasingly important.
Over the last few years, significant effort has been
made to construct the ontology manually according
to the domain expert’s knowledge. Manual
ontology merging using conventional editing tools
without intelligent support is difficult, labor
intensive and error prone. Therefore, several
systems and frameworks for supporting the
knowledge engineer in the ontology merging task
have recently been proposed. To avoid the
reiteration in ontology construction, the algorithm
of ontology merge (UMLS
http://umlsks.nlm.nih.gov/) (Langkilde and Knight
1998) and ontology alignment (Vossen and Peters
1997) (Weigard and Hoppenbrouwers 1998)
(Asanoma 2001) were invested. The final ontology
is a merged version of the original ontologies. The
two original ontologies persist, with links
established between them in alignment. Alignment
usually is performed when the ontologies cover
domains that are complementary to each other. In
the past, domain ontology was usually constructed
manually according to the knowledge or
experience of the experts or ontology engineers.
Recently, automatic and semi-automatic methods
have been developed. OntoExtract (Fensel et al.
2002) (Missikoff et al. 2002) provided an ontology
engineering chain to construct the domain ontology
from WordNet and SemCor.
On the other hand, multi-lingual ontology is very
important for natural language processing, such as
machine translation (MT), web mining and cross
language information retrieval (CLIR). Generally,
a multi-lingual ontology maps the keyword set of
one language to another language, or compute the
co-occurrence of the words among languages. In
addition, a key merit for multilingual ontology is
that it can increase the relation and structural
information coverage by aligning two or more
language-dependent ontologies with different
semantic features.
Nowadays large collections of information in
various styles are available on the Internet. And
finding desired information on the World Wide
Web is becoming a critical issue. Some general-
purpose search engine like Google
(http://www.google.com) and Altavista
(http://www.altavista.com/) provide the facility to
mine the web. There are three major research areas
about web mining: web content mining, web
structure mining and web usage mining. This paper
proposes a novel method to web content mining
with unstructured web pages. There are many
approaches in the view of natural language
processing. According to the representation of web
pages, there are three kinds of the content: bag of
words (with order or not) (Kargupta et al. 1997)
(Nahm and Mooney, 2000), phrases (Ahonen et al.
1998) (Frank et al. 1999)(Yang et al. 1999),
relational terms (Cohen, 1998) (Junker 1999) and
concept categories. We proposed an ontology-
based web search approach. Unfortunately, there
are some irrelevant pages obtained and these pages
result in low precision rate and recall rate due to
the problem of polysemy. To solve this problem,
domain knowledge becomes necessary. The
domain-specific web miners like SPIRAL, Cora
(Cohen, 1998), WebKB (Martin and Eklund 2000)
and HelpfulMed (Chen et al. 2003) are employed
as the special search engine for the interesting
topic. These ones dedicated to recipes are less
likely to return irrelevant web pages when the
query is entered.
In this paper, WordNet and HowNet knowledge
bases are aligned to construct a bilingual universal
ontology based on the co-occurrence of the words
in a parallel corpus. For domain-specific
applications, a medical domain ontology is further
extracted from the universal ontology using the
island-driven algorithm (Lee et al. 1995) and a
medical domain corpus. Finally, the axioms
between medical terms are derived based on
semantic relations. A web search system for
medical domain based on the extracted domain
ontology is realized to demonstrate the feasibility
of the methods proposed in this paper.
The rest of the paper is organized as follows.
Section 2 describes ontology construction process
and the web searching system framework. Section
3 presents the experimental results for the
evaluation of our approach. Section 4 gives some
concluding remarks.
</bodyText>
<sectionHeader confidence="0.986611" genericHeader="introduction">
2 Methodologies
</sectionHeader>
<bodyText confidence="0.999079571428571">
Figure 1 shows the block diagram for ontology
construction and the framework of the domain-
specific web search system. There are four major
processes in the proposed system: bilingual
ontology alignment, domain ontology extraction,
knowledge representation and domain-specific web
search.
</bodyText>
<subsectionHeader confidence="0.98968">
2.1 Bilingual Ontology Alignment
</subsectionHeader>
<bodyText confidence="0.999629125">
In this approach, the cross-lingual ontology is
constructed by aligning the words in WordNet with
their corresponding words in HowNet. First, the
Sinorama (Sinorama 2001) database is adopted as
the bilingual language parallel corpus to compute
the conditional probability of the words in
WordNet, given the words in HowNet. Second, a
bottom up algorithm is used for relation mapping.
</bodyText>
<figureCaption confidence="0.885529">
Figure 1 Ontology construction framework and the domain-specific web search system
</figureCaption>
<bodyText confidence="0.95723875">
In WordNet a word may be associated with many
synsets, each corresponding to a different sense of
the word. When we look for a relation between two
different words we consider all the synsets
associated with each word (Christiane 1998). In
HowNet, each word is composed of primary
features and secondary features. The primary
features indicate the word’s category. The purpose
of this approach is to increase the relation and
structural information coverage by aligning the
above two language-dependent ontologies,
WordNet and HowNet, with different semantic
features.
The relation “is-a” defined in WordNet
corresponds to the primary feature defined in
HowNet. Equation (1) shows the mapping between
the words in HowNet and the synsets in WordNet.
Given a Chinese word, CWi , the probability of the
word related to synset, synsetk , can be obtained
via its corresponding English synonyms,
EWj j = m , which are the elements in
k, 1,...,
synsetk . The probability is estimated as follows.
where {enitity,event,act,play} is the concept set in
the root nodes of HowNet and WordNet.
Finally, the Chinese concept,
been
integrated into the synset ,
, in WordNet as
long as the probability,
zero. Figure 2 shows the concept tree generated by
CWi, has
</bodyText>
<equation confidence="0.92273875">
k
synsetj
Pr (synset |CW i ) , is not
k
d HowNet.
Pr( synsetk  |CW i)
Pr( synsetk, EWj  |CW) (1)
(Pr( synsetk  |EWjk, CW,.) × Pr(EWjk  |CW i))
</equation>
<bodyText confidence="0.995793818181818">
where
Pr( s ynsetk  |EWk,CW ) = N synset EW CW (2)
k k
( , )
j ,
j i
l k
∑ N synset EW CW
( , , )
j j i
l
</bodyText>
<equation confidence="0.960223642857143">
m
= ∑
1
j=
m
=∑
1
j=
In the above equation, ( ,
N synset j EW j CW i
k k , )
represents the number of co-occurrences of C Wi ,
EWj and k
k synset j
</equation>
<bodyText confidence="0.562976">
set to one when at least one of the primary features,
PF i CWi , of the Chinese word defined in the
</bodyText>
<equation confidence="0.7677696">
l ( )
HowNet matches one of the ancestor nodes of
synset , ( )
synsetj EWj except the root nodes in
k
</equation>
<bodyText confidence="0.9956535">
the hierarchical structures of the noun and verb;
Otherwise set to zero. That is,
</bodyText>
<figure confidence="0.2305498">
Pr(EWj |CW,.)
(U PFl (CW,.) −{entity, event, act, play})
l
(3)
aligning WordNet an
</figure>
<figureCaption confidence="0.847152">
Figure 2. Concept tree generated by aligning
</figureCaption>
<bodyText confidence="0.9493384">
WordNet and HowNet. The nodes with bold circle
represent the operative nodes after concept
extraction. The nodes with gray background
represent the operative nodes after relation
expansion.
</bodyText>
<subsectionHeader confidence="0.998529">
2.2 Domain ontology extraction
</subsectionHeader>
<bodyText confidence="0.944175103448276">
There are two phases to construct the domain
ontology: 1) extract the ontology from the cross-
language ontology by island-driven algorithm, and
2) integrate the terms and axioms defined in a
medical encyclopaedia into the domain ontology.
2.2.1 Extraction by island-driven algorithm
Ontology provides consistent concepts and world
representations necessary for clear communication
within the knowledge domain. Even in domain-
specific applications, the number of words can be
expected to be numerous. Synonym pruning is an
effective alternative to word sense disambiguation.
This paper proposes a corpus-based statistical
approach to extracting the domain ontology. The
steps are listed as follows:
Step 1: Linearization: This step decomposed the
tree structure in the universal ontology shown in
Figure 2 into the vertex list that is an ordered node
sequence starting at the leaf nodes and ending at the
root node.
Step 2: Concept extraction from the corpus: The
node is defined as an operative node when the Tf-
idf value of word
in the domain corpus is
higher than
Wi
that in its corresponding contrastive
(out-of-domain) corpus. That is,
. The probability Pr(EWk |CWi) is
</bodyText>
<equation confidence="0.9973825">
⎪⎪ = ⎨ 1 if
∩(Uancestor(Usynsetj(EWj))−{entity, event, act, play} )≠ ∅
⎪ k
⎪⎩0 otherwise
node W
( )
i
1, if Tf idf W Tf idf
− ( ) &gt; − ( )
Domain i Contrastive W i
Otherwise
(Wi) _ freqi,Domain x log
Tf idf
− Contrastive (Wi) _ freqi,Contrastive x log
</equation>
<bodyText confidence="0.9952605">
In the above equations, freqi,Domain and
freqi,Contrastive are the frequencies of word Wi in
the domain documents and its contrastive (out-of-
domain) documents, respectively. ni,Domain and
ni,Contrastive are the numbers of the documents
containing word Wi in the domain documents and
its contrastive documents, respectively. The nodes
with bold circle in Figure 2 represent the operative
nodes.
Step 3: Relational expansion using the island-
driven algorithm: There are some domain concepts
not operative after the previous steps due to the
problem of insufficient data. From the observation
in ontology construction, most of the inoperative
concept nodes have operative hypernym nodes and
hyponym nodes. Therefore, the island-driven
algorithm is adopted to activate these inoperative
concept nodes if their ancestors and descendants are
all operative. The nodes with gray background
shown in Figure 2 are the activated operative nodes.
Step 4: Domain ontology extraction: The final
step is to merge the linear vertex list sequence into
a hierarchical tree. However, some noisy concepts
not belonging to this domain ontology are operative
after step 3. These noisy nodes with inoperative
noisy concepts should be filtered out automatically.
Finally, the domain ontology is extracted and the
final result is shown in Figure 3.
After the above steps, a dummy node is added as
the root node of the domain concept tree.
Figure 3 The domain ontology after filtering out the
isolated concepts
</bodyText>
<subsubsectionHeader confidence="0.817092">
2.2.2 Axiom and terminology integration
</subsubsectionHeader>
<bodyText confidence="0.999851928571429">
In practice, specific domain terminologies and
axioms should be derived and introduced into the
ontology for domain-specific applications. In our
approach, 1213 axioms derived from a medical
encyclopaedia have been integrated into the domain
ontology. Figure 4 shows an example of the axiom.
In this example, the disease “diabetes” is tagged as
level “A” which represents that this disease is
frequent in occurrence. And the degrees for the
corresponding syndromes represent the causality
between the disease and the syndromes. The axioms
also provide two fields “department of the clinical
care” and “the category of the disease” for medical
information retrieval.
</bodyText>
<figureCaption confidence="0.891271">
Figure 4 axiom example
</figureCaption>
<subsectionHeader confidence="0.96583">
2.3 Domain-specific web search
</subsectionHeader>
<bodyText confidence="0.97459175">
This paper proposed a medical web search engine
based on the constructed medical domain ontology.
The engine consists of natural language interface,
web crawler and indexer, relation inference module
and axiom inference module. The functions and
techniques of these modules are described as
follows.
2.3.1 Natural language interface and web
crawler and indexer
Natural language interface is generally
considered as an enticing prospect because it offers
many advantages: it would be easy to learn and
easy to remember, because its structure and
vocabulary are already familiar to the user; it is
particularly powerful because of the multitude of
ways in which to accomplish a search action by
using the natural language input. A natural
language query is transformed to obtain the desired
representation after the word segmentation,
removing the stop words, stemming and tagging
process.
The web crawler and indexer are designed to seek
medical web pages from Internet, extract the
content and establish the indices automatically.
</bodyText>
<figure confidence="0.967793266666667">
operative
⎧
⎨
⎩
0,
(4)
where
Tf idf
− Domain
ni,Domain + ni,Contrastive
ni,Domain
ni,Domain +ni,Contrastive
ni Contrastive
,
2.3.2 Concept inference module
</figure>
<bodyText confidence="0.999030142857143">
For semantic representation, traditionally, the
keyword-based systems will introduce two
problems. First, ambiguity usually results from the
polysemy of words. The domain ontology will give
a clear description of the concepts. In addition, not
all the synonyms of the word should be expanded
without constraints. Secondly, relations between the
concepts should be expanded and weighted in order
to include more semantic information for semantic
inference. We treat each of the user’s input and the
content of web pages as a sequence of words. This
means that the sequence of words is treated as a bag
of words regardless of the word order. For the word
sequence of the user’s input,
</bodyText>
<equation confidence="0.73191">
q=Wq=wq1, wq2 ,..., wqK ,
</equation>
<bodyText confidence="0.989713166666667">
and the word sequence of the web page,
A=WA=wA1, wA2, ..., wAL,
The similarity between input query and the page is
defined as the similarity between the two bags of
words. The similarity measure based on key
concepts in the ontology is defined as follows.
</bodyText>
<equation confidence="0.755840666666667">
Sim ( )
A q
,
relation i
( )
W W
,
A q
i
</equation>
<bodyText confidence="0.998474">
diseases with three levels according to its
occurrence and syndromes with four levels
according to its significance to the specific disease.
The “result in” relation score is defined as RI(Ai, q)
if a disease occurs in the input query and its
corresponding syndromes appear in the web page.
Similarly, if syndrome occurs in the input query and
its corresponding disease appears in the web page,
the “result from” relation score is defined as
RF(Ai,q) . The relation score is estimated as follows.
</bodyText>
<equation confidence="0.6752378">
= max{ ( , ), ( , )}
RI A q RF A q
i i
= max{ ( , ,..., , , ,..., ),
RI w w w w w w
A 1 A 2 A P q 1 q 2 qR
i i i
RF w w
( , ,..., , , ,..., )}
w w w w
A 1 A 2 A P q 1 q 2 qR
i i i
P ,R P ,R RI
RF},
=max{ ∑ dpr, ∑ dpr
p=1,r=1 p=1,r=1
(7)
RI −1
where 1/ 2n
=
</equation>
<bodyText confidence="0.710408">
dpr if disease wAp results in
syndrome wqr and wqr is the top-n feature of wAp .
</bodyText>
<equation confidence="0.97416">
RF −1
Similarly, 1/ 2n
=
</equation>
<bodyText confidence="0.906844">
dpr if syndrome wAp results
from disease wqr and wAp is the top-n feature of
wqr. The conditional probability of the i-th web
pages with respect to aspect
defined as
</bodyText>
<figure confidence="0.98177371875">
Simrelation
( )
A q
,
i
Axiom
sA ,2 and query q is
Simrelation
.
K ,L
= ∑
Sim A q
( )
, =
axiom i
Hkl
1, 1
l=
∑Axiom A q
( )
,
i
i
k =
( , ,..., , , ,..., )
w w w w w w
A 1 A 2 A L q 1 q 2 qK
i i i
Axiom A q
( )
,
i
</figure>
<bodyText confidence="0.99987725">
where Hkl is concept similarity of wAl and wqk. Most
of the keyword expansion approaches use the
extension of scope by the synonyms. In this paper
the similarity, Hkl, is defined as
</bodyText>
<subsectionHeader confidence="0.853431">
2.3.3 Axiom inference module
</subsectionHeader>
<bodyText confidence="0.99993">
Some axioms, such as “result in” and “result
from,” that are expected to affect the performance
of a web search system in a technical domain are
defined to describe the relationship between
syndromes and diseases. This aspect is the use of
specific terms used in the medical domain. We
collected the data about syndromes and diseases
from a medical encyclopedia and tagged the
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999959266666667">
To evaluate the proposed approach, a medical
web search system was constructed. The web pages
were collected from several Websites and totally
2322 web pages for medical domain and 8133 web
pages for contrastive domain were collected.
On the other hand, the training and test queries
for training and evaluating the system performance
were also collected. Forty users, who do not take
part in the system development, were asked to
provide a set of queries given the collected web
pages. After post-processing, the duplicate queries
and the queries out of the medical domain are
removed. Finally, 3207 test queries mixed Chinese
with English words using natural language were
obtained.
</bodyText>
<subsectionHeader confidence="0.6050755">
3.1 Keyword-Based VSM Approach: A
baseline system for comparison
</subsectionHeader>
<bodyText confidence="0.94157175">
In recent years, most of the information retrieval
approaches were based on the Vector-Space Model
(VSM). Assuming that the query is denoted as a
vector q = (q1, q2,..., qn) and the web page is
</bodyText>
<figure confidence="0.991867592592593">
1 w and are identical
w
Al qk
wAl
r is the number of levels in between
and are synonyms
w ,
qk
r is the number of their common concepts
others
Hkl
⎧
⎪
⎪
⎪
= ⎨
1
2r
and are hypernyms
w ,
qk
(1−
1 \2
2) r
⎪
⎪⎩0
w Al
</figure>
<bodyText confidence="0.92920625">
represented as a vector A = (a1 , a2,..., an) . The
Tf-idf measure is employed and the similarity can
be measured by the cosine function defined as
follows.
</bodyText>
<figure confidence="0.945022571428571">
11-avgP score 0.7
0.66
0.62
0.58
0.54
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α
</figure>
<figureCaption confidence="0.991501">
Figure 5 The 11-avgP score with different values of α
</figureCaption>
<subsectionHeader confidence="0.984887">
3.3 Evaluation on different inference modules
</subsectionHeader>
<bodyText confidence="0.999879166666667">
In the following experiments, web pages were
separately evaluated by focusing on one inference
module based on the domain-specific ontology at a
time. That is, the mixture weight is set to 1 for one
inference module and the other is set to 0 in each
evaluation. For comparison, the keyword-based
VSM approach and the ontology-based system are
also evaluated and shown in Figure 6. The precision
and recall rates are used as the evaluation measures.
And the ontology based approach means the
combination of concept inference and axiom
inference described in the section 3.2.
</bodyText>
<figure confidence="0.9853655">
0 10 20 30 40 50 60 70 80 90 100
Recall rate (%)
</figure>
<figureCaption confidence="0.993469">
Figure 6 The precision rates and recall rates of the
proposed method and the baseline system
</figureCaption>
<sectionHeader confidence="0.998324" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999535">
This paper has presented an approach to
automated ontology alignment and domain
ontology extraction from two knowledge bases. In
this approach, a bilingual ontology is developed
using a corpus-based statistical approach from two
well established language-dependent knowledge
bases, WordNet and HowNet. A domain-dependent
ontology is further extracted from the universal
ontology using the island-driven algorithm and a
domain corpus. In addition, domain-specific terms
and axioms are also added into the domain ontology.
We have applied the domain-specific ontology to
the web page search in medical domain. The
experimental results show that the proposed
approach outperformed the keyword-based and
synonym expansion approaches.
</bodyText>
<sectionHeader confidence="0.991577" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.611190823529412">
N. Asanoma. 2001 Alignment of Ontologies:
WordNet and Goi-Taikei. WordNet and Other
Lexical Resources Workshop Program,
NAACL2001. 89-94
Christiane Fellbaum, 1998 WordNet an electronic
Lexical Database, The MIT Press 1998. pp307-
308
Fensel, D., Bussler, C., Ding, Y., Kartseva1, V.,
Klein, M., Korotkiy, M., Omelayenko, B. and
Siebes R. 2002 Semantic Web Application Areas,
the 7th International Workshop on Applications
of Natural Language to Information Systems
(NLDB02).
M. Missikoff,, R. Navigli, and P. Velardi. 2002
Integrated approach to Web ontology learning
and engineering, Computer , Volume: 35 Issue:
11 . 60 –63
</bodyText>
<figure confidence="0.998860578947368">
Precision rate (%)
100
40
90
80
70
60
50
30
20
10
0
Ontology based
Baseline+concept infernece
module
Baseline+axiom inference
module
Baseline
(8)
</figure>
<bodyText confidence="0.89068525">
where a =1 . This approach for key term
expansion based on synonym set is also adopted in
the baseline system. The results and discussions are
described in the following sections.
</bodyText>
<subsectionHeader confidence="0.867887">
3.2 Weight determination using 11-avgP score
</subsectionHeader>
<bodyText confidence="0.998981">
The medical domain web search system is
modeled by the linear combination of relational
inference model and axiom inference model. The
normalized weight factor, α , is employed for
concept expansion as follows.
</bodyText>
<equation confidence="0.9862325">
Sim A q = − α Sim relation A i q + α × Simaxiom A i q (9)
( i , ) (1 ) ( , ) ( , )
</equation>
<bodyText confidence="0.997070142857143">
This experiment is conducted on the estimation of
the combination weights for each model. The
results are shown in Figure 5. The performance
measure called 11-AvgP [Eichmann and Srinivasan
1998] was used to summarize the precision and
recall rates. The best 11-AvgP score will be
obtained when the weight α = 0.428.
</bodyText>
<figure confidence="0.996114368421053">
Simkeyword based
−
n 2 n 2
∑ ∑
a ×
i i
1 i i
q
= =1
( , ) cos(a,q)
A q =
i
n
∑
a q
i i
i
=1
=
</figure>
<reference confidence="0.999746305555556">
N.F. Noy, and M. Musen,. 2000 PROMPT:
Algorithm and Tool for Automated Ontology
Merging and Alignment, Proceedings of the
National Conference on Artificial Intelligence.
AAAI2000. 450-455
Sinorama Magazine and Wordpedia.com Co. 2001
Multimedia CD-ROMs of Sinorama from 1976 to
2000, Taipei.
P. Vossen, and W. Peters, 1997 Multilingual design
of EuroWordNet, Proceedings of the Delos
workshop on Cross-language Information
Retrieval.
H. Weigard, and S. Hoppenbrouwers, 1998,
Experiences with a multilingual ontology-based
lexicon for news filtering, Proceedings in the 9th
International Workshop on Database and Expert
Systems Applications. 160-165
H. Kargupta, I. Hamzaoglu, and B. Stafford. 1997.
Distributed data mining using an agent based
architecture. In Proceedings of Knowledge
Discovery and Data Mining, 211-214.
U. Y. Nahm and R. J. Mooney. 2000. A mutually
beneficial integration of data mining and
information extraction. In Proceeding of the
AAAI-00.
H. Ahonen, O. Heinoen, M. Klemettinen, and A.
Verkamo. 1998. Applying data mining techniques
for descriptive phrase extraction in digital
document collections. In Advence in Digital
Libraries.
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin,
and C. G. Nevill-Manning. 1999. Domain-
specific keyphrase extraction. In proceding of
IJCAI-99, 668-673.
Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T.
Archibald, and X. Liu. 1999. Learning
approaches for detecting and tracking news
events. IEEE Intelligent Systems, 14(4):32-43.
W. W. Cohen. 1998. A web-based information
system that reasons with structured collocations
of text. In Proceedings of 2nd Agent&apos;98
M. Junker, M. Sintek, and M. Rinck. 1999 Learning
for text categorization and information extraction
with ilp. In Proceedings of the Workshop on
Learning Language in Logic, Bled, Slovenia
S. Oyama, T. Kokubo, and T. Ishida. 2004 Domain-
Specific Web Search with Keyword Spice. IEEE
Transactions on Knowledge and Data
Engineering, Vol 16,NO. 1, 17-27.
Sankar K. Pal, Varun Talwar, and Pabitra Mitra.
2002. Web Minging in Soft Computing
Framework: Relevance, State of the Art and
Future Directions. IEEE Transactions on Neural
Networks, Vol. 13, NO. 5.
T. Hofmann. 1999. The cluster-abstraction model:
Unsupervised learning of topic hierarchies from
text data. In Proceedings of 16th IJCAI, 682-687,
P. Martin and P. Eklund. 2000. Knowledge
Indexation and Retrieval and the Word Wide
Web. IEEE Intelligent Systems, special issue
&amp;quot;Knowledge Management and Knowledge
Distribution over the Internet&amp;quot;
H. Chen, A. M. Lally, B. Zhu, and M. Chau. ,2003
HelpfulMed: Intelligent Searching for Medical
Information over the internet. Journal od the
American Society for Information Science and
Technology, 54(7):683-694.
D. Eichmann, , Ruiz, M., Srinivasan, P., 1998
Cross-language information retrieval with the
UMLS Metathesaurus, Proceeding of ACM
Special Interest Group on Information Retreival
(SIGIR), ACM Press, NY (1998), 72-80.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365244">
<title confidence="0.9377585">Automated Alignment and Extraction of Bilingual Domain Ontology for Medical Domain Web Search</title>
<author confidence="0.803102">Chung-Hsien Ming-Jun</author>
<author confidence="0.803102">Liang-Chih</author>
<affiliation confidence="0.9355885">of Computer Science and Information National Cheng Kung University, Taiwan,</affiliation>
<email confidence="0.849471">{chwu,jfyeh,</email>
<title confidence="0.689183">of Computer Application</title>
<author confidence="0.612439">Far East College</author>
<author confidence="0.612439">R O C Taiwan</author>
<abstract confidence="0.999665962962963">This paper proposes an approach to automated ontology alignment and domain ontology extraction from two knowledge bases. First, WordNet and HowNet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus. The bilingual universal ontology has the merit that it contains more structural and semantic information coverage from complementary knowledge bases, WordNet and HowNet. For domain-specific applications, a medical domain ontology is further extracted from the universal ontology using the islanddriven algorithm and a medical domain corpus. Finally, the domain-dependent terms and some axioms between medical terms based on a medical encyclopaedia are added into the ontology. For ontology evaluation, experiments on web search were conducted using the constructed ontology. The experimental results show that the proposed approach can automatically align and extract the domain-specific ontology. In addition, the extracted ontology also shows its promising ability for medical web search.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N F Noy</author>
<author>M Musen</author>
</authors>
<title>PROMPT: Algorithm and Tool for Automated Ontology Merging and Alignment,</title>
<date>2000</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence. AAAI2000.</booktitle>
<pages>450--455</pages>
<marker>Noy, Musen, 2000</marker>
<rawString>N.F. Noy, and M. Musen,. 2000 PROMPT: Algorithm and Tool for Automated Ontology Merging and Alignment, Proceedings of the National Conference on Artificial Intelligence. AAAI2000. 450-455</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinorama Magazine</author>
<author>Wordpedia com Co</author>
</authors>
<title>Multimedia CD-ROMs of Sinorama from</title>
<date>2001</date>
<location>Taipei.</location>
<note>to</note>
<marker>Magazine, Co, 2001</marker>
<rawString>Sinorama Magazine and Wordpedia.com Co. 2001 Multimedia CD-ROMs of Sinorama from 1976 to 2000, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
<author>W Peters</author>
</authors>
<date>1997</date>
<booktitle>Multilingual design of EuroWordNet, Proceedings of the Delos workshop on Cross-language Information Retrieval.</booktitle>
<contexts>
<context position="2596" citStr="Vossen and Peters 1997" startWordPosition="346" endWordPosition="349">increasingly important. Over the last few years, significant effort has been made to construct the ontology manually according to the domain expert’s knowledge. Manual ontology merging using conventional editing tools without intelligent support is difficult, labor intensive and error prone. Therefore, several systems and frameworks for supporting the knowledge engineer in the ontology merging task have recently been proposed. To avoid the reiteration in ontology construction, the algorithm of ontology merge (UMLS http://umlsks.nlm.nih.gov/) (Langkilde and Knight 1998) and ontology alignment (Vossen and Peters 1997) (Weigard and Hoppenbrouwers 1998) (Asanoma 2001) were invested. The final ontology is a merged version of the original ontologies. The two original ontologies persist, with links established between them in alignment. Alignment usually is performed when the ontologies cover domains that are complementary to each other. In the past, domain ontology was usually constructed manually according to the knowledge or experience of the experts or ontology engineers. Recently, automatic and semi-automatic methods have been developed. OntoExtract (Fensel et al. 2002) (Missikoff et al. 2002) provided an </context>
</contexts>
<marker>Vossen, Peters, 1997</marker>
<rawString>P. Vossen, and W. Peters, 1997 Multilingual design of EuroWordNet, Proceedings of the Delos workshop on Cross-language Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Weigard</author>
<author>S Hoppenbrouwers</author>
</authors>
<title>Experiences with a multilingual ontology-based lexicon for news filtering,</title>
<date>1998</date>
<booktitle>Proceedings in the 9th International Workshop on Database and Expert Systems Applications.</booktitle>
<pages>160--165</pages>
<contexts>
<context position="2630" citStr="Weigard and Hoppenbrouwers 1998" startWordPosition="350" endWordPosition="353">ver the last few years, significant effort has been made to construct the ontology manually according to the domain expert’s knowledge. Manual ontology merging using conventional editing tools without intelligent support is difficult, labor intensive and error prone. Therefore, several systems and frameworks for supporting the knowledge engineer in the ontology merging task have recently been proposed. To avoid the reiteration in ontology construction, the algorithm of ontology merge (UMLS http://umlsks.nlm.nih.gov/) (Langkilde and Knight 1998) and ontology alignment (Vossen and Peters 1997) (Weigard and Hoppenbrouwers 1998) (Asanoma 2001) were invested. The final ontology is a merged version of the original ontologies. The two original ontologies persist, with links established between them in alignment. Alignment usually is performed when the ontologies cover domains that are complementary to each other. In the past, domain ontology was usually constructed manually according to the knowledge or experience of the experts or ontology engineers. Recently, automatic and semi-automatic methods have been developed. OntoExtract (Fensel et al. 2002) (Missikoff et al. 2002) provided an ontology engineering chain to cons</context>
</contexts>
<marker>Weigard, Hoppenbrouwers, 1998</marker>
<rawString>H. Weigard, and S. Hoppenbrouwers, 1998, Experiences with a multilingual ontology-based lexicon for news filtering, Proceedings in the 9th International Workshop on Database and Expert Systems Applications. 160-165</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kargupta</author>
<author>I Hamzaoglu</author>
<author>B Stafford</author>
</authors>
<title>Distributed data mining using an agent based architecture.</title>
<date>1997</date>
<booktitle>In Proceedings of Knowledge Discovery and Data Mining,</booktitle>
<pages>211--214</pages>
<contexts>
<context position="4571" citStr="Kargupta et al. 1997" startWordPosition="637" endWordPosition="640">ation on the World Wide Web is becoming a critical issue. Some generalpurpose search engine like Google (http://www.google.com) and Altavista (http://www.altavista.com/) provide the facility to mine the web. There are three major research areas about web mining: web content mining, web structure mining and web usage mining. This paper proposes a novel method to web content mining with unstructured web pages. There are many approaches in the view of natural language processing. According to the representation of web pages, there are three kinds of the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the </context>
</contexts>
<marker>Kargupta, Hamzaoglu, Stafford, 1997</marker>
<rawString>H. Kargupta, I. Hamzaoglu, and B. Stafford. 1997. Distributed data mining using an agent based architecture. In Proceedings of Knowledge Discovery and Data Mining, 211-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Y Nahm</author>
<author>R J Mooney</author>
</authors>
<title>A mutually beneficial integration of data mining and information extraction.</title>
<date>2000</date>
<booktitle>In Proceeding of the AAAI-00.</booktitle>
<contexts>
<context position="4595" citStr="Nahm and Mooney, 2000" startWordPosition="641" endWordPosition="644"> Web is becoming a critical issue. Some generalpurpose search engine like Google (http://www.google.com) and Altavista (http://www.altavista.com/) provide the facility to mine the web. There are three major research areas about web mining: web content mining, web structure mining and web usage mining. This paper proposes a novel method to web content mining with unstructured web pages. There are many approaches in the view of natural language processing. According to the representation of web pages, there are three kinds of the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the interesting topic. These</context>
</contexts>
<marker>Nahm, Mooney, 2000</marker>
<rawString>U. Y. Nahm and R. J. Mooney. 2000. A mutually beneficial integration of data mining and information extraction. In Proceeding of the AAAI-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ahonen</author>
<author>O Heinoen</author>
<author>M Klemettinen</author>
<author>A Verkamo</author>
</authors>
<title>Applying data mining techniques for descriptive phrase extraction in digital document collections.</title>
<date>1998</date>
<booktitle>In Advence in Digital Libraries.</booktitle>
<contexts>
<context position="4625" citStr="Ahonen et al. 1998" startWordPosition="646" endWordPosition="649">. Some generalpurpose search engine like Google (http://www.google.com) and Altavista (http://www.altavista.com/) provide the facility to mine the web. There are three major research areas about web mining: web content mining, web structure mining and web usage mining. This paper proposes a novel method to web content mining with unstructured web pages. There are many approaches in the view of natural language processing. According to the representation of web pages, there are three kinds of the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the interesting topic. These ones dedicated to recipes are</context>
</contexts>
<marker>Ahonen, Heinoen, Klemettinen, Verkamo, 1998</marker>
<rawString>H. Ahonen, O. Heinoen, M. Klemettinen, and A. Verkamo. 1998. Applying data mining techniques for descriptive phrase extraction in digital document collections. In Advence in Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>G W Paynter</author>
<author>I H Witten</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>Domainspecific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In proceding of IJCAI-99,</booktitle>
<pages>668--673</pages>
<contexts>
<context position="4645" citStr="Frank et al. 1999" startWordPosition="650" endWordPosition="653"> search engine like Google (http://www.google.com) and Altavista (http://www.altavista.com/) provide the facility to mine the web. There are three major research areas about web mining: web content mining, web structure mining and web usage mining. This paper proposes a novel method to web content mining with unstructured web pages. There are many approaches in the view of natural language processing. According to the representation of web pages, there are three kinds of the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the interesting topic. These ones dedicated to recipes are less likely to retu</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. Nevill-Manning. 1999. Domainspecific keyphrase extraction. In proceding of IJCAI-99, 668-673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J Carbonell</author>
<author>R Brown</author>
<author>T Pierce</author>
<author>B T Archibald</author>
<author>X Liu</author>
</authors>
<title>Learning approaches for detecting and tracking news events.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems,</journal>
<pages>14--4</pages>
<contexts>
<context position="4663" citStr="Yang et al. 1999" startWordPosition="653" endWordPosition="656"> Google (http://www.google.com) and Altavista (http://www.altavista.com/) provide the facility to mine the web. There are three major research areas about web mining: web content mining, web structure mining and web usage mining. This paper proposes a novel method to web content mining with unstructured web pages. There are many approaches in the view of natural language processing. According to the representation of web pages, there are three kinds of the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the interesting topic. These ones dedicated to recipes are less likely to return irrelevant web </context>
</contexts>
<marker>Yang, Carbonell, Brown, Pierce, Archibald, Liu, 1999</marker>
<rawString>Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald, and X. Liu. 1999. Learning approaches for detecting and tracking news events. IEEE Intelligent Systems, 14(4):32-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>A web-based information system that reasons with structured collocations of text.</title>
<date>1998</date>
<booktitle>In Proceedings of 2nd Agent&apos;98</booktitle>
<contexts>
<context position="4695" citStr="Cohen, 1998" startWordPosition="659" endWordPosition="660">ltavista (http://www.altavista.com/) provide the facility to mine the web. There are three major research areas about web mining: web content mining, web structure mining and web usage mining. This paper proposes a novel method to web content mining with unstructured web pages. There are many approaches in the view of natural language processing. According to the representation of web pages, there are three kinds of the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the interesting topic. These ones dedicated to recipes are less likely to return irrelevant web pages when the query is entered.</context>
</contexts>
<marker>Cohen, 1998</marker>
<rawString>W. W. Cohen. 1998. A web-based information system that reasons with structured collocations of text. In Proceedings of 2nd Agent&apos;98</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Junker</author>
<author>M Sintek</author>
<author>M Rinck</author>
</authors>
<title>Learning for text categorization and information extraction with ilp.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Learning Language in Logic,</booktitle>
<location>Bled, Slovenia</location>
<marker>Junker, Sintek, Rinck, 1999</marker>
<rawString>M. Junker, M. Sintek, and M. Rinck. 1999 Learning for text categorization and information extraction with ilp. In Proceedings of the Workshop on Learning Language in Logic, Bled, Slovenia</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oyama</author>
<author>T Kokubo</author>
<author>T Ishida</author>
</authors>
<title>DomainSpecific Web Search with Keyword Spice.</title>
<date>2004</date>
<journal>IEEE Transactions on Knowledge and Data Engineering, Vol</journal>
<volume>16</volume>
<pages>17--27</pages>
<marker>Oyama, Kokubo, Ishida, 2004</marker>
<rawString>S. Oyama, T. Kokubo, and T. Ishida. 2004 DomainSpecific Web Search with Keyword Spice. IEEE Transactions on Knowledge and Data Engineering, Vol 16,NO. 1, 17-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sankar K Pal</author>
<author>Varun Talwar</author>
<author>Pabitra Mitra</author>
</authors>
<title>Web Minging in Soft Computing Framework: Relevance, State of the Art and Future Directions.</title>
<date>2002</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>13</volume>
<marker>Pal, Talwar, Mitra, 2002</marker>
<rawString>Sankar K. Pal, Varun Talwar, and Pabitra Mitra. 2002. Web Minging in Soft Computing Framework: Relevance, State of the Art and Future Directions. IEEE Transactions on Neural Networks, Vol. 13, NO. 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data.</title>
<date>1999</date>
<booktitle>In Proceedings of 16th IJCAI,</booktitle>
<pages>682--687</pages>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. 1999. The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data. In Proceedings of 16th IJCAI, 682-687,</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Martin</author>
<author>P Eklund</author>
</authors>
<title>Knowledge Indexation and Retrieval and the Word Wide Web.</title>
<date>2000</date>
<journal>IEEE Intelligent Systems, special</journal>
<contexts>
<context position="5086" citStr="Martin and Eklund 2000" startWordPosition="716" endWordPosition="719"> of web pages, there are three kinds of the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the interesting topic. These ones dedicated to recipes are less likely to return irrelevant web pages when the query is entered. In this paper, WordNet and HowNet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus. For domain-specific applications, a medical domain ontology is further extracted from the universal ontology using the island-driven algorithm (Lee et al. 1995) and a medical domain corpus. Finally, the axioms between medi</context>
</contexts>
<marker>Martin, Eklund, 2000</marker>
<rawString>P. Martin and P. Eklund. 2000. Knowledge Indexation and Retrieval and the Word Wide Web. IEEE Intelligent Systems, special issue &amp;quot;Knowledge Management and Knowledge Distribution over the Internet&amp;quot;</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>A M Lally</author>
<author>B Zhu</author>
<author>M Chau</author>
</authors>
<title>HelpfulMed: Intelligent Searching for Medical Information over the internet.</title>
<date>2003</date>
<journal>Journal od the American Society for Information Science and Technology,</journal>
<pages>54--7</pages>
<contexts>
<context position="5120" citStr="Chen et al. 2003" startWordPosition="722" endWordPosition="725">the content: bag of words (with order or not) (Kargupta et al. 1997) (Nahm and Mooney, 2000), phrases (Ahonen et al. 1998) (Frank et al. 1999)(Yang et al. 1999), relational terms (Cohen, 1998) (Junker 1999) and concept categories. We proposed an ontologybased web search approach. Unfortunately, there are some irrelevant pages obtained and these pages result in low precision rate and recall rate due to the problem of polysemy. To solve this problem, domain knowledge becomes necessary. The domain-specific web miners like SPIRAL, Cora (Cohen, 1998), WebKB (Martin and Eklund 2000) and HelpfulMed (Chen et al. 2003) are employed as the special search engine for the interesting topic. These ones dedicated to recipes are less likely to return irrelevant web pages when the query is entered. In this paper, WordNet and HowNet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus. For domain-specific applications, a medical domain ontology is further extracted from the universal ontology using the island-driven algorithm (Lee et al. 1995) and a medical domain corpus. Finally, the axioms between medical terms are derived based on sem</context>
</contexts>
<marker>Chen, Lally, Zhu, Chau, 2003</marker>
<rawString>H. Chen, A. M. Lally, B. Zhu, and M. Chau. ,2003 HelpfulMed: Intelligent Searching for Medical Information over the internet. Journal od the American Society for Information Science and Technology, 54(7):683-694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ruiz</author>
<author>P Srinivasan</author>
</authors>
<date>1998</date>
<booktitle>Cross-language information retrieval with the UMLS Metathesaurus, Proceeding of ACM Special Interest Group on Information Retreival (SIGIR),</booktitle>
<pages>72--80</pages>
<publisher>ACM Press,</publisher>
<location>NY</location>
<marker>Ruiz, Srinivasan, 1998</marker>
<rawString>D. Eichmann, , Ruiz, M., Srinivasan, P., 1998 Cross-language information retrieval with the UMLS Metathesaurus, Proceeding of ACM Special Interest Group on Information Retreival (SIGIR), ACM Press, NY (1998), 72-80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>