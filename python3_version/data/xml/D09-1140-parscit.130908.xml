<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003871">
<title confidence="0.997134">
Predicting Subjectivity in Multimodal Conversations
</title>
<author confidence="0.984263">
Gabriel Murray and Giuseppe Carenini
</author>
<affiliation confidence="0.8870685">
University of British Columbia
Vancouver, Canada
</affiliation>
<email confidence="0.973967">
(gabrielm, carenini)@cs.ubc.ca
</email>
<sectionHeader confidence="0.997121" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920133333333">
In this research we aim to detect sub-
jective sentences in multimodal conversa-
tions. We introduce a novel technique
wherein subjective patterns are learned
from both labeled and unlabeled data, us-
ing n-gram word sequences with vary-
ing levels of lexical instantiation. Ap-
plying this technique to meeting speech
and email conversations, we gain signifi-
cant improvement over state-of-the-art ap-
proaches. Furthermore, we show that cou-
pling the pattern-based approach with fea-
tures that capture characteristics of gen-
eral conversation structure yields addi-
tional improvement.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911869565217">
Conversations are rich in subjectivity. Conversa-
tion participants agree and disagree with one other,
argue for and against various proposals, and gen-
erally take turns expressing their private states.
Being able to separate these subjective utterances
from more objective utterances would greatly fa-
cilitate the analysis, mining and summarization of
a large number of conversations.
Two of the most prevalent conversational me-
dia are meetings and emails. Face-to-face meet-
ings enable numerous people to exchange a large
amount of information and opinions in a short pe-
riod of time, while emails allow for concise ex-
changes between potentially far-flung participants.
Meetings and emails can also feed into one an-
other, with face-to-face meetings occurring at reg-
ular intervals and emails continuing the conver-
sations in the interim. This poses several inter-
esting questions, such as whether subjective utter-
ances are more or less likely to be found in email
exchanges compared with meetings, and whether
the ratios of positive and negative subjective utter-
ances differ between the two modalities.
In this paper we describe a novel approach for
predicting subjectivity, and test it in two sets of
experiments on meetings and emails. Our ap-
proach combines a new general purpose method
for learning subjective patterns, with features that
capture basic characteristics of conversation struc-
ture across modalities. The subjective patterns are
essentially n-gram sequences with varying levels
of lexical instantiation, and we demonstrate how
they can be learned from both labeled and un-
labeled data. The conversation features capture
structural characteristics of multimodal conversa-
tions as well as participant information.
We test our approach in two sets of experi-
ments. The goal of the first set of experiments is to
discriminate subjective from non-subjective utter-
ances, comparing the novel approach to existing
state-of-the-art techniques. In the second set of
experiments, the goal is to discriminate positive-
subjective and negative-subjective utterances, es-
tablishing their polarity. In both sets of experi-
ments, we assess the impact of features relating
to conversation structure.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="introduction">
2 Related Research
</sectionHeader>
<bodyText confidence="0.999893125">
Raaijmakers et al. (2008) have approached
the problem of detecting subjectivity in meeting
speech by using a variety of multimodal features
such as prosodic features, word n-grams, charac-
ter n-grams and phoneme n-grams. For subjec-
tivity detection, they found that a combination of
all features was best, while prosodic features were
less useful for discriminating between positive and
negative utterances. They found character n-grams
to be particularly useful.
Riloff and Wiebe (2004) presented a method for
learning subjective extraction patterns from a large
amount of data, which takes subjective and non-
subjective text as input, and outputs significant
lexico-syntactic patterns. These patterns are based
on syntactic structure output by the Sundance shal-
</bodyText>
<page confidence="0.93596">
1348
</page>
<note confidence="0.9965795">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1348–1357,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999892903225807">
low dependency parser (Riloff and Phillips, 2004).
They are extracted by exhaustively applying syn-
tactic templates such as &lt; subj &gt; passive-verb
and active-verb &lt; dobj &gt; to a training cor-
pus, with an extracted pattern for every instan-
tiation of the syntactic template. These patterns
are scored according to probability of relevance
given the pattern and frequency of the pattern. Be-
cause these patterns are based on syntactic struc-
ture, they can represent subjective expressions that
are not fixed word sequences and would therefore
be missed by a simple n-gram approach.
Riloff et al. (2006) explore feature subsumption
for opinion detection, where a given feature may
subsume another feature representationally if the
strings matched by the first feature include all of
the strings matched by the second feature. To give
their own example, the unigram happy subsumes
the bigram very happy. The first feature will be-
haviorally subsume the second if it representa-
tionally subsumes the second and has roughly the
same information gain, within an acceptable mar-
gin. They show that they can improve opinion
analysis results by modeling these relations and
reducing the feature set.
Our approach for learning subjective patterns
like Raaijmakers et al. relies on n-grams, but like
Riloff et al. moves beyond fixed sequences of
words by varying levels of lexical instantiation.
Yu and Hatzivassiloglou (2003) addressed three
challenges in the news article domain: discrimi-
nating between objective documents and subjec-
tive documents such as editorials, detecting sub-
jectivity at the sentence level, and determining po-
larity at the sentence level. They found that the
latter two tasks were substantially more difficult
than classification at the document level. Of par-
ticular relevance here is that they found that part-
of-speech (POS) features were especially useful
for assigning polarity scores, with adjectives, ad-
verbs and verbs comprising the best set of POS
tags. This work inspired us to look at generaliza-
tion of n-grams based on POS.
On the slightly different task of classifying the
intensity of opinions, Wilson et al. (2006) em-
ployed several types of features including depen-
dency structures in which words can be backed off
to POS tags. They found that this feature class im-
proved the overall accuracy of their system.
Somasundaran et al. (2007) investigated subjec-
tivity classification in meetings. Their findings in-
dicate that both lexical features (list of words and
expressions) and discourse features (dialogue acts
and adjacency pairs) can be beneficial. In the same
spirit, we effectively combine lexical patterns and
conversational features.
The approach to predicting subjectivity we
present in this paper is a novel contribution to the
field of opinion and sentiment analysis. Pang and
Lee (2008) give an overview of the state of the art,
discussing motivation, features, approaches and
available resources.
</bodyText>
<sectionHeader confidence="0.980071" genericHeader="method">
3 Subjectivity Detection
</sectionHeader>
<bodyText confidence="0.999974555555555">
In this section we describe our approach to sub-
jectivity detection. We begin by describing how
to learn subjective n-gram patterns with varying
levels of lexical instantiation. We then describe a
set of features characterizing multimodal conver-
sation structure which can be used to supplement
the n-gram approach. Finally, we describe the
baseline subjectivity detection approaches used
for comparison.
</bodyText>
<subsectionHeader confidence="0.999647">
3.1 Partially Instantiated N-Grams
</subsectionHeader>
<bodyText confidence="0.999969518518519">
Our approach to subjectivity detection and polar-
ity detection is to learn significant patterns that
correlate with the subjective and polar utterances.
These patterns are word trigrams, but with varying
levels of lexical instantiation, so that each unit of
the n-gram can be either a word or the word’s part-
of-speech (POS) tag. This contrasts, then, with
work such as that of Raaijmakers et al. (2008)
who include trigram features in their experiments,
but where their learned trigrams are fully instanti-
ated. As an example, while they may learn that a
trigram really great idea is positive, we may addi-
tionally find that really great NN and RB great NN
are informative patterns, and these patterns may
sometimes be better cues than the fully instanti-
ated trigrams. To differentiate this approach from
the typical use of trigrams, we will refer to it as the
VIN (varying instantiation n-grams) method.
In some respects, our approach to subjectiv-
ity detection is similar to Riloff and Wiebe’s
work cited above, in the sense that their extrac-
tion patterns are partly instantiated. However,
the AutoSlog-TS approach relies on deriving syn-
tactic structure with the Sundance shallow parser
(Riloff and Phillips, 2004). We hypothesize that
our trigram approach may be more robust to dis-
fluent and fragmented meeting speech and emails
</bodyText>
<page confidence="0.992596">
1349
</page>
<table confidence="0.828772888888889">
1 2 3
really great idea
really great NN
really JJ idea
RB great idea
really JJ NN
RB great NN
RB JJ idea
RB JJ NN
</table>
<tableCaption confidence="0.998749">
Table 1: Sample Instantiation Set
</tableCaption>
<bodyText confidence="0.999991741379311">
on which syntactic parsers may perform poorly.
Also, our learned trigram patterns range from fully
instantiated to completely uninstantiated. For ex-
ample, we might find that the pattern RB JJ NN
is a very good indicator of subjective utterances
because it matches a variety of scenarios where
people are ascribing qualities to things, e.g. re-
ally bad movie, horribly overcooked steak. Notice
that we do not see our approach and AutoSlog-TS
as mutually exclusive, and indeed we demonstrate
through these experiments that they can be effec-
tively combined.
Our approach begins by running the Brill POS
tagger (Brill, 1992) over all sentences in a doc-
ument. We then extract all of the word trigrams
from the document, and represent each trigram us-
ing every possible instantiation. Because we are
working at the trigram level, and each unit of the
trigram can be a word or its POS tag there are
23 = 8 representations in each trigram’s instantia-
tion set. To continue the example from above, the
instantiation set for the trigram really great idea is
given in Table 1. As we scan down the instanti-
ation set, we can see that the level of abstraction
increases until it is completely uninstantiated. It is
this multilevel abstraction that we are hypothesiz-
ing will be useful for learning new subjective and
polar cues.
All trigrams are then scored according to their
prevalence in relevant versus irrelevant documents
(e.g. subjective vs. non-subjective sentences),
following the scoring methodology of Riloff and
Wiebe (2003). We calculate the conditional prob-
ability p(relevanceltrigram) using the actual tri-
gram counts in relevant and irrelevant text. For
learning negative-subjective patterns, we treat all
negative sentences as the relevant text and the re-
mainder of the sentences as irrelevant text, and
conduct the same process for learning positive-
subjective patterns. We consider significant pat-
terns to be those where the conditional proba-
bility is greater than 0.65 and the pattern occurs
more than five times in the entire document set
(slightly higher than probability &gt;= 0.60 and
frequency &gt;= 2 used by Riloff and Wiebe
(2003)).
We possess a fairly small amount of conversa-
tional data annotated for subjectivity and polarity.
The AMI meeting corpus and BC3 email corpus
are described in more detail in Section 4.1. To ad-
dress this shortfall in annotated data, we take two
approaches to learning patterns. In the first, we
learn a set of patterns from the annotated conversa-
tion data. In the second approach, we complement
those patterns by learning additional patterns from
unannotated data that are typically overwhelm-
ingly subjective or objective in nature. We de-
scribe these two approaches here in turn.
</bodyText>
<subsectionHeader confidence="0.8930975">
3.1.1 Supervised Learning of Patterns from
Conversation Data
</subsectionHeader>
<bodyText confidence="0.999960142857143">
The first learning strategy is to apply the above-
described methods to the annotated conversation
data, learning the positive patterns by compar-
ing positive-subjective utterances to all other ut-
terances, and learning the negative patterns by
comparing the negative-subjective utterances to
all other utterances, using the described methods.
This results in 759 significant positive patterns and
67 significant negative patterns. This difference in
pattern numbers can be explained by negative ut-
terances being less common in the AMI meetings,
as noted by Wilson (2008). It may be that people
are less comfortable in expressing negative sen-
timents in face-to-face conversations, particularly
when the meeting participants do not know each
other well (in the AMI scenario meetings, many
participants were meeting each other for the first
time). But there may be a further explanation for
why we learn many more positive than negative
patterns. When conversation participants do ex-
press negative sentiments, they may couch those
sentiments in more euphemistic or guarded terms
compared with positive sentiments. Table 2 gives
examples of significant positive and negative pat-
terns learned from the labeled meeting data. The
last two rows in Table 2 show how two patterns
in the same instantiation set can have substantially
different probabilities.
</bodyText>
<page confidence="0.908701">
1350
</page>
<table confidence="0.757202375">
POS p(r|t) NEG p(r|t)
you MD change 1.0 VBD not RB 1.0
should VBP DT 1.0 doesn’t RB VB 0.875
very easy to 0.88 a bit JJ 0.66
we could VBP 0.78 think PRP might 0.66
NNS should VBP 0.71 be DT problem 0.71
PRP could do 0.66 doesn’t really VB 0.833
it could VBP 83 doesn’t RB VB 0.875
</table>
<tableCaption confidence="0.991611">
Table 2: Example Pos. and Neg. Patterns (AMI)
</tableCaption>
<table confidence="0.9707275">
Pattern p(r|t)
can not VB 0.99
i can RB 0.99
i have not 0.98
do RB think 0.97
RB think that 0.95
RB agree with 0.95
IN PRP opinion 0.95
</table>
<tableCaption confidence="0.998874">
Table 3: Example Subjective Patterns (BLOG06)
</tableCaption>
<subsectionHeader confidence="0.924818">
3.1.2 Unsupervised Learning of Patterns
from Blog Data
</subsectionHeader>
<bodyText confidence="0.999899815789473">
The second pattern learning strategy we take to
learning subjective patterns is to use a relevant,
but unannotated corpus. We focus on weblog
(blog) data for several reasons. First, blog posts
share many characteristics with both meetings and
emails: they are conversational, informal and the
language can be very ungrammatical. Second,
blog posts are known for being subjective; blog-
gers post on issues that are passionate to them, of-
fering arguments, opinions and invective. Third,
there is a huge amount of available blog data. But
because we do not possess blog data annotated
for subjectivity, we take the following approach
to learning subjective patterns from this data. We
work on the assumption that a great many blog
posts are inherently subjective, and that compar-
ing this data to inherently objective text such as
newswire articles, treating the latter as our irrele-
vant text, should lead to the detection of many new
subjective patterns and greatly increase our cover-
age. While the patterns learned will be noisy, we
hypothesize that the increased coverage will im-
prove our subjectivity detection overall.
For our blog data, we use the BLOG06 Corpus1
that was featured as training and testing data for
the Text Analysis Conference (TAC) 2008 track
on summarizing blog opinions. The portion used
totals approximately 4,000 documents on all man-
ner of topics. Treating that dataset as our rele-
vant, subjective data, we then learn the subjec-
tive trigrams by comparing with the irrelevant
TAC/DUC newswire data from the 2007 and 2008
update summarization tasks. To try to reduce the
amount of noise in our learned patterns, we set the
conditional probability threshold at 0.75 (vs. 0.65
for annotated data), and stipulate that all signif-
icant patterns must occur at least once in the ir-
relevant text. This last rule is meant to prevent
</bodyText>
<footnote confidence="0.951693">
1http://ir.dcs.gla.ac.uk/test collections/blog06info.html
</footnote>
<bodyText confidence="0.993984882352941">
us from learning completely blog-specific patterns
such as posted by NN or linked to DT. In the end,
more than 20,000 patterns were learned from the
blog data. While manual inspection does show
that many undesirable patterns were extracted,
among the highest-scoring patterns are many sen-
sible subjective trigrams such as those indicated in
Table 3.
This approach is similar in spirit to the work of
Biadsy et al. (2008) on unsupervised biography
production. Without access to labeled biographi-
cal data, the authors chose to use sentences from
Wikipedia biographies as their positive set and
sentences from newswire articles as their negative
set, on the assumption that most of the Wikipedia
sentences would be relevant to biographies and
most of the newswire sentences would not.
</bodyText>
<subsectionHeader confidence="0.996788">
3.2 Deriving VIN Features
</subsectionHeader>
<bodyText confidence="0.999987565217391">
For our machine learning experiments, we derive,
for each sentence, features indicating the presence
of the significant VIN patterns. Patterns are binned
according to their conditional probability range
(i.e., 0.65 &lt;= p &lt; 0.75, 0.75 &lt;= p &lt; 0.85,
0.85 &lt;= p &lt; 0.95, and 0.95 &lt;= p). There are
three bins for the blog patterns, since the proba-
bility cutoff is 0.75 For each bin, there is a feature
indicating the count of its patterns in the given sen-
tence. When attempting to match these trigram
patterns to sentences, we allow up to two wild-
card lexical items between the trigram units. In
this way a sentence can match a learned pattern
even if the units of the n-gram are not contiguous
(Raaijmakers et al. (2008) similarly include an n-
gram feature allowing such intervening material).
A key reason for counting the number of
matched patterns for each probability range as just
described, rather than including a feature for each
individual pattern, is to maintain the same level
of dimensionality in our machine learning exper-
iments when comparing the VIN approach to the
baseline approaches described in Section 3.4.
</bodyText>
<page confidence="0.958969">
1351
</page>
<subsectionHeader confidence="0.995187">
3.3 Conversational Features
</subsectionHeader>
<bodyText confidence="0.999996344827586">
While we hypothesize that the general pur-
pose pattern-based approach described above will
greatly aid subjectivity and polarity detection, we
also recognize that there are many additional fea-
tures specific for characterizing multimodal con-
versations that may correlate well with subjectiv-
ity and polarity. Such features include structural
characteristics like the position of a sentence in a
turn and the position of a turn in the conversation,
and participant features relating to dominance or
leadership. For example, it may be that subjective
sentences are more likely to come at the end of a
conversation, or that a person who dominates the
conversation may utter more negative sentences.
We use the feature set provided by Murray and
Carenini (2008), which they used for automatic
summarization of conversations and which are
shown in Table 4. Many of the features are based
on so-called Sprob and Tprob term-weights, the
former of which weights words based on their dis-
tributions across conversation participants and the
latter of which similarly weights words based on
their distributions across conversation turns. Other
features include word entropy of the candidate
sentence, lexical cohesion of the sentence with the
greater conversation, and structural features indi-
cating position of the candidate sentence in the
turn and in the conversation, such as the elapsed
time since the beginning of the conversation.
</bodyText>
<subsectionHeader confidence="0.919091">
3.4 Baseline Approaches
</subsectionHeader>
<bodyText confidence="0.99977647368421">
There are two baselines in particular to which
we are interested in comparing the VIN ap-
proach. As stated earlier, we are hypothesiz-
ing that the increasing levels of abstraction found
with partially instantiated trigrams will lead to im-
proved classification compared with using only
fully instantiated trigrams. To test this, we
also run the subjective/non-subjective and posi-
tive/negative experiments using only fully instan-
tiated trigrams. There are 71 such positive tri-
grams and 5 such negative trigrams learned from
the AMI data. There are just over 1200 fully in-
stantiated trigrams learned from the unannotated
BLOG06 data.
Believing that the current approach may offer
benefits over state-of-the-art pattern-based subjec-
tivity detection, we also implement the AutoSlog-
TS method of Riloff and Wiebe (2003) for extract-
ing subjective extraction patterns. In AutoSlog-
</bodyText>
<table confidence="0.954858214285714">
Feature ID Description
MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
</table>
<tableCaption confidence="0.93586875">
COS1 cosine of conv. splits, w/ Sprob
COS2 cosine of conv. splits, w/ Tprob
PENT entropy of conv. up to sentence
SENT entropy of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore (cohesion)
CENT1 cos. of sentence &amp; conv., w/ Sprob
CENT2 cos. of sentence &amp; conv., w/ Tprob
Table 4: Features Key
</tableCaption>
<bodyText confidence="0.99868416">
TS, once all of the patterns are extracted using
the Sundance parser, the scoring methodology is
much the same as desribed in Section 3.1. Con-
ditional probabilities are calculated by comparing
pattern occurrences in the relevant text with oc-
currences in all text, and we again use a thresh-
old of p &gt;= 0.65 and frequency &gt;= 5 for sig-
nificant patterns. For the BLOG06 data, we use
a probability cutoff of 0.75 as before. For deriv-
ing the features used in our machine learning ex-
periments, the patterns are similarly grouped ac-
cording to conditional probability. From the anno-
tated data, 48 patterns are learned in total, 46 pos-
itive and only 2 negative. From the BLOG06 data,
more than 3000 significant patterns are learned.
Among significant patterns learned from the AMI
corpus are &lt; subj &gt; BE good, change &lt; dobj &gt;,
&lt; subj &gt; agree and problem with &lt; NP &gt;.
To gauge the effectiveness of the various feature
types, for both sets of experiments we build multi-
ple models on a variety of feature combinations:
fully instantiated trigrams (TRIG), varying in-
stantiation n-grams (VIN), AutoSlog-TS (SLOG),
conversational structure features (CONV), and the
set of all features.
</bodyText>
<sectionHeader confidence="0.999203" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.933969">
In this section we describe the corpora used, the
relevant subjectivity annotation, and the statistical
</bodyText>
<page confidence="0.989426">
1352
</page>
<bodyText confidence="0.794944">
classifiers employed.
</bodyText>
<subsectionHeader confidence="0.946904">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.99993752631579">
We use two annotated corpora for these experi-
ments. The AMI corpus (Carletta et al., 2005) con-
sists of meetings in which participants take part in
role-playing exercises concerning the design and
development of a remote control. Participants are
grouped in fours, and each group takes part in a
sequence of four meetings, bringing the remote
control from design to market. The four members
of the group are assigned roles of project man-
ager, industrial designer, user interface designer,
and marketing expert. In total there are 140 such
scenario meetings, with individual meetings rang-
ing from approximately 15 to 45 minutes.
The BC3 corpus (Ulrich et al., 2008) contains
email threads from the World Wide Web Consor-
tium (W3C) mailing list. The threads feature a va-
riety of topics such as web accessibility and plan-
ning face-to-face meetings. The annotated portion
of the mailing list consists of 40 threads.
</bodyText>
<subsectionHeader confidence="0.992192">
4.2 Subjectivity Annotation
</subsectionHeader>
<bodyText confidence="0.99995875">
Wilson (2008) has annotated 20 AMI meetings for
a variety of subjective phenomena which fall into
the broad classes of subjective utterances, objec-
tive polar utterances and subjective questions. It
is this first class in which we are primarily in-
terested here. Two subclasses of subjective utter-
ances are positive subjective and negative subjec-
tive utterances. Such subjective utterances involve
the expression of a private state, such as a posi-
tive/negative opinion, positive/negative argument,
and agreement/disagreement. The 20 meetings
were labeled by a single annotator, though Wilson
(2008) did conduct a study of annotator agreement
on two meetings, reporting a κ of 0.56 for detect-
ing subjective utterances. Of the roughly 20,000
dialogue acts total in the 20 AMI meetings, nearly
4000 are labeled as positive-subjective and nearly
1300 as negative-subjective. For the first exper-
imental task, we consider the subjective class to
be the union of positive-subjective and negative-
subjective dialogue acts. For the second experi-
mental task, the goal is to discriminate positive-
subjective from negative-subjective.
For the BC3 emails, annotators were initially
asked to create extractive and abstractive sum-
maries of each thread, in addition to labeling a
variety of sentence-level phenomena, including
whether each sentence was subjective. In a second
round of annotations, three different annotators
were asked to go through all of the sentences pre-
viously labeled as subjective and indicate whether
each sentence was positive, negative, positive-
negative, or other. The definitions for positive and
negative subjectivity mirrored those given by Wil-
son (2008). For the purpose of these experiments,
we consider a sentence to be subjective if at least
two of the annotators labeled it as subjective, and
similarly consider a subjective sentence to be pos-
itive or negative if at least two annotators label it
as such. Using this majority vote labeling, 172
of 1800 sentences are considered subjective, with
44% of those labeled as positive-subjective and
37% as negative-subjective, showing that there is
much more of a balance between positive and neg-
ative sentiment in these email threads compared
with meeting speech (note that some subjective
sentences are not positive or negative). The κ for
labeling subjective sentences in the email corpus
is 0.32. The lower annotator agreement on emails
compared with meetings suggests that subjectiv-
ity in email text may be manifested more subtly or
conveyed somewhat amibiguously.
</bodyText>
<subsectionHeader confidence="0.991915">
4.3 Classifier and Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999980352941176">
For these experiments we use a maximum entropy
classifier using the liblinear toolkit2 (Fan et al.,
2008). Feature subset selection is carried out by
calculating the F-statistic for each feature, ranking
the features according to the statistic, and train-
ing on increasingly smaller subsets of feature in
a cross-validation procedure, ultimately choosing
the feature set with the highest balanced accuracy
during cross-validation.
Because the annotated portions of our corpora
are fairly small (20 meetings, 40 email threads),
we employ a leave-one-out method for training
and testing rather than using dedicated training
and test sets. For the polarity labeling task ap-
plied to the BC3 corpus, we pool all of the sen-
tences and perform 10-fold cross-validation at the
sentence level.
</bodyText>
<subsectionHeader confidence="0.995948">
4.4 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999366">
We employ two sets of metrics for evaluating all
classifiers: precision/recall/f-measure and the re-
ceiver operator characteristic (ROC) curve. The
ROC curve plots the true-positive/false-positive
ratio while the posterior threshold is varied, and
</bodyText>
<footnote confidence="0.996909">
2http://www.csie.ntu.edu.tw/ cjlin/liblinear/
</footnote>
<page confidence="0.983943">
1353
</page>
<bodyText confidence="0.999903">
we report the area under the curve (AUROC) as the
measure of interest. Random performance would
feature an AUROC of approximately 0.5, while
perfect classification would yield an AUROC of
1. The advantage of the AUROC score compared
with precision/recall/f-measure is that it evaluates
a given classifier across all thresholds, indicating
the classifier’s overall discriminating power. This
metric is also known to be appropriate when class
distributions are skewed (Fawcett, 2003), as is our
case. For completeness we report both AUROC
and p/r/f, but our discussions focus primarily on
the AUROC comparisons.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.99989575">
In this section we describe the experimental re-
sults, first for the subjective/non-subjective clas-
sification task, and subsequently for the positive-
negative classification task.
</bodyText>
<sectionHeader confidence="0.887746" genericHeader="method">
AUROC
</sectionHeader>
<subsectionHeader confidence="0.998193">
5.1 Subjective / Non-Subjective Classification
</subsectionHeader>
<bodyText confidence="0.999918">
For the subjectivity detection task, the results on
the AMI and BC3 data closely mirrored each
other, with the VIN approach constituting a very
effective feature set, outperforming both baselines.
We report the results on meeting and emails in
turn.
</bodyText>
<subsectionHeader confidence="0.629083">
5.1.1 AMI corpus
</subsectionHeader>
<bodyText confidence="0.999958">
For the subjectivity task with the AMI corpus, we
first report the precision, recall and f-measure re-
sults in Table 5 where the various classifiers are
compared with a lower bound (LB) in which the
positive class is always predicted, leading to per-
fect recall. It can be seen that the novel systems
exhibit substantial improvement in precision and
f-measure over this lower-bound. While the VIN
approach yields the best precision scores, the full
feature set achieves the highest f-measure.
As shown in Figure 1, the average AUROC with
the VIN approach is 0.69, compared with 0.61 for
AutoSlog-TS, a significant difference according to
paired t-test (p&lt;0.01). The VIN approach is also
significantly better than the standard fully instan-
tiated trigram pattern approach (p&lt;0.01). This
latter result suggests that the increased level of
abstraction found in the varying instantiation n-
grams does improve performance.
The conversational features alone give compa-
rable performance to the VIN method (no signifi-
cant difference), and the best results are found us-
ing the full feature set, which gives an average AU-
</bodyText>
<table confidence="0.999901866666667">
Sys Precision Recall F-Measure
AMI Corpus
LB 26 100 41
Trig 25 63 36
Slog 39 48 43
VIN 41 58 48
Conv 36 73 49
All Feas 38 70 49
BC3 Corpus
LB 10 100 17
Trig 27 10 14
Slog 24 13 17
VIN 27 22 24
Conv 25 29 27
All Feas 33 34 33
</table>
<tableCaption confidence="0.998614">
Table 5: P/R/F Results, Subjectivity Task
</tableCaption>
<figure confidence="0.989573666666667">
1
0.9
0.8
0.7
0.6
0.5
</figure>
<figureCaption confidence="0.995003">
Figure 1: AUROCs on Subjectivity Task for AMI
and BC3 corpora
</figureCaption>
<bodyText confidence="0.9811435">
ROC of 0.71, a significant improvement over VIN
only (p&lt;0.05).
</bodyText>
<subsubsectionHeader confidence="0.78835">
5.1.2 BC3 corpus
</subsubsectionHeader>
<bodyText confidence="0.999991642857143">
For the subjectivity task with the BC3 corpus, the
best precision and f-measure scores are found by
combining all features, as displayed in Table 5.
The f-measure for the VIN approach is ten points
higher than for the standard trigram approach.
The average AUROC with the VIN approach is
0.77, compared with 0.70 for AutoSlog-TS (sig-
nificant at p&lt;0.05). The varying instantiation ap-
proach is significantly better than the standard tri-
gram pattern approach (p&lt;0.01), where the aver-
age AUROC is 0.66. We again find that conver-
sational features are very useful for this task, and
that the best overall results utilize the entire fea-
ture set. These results are displayed in Figure 1.
</bodyText>
<subsectionHeader confidence="0.927306">
5.1.3 Impact of Blog Data
</subsectionHeader>
<bodyText confidence="0.956052">
An interesting question is whether our use of the
BLOG06 data was worthwhile. We can measure
this by comparing the VIN AUROC results re-
</bodyText>
<page confidence="0.968231">
1354
</page>
<table confidence="0.9999488">
Sys Precision Recall F-Measure
AMI Corpus
LB 76 100 86
Trig 87 8 14
Slog 75 46 57
VIN 83 60 70
Conv 82 47 60
All Feas 83 56 67
BC3 Corpus
LB 54 100 70
Trig 50 84 63
Slog 58 56 57
VIN 53 84 65
Conv 63 80 71
All Feas 60 76 67
</table>
<tableCaption confidence="0.998496">
Table 6: P/R/F Results, Polarity Task
</tableCaption>
<bodyText confidence="0.999777363636364">
ported above with the VIN AUROC scores using
only the annotated data for learning the significant
patterns. The finding is that the blog data was
very helpful, as the VIN approach averages only
0.66 on the BC3 data and 0.63 on the AMI data
when the blog patterns are not used, both signif-
icantly lower (p&lt;0.01). Figure 2 shows the ROC
curves for the VIN approach with and without blog
patterns applied to the AMI subjectivity detection
task, illustrating the impact of the unsupervised
pattern-learning strategy.
</bodyText>
<figure confidence="0.917752">
0 0.2 0.4 0.6 0.8 1
FP
</figure>
<figureCaption confidence="0.9544715">
Figure 2: Effect of Blog Patterns on AMI Subjec-
tivity Task
</figureCaption>
<subsectionHeader confidence="0.991555">
5.2 Positive / Negative Classification
</subsectionHeader>
<bodyText confidence="0.999916666666667">
For the polarity classification task, the results dif-
fer between the two corpora. We describe the re-
sults on meetings and emails in turn.
</bodyText>
<subsectionHeader confidence="0.730199">
5.2.1 AMI corpus
</subsectionHeader>
<bodyText confidence="0.999012705882353">
The p/r/f results for the AMI polarity task are pre-
sented in Table 6, with the scores pertaining to
the positive-subjective class. The VIN classifier
and full features classifier achieve the highest pre-
cision, but the f-measures are below the lower-
bound.
Comparing AUROC results, the VIN approach
is again significantly better than AutoSlog-TS,
averaging 0.65 compared with 0.56, and signifi-
cantly better than the standard trigram approach
(p&lt;0.01 in both cases). The results are dis-
played in Figure 3. The conversational features are
significantly less effective than the VIN features
(p&lt;0.05), and the best overall results are found by
utilizing all features, with significant improvement
over VIN only at p&lt;0.05 and significant improve-
ment over AutoSlog-TS only at p&lt;0.01.
</bodyText>
<subsubsectionHeader confidence="0.544532">
5.2.2 BC3 corpus
</subsubsectionHeader>
<bodyText confidence="0.9999915">
The results of the polarity task on the BC3 cor-
pus are markedly different from the other exper-
imental results. In this case, neither VIN nor
AutoSlog-TS are particularly good for discrimi-
nating between positive and negative sentences,
and the best strategy is to use features relating to
conversational structure. According to p/r/f (Ta-
ble 6), the only method outperforming the lower-
bound in terms of f-measure is the conversational
features classifier. According to AUROC scores
shown in Figure 3, the conversational features by
themselves are significantly better than the VIN
approach (p&lt;0.01 for non-paired t-test). So for
emails, we are more likely to correctly classify
positive and negative sentence by looking at fea-
tures such as position in the turn and participant
dominance than by matching our learned patterns.
While we showed previously that pattern-based
approaches perform well for the subjectivity task
on this dataset, there was less success in using the
patterns to discern the polarity of email sentences.
We are again interested in whether the use of the
BLOG06 data was beneficial. For the BC3 data,
there is very little difference between the VIN ap-
proach with and without the blog patterns, as they
both perform poorly, but with the AMI corpus, the
blog patterns yield significant improvement in po-
larity classification, increasing from an average of
0.56 without the blog patterns to 0.65 with them
(p&lt;0.01).
</bodyText>
<sectionHeader confidence="0.999536" genericHeader="evaluation">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.9996676">
A key difference between the AMI and BC3 data
with regards to subjectivity is that negative ut-
terances are much more common in the BC3
email threads. Additionally, the pattern-based ap-
proaches fared worst in discriminating between
</bodyText>
<figure confidence="0.994270555555555">
1p
0.8
0.6
0.4
0.2
0
1
VIN with Blog Patterns
VIN without Blog Patterns
chance level
1355
AUROC 1
0.9
0.8
0.7
0.6
0.5
0.4
</figure>
<figureCaption confidence="0.9343335">
Figure 3: AUROCs on Polarity Task for AMI and
BC3 corpora
</figureCaption>
<bodyText confidence="0.99998111627907">
negative and positive utterances in that corpus.
Positive and negative email sentences are more
easily recognized via features relating to conver-
sation structure and participant status than through
the learned lexical patterns.
The use of patterns learned from unlabeled blog
data significantly improved performance. We are
currently developing further techniques for learn-
ing subjective and polar patterns from such raw,
natural text.
A potential area of improvement is to reduce the
feature set by eliminating some of the subjective
patterns. In Section 2, we briefly described the
work of Riloff et al. (2006) on feature subsump-
tion relationships. In our case, in a VIN instantia-
tion set a given trigram instantiation subsumes the
less abstract instantiations in the set, so the most
abstract instantiation (i.e. completely uninstanti-
ated trigram) representationally subsumes the rest.
Eliminating some of the representationally sub-
sumed instantiations when they are also behav-
iorally subsumed may improve our results.
It is difficult to compare our results directly with
those of Raaijmakers et al. (2008) as they used a
smaller set of AMI meetings for their experiments,
and because for the first experiment we consider
the subjective class to be the union of positive-
subjective and negative-subjective dialogue acts
whereas they additionally include subjective ques-
tions and dialogue acts expressing uncertainty.
These differences are reflected by the substantially
differing scores reported for majority-vote base-
lines on each task. However, their success with
character n-gram features suggests that we could
improve our system by incorporating a variety of
character features. Character n-grams were the
best single feature class in their experiments.
The VIN representation is a general one and
may hold promise for learning patterns relevant to
other interesting conversation phenomena such as
decision-making and action items. We plan to ap-
ply the methods described here to these other ap-
plications in the near future.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980857142857">
In this work we have shown that learning subjec-
tive trigrams with varying instantiation levels from
both annotated and raw data can improve subjec-
tivity detection and polarity labeling for meeting
speech and email threads. The novel pattern-based
approach was significantly better than standard tri-
grams for three of the four tasks, and was signif-
icantly better than a state-of-the-art syntactic ap-
proach for those same tasks. We also found that
features relating to conversational structure were
beneficial for all tasks, and particularly for polar-
ity labeling in email data. Interestingly, in three
out of four cases combining all the features pro-
duced the best performance.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999846555555556">
F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An un-
supervised approach to biography production using
wikipedia. In Proc. of ACL-HLT 2008, Columbus,
OH, USA.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. ofDARPA Speech and Natural Lan-
guage Workshop, San Mateo, CA, USA, pages 112–
116.
J. Carletta, S. Ashby, S. Bourban, M. Flynn,
M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos,
W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, I. McCowan, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. ofMLMI 2005, Edinburgh,
UK, pages 28–39.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large lin-
ear classification. Journal ofMachine Learning Re-
search, 9:1871–1874.
T. Fawcett. 2003. Roc graphs: Notes and practical
considerations for researchers.
G. Murray and G. Carenini. 2008. Summarizing spo-
ken and written conversations. In Proc. of EMNLP
2008, Honolulu, HI, USA.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 1-2(2):1–135.
</reference>
<page confidence="0.824877">
1356
</page>
<reference confidence="0.999642">
S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conversa-
tion. In Proc. ofEMNLP 2008, Honolulu, HI, USA.
E. Riloff and W. Phillips. 2004. An introduction to the
sundance and autoslog systems.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proc. ofEMNLP
2003, Sapporo, Japan.
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Fea-
ture subsumption for opinion analysis. In Proc. of
EMNLP 2006, Sydney, Australia.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In
Proc. of SIGDIAL 2007, Antwerp, Belgium.
J. Ulrich, G. Murray, and G. Carenini. 2008. A
publicly available annotated corpus for supervised
email summarization. In Proc. of AAAI EMAIL-
2008 Workshop, Chicago, USA.
T. Wilson, J. Wiebe, and R. Hwa. 2006. Recognizing
strong and weak opinion clauses. ComputationalIn-
telligence, 22(2):73–99.
T. Wilson. 2008. Annotating subjective content in
meetings. In Proc. of LREC 2008, Marrakech, Mo-
rocco.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. ofEMNLP 2003, Sapporo, Japan.
</reference>
<page confidence="0.993773">
1357
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.543860">
<title confidence="0.999376">Predicting Subjectivity in Multimodal Conversations</title>
<author confidence="0.795388">Murray</author>
<affiliation confidence="0.999871">University of British</affiliation>
<address confidence="0.802589">Vancouver,</address>
<email confidence="0.988722">(gabrielm,carenini)@cs.ubc.ca</email>
<abstract confidence="0.990136625">In this research we aim to detect subjective sentences in multimodal conversations. We introduce a novel technique wherein subjective patterns are learned from both labeled and unlabeled data, using n-gram word sequences with varying levels of lexical instantiation. Applying this technique to meeting speech and email conversations, we gain significant improvement over state-of-the-art approaches. Furthermore, we show that coupling the pattern-based approach with features that capture characteristics of general conversation structure yields additional improvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Biadsy</author>
<author>J Hirschberg</author>
<author>E Filatova</author>
</authors>
<title>An unsupervised approach to biography production using wikipedia.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT 2008,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="15765" citStr="Biadsy et al. (2008)" startWordPosition="2485" endWordPosition="2488">otated data), and stipulate that all significant patterns must occur at least once in the irrelevant text. This last rule is meant to prevent 1http://ir.dcs.gla.ac.uk/test collections/blog06info.html us from learning completely blog-specific patterns such as posted by NN or linked to DT. In the end, more than 20,000 patterns were learned from the blog data. While manual inspection does show that many undesirable patterns were extracted, among the highest-scoring patterns are many sensible subjective trigrams such as those indicated in Table 3. This approach is similar in spirit to the work of Biadsy et al. (2008) on unsupervised biography production. Without access to labeled biographical data, the authors chose to use sentences from Wikipedia biographies as their positive set and sentences from newswire articles as their negative set, on the assumption that most of the Wikipedia sentences would be relevant to biographies and most of the newswire sentences would not. 3.2 Deriving VIN Features For our machine learning experiments, we derive, for each sentence, features indicating the presence of the significant VIN patterns. Patterns are binned according to their conditional probability range (i.e., 0.</context>
</contexts>
<marker>Biadsy, Hirschberg, Filatova, 2008</marker>
<rawString>F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An unsupervised approach to biography production using wikipedia. In Proc. of ACL-HLT 2008, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proc. ofDARPA Speech and Natural Language Workshop,</booktitle>
<pages>112--116</pages>
<location>San Mateo, CA, USA,</location>
<contexts>
<context position="9374" citStr="Brill, 1992" startWordPosition="1445" endWordPosition="1446">h syntactic parsers may perform poorly. Also, our learned trigram patterns range from fully instantiated to completely uninstantiated. For example, we might find that the pattern RB JJ NN is a very good indicator of subjective utterances because it matches a variety of scenarios where people are ascribing qualities to things, e.g. really bad movie, horribly overcooked steak. Notice that we do not see our approach and AutoSlog-TS as mutually exclusive, and indeed we demonstrate through these experiments that they can be effectively combined. Our approach begins by running the Brill POS tagger (Brill, 1992) over all sentences in a document. We then extract all of the word trigrams from the document, and represent each trigram using every possible instantiation. Because we are working at the trigram level, and each unit of the trigram can be a word or its POS tag there are 23 = 8 representations in each trigram’s instantiation set. To continue the example from above, the instantiation set for the trigram really great idea is given in Table 1. As we scan down the instantiation set, we can see that the level of abstraction increases until it is completely uninstantiated. It is this multilevel abstr</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part of speech tagger. In Proc. ofDARPA Speech and Natural Language Workshop, San Mateo, CA, USA, pages 112– 116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI meeting corpus: A preannouncement.</title>
<date>2005</date>
<booktitle>In Proc. ofMLMI 2005,</booktitle>
<pages>28--39</pages>
<location>Edinburgh, UK,</location>
<contexts>
<context position="21868" citStr="Carletta et al., 2005" startWordPosition="3476" endWordPosition="3479">ange &lt; dobj &gt;, &lt; subj &gt; agree and problem with &lt; NP &gt;. To gauge the effectiveness of the various feature types, for both sets of experiments we build multiple models on a variety of feature combinations: fully instantiated trigrams (TRIG), varying instantiation n-grams (VIN), AutoSlog-TS (SLOG), conversational structure features (CONV), and the set of all features. 4 Experimental Setup In this section we describe the corpora used, the relevant subjectivity annotation, and the statistical 1352 classifiers employed. 4.1 Corpora We use two annotated corpora for these experiments. The AMI corpus (Carletta et al., 2005) consists of meetings in which participants take part in role-playing exercises concerning the design and development of a remote control. Participants are grouped in fours, and each group takes part in a sequence of four meetings, bringing the remote control from design to market. The four members of the group are assigned roles of project manager, industrial designer, user interface designer, and marketing expert. In total there are 140 such scenario meetings, with individual meetings ranging from approximately 15 to 45 minutes. The BC3 corpus (Ulrich et al., 2008) contains email threads fro</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI meeting corpus: A preannouncement. In Proc. ofMLMI 2005, Edinburgh, UK, pages 28–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="25388" citStr="Fan et al., 2008" startWordPosition="4022" endWordPosition="4025">nd 37% as negative-subjective, showing that there is much more of a balance between positive and negative sentiment in these email threads compared with meeting speech (note that some subjective sentences are not positive or negative). The κ for labeling subjective sentences in the email corpus is 0.32. The lower annotator agreement on emails compared with meetings suggests that subjectivity in email text may be manifested more subtly or conveyed somewhat amibiguously. 4.3 Classifier and Experimental Setup For these experiments we use a maximum entropy classifier using the liblinear toolkit2 (Fan et al., 2008). Feature subset selection is carried out by calculating the F-statistic for each feature, ranking the features according to the statistic, and training on increasingly smaller subsets of feature in a cross-validation procedure, ultimately choosing the feature set with the highest balanced accuracy during cross-validation. Because the annotated portions of our corpora are fairly small (20 meetings, 40 email threads), we employ a leave-one-out method for training and testing rather than using dedicated training and test sets. For the polarity labeling task applied to the BC3 corpus, we pool all</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. Liblinear: A library for large linear classification. Journal ofMachine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fawcett</author>
</authors>
<title>Roc graphs: Notes and practical considerations for researchers.</title>
<date>2003</date>
<contexts>
<context position="26868" citStr="Fawcett, 2003" startWordPosition="4237" endWordPosition="4238">e true-positive/false-positive ratio while the posterior threshold is varied, and 2http://www.csie.ntu.edu.tw/ cjlin/liblinear/ 1353 we report the area under the curve (AUROC) as the measure of interest. Random performance would feature an AUROC of approximately 0.5, while perfect classification would yield an AUROC of 1. The advantage of the AUROC score compared with precision/recall/f-measure is that it evaluates a given classifier across all thresholds, indicating the classifier’s overall discriminating power. This metric is also known to be appropriate when class distributions are skewed (Fawcett, 2003), as is our case. For completeness we report both AUROC and p/r/f, but our discussions focus primarily on the AUROC comparisons. 5 Results In this section we describe the experimental results, first for the subjective/non-subjective classification task, and subsequently for the positivenegative classification task. AUROC 5.1 Subjective / Non-Subjective Classification For the subjectivity detection task, the results on the AMI and BC3 data closely mirrored each other, with the VIN approach constituting a very effective feature set, outperforming both baselines. We report the results on meeting </context>
</contexts>
<marker>Fawcett, 2003</marker>
<rawString>T. Fawcett. 2003. Roc graphs: Notes and practical considerations for researchers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>G Carenini</author>
</authors>
<title>Summarizing spoken and written conversations.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP 2008,</booktitle>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="18059" citStr="Murray and Carenini (2008)" startWordPosition="2856" endWordPosition="2859">detection, we also recognize that there are many additional features specific for characterizing multimodal conversations that may correlate well with subjectivity and polarity. Such features include structural characteristics like the position of a sentence in a turn and the position of a turn in the conversation, and participant features relating to dominance or leadership. For example, it may be that subjective sentences are more likely to come at the end of a conversation, or that a person who dominates the conversation may utter more negative sentences. We use the feature set provided by Murray and Carenini (2008), which they used for automatic summarization of conversations and which are shown in Table 4. Many of the features are based on so-called Sprob and Tprob term-weights, the former of which weights words based on their distributions across conversation participants and the latter of which similarly weights words based on their distributions across conversation turns. Other features include word entropy of the candidate sentence, lexical cohesion of the sentence with the greater conversation, and structural features indicating position of the candidate sentence in the turn and in the conversatio</context>
</contexts>
<marker>Murray, Carenini, 2008</marker>
<rawString>G. Murray and G. Carenini. 2008. Summarizing spoken and written conversations. In Proc. of EMNLP 2008, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>1--2</pages>
<contexts>
<context position="6699" citStr="Pang and Lee (2008)" startWordPosition="1013" endWordPosition="1016">res in which words can be backed off to POS tags. They found that this feature class improved the overall accuracy of their system. Somasundaran et al. (2007) investigated subjectivity classification in meetings. Their findings indicate that both lexical features (list of words and expressions) and discourse features (dialogue acts and adjacency pairs) can be beneficial. In the same spirit, we effectively combine lexical patterns and conversational features. The approach to predicting subjectivity we present in this paper is a novel contribution to the field of opinion and sentiment analysis. Pang and Lee (2008) give an overview of the state of the art, discussing motivation, features, approaches and available resources. 3 Subjectivity Detection In this section we describe our approach to subjectivity detection. We begin by describing how to learn subjective n-gram patterns with varying levels of lexical instantiation. We then describe a set of features characterizing multimodal conversation structure which can be used to supplement the n-gram approach. Finally, we describe the baseline subjectivity detection approaches used for comparison. 3.1 Partially Instantiated N-Grams Our approach to subjectiv</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 1-2(2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raaijmakers</author>
<author>K Truong</author>
<author>T Wilson</author>
</authors>
<title>Multimodal subjectivity analysis of multiparty conversation.</title>
<date>2008</date>
<booktitle>In Proc. ofEMNLP 2008,</booktitle>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="2992" citStr="Raaijmakers et al. (2008)" startWordPosition="439" endWordPosition="442">sation features capture structural characteristics of multimodal conversations as well as participant information. We test our approach in two sets of experiments. The goal of the first set of experiments is to discriminate subjective from non-subjective utterances, comparing the novel approach to existing state-of-the-art techniques. In the second set of experiments, the goal is to discriminate positivesubjective and negative-subjective utterances, establishing their polarity. In both sets of experiments, we assess the impact of features relating to conversation structure. 2 Related Research Raaijmakers et al. (2008) have approached the problem of detecting subjectivity in meeting speech by using a variety of multimodal features such as prosodic features, word n-grams, character n-grams and phoneme n-grams. For subjectivity detection, they found that a combination of all features was best, while prosodic features were less useful for discriminating between positive and negative utterances. They found character n-grams to be particularly useful. Riloff and Wiebe (2004) presented a method for learning subjective extraction patterns from a large amount of data, which takes subjective and nonsubjective text a</context>
<context position="7675" citStr="Raaijmakers et al. (2008)" startWordPosition="1161" endWordPosition="1164">aracterizing multimodal conversation structure which can be used to supplement the n-gram approach. Finally, we describe the baseline subjectivity detection approaches used for comparison. 3.1 Partially Instantiated N-Grams Our approach to subjectivity detection and polarity detection is to learn significant patterns that correlate with the subjective and polar utterances. These patterns are word trigrams, but with varying levels of lexical instantiation, so that each unit of the n-gram can be either a word or the word’s partof-speech (POS) tag. This contrasts, then, with work such as that of Raaijmakers et al. (2008) who include trigram features in their experiments, but where their learned trigrams are fully instantiated. As an example, while they may learn that a trigram really great idea is positive, we may additionally find that really great NN and RB great NN are informative patterns, and these patterns may sometimes be better cues than the fully instantiated trigrams. To differentiate this approach from the typical use of trigrams, we will refer to it as the VIN (varying instantiation n-grams) method. In some respects, our approach to subjectivity detection is similar to Riloff and Wiebe’s work cite</context>
<context position="16866" citStr="Raaijmakers et al. (2008)" startWordPosition="2671" endWordPosition="2674">the presence of the significant VIN patterns. Patterns are binned according to their conditional probability range (i.e., 0.65 &lt;= p &lt; 0.75, 0.75 &lt;= p &lt; 0.85, 0.85 &lt;= p &lt; 0.95, and 0.95 &lt;= p). There are three bins for the blog patterns, since the probability cutoff is 0.75 For each bin, there is a feature indicating the count of its patterns in the given sentence. When attempting to match these trigram patterns to sentences, we allow up to two wildcard lexical items between the trigram units. In this way a sentence can match a learned pattern even if the units of the n-gram are not contiguous (Raaijmakers et al. (2008) similarly include an ngram feature allowing such intervening material). A key reason for counting the number of matched patterns for each probability range as just described, rather than including a feature for each individual pattern, is to maintain the same level of dimensionality in our machine learning experiments when comparing the VIN approach to the baseline approaches described in Section 3.4. 1351 3.3 Conversational Features While we hypothesize that the general purpose pattern-based approach described above will greatly aid subjectivity and polarity detection, we also recognize that</context>
<context position="34716" citStr="Raaijmakers et al. (2008)" startWordPosition="5545" endWordPosition="5548"> feature set by eliminating some of the subjective patterns. In Section 2, we briefly described the work of Riloff et al. (2006) on feature subsumption relationships. In our case, in a VIN instantiation set a given trigram instantiation subsumes the less abstract instantiations in the set, so the most abstract instantiation (i.e. completely uninstantiated trigram) representationally subsumes the rest. Eliminating some of the representationally subsumed instantiations when they are also behaviorally subsumed may improve our results. It is difficult to compare our results directly with those of Raaijmakers et al. (2008) as they used a smaller set of AMI meetings for their experiments, and because for the first experiment we consider the subjective class to be the union of positivesubjective and negative-subjective dialogue acts whereas they additionally include subjective questions and dialogue acts expressing uncertainty. These differences are reflected by the substantially differing scores reported for majority-vote baselines on each task. However, their success with character n-gram features suggests that we could improve our system by incorporating a variety of character features. Character n-grams were </context>
</contexts>
<marker>Raaijmakers, Truong, Wilson, 2008</marker>
<rawString>S. Raaijmakers, K. Truong, and T. Wilson. 2008. Multimodal subjectivity analysis of multiparty conversation. In Proc. ofEMNLP 2008, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>W Phillips</author>
</authors>
<title>An introduction to the sundance and autoslog systems.</title>
<date>2004</date>
<contexts>
<context position="3935" citStr="Riloff and Phillips, 2004" startWordPosition="577" endWordPosition="580">ul for discriminating between positive and negative utterances. They found character n-grams to be particularly useful. Riloff and Wiebe (2004) presented a method for learning subjective extraction patterns from a large amount of data, which takes subjective and nonsubjective text as input, and outputs significant lexico-syntactic patterns. These patterns are based on syntactic structure output by the Sundance shal1348 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1348–1357, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP low dependency parser (Riloff and Phillips, 2004). They are extracted by exhaustively applying syntactic templates such as &lt; subj &gt; passive-verb and active-verb &lt; dobj &gt; to a training corpus, with an extracted pattern for every instantiation of the syntactic template. These patterns are scored according to probability of relevance given the pattern and frequency of the pattern. Because these patterns are based on syntactic structure, they can represent subjective expressions that are not fixed word sequences and would therefore be missed by a simple n-gram approach. Riloff et al. (2006) explore feature subsumption for opinion detection, wher</context>
<context position="8486" citStr="Riloff and Phillips, 2004" startWordPosition="1292" endWordPosition="1295">tive, we may additionally find that really great NN and RB great NN are informative patterns, and these patterns may sometimes be better cues than the fully instantiated trigrams. To differentiate this approach from the typical use of trigrams, we will refer to it as the VIN (varying instantiation n-grams) method. In some respects, our approach to subjectivity detection is similar to Riloff and Wiebe’s work cited above, in the sense that their extraction patterns are partly instantiated. However, the AutoSlog-TS approach relies on deriving syntactic structure with the Sundance shallow parser (Riloff and Phillips, 2004). We hypothesize that our trigram approach may be more robust to disfluent and fragmented meeting speech and emails 1349 1 2 3 really great idea really great NN really JJ idea RB great idea really JJ NN RB great NN RB JJ idea RB JJ NN Table 1: Sample Instantiation Set on which syntactic parsers may perform poorly. Also, our learned trigram patterns range from fully instantiated to completely uninstantiated. For example, we might find that the pattern RB JJ NN is a very good indicator of subjective utterances because it matches a variety of scenarios where people are ascribing qualities to thin</context>
</contexts>
<marker>Riloff, Phillips, 2004</marker>
<rawString>E. Riloff and W. Phillips. 2004. An introduction to the sundance and autoslog systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proc. ofEMNLP 2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="10273" citStr="Riloff and Wiebe (2003)" startWordPosition="1594" endWordPosition="1597">8 representations in each trigram’s instantiation set. To continue the example from above, the instantiation set for the trigram really great idea is given in Table 1. As we scan down the instantiation set, we can see that the level of abstraction increases until it is completely uninstantiated. It is this multilevel abstraction that we are hypothesizing will be useful for learning new subjective and polar cues. All trigrams are then scored according to their prevalence in relevant versus irrelevant documents (e.g. subjective vs. non-subjective sentences), following the scoring methodology of Riloff and Wiebe (2003). We calculate the conditional probability p(relevanceltrigram) using the actual trigram counts in relevant and irrelevant text. For learning negative-subjective patterns, we treat all negative sentences as the relevant text and the remainder of the sentences as irrelevant text, and conduct the same process for learning positivesubjective patterns. We consider significant patterns to be those where the conditional probability is greater than 0.65 and the pattern occurs more than five times in the entire document set (slightly higher than probability &gt;= 0.60 and frequency &gt;= 2 used by Riloff an</context>
<context position="19562" citStr="Riloff and Wiebe (2003)" startWordPosition="3085" endWordPosition="3088">h partially instantiated trigrams will lead to improved classification compared with using only fully instantiated trigrams. To test this, we also run the subjective/non-subjective and positive/negative experiments using only fully instantiated trigrams. There are 71 such positive trigrams and 5 such negative trigrams learned from the AMI data. There are just over 1200 fully instantiated trigrams learned from the unannotated BLOG06 data. Believing that the current approach may offer benefits over state-of-the-art pattern-based subjectivity detection, we also implement the AutoSlogTS method of Riloff and Wiebe (2003) for extracting subjective extraction patterns. In AutoSlogFeature ID Description MXS max Sprob score MNS mean Sprob score SMS sum of Sprob scores MXT max Tprob score MNT mean Tprob score SMT sum of Tprob scores TLOC position in turn CLOC position in conv. SLEN word count, globally normalized SLEN2 word count, locally normalized TPOS1 time from beg. of conv. to turn TPOS2 time from turn to end of conv. DOM participant dominance in words COS1 cosine of conv. splits, w/ Sprob COS2 cosine of conv. splits, w/ Tprob PENT entropy of conv. up to sentence SENT entropy of conv. after the sentence THISE</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>E. Riloff and J. Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proc. ofEMNLP 2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>S Patwardhan</author>
<author>J Wiebe</author>
</authors>
<title>Feature subsumption for opinion analysis.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP 2006,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4479" citStr="Riloff et al. (2006)" startWordPosition="666" endWordPosition="669">2009. c�2009 ACL and AFNLP low dependency parser (Riloff and Phillips, 2004). They are extracted by exhaustively applying syntactic templates such as &lt; subj &gt; passive-verb and active-verb &lt; dobj &gt; to a training corpus, with an extracted pattern for every instantiation of the syntactic template. These patterns are scored according to probability of relevance given the pattern and frequency of the pattern. Because these patterns are based on syntactic structure, they can represent subjective expressions that are not fixed word sequences and would therefore be missed by a simple n-gram approach. Riloff et al. (2006) explore feature subsumption for opinion detection, where a given feature may subsume another feature representationally if the strings matched by the first feature include all of the strings matched by the second feature. To give their own example, the unigram happy subsumes the bigram very happy. The first feature will behaviorally subsume the second if it representationally subsumes the second and has roughly the same information gain, within an acceptable margin. They show that they can improve opinion analysis results by modeling these relations and reducing the feature set. Our approach </context>
<context position="34219" citStr="Riloff et al. (2006)" startWordPosition="5471" endWordPosition="5474">corpora negative and positive utterances in that corpus. Positive and negative email sentences are more easily recognized via features relating to conversation structure and participant status than through the learned lexical patterns. The use of patterns learned from unlabeled blog data significantly improved performance. We are currently developing further techniques for learning subjective and polar patterns from such raw, natural text. A potential area of improvement is to reduce the feature set by eliminating some of the subjective patterns. In Section 2, we briefly described the work of Riloff et al. (2006) on feature subsumption relationships. In our case, in a VIN instantiation set a given trigram instantiation subsumes the less abstract instantiations in the set, so the most abstract instantiation (i.e. completely uninstantiated trigram) representationally subsumes the rest. Eliminating some of the representationally subsumed instantiations when they are also behaviorally subsumed may improve our results. It is difficult to compare our results directly with those of Raaijmakers et al. (2008) as they used a smaller set of AMI meetings for their experiments, and because for the first experiment</context>
</contexts>
<marker>Riloff, Patwardhan, Wiebe, 2006</marker>
<rawString>E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature subsumption for opinion analysis. In Proc. of EMNLP 2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>J Ruppenhofer</author>
<author>J Wiebe</author>
</authors>
<title>Detecting arguing and sentiment in meetings.</title>
<date>2007</date>
<booktitle>In Proc. of SIGDIAL</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="6238" citStr="Somasundaran et al. (2007)" startWordPosition="945" endWordPosition="948">cation at the document level. Of particular relevance here is that they found that partof-speech (POS) features were especially useful for assigning polarity scores, with adjectives, adverbs and verbs comprising the best set of POS tags. This work inspired us to look at generalization of n-grams based on POS. On the slightly different task of classifying the intensity of opinions, Wilson et al. (2006) employed several types of features including dependency structures in which words can be backed off to POS tags. They found that this feature class improved the overall accuracy of their system. Somasundaran et al. (2007) investigated subjectivity classification in meetings. Their findings indicate that both lexical features (list of words and expressions) and discourse features (dialogue acts and adjacency pairs) can be beneficial. In the same spirit, we effectively combine lexical patterns and conversational features. The approach to predicting subjectivity we present in this paper is a novel contribution to the field of opinion and sentiment analysis. Pang and Lee (2008) give an overview of the state of the art, discussing motivation, features, approaches and available resources. 3 Subjectivity Detection In</context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007. Detecting arguing and sentiment in meetings. In Proc. of SIGDIAL 2007, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ulrich</author>
<author>G Murray</author>
<author>G Carenini</author>
</authors>
<title>A publicly available annotated corpus for supervised email summarization.</title>
<date>2008</date>
<booktitle>In Proc. of AAAI EMAIL2008 Workshop,</booktitle>
<location>Chicago, USA.</location>
<contexts>
<context position="22441" citStr="Ulrich et al., 2008" startWordPosition="3568" endWordPosition="3571">riments. The AMI corpus (Carletta et al., 2005) consists of meetings in which participants take part in role-playing exercises concerning the design and development of a remote control. Participants are grouped in fours, and each group takes part in a sequence of four meetings, bringing the remote control from design to market. The four members of the group are assigned roles of project manager, industrial designer, user interface designer, and marketing expert. In total there are 140 such scenario meetings, with individual meetings ranging from approximately 15 to 45 minutes. The BC3 corpus (Ulrich et al., 2008) contains email threads from the World Wide Web Consortium (W3C) mailing list. The threads feature a variety of topics such as web accessibility and planning face-to-face meetings. The annotated portion of the mailing list consists of 40 threads. 4.2 Subjectivity Annotation Wilson (2008) has annotated 20 AMI meetings for a variety of subjective phenomena which fall into the broad classes of subjective utterances, objective polar utterances and subjective questions. It is this first class in which we are primarily interested here. Two subclasses of subjective utterances are positive subjective </context>
</contexts>
<marker>Ulrich, Murray, Carenini, 2008</marker>
<rawString>J. Ulrich, G. Murray, and G. Carenini. 2008. A publicly available annotated corpus for supervised email summarization. In Proc. of AAAI EMAIL2008 Workshop, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Recognizing strong and weak opinion clauses.</title>
<date>2006</date>
<journal>ComputationalIntelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="6016" citStr="Wilson et al. (2006)" startWordPosition="907" endWordPosition="910">jective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. They found that the latter two tasks were substantially more difficult than classification at the document level. Of particular relevance here is that they found that partof-speech (POS) features were especially useful for assigning polarity scores, with adjectives, adverbs and verbs comprising the best set of POS tags. This work inspired us to look at generalization of n-grams based on POS. On the slightly different task of classifying the intensity of opinions, Wilson et al. (2006) employed several types of features including dependency structures in which words can be backed off to POS tags. They found that this feature class improved the overall accuracy of their system. Somasundaran et al. (2007) investigated subjectivity classification in meetings. Their findings indicate that both lexical features (list of words and expressions) and discourse features (dialogue acts and adjacency pairs) can be beneficial. In the same spirit, we effectively combine lexical patterns and conversational features. The approach to predicting subjectivity we present in this paper is a nov</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2006</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2006. Recognizing strong and weak opinion clauses. ComputationalIntelligence, 22(2):73–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
</authors>
<title>Annotating subjective content in meetings.</title>
<date>2008</date>
<booktitle>In Proc. of LREC</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="12096" citStr="Wilson (2008)" startWordPosition="1880" endWordPosition="1881">d Learning of Patterns from Conversation Data The first learning strategy is to apply the abovedescribed methods to the annotated conversation data, learning the positive patterns by comparing positive-subjective utterances to all other utterances, and learning the negative patterns by comparing the negative-subjective utterances to all other utterances, using the described methods. This results in 759 significant positive patterns and 67 significant negative patterns. This difference in pattern numbers can be explained by negative utterances being less common in the AMI meetings, as noted by Wilson (2008). It may be that people are less comfortable in expressing negative sentiments in face-to-face conversations, particularly when the meeting participants do not know each other well (in the AMI scenario meetings, many participants were meeting each other for the first time). But there may be a further explanation for why we learn many more positive than negative patterns. When conversation participants do express negative sentiments, they may couch those sentiments in more euphemistic or guarded terms compared with positive sentiments. Table 2 gives examples of significant positive and negative</context>
<context position="22729" citStr="Wilson (2008)" startWordPosition="3616" endWordPosition="3617">ote control from design to market. The four members of the group are assigned roles of project manager, industrial designer, user interface designer, and marketing expert. In total there are 140 such scenario meetings, with individual meetings ranging from approximately 15 to 45 minutes. The BC3 corpus (Ulrich et al., 2008) contains email threads from the World Wide Web Consortium (W3C) mailing list. The threads feature a variety of topics such as web accessibility and planning face-to-face meetings. The annotated portion of the mailing list consists of 40 threads. 4.2 Subjectivity Annotation Wilson (2008) has annotated 20 AMI meetings for a variety of subjective phenomena which fall into the broad classes of subjective utterances, objective polar utterances and subjective questions. It is this first class in which we are primarily interested here. Two subclasses of subjective utterances are positive subjective and negative subjective utterances. Such subjective utterances involve the expression of a private state, such as a positive/negative opinion, positive/negative argument, and agreement/disagreement. The 20 meetings were labeled by a single annotator, though Wilson (2008) did conduct a st</context>
<context position="24380" citStr="Wilson (2008)" startWordPosition="3863" endWordPosition="3865">oal is to discriminate positivesubjective from negative-subjective. For the BC3 emails, annotators were initially asked to create extractive and abstractive summaries of each thread, in addition to labeling a variety of sentence-level phenomena, including whether each sentence was subjective. In a second round of annotations, three different annotators were asked to go through all of the sentences previously labeled as subjective and indicate whether each sentence was positive, negative, positivenegative, or other. The definitions for positive and negative subjectivity mirrored those given by Wilson (2008). For the purpose of these experiments, we consider a sentence to be subjective if at least two of the annotators labeled it as subjective, and similarly consider a subjective sentence to be positive or negative if at least two annotators label it as such. Using this majority vote labeling, 172 of 1800 sentences are considered subjective, with 44% of those labeled as positive-subjective and 37% as negative-subjective, showing that there is much more of a balance between positive and negative sentiment in these email threads compared with meeting speech (note that some subjective sentences are </context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>T. Wilson. 2008. Annotating subjective content in meetings. In Proc. of LREC 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proc. ofEMNLP 2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5290" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="792" endWordPosition="795">he strings matched by the second feature. To give their own example, the unigram happy subsumes the bigram very happy. The first feature will behaviorally subsume the second if it representationally subsumes the second and has roughly the same information gain, within an acceptable margin. They show that they can improve opinion analysis results by modeling these relations and reducing the feature set. Our approach for learning subjective patterns like Raaijmakers et al. relies on n-grams, but like Riloff et al. moves beyond fixed sequences of words by varying levels of lexical instantiation. Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. They found that the latter two tasks were substantially more difficult than classification at the document level. Of particular relevance here is that they found that partof-speech (POS) features were especially useful for assigning polarity scores, with adjectives, adverbs and verbs comprising the best set of POS tags. This work inspired us to look at general</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proc. ofEMNLP 2003, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>