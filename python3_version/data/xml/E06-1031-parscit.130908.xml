<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.5809">
CDER: Efficient MT Evaluation Using Block Movements
</title>
<note confidence="0.93572">
Gregor Leusch and Nicola Ueffing and Hermann Ney
Lehrstuhl f¨ur Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
</note>
<email confidence="0.987338">
{leusch,ueffing,ney}@i6.informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.994494" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866333333333">
Most state-of-the-art evaluation measures
for machine translation assign high costs
to movements of word blocks. In many
cases though such movements still result
in correct or almost correct sentences. In
this paper, we will present a new eval-
uation measure which explicitly models
block reordering as an edit operation.
Our measure can be exactly calculated in
quadratic time.
Furthermore, we will show how some
evaluation measures can be improved
by the introduction of word-dependent
substitution costs. The correlation of the
new measure with human judgment has
been investigated systematically on two
different language pairs. The experimental
results will show that it significantly
outperforms state-of-the-art approaches in
sentence-level correlation. Results from
experiments with word dependent substi-
tution costs will demonstrate an additional
increase of correlation between automatic
evaluation measures and human judgment.
</bodyText>
<sectionHeader confidence="0.998879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927233333333">
Research in machine translation (MT) depends
heavily on the evaluation of its results. Espe-
cially for the development of an MT system,
an evaluation measure is needed which reliably
assesses the quality of MT output. Such a measure
will help analyze the strengths and weaknesses of
different translation systems or different versions
of the same system by comparing output at
the sentence level. In most applications of
MT, understandability for humans in terms of
readability as well as semantical correctness
should be the evaluation criterion. But as human
evaluation is tedious and cost-intensive, automatic
evaluation measures are used in most MT research
tasks. A high correlation between these automatic
evaluation measures and human evaluation is thus
desirable.
State-of-the-art measures such as BLEU (Pap-
ineni et al., 2002) or NIST (Doddington, 2002)
aim at measuring the translation quality rather
on the document level1 than on the level of
single sentences. They are thus not well-suited
for sentence-level evaluation. The introduction
of smoothing (Lin and Och, 2004) solves this
problem only partially.
In this paper, we will present a new automatic
error measure for MT – the CDER – which is
designed for assessing MT quality on the sentence
level. It is based on edit distance – such as the
well-known word error rate (WER) – but allows
for reordering of blocks. Nevertheless, by defining
reordering costs, the ordering of the words in
a sentence is still relevant for the measure. In
this, the new measure differs significantly from
the position independent error rate (PER) by
(Tillmann et al., 1997). Generally, finding an
optimal solution for such a reordering problem is
NP hard, as is shown in (Lopresti and Tomkins,
1997). In previous work, researchers have tried to
reduce the complexity, for example by restricting
the possible permutations on the block-level, or by
approximation or heuristics during the calculation.
Nevertheless, most of the resulting algorithms still
have high run times and are hardly applied in
practice, or give only a rough approximation. An
overview of some better-known measures can be
found in Section 3.1. In contrast to this, our new
measure can be calculated very efficiently. This
is achieved by requiring complete and disjoint
coverage of the blocks only for the reference
sentence, and not for the candidate translation. We
will present an algorithm which computes the new
error measure in quadratic time.
The new evaluation measure will be investi-
gated and compared to state-of-the-art methods
on two translation tasks. The correlation with
human assessment will be measured for several
different statistical MT systems. We will see
that the new measure significantly outperforms the
existing approaches.
</bodyText>
<footnote confidence="0.911987">
1The n-gram precisions are measured at the sentence level
and then combined into a score over the whole document.
</footnote>
<page confidence="0.996975">
241
</page>
<bodyText confidence="0.999896">
As a further improvement, we will introduce
word dependent substitution costs. This method
will be applicable to the new measure as well
as to established measures like WER and PER.
Starting from the observation that the substitution
of a word with a similar one is likely to affect
translation quality less than the substitution with
a completely different word, we will show how
the similarity of words can be accounted for in
automatic evaluation measures.
This paper is organized as follows: In Section 2,
we will present the state of the art in MT
evaluation and discuss the problem of block
reordering. Section 3 will introduce the new
error measure CDER and will show how it can
be calculated efficiently. The concept of word-
dependent substitution costs will be explained in
Section 4. In Section 5, experimental results on
the correlation of human judgment with the CDER
and other well-known evaluation measures will be
presented. Section 6 will conclude the paper and
give an outlook on possible future work.
</bodyText>
<sectionHeader confidence="0.994556" genericHeader="method">
2 MT Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999986">
2.1 Block Reordering and State of the Art
</subsectionHeader>
<bodyText confidence="0.9999068">
In MT – as opposed to other natural language
processing tasks like speech recognition – there
is usually more than one correct outcome of a
task. In many cases, alternative translations of
a sentence differ from each other mostly by the
ordering of blocks of words. Consequently, an
evaluation measure for MT should be able to
detect and allow for block reordering. Neverthe-
less, a higher “amount” of reordering between a
candidate translation and a reference translation
should still be reflected in a worse evaluation
score. In other words, the more blocks there are
to be reordered between reference and candidate
sentence, the higher we want the measure to
evaluate the distance between these sentences.
State-of-the-art evaluation measures for MT
penalize movement of blocks rather severely: n-
gram based scores such as BLEU or NIST still
yield a high unigram precision if blocks are
reordered. For higher-order n-grams, though, the
precision drops. As a consequence, this affects the
overall score significantly. WER, which is based
on Levenshtein distance, penalizes the reordering
of blocks even more heavily. It measures the
distance by substitution, deletion and insertion
operations for each word in a relocated block.
PER, on the other hand, ignores the ordering
of the words in the sentences completely. This
often leads to an overly optimistic assessment of
translation quality.
</bodyText>
<subsectionHeader confidence="0.999745">
2.2 Long Jumps
</subsectionHeader>
<bodyText confidence="0.999990088888889">
The approach we pursue in this paper is to
extend the Levenshtein distance by an additional
operation, namely block movement. The number
of blocks in a sentence is equal to the number
of gaps among the blocks plus one. Thus, the
block movements can equivalently be expressed
as long jump operations that jump over the
gaps between two blocks. The costs of a
long jump are constant. The blocks are read
in the order of one of the sentences. These
long jumps are combined with the “classical”
Levenshtein edit operations, namely insertion,
deletion, substitution, and the zero-cost operation
identity. The resulting long jump distance dLJ
gives the minimum number of operations which
are necessary to transform the candidate sentence
into the reference sentence. Like the Levenshtein
distance, the long jump distance can be depicted
using an alignment grid as shown in Figure 1:
Here, each grid point corresponds to a pair of
inter-word positions in candidate and reference
sentence, respectively. dLJ is the minimum cost of
a path between the lower left (first) and the upper
right (last) alignment grid point which covers all
reference and candidate words. Deletions and
insertions correspond to horizontal and vertical
edges, respectively. Substitutions and identity
operations correspond to diagonal edges. Edges
between arbitrary grid points from the same row
correspond to long jump operations. It is easy to
see that dLJ is symmetrical.
In the example, the best path contains one dele-
tion edge, one substitution edge, and three long
jump edges. Therefore, the long jump distance
between the sentences is five. In contrast, the
best Levenshtein path contains one deletion edge,
four identity and five consecutive substitution
edges; the Levenshtein distance between the two
sentences is six. The effect of reordering on the
BLEU measure is even higher in this example:
Whereas 8 of the 10 unigrams from the candidate
sentence can be found in the reference sentence,
this holds for only 4 bigrams, and 1 trigram. Not a
single one of the 7 candidate four-grams occurs in
the reference sentence.
</bodyText>
<sectionHeader confidence="0.948936" genericHeader="method">
3 CDER: A New Evaluation Measure
</sectionHeader>
<subsectionHeader confidence="0.993786">
3.1 Approach
</subsectionHeader>
<bodyText confidence="0.996943333333333">
(Lopresti and Tomkins, 1997) showed that finding
an optimal path in a long jump alignment grid is
an NP-hard problem. Our experiments showed
that the calculation of exact long jump distances
becomes impractical for sentences longer than 20
words.
</bodyText>
<page confidence="0.998019">
242
</page>
<figureCaption confidence="0.72684275">
Figure 1: Example of a long jump alignment
grid. All possible deletion, insertion, identity and
substitution operations are depicted. Only long
jump edges from the best path are drawn.
</figureCaption>
<bodyText confidence="0.999951283018868">
A possible way to achieve polynomial run-
time is to restrict the number of admissible block
permutations. This has been implemented by
(Leusch et al., 2003) in the inversion word error
rate. Alternatively, a heuristic or approximative
distance can be calculated, as in GTM by (Turian et
al., 2003). An implementation of both approaches
at the same time can be found in TER by (Snover
et al., 2005). In this paper, we will present another
approach which has a suitable run-time, while
still maintaining completeness of the calculated
measure. The idea of the proposed method is to
drop some restrictions on the alignment path.
The long jump distance as well as the Lev-
enshtein distance require both reference and
candidate translation to be covered completely
and disjointly. When extending the metric by
block movements, we drop this constraint for the
candidate translation. That is, only the words
in the reference sentence have to be covered
exactly once, whereas those in the candidate
sentence can be covered zero, one, or multiple
times. Dropping the constraints makes an efficient
computation of the distance possible. We drop
the constraints for the candidate sentence and not
for the reference sentence because we do not want
any information contained in the reference to be
omitted. Moreover, the reference translation will
not contain unnecessary repetitions of blocks.
The new measure – which will be called
CDER in the following – can thus be seen as a
measure oriented towards recall, while measures
like BLEU are guided by precision. The CDER
is based on the CDCD distance2 introduced
in (Lopresti and Tomkins, 1997). The authors
show there that the problem of finding the optimal
solution can be solved in O(I2 · L) time, where
I is the length of the candidate sentence and L
the length of the reference sentence. Within this
paper, we will refer to this distance as dCD . In
the next subsection, we will show how it can be
computed in O(I · L) time using a modification of
the Levenshtein algorithm.
We also studied the reverse direction of the
described measure; that is, we dropped the
coverage constraints for the reference sentence
instead of the candidate sentence. Addition-
ally, the maximum of both directions has been
considered as distance measure. The results in
Section 5.2 will show that the measure using the
originally proposed direction has a significantly
higher correlation with human evaluation than the
other directions.
</bodyText>
<subsectionHeader confidence="0.998427">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.929232">
Our algorithm for calculating dCD is based
on the dynamic programming algorithm for the
Levenshtein distance (Levenshtein, 1966). The
Levenshtein distance dLev(eI1, ˜eL � between two
</bodyText>
<equation confidence="0.731716">
1
</equation>
<bodyText confidence="0.977174333333333">
strings eI1 and ˜eL1 can be calculated in con-
stant time if the Levenshtein distances of the
substrings, dLev(eI−1
</bodyText>
<equation confidence="0.7836942">
1 , ˜e1), dLev(eI1, ˜e1−1), and
dLev(e1−1, ˜e1 I, are known.
Consequently, an auxiliary quantity
�
DLev(i, l) := dLev�ei 1, ˜el1
</equation>
<bodyText confidence="0.991686375">
is stored in an I x L table. This auxiliary quantity
can then be calculated recursively from DLev(i −
1, l), DLev(i, l − 1), and DLev(i − 1, l − 1).
Consequently, the Levenshtein distance can be
calculated in time O(I · L).
This algorithm can easily be extended for the
calculation of dCD as follows: Again we define
an auxiliary quantity D(i, l) as
</bodyText>
<equation confidence="0.525213">
�
D(i, l) := dCD �ei 1, ˜el1
</equation>
<bodyText confidence="0.906595833333333">
Insertions, deletions, and substitutions are
handled the same way as in the Levenshtein
algorithm. Now assume that an optimal dCD path
has been found: Then, each long jump edge within
2C stands for cover and D for disjoint. We adopted this
notion for our measures.
</bodyText>
<figure confidence="0.979348772727273">
.
o’clock
seven
at
airport
the
at
reference
candidate
deletion
insertion
substitution
identity best path
long jump
block
start/
end node
met
we
243
l
l-1
</figure>
<figureCaption confidence="0.869803">
Figure 2: Predecessors of a grid point (i, l) in
Equation 1
</figureCaption>
<bodyText confidence="0.8061605">
this path will always start at a node with the lowest
D value in its row3.
Consequently, we use the following modifica-
tion of the Levenshtein recursion:
</bodyText>
<equation confidence="0.999740666666667">
D(0, 0) = 0
(1)
D(i−1, l−1) + (1−δ(ei, ˜el)) ,
D(i − 1,l) + 1,
D(i,l − 1) + 1,
D(i&apos;, l) + 1
</equation>
<bodyText confidence="0.9996512">
where δ is the Kronecker delta. Figure 2 shows the
possible predecessors of a grid point.
The calculation of D(i, l) requires all values of
D(i&apos;, l) to be known, even for i&apos; &gt; i. Thus, the
calculation takes three steps for each l:
</bodyText>
<listItem confidence="0.914626333333333">
1. For each i, calculate the minimum of the first
three terms.
2. Calculate min
</listItem>
<bodyText confidence="0.33167">
i0
</bodyText>
<subsectionHeader confidence="0.509396">
Length Difference
</subsectionHeader>
<bodyText confidence="0.990516833333334">
There is always an optimal dCD alignment path
that does not contain any deletion edges, because
each deletion can be replaced by a long jump, at
the same costs. This is different for a dLJ path,
because here each candidate word must be covered
exactly once. Assume now that the candidate
sentence consists of I words and the reference
sentence consists of L words, with I &gt; L.
Then, at most L candidate words can be covered
by substitution or identity edges. Therefore, the
remaining candidate words (at least I − L) must
be covered by deletion edges. This means that at
least I −L deletion edges will be found in any dLJ
path, which leads to dLJ − dCD ≥ I − L in this
case.
Consequently, the length difference between
the two sentences gives us a useful miscoverage
penalty lplen:
</bodyText>
<equation confidence="0.97761">
lplen := max(I − L, 0)
</equation>
<bodyText confidence="0.983620923076923">
This penalty is independent of the dCD alignment
path. Thus, an optimal dCD alignment path
is optimal for dCD + lplen as well. Therefore
the search algorithm in Section 3.2 will find the
optimum for this sum.
Absolute Miscoverage
Let coverage(i) be the number of substitution,
identity, and deletion edges that cover a candidate
word ei in a dCD path. If we had a complete and
disjoint alignment for the candidate word (i.e., a
dLJ path), coverage(i) would be 1 for each i.
In general this is not the case. We can use the
absolute miscoverage as a penalty lpmisc for dCD:
</bodyText>
<figure confidence="0.9478562">
deletion insertion subst/id long jump
...
i-1 i
D(i, l) = min {
min
i0
}
D(i&apos;, l).
3. For each i, calculate the minimum according Elpmisc := |1 − coverage(i)|
to Equation 1. i
</figure>
<bodyText confidence="0.791520666666667">
Each of these steps can be done in time O(I).
Therefore, this algorithm calculates dCD in time
O(I · L) and space O(I).
</bodyText>
<subsectionHeader confidence="0.999917">
3.3 Hypothesis Length and Penalties
</subsectionHeader>
<bodyText confidence="0.999964">
As the CDER does not penalize candidate trans-
lations which are too long, we studied the use
of a length penalty or miscoverage penalty. This
determines the difference in sentence lengths
between candidate and reference. Two definitions
of such a penalty have been studied for this work.
</bodyText>
<footnote confidence="0.500038333333333">
3Proof: Assume that the long jump edge goes from (i0, l)
to (i, l), and that there exists an i00 such that D(i00, l) &lt;
D(i0, l). This means that the path from (0, 0) to (i00, l) is
less expensive than the path from (0, 0) to (i0, l). Thus, the
path from (0, 0) through (i00, l) to (i, l) is less expensive than
the path through (i0, l). This contradicts the assumption.
</footnote>
<bodyText confidence="0.9979445">
This miscoverage penalty is not independent of
the alignment path. Consequently, the proposed
search algorithm will not necessarily find an
optimal solution for the sum of dCD and lpmisc.
The idea behind the absolute miscoverage is
that one can construct a valid – but not necessarily
optimal – dLJ path from a given dCD path. This
procedure is illustrated in Figure 3 and takes place
in two steps:
1. For each block of over-covered candidate
words, replace the aligned substitution and/or
identity edges by insertion edges; move the
long jump at the beginning of the block
accordingly.
2. For each block of under-covered candidate
words, add the corresponding number of
</bodyText>
<page confidence="0.998465">
244
</page>
<figureCaption confidence="0.9601945">
Figure 3: Transformation of a dCD path into a dLJ
path.
</figureCaption>
<bodyText confidence="0.998122333333333">
deletion edges; move the long jump at the
beginning of the block accordingly.
This also shows that there cannot be4 a
polynomial time algorithm that calculates the
minimum of dCD + lpmisc for arbitrary pairs of
sentences, because this minimum is equal to dLJ.
With these miscoverage penalties, inexpensive
lower and upper bounds for dLJ can be calculated,
because the following inequality holds:
</bodyText>
<listItem confidence="0.706428">
(2) dCD + lplen ≤ dLJ ≤ dCD + lpmisc
4 Word-dependent Substitution Costs
</listItem>
<subsectionHeader confidence="0.729368">
4.1 Idea
</subsectionHeader>
<bodyText confidence="0.996917666666667">
All automatic error measures which are based
on the edit distance (i.e. WER, PER, and
CDER) apply fixed costs for the substitution
of words. However, this is counter-intuitive,
as replacing a word with another one which
has a similar meaning will rarely change the
meaning of a sentence significantly. On the other
hand, replacing the same word with a completely
different one probably will. Therefore, it seems
advisable to make substitution costs dependent on
the semantical and/or syntactical dissimilarity of
the words.
To avoid awkward case distinctions, we assume
that a substitution cost function cSUB for two
words e, e˜ meets the following requirements:
</bodyText>
<listItem confidence="0.8086575">
1. cSUB depends only on e and ˜e.
2. cSUB is a metric; especially
(a) The costs are zero if e = ˜e, and larger
than zero otherwise.
(b) The triangular inequation holds: it is
always cheaper to replace e by e˜ than to
replace e by e&apos; and then e&apos; by ˜e.
4provided that P # NP, of course.
</listItem>
<bodyText confidence="0.9791063125">
3. The costs of substituting a word e by e˜ are
always equal or lower than those of deleting
e and then inserting ˜e. In short, cSUB ≤ 2.
Under these conditions the algorithms for
WER and CDER can easily be modified to use
word-dependent substitution costs. For example,
the only necessary modification in the CDER
algorithm in Equation 1 is to replace 1 − δ(e, ˜e)
by cSUB(e, ˜e).
For the PER, it is no longer possible to use a
linear time algorithm in the general case. Instead,
a modification of the Hungarian algorithm (Knuth,
1993) can be used.
The question is now how to define the word-
dependent substitution costs. We have studied two
different approaches.
</bodyText>
<subsectionHeader confidence="0.992899">
4.2 Character-based Levenshtein Distance
</subsectionHeader>
<bodyText confidence="0.999946555555555">
A pragmatic approach is to compare the spelling
of the words to be substituted with each other.
The more similar the spelling is, the more similar
we consider the words to be, and the lower we
want the substitution costs between them. In
English, this works well with similar tenses of
the same verb, or with genitives or plurals of the
same noun. Nevertheless, a similar spelling is no
guarantee for a similar meaning, because prefixes
such as “mis-”, “in-”, or “un-” can change the
meaning of a word significantly.
An obvious way of comparing the spelling is the
Levenshtein distance. Here, words are compared
on character level. To normalize this distance
into a range from 0 (for identical words) to 1
(for completely different words), we divide the
absolute distance by the length of the Levenshtein
alignment path.
</bodyText>
<subsectionHeader confidence="0.998849">
4.3 Common Prefix Length
</subsectionHeader>
<bodyText confidence="0.998315875">
Another character-based substitution cost function
we studied is based on the common prefix length
of both words. In English, different tenses of
the same verb share the same prefix; which is
usually the stem. The same holds for different
cases, numbers and genders of most nouns and
adjectives. However, it does not hold if verb
prefixes are changed or removed. On the other
hand, the common prefix length is sensitive to
critical prefixes such as “mis-” for the same
reason. Consequently, the common prefix length,
normalized by the average length of both words,
gives a reasonable measure for the similarity of
two words. To transform the normalized common
prefix length into costs, this fraction is then
subtracted from 1.
</bodyText>
<tableCaption confidence="0.6572365">
Table 1 gives an example of these two word-
dependent substitution costs.
</tableCaption>
<figure confidence="0.98576">
reference
coverage
1 2 2 0
1 1 1 1 1 1 1 1 1 1
dCD dLJ
candidate
candidate
deletion insertion subst/id long jump
</figure>
<page confidence="0.996724">
245
</page>
<tableCaption confidence="0.999855">
Table 1: Example of word-dependent substitution costs.
</tableCaption>
<table confidence="0.998833">
e e˜ Levenshtein prefix
distance substitution cost similarity substitution cost
usual unusual 2 2 1 1 − 16 = 0.83
7 = 0.29
understanding misunderstanding 3 3 0 1.00
16 = 0.19
talk talks 1 1 4 4.54
5 = 0.20
1− = 0.11
</table>
<subsectionHeader confidence="0.988762">
4.4 Perspectives
</subsectionHeader>
<bodyText confidence="0.999686666666667">
More sophisticated methods could be considered
for word-dependent substitution costs as well.
Examples of such methods are the introduction of
information weights as in the NIST measure or the
comparison of stems or synonyms, as in METEOR
(Banerjee and Lavie, 2005).
</bodyText>
<sectionHeader confidence="0.987346" genericHeader="method">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.973755">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999985777777778">
The different evaluation measures were assessed
experimentally on data from the Chinese–English
and the Arabic–English task of the NIST 2004
evaluation workshop (Przybocki, 2004). In this
evaluation campaign, 4460 and 1735 candidate
translations, respectively, generated by different
research MT systems were evaluated by human
judges with regard to fluency and adequacy.
Four reference translations are provided for each
candidate translation. Detailed corpus statistics
are listed in Table 2.
For the experiments in this study, the candidate
translations from these tasks were evaluated using
different automatic evaluation measures. Pear-
son’s correlation coefficient r between automatic
evaluation and the sum of fluency and adequacy
was calculated. As it could be arguable whether
Pearson’s r is meaningful for categorical data like
human MT evaluation, we have also calculated
Kendall’s correlation coefficient T. Because of
the high number of samples (= sentences, 4460)
versus the low number of categories (= out-
comes of adequacy+fluency, 9), we calculated
T separately for each source sentence. These
experiments showed that Kendall’s T reflects the
same tendencies as Pearson’s r regarding the
ranking of the evaluation measures. But only
the latter allows for an efficient calculation of
confidence intervals. Consequently, figures of T
are omitted in this paper.
Due to the small number of samples for eval-
uation on system level (10 and 5, respectively),
all correlation coefficients between automatic
and human evaluation on system level are very
close to 1. Therefore, they do not show any
significant differences for the different evaluation
</bodyText>
<tableCaption confidence="0.99739">
Table 2: Corpus statistics. TIDES corpora,
</tableCaption>
<table confidence="0.96563725">
NIST 2004 evaluation.
Source language Chinese Arabic
Target language English English
Sentences 446 347
Running words 13 016 10 892
Ref. translations 4 4
Avg. ref. length 29.2 31.4
Candidate systems 10 5
</table>
<bodyText confidence="0.99965936">
measures. Additional experiments on data from
the NIST 2002 and 2003 workshops and from
the IWSLT 2004 evaluation workshop confirm
the findings from the NIST 2004 experiments;
for the sake of clarity they are not included
here. All correlation coefficients presented here
were calculated for sentence level evaluation.
For comparison with state-of-the-art evaluation
measures, we have also calculated the correlation
between human evaluation and WER and BLEU,
which were both measures of choice in several
international MT evaluation campaigns. Further-
more, we included TER (Snover et al., 2005) as
a recent heuristic block movement measure in
some of our experiments for comparison with our
measure. As the BLEU score is unsuitable for
sentence level evaluation in its original definition,
BLEU-S smoothing as described by (Lin and
Och, 2004) is performed. Additionally, we
added sentence boundary symbols for BLEU, and
a different reference length calculation scheme
for TER, because these changes improved the
correlation between human evaluation and the two
automatic measures. Details on this have been
described in (Leusch et al., 2005).
</bodyText>
<subsectionHeader confidence="0.992266">
5.2 ODER
</subsectionHeader>
<bodyText confidence="0.999949">
Table 3 presents the correlation of BLEU, WER,
and CDER with human assessment. It can be
seen that CDER shows better correlation than
BLEU and WER on both corpora. On the
Chinese–English task, the smoothed BLEU score
has a higher sentence-level correlation than WER.
However, this is not the case for the Arabic–
</bodyText>
<page confidence="0.99915">
246
</page>
<tableCaption confidence="0.80300975">
Table 3: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalu-
ation with BLEU, WER, and CDER (NIST 2004
evaluation; sentence level).
</tableCaption>
<table confidence="0.999887">
Automatic measure Chin.–E. Arab.–E.
BLEU 0.615 0.603
WER 0.559 0.589
CDER 0.625 0.623
CDER reverseda 0.222 0.393
CDER maximumb 0.594 0.599
</table>
<tableCaption confidence="0.981979">
aCD constraints for candidate instead of reference.
bSentence-wise maximum of normal and reversed CDER
Table 4: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalua-
tion for CDER with different penalties.
</tableCaption>
<table confidence="0.9935304">
Penalty Chin.–E. Arab.–E.
− 0.625 0.623
lplen 0.581 0.567
lpmisc 0.466 0.528
(lplen + lpmisc)/2 0.534 0.557
</table>
<bodyText confidence="0.997953538461538">
English task. So none of these two measures
is superior to the other one, but they are both
outperformed by CDER.
If the direction of CDER is reversed (i.e, the
CD constraints are required for the candidate
instead of the reference, such that the measure
has precision instead of recall characteristics), the
correlation with human evaluation is much lower.
Additionally we studied the use of the maxi-
mum of the distances in both directions. This has
a lower correlation than taking the original CDER,
as Table 3 shows. Nevertheless, the maximum still
performs slightly better than BLEU and WER.
</bodyText>
<subsectionHeader confidence="0.999808">
5.3 Hypothesis Length and Penalties
</subsectionHeader>
<bodyText confidence="0.9991861">
The problem of how to avoid a preference of
overly long candidate sentences by CDER remains
unsolved, as can be found in Table 4: Each of
the proposed penalties infers a significant decrease
of correlation between the (extended) CDER and
human evaluation. Future research will aim at
finding a suitable length penalty. Especially
if CDER is applied in system development,
such a penalty will be needed, as preliminary
optimization experiments have shown.
</bodyText>
<subsectionHeader confidence="0.836559">
5.4 Substitution Costs
</subsectionHeader>
<tableCaption confidence="0.955909875">
Table 5 reveals that the inclusion of word-
dependent substitution costs yields a raise by more
than 1% absolute in the correlation of CDER
with human evaluation. The same is true for
Table 5: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalu-
ation for WER and CDER with word-dependent
substitution costs.
</tableCaption>
<table confidence="0.998549714285714">
Measure Subst. costs Chin.–E. Arab.–E.
WER const (1) 0.559 0.589
prefix 0.571 0.605
Levenshtein 0.580 0.611
CDER const (1) 0.625 0.623
prefix 0.637 0.634
Levenshtein 0.638 0.637
</table>
<bodyText confidence="0.998722916666667">
WER: the correlation with human judgment is
increased by about 2% absolute on both language
pairs. The Levenshtein-based substitution costs
are better suited for WER than the scheme based
on common prefix length. For CDER, there is
hardly any difference between the two methods.
Experiments on five more corpora did not give any
significant evidence which of the two substitution
costs correlates better with human evaluation. But
as the prefix-based substitution costs improved
correlation more consistently across all corpora,
we employed this method in our next experiment.
</bodyText>
<subsectionHeader confidence="0.997612">
5.5 Combination of CDER and PER
</subsectionHeader>
<bodyText confidence="0.999980068965517">
An interesting topic in MT evaluation research
is the question whether a linear combination of
two MT evaluation measures can improve the
correlation between automatic and human evalu-
ation. Particularly, we expected the combination
of CDER and PER to have a significantly higher
correlation with human evaluation than the mea-
sures alone. CDER (as opposed to PER) has the
ability to reward correct local ordering, whereas
PER (as opposed to CDER) penalizes overly long
candidate sentences. The two measures were
combined with linear interpolation. In order
to determine the weights, we performed data
analysis on seven different corpora. The result was
consistent across all different data collections and
language pairs: a linear combination of about 60%
CDER and 40% PER has a significantly higher
correlation with human evaluation than each of
the measures alone. For the two corpora studied
here, the results of the combination can be found
in Table 6: On the Chinese–English task, there is
an additional gain of more than 1% absolute in
correlation over CDER alone. The combined error
measure is the best method in both cases.
The last line in Table 6 shows the 95%-
confidence interval for the correlation. We see
that the new measure CDER, combined with PER,
has a significantly higher correlation with human
evaluation than the existing measures BLEU, TER,
</bodyText>
<page confidence="0.998281">
247
</page>
<tableCaption confidence="0.90954">
Table 6: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalua-
tion for different automatic evaluation measures.
</tableCaption>
<table confidence="0.997785888888889">
Automatic measure Chin.–E. Arab.–E.
BLEU 0.615 0.603
TER 0.548 0.582
WER 0.559 0.589
WER + Lev. subst. 0.580 0.611
CDER 0.625 0.623
CDER +prefix subst. 0.637 0.634
CDER +prefix+PER 0.649 0.635
95%-confidence ±0.018 ±0.028
</table>
<bodyText confidence="0.857254">
and WER on both corpora.
</bodyText>
<sectionHeader confidence="0.996075" genericHeader="conclusions">
6 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.9999892">
We presented CDER, a new automatic evalua-
tion measure for MT, which is based on edit
distance extended by block movements. CDER
allows for reordering blocks of words at constant
cost. Unlike previous block movement measures,
CDER can be exactly calculated in quadratic
time. Experimental evaluation on two different
translation tasks shows a significantly improved
correlation with human judgment in comparison
with state-of-the-art measures such as BLEU.
Additionally, we showed how word-dependent
substitution costs can be applied to enhance the
new error measure as well as existing approaches.
The highest correlation with human assessment
was achieved through linear interpolation of the
new CDER with PER.
Future work will aim at finding a suitable length
penalty for CDER. In addition, more sophisticated
definitions of the word-dependent substitution
costs will be investigated. Furthermore, it will
be interesting to see how this new error measure
affects system development: We expect it to
allow for a better sentence-wise error analysis.
For system optimization, preliminary experiments
have shown the need for a suitable length penalty.
</bodyText>
<sectionHeader confidence="0.977403" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.993430857142857">
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023, and was partly funded by the Euro-
pean Union under the integrated project TC-STAR
– Technology and Corpora for Speech to Speech
Translation
</bodyText>
<sectionHeader confidence="0.993947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925722222222">
S. Banerjee and A. Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
65–72, Ann Arbor, MI, Jun.
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. ARPA Workshop on Human
Language Technology.
D. E. Knuth, 1993. The Stanford GraphBase: a
platform for combinatorial computing, pages 74–87.
ACM Press, New York, NY.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications
to machine translation evaluation. MT Summit IX,
pages 240–247, New Orleans, LA, Sep.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic eval-
uation of machine translation. ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
17–24, Ann Arbor, MI, Jun.
V. I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707–710, Feb.
C.-Y. Lin and F. J. Och. 2004. Orange: a
method for evaluation automatic evaluation metrics
for machine translation. COLING 2004, pages 501–
507, Geneva, Switzerland, Aug.
D. Lopresti and A. Tomkins. 1997. Block edit models
for approximate string matching. Theoretical
Computer Science, 181(1):159–179, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002. BLEU: a method for automatic evaluation
of machine translation. 40th Annual Meeting of the
ACL, pages 311–318, Philadelphia, PA, Jul.
M. Przybocki. 2004. NIST machine translation 2004
evaluation: Summary of results. DARPA Machine
Translation Evaluation Workshop, Alexandria, VA.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul,
L. Micciulla, and R. Weischedel. 2005. A
study of translation error rate with targeted human
annotation. Technical Report LAMP-TR-126,
CS-TR-4755, UMIACS-TR-2005-58, University of
Maryland, College Park, MD.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based search for
statistical translation. European Conf. on Speech
Communication and Technology, pages 2667–2670,
Rhodes, Greece, Sep.
J. P. Turian, L. Shen, and I. D. Melamed. 2003.
Evaluation of machine translation and its evaluation.
MT Summit IX, pages 23–28, New Orleans, LA, Sep.
</reference>
<page confidence="0.997005">
248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647953">
<title confidence="0.999863">Efficient MT Evaluation Using Block Movements</title>
<author confidence="0.999903">Gregor Leusch</author>
<author confidence="0.999903">Nicola Ueffing</author>
<author confidence="0.999903">Hermann Ney</author>
<affiliation confidence="0.99487">Lehrstuhl f¨ur Informatik VI, Computer Science Department RWTH Aachen University</affiliation>
<address confidence="0.999774">D-52056 Aachen, Germany</address>
<abstract confidence="0.972296076923077">Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences. In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation. Our measure can be exactly calculated in quadratic time. Furthermore, we will show how some evaluation measures can be improved</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="21039" citStr="Banerjee and Lavie, 2005" startWordPosition="3492" endWordPosition="3495">1 dCD dLJ candidate candidate deletion insertion subst/id long jump 245 Table 1: Example of word-dependent substitution costs. e e˜ Levenshtein prefix distance substitution cost similarity substitution cost usual unusual 2 2 1 1 − 16 = 0.83 7 = 0.29 understanding misunderstanding 3 3 0 1.00 16 = 0.19 talk talks 1 1 4 4.54 5 = 0.20 1− = 0.11 4.4 Perspectives More sophisticated methods could be considered for word-dependent substitution costs as well. Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005). 5 Experimental Results 5.1 Experimental Setting The different evaluation measures were assessed experimentally on data from the Chinese–English and the Arabic–English task of the NIST 2004 evaluation workshop (Przybocki, 2004). In this evaluation campaign, 4460 and 1735 candidate translations, respectively, generated by different research MT systems were evaluated by human judges with regard to fluency and adequacy. Four reference translations are provided for each candidate translation. Detailed corpus statistics are listed in Table 2. For the experiments in this study, the candidate transl</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, MI, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. ARPA Workshop on Human Language Technology.</title>
<date>2002</date>
<contexts>
<context position="2077" citStr="Doddington, 2002" startWordPosition="292" endWordPosition="293">nalyze the strengths and weaknesses of different translation systems or different versions of the same system by comparing output at the sentence level. In most applications of MT, understandability for humans in terms of readability as well as semantical correctness should be the evaluation criterion. But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used in most MT research tasks. A high correlation between these automatic evaluation measures and human evaluation is thus desirable. State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences. They are thus not well-suited for sentence-level evaluation. The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. In this paper, we will present a new automatic error measure for MT – the CDER – which is designed for assessing MT quality on the sentence level. It is based on edit distance – such as the well-known word error rate (WER) – but allows for reordering of blocks. Nevertheless, by defining reordering costs, the ordering of the words in a sente</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>The Stanford GraphBase: a platform for combinatorial computing,</title>
<date>1993</date>
<pages>74--87</pages>
<publisher>ACM Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="18555" citStr="Knuth, 1993" startWordPosition="3075" endWordPosition="3076">han to replace e by e&apos; and then e&apos; by ˜e. 4provided that P # NP, of course. 3. The costs of substituting a word e by e˜ are always equal or lower than those of deleting e and then inserting ˜e. In short, cSUB ≤ 2. Under these conditions the algorithms for WER and CDER can easily be modified to use word-dependent substitution costs. For example, the only necessary modification in the CDER algorithm in Equation 1 is to replace 1 − δ(e, ˜e) by cSUB(e, ˜e). For the PER, it is no longer possible to use a linear time algorithm in the general case. Instead, a modification of the Hungarian algorithm (Knuth, 1993) can be used. The question is now how to define the worddependent substitution costs. We have studied two different approaches. 4.2 Character-based Levenshtein Distance A pragmatic approach is to compare the spelling of the words to be substituted with each other. The more similar the spelling is, the more similar we consider the words to be, and the lower we want the substitution costs between them. In English, this works well with similar tenses of the same verb, or with genitives or plurals of the same noun. Nevertheless, a similar spelling is no guarantee for a similar meaning, because pre</context>
</contexts>
<marker>Knuth, 1993</marker>
<rawString>D. E. Knuth, 1993. The Stanford GraphBase: a platform for combinatorial computing, pages 74–87. ACM Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>A novel string-to-string distance measure with applications to machine translation evaluation.</title>
<date>2003</date>
<booktitle>MT Summit IX,</booktitle>
<pages>240--247</pages>
<location>New Orleans, LA,</location>
<contexts>
<context position="9318" citStr="Leusch et al., 2003" startWordPosition="1456" endWordPosition="1459">luation Measure 3.1 Approach (Lopresti and Tomkins, 1997) showed that finding an optimal path in a long jump alignment grid is an NP-hard problem. Our experiments showed that the calculation of exact long jump distances becomes impractical for sentences longer than 20 words. 242 Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution operations are depicted. Only long jump edges from the best path are drawn. A possible way to achieve polynomial runtime is to restrict the number of admissible block permutations. This has been implemented by (Leusch et al., 2003) in the inversion word error rate. Alternatively, a heuristic or approximative distance can be calculated, as in GTM by (Turian et al., 2003). An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005). In this paper, we will present another approach which has a suitable run-time, while still maintaining completeness of the calculated measure. The idea of the proposed method is to drop some restrictions on the alignment path. The long jump distance as well as the Levenshtein distance require both reference and candidate translation to be covered complete</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2003</marker>
<rawString>G. Leusch, N. Ueffing, and H. Ney. 2003. A novel string-to-string distance measure with applications to machine translation evaluation. MT Summit IX, pages 240–247, New Orleans, LA, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>N Ueffing</author>
<author>D Vilar</author>
<author>H Ney</author>
</authors>
<title>Preprocessing and normalization for automatic evaluation of machine translation.</title>
<date>2005</date>
<booktitle>ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>17--24</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="24133" citStr="Leusch et al., 2005" startWordPosition="3954" endWordPosition="3957"> evaluation campaigns. Furthermore, we included TER (Snover et al., 2005) as a recent heuristic block movement measure in some of our experiments for comparison with our measure. As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. Additionally, we added sentence boundary symbols for BLEU, and a different reference length calculation scheme for TER, because these changes improved the correlation between human evaluation and the two automatic measures. Details on this have been described in (Leusch et al., 2005). 5.2 ODER Table 3 presents the correlation of BLEU, WER, and CDER with human assessment. It can be seen that CDER shows better correlation than BLEU and WER on both corpora. On the Chinese–English task, the smoothed BLEU score has a higher sentence-level correlation than WER. However, this is not the case for the Arabic– 246 Table 3: Correlation (r) between human evaluation (adequacy + fluency) and automatic evaluation with BLEU, WER, and CDER (NIST 2004 evaluation; sentence level). Automatic measure Chin.–E. Arab.–E. BLEU 0.615 0.603 WER 0.559 0.589 CDER 0.625 0.623 CDER reverseda 0.222 0.39</context>
</contexts>
<marker>Leusch, Ueffing, Vilar, Ney, 2005</marker>
<rawString>G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005. Preprocessing and normalization for automatic evaluation of machine translation. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 17–24, Ann Arbor, MI, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="11760" citStr="Levenshtein, 1966" startWordPosition="1857" endWordPosition="1858">cation of the Levenshtein algorithm. We also studied the reverse direction of the described measure; that is, we dropped the coverage constraints for the reference sentence instead of the candidate sentence. Additionally, the maximum of both directions has been considered as distance measure. The results in Section 5.2 will show that the measure using the originally proposed direction has a significantly higher correlation with human evaluation than the other directions. 3.2 Algorithm Our algorithm for calculating dCD is based on the dynamic programming algorithm for the Levenshtein distance (Levenshtein, 1966). The Levenshtein distance dLev(eI1, ˜eL � between two 1 strings eI1 and ˜eL1 can be calculated in constant time if the Levenshtein distances of the substrings, dLev(eI−1 1 , ˜e1), dLev(eI1, ˜e1−1), and dLev(e1−1, ˜e1 I, are known. Consequently, an auxiliary quantity � DLev(i, l) := dLev�ei 1, ˜el1 is stored in an I x L table. This auxiliary quantity can then be calculated recursively from DLev(i − 1, l), DLev(i, l − 1), and DLev(i − 1, l − 1). Consequently, the Levenshtein distance can be calculated in time O(I · L). This algorithm can easily be extended for the calculation of dCD as follows:</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710, Feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>Orange: a method for evaluation automatic evaluation metrics for machine translation. COLING</title>
<date>2004</date>
<pages>501--507</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2298" citStr="Lin and Och, 2004" startWordPosition="324" endWordPosition="327">f readability as well as semantical correctness should be the evaluation criterion. But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used in most MT research tasks. A high correlation between these automatic evaluation measures and human evaluation is thus desirable. State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences. They are thus not well-suited for sentence-level evaluation. The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. In this paper, we will present a new automatic error measure for MT – the CDER – which is designed for assessing MT quality on the sentence level. It is based on edit distance – such as the well-known word error rate (WER) – but allows for reordering of blocks. Nevertheless, by defining reordering costs, the ordering of the words in a sentence is still relevant for the measure. In this, the new measure differs significantly from the position independent error rate (PER) by (Tillmann et al., 1997). Generally, finding an optimal solution for such a reordering</context>
<context position="23834" citStr="Lin and Och, 2004" startWordPosition="3910" endWordPosition="3913">e. All correlation coefficients presented here were calculated for sentence level evaluation. For comparison with state-of-the-art evaluation measures, we have also calculated the correlation between human evaluation and WER and BLEU, which were both measures of choice in several international MT evaluation campaigns. Furthermore, we included TER (Snover et al., 2005) as a recent heuristic block movement measure in some of our experiments for comparison with our measure. As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. Additionally, we added sentence boundary symbols for BLEU, and a different reference length calculation scheme for TER, because these changes improved the correlation between human evaluation and the two automatic measures. Details on this have been described in (Leusch et al., 2005). 5.2 ODER Table 3 presents the correlation of BLEU, WER, and CDER with human assessment. It can be seen that CDER shows better correlation than BLEU and WER on both corpora. On the Chinese–English task, the smoothed BLEU score has a higher sentence-level correlation than WER. However, this is not th</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F. J. Och. 2004. Orange: a method for evaluation automatic evaluation metrics for machine translation. COLING 2004, pages 501– 507, Geneva, Switzerland, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lopresti</author>
<author>A Tomkins</author>
</authors>
<title>Block edit models for approximate string matching.</title>
<date>1997</date>
<journal>Theoretical Computer Science,</journal>
<volume>181</volume>
<issue>1</issue>
<contexts>
<context position="2962" citStr="Lopresti and Tomkins, 1997" startWordPosition="438" endWordPosition="441">In this paper, we will present a new automatic error measure for MT – the CDER – which is designed for assessing MT quality on the sentence level. It is based on edit distance – such as the well-known word error rate (WER) – but allows for reordering of blocks. Nevertheless, by defining reordering costs, the ordering of the words in a sentence is still relevant for the measure. In this, the new measure differs significantly from the position independent error rate (PER) by (Tillmann et al., 1997). Generally, finding an optimal solution for such a reordering problem is NP hard, as is shown in (Lopresti and Tomkins, 1997). In previous work, researchers have tried to reduce the complexity, for example by restricting the possible permutations on the block-level, or by approximation or heuristics during the calculation. Nevertheless, most of the resulting algorithms still have high run times and are hardly applied in practice, or give only a rough approximation. An overview of some better-known measures can be found in Section 3.1. In contrast to this, our new measure can be calculated very efficiently. This is achieved by requiring complete and disjoint coverage of the blocks only for the reference sentence, and</context>
<context position="8755" citStr="Lopresti and Tomkins, 1997" startWordPosition="1365" endWordPosition="1368">dges. Therefore, the long jump distance between the sentences is five. In contrast, the best Levenshtein path contains one deletion edge, four identity and five consecutive substitution edges; the Levenshtein distance between the two sentences is six. The effect of reordering on the BLEU measure is even higher in this example: Whereas 8 of the 10 unigrams from the candidate sentence can be found in the reference sentence, this holds for only 4 bigrams, and 1 trigram. Not a single one of the 7 candidate four-grams occurs in the reference sentence. 3 CDER: A New Evaluation Measure 3.1 Approach (Lopresti and Tomkins, 1997) showed that finding an optimal path in a long jump alignment grid is an NP-hard problem. Our experiments showed that the calculation of exact long jump distances becomes impractical for sentences longer than 20 words. 242 Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution operations are depicted. Only long jump edges from the best path are drawn. A possible way to achieve polynomial runtime is to restrict the number of admissible block permutations. This has been implemented by (Leusch et al., 2003) in the inversion word error rate. Al</context>
<context position="10793" citStr="Lopresti and Tomkins, 1997" startWordPosition="1696" endWordPosition="1699">e covered zero, one, or multiple times. Dropping the constraints makes an efficient computation of the distance possible. We drop the constraints for the candidate sentence and not for the reference sentence because we do not want any information contained in the reference to be omitted. Moreover, the reference translation will not contain unnecessary repetitions of blocks. The new measure – which will be called CDER in the following – can thus be seen as a measure oriented towards recall, while measures like BLEU are guided by precision. The CDER is based on the CDCD distance2 introduced in (Lopresti and Tomkins, 1997). The authors show there that the problem of finding the optimal solution can be solved in O(I2 · L) time, where I is the length of the candidate sentence and L the length of the reference sentence. Within this paper, we will refer to this distance as dCD . In the next subsection, we will show how it can be computed in O(I · L) time using a modification of the Levenshtein algorithm. We also studied the reverse direction of the described measure; that is, we dropped the coverage constraints for the reference sentence instead of the candidate sentence. Additionally, the maximum of both direction</context>
</contexts>
<marker>Lopresti, Tomkins, 1997</marker>
<rawString>D. Lopresti and A. Tomkins. 1997. Block edit models for approximate string matching. Theoretical Computer Science, 181(1):159–179, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="2050" citStr="Papineni et al., 2002" startWordPosition="285" endWordPosition="289">tput. Such a measure will help analyze the strengths and weaknesses of different translation systems or different versions of the same system by comparing output at the sentence level. In most applications of MT, understandability for humans in terms of readability as well as semantical correctness should be the evaluation criterion. But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used in most MT research tasks. A high correlation between these automatic evaluation measures and human evaluation is thus desirable. State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences. They are thus not well-suited for sentence-level evaluation. The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. In this paper, we will present a new automatic error measure for MT – the CDER – which is designed for assessing MT quality on the sentence level. It is based on edit distance – such as the well-known word error rate (WER) – but allows for reordering of blocks. Nevertheless, by defining reordering costs, the order</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. 40th Annual Meeting of the ACL, pages 311–318, Philadelphia, PA, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Przybocki</author>
</authors>
<title>NIST machine translation 2004 evaluation: Summary of results. DARPA Machine Translation Evaluation Workshop,</title>
<date>2004</date>
<location>Alexandria, VA.</location>
<contexts>
<context position="21267" citStr="Przybocki, 2004" startWordPosition="3524" endWordPosition="3525">.83 7 = 0.29 understanding misunderstanding 3 3 0 1.00 16 = 0.19 talk talks 1 1 4 4.54 5 = 0.20 1− = 0.11 4.4 Perspectives More sophisticated methods could be considered for word-dependent substitution costs as well. Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005). 5 Experimental Results 5.1 Experimental Setting The different evaluation measures were assessed experimentally on data from the Chinese–English and the Arabic–English task of the NIST 2004 evaluation workshop (Przybocki, 2004). In this evaluation campaign, 4460 and 1735 candidate translations, respectively, generated by different research MT systems were evaluated by human judges with regard to fluency and adequacy. Four reference translations are provided for each candidate translation. Detailed corpus statistics are listed in Table 2. For the experiments in this study, the candidate translations from these tasks were evaluated using different automatic evaluation measures. Pearson’s correlation coefficient r between automatic evaluation and the sum of fluency and adequacy was calculated. As it could be arguable w</context>
</contexts>
<marker>Przybocki, 2004</marker>
<rawString>M. Przybocki. 2004. NIST machine translation 2004 evaluation: Summary of results. DARPA Machine Translation Evaluation Workshop, Alexandria, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B J Dorr</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
<author>L Micciulla</author>
<author>R Weischedel</author>
</authors>
<title>A study of translation error rate with targeted human annotation.</title>
<date>2005</date>
<tech>Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58,</tech>
<institution>University of Maryland, College Park, MD.</institution>
<contexts>
<context position="9559" citStr="Snover et al., 2005" startWordPosition="1498" endWordPosition="1501">or sentences longer than 20 words. 242 Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution operations are depicted. Only long jump edges from the best path are drawn. A possible way to achieve polynomial runtime is to restrict the number of admissible block permutations. This has been implemented by (Leusch et al., 2003) in the inversion word error rate. Alternatively, a heuristic or approximative distance can be calculated, as in GTM by (Turian et al., 2003). An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005). In this paper, we will present another approach which has a suitable run-time, while still maintaining completeness of the calculated measure. The idea of the proposed method is to drop some restrictions on the alignment path. The long jump distance as well as the Levenshtein distance require both reference and candidate translation to be covered completely and disjointly. When extending the metric by block movements, we drop this constraint for the candidate translation. That is, only the words in the reference sentence have to be covered exactly once, whereas those in the candidate sentenc</context>
<context position="23586" citStr="Snover et al., 2005" startWordPosition="3870" endWordPosition="3873">4 Candidate systems 10 5 measures. Additional experiments on data from the NIST 2002 and 2003 workshops and from the IWSLT 2004 evaluation workshop confirm the findings from the NIST 2004 experiments; for the sake of clarity they are not included here. All correlation coefficients presented here were calculated for sentence level evaluation. For comparison with state-of-the-art evaluation measures, we have also calculated the correlation between human evaluation and WER and BLEU, which were both measures of choice in several international MT evaluation campaigns. Furthermore, we included TER (Snover et al., 2005) as a recent heuristic block movement measure in some of our experiments for comparison with our measure. As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. Additionally, we added sentence boundary symbols for BLEU, and a different reference length calculation scheme for TER, because these changes improved the correlation between human evaluation and the two automatic measures. Details on this have been described in (Leusch et al., 2005). 5.2 ODER Table 3 presents the correlation of BLEU, </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Makhoul, Micciulla, Weischedel, 2005</marker>
<rawString>M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micciulla, and R. Weischedel. 2005. A study of translation error rate with targeted human annotation. Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based search for statistical translation.</title>
<date>1997</date>
<booktitle>European Conf. on Speech Communication and Technology,</booktitle>
<pages>2667--2670</pages>
<location>Rhodes, Greece,</location>
<contexts>
<context position="2836" citStr="Tillmann et al., 1997" startWordPosition="417" endWordPosition="420">ted for sentence-level evaluation. The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. In this paper, we will present a new automatic error measure for MT – the CDER – which is designed for assessing MT quality on the sentence level. It is based on edit distance – such as the well-known word error rate (WER) – but allows for reordering of blocks. Nevertheless, by defining reordering costs, the ordering of the words in a sentence is still relevant for the measure. In this, the new measure differs significantly from the position independent error rate (PER) by (Tillmann et al., 1997). Generally, finding an optimal solution for such a reordering problem is NP hard, as is shown in (Lopresti and Tomkins, 1997). In previous work, researchers have tried to reduce the complexity, for example by restricting the possible permutations on the block-level, or by approximation or heuristics during the calculation. Nevertheless, most of the resulting algorithms still have high run times and are hardly applied in practice, or give only a rough approximation. An overview of some better-known measures can be found in Section 3.1. In contrast to this, our new measure can be calculated ver</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based search for statistical translation. European Conf. on Speech Communication and Technology, pages 2667–2670, Rhodes, Greece, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Turian</author>
<author>L Shen</author>
<author>I D Melamed</author>
</authors>
<title>Evaluation of machine translation and its evaluation.</title>
<date>2003</date>
<booktitle>MT Summit IX,</booktitle>
<pages>23--28</pages>
<location>New Orleans, LA,</location>
<contexts>
<context position="9459" citStr="Turian et al., 2003" startWordPosition="1479" endWordPosition="1482">blem. Our experiments showed that the calculation of exact long jump distances becomes impractical for sentences longer than 20 words. 242 Figure 1: Example of a long jump alignment grid. All possible deletion, insertion, identity and substitution operations are depicted. Only long jump edges from the best path are drawn. A possible way to achieve polynomial runtime is to restrict the number of admissible block permutations. This has been implemented by (Leusch et al., 2003) in the inversion word error rate. Alternatively, a heuristic or approximative distance can be calculated, as in GTM by (Turian et al., 2003). An implementation of both approaches at the same time can be found in TER by (Snover et al., 2005). In this paper, we will present another approach which has a suitable run-time, while still maintaining completeness of the calculated measure. The idea of the proposed method is to drop some restrictions on the alignment path. The long jump distance as well as the Levenshtein distance require both reference and candidate translation to be covered completely and disjointly. When extending the metric by block movements, we drop this constraint for the candidate translation. That is, only the wor</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>J. P. Turian, L. Shen, and I. D. Melamed. 2003. Evaluation of machine translation and its evaluation. MT Summit IX, pages 23–28, New Orleans, LA, Sep.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>