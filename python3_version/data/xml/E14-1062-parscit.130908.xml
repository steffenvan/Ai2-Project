<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005328">
<title confidence="0.996551">
Type-Supervised Domain Adaptation for Joint Segmentation and
POS-Tagging
</title>
<author confidence="0.9884">
Meishan Zhang†, Yue Zhang$ , Wanxiang Che†, Ting Liu†�
</author>
<affiliation confidence="0.984141">
†Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
</affiliation>
<email confidence="0.784116">
{mszhang, car, tliu}@ir.hit.edu.cn
</email>
<affiliation confidence="0.989727">
$Singapore University of Technology and Design
</affiliation>
<email confidence="0.984012">
yue zhang@sutd.edu.sg
</email>
<sectionHeader confidence="0.993265" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999842086956522">
We report an empirical investigation on
type-supervised domain adaptation for
joint Chinese word segmentation and
POS-tagging, making use of domain-
specific tag dictionaries and only un-
labeled target domain data to improve
target-domain accuracies, given a set of
annotated source domain sentences. Pre-
vious work on POS-tagging of other lan-
guages showed that type-supervision can
be a competitive alternative to token-
supervision, while semi-supervised tech-
niques such as label propagation are
important to the effectiveness of type-
supervision. We report similar findings
using a novel approach for joint Chinese
segmentation and POS-tagging, under a
cross-domain setting. With the help of un-
labeled sentences and a lexicon of 3,000
words, we obtain 33% error reduction in
target-domain tagging. In addition, com-
bined type- and token-supervision can lead
to improved cost-effectiveness.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990655777777778">
With accuracies of over 97%, POS-tagging of
WSJ can be treated as a solved problem (Man-
ning, 2011). However, performance is still well
below satisfactory for many other languages and
domains (Petrov et al., 2012; Christodoulopoulos
et al., 2010). There has been a line of research on
using a tag-dictionary for POS-tagging (Merialdo,
1994; Toutanova and Johnson, 2007; Ravi and
Knight, 2009; Garrette and Baldridge, 2012). The
idea is compelling: on the one hand, a list of lex-
icons is often available for special domains, such
as bio-informatics; on the other hand, compiling a
∗Corresponding author.
lexicon of word-tag pairs appears to be less time-
consuming than annotating full sentences.
However, success in type-supervised POS-
tagging turns out to depend on several subtle fac-
tors. For example, recent research has found that
the quality of the tag-dictionary is crucial to the
success of such methods (Banko and Moore, 2004;
Goldberg et al., 2008; Garrette and Baldridge,
2012). Banko and Moore (2004) found that the
accuracies can drop from 96% to 77% when a
hand-crafted tag dictionary is replaced with a raw
tag dictionary gleaned from data, without any hu-
man intervention. These facts indicate that careful
considerations need to be given for effective type-
supervision. In addition, significant manual work
might be required to ensure the quality of lexicons.
To compare type- and token-supervised tagging,
Garrette and Baldridge (2013) performed a set of
experiments by conducting each type of annota-
tion for two hours. They showed that for low-
resource languages, a tag-dictionary can be rea-
sonably effective if label propagation (Talukdar
and Crammer, 2009) and model minimizations
(Ravi and Knight, 2009) are applied to expand and
filter the lexicons. Similar findings were reported
in Garrette et al. (2013).
Do the above findings carry over to the Chi-
nese language? In this paper, we perform an
empirical study on the effects of tag-dictionaries
for domain adaptation of Chinese POS-tagging.
We aim to answer the following research ques-
tions: (a) Is domain adaptation feasible with only
a target-domain lexicon? (b) Can we further im-
prove type-supervised domain adaptation using
unlabeled target-domain sentences? (c) Is craft-
ing a tag dictionary for domain adaptation more
effective than manually annotating target domain
sentences, given similar efforts?
Our investigations are performed under two
Chinese-specific settings. First, unlike low-
resource languages, large amounts of annotation
</bodyText>
<page confidence="0.95717">
588
</page>
<note confidence="0.9930775">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 588–597,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940952380953">
are available for Chinese. For example, the Chi-
nese Treebank (CTB) (Xue et al., 2005) contains
over 50,000 manually tagged news sentences.
Hence rather than studying purely type-supervised
POS-tagging, we make use of CTB as the source
domain, and study domain adaptation to the Inter-
net literature.
Second, one uniqueness of Chinese POS-
tagging, in contrast to the POS-tagging of alpha-
betical languages, is that word segmentation can
be performed jointly to avoid error propagation
(Ng and Low, 2004; Zhang and Clark, 2008; Kru-
engkrai et al., 2009; Zhang and Clark, 2010). We
adopt this approach for a strong baseline. Previous
studies showed that unsupervised domain adap-
tation can give moderate improvements (Liu and
Zhang, 2012). We show that accuracies can be
much more significantly improved by using target-
domain knowledge in the form of lexicons.
Both token-supervised and type-supervised do-
main adaptation rely on a set of source-domain
annotations; while the former makes additional
use of a small set of target annotations, the lat-
ter leverages a target-domain lexicon. We take
a feature-based method, analogous to that of
Daume III (2007), which tunes domain-dependent
versions of features using domain-specific data.
Our method tunes a set of lexicon-based features,
so that domain-dependent models are derived from
inserting domain-specific lexicons.
The conceptually simple method worked highly
effectively on a test set of 1,394 sentences from
the Internet novel “Zhuxian”. Combined with
the use of unlabeled data, a tag lexicon of 3,000
words gave a 33% error reduction when com-
pared with a strong baseline system trained using
CTB data. We observe that joint use of type- and
token-supervised domain adaptation is more cost-
effective than pure type- or token-supervision.
With 10 hours of annotation, the best error reduc-
tion reaches 47%, with F-score increasing from
80.81% to 89.84%.
</bodyText>
<sectionHeader confidence="0.992349" genericHeader="introduction">
2 Baseline
</sectionHeader>
<bodyText confidence="0.999861555555556">
We take as the baseline system a discriminative
joint segmentation and tagging model, proposed
by Zhang and Clark (2010), together with simple
self-training (Liu and Zhang, 2012). While the
baseline discriminative model gives state-of-the-
art joint segmentation and tagging accuracies on
CTB data, the baseline self-training makes use of
unlabeled target domain data to find improved tar-
get domain accuracies over bare CTB training.
</bodyText>
<subsectionHeader confidence="0.996505">
2.1 The Baseline Discriminative Chinese
POS-Tagging Model
</subsectionHeader>
<bodyText confidence="0.999968689655172">
The baseline discriminative model performs
segmentation and POS-tagging simultaneously.
Given an input sentence ci · · · c,,, (cz refers to the
ith character in the sentence), it operates incre-
mentally, from left to right. At each step, the cur-
rent character can either be appended to the last
word of the existing partial output, or seperated as
the start of a new word with tag p. A beam is used
to maintain the N-best partial results at each step
during decoding. At step i (0 ≤ i &lt; n), each
item in the beam corresponds to a segmentation
and POS-tagging hypothesis for the first i−1 char-
acters, with the last word being associated with a
POS, but marked as incomplete. When the next
character cz is processed, it is combined with all
the partial results from the beam to generate new
partial results, using two types of actions: (1) Ap-
pend, which appends cz to the last (partial) word
in a partial result; (2) Separate(p), which makes
the last word in the partial result as completed and
adds cz as a new partial word with a POS tag p.
Partial results in the beam are scored globally
over all actions used to build them, so that the N-
best can be put back to the agenda for the next step.
For each action, features are extracted differently.
We use the features from Zhang and Clark (2010).
Discriminative learning with early-update (Collins
and Roark, 2004; Zhang and Clark, 2011) is used
to train the model with beam-search.
</bodyText>
<subsectionHeader confidence="0.9985435">
2.2 Baseline Unsupervised Adaptation by
Self-Training
</subsectionHeader>
<bodyText confidence="0.999935466666667">
A simple unsupervised approach for POS-tagging
with unlabeled data is EM. For a generative model
such as HMM, EM can locally maximize the like-
lihood of training data. Given a good start, EM
can result in a competitive HMM tagging model
(Goldberg et al., 2008).
For discriminative models with source-domain
training examples, an initial model can be trained
using the source-domain data, and self-training
can be applied to find a locally-optimized model
using raw target domain sentences. The training
process is sometimes associated with the EM al-
gorithm. Liu and Zhang (2012) used perplexities
of character trigrams to order unlabeled sentences,
and applied self-training to achieve a 6.3% error
</bodyText>
<page confidence="0.994372">
589
</page>
<figure confidence="0.999848166666667">
Common
Lexicon
Training Tagging
Training
Source
Lexicon
Source
Corpus
Model
Target
Sentences
Target
Lexicon
Tagging
Common
Lexicon
Tagging
Results
</figure>
<figureCaption confidence="0.999997">
Figure 1: Architecture of our lexicon-based model for domain adaptation.
</figureCaption>
<bodyText confidence="0.995768333333333">
reduction on target-domain data when compared
with source domain training. Their method is sim-
ple to implement, and we take it as our baseline.
</bodyText>
<sectionHeader confidence="0.991946" genericHeader="method">
3 Type-Supervised Domain Adaptation
</sectionHeader>
<bodyText confidence="0.999974447368421">
To give a formal definition of the domain adap-
tation tasks, we denote by C3 a set of anno-
tated source-domain sentences, Ct a set of anno-
tated target-domain sentences, and ,fit an anno-
tated target-domain lexicon. The form of ,,t is a
list of target-domain words, each associated with
a set of POS tags. Token-supervised domain adap-
tation is the task of making use of C3 and Ct to
improve target-domain performances, while type-
supervised domain adaptation is to make use of C3
and ,,t instead for the same purpose.
As described in the introduction, type-
supervised domain adaptation is useful when
annotated sentences are absent, but lexicons are
available. In addition, it is an interesting question
which type of annotation is more cost-effective
when neither is available. We empirically com-
pare the two approaches by proposing a novel
method for type-supervised domain adaptation of
a discriminate tagging model, showing that it can
be a favourable choice in practical situation.
In particular, we split Chinese words into
domain-independent and domain-specific cate-
gories, and define unlexicalized features for
domain-specific words. We train lexicalized
domain-independent and unlexicalized domain-
specific features using the source domain anno-
tated sentences and a source-domain lexicon, and
then apply the resulting model to the target do-
main by replacing the source-domain lexicon with
a target domain lexicon. Combined with unsu-
pervised learning with unlabeled target-domain
of sentences, the conceptually simple method
worked highly effectively. Following Garrette and
Baldridge (2013), we address practical questions
on type-supervised domain adaptation by compar-
ison with token-supervised methods under similar
human annotation efforts.
</bodyText>
<subsectionHeader confidence="0.999453">
3.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.9883922">
Our method is based on the intuition that domain-
specific words of certain types (e.g. proper names)
can behave similarly across domains. For exam-
ple, consider the source-domain sentence “江泽
民|NR (Jiang Zemin) 随后|AD (afterwards) 访
问|VV (visit) 上汽|NR (Shanghai Automobiles
Corp.)” and the target-domain sentence “碧
瑶|NR (Biyao) 随后|AD (afterwards) 来到|VV
(arrive) 大竹峰|NR (the Bamboo Mountains)”.
“江泽民 (Jiang Zemin)” and “碧瑶 (Biyao)” are
person names in the two domains, respectively,
whereas “上汽 (Shanghai Automobiles Corp.)”
and “大竹峰 (the Bamboo Mountains)” are loca-
tion names in the two domains, respectively. If the
four words are simply treated as domain-specific
nouns, the two sentences both have the pattern
“(domain-NR) AD VV (domain-NR)”, and hence
source domain training data can be useful in train-
ing the distributions of the lexicon-based features
for both domains.
Further, we assume that the syntax structures
and the usage of function words do not vary sig-
nificantly across domains. For example, verbs, ad-
jectives or proper nouns can be different from do-
main to domain, but the subject-verb-object sen-
tence structure does not change. In addition, the
usage of closed-set function words remains sta-
ble across different domains. In the CTB tagset,
closed-set POS tags are the vast majority. Under
this assumption, we introduce a set of unlexical-
ized features into the discriminative model, in or-
der to capture the distributions of domain-specific
dictionary words. Unlexicalized features trained
for source domain words can carry over to the tar-
get domain. The overall architecture of our sys-
</bodyText>
<page confidence="0.989664">
590
</page>
<table confidence="0.992134666666667">
Action Lexicon Feature templates
Separate in-lex(w_1), l(w_1) ◦ in-lex(w_1),
in-lex(w_1, t_1), l(w_1) ◦ in-lex(w_1, t_1)
</table>
<tableCaption confidence="0.964093">
Table 1: Dictionary features of the type-
</tableCaption>
<bodyText confidence="0.99733808">
supervised model, where w_1 and t_1 denote the
last word and POS tag of a partial result, re-
spectively; l(w) denotes the length of the word
w; in-lex(w, t) denotes whether the word-tag pair
(w, t) is in the lexicon.
tem is shown in Figure 1, where lexicons can be
treated as “plugins” to the model for different do-
mains, and one model trained from the source do-
main can be applied to many different target do-
mains, as long as a lexicon is available.
The method can be the most effective
when there is a significant amount of domain-
independent words in the data, which provide rich
lexicalized contexts for estimating unlexicalized
features for domain-specific words. For scientific
domains (e.g. the biomedical domain) which
share a significant proportion of common words
with the news domain, and have most domain
specific words being nouns (e.g. “糖尿病 (dia-
betes)”), the method can be the most effective.
We choose a comparatively difficult domain pair
(e.g. modern news v.s. ancient style novel),
for which the use of many word types are quite
different. Results on this data can be relatively
more indicative of the usefulness of the method.
</bodyText>
<subsectionHeader confidence="0.998225">
3.2 Lexicon-Based Features
</subsectionHeader>
<bodyText confidence="0.999991111111111">
Table 1 shows the set of new unlexicalized fea-
tures for the domain-specific lexicons. In addition
to words and POS tags, length information is also
encoded in the features, to capture different dis-
tributions of different word sizes. For example,
a one-character word in the dictionary might not
be identified as confidently using the lexicon as a
three-character word in the dictionary.
To acquire a domain-specific lexicon for the
source domain, we use HowNet (Dong and
Dong, 2006) to classify CTB words into domain-
independent and domain-specific categories. Con-
sisting of semantic information for nearly 100,000
common Chinese words, HowNet can serve as a
resource of domain-independent Chinese words.
We choose out of all words in the source domain
training data those that also occur in HowNet for
domain-independent words, and out of the remain-
ing words those that occur more than 3 times for
words specific to the source domain. We assume
that the domain-independent lexicon applies to all
target domains also. For some target domains,
we can obtain domain-specific terminologies eas-
ily from the Internet. However, this can be a very
small portion depending on the domain. Thus, it
may still be necessary to obtain new lexicons by
manual annotation.
</bodyText>
<subsectionHeader confidence="0.999866">
3.3 Lexicon and Self-Training
</subsectionHeader>
<bodyText confidence="0.999911615384615">
The lexicon-based features can be combined with
unsupervised learning to further improve target-
domain accuracies. We apply self-training on top
of the lexicon-based features in the following way:
we train a lexicon-based model M using a lexi-
con 2, of the source domain, and then apply M
together with a target-domain lexicon Zt to auto-
matically label a set of target domain sentences.
We combine the automatically labeled target sen-
tences with the source-domain training data to ob-
tain an extended set of training data, and train a
final model Mself, using the lexicon 2, and 2t for
source- and target-domain data, respectively.
Different numbers of target domain sentences
can be used for self-training. Liu and Zhang
(2012) showed that an increased amount of tar-
get sentences do not constantly lead to improved
development accuracies. They use character per-
plexity to order target domain sentences, taking
the top K sentences for self-training. They eval-
uate the optimal development accuracies using a
range of different Kvalues, and select the best K
for a final model. This method gave better results
than using sentences in the internet novel in their
original order (Liu and Zhang, 2012). We follow
this method in ranking target domain sentences.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.993398">
4.1 Setting
</subsectionHeader>
<bodyText confidence="0.999909181818182">
We use annotated sentences from the CTB5 for
source-domain training, splitting the corpus into
training, development and test sections in the same
way as previous work (Kruengkrai et al., 2009;
Zhang and Clark, 2010; Sun, 2011).
Following Liu and Zhang (2012), we use the
free Internet novel “Zhuxian” (henceforth referred
to as ZX; also known as “Jade dynasty”) as our tar-
get domain data. The writing style of the novel is
in the literature genre, with the style of Ming and
Qing novels, very different from news in CTB. Ex-
</bodyText>
<page confidence="0.9826">
591
</page>
<table confidence="0.999899125">
CTB sentences ZX sentences
�石会见俄罗W&amp;quot;r议员团 )� F )E��有,山川灵秀,71-,4;$魔鬼怪。
(Qiaoshi meets the Russian delegates.) r (The world was big. It held everything. There were fascinating
李鹏� 调要RQ1,推行 公7i 员 qpt landscapes. There were haunting ghosts.)
(Lipeng stressed on speke+,diing the reform Q1 间 )E 4; , 我去请 出 诛f 11古Pii
,�offto�fficial regulations.) (No time left. Let me call out Zhuxian, the ancient sword.)
国It学工 �QI1 11对�$ffRy lx, V听%狂笑风起,法宝4光闪M。(There came suddenly
(Chinese chemistry industry increases the pace of opening up.) a gust of wind, out of which was laughters and magic flashes.)
</table>
<tableCaption confidence="0.951132">
Table 2: Example sentences from CTB and ZX to illustrate the differences between news and novel.
</tableCaption>
<table confidence="0.999973777777778">
Data Set Chap. IDs # sents # words
Train 1-270, 400-931, 10,086 493,930
CTB5 1001-1151
Devel 301-325 350 6,821
Test 271-300 348 8,008
Train 6.6-6.10, 2,373 67,648
ZX 7.6-7.10, 19
Devel 6.1-6.5 788 20,393
Test 7.1-7.5 1,394 34,355
</table>
<tableCaption confidence="0.999895">
Table 3: Corpus statistics.
</tableCaption>
<bodyText confidence="0.999940961538462">
ample sentences from the two corpora are shown
in Table 2. Liu and Zhang (2012) manually anno-
tated 385 sentences as development and test data,
which we download from their website.1 These
data follow the same annotation guidelines as the
Chinese Treebank (Xue et al., 2000).
To gain more reliable statistics in our results,
we extend their annotation work to a total 4,555
sentences, covering the sections 6, 7 and 19 of the
novel. The annotation work is based on the auto-
matically labeled sentences by our baseline model
trained with CTB5 corpus. It took an experienced
native speaker 80 hours, about one minute on av-
erage to annotate one sentence. We use chapters
1-5 of section 6 as the development data, chap-
ters 1-5 of section 7 as the test data, and the re-
maining data for target-domain training,2 in order
to compare type-supervised methods with token-
supervised methods. Under permission from the
author of the novel, we release our annotation for
future reference. Statistics of both the source and
the target domain data are shown in Table 3. The
rest of the novel is treated as unlabeled sentences,
used for type-annotation and self-training.
We perform the standard evaluation, using F-
scores for both the segmentation accuracy and the
</bodyText>
<footnote confidence="0.964165">
1http://faculty.sutd.edu.sg/˜yue zhang/emnlp12yang.zip
2We only use part of the training sentences in our experi-
ments, and the remaining can be used for further research.
</footnote>
<bodyText confidence="0.491626">
overall segmentation and POS tagging accuracy.
</bodyText>
<subsectionHeader confidence="0.959237">
4.2 Baseline Performances
</subsectionHeader>
<bodyText confidence="0.999960111111111">
The baseline discriminative model can achieve
state-of-the-art performances on the CTB5, with
a 97.62% segmentation accuracy and a 93.85% on
overall segmentation and tagging accuracy. Using
the CTB model, the performance on ZX drops sig-
nificantly, to a 87.71% segmentation accuracy and
a 80.81% overall accuracy. Applying self-training,
the segmentation and overall F-scores can be im-
proved to 88.62% and 81.94% respectively.
</bodyText>
<subsectionHeader confidence="0.997787">
4.3 Development Experiments
</subsectionHeader>
<bodyText confidence="0.999982636363636">
In this section, we study type-supervised domain
adaptation by conducting a series of experiments
on the development data, addressing the follow-
ing questions. First, what is the influence of tag-
dictionaries through lexicon-based features? Sec-
ond, what is the effect of type-supervised domain
adaptation in contrast to token-supervised domain
adaptation under the same annotation cost? Third,
what is the interaction between tag-dictionary and
self-training? Finally, what is the combined effect
of type- and token-supervised domain adaptation?
</bodyText>
<subsubsectionHeader confidence="0.737972">
4.3.1 The Influence of The Tag Dictionary
</subsubsectionHeader>
<bodyText confidence="0.999916">
We investigate the effects of two different tag dic-
tionaries. The first dictionary contains names of
characters (e.g. 鬼厉 (Guili)) and artifacts (e.g.
swords such as W龙 (Dragonslayer)) in the novel,
which are obtained from an Internet Encyclope-
dia,3 and requires little human effort. We ex-
tracted 159 words from this page, verified them,
and put them into a tag dictionary. We associate
every word in this tag dictionary with the POS
“NR (proper noun)”, and name the lexicon by NR.
The second dictionary was constructed man-
ually, by first employing our baseline tagger to
tag the unlabeled ZX sentences automatically,
</bodyText>
<footnote confidence="0.958126">
3http://baike.baidu.com/view/18277.htm
</footnote>
<page confidence="0.989149">
592
</page>
<table confidence="0.999532923076923">
Model Target-Domain Cost Supervised +Self-Training
Resources
SEG POS SEG POS ER
Baseline — 0 89.77 82.92 90.35 83.95 6.03
Type-Supervision NR(T) 0 89.84 83.91 91.18 85.22 8.14
3K(T) 5h 91.93 86.53 92.86 87.67 8.46
ORACLE(T) ∞ 93.10 88.87 94.00 89.91 9.34
Token-Supervision 300(S) 5h 92.59 86.86 93.33 87.85 7.53
600(S) 10h 93.19 88.13 93.81 89.01 7.41
900(S) 15h 93.53 88.53 94.15 89.33 6.97
Combined 3K(T) + 300(S) 10h 93.49 88.54 94.00 89.21 5.85
Type-and Token-Supervision
3K(T) + 600(S) 15h 93.98 89.27 94.61 89.87 5.59
</table>
<tableCaption confidence="0.999116">
Table 4: Development test results, where Cost denotes the cost of type- or token-annotation measured
</tableCaption>
<bodyText confidence="0.960810076923077">
by person hours, ER denotes the error reductions of overall performances brought by self-training, T
denotes type-annotation and S denotes token-annotation.
and then randomly selecting the words that are
not domain-independent for an experienced native
speaker to annotate. To facilitate comparison with
token-supervision, we spent about 5 person hours
in annotating 3,000 word-tag pairs, at about the
same cost as annotating 300 sentences. Finally we
conjoined the 3,000 word-tag pairs with the NR
lexicon, and name the resulting lexicon by 3K.
For the target domain, we mark the words from
both NR and 3K as the domain-specific lexicons.
In all experiments, we use the same domain-
independent lexicon, which is extracted from the
source domain training data by HowNet matching.
The accuracies are shown in Table 4, where
the NR lexicon improved the overall F-score
slightly over the baseline, and the larger lexicon
3K brought more significant improvements. These
experiments agree with the intuition that the size
and the coverage of the tag dictionary is impor-
tant to the accuracies. To understand the extent to
which a lexicon can improve the accuracies, we
perform an oracle test, in which lexicons in the
gold-standard test outputs are included in the dic-
tionary. The accuracy is 88.87%.
</bodyText>
<subsectionHeader confidence="0.996224">
4.3.2 Comparing Type-Supervised and
Token-Supervised Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999531111111111">
Table 4 shows that the accuracy improvement by
3,000 annotated word-tag pairs (86.53%) is close
to that by 300 annotated sentences (86.86%). This
suggest that using our method, type-supervised
domain adaptation can be a competitive choice to
the token-supervised methods.
The fact that the token-supervised model gives
slightly better results than our type-annotation
method under similar efforts can probably be ex-
</bodyText>
<figure confidence="0.992372571428571">
Type-Supervision with 3K(T) 1
0.9
0.8
0.7
0.60
.6 0.7 0.8 0.9
Token-Supervision with 300(S)
</figure>
<figureCaption confidence="0.9734805">
Figure 2: Sentence accuracy comparisons for
type- and token-supervision with equal cost.
</figureCaption>
<bodyText confidence="0.9988842">
plained by the nature of domain differences. Texts
in the Internet novel are different with CTB news
in not only the vocabulary, but also POS n-gram
distributions. The latter cannot be transferred from
the source-domain training data directly. Texts
from domains such as modern-style novels and
scientific articles might have more similar POS
distributions to the CTB data, and can potentially
benefit more from pure lexicons. We leave the ver-
ification of this intuition to future work.
</bodyText>
<subsectionHeader confidence="0.918939">
4.3.3 Making Use of Unlabeled Sentences
</subsectionHeader>
<bodyText confidence="0.999767428571429">
Both type- and token-supervised domain adapta-
tion methods can be further improved via unla-
beled target sentences. We apply self-training to
both methods, and find improved results across the
board in Table 4. The results indicate that unla-
beled data is useful in further improving both type-
and token-supervised domain adaptation.
</bodyText>
<page confidence="0.99763">
593
</page>
<bodyText confidence="0.999984">
Interestingly, the effects of the two methods
on self-training are slightly different. The er-
ror reduction by self-training improves from 6.0%
(baseline) to averaged 7.3% and 8.6% for token-
and type-supervised adaptation, respectively. The
better effect for the type-supervised method may
result from comparatively more uniform cover-
age of the lexicon on sentences, since the target-
domain lexicon is annotated by selecting words
from much more than 300 sentences.
</bodyText>
<sectionHeader confidence="0.5891105" genericHeader="method">
4.3.4 Combined Model of Type- and
Token-Supervision
</sectionHeader>
<bodyText confidence="0.999682516129032">
Figure 2 shows the F-scores of each development
test sentence by type- and token-supervised do-
main adaptation with 5 person hours, respectively.
It indicates that the two methods make different
types of errors, and can potentially be used jointly
for better improvements. We conduct a set of ex-
periments as shown in Table 4, finding that the
combined type- and token-supervised model with
lexicon 3K and 300 labeled sentences achieves
an overall accuracy of 88.54%, exceeding the ac-
curacies of both the type-supervised model with
lexicon 3K and the token-supervised model with
300 labeled sentences. Similar observation can
be found for the combined model with lexicon 3K
and 600 labeled sentences. If combined with self-
training, the same fact can be observed.
More interestingly, the combined model also
exceeds pure type- and token-supervised mod-
els with the same annotation cost. For exam-
ple, the combined model with 3K and 300 la-
beled sentences gives a better accuracy than the
token-supervised model with 600 sentences, with
or without self-training. Similar observations hold
between the combined model with 3K and 600 la-
beled sentences and the token-supervised model
with 900 sentences. The results suggest that the
most cost-effective approach for domain adapta-
tion can be combined type- and token-supervision:
after annotating a set of raw sentences, one could
stop to annotate some words, rather than continu-
ing sentence annotation.
</bodyText>
<subsectionHeader confidence="0.998636">
4.4 Final Results
</subsectionHeader>
<bodyText confidence="0.9995965">
Table 5 shows the final results on test corpus
within ten person hours’ annotation. With five per-
son hours (lexicon 3K), the type-supervised model
gave an error reduction of 32.99% compared with
the baseline. The best result was obtained by the
combined type- and token-supervised model, with
</bodyText>
<table confidence="0.9990460625">
SEG POS ER Time
Baseline 87.71 80.81 0.00 0
Baseline+Self-Training 88.62 81.94 5.89 0
Type-Supervision
NR(T) 88.34 82.54 9.02 0
NR(T)+ Self-Training 89.52 83.93 16.26 0
3K(T) 91.11 86.04 27.25 5h
3K(T)+Self-Training 92.11 87.14 32.99 5h
Token-Supervision
300(S) 92.44 86.87 31.58 5h
300(S)+Self-Training 93.24 87.48 34.76 5h
600(S) 93.09 88.05 37.73 10h
600(S)+Self-Training 93.77 88.78 41.53 10h
Combined Type- and Token-Supervision
3K(T)+300(S) 93.27 89.03 42.83 10h
3K(T)+300(S)+Self-Training 93.98 89.84 47.06 10h
</table>
<tableCaption confidence="0.91398">
Table 5: Final results on test set within ten per-
</tableCaption>
<bodyText confidence="0.994211785714286">
son hours’ annotation, where ER denotes the over-
all error reductions compared with the baseline
model, Time denotes the cost of type- or token-
annotation measured by person hours, T denotes
type-annotation and S denotes token-annotation.
an error reduction of 47.06%, higher than that the
token-supervised model with the same cost under
the same setting (the model of 600 labeled sen-
tences with an error reduction of 41.53%). The
results confirm that the type-supervised model
is a competitive alternative for joint segmenta-
tion and POS-tagging under the cross-domain set-
ting. Combined type- and token-supervised model
yields better results than single models.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999718058823529">
As mentioned in the introduction, tag dictionaries
have been applied to type-supervised POS tagging
of English (Toutanova and Johnson, 2007; Gold-
water and Griffiths, 2007; Ravi and Knight, 2009;
Garrette and Baldridge, 2012), Hebrew (Goldberg
et al., 2008), Kinyarwanda and Malagasy (Gar-
rette and Baldridge, 2013; Garrette et al., 2013),
and other languages (T¨ackstr¨om et al., 2013).
These methods assume that lexicon can be ob-
tained by manual annotation or semi-supervised
learning, and use the lexicon to induce tag se-
quences on unlabeled sentences. We study type-
supervised Chinese POS-tagging, but under the
setting of domain adaptation. The problem is
how to leverage a target domain lexicon and an
available annotated resources in a different source
domain to improving POS-tagging. Consistent
</bodyText>
<page confidence="0.996276">
594
</page>
<bodyText confidence="0.999739652777778">
with Garrette et al. (2013), we also find that the
type-supervised method is a competitive choice to
token-supervised adaptation.
There has been a line of work on using graph-
based label propagation to expand tag-lexicons for
POS-tagging (Subramanya et al., 2010; Das and
Petrov, 2011). Similar methods have been ap-
plied to character-level Chinese tagging (Zeng et
al., 2013). We found that label propagation from
neither the source domain nor auto-labeled target
domain sentences can improve domain adaptation.
The main reason could be significant domain dif-
ferences. Due to space limitations, we omit this
negative result in our experiments.
With respect to domain adaptation, existing
methods can be classified into three categories.
The first category does not explicitly model dif-
ferences between the source and target domains,
but use standard semi-supervised learning meth-
ods with labeled source domain data and unla-
beled target domain data (Dai et al., 2007; Raina
et al., 2007). The baseline self-training ap-
proach (Liu and Zhang, 2012) belongs to this cat-
egory. The second considers the differences in the
two domains in terms of features (Blitzer et al.,
2006; Daume III, 2007), classifying features into
domain-independent source domain and target do-
main groups and training these types consistently.
The third considers differences between the dis-
tributions of instances in the two domains, treat-
ing them differently (Jiang and Zhai, 2007). Our
type-supervised method is closer to the second cat-
egory. However, rather than splitting features into
domain-independent and domain-specific types,
we use domain-specific dictionaries to capture do-
main differences, and train a model on the source
domain only. Our method can be treated as an ap-
proach specific to the POS-tagging task.
With respect to Chinese lexical analysis, lit-
tle previous work has been reported on using a
tag dictionary to improve joint segmentation and
POS-tagging. There has been work on using a
lexicon in improving segmentation in a Chinese
analysis pipeline. Peng et al. (2004) used fea-
tures from a set of Chinese words and characters
to improve CRF-based segmentation; Low et al.
(2005) extracted features based on a Chinese lex-
icon from Peking University to help a maximum
segmentor; Sun (2011) collected 12,992 idioms
from Chinese dictionaries, and used them for rule-
based pre-segmentation; Hatori et al. (2012) col-
lected Chinese words from HowNet and the Chi-
nese Wikipedia to enhance segmentation accura-
cies of their joint dependency parsing systems. In
comparison with their work, our lexicon contain
additional POS information, and are used for word
segmentation and POS-tagging simultaneously. In
addition, we separate domain-dependent lexicons
for the source and target lexicons, and use a novel
framework to perform domain adaptation.
Wang et al. (2011) collect word-tag statistics
from automatically labeled texts, and use them as
features to improve POS-tagging. Their word-tag
statistics can be treated as a type of lexicon. How-
ever, their efforts differ from ours in several as-
pects: (1) they focus on in-domain POS-tagging,
while our concern is cross-domain tagging; (2)
they study POS-tagging on segmented sentences,
while we investigate joint segmentation and POS-
tagging for Chinese; (3) their tag-dictionaries are
not tag-dictionaries literally, but statistics of word-
tag associations.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999942166666667">
We performed an empirical study on the use of
tag-dictionaries for the domain adaptation of joint
Chinese segmentation and POS-tagging, showing
that type-supervised methods can be a compet-
itive alternative to token-supervised methods
in cost-effectiveness. In addition, combination
of the two methods gives the best cost-effect.
Finally, we release our annotation of over 4,000
sentences in the Internet literature domain on-
line at http://faculty.sutd.edu.sg/
˜yue_zhang/eacl14meishan.zip as a
free resource for Chinese POS-tagging.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998208333333333">
We thank the anonymous reviewers for their con-
structive comments. We gratefully acknowl-
edge the support of the National Key Basic
Research Program (973 Program) of China via
Grant 2014CB340503 and the National Natural
Science Foundation of China (NSFC) via Grant
61133012 and 61370164, the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the Singaporean Ministration of
Education Tier 2 grant T2MOE201301 and SRG
ISTD 2012 038 from Singapore University of
Technology and Design.
</bodyText>
<page confidence="0.995502">
595
</page>
<bodyText confidence="0.914131666666667">
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 583–592, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
</bodyText>
<note confidence="0.523197">
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In COLING.
</note>
<reference confidence="0.999511215686274">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120–128, Sydney, Australia, July.
Association for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised POS induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575–584, Cambridge, MA, October. Association for
Computational Linguistics.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain, July.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring Naive Bayes Classifiers for
Text Classification. In AAAI, pages 540–545.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600–609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Zhendong Dong and Qiang Dong. 2006. Hownet And
the Computation of Meaning. World Scientific Pub-
lishing Co., Inc., River Edge, NJ, USA.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markov models for part-of-speech
tagging with incomplete tag dictionaries. In
EMNLP-CoNLL, pages 821–831.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 138–147, Atlanta, Georgia, June. Association
for Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08: HLT, pages 746–754, Columbus, Ohio, June.
Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744–751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1045–
1053, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513–521,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745–754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161–164.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Proceeding of CICLing’11.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2).
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277–284, Barcelona, Spain, July. Association
for Computational Linguistics.
</reference>
<page confidence="0.985325">
596
</page>
<reference confidence="0.99977775">
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceedings
of Coling 2004, pages 562–568, Geneva, Switzer-
land, Aug 23–Aug 27. COLING.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y. Ng. 2007. Self-taught learn-
ing: transfer learning from unlabeled data. In ICML,
pages 759–766.
Sujith Ravi and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
ACL/IJCNLP, pages 504–512.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
167–176, Cambridge, MA, October. Association for
Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385–
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, McDon-
ald Ryan, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging.
In Transactions of the ACL. Association for Compu-
tational Linguistics, March.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In ECML/PKDD (2), pages 442–457.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In NIPS.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309–317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Nianwen Xue, Fei Xia, Shizhe Huang, and Tony Kroch.
2000. The bracketing guidelines for the chinese
treebank. Technical report, University of Pennsyl-
vania.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint chinese word segmentation and part-
of-speech tagging. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 770–
779, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, pages 888–
896, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843–852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
</reference>
<page confidence="0.99761">
597
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.242516">
<title confidence="0.984897">Type-Supervised Domain Adaptation for Joint Segmentation POS-Tagging</title>
<author confidence="0.95983">Yue Wanxiang Ting</author>
<affiliation confidence="0.8939745">Center for Social Computing and Information Harbin Institute of Technology, car, University of Technology and</affiliation>
<abstract confidence="0.97704628">yue zhang@sutd.edu.sg Abstract We report an empirical investigation on type-supervised domain adaptation for joint Chinese word segmentation and POS-tagging, making use of domainspecific tag dictionaries and only unlabeled target domain data to improve target-domain accuracies, given a set of annotated source domain sentences. Previous work on POS-tagging of other languages showed that type-supervision can be a competitive alternative to tokensupervision, while semi-supervised techniques such as label propagation are important to the effectiveness of typesupervision. We report similar findings using a novel approach for joint Chinese segmentation and POS-tagging, under a cross-domain setting. With the help of unlabeled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined typeand token-supervision can lead to improved cost-effectiveness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="29816" citStr="Blitzer et al., 2006" startWordPosition="4644" endWordPosition="4647">omain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and domain-specific types, we use domain-specific dictionaries to capture domain differences, and train a model on the source domain only. Our method can be treated as an approach specific to</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Christodoulopoulos</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Two decades of unsupervised POS induction: How far have we come?</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>575--584</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1488" citStr="Christodoulopoulos et al., 2010" startWordPosition="208" endWordPosition="211">ctiveness of typesupervision. We report similar findings using a novel approach for joint Chinese segmentation and POS-tagging, under a cross-domain setting. With the help of unlabeled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined type- and token-supervision can lead to improved cost-effectiveness. 1 Introduction With accuracies of over 97%, POS-tagging of WSJ can be treated as a solved problem (Manning, 2011). However, performance is still well below satisfactory for many other languages and domains (Petrov et al., 2012; Christodoulopoulos et al., 2010). There has been a line of research on using a tag-dictionary for POS-tagging (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. However, success in type-supervised POStagging turns out to depend on several subtle factors. For example, recent research has found that the quality of</context>
</contexts>
<marker>Christodoulopoulos, Goldwater, Steedman, 2010</marker>
<rawString>Christos Christodoulopoulos, Sharon Goldwater, and Mark Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 575–584, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7727" citStr="Collins and Roark, 2004" startWordPosition="1201" endWordPosition="1204"> the partial results from the beam to generate new partial results, using two types of actions: (1) Append, which appends cz to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds cz as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domai</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenyuan Dai</author>
<author>Gui-Rong Xue</author>
<author>Qiang Yang</author>
<author>Yong Yu</author>
</authors>
<title>Transferring Naive Bayes Classifiers for Text Classification. In</title>
<date>2007</date>
<booktitle>AAAI,</booktitle>
<pages>540--545</pages>
<contexts>
<context position="29611" citStr="Dai et al., 2007" startWordPosition="4609" endWordPosition="4612">agging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domai</context>
</contexts>
<marker>Dai, Xue, Yang, Yu, 2007</marker>
<rawString>Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu. 2007. Transferring Naive Bayes Classifiers for Text Classification. In AAAI, pages 540–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graphbased projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="28931" citStr="Das and Petrov, 2011" startWordPosition="4505" endWordPosition="4508">ervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Consistent 594 with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods wi</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graphbased projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600–609, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<title>Hownet And the Computation of Meaning.</title>
<date>2006</date>
<publisher>World Scientific Publishing Co., Inc.,</publisher>
<location>River Edge, NJ, USA.</location>
<contexts>
<context position="14142" citStr="Dong and Dong, 2006" startWordPosition="2199" endWordPosition="2202">s are quite different. Results on this data can be relatively more indicative of the usefulness of the method. 3.2 Lexicon-Based Features Table 1 shows the set of new unlexicalized features for the domain-specific lexicons. In addition to words and POS tags, length information is also encoded in the features, to capture different distributions of different word sizes. For example, a one-character word in the dictionary might not be identified as confidently using the lexicon as a three-character word in the dictionary. To acquire a domain-specific lexicon for the source domain, we use HowNet (Dong and Dong, 2006) to classify CTB words into domainindependent and domain-specific categories. Consisting of semantic information for nearly 100,000 common Chinese words, HowNet can serve as a resource of domain-independent Chinese words. We choose out of all words in the source domain training data those that also occur in HowNet for domain-independent words, and out of the remaining words those that occur more than 3 times for words specific to the source domain. We assume that the domain-independent lexicon applies to all target domains also. For some target domains, we can obtain domain-specific terminolog</context>
</contexts>
<marker>Dong, Dong, 2006</marker>
<rawString>Zhendong Dong and Qiang Dong. 2006. Hownet And the Computation of Meaning. World Scientific Publishing Co., Inc., River Edge, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Typesupervised hidden markov models for part-of-speech tagging with incomplete tag dictionaries. In EMNLP-CoNLL,</title>
<date>2012</date>
<pages>821--831</pages>
<contexts>
<context position="1664" citStr="Garrette and Baldridge, 2012" startWordPosition="235" endWordPosition="238">beled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined type- and token-supervision can lead to improved cost-effectiveness. 1 Introduction With accuracies of over 97%, POS-tagging of WSJ can be treated as a solved problem (Manning, 2011). However, performance is still well below satisfactory for many other languages and domains (Petrov et al., 2012; Christodoulopoulos et al., 2010). There has been a line of research on using a tag-dictionary for POS-tagging (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. However, success in type-supervised POStagging turns out to depend on several subtle factors. For example, recent research has found that the quality of the tag-dictionary is crucial to the success of such methods (Banko and Moore, 2004; Goldberg et al., 2008; Garrette and Baldridge, 2012). Banko and Moore (2004) found that th</context>
<context position="28066" citStr="Garrette and Baldridge, 2012" startWordPosition="4371" endWordPosition="4374">her than that the token-supervised model with the same cost under the same setting (the model of 600 labeled sentences with an error reduction of 41.53%). The results confirm that the type-supervised model is a competitive alternative for joint segmentation and POS-tagging under the cross-domain setting. Combined type- and token-supervised model yields better results than single models. 5 Related Work As mentioned in the introduction, tag dictionaries have been applied to type-supervised POS tagging of English (Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012), Hebrew (Goldberg et al., 2008), Kinyarwanda and Malagasy (Garrette and Baldridge, 2013; Garrette et al., 2013), and other languages (T¨ackstr¨om et al., 2013). These methods assume that lexicon can be obtained by manual annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Consistent 594 with Garrette et al</context>
</contexts>
<marker>Garrette, Baldridge, 2012</marker>
<rawString>Dan Garrette and Jason Baldridge. 2012. Typesupervised hidden markov models for part-of-speech tagging with incomplete tag dictionaries. In EMNLP-CoNLL, pages 821–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>138--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2689" citStr="Garrette and Baldridge (2013)" startWordPosition="398" endWordPosition="401">as found that the quality of the tag-dictionary is crucial to the success of such methods (Banko and Moore, 2004; Goldberg et al., 2008; Garrette and Baldridge, 2012). Banko and Moore (2004) found that the accuracies can drop from 96% to 77% when a hand-crafted tag dictionary is replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research</context>
<context position="10545" citStr="Garrette and Baldridge (2013)" startWordPosition="1629" endWordPosition="1632">ctical situation. In particular, we split Chinese words into domain-independent and domain-specific categories, and define unlexicalized features for domain-specific words. We train lexicalized domain-independent and unlexicalized domainspecific features using the source domain annotated sentences and a source-domain lexicon, and then apply the resulting model to the target domain by replacing the source-domain lexicon with a target domain lexicon. Combined with unsupervised learning with unlabeled target-domain of sentences, the conceptually simple method worked highly effectively. Following Garrette and Baldridge (2013), we address practical questions on type-supervised domain adaptation by comparison with token-supervised methods under similar human annotation efforts. 3.1 System Architecture Our method is based on the intuition that domainspecific words of certain types (e.g. proper names) can behave similarly across domains. For example, consider the source-domain sentence “江泽 民|NR (Jiang Zemin) 随后|AD (afterwards) 访 问|VV (visit) 上汽|NR (Shanghai Automobiles Corp.)” and the target-domain sentence “碧 瑶|NR (Biyao) 随后|AD (afterwards) 来到|VV (arrive) 大竹峰|NR (the Bamboo Mountains)”. “江泽民 (Jiang Zemin)” and “碧瑶 (B</context>
<context position="28154" citStr="Garrette and Baldridge, 2013" startWordPosition="4383" endWordPosition="4387">odel of 600 labeled sentences with an error reduction of 41.53%). The results confirm that the type-supervised model is a competitive alternative for joint segmentation and POS-tagging under the cross-domain setting. Combined type- and token-supervised model yields better results than single models. 5 Related Work As mentioned in the introduction, tag dictionaries have been applied to type-supervised POS tagging of English (Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012), Hebrew (Goldberg et al., 2008), Kinyarwanda and Malagasy (Garrette and Baldridge, 2013; Garrette et al., 2013), and other languages (T¨ackstr¨om et al., 2013). These methods assume that lexicon can be obtained by manual annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Consistent 594 with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-</context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 138–147, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Mielens</author>
<author>Jason Baldridge</author>
</authors>
<title>Real-world semi-supervised learning of postaggers for low-resource languages.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association</booktitle>
<contexts>
<context position="3064" citStr="Garrette et al. (2013)" startWordPosition="458" endWordPosition="461">cate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually annotating target domain sentences, given similar efforts? Our investigations are performed under two Chines</context>
<context position="28178" citStr="Garrette et al., 2013" startWordPosition="4388" endWordPosition="4391">with an error reduction of 41.53%). The results confirm that the type-supervised model is a competitive alternative for joint segmentation and POS-tagging under the cross-domain setting. Combined type- and token-supervised model yields better results than single models. 5 Related Work As mentioned in the introduction, tag dictionaries have been applied to type-supervised POS tagging of English (Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012), Hebrew (Goldberg et al., 2008), Kinyarwanda and Malagasy (Garrette and Baldridge, 2013; Garrette et al., 2013), and other languages (T¨ackstr¨om et al., 2013). These methods assume that lexicon can be obtained by manual annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Consistent 594 with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. T</context>
</contexts>
<marker>Garrette, Mielens, Baldridge, 2013</marker>
<rawString>Dan Garrette, Jason Mielens, and Jason Baldridge. 2013. Real-world semi-supervised learning of postaggers for low-resource languages. In Proceedings of the 51st Annual Meeting of the Association</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>746--754</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2195" citStr="Goldberg et al., 2008" startWordPosition="322" endWordPosition="325">o, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. However, success in type-supervised POStagging turns out to depend on several subtle factors. For example, recent research has found that the quality of the tag-dictionary is crucial to the success of such methods (Banko and Moore, 2004; Goldberg et al., 2008; Garrette and Baldridge, 2012). Banko and Moore (2004) found that the accuracies can drop from 96% to 77% when a hand-crafted tag dictionary is replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for </context>
<context position="8110" citStr="Goldberg et al., 2008" startWordPosition="1264" endWordPosition="1267">em, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domain sentences. The training process is sometimes associated with the EM algorithm. Liu and Zhang (2012) used perplexities of character trigrams to order unlabeled sentences, and applied self-training to achieve a 6.3% error 589 Common Lexicon Training Tagging Training Source Lexicon Source Corpus Model Target Sentences Target Lexicon Tagging Common Lexicon Tagging Results Figure 1: </context>
<context position="28098" citStr="Goldberg et al., 2008" startWordPosition="4376" endWordPosition="4379">l with the same cost under the same setting (the model of 600 labeled sentences with an error reduction of 41.53%). The results confirm that the type-supervised model is a competitive alternative for joint segmentation and POS-tagging under the cross-domain setting. Combined type- and token-supervised model yields better results than single models. 5 Related Work As mentioned in the introduction, tag dictionaries have been applied to type-supervised POS tagging of English (Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012), Hebrew (Goldberg et al., 2008), Kinyarwanda and Malagasy (Garrette and Baldridge, 2013; Garrette et al., 2013), and other languages (T¨ackstr¨om et al., 2013). These methods assume that lexicon can be obtained by manual annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Consistent 594 with Garrette et al. (2013), we also find that the </context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings of ACL08: HLT, pages 746–754, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>744--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28012" citStr="Goldwater and Griffiths, 2007" startWordPosition="4362" endWordPosition="4366">es token-annotation. an error reduction of 47.06%, higher than that the token-supervised model with the same cost under the same setting (the model of 600 labeled sentences with an error reduction of 41.53%). The results confirm that the type-supervised model is a competitive alternative for joint segmentation and POS-tagging under the cross-domain setting. Combined type- and token-supervised model yields better results than single models. 5 Related Work As mentioned in the introduction, tag dictionaries have been applied to type-supervised POS tagging of English (Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012), Hebrew (Goldberg et al., 2008), Kinyarwanda and Malagasy (Garrette and Baldridge, 2013; Garrette et al., 2013), and other languages (T¨ackstr¨om et al., 2013). These methods assume that lexicon can be obtained by manual annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to imp</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1045--1053</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="31045" citStr="Hatori et al. (2012)" startWordPosition="4835" endWordPosition="4838">tagging task. With respect to Chinese lexical analysis, little previous work has been reported on using a tag dictionary to improve joint segmentation and POS-tagging. There has been work on using a lexicon in improving segmentation in a Chinese analysis pipeline. Peng et al. (2004) used features from a set of Chinese words and characters to improve CRF-based segmentation; Low et al. (2005) extracted features based on a Chinese lexicon from Peking University to help a maximum segmentor; Sun (2011) collected 12,992 idioms from Chinese dictionaries, and used them for rulebased pre-segmentation; Hatori et al. (2012) collected Chinese words from HowNet and the Chinese Wikipedia to enhance segmentation accuracies of their joint dependency parsing systems. In comparison with their work, our lexicon contain additional POS information, and are used for word segmentation and POS-tagging simultaneously. In addition, we separate domain-dependent lexicons for the source and target lexicons, and use a novel framework to perform domain adaptation. Wang et al. (2011) collect word-tag statistics from automatically labeled texts, and use them as features to improve POS-tagging. Their word-tag statistics can be treated</context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2012</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2012. Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1045– 1053, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>513--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4511" citStr="Kruengkrai et al., 2009" startWordPosition="673" endWordPosition="677">, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (200</context>
<context position="16417" citStr="Kruengkrai et al., 2009" startWordPosition="2564" endWordPosition="2567">aracter perplexity to order target domain sentences, taking the top K sentences for self-training. They evaluate the optimal development accuracies using a range of different Kvalues, and select the best K for a final model. This method gave better results than using sentences in the internet novel in their original order (Liu and Zhang, 2012). We follow this method in ranking target domain sentences. 4 Experiments 4.1 Setting We use annotated sentences from the CTB5 for source-domain training, splitting the corpus into training, development and test sections in the same way as previous work (Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Following Liu and Zhang (2012), we use the free Internet novel “Zhuxian” (henceforth referred to as ZX; also known as “Jade dynasty”) as our target domain data. The writing style of the novel is in the literature genre, with the style of Ming and Qing novels, very different from news in CTB. Ex591 CTB sentences ZX sentences �石会见俄罗W&amp;quot;r议员团 )� F )E��有,山川灵秀,71-,4;$魔鬼怪。 (Qiaoshi meets the Russian delegates.) r (The world was big. It held everything. There were fascinating 李鹏� 调要RQ1,推行 公7i 员 qpt landscapes. There were haunting ghosts.) (Lipeng stressed on speke+,d</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yue Zhang</author>
</authors>
<title>Unsupervised domain adaptation for joint segmentation and POS-tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>745--754</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="4695" citStr="Liu and Zhang, 2012" startWordPosition="702" endWordPosition="705">ually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-dependent versions of features using domain-specific data. Our method tunes a set of lexicon-based features, so that domain-dependent models are derived from ins</context>
<context position="6055" citStr="Liu and Zhang, 2012" startWordPosition="912" endWordPosition="915"> novel “Zhuxian”. Combined with the use of unlabeled data, a tag lexicon of 3,000 words gave a 33% error reduction when compared with a strong baseline system trained using CTB data. We observe that joint use of type- and token-supervised domain adaptation is more costeffective than pure type- or token-supervision. With 10 hours of annotation, the best error reduction reaches 47%, with F-score increasing from 80.81% to 89.84%. 2 Baseline We take as the baseline system a discriminative joint segmentation and tagging model, proposed by Zhang and Clark (2010), together with simple self-training (Liu and Zhang, 2012). While the baseline discriminative model gives state-of-theart joint segmentation and tagging accuracies on CTB data, the baseline self-training makes use of unlabeled target domain data to find improved target domain accuracies over bare CTB training. 2.1 The Baseline Discriminative Chinese POS-Tagging Model The baseline discriminative model performs segmentation and POS-tagging simultaneously. Given an input sentence ci · · · c,,, (cz refers to the ith character in the sentence), it operates incrementally, from left to right. At each step, the current character can either be appended to the</context>
<context position="8428" citStr="Liu and Zhang (2012)" startWordPosition="1311" endWordPosition="1314">e Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domain sentences. The training process is sometimes associated with the EM algorithm. Liu and Zhang (2012) used perplexities of character trigrams to order unlabeled sentences, and applied self-training to achieve a 6.3% error 589 Common Lexicon Training Tagging Training Source Lexicon Source Corpus Model Target Sentences Target Lexicon Tagging Common Lexicon Tagging Results Figure 1: Architecture of our lexicon-based model for domain adaptation. reduction on target-domain data when compared with source domain training. Their method is simple to implement, and we take it as our baseline. 3 Type-Supervised Domain Adaptation To give a formal definition of the domain adaptation tasks, we denote by C3</context>
<context position="15671" citStr="Liu and Zhang (2012)" startWordPosition="2445" endWordPosition="2448">uracies. We apply self-training on top of the lexicon-based features in the following way: we train a lexicon-based model M using a lexicon 2, of the source domain, and then apply M together with a target-domain lexicon Zt to automatically label a set of target domain sentences. We combine the automatically labeled target sentences with the source-domain training data to obtain an extended set of training data, and train a final model Mself, using the lexicon 2, and 2t for source- and target-domain data, respectively. Different numbers of target domain sentences can be used for self-training. Liu and Zhang (2012) showed that an increased amount of target sentences do not constantly lead to improved development accuracies. They use character perplexity to order target domain sentences, taking the top K sentences for self-training. They evaluate the optimal development accuracies using a range of different Kvalues, and select the best K for a final model. This method gave better results than using sentences in the internet novel in their original order (Liu and Zhang, 2012). We follow this method in ranking target domain sentences. 4 Experiments 4.1 Setting We use annotated sentences from the CTB5 for s</context>
<context position="17773" citStr="Liu and Zhang (2012)" startWordPosition="2788" endWordPosition="2791">It学工 �QI1 11对�$ffRy lx, V听%狂笑风起,法宝4光闪M。(There came suddenly (Chinese chemistry industry increases the pace of opening up.) a gust of wind, out of which was laughters and magic flashes.) Table 2: Example sentences from CTB and ZX to illustrate the differences between news and novel. Data Set Chap. IDs # sents # words Train 1-270, 400-931, 10,086 493,930 CTB5 1001-1151 Devel 301-325 350 6,821 Test 271-300 348 8,008 Train 6.6-6.10, 2,373 67,648 ZX 7.6-7.10, 19 Devel 6.1-6.5 788 20,393 Test 7.1-7.5 1,394 34,355 Table 3: Corpus statistics. ample sentences from the two corpora are shown in Table 2. Liu and Zhang (2012) manually annotated 385 sentences as development and test data, which we download from their website.1 These data follow the same annotation guidelines as the Chinese Treebank (Xue et al., 2000). To gain more reliable statistics in our results, we extend their annotation work to a total 4,555 sentences, covering the sections 6, 7 and 19 of the novel. The annotation work is based on the automatically labeled sentences by our baseline model trained with CTB5 corpus. It took an experienced native speaker 80 hours, about one minute on average to annotate one sentence. We use chapters 1-5 of sectio</context>
<context position="29691" citStr="Liu and Zhang, 2012" startWordPosition="4622" endWordPosition="4625"> source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and domain-specific types, we use domain-specific dictionaries to </context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Yang Liu and Yue Zhang. 2012. Unsupervised domain adaptation for joint segmentation and POS-tagging. In Proceedings of COLING 2012: Posters, pages 745–754, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="30818" citStr="Low et al. (2005)" startWordPosition="4800" endWordPosition="4803">into domain-independent and domain-specific types, we use domain-specific dictionaries to capture domain differences, and train a model on the source domain only. Our method can be treated as an approach specific to the POS-tagging task. With respect to Chinese lexical analysis, little previous work has been reported on using a tag dictionary to improve joint segmentation and POS-tagging. There has been work on using a lexicon in improving segmentation in a Chinese analysis pipeline. Peng et al. (2004) used features from a set of Chinese words and characters to improve CRF-based segmentation; Low et al. (2005) extracted features based on a Chinese lexicon from Peking University to help a maximum segmentor; Sun (2011) collected 12,992 idioms from Chinese dictionaries, and used them for rulebased pre-segmentation; Hatori et al. (2012) collected Chinese words from HowNet and the Chinese Wikipedia to enhance segmentation accuracies of their joint dependency parsing systems. In comparison with their work, our lexicon contain additional POS information, and are used for word segmentation and POS-tagging simultaneously. In addition, we separate domain-dependent lexicons for the source and target lexicons,</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: is it time for some linguistics?</title>
<date>2011</date>
<booktitle>In Proceeding of CICLing’11.</booktitle>
<contexts>
<context position="1341" citStr="Manning, 2011" startWordPosition="188" endWordPosition="190">competitive alternative to tokensupervision, while semi-supervised techniques such as label propagation are important to the effectiveness of typesupervision. We report similar findings using a novel approach for joint Chinese segmentation and POS-tagging, under a cross-domain setting. With the help of unlabeled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined type- and token-supervision can lead to improved cost-effectiveness. 1 Introduction With accuracies of over 97%, POS-tagging of WSJ can be treated as a solved problem (Manning, 2011). However, performance is still well below satisfactory for many other languages and domains (Petrov et al., 2012; Christodoulopoulos et al., 2010). There has been a line of research on using a tag-dictionary for POS-tagging (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. Howev</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In Proceeding of CICLing’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1581" citStr="Merialdo, 1994" startWordPosition="225" endWordPosition="226">and POS-tagging, under a cross-domain setting. With the help of unlabeled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined type- and token-supervision can lead to improved cost-effectiveness. 1 Introduction With accuracies of over 97%, POS-tagging of WSJ can be treated as a solved problem (Manning, 2011). However, performance is still well below satisfactory for many other languages and domains (Petrov et al., 2012; Christodoulopoulos et al., 2010). There has been a line of research on using a tag-dictionary for POS-tagging (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. However, success in type-supervised POStagging turns out to depend on several subtle factors. For example, recent research has found that the quality of the tag-dictionary is crucial to the success of such methods (Banko and Moore, 2004; Goldber</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging english text with a probabilistic model. Computational Linguistics, 20(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>277--284</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4463" citStr="Ng and Low, 2004" startWordPosition="665" endWordPosition="668">al Linguistics, pages 588–597, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-b</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 277–284, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>562--568</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="30708" citStr="Peng et al. (2004)" startWordPosition="4781" endWordPosition="4784">i, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and domain-specific types, we use domain-specific dictionaries to capture domain differences, and train a model on the source domain only. Our method can be treated as an approach specific to the POS-tagging task. With respect to Chinese lexical analysis, little previous work has been reported on using a tag dictionary to improve joint segmentation and POS-tagging. There has been work on using a lexicon in improving segmentation in a Chinese analysis pipeline. Peng et al. (2004) used features from a set of Chinese words and characters to improve CRF-based segmentation; Low et al. (2005) extracted features based on a Chinese lexicon from Peking University to help a maximum segmentor; Sun (2011) collected 12,992 idioms from Chinese dictionaries, and used them for rulebased pre-segmentation; Hatori et al. (2012) collected Chinese words from HowNet and the Chinese Wikipedia to enhance segmentation accuracies of their joint dependency parsing systems. In comparison with their work, our lexicon contain additional POS information, and are used for word segmentation and POS-</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of Coling 2004, pages 562–568, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC,</booktitle>
<contexts>
<context position="1454" citStr="Petrov et al., 2012" startWordPosition="204" endWordPosition="207">important to the effectiveness of typesupervision. We report similar findings using a novel approach for joint Chinese segmentation and POS-tagging, under a cross-domain setting. With the help of unlabeled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined type- and token-supervision can lead to improved cost-effectiveness. 1 Introduction With accuracies of over 97%, POS-tagging of WSJ can be treated as a solved problem (Manning, 2011). However, performance is still well below satisfactory for many other languages and domains (Petrov et al., 2012; Christodoulopoulos et al., 2010). There has been a line of research on using a tag-dictionary for POS-tagging (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. However, success in type-supervised POStagging turns out to depend on several subtle factors. For example, recent rese</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Alexis Battle</author>
<author>Honglak Lee</author>
<author>Benjamin Packer</author>
<author>Andrew Y Ng</author>
</authors>
<title>Self-taught learning: transfer learning from unlabeled data. In</title>
<date>2007</date>
<booktitle>ICML,</booktitle>
<pages>759--766</pages>
<contexts>
<context position="29632" citStr="Raina et al., 2007" startWordPosition="4613" endWordPosition="4616">., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and dom</context>
</contexts>
<marker>Raina, Battle, Lee, Packer, Ng, 2007</marker>
<rawString>Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y. Ng. 2007. Self-taught learning: transfer learning from unlabeled data. In ICML, pages 759–766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In ACL/IJCNLP,</booktitle>
<pages>504--512</pages>
<contexts>
<context position="1633" citStr="Ravi and Knight, 2009" startWordPosition="231" endWordPosition="234">. With the help of unlabeled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined type- and token-supervision can lead to improved cost-effectiveness. 1 Introduction With accuracies of over 97%, POS-tagging of WSJ can be treated as a solved problem (Manning, 2011). However, performance is still well below satisfactory for many other languages and domains (Petrov et al., 2012; Christodoulopoulos et al., 2010). There has been a line of research on using a tag-dictionary for POS-tagging (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. However, success in type-supervised POStagging turns out to depend on several subtle factors. For example, recent research has found that the quality of the tag-dictionary is crucial to the success of such methods (Banko and Moore, 2004; Goldberg et al., 2008; Garrette and Baldridge, 2012). Banko</context>
<context position="2960" citStr="Ravi and Knight, 2009" startWordPosition="441" endWordPosition="444">s replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually anno</context>
<context position="28035" citStr="Ravi and Knight, 2009" startWordPosition="4367" endWordPosition="4370">eduction of 47.06%, higher than that the token-supervised model with the same cost under the same setting (the model of 600 labeled sentences with an error reduction of 41.53%). The results confirm that the type-supervised model is a competitive alternative for joint segmentation and POS-tagging under the cross-domain setting. Combined type- and token-supervised model yields better results than single models. 5 Related Work As mentioned in the introduction, tag dictionaries have been applied to type-supervised POS tagging of English (Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012), Hebrew (Goldberg et al., 2008), Kinyarwanda and Malagasy (Garrette and Baldridge, 2013; Garrette et al., 2013), and other languages (T¨ackstr¨om et al., 2013). These methods assume that lexicon can be obtained by manual annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Con</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In ACL/IJCNLP, pages 504–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semisupervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>167--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="28908" citStr="Subramanya et al., 2010" startWordPosition="4501" endWordPosition="4504">al annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Consistent 594 with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervi</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semisupervised learning of structured tagging models. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167–176, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1385--1394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="16452" citStr="Sun, 2011" startWordPosition="2572" endWordPosition="2573">ces, taking the top K sentences for self-training. They evaluate the optimal development accuracies using a range of different Kvalues, and select the best K for a final model. This method gave better results than using sentences in the internet novel in their original order (Liu and Zhang, 2012). We follow this method in ranking target domain sentences. 4 Experiments 4.1 Setting We use annotated sentences from the CTB5 for source-domain training, splitting the corpus into training, development and test sections in the same way as previous work (Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Following Liu and Zhang (2012), we use the free Internet novel “Zhuxian” (henceforth referred to as ZX; also known as “Jade dynasty”) as our target domain data. The writing style of the novel is in the literature genre, with the style of Ming and Qing novels, very different from news in CTB. Ex591 CTB sentences ZX sentences �石会见俄罗W&amp;quot;r议员团 )� F )E��有,山川灵秀,71-,4;$魔鬼怪。 (Qiaoshi meets the Russian delegates.) r (The world was big. It held everything. There were fascinating 李鹏� 调要RQ1,推行 公7i 员 qpt landscapes. There were haunting ghosts.) (Lipeng stressed on speke+,diing the reform Q1 间 )E 4; , 我去请 出 </context>
<context position="30927" citStr="Sun (2011)" startWordPosition="4820" endWordPosition="4821">, and train a model on the source domain only. Our method can be treated as an approach specific to the POS-tagging task. With respect to Chinese lexical analysis, little previous work has been reported on using a tag dictionary to improve joint segmentation and POS-tagging. There has been work on using a lexicon in improving segmentation in a Chinese analysis pipeline. Peng et al. (2004) used features from a set of Chinese words and characters to improve CRF-based segmentation; Low et al. (2005) extracted features based on a Chinese lexicon from Peking University to help a maximum segmentor; Sun (2011) collected 12,992 idioms from Chinese dictionaries, and used them for rulebased pre-segmentation; Hatori et al. (2012) collected Chinese words from HowNet and the Chinese Wikipedia to enhance segmentation accuracies of their joint dependency parsing systems. In comparison with their work, our lexicon contain additional POS information, and are used for word segmentation and POS-tagging simultaneously. In addition, we separate domain-dependent lexicons for the source and target lexicons, and use a novel framework to perform domain adaptation. Wang et al. (2011) collect word-tag statistics from </context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1385– 1394, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>McDonald Ryan</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<booktitle>In Transactions of the ACL. Association for Computational Linguistics,</booktitle>
<marker>T¨ackstr¨om, Das, Petrov, Ryan, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, McDonald Ryan, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. In Transactions of the ACL. Association for Computational Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Koby Crammer</author>
</authors>
<title>New regularized algorithms for transductive learning.</title>
<date>2009</date>
<booktitle>In ECML/PKDD (2),</booktitle>
<pages>442--457</pages>
<contexts>
<context position="2912" citStr="Talukdar and Crammer, 2009" startWordPosition="434" endWordPosition="437"> from 96% to 77% when a hand-crafted tag dictionary is replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for dom</context>
</contexts>
<marker>Talukdar, Crammer, 2009</marker>
<rawString>Partha Pratim Talukdar and Koby Crammer. 2009. New regularized algorithms for transductive learning. In ECML/PKDD (2), pages 442–457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A bayesian lda-based model for semi-supervised partof-speech tagging.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1610" citStr="Toutanova and Johnson, 2007" startWordPosition="227" endWordPosition="230"> under a cross-domain setting. With the help of unlabeled sentences and a lexicon of 3,000 words, we obtain 33% error reduction in target-domain tagging. In addition, combined type- and token-supervision can lead to improved cost-effectiveness. 1 Introduction With accuracies of over 97%, POS-tagging of WSJ can be treated as a solved problem (Manning, 2011). However, performance is still well below satisfactory for many other languages and domains (Petrov et al., 2012; Christodoulopoulos et al., 2010). There has been a line of research on using a tag-dictionary for POS-tagging (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012). The idea is compelling: on the one hand, a list of lexicons is often available for special domains, such as bio-informatics; on the other hand, compiling a ∗Corresponding author. lexicon of word-tag pairs appears to be less timeconsuming than annotating full sentences. However, success in type-supervised POStagging turns out to depend on several subtle factors. For example, recent research has found that the quality of the tag-dictionary is crucial to the success of such methods (Banko and Moore, 2004; Goldberg et al., 2008; Garrette and </context>
<context position="27981" citStr="Toutanova and Johnson, 2007" startWordPosition="4358" endWordPosition="4361">s type-annotation and S denotes token-annotation. an error reduction of 47.06%, higher than that the token-supervised model with the same cost under the same setting (the model of 600 labeled sentences with an error reduction of 41.53%). The results confirm that the type-supervised model is a competitive alternative for joint segmentation and POS-tagging under the cross-domain setting. Combined type- and token-supervised model yields better results than single models. 5 Related Work As mentioned in the introduction, tag dictionaries have been applied to type-supervised POS tagging of English (Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Ravi and Knight, 2009; Garrette and Baldridge, 2012), Hebrew (Goldberg et al., 2008), Kinyarwanda and Malagasy (Garrette and Baldridge, 2013; Garrette et al., 2013), and other languages (T¨ackstr¨om et al., 2013). These methods assume that lexicon can be obtained by manual annotation or semi-supervised learning, and use the lexicon to induce tag sequences on unlabeled sentences. We study typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a</context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2007. A bayesian lda-based model for semi-supervised partof-speech tagging. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>309--317</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="31493" citStr="Wang et al. (2011)" startWordPosition="4902" endWordPosition="4905">g University to help a maximum segmentor; Sun (2011) collected 12,992 idioms from Chinese dictionaries, and used them for rulebased pre-segmentation; Hatori et al. (2012) collected Chinese words from HowNet and the Chinese Wikipedia to enhance segmentation accuracies of their joint dependency parsing systems. In comparison with their work, our lexicon contain additional POS information, and are used for word segmentation and POS-tagging simultaneously. In addition, we separate domain-dependent lexicons for the source and target lexicons, and use a novel framework to perform domain adaptation. Wang et al. (2011) collect word-tag statistics from automatically labeled texts, and use them as features to improve POS-tagging. Their word-tag statistics can be treated as a type of lexicon. However, their efforts differ from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. 6 Conclusions We performed an empirical study on the use of</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 309–317, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Shizhe Huang</author>
<author>Tony Kroch</author>
</authors>
<title>The bracketing guidelines for the chinese treebank.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="17967" citStr="Xue et al., 2000" startWordPosition="2819" endWordPosition="2822">ample sentences from CTB and ZX to illustrate the differences between news and novel. Data Set Chap. IDs # sents # words Train 1-270, 400-931, 10,086 493,930 CTB5 1001-1151 Devel 301-325 350 6,821 Test 271-300 348 8,008 Train 6.6-6.10, 2,373 67,648 ZX 7.6-7.10, 19 Devel 6.1-6.5 788 20,393 Test 7.1-7.5 1,394 34,355 Table 3: Corpus statistics. ample sentences from the two corpora are shown in Table 2. Liu and Zhang (2012) manually annotated 385 sentences as development and test data, which we download from their website.1 These data follow the same annotation guidelines as the Chinese Treebank (Xue et al., 2000). To gain more reliable statistics in our results, we extend their annotation work to a total 4,555 sentences, covering the sections 6, 7 and 19 of the novel. The annotation work is based on the automatically labeled sentences by our baseline model trained with CTB5 corpus. It took an experienced native speaker 80 hours, about one minute on average to annotate one sentence. We use chapters 1-5 of section 6 as the development data, chapters 1-5 of section 7 as the test data, and the remaining data for target-domain training,2 in order to compare type-supervised methods with tokensupervised meth</context>
</contexts>
<marker>Xue, Xia, Huang, Kroch, 2000</marker>
<rawString>Nianwen Xue, Fei Xia, Shizhe Huang, and Tony Kroch. 2000. The bracketing guidelines for the chinese treebank. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="4050" citStr="Xue et al., 2005" startWordPosition="601" endWordPosition="604">ing unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually annotating target domain sentences, given similar efforts? Our investigations are performed under two Chinese-specific settings. First, unlike lowresource languages, large amounts of annotation 588 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 588–597, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can giv</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Graph-based semi-supervised model for joint chinese word segmentation and partof-speech tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>770--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="29021" citStr="Zeng et al., 2013" startWordPosition="4519" endWordPosition="4522">y typesupervised Chinese POS-tagging, but under the setting of domain adaptation. The problem is how to leverage a target domain lexicon and an available annotated resources in a different source domain to improving POS-tagging. Consistent 594 with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et</context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Isabel Trancoso. 2013. Graph-based semi-supervised model for joint chinese word segmentation and partof-speech tagging. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 770– 779, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>888--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4486" citStr="Zhang and Clark, 2008" startWordPosition="669" endWordPosition="672">ges 588–597, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of ACL-08: HLT, pages 888– 896, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>843--852</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="4535" citStr="Zhang and Clark, 2010" startWordPosition="678" endWordPosition="681">4. c�2014 Association for Computational Linguistics are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-d</context>
<context position="5997" citStr="Zhang and Clark (2010)" startWordPosition="904" endWordPosition="907">fectively on a test set of 1,394 sentences from the Internet novel “Zhuxian”. Combined with the use of unlabeled data, a tag lexicon of 3,000 words gave a 33% error reduction when compared with a strong baseline system trained using CTB data. We observe that joint use of type- and token-supervised domain adaptation is more costeffective than pure type- or token-supervision. With 10 hours of annotation, the best error reduction reaches 47%, with F-score increasing from 80.81% to 89.84%. 2 Baseline We take as the baseline system a discriminative joint segmentation and tagging model, proposed by Zhang and Clark (2010), together with simple self-training (Liu and Zhang, 2012). While the baseline discriminative model gives state-of-theart joint segmentation and tagging accuracies on CTB data, the baseline self-training makes use of unlabeled target domain data to find improved target domain accuracies over bare CTB training. 2.1 The Baseline Discriminative Chinese POS-Tagging Model The baseline discriminative model performs segmentation and POS-tagging simultaneously. Given an input sentence ci · · · c,,, (cz refers to the ith character in the sentence), it operates incrementally, from left to right. At each</context>
<context position="7659" citStr="Zhang and Clark (2010)" startWordPosition="1193" endWordPosition="1196">e. When the next character cz is processed, it is combined with all the partial results from the beam to generate new partial results, using two types of actions: (1) Append, which appends cz to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds cz as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can</context>
<context position="16440" citStr="Zhang and Clark, 2010" startWordPosition="2568" endWordPosition="2571">er target domain sentences, taking the top K sentences for self-training. They evaluate the optimal development accuracies using a range of different Kvalues, and select the best K for a final model. This method gave better results than using sentences in the internet novel in their original order (Liu and Zhang, 2012). We follow this method in ranking target domain sentences. 4 Experiments 4.1 Setting We use annotated sentences from the CTB5 for source-domain training, splitting the corpus into training, development and test sections in the same way as previous work (Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Following Liu and Zhang (2012), we use the free Internet novel “Zhuxian” (henceforth referred to as ZX; also known as “Jade dynasty”) as our target domain data. The writing style of the novel is in the literature genre, with the style of Ming and Qing novels, very different from news in CTB. Ex591 CTB sentences ZX sentences �石会见俄罗W&amp;quot;r议员团 )� F )E��有,山川灵秀,71-,4;$魔鬼怪。 (Qiaoshi meets the Russian delegates.) r (The world was big. It held everything. There were fascinating 李鹏� 调要RQ1,推行 公7i 员 qpt landscapes. There were haunting ghosts.) (Lipeng stressed on speke+,diing the reform Q1 间 )E</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="7751" citStr="Zhang and Clark, 2011" startWordPosition="1205" endWordPosition="1208"> the beam to generate new partial results, using two types of actions: (1) Append, which appends cz to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds cz as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domain sentences. The trainin</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>