<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998774">
A Large Scale Distributed Syntactic, Semantic and Lexical
Language Model for Machine Translation
</title>
<author confidence="0.972114">
Ming Tan Wenli Zhou Lei Zheng Shaojun Wang
</author>
<affiliation confidence="0.982775666666667">
Kno.e.sis Center
Department of Computer Science and Engineering
Wright State University
</affiliation>
<address confidence="0.64415">
Dayton, OH 45435, USA
</address>
<email confidence="0.999034">
{tan.6,zhou.23,lei.zheng,shaojun.wang}@wright.edu
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998105">
This paper presents an attempt at building
a large scale distributed composite language
model that simultaneously accounts for local
word lexical information, mid-range sentence
syntactic structure, and long-span document
semantic content under a directed Markov ran-
dom field paradigm. The composite language
model has been trained by performing a con-
vergent N-best list approximate EM algorithm
that has linear time complexity and a follow-
up EM algorithm to improve word prediction
power on corpora with up to a billion tokens
and stored on a supercomputer. The large
scale distributed composite language model
gives drastic perplexity reduction over n-
grams and achieves significantly better trans-
lation quality measured by the BLEU score
and “readability” when applied to the task of
re-ranking the N-best list from a state-of-the-
art parsing-based machine translation system.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998655319149">
The Markov chain (n-gram) source models, which
predict each word on the basis of previous n-1
words, have been the workhorses of state-of-the-art
speech recognizers and machine translators that help
to resolve acoustic or foreign language ambiguities
by placing higher probability on more likely original
underlying word strings. Research groups (Brants et
al., 2007; Zhang, 2008) have shown that using an
immense distributed computing paradigm, up to 6-
grams can be trained on up to billions and trillions
of words, yielding consistent system improvements,
but Zhang (2008) did not observe much improve-
ment beyond 6-grams. Although the Markov chains
are efficient at encoding local word interactions, the
n-gram model clearly ignores the rich syntactic and
semantic structures that constrain natural languages.
As the machine translation (MT) working groups
stated on page 3 of their final report (Lavie et al.,
2006), “These approaches have resulted in small im-
provements in MT quality, but have not fundamen-
tally solved the problem. There is a dire need for de-
veloping novel approaches to language modeling.”
Wang et al. (2006) integrated n-gram, structured
language model (SLM) (Chelba and Jelinek, 2000)
and probabilistic latent semantic analysis (PLSA)
(Hofmann, 2001) under the directed MRF frame-
work (Wang et al., 2005) and studied the stochas-
tic properties for the composite language model.
They derived a generalized inside-outside algorithm
to train the composite language model from a gen-
eral EM (Dempster et al., 1977) by following Je-
linek’s ingenious definition of the inside and outside
probabilities for SLM (Jelinek, 2004) with 6th order
of sentence length time complexity. Unfortunately,
there are no experimental results reported.
In this paper, we study the same composite lan-
guage model. Instead of using the 6th order general-
ized inside-outside algorithm proposed in (Wang et
al., 2006), we train this composite model by a con-
vergent N-best list approximate EM algorithm that
has linear time complexity and a follow-up EM al-
gorithm to improve word prediction power. We con-
duct comprehensive experiments on corpora with 44
million tokens, 230 million tokens, and 1.3 billion
tokens and compare perplexity results with n-grams
(n=3,4,5 respectively) on these three corpora, we
obtain drastic perplexity reductions. Finally, we ap-
</bodyText>
<page confidence="0.977809">
201
</page>
<note confidence="0.979483">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 201–210,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997002">
ply our language models to the task of re-ranking
the N-best list from Hiero (Chiang, 2005; Chiang,
2007), a state-of-the-art parsing-based MT system,
we achieve significantly better translation quality
measured by the BLEU score and “readability”.
</bodyText>
<sectionHeader confidence="0.795502" genericHeader="method">
2 Composite language model
</sectionHeader>
<bodyText confidence="0.99987975862069">
The n-gram language model is essentially a word
predictor that given its entire document history it
predicts next word wk+1 based on the last n-1 words
with probability p(wk+1|wkk−n+2) where wkk−n+2 =
wk−n+2, ··· , wk.
The SLM (Chelba and Jelinek, 1998; Chelba and
Jelinek, 2000) uses syntactic information beyond
the regular n-gram models to capture sentence level
long range dependencies. The SLM is based on sta-
tistical parsing techniques that allow syntactic anal-
ysis of sentences; it assigns a probability p(W, T) to
every sentence W and every possible binary parse
T. The terminals of T are the words of W with POS
tags, and the nodes of T are annotated with phrase
headwords and non-terminal labels. Let W be a sen-
tence of length n words to which we have prepended
the sentence beginning marker &lt;s&gt; and appended
the sentence end marker &lt;/s&gt; so that w0 =&lt;s&gt;
and wn+1 =&lt;/s&gt;. Let Wk = w0, · · · , wk be the
word k-prefix of the sentence – the words from the
beginning of the sentence up to the current position
k and WkTk the word-parse k-prefix. A word-parse
k-prefix has a set of exposed heads h−m, · · · , h−1,
with each head being a pair (headword, non-terminal
label), or in the case of a root-only tree (word,
POS tag). An m-th order SLM (m-SLM) has
three operators to generate a sentence: WORD-
PREDICTOR predicts the next word wk+1 based
on the m left-most exposed headwords h−1
</bodyText>
<equation confidence="0.643493">
−m =
h−m, · · · , h−1 in the word-parse k-prefix with prob-
ability p(wk+1|h−1
</equation>
<bodyText confidence="0.998255327868852">
−m), and then passes control to the
TAGGER; the TAGGER predicts the POS tag tk+1
to the next word wk+1 based on the next word wk+1
and the POS tags of the m left-most exposed head-
words h−1
−m in the word-parse k-prefix with prob-
ability p(tk+1|wk+1, h−m.tag, · · · , h−1.tag); the
CONSTRUCTOR builds the partial parse Tk from
Tk−1, wk, and tk in a series of moves ending with
NULL, where a parse move a is made with proba-
bility p(a|h−1
−m); a E A={(unary, NTlabel), (adjoin-
left, NTlabel), (adjoin-right, NTlabel), null}. Once
the CONSTRUCTOR hits NULL, it passes control
to the WORD-PREDICTOR. See detailed descrip-
tion in (Chelba and Jelinek, 2000).
A PLSA model (Hofmann, 2001) is a gener-
ative probabilistic model of word-document co-
occurrences using the bag-of-words assumption de-
scribed as follows: (i) choose a document d with
probability p(d); (ii) SEMANTIZER: select a se-
mantic class g with probability p(g|d); and (iii)
WORD-PREDICTOR: pick a word w with proba-
bility p(w|g). Since only one pair of (d, w) is being
observed, as a result, the joint probability model is
a mixture of log-linear model with the expression
p(d, w) = p(d) Eg p(w|g)p(g|d). Typically, the
number of documents and vocabulary size are much
larger than the size of latent semantic class variables.
Thus, latent semantic class variables function as bot-
tleneck variables to constrain word occurrences in
documents.
When combining n-gram, m order SLM and
PLSA models together to build a composite gen-
erative language model under the directed MRF
paradigm (Wang et al., 2005; Wang et al., 2006),
the TAGGER and CONSTRUCTOR in SLM and
SEMANTIZER in PLSA remain unchanged; how-
ever the WORD-PREDICTORs in n-gram, m-SLM
and PLSA are combined to form a stronger WORD-
PREDICTOR that generates the next word, wk+1,
not only depending on the m left-most exposed
headwords h−1
−m in the word-parse k-prefix but also
its n-gram history wkk−n+2 and its semantic con-
tent gk+1. The parameter for WORD-PREDICTOR
in the composite n-gram/m-SLM/PLSA language
model becomes p(wk+1|wkk−n+2h−1
−mgk+1). The re-
sulting composite language model has an even more
complex dependency structure but with more ex-
pressive power than the original SLM. Figure 1 il-
lustrates the structure of a composite n-gram/m-
SLM/PLSA language model.
The composite n-gram/m-SLM/PLSA lan-
guage model can be formulated as a directed
MRF model (Wang et al., 2006) with lo-
cal normalization constraints for the param-
eters of each model component, WORD-
PREDICTOR, TAGGER, CONSTRUCTOR,
SEMANTIZER, i.e., EwCV p(w|w−1
</bodyText>
<equation confidence="0.969380833333333">
−n+1h−1
−mg) =
1, EtCO p(t|wh−1
−m.tag) = 1, E aCA p(a|h−1
−m) =
1, EgCg p(g|d) = 1.
</equation>
<page confidence="0.997698">
202
</page>
<figureCaption confidence="0.92334075">
Figure 1: A composite n-gram/m-SLM/PLSA language
model where the hidden information is the parse tree
T and semantic content g. The WORD-PREDICTOR
generates the next word wk+1 with probability
</figureCaption>
<equation confidence="0.9398655">
p(wk+1|wk k−n+2h−1
−�gk+1) instead ofp(wk+1|wkk−n+2),
p(wk+1|h−1
−�) and p(wk+1|gk+1) respectively.
</equation>
<sectionHeader confidence="0.982674" genericHeader="method">
3 Training algorithm
</sectionHeader>
<bodyText confidence="0.998898">
Under the composite n-gram/m-SLM/PLSA lan-
guage model, the likelihood of a training corpus D,
a collection of documents, can be written as
</bodyText>
<equation confidence="0.924538">
!!!Pp(Wl,Tl,Gl|d) (1)
</equation>
<bodyText confidence="0.99591225">
where (Wl, Tl, Gl, d) denote the joint sequence of
the lth sentence Wl with its parse tree structure Tl
and semantic annotation string Gl in document d.
This sequence is produced by a unique sequence
of model actions: WORD-PREDICTOR, TAGGER,
CONSTRUCTOR, SEMANTIZER moves, its prob-
ability is obtained by chaining the probabilities of
these moves
</bodyText>
<equation confidence="0.995524">
Pp(Wl,Tl,Gl|d)
0 @P(9|d)#(g,Wl,Gl,d) Y
h−1,··· ,h−
P( P(
I −1 −1 9)#(w-n+1wh-1 g,Wl,Tl,Gl,d)
w w−n+1h
w,w−1,··· ,w−n+1∈V
�(t|wh−1
−m.tag)#(t,wh−1−m.tag,Wl,Tl,d)
m,
)#(a,h−1
−m,Wl,Tl,d)
P(a |h− /
</equation>
<bodyText confidence="0.938864846153846">
where #(g, Wl, Gl, d) is the count of seman-
tic content g in semantic annotation string
Gl of the lth sentence Wl in document d,
#(w−1
−n+1wh−1
−mg, Wl, Tl, Gl, d) is the count
of n-grams, its m most recent exposed headwords
and semantic content g in parse Tl and semantic
annotation string Gl of the lth sentence Wl in
document d, #(twh−1 −mAag, Wl, Tl, d) is the count
of tag t predicted by word w and the tags of m
most recent exposed headwords in parse tree Tl
of the lth sentence Wl in document d, and finally
</bodyText>
<equation confidence="0.787117">
#(ah−1
−m, Wl, Tl, d) is the count of constructor
</equation>
<bodyText confidence="0.98167">
move a conditioning on m exposed headwords h−1
−m
in parse tree Tl of the lth sentence Wl in document
d.
The objective of maximum likelihood estimation
is to maximize the likelihood L(D, p) respect to
model parameters. For a given sentence, its parse
tree and semantic content are hidden and the num-
ber of parse trees grows faster than exponential with
sentence length, Wang et al. (2006) have derived a
generalized inside-outside algorithm by applying the
standard EM algorithm. However, the complexity of
this algorithm is 6th order of sentence length, thus it
is computationally too expensive to be practical for
a large corpus even with the use of pruning on charts
(Jelinek and Chelba, 1999; Jelinek, 2004).
</bodyText>
<subsectionHeader confidence="0.998501">
3.1 N-best list approximate EM
</subsectionHeader>
<bodyText confidence="0.998767666666667">
Similar to SLM (Chelba and Jelinek, 2000), we
adopt an N-best list approximate EM re-estimation
with modular modifications to seamlessly incorpo-
rate the effect of n-gram and PLSA components.
Instead of maximizing the likelihood L(D, p), we
maximize the N-best list likelihood,
</bodyText>
<equation confidence="0.97029025">
L(D, �, T ′ N) = Y Y
d∈D \ l
0111
@ X Pp(Wl,Tl,Gl|d) A A A
</equation>
<subsectionHeader confidence="0.710636">
Tl∈T′lN ,||T′lN ||=N
</subsectionHeader>
<bodyText confidence="0.998410833333333">
where T′lN is a set of N parse trees for sentence Wl
in document d and  ||�  ||denotes the cardinality and
T′N is a collection of T′lN for sentences over entire
corpus D.
The N-best list approximate EM involves two
steps:
</bodyText>
<listItem confidence="0.824945">
1. N-best list search: For each sentence W in doc-
ument d, find N-best parse trees,
</listItem>
<equation confidence="0.8789756">
n X X o
Pp(Wl, Tl,Gl|d),||T′l N ||= N
T ′l
N Gl Tl∈T ′l
N
</equation>
<bodyText confidence="0.974402333333333">
and denote TN as the collection of N-best list
parse trees for sentences over entire corpus D
under model parameter p.
</bodyText>
<listItem confidence="0.9871695">
2. EM update: Perform one iteration (or several
iterations) of EM algorithm to estimate model
</listItem>
<figure confidence="0.9478595">
&lt;s&gt; w1 wi
wk wk+1
......
&lt;/s&gt;
wj
wj+1
w k−n+2.
g1
gi
k
gk+1
d
h−m
h−1
h−2
gj g g g
</figure>
<equation confidence="0.961276260869565">
j+1 k−n+2
Y
L(D, �) =
d∈D
Y
l
X
T l
X
Gl
Y=
g∈G
Y
m∈H
Y
t∈O
aH
max
T ′N
Xmax
T ′lN∈T′N l
G
T N l = arg max
</equation>
<page confidence="0.992812">
203
</page>
<bodyText confidence="0.9639855">
parameters that maximizes N-best-list likeli-
hood of the training corpus D,
</bodyText>
<equation confidence="0.555856">
Pp(Wl,Tl,Gl|d))))
T l∈TlN ∈TN
That is,
</equation>
<listItem confidence="0.448121">
(a) E-step: Compute the auxiliary function of
the N-best-list likelihood
</listItem>
<equation confidence="0.59449725">
X
�Q(p′, p, TN) =
T l∈TlN ∈TN
log Pp′(Wl,Tl,Gl|d)
</equation>
<listItem confidence="0.4715105">
(b) M-step: Maximize �Q(p′, p, TN) with re-
spect to p′ to get new update for p.
</listItem>
<bodyText confidence="0.999410595744681">
Iterate steps (1) and (2) until the convergence of the
N-best-list likelihood. Due to space constraints, we
omit the proof of the convergence of the N-best list
approximate EM algorithm which uses Zangwill’s
global convergence theorem (Zangwill, 1969).
N-best list search strategy: To extract the N-
best parse trees, we adopt a synchronous, multi-
stack search strategy that is similar to the one in
(Chelba and Jelinek, 2000), which involves a set
of stacks storing partial parses of the most likely
ones for a given prefix Wk and the less probable
parses are purged. Each stack contains hypotheses
(partial parses) that have been constructed by the
same number of WORD-PREDICTOR and the same
number of CONSTRUCTOR operations. The hy-
potheses in each stack are ranked according to the
log(EGk Pp(Wk, Tk, Gk|d)) score with the highest
on top, where Pp(Wk, Tk, Gk|d) is the joint prob-
ability of prefix Wk = w0, · · · , wk with its parse
structure Tk and semantic annotation string Gk =
g1, · · · , gk in a document d. A stack vector consists
of the ordered set of stacks containing partial parses
with the same number of WORD-PREDICTOR op-
erations but different number of CONSTRUCTOR
operations. In WORD-PREDICTOR and TAGGER
operations, some hypotheses are discarded due to
the maximum number of hypotheses the stack can
contain at any given time. In CONSTRUCTOR
operation, the resulting hypotheses are discarded
due to either finite stack size or the log-probability
threshold: the maximum tolerable difference be-
tween the log-probability score of the top-most hy-
pothesis and the bottom-most hypothesis at any
given state of the stack.
EM update: Once we have the N-best parse trees
for each sentence in document d and N-best topics
for document d, we derive the EM algorithm to esti-
mate model parameters.
In E-step, we compute the expected count of
each model parameter over sentence Wl in docu-
ment d in the training corpus D. For the WORD-
PREDICTOR and the SEMANTIZER, the number
of possible semantic annotation sequences is expo-
nential, we use forward-backward recursive formu-
las that are similar to those in hidden Markov mod-
els to compute the expected counts. We define the
forward vector αl(g|d) to be
</bodyText>
<equation confidence="0.823557">
αlk+1(g|d) = X Pp(Wlk,Tlk,wkk−n+2wk+1h−mg,Glk|d)
1
Gl k
</equation>
<bodyText confidence="0.99267775">
that can be recursively computed in a forward man-
ner, where Wkl is the word k-prefix for sentence Wl,
Tkl is the parse for k-prefix. We define backward
vector 0l(g|d) to be
</bodyText>
<equation confidence="0.94601">
βlk+1(g|d)
X= Pp(Wlk+1,·,Tlk+1,·, Glk+1,· |wk−n+2wk+1h−mg, d)
k−1
Gl
k+1�·
</equation>
<bodyText confidence="0.945046909090909">
that can be computed in a backward manner, here
Wlk+1,· is the subsequence after k+1th word in sen-
tence Wl, Tlk+1,· is the incremental parse struc-
ture after the parse structure Tlk+1 of word k+1-
prefix Wl k+1 that generates parse tree Tl, Glk+1,· is
the semantic subsequence in Gl relevant to Wl k+1,·.
Then, the expected count of w−1
−n+1wh−1
−mg for the
WORD-PREDICTOR on sentence Wl in document
d is
</bodyText>
<equation confidence="0.962472444444444">
X Pp(Tl, Gl|Wl, d)#(w−1
−n+1wh−1
−mg, Wl, Tl, Gl, d)
Gl
αlk+1(g|d)βlk+1(g|d)p(g|d)
δ(wk k−n+2wk+1h−1
−mgk+1 = w−1
−n+1wh−1
−mg)/Pp(Wl|d)
</equation>
<bodyText confidence="0.974765">
where S(·) is an indicator function and the expected
count of g for the SEMANTIZER on sentence Wl
in document d is
</bodyText>
<equation confidence="0.343880666666667">
X Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d)
Gl
αlk+1(g|d)βlk+1(g|d)p(g|d)/Pp(Wl|d)
</equation>
<bodyText confidence="0.6946028">
For the TAGGER and the CONSTRUCTOR,
the expected count of each event of twh−1
−m.tag
and ah−1
−m over parse Tl of sentence Wl in
</bodyText>
<equation confidence="0.994971208333333">
Y
�L(D, p, TN) =
d∈D
Y
(
l
X
(
Gl
X
(
d∈D
X
Gl
X
l
X Pp(Tl,Gl|Wl,d)
X
k
X=
l
=
j−1 X
k=0
</equation>
<page confidence="0.99097">
204
</page>
<bodyText confidence="0.952667052631579">
document d is the real count appeared in parse
tree Tl of sentence Wl in document d times
the conditional distribution Pp(Tl|Wl, d) =
Pp(Tl, Wl|d)/ ETlETl Pp(Tl, Wl|d) respectively.
In M-step, the recursive linear interpolation
scheme (Jelinek and Mercer, 1981) is used
to obtain a smooth probability estimate for
each model component, WORD-PREDICTOR,
TAGGER, and CONSTRUCTOR. The TAGGER
and CONSTRUCTOR are conditional probabilis-
tic models of the type p(u|z1, · · · , zn) where
u, z1, · · · , zn belong to a mixed set of words, POS
tags, NTtags, CONSTRUCTOR actions (u only),
and z1, · · · , zn form a linear Markov chain. The re-
cursive mixing scheme is the standard one among
relative frequency estimates of different orders k =
0, · · · , n as explained in (Chelba and Jelinek, 2000).
The WORD-PREDICTOR is, however, a condi-
tional probabilistic model p(w|w−1
</bodyText>
<equation confidence="0.9512445">
−n+1h−1
−mg) where
</equation>
<bodyText confidence="0.780571">
there are three kinds of context w−1
</bodyText>
<equation confidence="0.796486">
−n+1, h−1
</equation>
<bodyText confidence="0.999169125">
−m and g,
each forms a linear Markov chain. The model has
a combinatorial number of relative frequency esti-
mates of different orders among three linear Markov
chains. We generalize Jelinek and Mercer’s original
recursive mixing scheme (Jelinek and Mercer, 1981)
and form a lattice to handle the situation where the
context is a mixture of Markov chains.
</bodyText>
<subsectionHeader confidence="0.964214">
3.2 Follow-up EM
</subsectionHeader>
<bodyText confidence="0.999949333333333">
As explained in (Chelba and Jelinek, 2000), for the
SLM component, a large fraction of the partial parse
trees that can be used for assigning probability to the
next word do not survive in the synchronous, multi-
stack search strategy, thus they are not used in the
N-best approximate EM algorithm for the estima-
tion of WORD-PREDICTOR to improve its predic-
tive power. To remedy this weakness, we estimate
WORD-PREDICTOR using the algorithm below.
The language model probability assignment for
the word at position k+1 in the input sentence of
document d can be computed as
</bodyText>
<equation confidence="0.9255926">
EPp(wk+1|Wk,d) =
h−1
−�∈Tk;Tk∈Zk,�k+1∈Gd
Pp(Tk|Wk, d)P(9k+1|d) (2)
P^p(Wk,Tk,Gk|d)
</equation>
<bodyText confidence="0.9308112">
where Pp(Tk |Wk, d) = ETk∈Zk &apos;:Gk LGk Pp(Wk,Tk,Gk|d)
and Zk is the set of all parses present in the stacks
at the current stage k during the synchronous multi-
stack pruning strategy and it is a function of the word
k-prefix Wk.
The likelihood of a training corpus D under this
language model probability assignment that uses
partial parse trees generated during the process of
the synchronous, multi-stack search strategy can be
written as
</bodyText>
<equation confidence="0.911029">
Pp(w(l+1 |Wkl, d)) (3)
d∈D
</equation>
<bodyText confidence="0.948911166666667">
We employ a second stage of parameter re-
estimation for p(wk+1|wk k−n+2h−1
−mgk+1) and
p(gk+1|d) by using EM again to maximize
Equation (3) to improve the predictive power of
WORD-PREDICTOR.
</bodyText>
<subsectionHeader confidence="0.996102">
3.3 Distributed architecture
</subsectionHeader>
<bodyText confidence="0.999977935483871">
When using very large corpora to train our compos-
ite language model, both the data and the parameters
can’t be stored in a single machine, so we have to
resort to distributed computing. The topic of large
scale distributed language models is relatively new,
and existing works are restricted to n-grams only
(Brants et al., 2007; Emami et al., 2007; Zhang et
al., 2006). Even though all use distributed archi-
tectures that follow the client-server paradigm, the
real implementations are in fact different. Zhang
et al. (2006) and Emami et al. (2007) store train-
ing corpora in suffix arrays such that one sub-corpus
per server serves raw counts and test sentences are
loaded in a client. This implies that when comput-
ing the language model probability of a sentence in
a client, all servers need to be contacted for each n-
gram request. The approach by Brants et al. (2007)
follows a standard MapReduce paradigm (Dean and
Ghemawat, 2004): the corpus is first divided and
loaded into a number of clients, and n-gram counts
are collected at each client, then the n-gram counts
mapped and stored in a number of servers, result-
ing in exactly one server being contacted per n-gram
when computing the language model probability of
a sentence. We adopt a similar approach to Brants
et al. and make it suitable to perform iterations
of N-best list approximate EM algorithm, see Fig-
ure 2. The corpus is divided and loaded into a num-
ber of clients. We use a public available parser to
parse the sentences in each client to get the initial
counts for w−1
</bodyText>
<page confidence="0.393445">
−n+1wh−1
</page>
<bodyText confidence="0.926154">
−mg etc., finish the Map part,
and then the counts for a particular w−1
</bodyText>
<equation confidence="0.661867454545455">
−n+1wh−1
−mg
at different clients are summed up and stored in one
�(wk+1|wk k−n+2h−1
−�9k+1)
11
�L(D, �) =
11
l
XE
k
</equation>
<page confidence="0.944422">
205
</page>
<figureCaption confidence="0.9933018">
Figure 2: Distributed architecture is essentially a MapRe-
duce paradigm: clients store partitioned data and per-
form E-step: compute expected counts, this is Map;
servers store parameters (counts) for M-step where
counts of w−1
</figureCaption>
<bodyText confidence="0.995801277777778">
−�+1wh−1 −�g are hashed by word w−1 (or
h−1) and its topic g to evenly distribute these model pa-
rameters into servers as much as possible, this is Reduce.
of the servers by hashing through the word w−1 (or
h−1) and its topic g, finish the Reduce part. This
is the initialization of the N-best list approximate
EM step. Each client then calls the servers for pa-
rameters to perform synchronous multi-stack search
for each sentence to get the N-best list parse trees.
Again, the expected count for a particular parameter
of w=n+1wh=mg at the clients are computed, thus
we finish a Map part, then summed up and stored in
one of the servers by hashing through the word w−1
(or h−1) and its topic g, thus we finish the Reduce
part. We repeat this procedure until convergence.
Similarly, we use a distributed architecture as in
Figure 2 to perform the follow-up EM algorithm to
re-estimate WORD-PREDICTOR.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="evaluation">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999857533333333">
We have trained our language models using three
different training sets: one has 44 million tokens,
another has 230 million tokens, and the other has
1.3 billion tokens. An independent test set which
has 354 k tokens is chosen. The independent check
data set used to determine the linear interpolation
coefficients has 1.7 million tokens for the 44 mil-
lion tokens training corpus, 13.7 million tokens for
both 230 million and 1.3 billion tokens training cor-
pora. All these data sets are taken from the LDC
English Gigaword corpus with non-verbalized punc-
tuation and we remove all punctuation. Table 1 gives
the detailed information on how these data sets are
chosen from the LDC English Gigaword corpus.
The vocabulary sizes in all three cases are:
</bodyText>
<listItem confidence="0.976592">
• word (also WORD-PREDICTOR operation)
</listItem>
<table confidence="0.999767">
1.3 BILLION TOKENS TRAINING CORPUS
AFP 19940512.0003 - 19961015.0568
AFW 19941111.0001 - 19960414.0652
NYT 19940701.0001 - 19950131.0483
NYT 19950401.0001 - 20040909.0063
XIN 19970901.0001 - 20041125.0119
230 MILLION TOKENS TRAINING CORPUS
AFP 19940622.0336 - 19961031.0797
APW 19941111.0001 - 19960419.0765
NYT 19940701.0001 - 19941130.0405
44 MILLION TOKENS TRAINING CORPUS
AFP 19940601.0001 - 19950721.0137
13.7 MILLION TOKENS CHECK CORPUS
NYT 19950201.0001 - 19950331.0494
1.7 MILLION TOKENS CHECK CORPUS
AFP 19940512.0003 - 19940531.0197
354 K TOKENS TEST CORPUS
CNA 20041101.0006 - 20041217.0009
</table>
<tableCaption confidence="0.8447275">
Table 1: The corpora used in our experiments are selected
from the LDC English Gigaword corpus and specified in
this table, AFP, AFW, NYT, XIN and CNA denote the
sections of the LDC English Gigaword corpus.
</tableCaption>
<bodyText confidence="0.991259">
vocabulary: 60 k, open - all words outside the
vocabulary are mapped to the &lt;unk&gt; token,
these 60 k words are chosen from the most fre-
quently occurred words in 44 millions tokens
corpus;
</bodyText>
<listItem confidence="0.9957812">
• POS tag (also TAGGER operation) vocabulary:
69, closed;
• non-terminal tag vocabulary: 54, closed;
• CONSTRUCTOR operation vocabulary: 157,
closed.
</listItem>
<bodyText confidence="0.999117266666667">
Similar to SLM (Chelba and Jelinek, 2000), af-
ter the parses undergo headword percolation and
binarization, each model component of WORD-
PREDICTOR, TAGGER, and CONSTRUCTOR is
initialized from a set of parsed sentences. We use
the “openNLP” software (Northedge, 2005) to parse
a large amount of sentences in the LDC English Gi-
gaword corpus to generate an automatic treebank,
which has a slightly different word-tokenization
than that of the manual treebank such as the Upenn
Treebank used in (Chelba and Jelinek, 2000). For
the 44 and 230 million tokens corpora, all sentences
are automatically parsed and used to initialize model
parameters, while for 1.3 billion tokens corpus, we
parse the sentences from a portion of the corpus that
</bodyText>
<figure confidence="0.9975498">
Server L
Server 1 Server 2
Client M
Client 1
Client 2
</figure>
<page confidence="0.997591">
206
</page>
<bodyText confidence="0.999973989583334">
contain 230 million tokens, then use them to initial-
ize model parameters. The parser at ”openNLP” is
trained by Upenn treebank with 1 million tokens and
there is a mismatch between Upenn treebank and
LDC English Gigaword corpus. Nevertheless, ex-
perimental results show that this approach is effec-
tive to provide initial values of model parameters.
As we have explained, the proposed EM algo-
rithms can be naturally cast into a MapReduce
framework, see more discussion in (Lin and Dyer,
2010). If we have access to a large cluster of
machines with Hadoop installed that are powerful
enough to process a billion tokens level corpus,
we just need to specify a map function and a re-
duce function etc., Hadoop will automatically par-
allelize and execute programs written in this func-
tional style. Unfortunately, we don’t have this kind
of resources available. Instead, we have access to a
supercomputer at a supercomputer center with MPI
installed that has more than 1000 core processors us-
able. Thus we implement our algorithms using C++
under MPI on the supercomputer, where we have to
write C++ codes for Map part and Reduce part, and
the MPI is used to take care of massage passing,
scheduling, synchronization, etc. between clients
and servers. This involves a fair amount of pro-
gramming work, even though our implementation
under MPI is not as reliable as under Hadoop but
it is more efficient. We use up to 1000 core proces-
sors to train the composite language models for 1.3
billion tokens corpus where 900 core processors are
used to store the parameters alone. We decide to use
linearly smoothed trigram as the baseline model for
44 million token corpus, linearly smoothed 4-gram
as the baseline model for 230 million token corpus,
and linearly smoothed 5-gram as the baseline model
for 1.3 billion token corpus. Model size is a big is-
sue, we have to keep only a small set of topics due to
the consideration in both computational time and re-
source demand. Table 2 shows the perplexity results
and computation time of composite n-gram/PLSA
language models that are trained on three corpora
when the pre-defined number of total topics is 200
but different numbers of most likely topics are kept
for each document in PLSA, the rest are pruned. For
composite 5-gram/PLSA model trained on 1.3 bil-
lion tokens corpus, 400 cores have to be used to
keep top 5 most likely topics. For composite tri-
gram/PLSA model trained on 44M tokens corpus,
the computation time increases drastically with less
than 5% percent perplexity improvement. So in the
following experiments, we keep top 5 topics for each
document from total 200 topics and all other 195
topics are pruned.
All composite language models are first trained
by performing N-best list approximate EM algo-
rithm until convergence, then EM algorithm for a
second stage of parameter re-estimation for WORD-
PREDICTOR and SEMANTIZER until conver-
gence. We fix the size of topics in PLSA to be 200
and then prune to 5 in the experiments, where the
unpruned 5 topics in general account for 70% prob-
ability in p(gId). Table 3 shows comprehensive per-
plexity results for a variety of different models such
as composite n-gram/m-SLM, n-gram/PLSA, m-
SLM/PLSA, their linear combinations, etc., where
we use online EM with fixed learning rate to re-
estimate the parameters of the SEMANTIZER of
test document. The m-SLM performs competitively
with its counterpart n-gram (n=m+1) on large scale
corpus. In Table 3, for composite n-gram/m-SLM
model (n = 3, m = 2 and n = 4, m = 3) trained
on 44 million tokens and 230 million tokens, we cut
off its fractional expected counts that are less than a
threshold 0.005, this significantly reduces the num-
ber of predictor’s types by 85%. When we train
the composite language on 1.3 billion tokens cor-
pus, we have to both aggressively prune the param-
eters of WORD-PREDICTOR and shrink the order
of n-gram and m-SLM in order to store them in a
supercomputer having 1000 cores. In particular, for
composite 5-gram/4-SLM model, its size is too big
to store, thus we use its approximation, a linear com-
bination of 5-gram/2-SLM and 2-gram/4-SLM, and
for 5-gram/2-SLM or 2-gram/4-SLM, again we cut
off its fractional expected counts that are less than a
threshold 0.005, this significantly reduces the num-
ber of predictor’s types by 85%. For composite 4-
SLM/PLSA model, we cut off its fractional expected
counts that are less than a threshold 0.002, again this
significantly reduces the number of predictor’s types
by 85%. For composite 4-SLM/PLSA model or its
linear combination with models, we ignore all the
tags and use only the words in the 4 head words.
In this table, we have three items missing (marked
by —), since the size of corresponding model is
</bodyText>
<page confidence="0.993246">
207
</page>
<table confidence="0.997651416666667">
CORPUS n # OF PPL TIME # OF # OF # OF TYPES
TOPICS (HOURS) SERVERS CLIENTS OF ww−1
−�+19
44M 3 5 196 0.5 40 100 120.1M
3 10 194 1.0 40 100 218.6M
3 20 190 2.7 80 100 537.8M
3 50 189 6.3 80 100 1.123B
3 100 189 11.2 80 100 1.616B
3 200 188 19.3 80 100 2.280B
230M 4 5 146 25.6 280 100 0.681B
1.3B 5 2 111 26.5 400 100 1.790B
5 5 102 75.0 400 100 4.391B
</table>
<tableCaption confidence="0.9911915">
Table 2: Perplexity (ppl) results and time consumed of composite n-gram/PLSA language model trained on three
corpora when different numbers of most likely topics are kept for each document in PLSA.
</tableCaption>
<table confidence="0.999854277777778">
LANGUAGE MODEL 44M REDUC- 230M REDUC- 1.3B REDUC-
n=3,m=2 TION n=4,m=3 TION n=5,m=4 TION
BASELINE n-GRAM (LINEAR) 262 200 138
n-GRAM (KNESER-NEY) 244 6.9% 183 8.5% — —
m-SLM 279 -6.5% 190 5.0% 137 0.0%
PLSA 825 -214.9% 812 -306.0% 773 -460.0%
n-GRAM+m-SLM 247 5.7% 184 8.0% 129 6.5%
n-GRAM+PLSA 235 10.3% 179 10.5% 128 7.2%
n-GRAM+m-SLM+PLSA 222 15.3% 175 12.5% 123 10.9%
n-GRAM/m-SLM 243 7.3% 171 14.5% (125) 9.4%
n-GRAM/PLSA 196 25.2% 146 27.0% 102 26.1%
m-SLM/PLSA 198 24.4% 140 30.0% (103) 25.4%
n-GRAM/PLSA+m-SLM/PLSA 183 30.2% 140 30.0% (93) 32.6%
n-GRAM/m-SLM+m-SLM/PLSA 183 30.2% 139 30.5% (94) 31.9%
n-GRAM/m-SLM+n-GRAM/PLSA 184 29.8% 137 31.5% (91) 34.1%
n-GRAM/m-SLM+n-GRAM/PLSA 180 31.3% 130 35.0% — —
+m-SLM/PLSA
n-GRAM/m-SLM/PLSA 176 32.8% — — — —
</table>
<tableCaption confidence="0.974543333333333">
Table 3: Perplexity results for various language models on test corpus, where + denotes linear combination, / denotes
composite model; n denotes the order of n-gram and m denotes the order of SLM; the topic nodes are pruned from
200 to 5.
</tableCaption>
<bodyText confidence="0.999060961538461">
too big to store in the supercomputer. The com-
posite n-gram/m-SLM/PLSA model gives signifi-
cant perplexity reductions over baseline n-grams,
n = 3, 4, 5 and m-SLMs, m = 2, 3, 4. The major-
ity of gains comes from PLSA component, but when
adding SLM component into n-gram/PLSA, there is
a further 10% relative perplexity reduction.
We have applied our composite 5-gram/2-
SLM+2-gram/4-SLM+5-gram/PLSA language
model that is trained by 1.3 billion word corpus for
the task of re-ranking the N-best list in statistical
machine translation. We used the same 1000-best
list that is used by Zhang et al. (2006). This
list was generated on 919 sentences from the
MT03 Chinese-English evaluation set by Hiero
(Chiang, 2005; Chiang, 2007), a state-of-the-art
parsing-based translation model. Its decoder uses
a trigram language model trained with modified
Kneser-Ney smoothing (Kneser and Ney, 1995) on
a 200 million tokens corpus. Each translation has
11 features and language model is one of them.
We substitute our language model and use MERT
(Och, 2003) to optimize the BLEU score (Papineni
et al., 2002). We partition the data into ten pieces,
9 pieces are used as training data to optimize the
BLEU score (Papineni et al., 2002) by MERT (Och,
</bodyText>
<page confidence="0.994057">
208
</page>
<bodyText confidence="0.999907592592593">
2003), a remaining single piece is used to re-rank
the 1000-best list and obtain the BLEU score. The
cross-validation process is then repeated 10 times
(the folds), with each of the 10 pieces used exactly
once as the validation data. The 10 results from the
folds then can be averaged (or otherwise combined)
to produce a single estimation for BLEU score.
Table 4 shows the BLEU scores through 10-fold
cross-validation. The composite 5-gram/2-SLM+2-
gram/4-SLM+5-gram/PLSA language model gives
1.57% BLEU score improvement over the baseline
and 0.79% BLEU score improvement over the
5-gram. This is because there is not much diversity
on the 1000-best list, and essentially only 20 — 30
distinct sentences are there in the 1000-best list.
Chiang (2007) studied the performance of machine
translation on Hiero, the BLEU score is 33.31%
when n-gram is used to re-rank the N-best list, how-
ever, the BLEU score becomes significantly higher
37.09% when the n-gram is embedded directly into
Hiero’s one pass decoder, this is because there is not
much diversity in the N-best list. It is expected that
putting the our composite language into a one pass
decoder of both phrase-based (Koehn et al., 2003)
and parsing-based (Chiang, 2005; Chiang, 2007)
MT systems should result in much improved BLEU
scores.
</bodyText>
<table confidence="0.991684571428571">
SYSTEM MODEL MEAN (%)
BASELINE 31.75
5-GRAM 32.53
5-GRAM/2-SLM+2-GRAM/4-SLM 32.87
5-GRAM/PLSA 33.01
5-GRAM/2-SLM+2-GRAM/4-SLM 33.32
+5-GRAM/PLSA
</table>
<tableCaption confidence="0.964337">
Table 4: 10-fold cross-validation BLEU score results for
the task of re-ranking the N-best list.
</tableCaption>
<bodyText confidence="0.999869666666667">
Besides reporting the BLEU scores, we look at the
“readability” of translations similar to the study con-
ducted by Charniak et al. (2003). The translations
are sorted into four groups: good/bad syntax crossed
with good/bad meaning by human judges, see Ta-
ble 5. We find that many more sentences are perfect,
many more are grammatically correct, and many
more are semantically correct. The syntactic lan-
guage model (Charniak, 2001; Charniak, 2003) only
improves translations to have good grammar, but
does not improve translations to preserve meaning.
The composite 5-gram/2-SLM+2-gram/4-SLM+5-
gram/PLSA language model improves both signif-
icantly. Bear in mind that Charniak et al. (2003) in-
tegrated Charniak’s language model with the syntax-
based translation model Yamada and Knight pro-
posed (2001) to rescore a tree-to-string translation
forest, whereas we use only our language model
for N-best list re-ranking. Also, in the same study
in (Charniak, 2003), they found that the outputs
produced using the n-grams received higher scores
from BLEU; ours did not. The difference between
human judgments and BLEU scores indicate that
closer agreement may be possible by incorporating
syntactic structure and semantic information into the
BLEU score evaluation. For example, semantically
similar words like “insure” and “ensure” in the ex-
ample of BLEU paper (Papineni et al., 2002) should
be substituted in the formula, and there is a weight
to measure the goodness of syntactic structure. This
modification will lead to a better metric and such
information can be provided by our composite lan-
guage models.
</bodyText>
<table confidence="0.990467833333333">
SYSTEM MODEL P S G W
BASELINE 95 398 20 406
5-GRAM 122 406 24 367
5-GRAM/2-SLM 151 425 33 310
+2-GRAM/4-SLM
+5-GRAM/PLSA
</table>
<tableCaption confidence="0.979943333333333">
Table 5: Results of “readability” evaluation on 919 trans-
lated sentences, P: perfect, S: only semantically correct,
G: only grammatically correct, W: wrong.
</tableCaption>
<sectionHeader confidence="0.999283" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999716692307692">
As far as we know, this is the first work of building a
complex large scale distributed language model with
a principled approach that is more powerful than n-
grams when both trained on a very large corpus with
up to a billion tokens. We believe our results still
hold on web scale corpora that have trillion tokens,
since the composite language model effectively en-
codes long range dependencies of natural language
that n-gram is not viable to consider. Of course,
this implies that we have to take a huge amount of
resources to perform the computation, nevertheless
this becomes feasible, affordable, and cheap in the
era of cloud computing.
</bodyText>
<page confidence="0.998148">
209
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947942307692">
L. Bahl and J. Baker,F. Jelinek and R. Mercer. 1977. Per-
plexityła measure of difficulty of speech recognition
tasks. 94th Meeting of the Acoustical Society ofAmer-
ica, 62:S63, Supplement 1.
T. Brants et al.. 2007. Large language models in ma-
chine translation. The 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
858-867.
E. Charniak. 2001. Immediate-head parsing for language
models. The 39th Annual Conference on Association
of Computational Linguistics (ACL), 124-131.
E. Charniak, K. Knight and K. Yamada. 2003. Syntax-
based language models for statistical machine transla-
tion. MT Summit IX., Intl. Assoc. for Machine Trans-
lation.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. The 36th Annual
Conference on Association of Computational Linguis-
tics (ACL), 225-231.
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283-332.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. The 43th Annual Con-
ference on Association of Computational Linguistics
(ACL), 263-270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. Operating Sys-
tems Design and Implementation (OSDI), 137-150.
A. Dempster, N. Laird and D. Rubin. 1977. Maximum
likelihood estimation from incomplete data via the EM
algorithm. Journal of Royal Statistical Society, 39:1-
38.
A. Emami, K. Papineni and J. Sorensen. 2007. Large-
scale distributed language modeling. The 32nd IEEE
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), IV:37-40.
T. Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177-196.
F. Jelinek and R. Mercer. 1981. Interpolated estimation
of Markov source parameters from sparse data. Pat-
tern Recognition in Practice, 381-397.
F. Jelinek and C. Chelba. 1999. Putting language
into language modeling. Sixth European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), Keynote Paper 1.
F. Jelinek. 2004. Stochastic analysis of structured lan-
guage modeling. Mathematical Foundations ofSpeech
and Language Processing, 37-72, Springer-Verlag.
D. Jurafsky and J. Martin. 2008. Speech and Language
Processing, 2nd Edition, Prentice Hall.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. The 20th IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP), 181-184.
P. Koehn, F. Och and D. Marcu. 2003. Statistical phrase-
based translation. The Human Language Technology
Conference (HLT), 48-54.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, 14(4):355-372.
A. Lavie et al. 2006. MINDS Workshops Machine
Translation Working Group Final Report. http://www-
nlpir.nist.gov/MINDS/FINAL/MT.web.pdf
J. Lin and C. Dyer. 2010. Data-Intensive Text Processing
with MapReduce. Morgan and Claypool Publishers.
R. Northedge. 2005. OpenNLP software
http://www.codeproject.com/KB/recipes/englishpar
sing.aspx
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. The 41th Annual meeting of
the Association for Computational Linguistics (ACL),
311-318.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. The 40th Annual meeting of the Associa-
tion for Computational Linguistics (ACL), 311-318.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249-276.
S. Wang et al. 2005. Exploiting syntactic, semantic and
lexical regularities in language modeling via directed
Markov random fields. The 22nd International Con-
ference on Machine Learning (ICML), 953-960.
S. Wang et al. 2006. Stochastic analysis of lexical and
semantic enhanced structural language model. The 8th
International Colloquium on Grammatical Inference
(ICGI), 97-111.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. The 39th Annual Conference
on Association of Computational Linguistics (ACL),
1067-1074.
W. Zangwill. 1969. Nonlinear Programming: A Unified
Approach. Prentice-Hall.
Y. Zhang, A. Hildebrand and S. Vogel. 2006. Dis-
tributed language modeling for N-best list re-ranking.
The 2006 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), 216-223.
Y. Zhang, 2008. Structured language models for statisti-
cal machine translation. Ph.D. dissertation, CMU.
</reference>
<page confidence="0.998963">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.420432">
<title confidence="0.9991325">A Large Scale Distributed Syntactic, Semantic and Language Model for Machine Translation</title>
<author confidence="0.739087">Ming Tan Wenli Zhou Lei Zheng Shaojun</author>
<email confidence="0.509201">Kno.e.sis</email>
<affiliation confidence="0.962831">Department of Computer Science and Wright State</affiliation>
<address confidence="0.994554">Dayton, OH 45435,</address>
<abstract confidence="0.999510047619048">This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model drastic perplexity reduction over grams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-theart parsing-based machine translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>J Baker</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Perplexityła measure of difficulty of speech recognition tasks.</title>
<date>1977</date>
<booktitle>94th Meeting of the Acoustical Society ofAmerica, 62:S63, Supplement 1.</booktitle>
<marker>Bahl, Baker, Jelinek, Mercer, 1977</marker>
<rawString>L. Bahl and J. Baker,F. Jelinek and R. Mercer. 1977. Perplexityła measure of difficulty of speech recognition tasks. 94th Meeting of the Acoustical Society ofAmerica, 62:S63, Supplement 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>The 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>858--867</pages>
<marker>Brants, 2007</marker>
<rawString>T. Brants et al.. 2007. Large language models in machine translation. The 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP), 858-867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>The 39th Annual Conference on Association of Computational Linguistics (ACL),</booktitle>
<pages>124--131</pages>
<contexts>
<context position="33072" citStr="Charniak, 2001" startWordPosition="5529" endWordPosition="5530">M/2-SLM+2-GRAM/4-SLM 32.87 5-GRAM/PLSA 33.01 5-GRAM/2-SLM+2-GRAM/4-SLM 33.32 +5-GRAM/PLSA Table 4: 10-fold cross-validation BLEU score results for the task of re-ranking the N-best list. Besides reporting the BLEU scores, we look at the “readability” of translations similar to the study conducted by Charniak et al. (2003). The translations are sorted into four groups: good/bad syntax crossed with good/bad meaning by human judges, see Table 5. We find that many more sentences are perfect, many more are grammatically correct, and many more are semantically correct. The syntactic language model (Charniak, 2001; Charniak, 2003) only improves translations to have good grammar, but does not improve translations to preserve meaning. The composite 5-gram/2-SLM+2-gram/4-SLM+5- gram/PLSA language model improves both significantly. Bear in mind that Charniak et al. (2003) integrated Charniak’s language model with the syntaxbased translation model Yamada and Knight proposed (2001) to rescore a tree-to-string translation forest, whereas we use only our language model for N-best list re-ranking. Also, in the same study in (Charniak, 2003), they found that the outputs produced using the n-grams received higher</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. The 39th Annual Conference on Association of Computational Linguistics (ACL), 124-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntaxbased language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>MT Summit IX., Intl. Assoc. for Machine Translation.</booktitle>
<contexts>
<context position="32781" citStr="Charniak et al. (2003)" startWordPosition="5481" endWordPosition="5484"> in the N-best list. It is expected that putting the our composite language into a one pass decoder of both phrase-based (Koehn et al., 2003) and parsing-based (Chiang, 2005; Chiang, 2007) MT systems should result in much improved BLEU scores. SYSTEM MODEL MEAN (%) BASELINE 31.75 5-GRAM 32.53 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 5-GRAM/PLSA 33.01 5-GRAM/2-SLM+2-GRAM/4-SLM 33.32 +5-GRAM/PLSA Table 4: 10-fold cross-validation BLEU score results for the task of re-ranking the N-best list. Besides reporting the BLEU scores, we look at the “readability” of translations similar to the study conducted by Charniak et al. (2003). The translations are sorted into four groups: good/bad syntax crossed with good/bad meaning by human judges, see Table 5. We find that many more sentences are perfect, many more are grammatically correct, and many more are semantically correct. The syntactic language model (Charniak, 2001; Charniak, 2003) only improves translations to have good grammar, but does not improve translations to preserve meaning. The composite 5-gram/2-SLM+2-gram/4-SLM+5- gram/PLSA language model improves both significantly. Bear in mind that Charniak et al. (2003) integrated Charniak’s language model with the syn</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>E. Charniak, K. Knight and K. Yamada. 2003. Syntaxbased language models for statistical machine translation. MT Summit IX., Intl. Assoc. for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>The 36th Annual Conference on Association of Computational Linguistics (ACL),</booktitle>
<pages>225--231</pages>
<contexts>
<context position="4273" citStr="Chelba and Jelinek, 1998" startWordPosition="636" endWordPosition="639">s 201–210, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ply our language models to the task of re-ranking the N-best list from Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based MT system, we achieve significantly better translation quality measured by the BLEU score and “readability”. 2 Composite language model The n-gram language model is essentially a word predictor that given its entire document history it predicts next word wk+1 based on the last n-1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, ··· , wk. The SLM (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000) uses syntactic information beyond the regular n-gram models to capture sentence level long range dependencies. The SLM is based on statistical parsing techniques that allow syntactic analysis of sentences; it assigns a probability p(W, T) to every sentence W and every possible binary parse T. The terminals of T are the words of W with POS tags, and the nodes of T are annotated with phrase headwords and non-terminal labels. Let W be a sentence of length n words to which we have prepended the sentence beginning marker &lt;s&gt; and appended the sentence end marker &lt;/s&gt; so t</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>C. Chelba and F. Jelinek. 1998. Exploiting syntactic structure for language modeling. The 36th Annual Conference on Association of Computational Linguistics (ACL), 225-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<pages>14--4</pages>
<contexts>
<context position="2411" citStr="Chelba and Jelinek, 2000" startWordPosition="353" endWordPosition="356">observe much improvement beyond 6-grams. Although the Markov chains are efficient at encoding local word interactions, the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages. As the machine translation (MT) working groups stated on page 3 of their final report (Lavie et al., 2006), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Wang et al. (2006) integrated n-gram, structured language model (SLM) (Chelba and Jelinek, 2000) and probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) under the directed MRF framework (Wang et al., 2005) and studied the stochastic properties for the composite language model. They derived a generalized inside-outside algorithm to train the composite language model from a general EM (Dempster et al., 1977) by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek, 2004) with 6th order of sentence length time complexity. Unfortunately, there are no experimental results reported. In this paper, we study the same composite language model.</context>
<context position="4300" citStr="Chelba and Jelinek, 2000" startWordPosition="640" endWordPosition="643">n, June 19-24, 2011. c�2011 Association for Computational Linguistics ply our language models to the task of re-ranking the N-best list from Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based MT system, we achieve significantly better translation quality measured by the BLEU score and “readability”. 2 Composite language model The n-gram language model is essentially a word predictor that given its entire document history it predicts next word wk+1 based on the last n-1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, ··· , wk. The SLM (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000) uses syntactic information beyond the regular n-gram models to capture sentence level long range dependencies. The SLM is based on statistical parsing techniques that allow syntactic analysis of sentences; it assigns a probability p(W, T) to every sentence W and every possible binary parse T. The terminals of T are the words of W with POS tags, and the nodes of T are annotated with phrase headwords and non-terminal labels. Let W be a sentence of length n words to which we have prepended the sentence beginning marker &lt;s&gt; and appended the sentence end marker &lt;/s&gt; so that w0 =&lt;s&gt; and wn+1 =&lt;/s&gt;.</context>
<context position="6134" citStr="Chelba and Jelinek, 2000" startWordPosition="975" endWordPosition="978">ontrol to the TAGGER; the TAGGER predicts the POS tag tk+1 to the next word wk+1 based on the next word wk+1 and the POS tags of the m left-most exposed headwords h−1 −m in the word-parse k-prefix with probability p(tk+1|wk+1, h−m.tag, · · · , h−1.tag); the CONSTRUCTOR builds the partial parse Tk from Tk−1, wk, and tk in a series of moves ending with NULL, where a parse move a is made with probability p(a|h−1 −m); a E A={(unary, NTlabel), (adjoinleft, NTlabel), (adjoin-right, NTlabel), null}. Once the CONSTRUCTOR hits NULL, it passes control to the WORD-PREDICTOR. See detailed description in (Chelba and Jelinek, 2000). A PLSA model (Hofmann, 2001) is a generative probabilistic model of word-document cooccurrences using the bag-of-words assumption described as follows: (i) choose a document d with probability p(d); (ii) SEMANTIZER: select a semantic class g with probability p(g|d); and (iii) WORD-PREDICTOR: pick a word w with probability p(w|g). Since only one pair of (d, w) is being observed, as a result, the joint probability model is a mixture of log-linear model with the expression p(d, w) = p(d) Eg p(w|g)p(g|d). Typically, the number of documents and vocabulary size are much larger than the size of lat</context>
<context position="10476" citStr="Chelba and Jelinek, 2000" startWordPosition="1686" endWordPosition="1689">ximize the likelihood L(D, p) respect to model parameters. For a given sentence, its parse tree and semantic content are hidden and the number of parse trees grows faster than exponential with sentence length, Wang et al. (2006) have derived a generalized inside-outside algorithm by applying the standard EM algorithm. However, the complexity of this algorithm is 6th order of sentence length, thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba, 1999; Jelinek, 2004). 3.1 N-best list approximate EM Similar to SLM (Chelba and Jelinek, 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood, L(D, �, T ′ N) = Y Y d∈D \ l 0111 @ X Pp(Wl,Tl,Gl|d) A A A Tl∈T′lN ,||T′lN ||=N where T′lN is a set of N parse trees for sentence Wl in document d and ||� ||denotes the cardinality and T′N is a collection of T′lN for sentences over entire corpus D. The N-best list approximate EM involves two steps: 1. N-best list search: For each sentence W in document d, find N</context>
<context position="12299" citStr="Chelba and Jelinek, 2000" startWordPosition="2032" endWordPosition="2035">(a) E-step: Compute the auxiliary function of the N-best-list likelihood X �Q(p′, p, TN) = T l∈TlN ∈TN log Pp′(Wl,Tl,Gl|d) (b) M-step: Maximize �Q(p′, p, TN) with respect to p′ to get new update for p. Iterate steps (1) and (2) until the convergence of the N-best-list likelihood. Due to space constraints, we omit the proof of the convergence of the N-best list approximate EM algorithm which uses Zangwill’s global convergence theorem (Zangwill, 1969). N-best list search strategy: To extract the Nbest parse trees, we adopt a synchronous, multistack search strategy that is similar to the one in (Chelba and Jelinek, 2000), which involves a set of stacks storing partial parses of the most likely ones for a given prefix Wk and the less probable parses are purged. Each stack contains hypotheses (partial parses) that have been constructed by the same number of WORD-PREDICTOR and the same number of CONSTRUCTOR operations. The hypotheses in each stack are ranked according to the log(EGk Pp(Wk, Tk, Gk|d)) score with the highest on top, where Pp(Wk, Tk, Gk|d) is the joint probability of prefix Wk = w0, · · · , wk with its parse structure Tk and semantic annotation string Gk = g1, · · · , gk in a document d. A stack ve</context>
<context position="16119" citStr="Chelba and Jelinek, 2000" startWordPosition="2701" endWordPosition="2704"> Wl|d) respectively. In M-step, the recursive linear interpolation scheme (Jelinek and Mercer, 1981) is used to obtain a smooth probability estimate for each model component, WORD-PREDICTOR, TAGGER, and CONSTRUCTOR. The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p(u|z1, · · · , zn) where u, z1, · · · , zn belong to a mixed set of words, POS tags, NTtags, CONSTRUCTOR actions (u only), and z1, · · · , zn form a linear Markov chain. The recursive mixing scheme is the standard one among relative frequency estimates of different orders k = 0, · · · , n as explained in (Chelba and Jelinek, 2000). The WORD-PREDICTOR is, however, a conditional probabilistic model p(w|w−1 −n+1h−1 −mg) where there are three kinds of context w−1 −n+1, h−1 −m and g, each forms a linear Markov chain. The model has a combinatorial number of relative frequency estimates of different orders among three linear Markov chains. We generalize Jelinek and Mercer’s original recursive mixing scheme (Jelinek and Mercer, 1981) and form a lattice to handle the situation where the context is a mixture of Markov chains. 3.2 Follow-up EM As explained in (Chelba and Jelinek, 2000), for the SLM component, a large fraction of </context>
<context position="22837" citStr="Chelba and Jelinek, 2000" startWordPosition="3808" endWordPosition="3811">ORPUS CNA 20041101.0006 - 20041217.0009 Table 1: The corpora used in our experiments are selected from the LDC English Gigaword corpus and specified in this table, AFP, AFW, NYT, XIN and CNA denote the sections of the LDC English Gigaword corpus. vocabulary: 60 k, open - all words outside the vocabulary are mapped to the &lt;unk&gt; token, these 60 k words are chosen from the most frequently occurred words in 44 millions tokens corpus; • POS tag (also TAGGER operation) vocabulary: 69, closed; • non-terminal tag vocabulary: 54, closed; • CONSTRUCTOR operation vocabulary: 157, closed. Similar to SLM (Chelba and Jelinek, 2000), after the parses undergo headword percolation and binarization, each model component of WORDPREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the “openNLP” software (Northedge, 2005) to parse a large amount of sentences in the LDC English Gigaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, w</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>C. Chelba and F. Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>The 43th Annual Conference on Association of Computational Linguistics (ACL),</booktitle>
<pages>263--270</pages>
<contexts>
<context position="3835" citStr="Chiang, 2005" startWordPosition="572" endWordPosition="573">xity and a follow-up EM algorithm to improve word prediction power. We conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens and compare perplexity results with n-grams (n=3,4,5 respectively) on these three corpora, we obtain drastic perplexity reductions. Finally, we ap201 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 201–210, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ply our language models to the task of re-ranking the N-best list from Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based MT system, we achieve significantly better translation quality measured by the BLEU score and “readability”. 2 Composite language model The n-gram language model is essentially a word predictor that given its entire document history it predicts next word wk+1 based on the last n-1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, ··· , wk. The SLM (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000) uses syntactic information beyond the regular n-gram models to capture sentence level long range dependencies. The SLM is based on sta</context>
<context position="30575" citStr="Chiang, 2005" startWordPosition="5133" endWordPosition="5134">lexity reductions over baseline n-grams, n = 3, 4, 5 and m-SLMs, m = 2, 3, 4. The majority of gains comes from PLSA component, but when adding SLM component into n-gram/PLSA, there is a further 10% relative perplexity reduction. We have applied our composite 5-gram/2- SLM+2-gram/4-SLM+5-gram/PLSA language model that is trained by 1.3 billion word corpus for the task of re-ranking the N-best list in statistical machine translation. We used the same 1000-best list that is used by Zhang et al. (2006). This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) on a 200 million tokens corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och, 2003) to optimize the BLEU score (Papineni et al., 2002). We partition the data into ten pieces, 9 pieces are used as training data to optimize the BLEU score (Papineni et al., 2002) by MERT (Och, 208 2003), a remaining single piece is used to re-rank the 1000-best list </context>
<context position="32332" citStr="Chiang, 2005" startWordPosition="5419" endWordPosition="5420"> not much diversity on the 1000-best list, and essentially only 20 — 30 distinct sentences are there in the 1000-best list. Chiang (2007) studied the performance of machine translation on Hiero, the BLEU score is 33.31% when n-gram is used to re-rank the N-best list, however, the BLEU score becomes significantly higher 37.09% when the n-gram is embedded directly into Hiero’s one pass decoder, this is because there is not much diversity in the N-best list. It is expected that putting the our composite language into a one pass decoder of both phrase-based (Koehn et al., 2003) and parsing-based (Chiang, 2005; Chiang, 2007) MT systems should result in much improved BLEU scores. SYSTEM MODEL MEAN (%) BASELINE 31.75 5-GRAM 32.53 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 5-GRAM/PLSA 33.01 5-GRAM/2-SLM+2-GRAM/4-SLM 33.32 +5-GRAM/PLSA Table 4: 10-fold cross-validation BLEU score results for the task of re-ranking the N-best list. Besides reporting the BLEU scores, we look at the “readability” of translations similar to the study conducted by Charniak et al. (2003). The translations are sorted into four groups: good/bad syntax crossed with good/bad meaning by human judges, see Table 5. We find that many more sent</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. The 43th Annual Conference on Association of Computational Linguistics (ACL), 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--2</pages>
<contexts>
<context position="3850" citStr="Chiang, 2007" startWordPosition="574" endWordPosition="575">low-up EM algorithm to improve word prediction power. We conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens and compare perplexity results with n-grams (n=3,4,5 respectively) on these three corpora, we obtain drastic perplexity reductions. Finally, we ap201 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 201–210, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ply our language models to the task of re-ranking the N-best list from Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based MT system, we achieve significantly better translation quality measured by the BLEU score and “readability”. 2 Composite language model The n-gram language model is essentially a word predictor that given its entire document history it predicts next word wk+1 based on the last n-1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2, ··· , wk. The SLM (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000) uses syntactic information beyond the regular n-gram models to capture sentence level long range dependencies. The SLM is based on statistical parsin</context>
<context position="30590" citStr="Chiang, 2007" startWordPosition="5135" endWordPosition="5136">ons over baseline n-grams, n = 3, 4, 5 and m-SLMs, m = 2, 3, 4. The majority of gains comes from PLSA component, but when adding SLM component into n-gram/PLSA, there is a further 10% relative perplexity reduction. We have applied our composite 5-gram/2- SLM+2-gram/4-SLM+5-gram/PLSA language model that is trained by 1.3 billion word corpus for the task of re-ranking the N-best list in statistical machine translation. We used the same 1000-best list that is used by Zhang et al. (2006). This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) on a 200 million tokens corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och, 2003) to optimize the BLEU score (Papineni et al., 2002). We partition the data into ten pieces, 9 pieces are used as training data to optimize the BLEU score (Papineni et al., 2002) by MERT (Och, 208 2003), a remaining single piece is used to re-rank the 1000-best list and obtain the </context>
<context position="31857" citStr="Chiang (2007)" startWordPosition="5340" endWordPosition="5341">ted 10 times (the folds), with each of the 10 pieces used exactly once as the validation data. The 10 results from the folds then can be averaged (or otherwise combined) to produce a single estimation for BLEU score. Table 4 shows the BLEU scores through 10-fold cross-validation. The composite 5-gram/2-SLM+2- gram/4-SLM+5-gram/PLSA language model gives 1.57% BLEU score improvement over the baseline and 0.79% BLEU score improvement over the 5-gram. This is because there is not much diversity on the 1000-best list, and essentially only 20 — 30 distinct sentences are there in the 1000-best list. Chiang (2007) studied the performance of machine translation on Hiero, the BLEU score is 33.31% when n-gram is used to re-rank the N-best list, however, the BLEU score becomes significantly higher 37.09% when the n-gram is embedded directly into Hiero’s one pass decoder, this is because there is not much diversity in the N-best list. It is expected that putting the our composite language into a one pass decoder of both phrase-based (Koehn et al., 2003) and parsing-based (Chiang, 2005; Chiang, 2007) MT systems should result in much improved BLEU scores. SYSTEM MODEL MEAN (%) BASELINE 31.75 5-GRAM 32.53 5-GR</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dean</author>
<author>S Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2004</date>
<booktitle>Operating Systems Design and Implementation (OSDI),</booktitle>
<pages>137--150</pages>
<contexts>
<context position="18905" citStr="Dean and Ghemawat, 2004" startWordPosition="3160" endWordPosition="3163">rants et al., 2007; Emami et al., 2007; Zhang et al., 2006). Even though all use distributed architectures that follow the client-server paradigm, the real implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts and test sentences are loaded in a client. This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each ngram request. The approach by Brants et al. (2007) follows a standard MapReduce paradigm (Dean and Ghemawat, 2004): the corpus is first divided and loaded into a number of clients, and n-gram counts are collected at each client, then the n-gram counts mapped and stored in a number of servers, resulting in exactly one server being contacted per n-gram when computing the language model probability of a sentence. We adopt a similar approach to Brants et al. and make it suitable to perform iterations of N-best list approximate EM algorithm, see Figure 2. The corpus is divided and loaded into a number of clients. We use a public available parser to parse the sentences in each client to get the initial counts f</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>J. Dean and S. Ghemawat. 2004. MapReduce: Simplified data processing on large clusters. Operating Systems Design and Implementation (OSDI), 137-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood estimation from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="2734" citStr="Dempster et al., 1977" startWordPosition="403" endWordPosition="406">l., 2006), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Wang et al. (2006) integrated n-gram, structured language model (SLM) (Chelba and Jelinek, 2000) and probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) under the directed MRF framework (Wang et al., 2005) and studied the stochastic properties for the composite language model. They derived a generalized inside-outside algorithm to train the composite language model from a general EM (Dempster et al., 1977) by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek, 2004) with 6th order of sentence length time complexity. Unfortunately, there are no experimental results reported. In this paper, we study the same composite language model. Instead of using the 6th order generalized inside-outside algorithm proposed in (Wang et al., 2006), we train this composite model by a convergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power. We conduct comprehensive experiments on cor</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird and D. Rubin. 1977. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of Royal Statistical Society, 39:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Emami</author>
<author>K Papineni</author>
<author>J Sorensen</author>
</authors>
<title>Largescale distributed language modeling.</title>
<date>2007</date>
<booktitle>The 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>37--40</pages>
<contexts>
<context position="18319" citStr="Emami et al., 2007" startWordPosition="3061" endWordPosition="3064">ategy can be written as Pp(w(l+1 |Wkl, d)) (3) d∈D We employ a second stage of parameter reestimation for p(wk+1|wk k−n+2h−1 −mgk+1) and p(gk+1|d) by using EM again to maximize Equation (3) to improve the predictive power of WORD-PREDICTOR. 3.3 Distributed architecture When using very large corpora to train our composite language model, both the data and the parameters can’t be stored in a single machine, so we have to resort to distributed computing. The topic of large scale distributed language models is relatively new, and existing works are restricted to n-grams only (Brants et al., 2007; Emami et al., 2007; Zhang et al., 2006). Even though all use distributed architectures that follow the client-server paradigm, the real implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts and test sentences are loaded in a client. This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each ngram request. The approach by Brants et al. (2007) follows a standard MapReduce paradigm (Dean and Ghemawat, 2004): the corpus i</context>
</contexts>
<marker>Emami, Papineni, Sorensen, 2007</marker>
<rawString>A. Emami, K. Papineni and J. Sorensen. 2007. Largescale distributed language modeling. The 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), IV:37-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--1</pages>
<contexts>
<context position="2477" citStr="Hofmann, 2001" startWordPosition="363" endWordPosition="364">ient at encoding local word interactions, the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages. As the machine translation (MT) working groups stated on page 3 of their final report (Lavie et al., 2006), “These approaches have resulted in small improvements in MT quality, but have not fundamentally solved the problem. There is a dire need for developing novel approaches to language modeling.” Wang et al. (2006) integrated n-gram, structured language model (SLM) (Chelba and Jelinek, 2000) and probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) under the directed MRF framework (Wang et al., 2005) and studied the stochastic properties for the composite language model. They derived a generalized inside-outside algorithm to train the composite language model from a general EM (Dempster et al., 1977) by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek, 2004) with 6th order of sentence length time complexity. Unfortunately, there are no experimental results reported. In this paper, we study the same composite language model. Instead of using the 6th order generalized inside-outside algorit</context>
<context position="6164" citStr="Hofmann, 2001" startWordPosition="982" endWordPosition="983"> the POS tag tk+1 to the next word wk+1 based on the next word wk+1 and the POS tags of the m left-most exposed headwords h−1 −m in the word-parse k-prefix with probability p(tk+1|wk+1, h−m.tag, · · · , h−1.tag); the CONSTRUCTOR builds the partial parse Tk from Tk−1, wk, and tk in a series of moves ending with NULL, where a parse move a is made with probability p(a|h−1 −m); a E A={(unary, NTlabel), (adjoinleft, NTlabel), (adjoin-right, NTlabel), null}. Once the CONSTRUCTOR hits NULL, it passes control to the WORD-PREDICTOR. See detailed description in (Chelba and Jelinek, 2000). A PLSA model (Hofmann, 2001) is a generative probabilistic model of word-document cooccurrences using the bag-of-words assumption described as follows: (i) choose a document d with probability p(d); (ii) SEMANTIZER: select a semantic class g with probability p(g|d); and (iii) WORD-PREDICTOR: pick a word w with probability p(w|g). Since only one pair of (d, w) is being observed, as a result, the joint probability model is a mixture of log-linear model with the expression p(d, w) = p(d) Eg p(w|g)p(g|d). Typically, the number of documents and vocabulary size are much larger than the size of latent semantic class variables. </context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>T. Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42(1):177-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data. Pattern Recognition in Practice,</title>
<date>1981</date>
<pages>381--397</pages>
<contexts>
<context position="15594" citStr="Jelinek and Mercer, 1981" startWordPosition="2602" endWordPosition="2605">pected count of g for the SEMANTIZER on sentence Wl in document d is X Pp(Tl, Gl|Wl, d)#(g, Wl, Gl, d) Gl αlk+1(g|d)βlk+1(g|d)p(g|d)/Pp(Wl|d) For the TAGGER and the CONSTRUCTOR, the expected count of each event of twh−1 −m.tag and ah−1 −m over parse Tl of sentence Wl in Y �L(D, p, TN) = d∈D Y ( l X ( Gl X ( d∈D X Gl X l X Pp(Tl,Gl|Wl,d) X k X= l = j−1 X k=0 204 document d is the real count appeared in parse tree Tl of sentence Wl in document d times the conditional distribution Pp(Tl|Wl, d) = Pp(Tl, Wl|d)/ ETlETl Pp(Tl, Wl|d) respectively. In M-step, the recursive linear interpolation scheme (Jelinek and Mercer, 1981) is used to obtain a smooth probability estimate for each model component, WORD-PREDICTOR, TAGGER, and CONSTRUCTOR. The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p(u|z1, · · · , zn) where u, z1, · · · , zn belong to a mixed set of words, POS tags, NTtags, CONSTRUCTOR actions (u only), and z1, · · · , zn form a linear Markov chain. The recursive mixing scheme is the standard one among relative frequency estimates of different orders k = 0, · · · , n as explained in (Chelba and Jelinek, 2000). The WORD-PREDICTOR is, however, a conditional probabilistic model p(w|w−1</context>
</contexts>
<marker>Jelinek, Mercer, 1981</marker>
<rawString>F. Jelinek and R. Mercer. 1981. Interpolated estimation of Markov source parameters from sparse data. Pattern Recognition in Practice, 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>C Chelba</author>
</authors>
<title>Putting language into language modeling.</title>
<date>1999</date>
<booktitle>Sixth European Conference on Speech Communication and Technology (EUROSPEECH), Keynote Paper 1.</booktitle>
<contexts>
<context position="10386" citStr="Jelinek and Chelba, 1999" startWordPosition="1672" endWordPosition="1675">he lth sentence Wl in document d. The objective of maximum likelihood estimation is to maximize the likelihood L(D, p) respect to model parameters. For a given sentence, its parse tree and semantic content are hidden and the number of parse trees grows faster than exponential with sentence length, Wang et al. (2006) have derived a generalized inside-outside algorithm by applying the standard EM algorithm. However, the complexity of this algorithm is 6th order of sentence length, thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba, 1999; Jelinek, 2004). 3.1 N-best list approximate EM Similar to SLM (Chelba and Jelinek, 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood, L(D, �, T ′ N) = Y Y d∈D \ l 0111 @ X Pp(Wl,Tl,Gl|d) A A A Tl∈T′lN ,||T′lN ||=N where T′lN is a set of N parse trees for sentence Wl in document d and ||� ||denotes the cardinality and T′N is a collection of T′lN for sentences over entire corpus D. The N-best list approxima</context>
</contexts>
<marker>Jelinek, Chelba, 1999</marker>
<rawString>F. Jelinek and C. Chelba. 1999. Putting language into language modeling. Sixth European Conference on Speech Communication and Technology (EUROSPEECH), Keynote Paper 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Stochastic analysis of structured language modeling.</title>
<date>2004</date>
<booktitle>Mathematical Foundations ofSpeech and Language Processing,</booktitle>
<pages>37--72</pages>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="2842" citStr="Jelinek, 2004" startWordPosition="421" endWordPosition="422">e problem. There is a dire need for developing novel approaches to language modeling.” Wang et al. (2006) integrated n-gram, structured language model (SLM) (Chelba and Jelinek, 2000) and probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) under the directed MRF framework (Wang et al., 2005) and studied the stochastic properties for the composite language model. They derived a generalized inside-outside algorithm to train the composite language model from a general EM (Dempster et al., 1977) by following Jelinek’s ingenious definition of the inside and outside probabilities for SLM (Jelinek, 2004) with 6th order of sentence length time complexity. Unfortunately, there are no experimental results reported. In this paper, we study the same composite language model. Instead of using the 6th order generalized inside-outside algorithm proposed in (Wang et al., 2006), we train this composite model by a convergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power. We conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1.3 billion tokens and compare perplexity results with </context>
<context position="10402" citStr="Jelinek, 2004" startWordPosition="1676" endWordPosition="1677">ment d. The objective of maximum likelihood estimation is to maximize the likelihood L(D, p) respect to model parameters. For a given sentence, its parse tree and semantic content are hidden and the number of parse trees grows faster than exponential with sentence length, Wang et al. (2006) have derived a generalized inside-outside algorithm by applying the standard EM algorithm. However, the complexity of this algorithm is 6th order of sentence length, thus it is computationally too expensive to be practical for a large corpus even with the use of pruning on charts (Jelinek and Chelba, 1999; Jelinek, 2004). 3.1 N-best list approximate EM Similar to SLM (Chelba and Jelinek, 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. Instead of maximizing the likelihood L(D, p), we maximize the N-best list likelihood, L(D, �, T ′ N) = Y Y d∈D \ l 0111 @ X Pp(Wl,Tl,Gl|d) A A A Tl∈T′lN ,||T′lN ||=N where T′lN is a set of N parse trees for sentence Wl in document d and ||� ||denotes the cardinality and T′N is a collection of T′lN for sentences over entire corpus D. The N-best list approximate EM involves t</context>
</contexts>
<marker>Jelinek, 2004</marker>
<rawString>F. Jelinek. 2004. Stochastic analysis of structured language modeling. Mathematical Foundations ofSpeech and Language Processing, 37-72, Springer-Verlag. D. Jurafsky and J. Martin. 2008. Speech and Language Processing, 2nd Edition, Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>The 20th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>181--184</pages>
<contexts>
<context position="30751" citStr="Kneser and Ney, 1995" startWordPosition="5154" endWordPosition="5157">m/PLSA, there is a further 10% relative perplexity reduction. We have applied our composite 5-gram/2- SLM+2-gram/4-SLM+5-gram/PLSA language model that is trained by 1.3 billion word corpus for the task of re-ranking the N-best list in statistical machine translation. We used the same 1000-best list that is used by Zhang et al. (2006). This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) on a 200 million tokens corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och, 2003) to optimize the BLEU score (Papineni et al., 2002). We partition the data into ten pieces, 9 pieces are used as training data to optimize the BLEU score (Papineni et al., 2002) by MERT (Och, 208 2003), a remaining single piece is used to re-rank the 1000-best list and obtain the BLEU score. The cross-validation process is then repeated 10 times (the folds), with each of the 10 pieces used exactly once as the validation data. The 10 resul</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. The 20th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 181-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>The Human Language Technology Conference (HLT),</booktitle>
<pages>48--54</pages>
<contexts>
<context position="32300" citStr="Koehn et al., 2003" startWordPosition="5413" endWordPosition="5416">er the 5-gram. This is because there is not much diversity on the 1000-best list, and essentially only 20 — 30 distinct sentences are there in the 1000-best list. Chiang (2007) studied the performance of machine translation on Hiero, the BLEU score is 33.31% when n-gram is used to re-rank the N-best list, however, the BLEU score becomes significantly higher 37.09% when the n-gram is embedded directly into Hiero’s one pass decoder, this is because there is not much diversity in the N-best list. It is expected that putting the our composite language into a one pass decoder of both phrase-based (Koehn et al., 2003) and parsing-based (Chiang, 2005; Chiang, 2007) MT systems should result in much improved BLEU scores. SYSTEM MODEL MEAN (%) BASELINE 31.75 5-GRAM 32.53 5-GRAM/2-SLM+2-GRAM/4-SLM 32.87 5-GRAM/PLSA 33.01 5-GRAM/2-SLM+2-GRAM/4-SLM 33.32 +5-GRAM/PLSA Table 4: 10-fold cross-validation BLEU score results for the task of re-ranking the N-best list. Besides reporting the BLEU scores, we look at the “readability” of translations similar to the study conducted by Charniak et al. (2003). The translations are sorted into four groups: good/bad syntax crossed with good/bad meaning by human judges, see Tabl</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och and D. Marcu. 2003. Statistical phrasebased translation. The Human Language Technology Conference (HLT), 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khudanpur</author>
<author>J Wu</author>
</authors>
<title>Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<pages>14--4</pages>
<marker>Khudanpur, Wu, 2000</marker>
<rawString>S. Khudanpur and J. Wu. 2000. Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling. Computer Speech and Language, 14(4):355-372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
</authors>
<title>MINDS Workshops Machine Translation Working Group Final Report.</title>
<date>2006</date>
<note>http://wwwnlpir.nist.gov/MINDS/FINAL/MT.web.pdf</note>
<marker>Lavie, 2006</marker>
<rawString>A. Lavie et al. 2006. MINDS Workshops Machine Translation Working Group Final Report. http://wwwnlpir.nist.gov/MINDS/FINAL/MT.web.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>C Dyer</author>
</authors>
<date>2010</date>
<booktitle>Data-Intensive Text Processing with MapReduce.</booktitle>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="24078" citStr="Lin and Dyer, 2010" startWordPosition="4013" endWordPosition="4016">okens corpus, we parse the sentences from a portion of the corpus that Server L Server 1 Server 2 Client M Client 1 Client 2 206 contain 230 million tokens, then use them to initialize model parameters. The parser at ”openNLP” is trained by Upenn treebank with 1 million tokens and there is a mismatch between Upenn treebank and LDC English Gigaword corpus. Nevertheless, experimental results show that this approach is effective to provide initial values of model parameters. As we have explained, the proposed EM algorithms can be naturally cast into a MapReduce framework, see more discussion in (Lin and Dyer, 2010). If we have access to a large cluster of machines with Hadoop installed that are powerful enough to process a billion tokens level corpus, we just need to specify a map function and a reduce function etc., Hadoop will automatically parallelize and execute programs written in this functional style. Unfortunately, we don’t have this kind of resources available. Instead, we have access to a supercomputer at a supercomputer center with MPI installed that has more than 1000 core processors usable. Thus we implement our algorithms using C++ under MPI on the supercomputer, where we have to write C++</context>
</contexts>
<marker>Lin, Dyer, 2010</marker>
<rawString>J. Lin and C. Dyer. 2010. Data-Intensive Text Processing with MapReduce. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Northedge</author>
</authors>
<date>2005</date>
<note>OpenNLP software http://www.codeproject.com/KB/recipes/englishpar sing.aspx</note>
<contexts>
<context position="23060" citStr="Northedge, 2005" startWordPosition="3843" endWordPosition="3844">gaword corpus. vocabulary: 60 k, open - all words outside the vocabulary are mapped to the &lt;unk&gt; token, these 60 k words are chosen from the most frequently occurred words in 44 millions tokens corpus; • POS tag (also TAGGER operation) vocabulary: 69, closed; • non-terminal tag vocabulary: 54, closed; • CONSTRUCTOR operation vocabulary: 157, closed. Similar to SLM (Chelba and Jelinek, 2000), after the parses undergo headword percolation and binarization, each model component of WORDPREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. We use the “openNLP” software (Northedge, 2005) to parse a large amount of sentences in the LDC English Gigaword corpus to generate an automatic treebank, which has a slightly different word-tokenization than that of the manual treebank such as the Upenn Treebank used in (Chelba and Jelinek, 2000). For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1.3 billion tokens corpus, we parse the sentences from a portion of the corpus that Server L Server 1 Server 2 Client M Client 1 Client 2 206 contain 230 million tokens, then use them to initialize model parameters</context>
</contexts>
<marker>Northedge, 2005</marker>
<rawString>R. Northedge. 2005. OpenNLP software http://www.codeproject.com/KB/recipes/englishpar sing.aspx</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>The 41th Annual meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="30909" citStr="Och, 2003" startWordPosition="5184" endWordPosition="5185">billion word corpus for the task of re-ranking the N-best list in statistical machine translation. We used the same 1000-best list that is used by Zhang et al. (2006). This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) on a 200 million tokens corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och, 2003) to optimize the BLEU score (Papineni et al., 2002). We partition the data into ten pieces, 9 pieces are used as training data to optimize the BLEU score (Papineni et al., 2002) by MERT (Och, 208 2003), a remaining single piece is used to re-rank the 1000-best list and obtain the BLEU score. The cross-validation process is then repeated 10 times (the folds), with each of the 10 pieces used exactly once as the validation data. The 10 results from the folds then can be averaged (or otherwise combined) to produce a single estimation for BLEU score. Table 4 shows the BLEU scores through 10-fold cr</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. The 41th Annual meeting of the Association for Computational Linguistics (ACL), 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>The 40th Annual meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="30960" citStr="Papineni et al., 2002" startWordPosition="5191" endWordPosition="5194">ranking the N-best list in statistical machine translation. We used the same 1000-best list that is used by Zhang et al. (2006). This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) on a 200 million tokens corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och, 2003) to optimize the BLEU score (Papineni et al., 2002). We partition the data into ten pieces, 9 pieces are used as training data to optimize the BLEU score (Papineni et al., 2002) by MERT (Och, 208 2003), a remaining single piece is used to re-rank the 1000-best list and obtain the BLEU score. The cross-validation process is then repeated 10 times (the folds), with each of the 10 pieces used exactly once as the validation data. The 10 results from the folds then can be averaged (or otherwise combined) to produce a single estimation for BLEU score. Table 4 shows the BLEU scores through 10-fold cross-validation. The composite 5-gram/2-SLM+2- gram/</context>
<context position="34020" citStr="Papineni et al., 2002" startWordPosition="5668" endWordPosition="5671">ion model Yamada and Knight proposed (2001) to rescore a tree-to-string translation forest, whereas we use only our language model for N-best list re-ranking. Also, in the same study in (Charniak, 2003), they found that the outputs produced using the n-grams received higher scores from BLEU; ours did not. The difference between human judgments and BLEU scores indicate that closer agreement may be possible by incorporating syntactic structure and semantic information into the BLEU score evaluation. For example, semantically similar words like “insure” and “ensure” in the example of BLEU paper (Papineni et al., 2002) should be substituted in the formula, and there is a weight to measure the goodness of syntactic structure. This modification will lead to a better metric and such information can be provided by our composite language models. SYSTEM MODEL P S G W BASELINE 95 398 20 406 5-GRAM 122 406 24 367 5-GRAM/2-SLM 151 425 33 310 +2-GRAM/4-SLM +5-GRAM/PLSA Table 5: Results of “readability” evaluation on 919 translated sentences, P: perfect, S: only semantically correct, G: only grammatically correct, W: wrong. 5 Conclusion As far as we know, this is the first work of building a complex large scale distri</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. The 40th Annual meeting of the Association for Computational Linguistics (ACL), 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--2</pages>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249-276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
</authors>
<title>Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields.</title>
<date>2005</date>
<booktitle>The 22nd International Conference on Machine Learning (ICML),</booktitle>
<pages>953--960</pages>
<marker>Wang, 2005</marker>
<rawString>S. Wang et al. 2005. Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields. The 22nd International Conference on Machine Learning (ICML), 953-960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
</authors>
<title>Stochastic analysis of lexical and semantic enhanced structural language model.</title>
<date>2006</date>
<booktitle>The 8th International Colloquium on Grammatical Inference (ICGI),</booktitle>
<pages>97--111</pages>
<marker>Wang, 2006</marker>
<rawString>S. Wang et al. 2006. Stochastic analysis of lexical and semantic enhanced structural language model. The 8th International Colloquium on Grammatical Inference (ICGI), 97-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>The 39th Annual Conference on Association of Computational Linguistics (ACL),</booktitle>
<pages>1067--1074</pages>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. The 39th Annual Conference on Association of Computational Linguistics (ACL), 1067-1074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zangwill</author>
</authors>
<title>Nonlinear Programming: A Unified Approach.</title>
<date>1969</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="12127" citStr="Zangwill, 1969" startWordPosition="2004" endWordPosition="2005">max T ′N Xmax T ′lN∈T′N l G T N l = arg max 203 parameters that maximizes N-best-list likelihood of the training corpus D, Pp(Wl,Tl,Gl|d)))) T l∈TlN ∈TN That is, (a) E-step: Compute the auxiliary function of the N-best-list likelihood X �Q(p′, p, TN) = T l∈TlN ∈TN log Pp′(Wl,Tl,Gl|d) (b) M-step: Maximize �Q(p′, p, TN) with respect to p′ to get new update for p. Iterate steps (1) and (2) until the convergence of the N-best-list likelihood. Due to space constraints, we omit the proof of the convergence of the N-best list approximate EM algorithm which uses Zangwill’s global convergence theorem (Zangwill, 1969). N-best list search strategy: To extract the Nbest parse trees, we adopt a synchronous, multistack search strategy that is similar to the one in (Chelba and Jelinek, 2000), which involves a set of stacks storing partial parses of the most likely ones for a given prefix Wk and the less probable parses are purged. Each stack contains hypotheses (partial parses) that have been constructed by the same number of WORD-PREDICTOR and the same number of CONSTRUCTOR operations. The hypotheses in each stack are ranked according to the log(EGk Pp(Wk, Tk, Gk|d)) score with the highest on top, where Pp(Wk,</context>
</contexts>
<marker>Zangwill, 1969</marker>
<rawString>W. Zangwill. 1969. Nonlinear Programming: A Unified Approach. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>A Hildebrand</author>
<author>S Vogel</author>
</authors>
<title>Distributed language modeling for N-best list re-ranking.</title>
<date>2006</date>
<booktitle>The 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>216--223</pages>
<contexts>
<context position="18340" citStr="Zhang et al., 2006" startWordPosition="3065" endWordPosition="3068"> as Pp(w(l+1 |Wkl, d)) (3) d∈D We employ a second stage of parameter reestimation for p(wk+1|wk k−n+2h−1 −mgk+1) and p(gk+1|d) by using EM again to maximize Equation (3) to improve the predictive power of WORD-PREDICTOR. 3.3 Distributed architecture When using very large corpora to train our composite language model, both the data and the parameters can’t be stored in a single machine, so we have to resort to distributed computing. The topic of large scale distributed language models is relatively new, and existing works are restricted to n-grams only (Brants et al., 2007; Emami et al., 2007; Zhang et al., 2006). Even though all use distributed architectures that follow the client-server paradigm, the real implementations are in fact different. Zhang et al. (2006) and Emami et al. (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts and test sentences are loaded in a client. This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each ngram request. The approach by Brants et al. (2007) follows a standard MapReduce paradigm (Dean and Ghemawat, 2004): the corpus is first divided and l</context>
<context position="30465" citStr="Zhang et al. (2006)" startWordPosition="5114" endWordPosition="5117">ed from 200 to 5. too big to store in the supercomputer. The composite n-gram/m-SLM/PLSA model gives significant perplexity reductions over baseline n-grams, n = 3, 4, 5 and m-SLMs, m = 2, 3, 4. The majority of gains comes from PLSA component, but when adding SLM component into n-gram/PLSA, there is a further 10% relative perplexity reduction. We have applied our composite 5-gram/2- SLM+2-gram/4-SLM+5-gram/PLSA language model that is trained by 1.3 billion word corpus for the task of re-ranking the N-best list in statistical machine translation. We used the same 1000-best list that is used by Zhang et al. (2006). This list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based translation model. Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) on a 200 million tokens corpus. Each translation has 11 features and language model is one of them. We substitute our language model and use MERT (Och, 2003) to optimize the BLEU score (Papineni et al., 2002). We partition the data into ten pieces, 9 pieces are used as training data to optimize the BLEU score (P</context>
</contexts>
<marker>Zhang, Hildebrand, Vogel, 2006</marker>
<rawString>Y. Zhang, A. Hildebrand and S. Vogel. 2006. Distributed language modeling for N-best list re-ranking. The 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP), 216-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
</authors>
<title>Structured language models for statistical machine translation.</title>
<date>2008</date>
<note>Ph.D. dissertation, CMU.</note>
<contexts>
<context position="1582" citStr="Zhang, 2008" startWordPosition="225" endWordPosition="226">d achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-theart parsing-based machine translation system. 1 Introduction The Markov chain (n-gram) source models, which predict each word on the basis of previous n-1 words, have been the workhorses of state-of-the-art speech recognizers and machine translators that help to resolve acoustic or foreign language ambiguities by placing higher probability on more likely original underlying word strings. Research groups (Brants et al., 2007; Zhang, 2008) have shown that using an immense distributed computing paradigm, up to 6- grams can be trained on up to billions and trillions of words, yielding consistent system improvements, but Zhang (2008) did not observe much improvement beyond 6-grams. Although the Markov chains are efficient at encoding local word interactions, the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages. As the machine translation (MT) working groups stated on page 3 of their final report (Lavie et al., 2006), “These approaches have resulted in small improvements in MT</context>
</contexts>
<marker>Zhang, 2008</marker>
<rawString>Y. Zhang, 2008. Structured language models for statistical machine translation. Ph.D. dissertation, CMU.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>