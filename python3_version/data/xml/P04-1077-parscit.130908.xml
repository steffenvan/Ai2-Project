<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000073">
<title confidence="0.998473">
Automatic Evaluation of Machine Translation Quality Using Longest Com-
mon Subsequence and Skip-Bigram Statistics
</title>
<author confidence="0.993471">
Chin-Yew Lin and Franz Josef Och
</author>
<affiliation confidence="0.9968545">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.8715525">
4676 Admiralty Way
Marina del Rey, CA 90292, USA
</address>
<email confidence="0.997516">
{cyl,och}@isi.edu
</email>
<sectionHeader confidence="0.992542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999225055555556">
In this paper we describe two new objective
automatic evaluation methods for machine
translation. The first method is based on long-
est common subsequence between a candidate
translation and a set of reference translations.
Longest common subsequence takes into ac-
count sentence level structure similarity natu-
rally and identifies longest co-occurring in-
sequence n-grams automatically. The second
method relaxes strict n-gram matching to skip-
bigram matching. Skip-bigram is any pair of
words in their sentence order. Skip-bigram co-
occurrence statistics measure the overlap of
skip-bigrams between a candidate translation
and a set of reference translations. The empiri-
cal results show that both methods correlate
with human judgments very well in both ade-
quacy and fluency.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947152542373">
Using objective functions to automatically evalu-
ate machine translation quality is not new. Su et al.
(1992) proposed a method based on measuring edit
distance (Levenshtein 1966) between candidate
and reference translations. Akiba et al. (2001) ex-
tended the idea to accommodate multiple refer-
ences. Nießen et al. (2000) calculated the length-
normalized edit distance, called word error rate
(WER), between a candidate and multiple refer-
ence translations. Leusch et al. (2003) proposed a
related measure called position-independent word
error rate (PER) that did not consider word posi-
tion, i.e. using bag-of-words instead. Instead of
error measures, we can also use accuracy measures
that compute similarity between candidate and ref-
erence translations in proportion to the number of
common words between them as suggested by
Melamed (1995). An n-gram co-occurrence meas-
ure, BLEU, proposed by Papineni et al. (2001) that
calculates co-occurrence statistics based on n-gram
overlaps have shown great potential. A variant of
BLEU developed by NIST (2002) has been used in
two recent large-scale machine translation evalua-
tions.
Recently, Turian et al. (2003) indicated that
standard accuracy measures such as recall, preci-
sion, and the F-measure can also be used in evalua-
tion of machine translation. However, results based
on their method, General Text Matcher (GTM),
showed that unigram F-measure correlated best
with human judgments while assigning more
weight to higher n-gram (n &gt; 1) matches achieved
similar performance as Bleu. Since unigram
matches do not distinguish words in consecutive
positions from words in the wrong order, measures
based on position-independent unigram matches
are not sensitive to word order and sentence level
structure. Therefore, systems optimized for these
unigram-based measures might generate adequate
but not fluent target language.
Since BLEU has been used to report the perform-
ance of many machine translation systems and it
has been shown to correlate well with human
judgments, we will explain BLEU in more detail
and point out its limitations in the next section. We
then introduce a new evaluation method called
ROUGE-L that measures sentence-to-sentence
similarity based on the longest common subse-
quence statistics between a candidate translation
and a set of reference translations in Section 3.
Section 4 describes another automatic evaluation
method called ROUGE-S that computes skip-
bigram co-occurrence statistics. Section 5 presents
the evaluation results of ROUGE-L, and ROUGE-
S and compare them with BLEU, GTM, NIST,
PER, and WER in correlation with human judg-
ments in terms of adequacy and fluency. We con-
clude this paper and discuss extensions of the
current work in Section 6.
</bodyText>
<sectionHeader confidence="0.893083" genericHeader="introduction">
2 BLEU and N-gram Co-Occurrence
</sectionHeader>
<bodyText confidence="0.998924761904762">
To automatically evaluate machine translations
the machine translation community recently
adopted an n-gram co-occurrence scoring proce-
dure BLEU (Papineni et al. 2001). In two recent
large-scale machine translation evaluations spon-
sored by NIST, a closely related automatic evalua-
tion method, simply called NIST score, was used.
The NIST (NIST 2002) scoring method is based on
BLEU.
The main idea of BLEU is to measure the simi-
larity between a candidate translation and a set of
reference translations with a numerical metric.
They used a weighted average of variable length n-
gram matches between system translations and a
set of human reference translations and showed
that the weighted average metric correlating highly
with human assessments.
BLEU measures how well a machine translation
overlaps with multiple human translations using n-
gram co-occurrence statistics. N-gram precision in
BLEU is computed as follows:
</bodyText>
<equation confidence="0.99962725">
∑ ∑ Countclip (n−gram)
p = C∈{Candidates}n−gram∈C (1)
n
∑ ∑ Count n gram
( − )
C Candidates
∈ { } n gram C
− ∈
</equation>
<bodyText confidence="0.993412428571428">
Where Countclip(n-gram) is the maximum num-
ber of n-grams co-occurring in a candidate transla-
tion and a reference translation, and Count(n-
gram) is the number of n-grams in the candidate
translation. To prevent very short translations that
try to maximize their precision scores, BLEU adds a
brevity penalty, BP, to the formula:
</bodyText>
<equation confidence="0.997804166666667">
1 if c r
&gt; ⎫
⎬
(1   ||/  ||)
− r c if c r
≤ ⎭
</equation>
<bodyText confidence="0.984472666666667">
Where |c |is the length of the candidate transla-
tion and |r |is the length of the reference transla-
tion. The BLEU formula is then written as follows:
</bodyText>
<equation confidence="0.998033">
N
⎛ ⎞
BLEU=BP•exp⎜∑wnlogpn
n=1 ⎠
</equation>
<bodyText confidence="0.965584625">
The weighting factor, wn, is set at 1/N.
Although BLEU has been shown to correlate well
with human assessments, it has a few things that
can be improved. First the subjective application of
the brevity penalty can be replaced with a recall
related parameter that is sensitive to reference
length. Although brevity penalty will penalize can-
didate translations with low recall by a factor of e(1-
|r|/|c|), it would be nice if we can use the traditional
recall measure that has been a well known measure
in NLP as suggested by Melamed (2003). Of
course we have to make sure the resulting compos-
ite function of precision and recall is still correlates
highly with human judgments.
Second, although BLEU uses high order n-gram
(n&gt;1) matches to favor candidate sentences with
consecutive word matches and to estimate their
fluency, it does not consider sentence level struc-
ture. For example, given the following sentences:
S1. police killed the gunman
S2. police kill the gunman1
S3. the gunman kill police
We only consider BLEU with unigram and bi-
gram, i.e. N=2, for the purpose of explanation and
call this BLEU-2. Using S1 as the reference and S2
and S3 as the candidate translations, S2 and S3
would have the same BLEU-2 score, since they
both have one bigram and three unigram matches2.
However, S2 and S3 have very different meanings.
Third, BLEU is a geometric mean of unigram to
N-gram precisions. Any candidate translation
without a N-gram match has a per-sentence BLEU
score of zero. Although BLEU is usually calculated
over the whole test corpus, it is still desirable to
have a measure that works reliably at sentence
level for diagnostic and introspection purpose.
To address these issues, we propose three new
automatic evaluation measures based on longest
common subsequence statistics and skip bigram
co-occurrence statistics in the following sections.
</bodyText>
<sectionHeader confidence="0.927668" genericHeader="method">
3 Longest Common Subsequence
</sectionHeader>
<subsectionHeader confidence="0.870572">
3.1 ROUGE-L
</subsectionHeader>
<bodyText confidence="0.999894782608695">
A sequence Z = [z1, z2, ..., zn] is a subsequence of
another sequence X = [x1, x2, ..., xm], if there exists
a strict increasing sequence [i1, i2, ..., ik] of indices
of X such that for all j = 1, 2, ..., k, we have xij = zj
(Cormen et al. 1989). Given two sequences X and
Y, the longest common subsequence (LCS) of X
and Y is a common subsequence with maximum
length. We can find the LCS of two sequences of
length m and n using standard dynamic program-
ming technique in O(mn) time.
LCS has been used to identify cognate candi-
dates during construction of N-best translation
lexicons from parallel text. Melamed (1995) used
the ratio (LCSR) between the length of the LCS of
two words and the length of the longer word of the
two words to measure the cognateness between
them. He used as an approximate string matching
algorithm. Saggion et al. (2002) used normalized
pairwise LCS (NP-LCS) to compare similarity be-
tween two texts in automatic summarization
evaluation. NP-LCS can be shown as a special case
of Equation (6) with β = 1. However, they did not
provide the correlation analysis of NP-LCS with
</bodyText>
<footnote confidence="0.993724333333333">
1 This is a real machine translation output.
2 The “kill” in S2 or S3 does not match with “killed” in
S1 in strict word-to-word comparison.
</footnote>
<equation confidence="0.928019363636364">
BP
e
⎧
⎨
⎩
(
2
)
(
3
)
</equation>
<bodyText confidence="0.999156545454545">
human judgments and its effectiveness as an auto-
matic evaluation measure.
To apply LCS in machine translation evaluation,
we view a translation as a sequence of words. The
intuition is that the longer the LCS of two transla-
tions is, the more similar the two translations are.
We propose using LCS-based F-measure to esti-
mate the similarity between two translations X of
length m and Y of length n, assuming X is a refer-
ence translation and Y is a candidate translation, as
follows:
</bodyText>
<equation confidence="0.9870945">
LCS(X, Y)
=
m
LCS(X, Y)
=
n
</equation>
<bodyText confidence="0.955425666666667">
S1. police killed the gunman
S2. police kill the gunman
S3. the gunman kill police
As we have shown earlier, BLEU-2 cannot differ-
entiate S2 from S3. However, S2 has a ROUGE-L
score of 3/4 = 0.75 and S3 has a ROUGE-L score
of 2/4 = 0.5, with ,B = 1. Therefore S2 is better than
S3 according to ROUGE-L. This example also il-
lustrated that ROUGE-L can work reliably at sen-
tence level.
However, LCS only counts the main in-sequence
words; therefore, other longest common subse-
quences and shorter sequences are not reflected in
the final score. For example, consider the follow-
ing candidate sentence:
</bodyText>
<figure confidence="0.801344461538462">
Rlcs
Plcs
Flcs
(6)
β2
+
Plcs
(1 2)
+β R P
lcs lcs
=
Rlcs
S4. the gunman police killed
</figure>
<bodyText confidence="0.999891422222222">
Where LCS(X,Y) is the length of a longest common
subsequence of X and Y, and ,B = Plcs/Rlcs when
aFlcs/aRlcs=aFlcs/aPlcs. We call the LCS-based F-
measure, i.e. Equation 6, ROUGE-L. Notice that
ROUGE-L is 1 when X = Y since LCS(X,Y) = m or
n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e.
there is nothing in common between X and Y. F-
measure or its equivalents has been shown to have
met several theoretical criteria in measuring accu-
racy involving more than one factor (Van Rijsber-
gen 1979). The composite factors are LCS-based
recall and precision in this case. Melamed et al.
(2003) used unigram F-measure to estimate ma-
chine translation quality and showed that unigram
F-measure was as good as BLEU.
One advantage of using LCS is that it does not
require consecutive matches but in-sequence
matches that reflect sentence level word order as n-
grams. The other advantage is that it automatically
includes longest in-sequence common n-grams,
therefore no predefined n-gram length is necessary.
ROUGE-L as defined in Equation 6 has the prop-
erty that its value is less than or equal to the mini-
mum of unigram F-measure of X and Y. Unigram
recall reflects the proportion of words in X (refer-
ence translation) that are also present in Y (candi-
date translation); while unigram precision is the
proportion of words in Y that are also in X. Uni-
gram recall and precision count all co-occurring
words regardless their orders; while ROUGE-L
counts only in-sequence co-occurrences.
By only awarding credit to in-sequence unigram
matches, ROUGE-L also captures sentence level
structure in a natural way. Consider again the ex-
ample given in Section 2 that is copied here for
convenience:
Using S1 as its reference, LCS counts either “the
gunman” or “police killed”, but not both; therefore,
S4 has the same ROUGE-L score as S3. BLEU-2
would prefer S4 over S3. In Section 4, we will in-
troduce skip-bigram co-occurrence statistics that
do not have this problem while still keeping the
advantage of in-sequence (not necessary consecu-
tive) matching that reflects sentence level word
order.
</bodyText>
<subsectionHeader confidence="0.999617">
3.2 Multiple References
</subsectionHeader>
<bodyText confidence="0.999916714285714">
So far, we only demonstrated how to compute
ROUGE-L using a single reference. When multiple
references are used, we take the maximum LCS
matches between a candidate translation, c, of n
words and a set of u reference translations of mj
words. The LCS-based F-measure can be
computed as follows:
</bodyText>
<equation confidence="0.998463823529412">
⎛LCS ( r , c ) ⎞
Rlcs-multi max 1
u
= j
⎜⎜ ⎟⎟ (7)
j =m
⎝ j ⎠
⎛ LCS rj c
( , )⎞
⎜ ⎟
⎝ n ⎠
β2 )Rlcs−mu lti lcs−multi (9)
2
R β P
lcs multi +
− lcs multi
−
</equation>
<bodyText confidence="0.999459888888889">
where ,B = Plcs-multi/Rlcs-multi when aFlcs-multi/aRlcs-
multi=aFlcs-multi/aPlcs-multi.
This procedure is also applied to computation of
ROUGE-S when multiple references are used. In
the next section, we introduce the skip-bigram co-
occurrence statistics. In the next section, we de-
scribe how to extend ROUGE-L to assign more
credits to longest common subsequences with con-
secutive words.
</bodyText>
<equation confidence="0.803827">
lcs-multi = max ju=1
(8)
Flcs-multi
(
1+
=
</equation>
<subsectionHeader confidence="0.8119585">
3.3 ROUGE-W: Weighted Longest Common
Subsequence
</subsectionHeader>
<bodyText confidence="0.998974857142857">
LCS has many nice properties as we have de-
scribed in the previous sections. Unfortunately, the
basic LCS also has a problem that it does not dif-
ferentiate LCSes of different spatial relations
within their embedding sequences. For example,
given a reference sequence X and two candidate
sequences Y1 and Y2 as follows:
</bodyText>
<listItem confidence="0.899340333333333">
X: [A B C D E F G]
Y1: [A B C D H I K]
Y2: [A H B K C I D]
</listItem>
<bodyText confidence="0.991129166666667">
Y1 and Y2 have the same ROUGE-L score. How-
ever, in this case, Y1 should be the better choice
than Y2 because Y1 has consecutive matches. To
improve the basic LCS method, we can simply re-
member the length of consecutive matches encoun-
tered so far to a regular two dimensional dynamic
program table computing LCS. We call this
weighted LCS (WLCS) and use k to indicate the
length of the current consecutive matches ending at
words xi and yj. Given two sentences X and Y, the
WLCS score of X and Y can be computed using the
following dynamic programming procedure:
</bodyText>
<equation confidence="0.988485666666667">
(1) For (i = 0; i &lt;=m; i++)
c(i,j) = 0 // initialize c-table
w(i,j) = 0 // initialize w-table
</equation>
<bodyText confidence="0.5949526">
(2) For (i = 1; i &lt;= m; i++)
For (j = 1; j &lt;= n; j++)
If xi = yj Then
// the length of consecutive matches at
// position i-1 and j-1
</bodyText>
<equation confidence="0.983245666666666">
k = w(i-1,j-1)
c(i,j) = c(i-1,j-1) + f(k+1) – f(k)
// remember the length of consecutive
// matches at position i, j
w(i,j) = k+1
Otherwise
If c(i-1,j) &gt; c(i,j-1) Then
c(i,j) = c(i-1,j)
w(i,j) = 0 // no match at i, j
Else c(i,j) = c(i,j-1)
w(i,j) = 0 // no match at i, j
(3) WLCS(X,Y) = c(m,n)
</equation>
<bodyText confidence="0.99931336">
Where c is the dynamic programming table, c(i,j)
stores the WLCS score ending at word xi of X and
yj of Y, w is the table storing the length of consecu-
tive matches ended at c table position i and j, and f
is a function of consecutive matches at the table
position, c(i,j). Notice that by providing different
weighting function f, we can parameterize the
WLCS algorithm to assign different credit to con-
secutive in-sequence matches.
The weighting function f must have the property
that f(x+y) &gt; f(x) + f(y) for any positive integers x
and y. In other words, consecutive matches are
awarded more scores than non-consecutive
matches. For example, f(k)=αk – β when k &gt;= 0,
and α, β &gt; 0. This function charges a gap penalty
of –β for each non-consecutive n-gram sequences.
Another possible function family is the polynomial
family of the form kα where α &gt; 1. However, in
order to normalize the final ROUGE-W score, we
also prefer to have a function that has a close form
inverse function. For example, f(k)=k2 has a close
form inverse function f -1(k)=k1/2. F-measure
based on WLCS can be computed as follows,
given two sequences X of length m and Y of length
n:
</bodyText>
<equation confidence="0.996449375">
Rwlcs = f —1⎛⎜WLCS(X,Y) ⎟(10)
⎝f(m) ⎠
Pwlcs = f —1⎛⎜WLCS(X,Y) ⎟(11)
⎝f (n) ⎠
(1 2)
+β R P
= wlcs wlcs
β 2P wlcs
</equation>
<bodyText confidence="0.9998976">
Where f -1 is the inverse function of f. We call the
WLCS-based F-measure, i.e. Equation 12,
ROUGE-W. Using Equation 12 and f(k)=k2 as the
weighting function, the ROUGE-W scores for se-
quences Y1 and Y2 are 0.571 and 0.286 respec-
tively. Therefore, Y1 would be ranked higher than
Y2 using WLCS. We use the polynomial function
of the form kα in the ROUGE evaluation package. In
the next section, we introduce the skip-bigram co-
occurrence statistics.
</bodyText>
<sectionHeader confidence="0.953463" genericHeader="method">
4 ROUGE-S: Skip-Bigram Co-Occurrence
Statistics
</sectionHeader>
<bodyText confidence="0.820369916666667">
Skip-bigram is any pair of words in their sen-
tence order, allowing for arbitrary gaps. Skip-
bigram co-occurrence statistics measure the over-
lap of skip-bigrams between a candidate translation
and a set of reference translations. Using the ex-
ample given in Section 3.1:
S1. police killed the gunman
S2. police kill the gunman
S3. the gunman kill police
S4. the gunman police killed
Each sentence has C(4,2)3 = 6 skip-bigrams. For
example, S1 has the following skip-bigrams:
</bodyText>
<equation confidence="0.9556232">
3 Combination: C(4,2) = 4!/(2!*2!) = 6.
Fwlcs
+
Rwlcs
(12)
</equation>
<bodyText confidence="0.998709181818182">
(“police killed”, “police the”, “police gunman”,
“killed the”, “killed gunman”, “the gunman”)
S2 has three skip-bigram matches with S1 (“po-
lice the”, “police gunman”, “the gunman”), S3 has
one skip-bigram match with S1 (“the gunman”),
and S4 has two skip-bigram matches with S1 (“po-
lice killed”, “the gunman”). Given translations X
of length m and Y of length n, assuming X is a ref-
erence translation and Y is a candidate translation,
we compute skip-bigram-based F-measure as fol-
lows:
</bodyText>
<equation confidence="0.9953172">
Rskip2 SKIP2(X,Y) (13)
= C(m,2)
Pskip2 SKIP2(X,Y) (14)
=
C(n, 2)
</equation>
<bodyText confidence="0.998454631578948">
Where SKIP2(X,Y) is the number of skip-bigram
matches between X and Y, ,B = Pskip2/Rskip2 when
aFskip2/aRskip2=aFskip2/aPskip2, and C is the combi-
nation function. We call the skip-bigram-based F-
measure, i.e. Equation 15, ROUGE-S.
Using Equation 15 with ,B = 1 and S1 as the ref-
erence, S2’s ROUGE-S score is 0.5, S3 is 0.167,
and S4 is 0.333. Therefore, S2 is better than S3 and
S4, and S4 is better than S3. This result is more
intuitive than using BLEU-2 and ROUGE-L. One
advantage of skip-bigram vs. BLEU is that it does
not require consecutive matches but is still sensi-
tive to word order. Comparing skip-bigram with
LCS, skip-bigram counts all in-order matching
word pairs while LCS only counts one longest
common subsequence.
We can limit the maximum skip distance, dskip,
between two in-order words that is allowed to form
a skip-bigram. Applying such constraint, we limit
skip-bigram formation to a fix window size. There-
fore, computation time can be reduced and hope-
fully performance can be as good as the version
without such constraint. For example, if we set dskip
to 0 then ROUGE-S is equivalent to bigram over-
lap. If we set dskip to 4 then only word pairs of at
most 4 words apart can form skip-bigrams.
Adjusting Equations 13, 14, and 15 to use maxi-
mum skip distance limit is straightforward: we
only count the skip-bigram matches, SKIP2(X,Y),
within the maximum skip distance and replace de-
nominators of Equations 13, C(m,2), and 14,
C(n,2), with the actual numbers of within distance
skip-bigrams from the reference and the candidate
respectively.
In the next section, we present the evaluations of
ROUGE-L, ROUGE-S, and compare their per-
formance with other automatic evaluation meas-
ures.
</bodyText>
<sectionHeader confidence="0.998151" genericHeader="method">
5 Evaluations
</sectionHeader>
<bodyText confidence="0.998109588235294">
One of the goals of developing automatic evalua-
tion measures is to replace labor-intensive human
evaluations. Therefore the first criterion to assess
the usefulness of an automatic evaluation measure
is to show that it correlates highly with human
judgments in different evaluation settings. How-
ever, high quality large-scale human judgments are
hard to come by. Fortunately, we have access to
eight MT systems’ outputs, their human assess-
ment data, and the reference translations from 2003
NIST Chinese MT evaluation (NIST 2002a). There
were 919 sentence segments in the corpus. We first
computed averages of the adequacy and fluency
scores of each system assigned by human evalua-
tors. For the input of automatic evaluation meth-
ods, we created three evaluation sets from the MT
outputs:
</bodyText>
<listItem confidence="0.999494">
1. Case set: The original system outputs with
case information.
2. NoCase set: All words were converted
</listItem>
<bodyText confidence="0.998685">
into lower case, i.e. no case information
was used. This set was used to examine
whether human assessments were affected
by case information since not all MT sys-
tems generate properly cased output.
</bodyText>
<listItem confidence="0.988704">
3. Stem set: All words were converted into
</listItem>
<bodyText confidence="0.95520675">
lower case and stemmed using the Porter
stemmer (Porter 1980). Since ROUGE
computed similarity on surface word
level, stemmed version allowed ROUGE
to perform more lenient matches.
To accommodate multiple references, we use a
Jackknifing procedure. Given N references, we
compute the best score over N sets of N-1 refer-
ences. The final score is the average of the N best
scores using N different sets of N-1 references.
The Jackknifing procedure is adopted since we
often need to compare system and human perform-
ance and the reference translations are usually the
only human translations available. Using this pro-
cedure, we are able to estimate average human per-
formance by averaging N best scores of one
reference vs. the rest N-1 references.
We then computed average BLEU1-124, GTM
with exponents of 1.0, 2.0, and 3.0, NIST, WER,
and PER scores over these three sets. Finally we
applied ROUGE-L, ROUGE-W with weighting
function k1.2, and ROUGE-S without skip distance
4 BLEUN computes BLEU over n-grams up to length N.
Only BLEU1, BLEU4, and BLEU12 are shown in Table 1.
</bodyText>
<equation confidence="0.8612785">
= (15)
2
2
Fskip2
Rskip2Pskip2
Rskip2 + Q Pkip
(
1+
2)
Q
</equation>
<table confidence="0.999765470588235">
Adequacy With Case Information (Case) Lower Case (NoCase) Lower Case &amp; Stemmed (Stem)
Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U
BLEU1 0.86 0.83 0.89 0.80 0.71 0.90 0.87 0.84 0.90 0.76 0.67 0.89 0.91 0.89 0.93 0.85 0.76 0.95
BLEU4 0.77 0.72 0.81 0.77 0.71 0.89 0.79 0.75 0.82 0.67 0.55 0.83 0.82 0.78 0.85 0.76 0.67 0.89
BLEU12 0.66 0.60 0.72 0.53 0.44 0.65 0.72 0.57 0.81 0.65 0.25 0.88 0.72 0.58 0.81 0.66 0.28 0.88
NIST 0.89 0.86 0.92 0.78 0.71 0.89 0.87 0.85 0.90 0.80 0.74 0.92 0.90 0.88 0.93 0.88 0.83 0.97
WER 0.47 0.41 0.53 0.56 0.45 0.74 0.43 0.37 0.49 0.66 0.60 0.82 0.48 0.42 0.54 0.66 0.60 0.81
PER 0.67 0.62 0.72 0.56 0.48 0.75 0.63 0.58 0.68 0.67 0.60 0.83 0.72 0.68 0.76 0.69 0.62 0.86
ROUGE-L 0.87 0.84 0.90 0.84 0.79 0.93 0.89 0.86 0.92 0.84 0.71 0.94 0.92 0.90 0.94 0.87 0.76 0.95
ROUGE-W 0.84 0.81 0.87 0.83 0.74 0.90 0.85 0.82 0.88 0.77 0.67 0.90 0.89 0.86 0.91 0.86 0.76 0.95
ROUGE-S* 0.85 0.81 0.88 0.83 0.76 0.90 0.90 0.88 0.93 0.82 0.70 0.92 0.95 0.93 0.97 0.85 0.76 0.94
ROUGE-S0 0.82 0.78 0.85 0.82 0.71 0.90 0.84 0.81 0.87 0.76 0.67 0.90 0.87 0.84 0.90 0.82 0.68 0.90
ROUGE-S4 0.82 0.78 0.85 0.84 0.79 0.93 0.87 0.85 0.90 0.83 0.71 0.90 0.92 0.90 0.94 0.84 0.74 0.93
ROUGE-S9 0.84 0.80 0.87 0.84 0.79 0.92 0.89 0.86 0.92 0.84 0.76 0.93 0.94 0.92 0.96 0.84 0.76 0.94
GTM10 0.82 0.79 0.85 0.79 0.74 0.83 0.91 0.89 0.94 0.84 0.79 0.93 0.94 0.92 0.96 0.84 0.79 0.92
GTM20 0.77 0.73 0.81 0.76 0.69 0.88 0.79 0.76 0.83 0.70 0.55 0.83 0.83 0.79 0.86 0.80 0.67 0.90
GTM30 0.74 0.70 0.78 0.73 0.60 0.86 0.74 0.70 0.78 0.63 0.52 0.79 0.77 0.73 0.81 0.64 0.52 0.80
Fluency With Case Information (Case) Lower Case (NoCase) Lower Case &amp; Stemmed (Stem)
Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U
BLEU1 0.81 0.75 0.86 0.76 0.62 0.90 0.73 0.67 0.79 0.70 0.62 0.81 0.70 0.63 0.77 0.79 0.67 0.90
BLEU4 0.86 0.81 0.90 0.74 0.62 0.86 0.83 0.78 0.88 0.68 0.60 0.81 0.83 0.78 0.88 0.70 0.62 0.81
BLEU12 0.87 0.76 0.93 0.66 0.33 0.79 0.93 0.81 0.97 0.78 0.44 0.94 0.93 0.84 0.97 0.80 0.49 0.94
NIST 0.81 0.75 0.87 0.74 0.62 0.86 0.70 0.64 0.77 0.68 0.60 0.79 0.68 0.61 0.75 0.77 0.67 0.88
WER 0.69 0.62 0.75 0.68 0.57 0.85 0.59 0.51 0.66 0.70 0.57 0.82 0.60 0.52 0.68 0.69 0.57 0.81
PER 0.79 0.74 0.85 0.67 0.57 0.82 0.68 0.60 0.73 0.69 0.60 0.81 0.70 0.63 0.76 0.65 0.57 0.79
ROUGE-L 0.83 0.77 0.88 0.80 0.67 0.90 0.76 0.69 0.82 0.79 0.64 0.90 0.73 0.66 0.80 0.78 0.67 0.90
ROUGE-W 0.85 0.80 0.90 0.79 0.63 0.90 0.78 0.73 0.84 0.72 0.62 0.83 0.77 0.71 0.83 0.78 0.67 0.90
ROUGE-S* 0.84 0.78 0.89 0.79 0.62 0.90 0.80 0.74 0.86 0.77 0.64 0.90 0.78 0.71 0.84 0.79 0.69 0.90
ROUGE-S0 0.87 0.81 0.91 0.78 0.62 0.90 0.83 0.78 0.88 0.71 0.62 0.82 0.82 0.77 0.88 0.76 0.62 0.90
ROUGE-S4 0.84 0.79 0.89 0.80 0.67 0.90 0.82 0.77 0.87 0.78 0.64 0.90 0.81 0.75 0.86 0.79 0.67 0.90
ROUGE-S9 0.84 0.79 0.89 0.80 0.67 0.90 0.81 0.76 0.87 0.79 0.69 0.90 0.79 0.73 0.85 0.79 0.69 0.90
GTM10 0.73 0.66 0.79 0.76 0.60 0.87 0.71 0.64 0.78 0.80 0.67 0.90 0.66 0.58 0.74 0.80 0.64 0.90
GTM20 0.86 0.81 0.90 0.80 0.67 0.90 0.83 0.77 0.88 0.69 0.62 0.81 0.83 0.77 0.87 0.74 0.62 0.89
GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83
</table>
<tableCaption confidence="0.960813">
Table 1. Pearson’s p and Spearman’s p correlations of automatic evaluation measures vs. adequacy
</tableCaption>
<bodyText confidence="0.995488173913044">
and fluency: BLEU1, 4, and 12 are BLEU with maximum of 1, 4, and 12 grams, NIST is the NIST
score, ROUGE-L is LCS-based F-measure (0 = 1), ROUGE-W is weighted LCS-based F-measure (0
= 1). ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGE-
SN is skip-bigram-based F-measure (0 = 1) with maximum skip distance of N, PER is position inde-
pendent word error rate, and WER is word error rate. GTM 10, 20, and 30 are general text matcher
with exponents of 1.0, 2.0, and 3.0. (Note, only BLEU1, 4, and 12 are shown here to preserve space.)
limit and with skip distant limits of 0, 4, and 9.
Correlation analysis based on two different correla-
tion statistics, Pearson’s p and Spearman’s p, with
respect to adequacy and fluency are shown in Ta-
ble 1.
The Pearson’s correlation coefficient5 measures the
strength and direction of a linear relationship be-
tween any two variables, i.e. automatic metric
score and human assigned mean coverage score in
our case. It ranges from +1 to -1. A correlation of 1
means that there is a perfect positive linear rela-
tionship between the two variables, a correlation of
-1 means that there is a perfect negative linear rela-
tionship between them, and a correlation of 0
means that there is no linear relationship between
them. Since we would like to use automatic
evaluation metric not only in comparing systems
</bodyText>
<footnote confidence="0.8568215">
5 For a quick overview of the Pearson’s coefficient, see:
http://davidmlane.com/hyperstat/A34739.html.
</footnote>
<bodyText confidence="0.999700235294118">
but also in in-house system development, a good
linear correlation with human judgment would en-
able us to use automatic scores to predict corre-
sponding human judgment scores. Therefore,
Pearson’s correlation coefficient is a good measure
to look at.
Spearman’s correlation coefficient 6 is also a
measure of correlation between two variables. It is
a non-parametric measure and is a special case of
the Pearson’s correlation coefficient when the val-
ues of data are converted into ranks before comput-
ing the coefficient. Spearman’s correlation
coefficient does not assume the correlation be-
tween the variables is linear. Therefore it is a use-
ful correlation indicator even when good linear
correlation, for example, according to Pearson’s
correlation coefficient between two variables could
</bodyText>
<footnote confidence="0.949833">
6 For a quick overview of the Spearman’s coefficient, see:
http://davidmlane.com/hyperstat/A62436.html.
</footnote>
<bodyText confidence="0.99988670967742">
not be found. It also suits the NIST MT evaluation
scenario where multiple systems are ranked ac-
cording to some performance metrics.
To estimate the significance of these correlation
statistics, we applied bootstrap resampling, gener-
ating random samples of the 919 different sentence
segments. The lower and upper values of 95% con-
fidence interval are also shown in the table. Dark
(green) cells are the best correlation numbers in
their categories and light gray cells are statistically
equivalent to the best numbers in their categories.
Analyzing all runs according to the adequacy and
fluency table, we make the following observations:
Applying the stemmer achieves higher correla-
tion with adequacy but keeping case information
achieves higher correlation with fluency except for
BLEU7-12 (only BLEU12 is shown). For example,
the Pearson’s p (P) correlation of ROUGE-S* with
adequacy increases from 0.85 (Case) to 0.95
(Stem) while its Pearson’s p correlation with flu-
ency drops from 0.84 (Case) to 0.78 (Stem). We
will focus our discussions on the Stem set in ade-
quacy and Case set in fluency.
The Pearson&apos;s p correlation values in the Stem
set of the Adequacy Table, indicates that ROUGE-
L and ROUGE-S with a skip distance longer than 0
correlate highly and linearly with adequacy and
outperform BLEU and NIST. ROUGE-S* achieves
that best correlation with a Pearson’s p of 0.95.
Measures favoring consecutive matches, i.e.
BLEU4 and 12, ROUGE-W, GTM20 and 30,
ROUGE-S0 (bigram), and WER have lower Pear-
son’s p. Among them WER (0.48) that tends to
penalize small word movement is the worst per-
former. One interesting observation is that longer
BLEU has lower correlation with adequacy.
Spearman’s p values generally agree with Pear-
son&apos;s p but have more equivalents.
The Pearson&apos;s p correlation values in the Stem
set of the Fluency Table, indicates that BLEU12 has
the highest correlation (0.93) with fluency. How-
ever, it is statistically indistinguishable with 95%
confidence from all other metrics shown in the
Case set of the Fluency Table except for WER and
GTM10.
GTM10 has good correlation with human judg-
ments in adequacy but not fluency; while GTM20
and GTM30, i.e. GTM with exponent larger than
1.0, has good correlation with human judgment in
fluency but not adequacy.
ROUGE-L and ROUGE-S*, 4, and 9 are good
automatic evaluation metric candidates since they
perform as well as BLEU in fluency correlation
analysis and outperform BLEU4 and 12 signifi-
cantly in adequacy. Among them, ROUGE-L is the
best metric in both adequacy and fluency correla-
tion with human judgment according to Spear-
man’s correlation coefficient and is statistically
indistinguishable from the best metrics in both
adequacy and fluency correlation with human
judgment according to Pearson’s correlation coef-
ficient.
</bodyText>
<sectionHeader confidence="0.999147" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99892376923077">
In this paper we presented two new objective
automatic evaluation methods for machine transla-
tion, ROUGE-L based on longest common subse-
quence (LCS) statistics between a candidate
translation and a set of reference translations.
Longest common subsequence takes into account
sentence level structure similarity naturally and
identifies longest co-occurring in-sequence n-
grams automatically while this is a free parameter
in BLEU.
To give proper credit to shorter common se-
quences that are ignored by LCS but still retain the
flexibility of non-consecutive matches, we pro-
posed counting skip bigram co-occurrence. The
skip-bigram-based ROUGE-S* (without skip dis-
tance restriction) had the best Pearson&apos;s p correla-
tion of 0.95 in adequacy when all words were
lower case and stemmed. ROUGE-L, ROUGE-W,
ROUGE-S*, ROUGE-S4, and ROUGE-S9 were
equal performers to BLEU in measuring fluency.
However, they have the advantage that we can ap-
ply them on sentence level while longer BLEU such
as BLEU12 would not differentiate any sentences
with length shorter than 12 words (i.e. no 12-gram
matches). We plan to explore their correlation with
human judgments on sentence-level in the future.
We also confirmed empirically that adequacy and
fluency focused on different aspects of machine
translations. Adequacy placed more emphasis on
terms co-occurred in candidate and reference trans-
lations as shown in the higher correlations in Stem
set than Case set in Table 1; while the reverse was
true in the terms of fluency.
The evaluation results of ROUGE-L, ROUGE-
W, and ROUGE-S in machine translation evalua-
tion are very encouraging. However, these meas-
ures in their current forms are still only applying
string-to-string matching. We have shown that bet-
ter correlation with adequacy can be reached by
applying stemmer. In the next step, we plan to ex-
tend them to accommodate synonyms and para-
phrases. For example, we can use an existing
thesaurus such as WordNet (Miller 1990) or creat-
ing a customized one by applying automated syno-
nym set discovery methods (Pantel and Lin 2002)
to identify potential synonyms. Paraphrases can
also be automatically acquired using statistical
methods as shown by Barzilay and Lee (2003).
Once we have acquired synonym and paraphrase
data, we then need to design a soft matching func-
tion that assigns partial credits to these approxi-
mate matches. In this scenario, statistically
generated data has the advantage of being able to
provide scores reflecting the strength of similarity
between synonyms and paraphrased.
ROUGE-L, ROUGE-W, and ROUGE-S have
also been applied in automatic evaluation of sum-
marization and achieved very promising results
(Lin 2004). In Lin and Och (2004), we proposed a
framework that automatically evaluated automatic
MT evaluation metrics using only manual transla-
tions without further human involvement. Accord-
ing to the results reported in that paper, ROUGE-L,
ROUGE-W, and ROUGE-S also outperformed
BLEU and NIST.
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988808974359">
Akiba, Y., K. Imamura, and E. Sumita. 2001. Us-
ing Multiple Edit Distances to Automatically
Rank Machine Translation Output. In Proceed-
ings of the MT Summit VIII, Santiago de Com-
postela, Spain.
Barzilay, R. and L. Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Mul-
tiple-Sequence Alignmen. In Proceeding of
NAACL-HLT 2003, Edmonton, Canada.
Leusch, G., N. Ueffing, and H. Ney. 2003. A
Novel String-to-String Distance Measure with
Applications to Machine Translation Evaluation.
In Proceedings of MT Summit IY, New Orleans,
U.S.A.
Levenshtein, V. I. 1966. Binary codes capable of
correcting deletions, insertions and reversals.
Soviet Physics Doklady.
Lin, C.Y. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proceedings of the
Workshop on Text Summarization Branches
Out, post-conference workshop of ACL 2004,
Barcelona, Spain.
Lin, C.-Y. and F. J. Och. 2004. ORANGE: a Method
for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of 20th In-
ternational Conference on Computational Lin-
guistic (COLING 2004), Geneva, Switzerland.
Miller, G. 1990. WordNet: An Online Lexical Da-
tabase. International Journal of Lexicography,
3(4).
Melamed, I.D. 1995. Automatic Evaluation and
Uniform Filter Cascades for Inducing N-best
Translation Lexicons. In Proceedings of the 3rd
Workshop on Very Large Corpora (WVLC3).
Boston, U.S.A.
Melamed, I.D., R. Green and J. P. Turian. 2003.
Precision and Recall of Machine Translation. In
Proceedings of NAACL/HLT 2003, Edmonton,
Canada.
Nießen S., F.J. Och, G, Leusch, H. Ney. 2000. An
Evaluation Tool for Machine Translation: Fast
Evaluation for MT Research. In Proceedings of
the 2nd International Conference on Language
Resources and Evaluation, Athens, Greece.
NIST. 2002. Automatic Evaluation of Machine
Translation Quality using N-gram Co-
Occurrence Statistics.
http://www.nist.gov/speech/tests/mt/doc/ngram-
study.pdf
Pantel, P. and Lin, D. 2002. Discovering Word
Senses from Text. In Proceedings of SIGKDD-
02. Edmonton, Canada.
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu.
2001. BLEU: a Method for Automatic Evaluation
of Machine Translation. IBM Research Report
RC22176 (W0109-022).
Porter, M.F. 1980. An Algorithm for Suffix Strip-
ping. Program, 14, pp. 130-137.
Saggion H., D. Radev, S. Teufel, and W. Lam.
2002. Meta-Evaluation of Summaries in a
Cross-Lingual Environment Using Content-
Based Metrics. In Proceedings of COLING-
2002, Taipei, Taiwan.
Su, K.-Y., M.-W. Wu, and J.-S. Chang. 1992. A
New Quantitative Quality Measure for Machine
Translation System. In Proceedings of
COLING-92, Nantes, France.
Thompson, H. S. 1991. Automatic Evaluation of
Translation Quality: Outline of Methodology
and Report on Pilot Experiment. In Proceedings
of the Evaluator’s Forum, ISSCO, Geneva,
Switzerland.
Turian, J. P., L. Shen, and I. D. Melamed. 2003.
Evaluation of Machine Translation and its
Evaluation. In Proceedings of MT Summit IY,
New Orleans, U.S.A.
Van Rijsbergen, C.J. 1979. Information Retrieval.
Butterworths. London.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9672525">Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics</title>
<author confidence="0.998777">Chin-Yew Lin</author>
<author confidence="0.998777">Franz Josef Och</author>
<affiliation confidence="0.9986745">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.99878">4676 Admiralty Way Marina del Rey, CA 90292, USA</address>
<email confidence="0.999788">cyl@isi.edu</email>
<email confidence="0.999788">och@isi.edu</email>
<abstract confidence="0.991810671206226">In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring insequence n-grams automatically. The second method relaxes strict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measproposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n &gt; 1) matches achieved similar performance as Bleu. Since unigram matches do not distinguish words in consecutive positions from words in the wrong order, measures based on position-independent unigram matches are not sensitive to word order and sentence level structure. Therefore, systems optimized for these unigram-based measures might generate adequate but not fluent target language. been used to report the performance of many machine translation systems and it has been shown to correlate well with human we will explain more detail and point out its limitations in the next section. We then introduce a new evaluation method called ROUGE-L that measures sentence-to-sentence similarity based on the longest common subsequence statistics between a candidate translation and a set of reference translations in Section 3. Section 4 describes another automatic evaluation method called ROUGE-S that computes skipbigram co-occurrence statistics. Section 5 presents the evaluation results of ROUGE-L, and ROUGEand compare them with GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency. We conclude this paper and discuss extensions of the current work in Section 6. N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring proceet al. 2001). In two recent large-scale machine translation evaluations sponby NIST, a closely related automatic evaluation method, simply called NIST score, was used. The NIST (NIST 2002) scoring method is based on main idea of to measure the similarity between a candidate translation and a set of reference translations with a numerical metric. They used a weighted average of variable length ngram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments. how well a machine translation overlaps with multiple human translations using ngram co-occurrence statistics. N-gram precision in computed as follows: ∑ n ∑ n gram ( C Candidates } gram C − ∈ is the maximum numof co-occurring in a candidate translaand a reference translation, and is the number of in the candidate translation. To prevent very short translations that to maximize their precision scores, a penalty, to the formula: c r &gt; ⎫ ⎬ (1   ||/  ||) c c r ≤ ⎭ is the length of the candidate translaand is the length of the reference transla- The is then written as follows: N ⎛ ⎞ weighting factor, is set at been shown to correlate well with human assessments, it has a few things that can be improved. First the subjective application of the brevity penalty can be replaced with a recall related parameter that is sensitive to reference length. Although brevity penalty will penalize cantranslations with low recall by a factor of it would be nice if we can use the traditional recall measure that has been a well known measure in NLP as suggested by Melamed (2003). Of course we have to make sure the resulting composite function of precision and recall is still correlates highly with human judgments. although high order n-gram (n&gt;1) matches to favor candidate sentences with consecutive word matches and to estimate their fluency, it does not consider sentence level structure. For example, given the following sentences: S1. police killed the gunman police kill the S3. the gunman kill police only consider unigram and bii.e. for the purpose of explanation and this Using S1 as the reference and S2 and S3 as the candidate translations, S2 and S3 have the same score, since they have one bigram and three unigram However, S2 and S3 have very different meanings. a geometric mean of unigram to N-gram precisions. Any candidate translation a N-gram match has a per-sentence of zero. Although usually calculated over the whole test corpus, it is still desirable to have a measure that works reliably at sentence level for diagnostic and introspection purpose. To address these issues, we propose three new automatic evaluation measures based on longest common subsequence statistics and skip bigram co-occurrence statistics in the following sections. Common Subsequence 3.1 ROUGE-L sequence ..., is a subsequence of sequence ..., if there exists strict increasing sequence ..., of indices that for all 2, ..., we have et al. 1989). Given two sequences the longest common subsequence (LCS) of a common subsequence with maximum length. We can find the LCS of two sequences of standard dynamic programtechnique in time. LCS has been used to identify cognate candidates during construction of N-best translation lexicons from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS (NP-LCS) to compare similarity between two texts in automatic summarization evaluation. NP-LCS can be shown as a special case Equation (6) with 1. However, they did not provide the correlation analysis of NP-LCS with is a real machine translation output. “kill” in S2 or S3 does not match with “killed” in S1 in strict word-to-word comparison. BP e ⎧ ⎨ ⎩ ( 2 ) ( 3 ) human judgments and its effectiveness as an automatic evaluation measure. To apply LCS in machine translation evaluation, we view a translation as a sequence of words. The intuition is that the longer the LCS of two translations is, the more similar the two translations are. We propose using LCS-based F-measure to estithe similarity between two translations length assuming a refertranslation and a candidate translation, as follows: = m = n S1. police killed the gunman S2. policekill gunman the gunmankill police we have shown earlier, cannot differentiate S2 from S3. However, S2 has a ROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE-L score 2/4 = 0.5, with 1. Therefore S2 is better than S3 according to ROUGE-L. This example also illustrated that ROUGE-L can work reliably at sentence level. However, LCS only counts the main in-sequence words; therefore, other longest common subsequences and shorter sequences are not reflected in the final score. For example, consider the following candidate sentence: (6) + RP lcs lcs = gunman police killed is the length of a longest common of and We call the LCS-based Fmeasure, i.e. Equation 6, ROUGE-L. Notice that is 1 when = while ROUGE-L is zero when = 0, i.e. is nothing in common between Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen 1979). The composite factors are LCS-based recall and precision in this case. Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram was as good as One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as ngrams. The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary. ROUGE-L as defined in Equation 6 has the property that its value is less than or equal to the miniof unigram F-measure of Unigram reflects the proportion of words in (refertranslation) that are also present in (candidate translation); while unigram precision is the of words in are also in Unigram recall and precision count all co-occurring words regardless their orders; while ROUGE-L counts only in-sequence co-occurrences. By only awarding credit to in-sequence unigram matches, ROUGE-L also captures sentence level structure in a natural way. Consider again the example given in Section 2 that is copied here for convenience: Using S1 as its reference, LCS counts either “the gunman” or “police killed”, but not both; therefore, has the same ROUGE-L score as S3. would prefer S4 over S3. In Section 4, we will introduce skip-bigram co-occurrence statistics that do not have this problem while still keeping the advantage of in-sequence (not necessary consecutive) matching that reflects sentence level word order. 3.2 Multiple References So far, we only demonstrated how to compute ROUGE-L using a single reference. When multiple references are used, we take the maximum LCS between a candidate translation, of and a set of translations of words. The LCS-based F-measure can be computed as follows: u ⎟⎟ c , ⎜ ⎟ lti (9) 2 multi multi − when This procedure is also applied to computation of ROUGE-S when multiple references are used. In the next section, we introduce the skip-bigram cooccurrence statistics. In the next section, we describe how to extend ROUGE-L to assign more credits to longest common subsequences with consecutive words. (8) ( = 3.3 ROUGE-W: Weighted Longest Common Subsequence LCS has many nice properties as we have described in the previous sections. Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences. For example, a reference sequence two candidate follows: [A B C D E F G] Y1: [A B C D H I K] Y2: [A H B K C I D] the same score. Howin this case, be the better choice consecutive matches. To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS. We call this LCS (WLCS) and use indicate the length of the current consecutive matches ending at Given two sentences the score of be computed using the following dynamic programming procedure: For 0; = 0 // c-table = 0 // w-table For 1; 1; length of consecutive matches at i-1 and j-1 = + – the length of consecutive at position i, j = Otherwise &gt; Then = = 0 // match at j = = 0 // match at (3) = the dynamic programming table, the WLCS score ending at word the table storing the length of consecumatches ended at position and is a function of consecutive matches at the table Notice that by providing different function we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches. weighting function have the property &gt; + for any positive integers In other words, consecutive matches are awarded more scores than non-consecutive For example, – 0, 0. This function charges a gap penalty each non-consecutive n-gram sequences. Another possible function family is the polynomial of the form where 1. However, in order to normalize the final ROUGE-W score, we also prefer to have a function that has a close form function. For example, has a close inverse function F-measure based on WLCS can be computed as follows, two sequences length length RP wlcs -1is the inverse function of We call the WLCS-based F-measure, i.e. Equation 12, Using Equation 12 and as the weighting function, the ROUGE-W scores for se- 0.571 and 0.286 respec- Therefore, be ranked higher than WLCS. We use the polynomial function the form in the package. In the next section, we introduce the skip-bigram cooccurrence statistics. Skip-Bigram Co-Occurrence Statistics Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps. Skipbigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. Using the example given in Section 3.1: S1. police killed the gunman S2. police kill the gunman S3. the gunman kill police S4. the gunman police killed sentence has = 6 skip-bigrams. For example, S1 has the following skip-bigrams: = 4!/(2!*2!) = 6. + (12) (“police killed”, “police the”, “police gunman”, “killed the”, “killed gunman”, “the gunman”) has three skip-bigram matches with S1 S3 has skip-bigram match with S1 S4 has two skip-bigram matches with S1 Given translations length length assuming a reftranslation and a candidate translation, we compute skip-bigram-based F-measure as follows: (13) (14) = 2) is the number of skip-bigram between and the combination function. We call the skip-bigram-based Fmeasure, i.e. Equation 15, ROUGE-S. Equation 15 with 1 and S1 as the reference, S2’s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333. Therefore, S2 is better than S3 and S4, and S4 is better than S3. This result is more than using and ROUGE-L. One of skip-bigram vs. that it does not require consecutive matches but is still sensitive to word order. Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence. can limit the maximum skip distance, between two in-order words that is allowed to form a skip-bigram. Applying such constraint, we limit skip-bigram formation to a fix window size. Therefore, computation time can be reduced and hopefully performance can be as good as the version such constraint. For example, if we set to 0 then ROUGE-S is equivalent to bigram over- If we set to 4 then only word pairs of at most 4 words apart can form skip-bigrams. Adjusting Equations 13, 14, and 15 to use maximum skip distance limit is straightforward: we count the skip-bigram matches, within the maximum skip distance and replace deof Equations 13, and 14, with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively. In the next section, we present the evaluations of ROUGE-L, ROUGE-S, and compare their performance with other automatic evaluation measures. One of the goals of developing automatic evaluation measures is to replace labor-intensive human evaluations. Therefore the first criterion to assess the usefulness of an automatic evaluation measure is to show that it correlates highly with human judgments in different evaluation settings. However, high quality large-scale human judgments are hard to come by. Fortunately, we have access to eight MT systems’ outputs, their human assessment data, and the reference translations from 2003 NIST Chinese MT evaluation (NIST 2002a). There were 919 sentence segments in the corpus. We first computed averages of the adequacy and fluency scores of each system assigned by human evaluators. For the input of automatic evaluation methods, we created three evaluation sets from the MT outputs: 1. Case set: The original system outputs with case information. 2. NoCase set: All words were converted into lower case, i.e. no case information was used. This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output. 3. Stem set: All words were converted into lower case and stemmed using the Porter stemmer (Porter 1980). Since ROUGE computed similarity on surface word level, stemmed version allowed ROUGE to perform more lenient matches. To accommodate multiple references, we use a Jackknifing procedure. Given N references, we compute the best score over N sets of N-1 references. The final score is the average of the N best scores using N different sets of N-1 references. The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference translations are usually the only human translations available. Using this procedure, we are able to estimate average human performance by averaging N best scores of one reference vs. the rest N-1 references. then computed average GTM with exponents of 1.0, 2.0, and 3.0, NIST, WER, and PER scores over these three sets. Finally we applied ROUGE-L, ROUGE-W with weighting and ROUGE-S without skip distance computes n-grams up to length N. and are shown in Table 1. 2 ( Q</abstract>
<note confidence="0.7188979375">Adequacy With Case Information (Case) Lower Case (NoCase) Lower Case &amp; Stemmed (Stem) Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U BLEU1 0.86 0.83 0.89 0.80 0.71 0.90 0.87 0.84 0.90 0.76 0.67 0.89 0.91 0.89 0.93 0.85 0.76 0.95 BLEU4 0.77 0.72 0.81 0.77 0.71 0.89 0.79 0.75 0.82 0.67 0.55 0.83 0.82 0.78 0.85 0.76 0.67 0.89 BLEU12 0.66 0.60 0.72 0.53 0.44 0.65 0.72 0.57 0.81 0.65 0.25 0.88 0.72 0.58 0.81 0.66 0.28 0.88 NIST 0.89 0.86 0.92 0.78 0.71 0.89 0.87 0.85 0.90 0.80 0.74 0.92 0.90 0.88 0.93 0.88 0.83 0.97 WER 0.47 0.41 0.53 0.56 0.45 0.74 0.43 0.37 0.49 0.66 0.60 0.82 0.48 0.42 0.54 0.66 0.60 0.81 PER 0.67 0.62 0.72 0.56 0.48 0.75 0.63 0.58 0.68 0.67 0.60 0.83 0.72 0.68 0.76 0.69 0.62 0.86 ROUGE-L 0.87 0.84 0.90 0.84 0.79 0.93 0.89 0.86 0.92 0.84 0.71 0.94 0.92 0.90 0.94 0.87 0.76 0.95 ROUGE-W 0.84 0.81 0.87 0.83 0.74 0.90 0.85 0.82 0.88 0.77 0.67 0.90 0.89 0.86 0.91 0.86 0.76 0.95 ROUGE-S* 0.85 0.81 0.88 0.83 0.76 0.90 0.90 0.88 0.93 0.82 0.70 0.92 0.95 0.93 0.97 0.85 0.76 0.94 ROUGE-S0 0.82 0.78 0.85 0.82 0.71 0.90 0.84 0.81 0.87 0.76 0.67 0.90 0.87 0.84 0.90 0.82 0.68 0.90 ROUGE-S4 0.82 0.78 0.85 0.84 0.79 0.93 0.87 0.85 0.90 0.83 0.71 0.90 0.92 0.90 0.94 0.84 0.74 0.93 ROUGE-S9 0.84 0.80 0.87 0.84 0.79 0.92 0.89 0.86 0.92 0.84 0.76 0.93 0.94 0.92 0.96 0.84 0.76 0.94 GTM10 0.82 0.79 0.85 0.79 0.74 0.83 0.91 0.89 0.94 0.84 0.79 0.93 0.94 0.92 0.96 0.84 0.79 0.92 GTM20 0.77 0.73 0.81 0.76 0.69 0.88 0.79 0.76 0.83 0.70 0.55 0.83 0.83 0.79 0.86 0.80 0.67 0.90 GTM30 0.74 0.70 0.78 0.73 0.60 0.86 0.74 0.70 0.78 0.63 0.52 0.79 0.77 0.73 0.81 0.64 0.52 0.80 Fluency With Case Information (Case) Lower Case (NoCase) Lower Case &amp; Stemmed (Stem) Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U BLEU1 0.81 0.75 0.86 0.76 0.62 0.90 0.73 0.67 0.79 0.70 0.62 0.81 0.70 0.63 0.77 0.79 0.67 0.90 BLEU4 0.86 0.81 0.90 0.74 0.62 0.86 0.83 0.78 0.88 0.68 0.60 0.81 0.83 0.78 0.88 0.70 0.62 0.81 BLEU12 0.87 0.76 0.93 0.66 0.33 0.79 0.93 0.81 0.97 0.78 0.44 0.94 0.93 0.84 0.97 0.80 0.49 0.94 NIST 0.81 0.75 0.87 0.74 0.62 0.86 0.70 0.64 0.77 0.68 0.60 0.79 0.68 0.61 0.75 0.77 0.67 0.88 WER 0.69 0.62 0.75 0.68 0.57 0.85 0.59 0.51 0.66 0.70 0.57 0.82 0.60 0.52 0.68 0.69 0.57 0.81 PER 0.79 0.74 0.85 0.67 0.57 0.82 0.68 0.60 0.73 0.69 0.60 0.81 0.70 0.63 0.76 0.65 0.57 0.79 ROUGE-L 0.83 0.77 0.88 0.80 0.67 0.90 0.76 0.69 0.82 0.79 0.64 0.90 0.73 0.66 0.80 0.78 0.67 0.90 ROUGE-W 0.85 0.80 0.90 0.79 0.63 0.90 0.78 0.73 0.84 0.72 0.62 0.83 0.77 0.71 0.83 0.78 0.67 0.90 ROUGE-S* 0.84 0.78 0.89 0.79 0.62 0.90 0.80 0.74 0.86 0.77 0.64 0.90 0.78 0.71 0.84 0.79 0.69 0.90 ROUGE-S0 0.87 0.81 0.91 0.78 0.62 0.90 0.83 0.78 0.88 0.71 0.62 0.82 0.82 0.77 0.88 0.76 0.62 0.90 ROUGE-S4 0.84 0.79 0.89 0.80 0.67 0.90 0.82 0.77 0.87 0.78 0.64 0.90 0.81 0.75 0.86 0.79 0.67 0.90 ROUGE-S9 0.84 0.79 0.89 0.80 0.67 0.90 0.81 0.76 0.87 0.79 0.69 0.90 0.79 0.73 0.85 0.79 0.69 0.90 GTM10 0.73 0.66 0.79 0.76 0.60 0.87 0.71 0.64 0.78 0.80 0.67 0.90 0.66 0.58 0.74 0.80 0.64 0.90</note>
<phone confidence="0.337835">GTM20 0.86 0.81 0.90 0.80 0.67 0.90 0.83 0.77 0.88 0.69 0.62 0.81 0.83 0.77 0.87 0.74 0.62 0.89</phone>
<abstract confidence="0.99687675">GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83 1. Pearson’s Spearman’s of automatic evaluation measures vs. 4, and 12 are maximum of 1, 4, and 12 grams, NIST is the NIST ROUGE-L is LCS-based F-measure 1), ROUGE-W is weighted LCS-based F-measure = 1). ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGEis skip-bigram-based F-measure 1) with maximum skip distance of N, PER is position independent word error rate, and WER is word error rate. GTM 10, 20, and 30 are general text matcher exponents of 1.0, 2.0, and 3.0. (Note, only 4, and 12 are shown here to preserve space.) limit and with skip distant limits of 0, 4, and 9. Correlation analysis based on two different correlastatistics, Pearson’s Spearman’s with respect to adequacy and fluency are shown in Table 1. Pearson’s correlation measures the and direction of a between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case. It ranges from +1 to -1. A correlation of 1 means that there is a perfect positive linear relationship between the two variables, a correlation of -1 means that there is a perfect negative linear relationship between them, and a correlation of 0 means that there is no linear relationship between them. Since we would like to use automatic evaluation metric not only in comparing systems a quick overview of the Pearson’s coefficient, see: http://davidmlane.com/hyperstat/A34739.html. but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores. Therefore, Pearson’s correlation coefficient is a good measure to look at. correlation coefficient 6is also a measure of correlation between two variables. It is a non-parametric measure and is a special case of the Pearson’s correlation coefficient when the values of data are converted into ranks before computing the coefficient. Spearman’s correlation coefficient does not assume the correlation between the variables is linear. Therefore it is a useful correlation indicator even when good linear correlation, for example, according to Pearson’s correlation coefficient between two variables could a quick overview of the Spearman’s coefficient, see: http://davidmlane.com/hyperstat/A62436.html. not be found. It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics. To estimate the significance of these correlation statistics, we applied bootstrap resampling, generating random samples of the 919 different sentence segments. The lower and upper values of 95% confidence interval are also shown in the table. Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories. Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for (only is shown). For example, Pearson’s correlation of ROUGE-S* with adequacy increases from 0.85 (Case) to 0.95 while its Pearson’s with fluency drops from 0.84 (Case) to 0.78 (Stem). We will focus our discussions on the Stem set in adequacy and Case set in fluency. Pearson&apos;s values in the Stem set of the Adequacy Table, indicates that ROUGE- L and ROUGE-S with a skip distance longer than 0 correlate highly and linearly with adequacy and NIST. ROUGE-S* achieves best correlation with a Pearson’s 0.95. Measures favoring consecutive matches, i.e. and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pear- Among them WER (0.48) that tends to penalize small word movement is the worst performer. One interesting observation is that longer lower correlation with adequacy. generally agree with Pearhave more equivalents. Pearson&apos;s values in the Stem of the Fluency Table, indicates that has the highest correlation (0.93) with fluency. However, it is statistically indistinguishable with 95% confidence from all other metrics shown in the Case set of the Fluency Table except for WER and GTM10. GTM10 has good correlation with human judgments in adequacy but not fluency; while GTM20 and GTM30, i.e. GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy. ROUGE-L and ROUGE-S*, 4, and 9 are good automatic evaluation metric candidates since they as well as fluency correlation and outperform and 12 significantly in adequacy. Among them, ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spearman’s correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearson’s correlation coefficient. In this paper we presented two new objective automatic evaluation methods for machine translation, ROUGE-L based on longest common subsequence (LCS) statistics between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence ngrams automatically while this is a free parameter To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches, we proposed counting skip bigram co-occurrence. The skip-bigram-based ROUGE-S* (without skip disrestriction) had the best Pearson&apos;s correlation of 0.95 in adequacy when all words were lower case and stemmed. ROUGE-L, ROUGE-W, ROUGE-S*, ROUGE-S4, and ROUGE-S9 were performers to measuring fluency. However, they have the advantage that we can apthem on sentence level while longer would not differentiate any sentences with length shorter than 12 words (i.e. no 12-gram matches). We plan to explore their correlation with human judgments on sentence-level in the future. We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations. Adequacy placed more emphasis on terms co-occurred in candidate and reference translations as shown in the higher correlations in Stem set than Case set in Table 1; while the reverse was true in the terms of fluency. The evaluation results of ROUGE-L, ROUGE- W, and ROUGE-S in machine translation evaluation are very encouraging. However, these measures in their current forms are still only applying string-to-string matching. We have shown that better correlation with adequacy can be reached by applying stemmer. In the next step, we plan to extend them to accommodate synonyms and paraphrases. For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches. In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased. ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed NIST.</abstract>
<note confidence="0.578101045454546">References Akiba, Y., K. Imamura, and E. Sumita. 2001. Using Multiple Edit Distances to Automatically Machine Translation Output. In Proceedof the MT Summit Santiago de Compostela, Spain. Barzilay, R. and L. Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using Mul- Alignmen. In of Edmonton, Canada. Leusch, G., N. Ueffing, and H. Ney. 2003. A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation. of MT Summit New Orleans, U.S.A. Levenshtein, V. I. 1966. Binary codes capable of correcting deletions, insertions and reversals. Physics C.Y. 2004. A Package for Automatic of Summaries. In of the Workshop on Text Summarization Branches post-conference workshop of ACL 2004,</note>
<address confidence="0.736816">Barcelona, Spain.</address>
<note confidence="0.554289">C.-Y. and F. J. Och. 2004. a Method</note>
<title confidence="0.881737">for Evaluating Automatic Evaluation Metrics for</title>
<author confidence="0.753356">In of In-</author>
<affiliation confidence="0.977061">ternational Conference on Computational Lin-</affiliation>
<address confidence="0.979857">2004), Geneva, Switzerland.</address>
<note confidence="0.67336385">Miller, G. 1990. WordNet: An Online Lexical Da- Journal of 3(4). Melamed, I.D. 1995. Automatic Evaluation and Uniform Filter Cascades for Inducing N-best Lexicons. In of the on Very Large Corpora Boston, U.S.A. Melamed, I.D., R. Green and J. P. Turian. 2003. and Recall of Machine of NAACL/HLT Edmonton, Canada. Nießen S., F.J. Och, G, Leusch, H. Ney. 2000. An Evaluation Tool for Machine Translation: Fast for MT Research. In of the 2nd International Conference on Language and Athens, Greece. NIST. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co- Occurrence Statistics.</note>
<web confidence="0.984249">http://www.nist.gov/speech/tests/mt/doc/ngram-</web>
<email confidence="0.660553">study.pdf</email>
<author confidence="0.762569">P Pantel</author>
<author confidence="0.762569">D Lin</author>
<affiliation confidence="0.452773">from Text. In of SIGKDD- Edmonton, Canada. Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. a Method for Automatic Evaluation</affiliation>
<note confidence="0.6474606">Machine IBM Research Report Porter, M.F. 1980. An Algorithm for Suffix Strip- 14, pp. 130-137. Saggion H., D. Radev, S. Teufel, and W. Lam. 2002. Meta-Evaluation of Summaries in a</note>
<affiliation confidence="0.665969">Cross-Lingual Environment Using Content- Metrics. In of COLING-</affiliation>
<address confidence="0.901518">Taipei, Taiwan. Su, K.-Y., M.-W. Wu, and J.-S. Chang. 1992. A</address>
<title confidence="0.702383">New Quantitative Quality Measure for Machine System. In of</title>
<address confidence="0.7436985">Nantes, France. Thompson, H. S. 1991. Automatic Evaluation of</address>
<note confidence="0.567695">Translation Quality: Outline of Methodology Report on Pilot Experiment. In the Evaluator’s ISSCO, Geneva, Switzerland. Turian, J. P., L. Shen, and I. D. Melamed. 2003. Evaluation of Machine Translation and its In of MT Summit New Orleans, U.S.A. Rijsbergen, C.J. 1979. Butterworths. London.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Akiba</author>
<author>K Imamura</author>
<author>E Sumita</author>
</authors>
<title>Using Multiple Edit Distances to Automatically Rank Machine Translation Output.</title>
<date>2001</date>
<booktitle>In Proceedings of the MT Summit VIII, Santiago de Compostela,</booktitle>
<contexts>
<context position="1318" citStr="Akiba et al. (2001)" startWordPosition="187" endWordPosition="190">rict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An </context>
</contexts>
<marker>Akiba, Imamura, Sumita, 2001</marker>
<rawString>Akiba, Y., K. Imamura, and E. Sumita. 2001. Using Multiple Edit Distances to Automatically Rank Machine Translation Output. In Proceedings of the MT Summit VIII, Santiago de Compostela, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignmen.</title>
<date>2003</date>
<booktitle>In Proceeding of NAACL-HLT 2003,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="31819" citStr="Barzilay and Lee (2003)" startWordPosition="5410" endWordPosition="5413">on evaluation are very encouraging. However, these measures in their current forms are still only applying string-to-string matching. We have shown that better correlation with adequacy can be reached by applying stemmer. In the next step, we plan to extend them to accommodate synonyms and paraphrases. For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches. In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased. ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual tra</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Barzilay, R. and L. Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignmen. In Proceeding of NAACL-HLT 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IY,</booktitle>
<location>New Orleans, U.S.A.</location>
<contexts>
<context position="1548" citStr="Leusch et al. (2003)" startWordPosition="223" endWordPosition="226"> translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two r</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2003</marker>
<rawString>Leusch, G., N. Ueffing, and H. Ney. 2003. A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation. In Proceedings of MT Summit IY, New Orleans, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady.</title>
<date>1966</date>
<contexts>
<context position="1252" citStr="Levenshtein 1966" startWordPosition="180" endWordPosition="181">g insequence n-grams automatically. The second method relaxes strict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the numbe</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Levenshtein, V. I. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out, post-conference workshop of ACL 2004,</booktitle>
<location>Barcelona,</location>
<marker>Lin, 2004</marker>
<rawString>Lin, C.Y. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of the Workshop on Text Summarization Branches Out, post-conference workshop of ACL 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of 20th International Conference on Computational Linguistic (COLING 2004),</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Lin, Och, 2004</marker>
<rawString>Lin, C.-Y. and F. J. Och. 2004. ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation. In Proceedings of 20th International Conference on Computational Linguistic (COLING 2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: An Online Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="31575" citStr="Miller 1990" startWordPosition="5374" endWordPosition="5375">date and reference translations as shown in the higher correlations in Stem set than Case set in Table 1; while the reverse was true in the terms of fluency. The evaluation results of ROUGE-L, ROUGEW, and ROUGE-S in machine translation evaluation are very encouraging. However, these measures in their current forms are still only applying string-to-string matching. We have shown that better correlation with adequacy can be reached by applying stemmer. In the next step, we plan to extend them to accommodate synonyms and paraphrases. For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches. In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased. ROUGE-L, ROUGE-W, and ROUGE</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, G. 1990. WordNet: An Online Lexical Database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Automatic Evaluation and Uniform Filter Cascades for Inducing N-best Translation Lexicons.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora (WVLC3).</booktitle>
<location>Boston, U.S.A.</location>
<contexts>
<context position="1913" citStr="Melamed (1995)" startWordPosition="280" endWordPosition="281">Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while </context>
<context position="7944" citStr="Melamed (1995)" startWordPosition="1278" endWordPosition="1279">quence Z = [z1, z2, ..., zn] is a subsequence of another sequence X = [x1, x2, ..., xm], if there exists a strict increasing sequence [i1, i2, ..., ik] of indices of X such that for all j = 1, 2, ..., k, we have xij = zj (Cormen et al. 1989). Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length. We can find the LCS of two sequences of length m and n using standard dynamic programming technique in O(mn) time. LCS has been used to identify cognate candidates during construction of N-best translation lexicons from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS (NP-LCS) to compare similarity between two texts in automatic summarization evaluation. NP-LCS can be shown as a special case of Equation (6) with β = 1. However, they did not provide the correlation analysis of NP-LCS with 1 This is a real machine translation output. 2 The “kill” in S2 or S3 does not match with “killed” in S1 in strict</context>
</contexts>
<marker>Melamed, 1995</marker>
<rawString>Melamed, I.D. 1995. Automatic Evaluation and Uniform Filter Cascades for Inducing N-best Translation Lexicons. In Proceedings of the 3rd Workshop on Very Large Corpora (WVLC3). Boston, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
<author>R Green</author>
<author>J P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL/HLT 2003,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="10368" citStr="Melamed et al. (2003)" startWordPosition="1721" endWordPosition="1724">s = Rlcs S4. the gunman police killed Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and ,B = Plcs/Rlcs when aFlcs/aRlcs=aFlcs/aPlcs. We call the LCS-based Fmeasure, i.e. Equation 6, ROUGE-L. Notice that ROUGE-L is 1 when X = Y since LCS(X,Y) = m or n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen 1979). The composite factors are LCS-based recall and precision in this case. Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F-measure was as good as BLEU. One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as ngrams. The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary. ROUGE-L as defined in Equation 6 has the property that its value is less than or equal to the minimum of unigram F-measure of X and Y. Unigram recall reflects the proportion of words in X (r</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>Melamed, I.D., R. Green and J. P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of NAACL/HLT 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1393" citStr="Nießen et al. (2000)" startWordPosition="200" endWordPosition="203">rds in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Nießen S., F.J. Och, G, Leusch, H. Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality using N-gram CoOccurrence Statistics.</title>
<date>2002</date>
<note>http://www.nist.gov/speech/tests/mt/doc/ngramstudy.pdf</note>
<contexts>
<context position="2125" citStr="NIST (2002)" startWordPosition="312" endWordPosition="313">translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n &gt; 1) matches achieved similar performance as Bleu. Since unigram matches do not distinguish words in consecutive positions from words in the wrong order, measures based o</context>
<context position="4186" citStr="NIST 2002" startWordPosition="627" endWordPosition="628">lts of ROUGE-L, and ROUGES and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency. We conclude this paper and discuss extensions of the current work in Section 6. 2 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001). In two recent large-scale machine translation evaluations sponsored by NIST, a closely related automatic evaluation method, simply called NIST score, was used. The NIST (NIST 2002) scoring method is based on BLEU. The main idea of BLEU is to measure the similarity between a candidate translation and a set of reference translations with a numerical metric. They used a weighted average of variable length ngram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments. BLEU measures how well a machine translation overlaps with multiple human translations using ngram co-occurrence statistics. N-gram precision in BLEU is computed as follows: ∑ ∑ Countclip (n−gram) p = C∈</context>
<context position="19362" citStr="NIST 2002" startWordPosition="3295" endWordPosition="3296">ompare their performance with other automatic evaluation measures. 5 Evaluations One of the goals of developing automatic evaluation measures is to replace labor-intensive human evaluations. Therefore the first criterion to assess the usefulness of an automatic evaluation measure is to show that it correlates highly with human judgments in different evaluation settings. However, high quality large-scale human judgments are hard to come by. Fortunately, we have access to eight MT systems’ outputs, their human assessment data, and the reference translations from 2003 NIST Chinese MT evaluation (NIST 2002a). There were 919 sentence segments in the corpus. We first computed averages of the adequacy and fluency scores of each system assigned by human evaluators. For the input of automatic evaluation methods, we created three evaluation sets from the MT outputs: 1. Case set: The original system outputs with case information. 2. NoCase set: All words were converted into lower case, i.e. no case information was used. This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output. 3. Stem set: All words were converted </context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic Evaluation of Machine Translation Quality using N-gram CoOccurrence Statistics. http://www.nist.gov/speech/tests/mt/doc/ngramstudy.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGKDD02.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="31678" citStr="Pantel and Lin 2002" startWordPosition="5390" endWordPosition="5393">n Table 1; while the reverse was true in the terms of fluency. The evaluation results of ROUGE-L, ROUGEW, and ROUGE-S in machine translation evaluation are very encouraging. However, these measures in their current forms are still only applying string-to-string matching. We have shown that better correlation with adequacy can be reached by applying stemmer. In the next step, we plan to extend them to accommodate synonyms and paraphrases. For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches. In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased. ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results </context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of SIGKDD02. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Report</journal>
<volume>22176</volume>
<pages>0109--022</pages>
<contexts>
<context position="1988" citStr="Papineni et al. (2001)" startWordPosition="290" endWordPosition="293">ences. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n &gt; 1) matches achieved similar per</context>
<context position="4004" citStr="Papineni et al. 2001" startWordPosition="597" endWordPosition="600">rence translations in Section 3. Section 4 describes another automatic evaluation method called ROUGE-S that computes skipbigram co-occurrence statistics. Section 5 presents the evaluation results of ROUGE-L, and ROUGES and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency. We conclude this paper and discuss extensions of the current work in Section 6. 2 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001). In two recent large-scale machine translation evaluations sponsored by NIST, a closely related automatic evaluation method, simply called NIST score, was used. The NIST (NIST 2002) scoring method is based on BLEU. The main idea of BLEU is to measure the similarity between a candidate translation and a set of reference translations with a numerical metric. They used a weighted average of variable length ngram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments. BLEU measures how wel</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2001. BLEU: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176 (W0109-022).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An Algorithm for Suffix</title>
<date>1980</date>
<journal>Stripping. Program,</journal>
<volume>14</volume>
<pages>130--137</pages>
<contexts>
<context position="20028" citStr="Porter 1980" startWordPosition="3406" endWordPosition="3407"> first computed averages of the adequacy and fluency scores of each system assigned by human evaluators. For the input of automatic evaluation methods, we created three evaluation sets from the MT outputs: 1. Case set: The original system outputs with case information. 2. NoCase set: All words were converted into lower case, i.e. no case information was used. This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output. 3. Stem set: All words were converted into lower case and stemmed using the Porter stemmer (Porter 1980). Since ROUGE computed similarity on surface word level, stemmed version allowed ROUGE to perform more lenient matches. To accommodate multiple references, we use a Jackknifing procedure. Given N references, we compute the best score over N sets of N-1 references. The final score is the average of the N best scores using N different sets of N-1 references. The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference translations are usually the only human translations available. Using this procedure, we are able to estimate average human pe</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M.F. 1980. An Algorithm for Suffix Stripping. Program, 14, pp. 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>D Radev</author>
<author>S Teufel</author>
<author>W Lam</author>
</authors>
<title>Meta-Evaluation of Summaries in a Cross-Lingual Environment Using ContentBased Metrics.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING2002,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="8176" citStr="Saggion et al. (2002)" startWordPosition="1318" endWordPosition="1321">men et al. 1989). Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length. We can find the LCS of two sequences of length m and n using standard dynamic programming technique in O(mn) time. LCS has been used to identify cognate candidates during construction of N-best translation lexicons from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS (NP-LCS) to compare similarity between two texts in automatic summarization evaluation. NP-LCS can be shown as a special case of Equation (6) with β = 1. However, they did not provide the correlation analysis of NP-LCS with 1 This is a real machine translation output. 2 The “kill” in S2 or S3 does not match with “killed” in S1 in strict word-to-word comparison. BP e ⎧ ⎨ ⎩ ( 2 ) ( 3 ) human judgments and its effectiveness as an automatic evaluation measure. To apply LCS in machine translation evaluation, we view a translation as a sequence of words. The intuition i</context>
</contexts>
<marker>Saggion, Radev, Teufel, Lam, 2002</marker>
<rawString>Saggion H., D. Radev, S. Teufel, and W. Lam. 2002. Meta-Evaluation of Summaries in a Cross-Lingual Environment Using ContentBased Metrics. In Proceedings of COLING2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-Y Su</author>
<author>M-W Wu</author>
<author>J-S Chang</author>
</authors>
<title>A New Quantitative Quality Measure for Machine Translation System. In</title>
<date>1992</date>
<booktitle>Proceedings of COLING-92,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="1182" citStr="Su et al. (1992)" startWordPosition="168" endWordPosition="171">vel structure similarity naturally and identifies longest co-occurring insequence n-grams automatically. The second method relaxes strict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity b</context>
</contexts>
<marker>Su, Wu, Chang, 1992</marker>
<rawString>Su, K.-Y., M.-W. Wu, and J.-S. Chang. 1992. A New Quantitative Quality Measure for Machine Translation System. In Proceedings of COLING-92, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Thompson</author>
</authors>
<title>Automatic Evaluation of Translation Quality: Outline of Methodology and Report on Pilot Experiment.</title>
<date>1991</date>
<booktitle>In Proceedings of the Evaluator’s Forum, ISSCO,</booktitle>
<location>Geneva, Switzerland.</location>
<marker>Thompson, 1991</marker>
<rawString>Thompson, H. S. 1991. Automatic Evaluation of Translation Quality: Outline of Methodology and Report on Pilot Experiment. In Proceedings of the Evaluator’s Forum, ISSCO, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Turian</author>
<author>L Shen</author>
<author>I D Melamed</author>
</authors>
<title>Evaluation of Machine Translation and its Evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IY,</booktitle>
<location>New Orleans, U.S.A.</location>
<contexts>
<context position="2229" citStr="Turian et al. (2003)" startWordPosition="326" endWordPosition="329">error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n &gt; 1) matches achieved similar performance as Bleu. Since unigram matches do not distinguish words in consecutive positions from words in the wrong order, measures based on position-independent unigram matches are not sensitive to word order and sentence level structure. The</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Turian, J. P., L. Shen, and I. D. Melamed. 2003. Evaluation of Machine Translation and its Evaluation. In Proceedings of MT Summit IY, New Orleans, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Butterworths. London.</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>Van Rijsbergen, C.J. 1979. Information Retrieval. Butterworths. London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>