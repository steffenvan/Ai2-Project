<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.999046">
Punctuation as Implicit Annotations
for Chinese Word Segmentation
</title>
<author confidence="0.999249">
Zhongguo Li*
</author>
<affiliation confidence="0.986257">
Tsinghua University
</affiliation>
<author confidence="0.97404">
Maosong Sun**
</author>
<affiliation confidence="0.898631">
Tsinghua University
</affiliation>
<bodyText confidence="0.9988635">
We present a Chinese word segmentation model learned from punctuation marks which are
perfect word delimiters. The learning is aided by a manually segmented corpus. Our method
is considerably more effective than previous methods in unknown word recognition. This is a
step toward addressing one of the toughest problems in Chinese word segmentation.
</bodyText>
<sectionHeader confidence="0.987073" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99963025">
Paragraphs are composed of sentences. Hence when a paragraph begins, a sentence
must begin, and as a paragraph closes, some sentence must finish. This observation
is the basis of the sentence boundary detection method proposed by Riley (1989).
Similarly, sentences consist of words. As a sentence begins or ends there must be word
boundaries.
Inspired by this notion, we invent a method to learn a Chinese word segmenta-
tion model with punctuation marks in a large raw corpus. The learning is guided by
a segmented corpus (Section 3.2). Section 4 demonstrates that our method improves
notably the recognition of out-of-vocabulary (OOV) words with respect to approaches
which use only annotated data (Xue 2003; Low, Ng, and Guo 2005). This work has
practical implications in that the OOV problem has long been a big challenge for the
research community.
</bodyText>
<sectionHeader confidence="0.816685" genericHeader="keywords">
2. Segmentation as Tagging
</sectionHeader>
<bodyText confidence="0.999907">
We call the first character of a Chinese word its left boundary L, and the last character
its right boundary R. If we regard L and R as random events, then we can derive four
events (or tags) from them:
</bodyText>
<figure confidence="0.505742">
b = L·R, m = L · R, s = L·R, e = L · R
</figure>
<affiliation confidence="0.773643">
* Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China.
</affiliation>
<email confidence="0.884764">
E-mail: eemath@gmail.com.
</email>
<affiliation confidence="0.743027">
** Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China.
</affiliation>
<email confidence="0.967868">
E-mail: sms@mail.thu.edu.cn.
</email>
<note confidence="0.8707305">
Submission received: 16 July 2008; revised submission received: 26 March 2009; accepted for publication:
4 May 2009.
© 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
</note>
<bodyText confidence="0.837754428571429">
Here R means not R, and thus tag b represents the left but not the right boundary of
a word. The other tags can be interpreted similarly. This coding scheme was used by
Borthwick (1999) and Xue (2003), where b, m, s, and e stand for begin, middle, only
member, and end of a word, respectively. We reformulate them in terms of L and R to
facilitate the presentation of our method.
For a sentence S = c1c2 · · · cn and a sequence T = t1t2 · · · tn of b, m, s, e tags, we
define
</bodyText>
<equation confidence="0.9982865">
P(T|S) = Yn Pr(ti|contexti) (1)
i=1
</equation>
<bodyText confidence="0.993342888888889">
where contexti is ci with up to four surrounding characters. The legal tag sequence
(e.g., tag b followed by s is illegal) with highest P gives the segmentation result of
S. Then from Equation (1) it is obvious that knowing the probability distribution of
b, m, s, and e given context is adequate for carrying out Chinese word segmentation. The
purpose of this article is to show that punctuation can play a major role in estimating
this distribution.
We use the maximum entropy approach to model the conditional probability
Pr(y|x), which has the following parametric form according to Berger, Della Pietra, and
Della Pietra (1996):
</bodyText>
<equation confidence="0.9970752">
Pr(y|x) = Z1x) expÃXλifi(x,y) (2)
i
X ÃX !
Z(x) = exp λifi(x, y) (3)
y i
</equation>
<bodyText confidence="0.892381666666667">
For Chinese word segmentation, the binary valued functions fi are defined through the
10 features shown in Table 2. Xue (2003) explains how these features map to the feature
functions in Equations (2) and (3).
</bodyText>
<sectionHeader confidence="0.553189" genericHeader="introduction">
3. Method
</sectionHeader>
<bodyText confidence="0.996427333333333">
Our key idea is to approximate probabilities of b, m, s, and e with those of L and R.
To do this, we assume L and R are conditionally independent given context. Then we
have
</bodyText>
<equation confidence="0.997468333333333">
Pr(b |context) = Pr(L ·R|context) (definition of b)
= Pr(L |context) · Pr(R|context) (independence)
= Pr(L |context) · (1 − Pr(R |context)) (4)
</equation>
<bodyText confidence="0.774153333333333">
Probabilities for m, s, and e can be derived in the same way and so their derivations are
not provided here. As mentioned earlier, these probabilities are sufficient for Chinese
word segmentation. Now to model Pr(L |context) and Pr(R |context) with the maximum
</bodyText>
<page confidence="0.991537">
506
</page>
<note confidence="0.601672">
Li and Sun Punctuation as Implicit Annotations
</note>
<bodyText confidence="0.9972785">
entropy technique, we must have positive and negative examples of L and R. It is here
that punctuation comes into play.
</bodyText>
<subsectionHeader confidence="0.999307">
3.1 Positive Examples
</subsectionHeader>
<bodyText confidence="0.999892">
Punctuation offers directly positive examples of L and R. For instance, we can extract
four training examples from the sentence in Table 1, as listed in Table 2.
</bodyText>
<subsectionHeader confidence="0.999037">
3.2 Negative Examples
</subsectionHeader>
<bodyText confidence="0.9997945">
Suppose for the moment we know the real probability distribution of tags b, m, s, and e
given context. Then a character in context is itself a word and should be tagged s if
</bodyText>
<equation confidence="0.9720445">
Pr(s |context) &gt; max Pr(y|context) (5)
yE{b,m,e}
</equation>
<bodyText confidence="0.999320538461538">
Each positive example given by punctuation is subjected to the test in (5). If an example
labeled L passes this test, then it is also a positive example of R because s = L · R, and
failing this test gives a negative R. In a similar way we obtain negative examples of L.
This process is summarized in Figure 1.
A segmented corpus is needed to estimate the probabilities in test (5) with maxi-
mum entropy modeling. Here we use the data provided by Microsoft Research in the
SIGHAN 2005 Bakeoff. The trained model (the MSR model) was used in earlier work
(Low, Ng, and Guo 2005) and is one of the state-of-the-art models for Chinese word
segmentation.
With the MSR model, only the last example in Table 2 passes test (5). Hence we get
the three negative examples shown in Table 3. Examples like 1, 3, 6, and 8 are used to
estimate Pr(L |context) and those like 2, 4, 5, and 7 are used to estimate Pr(R |context).
Appendix A provides more details on this issue.
</bodyText>
<tableCaption confidence="0.898588">
Table 1
</tableCaption>
<bodyText confidence="0.660819">
Illustration of word boundaries near punctuation in a simple sentence.
– means the label is unknown with only the help of punctuation.
</bodyText>
<figure confidence="0.814371083333333">
sentence M I š 0 Of G n
word boundary L – – R L – – R
Table 2
Positive training examples extracted from the sentence in Table 1.
features of context
No. label c_2 c_1 c0 c1 c2 c_1c1 c_2c_1 c_1c0 c0c1 c1c2
1 L M I FA fflI IFA
2 R I ~ š 0 Of ~ ~š š0 0Of
3 L š 0 ~ G šVf FAš š0 0Of OfG
4 R f f G OfG GM
507
Computational Linguistics Volume 35, Number 4
</figure>
<figureCaption confidence="0.996533">
Figure 1
</figureCaption>
<bodyText confidence="0.969963333333333">
How to get negative examples of L and R. Test (5) is applied to all positive examples given by
punctuation. Those failing this test are negative training examples. It is test (5) that invokes the
need of a manually segmented corpus.
</bodyText>
<tableCaption confidence="0.9356">
Table 3
</tableCaption>
<figure confidence="0.847604">
Training examples derived from those in Table 2. We have 1→5, 2→6, 3→7, and 4→8.
features of context
No. label c_2 c_1 c0 c1 c2 c_1c1 c_2c_1 c_1c0 c0c1 c1c2
5 R ffl )&apos;E FA ffl)&apos;E )&apos;EFA
6 L š 0 Of FA0 )&apos;E� �š š0 0Of
7 R š 0 ~ # šVf FAš š0 0Of Offf
8 L f f ff Offf ffn�
</figure>
<subsectionHeader confidence="0.993399">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.9990828">
In all, we collected 10 billion L-L and R-R examples, each from a comprehensive Web
corpus.1 To cope with so much training data, we use the partitioning method of Yamada
and Matsumoto (2003). An alternative is the Vowpal Wabbit (fast on-line learning)
algorithm.2 Such an algorithm allows incremental training as more raw texts become
available.
</bodyText>
<sectionHeader confidence="0.98711" genericHeader="background">
4. Evaluation
</sectionHeader>
<bodyText confidence="0.9999545">
We evaluate our method with the data and scoring script provided by the SIGHAN 2005
Bakeoff. The data sets of Academia Sinica and City University of Hong Kong, which are
in Traditional Chinese, are not used here because the raw corpus is mainly in Simplified
Chinese. Table 4 gives the evaluation results on the data from Microsoft Research (MSR)
and Peking University (PKU).
It seems our method is over 10% below state of the art in precision on the MSR data.
However, we find that multiword expressions are consistently segmented into smaller
words. Take the one multiword ‘rPWz/-J�6}FVic,P)—crPW-ZTL6}FVic,PN’ [Institute of Chinese
</bodyText>
<footnote confidence="0.998451333333333">
1 Freely available for research purposes. See www.sogou.com/labs.
2 http://hunch.net/∼vw/. We thank one of the anonymous reviewers for telling us about this
implementation.
</footnote>
<page confidence="0.980533">
508
</page>
<note confidence="0.779045">
Li and Sun Punctuation as Implicit Annotations
</note>
<bodyText confidence="0.9976372">
Culture, Chinese Academy of Arts] in the standard answer of the test data as an example.
Our method segments it into six correct words ‘r&apos;W z/-J� f}F�kpit r&apos;W -ZTL f}F�kPN’
[China, art, academy, China, culture, institute], all of which are considered wrong by the
scoring script. This is arguable because the only difference is the granularity of the
segmentation.
</bodyText>
<subsectionHeader confidence="0.992493">
4.1 Influence of Granularity
</subsectionHeader>
<bodyText confidence="0.9999921">
We check every error detected by the scoring script on the MSR data, and find that
for our method, 15,071 errors are actually correct segmentations of 5,463 multiwords,
whereas for the MSR model, the corresponding counts are 858 and 355, respectively. The
gold standard contains 106,873 words. These statistics combined with Table 4 allow us
to calculate the metrics as in Table 5, if errors caused by correctly segmented multiwords
are not counted.
We see that, when the influence of granularity is considered, our method is slightly
better than the MSR model. However, as Table 4 shows, both models degrade on the
PKU data due to the difference in segmentation standards. This kind of degradation
was also documented by Peng, Feng, and McCallum (2004).
</bodyText>
<subsectionHeader confidence="0.984716">
4.2 Named Entity List Recovery
</subsectionHeader>
<bodyText confidence="0.99997775">
The SIGHAN data sets contain relatively few OOV words (2.6% for the MSR data). What
if the rate is much higher than that? We expect our model to be less vulnerable to OOV
problems because it is trained with billions of examples from a large corpus. To verify
this, we generate four data sets from each of these lists of names:
</bodyText>
<listItem confidence="0.97379175">
(a) 702 cities and counties of China seen in the MSR data
(b) 1,634 cities and counties of China not seen in the MSR data
(c) 7,470 Chinese personal names seen in the MSR data
(d) 20,000 Chinese personal names not seen in the MSR data
</listItem>
<tableCaption confidence="0.999445">
Table 4
</tableCaption>
<table confidence="0.9619452">
Evaluation results on SIGHAN Bakeoff 2005 data sets.
our method the MSR model
data set P R F P R F
MSR 84.8 91.3 87.9 96.0 95.6 95.8
PKU 84.2 86.1 85.1 85.2 82.3 83.7
</table>
<tableCaption confidence="0.995194">
Table 5
</tableCaption>
<table confidence="0.835779166666667">
Amended evaluation results for MSR data.
P R F
our method 98.0 96.7 97.3
the MSR model 96.7 96.0 96.3
509
Computational Linguistics Volume 35, Number 4
</table>
<tableCaption confidence="0.687149">
Table 6
</tableCaption>
<figure confidence="0.966229428571428">
Results on tasks of named entity list recovery.
our method the MSR model
data set P R F P R F
(a) 91.0 93.8 92.4 43.3 29.1 34.8
(b) 79.4 85.3 82.2 25.1 16.9 20.2
(c) 74.9 85.0 79.6 69.4 66.5 67.9
(d) 86.3 91.5 88.8 65.4 61.0 63.1
</figure>
<bodyText confidence="0.998799090909091">
The generation method is: Randomly permute each list and then put the result into lines,
with each line having about 30 names, and repeat this process until we get 1 million
tokens for each data set. We use the MSR model and our method to segment these data
sets. The results are in Table 6.
It is clear that our method performs better on these data sets. This provides evidence
that it could handle situations where many OOV words turn up. Table 6 also indicates
that, especially for the MSR model, recognition of Chinese personal names is easier
than location names. This is reasonable because the former has more regularity than the
latter. Besides, although there are no OOV words in data sets (a) and (c), many words
occur very sparsely in the MSR data. Hence the MSR model doesn’t do well even on
these two data sets.
</bodyText>
<subsectionHeader confidence="0.997288">
4.3 Unknown Words Recognition
</subsectionHeader>
<bodyText confidence="0.999985285714286">
To further test our model’s ability to recognize unknown words, we make 27,470 sen-
tences with the pattern ‘X � Y K , X pr), Y o’ (X is a resident of Y, and X loves
Y), where X and Y are the personal and location names in Section 4.2. The results on
this data set are in Table 7. Again our method outperforms the MSR model by a large
margin, proving once more that it is stronger in unknown word recognition. For both
methods, the metrics in Table 7 are better than those in Table 6, reflecting the fact that
unknown word recognition here is easier than the named entity list recovery task.
</bodyText>
<subsectionHeader confidence="0.987095">
4.4 Summary
</subsectionHeader>
<bodyText confidence="0.970214666666667">
Evaluation shows that when there are many new words, the improvement of our
method is obvious. In addition, a model is of limited use if it fits the SIGHAN data well,
but can’t maintain that accuracy elsewhere. Our model has a wider coverage through
</bodyText>
<tableCaption confidence="0.997305">
Table 7
</tableCaption>
<table confidence="0.94104825">
Results of unknown word recognition in 24,470 sentences.
P R F
our method 96.2 97.9 97.1
the MSR model 88.3 84.5 86.3
</table>
<page confidence="0.971554">
510
</page>
<note confidence="0.773107">
Li and Sun Punctuation as Implicit Annotations
</note>
<bodyText confidence="0.821234">
mining the Web. It tends to segment long multiword expressions into their component
words. This is not a disadvantage as long as the result is consistent.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="related work">
5. Related Work
</sectionHeader>
<bodyText confidence="0.999911066666667">
Punctuation gives naturally occurring unambiguous word boundaries. Gao et al. (2005)
described how to remove overlapping ambiguities in an annotated corpus to train a
model for resolving these ambiguities. A raw corpus doesn’t play a role in that method,
and the model involves no punctuation marks.
Chinese word segmentation based on position tagging was initiated by Xue (2003).
This method and its subsequent developments have achieved state-of-the-art perfor-
mance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005;
Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously
unknown words, whereas our method performs particular well in this case thanks to
the use of a huge Web corpus.
In the past decade, much work has been done in unsupervised word segmentation
(Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater,
Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take
advantage of the ever-growing amount of online text to model Chinese word segmen-
tation, but usually are less accurate and more complicated than ours.
</bodyText>
<sectionHeader confidence="0.978818" genericHeader="method">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.9999698">
With a virtually unlimited supply of raw corpus data, punctuation marks give us ample
training examples and thus can be quite useful as implicit annotations for Chinese
word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close
analogy to word segmentation. Hence our method can potentially be applied to this
task as well.
</bodyText>
<sectionHeader confidence="0.87614" genericHeader="method">
Appendix A: Input to the Training Algorithm
</sectionHeader>
<bodyText confidence="0.996984666666667">
We give readers a feel for the input data used to train our probability models. First, to
estimate Pr(L context), the input to the learning algorithm for the maximum entropy
models looks like this:
</bodyText>
<table confidence="0.948603777777778">
+L C0=FH C1=t C2=R� C0C1=FHt C1C2=ta
+L C0=fTV C1=tXt C2=G C0C1=fTV#t C1C2=fktG
-L C-2=t C-1=R� C0=f C-2C-1=tai C-1C0=a�f
+L C-2=#t C-1=G C0=n C-2C-1=fktG C-1C0=Gn
Whereas to estimate Pr(R I context), the input data are something like the following
+R C-2=t C-1=R� C0=f C-2C-1=tai C-1C0=a�f
+R C-2=#t C-1=G C0=n C-2C-1=fktG C-1C0=Gn
-R C0=FH C1=t C2=R� C0C1=FHt C1C2=ta
-R C0=fTV C1=tXt C2=G C0C1=fTV� C1C2=fktG
</table>
<bodyText confidence="0.568889333333333">
To save space, not all features in Table 2 are included here. From this illustration,
interested readers can get a general idea of our input to the learning algorithm in
Section 3.3.
</bodyText>
<page confidence="0.986986">
511
</page>
<note confidence="0.674976">
Computational Linguistics Volume 35, Number 4
</note>
<sectionHeader confidence="0.794706" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.831629727272727">
This work is supported by the National
Science Foundation of China under Grant
No. 60621062 and 60873174, and the National
863 Project under Grant No. 2007AA01Z148.
We thank our reviewers sincerely for many
helpful comments and suggestions which
greatly improved this article. Thanks also go
to sogou.com for sharing their Web corpora
and entity names. The maximum entropy
modeling toolkit used here is contributed by
Zhang Le of the University of Edinburgh.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999832647058823">
Berger, Adam L., Vincent J. Della Pietra, and
Stephen A. Della Pietra.1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39–71.
Borthwick, Andrew. 1999. A Maximum
Entropy Approach to Named Entity
Recognition. Ph.D. thesis, New York
University.
Feng, Haodi, Kang Chen, Xiaotie Deng, and
Weimin Zheng. 2004. Accessor variety
criteria for Chinese word extraction.
Computational Linguistics, 30(1):75–93.
Gao, Jianfeng, Mu Li, Andi Wu, and
Chang-Ning Huang. 2005. Chinese word
segmentation and named entity
recognition: A pragmatic approach.
Computational Linguistics, 31(4):531–574.
Goldwater, Sharon, Thomas L. Griffiths,
and Mark Johnson. 2006. Contextual
dependencies in unsupervised word
segmentation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 673–680, Sydney.
Jin, Zhihui and Kumiko Tanaka-Ishii. 2006.
Unsupervised segmentation of Chinese
text by use of branching entropy. In
Proceedings of the COLING/ACL on Main
Conference Poster Sessions, pages 428–435,
Morristown, NJ.
Low, Jim Kiat, Hwee Tou Ng, and Wenyuan
Guo. 2005. A maximum entropy
approach to Chinese word segmentation.
In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing,
pages 161–164, Jeju Island.
Peng, Fuchun, Fangfang Feng, and Andrew
McCallum. 2004. Chinese segmentation
and new word detection using conditional
random fields. In COLING ’04: Proceedings
of the 20th International Conference on
Computational Linguistics, pages 562–569,
Morristown, NJ.
Peng, Fuchun and Dale Schuurmans.
2001. Self-supervised Chinese word
segmentation. Lecture Notes in Computer
Science, 2189:238–249.
Riley, Michael D. 1989. Some applications
of tree-based modelling to speech and
language. In HLT ’89: Proceedings of the
Workshop on Speech and Natural Language,
pages 339–352, Morristown, NJ.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional
random fields. In NAACL ’03: Proceedings
of the 2003 Conference of the North
American Chapter of the Association for
Computational Linguistics on Human
Language Technology, pages 134–141,
Morristown, NJ.
Sun, Maosong, Dayang Shen, and
Benjamin K. Tsou. 1998. Chinese word
segmentation without using lexicon and
hand-crafted training data. In Proceedings
of the 17th International Conference on
Computational Linguistics, pages 1265–1271,
Morristown, NJ.
Xue, Nianwen. 2003. Chinese word
segmentation as character tagging.
Computational Linguistics and Chinese
Language Processing, 8(1):29–48.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis
with support vector machines. In
Proceedings of the 8th International Workshop
on Parsing Technologies (IWPT2003),
pages 195–206, Nancy.
Zhao, Hai, Chang-Ning Huang, and Mu Li.
2006. An improved Chinese word
segmentation system with conditional
random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language
Processing, pages 162–165, Sydney.
</reference>
<page confidence="0.997067">
512
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940533">
<title confidence="0.983214">Punctuation as Implicit Annotations for Chinese Word Segmentation</title>
<affiliation confidence="0.9982025">Tsinghua University Tsinghua University</affiliation>
<abstract confidence="0.99389725">We present a Chinese word segmentation model learned from punctuation marks which are perfect word delimiters. The learning is aided by a manually segmented corpus. Our method is considerably more effective than previous methods in unknown word recognition. This is a step toward addressing one of the toughest problems in Chinese word segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>A Stephen</author>
</authors>
<title>Della Pietra.1996. A maximum entropy approach to natural language processing.</title>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Stephen, </marker>
<rawString>Berger, Adam L., Vincent J. Della Pietra, and Stephen A. Della Pietra.1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="2259" citStr="Borthwick (1999)" startWordPosition="359" endWordPosition="360">r Science and Technology, Tsinghua University, Beijing 100084, China. E-mail: eemath@gmail.com. ** Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China. E-mail: sms@mail.thu.edu.cn. Submission received: 16 July 2008; revised submission received: 26 March 2009; accepted for publication: 4 May 2009. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 Here R means not R, and thus tag b represents the left but not the right boundary of a word. The other tags can be interpreted similarly. This coding scheme was used by Borthwick (1999) and Xue (2003), where b, m, s, and e stand for begin, middle, only member, and end of a word, respectively. We reformulate them in terms of L and R to facilitate the presentation of our method. For a sentence S = c1c2 · · · cn and a sequence T = t1t2 · · · tn of b, m, s, e tags, we define P(T|S) = Yn Pr(ti|contexti) (1) i=1 where contexti is ci with up to four surrounding characters. The legal tag sequence (e.g., tag b followed by s is illegal) with highest P gives the segmentation result of S. Then from Equation (1) it is obvious that knowing the probability distribution of b, m, s, and e gi</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Borthwick, Andrew. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
<author>Weimin Zheng</author>
</authors>
<title>Accessor variety criteria for Chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="13149" citStr="Feng et al. 2004" startWordPosition="2316" endWordPosition="2319">olves no punctuation marks. Chinese word segmentation based on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular well in this case thanks to the use of a huge Web corpus. In the past decade, much work has been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take advantage of the ever-growing amount of online text to model Chinese word segmentation, but usually are less accurate and more complicated than ours. 6. Conclusion With a virtually unlimited supply of raw corpus data, punctuation marks give us ample training examples and thus can be quite useful as implicit annotations for Chinese word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close analogy to word segmentation. Hence our method can potentially be applied to thi</context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Feng, Haodi, Kang Chen, Xiaotie Deng, and Weimin Zheng. 2004. Accessor variety criteria for Chinese word extraction. Computational Linguistics, 30(1):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese word segmentation and named entity recognition: A pragmatic approach.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="12345" citStr="Gao et al. (2005)" startWordPosition="2184" endWordPosition="2187">ent of our method is obvious. In addition, a model is of limited use if it fits the SIGHAN data well, but can’t maintain that accuracy elsewhere. Our model has a wider coverage through Table 7 Results of unknown word recognition in 24,470 sentences. P R F our method 96.2 97.9 97.1 the MSR model 88.3 84.5 86.3 510 Li and Sun Punctuation as Implicit Annotations mining the Web. It tends to segment long multiword expressions into their component words. This is not a disadvantage as long as the result is consistent. 5. Related Work Punctuation gives naturally occurring unambiguous word boundaries. Gao et al. (2005) described how to remove overlapping ambiguities in an annotated corpus to train a model for resolving these ambiguities. A raw corpus doesn’t play a role in that method, and the model involves no punctuation marks. Chinese word segmentation based on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular w</context>
</contexts>
<marker>Gao, Li, Wu, Huang, 2005</marker>
<rawString>Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang. 2005. Chinese word segmentation and named entity recognition: A pragmatic approach. Computational Linguistics, 31(4):531–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<location>Sydney.</location>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Goldwater, Sharon, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673–680, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhihui Jin</author>
<author>Kumiko Tanaka-Ishii</author>
</authors>
<title>Unsupervised segmentation of Chinese text by use of branching entropy.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main Conference Poster Sessions,</booktitle>
<pages>428--435</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="13217" citStr="Jin and Tanaka-Ishii 2006" startWordPosition="2325" endWordPosition="2328">d on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular well in this case thanks to the use of a huge Web corpus. In the past decade, much work has been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take advantage of the ever-growing amount of online text to model Chinese word segmentation, but usually are less accurate and more complicated than ours. 6. Conclusion With a virtually unlimited supply of raw corpus data, punctuation marks give us ample training examples and thus can be quite useful as implicit annotations for Chinese word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close analogy to word segmentation. Hence our method can potentially be applied to this task as well. Appendix A: Input to the Training Algorithm We give </context>
</contexts>
<marker>Jin, Tanaka-Ishii, 2006</marker>
<rawString>Jin, Zhihui and Kumiko Tanaka-Ishii. 2006. Unsupervised segmentation of Chinese text by use of branching entropy. In Proceedings of the COLING/ACL on Main Conference Poster Sessions, pages 428–435, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Low, Jim Kiat, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<location>Jeju Island.</location>
<marker></marker>
<rawString>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161–164, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>562--569</pages>
<location>Morristown, NJ.</location>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Peng, Fuchun, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In COLING ’04: Proceedings of the 20th International Conference on Computational Linguistics, pages 562–569, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Dale Schuurmans</author>
</authors>
<title>Self-supervised Chinese word segmentation.</title>
<date>2001</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>2189--238</pages>
<contexts>
<context position="13131" citStr="Peng and Schuurmans 2001" startWordPosition="2312" endWordPosition="2315"> method, and the model involves no punctuation marks. Chinese word segmentation based on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular well in this case thanks to the use of a huge Web corpus. In the past decade, much work has been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take advantage of the ever-growing amount of online text to model Chinese word segmentation, but usually are less accurate and more complicated than ours. 6. Conclusion With a virtually unlimited supply of raw corpus data, punctuation marks give us ample training examples and thus can be quite useful as implicit annotations for Chinese word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close analogy to word segmentation. Hence our method can potentially</context>
</contexts>
<marker>Peng, Schuurmans, 2001</marker>
<rawString>Peng, Fuchun and Dale Schuurmans. 2001. Self-supervised Chinese word segmentation. Lecture Notes in Computer Science, 2189:238–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Riley</author>
</authors>
<title>Some applications of tree-based modelling to speech and language. In</title>
<date>1989</date>
<booktitle>HLT ’89: Proceedings of the Workshop on Speech and Natural Language,</booktitle>
<pages>339--352</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="741" citStr="Riley (1989)" startWordPosition="107" endWordPosition="108">ity We present a Chinese word segmentation model learned from punctuation marks which are perfect word delimiters. The learning is aided by a manually segmented corpus. Our method is considerably more effective than previous methods in unknown word recognition. This is a step toward addressing one of the toughest problems in Chinese word segmentation. 1. Introduction Paragraphs are composed of sentences. Hence when a paragraph begins, a sentence must begin, and as a paragraph closes, some sentence must finish. This observation is the basis of the sentence boundary detection method proposed by Riley (1989). Similarly, sentences consist of words. As a sentence begins or ends there must be word boundaries. Inspired by this notion, we invent a method to learn a Chinese word segmentation model with punctuation marks in a large raw corpus. The learning is guided by a segmented corpus (Section 3.2). Section 4 demonstrates that our method improves notably the recognition of out-of-vocabulary (OOV) words with respect to approaches which use only annotated data (Xue 2003; Low, Ng, and Guo 2005). This work has practical implications in that the OOV problem has long been a big challenge for the research c</context>
</contexts>
<marker>Riley, 1989</marker>
<rawString>Riley, Michael D. 1989. Some applications of tree-based modelling to speech and language. In HLT ’89: Proceedings of the Workshop on Speech and Natural Language, pages 339–352, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North</booktitle>
<contexts>
<context position="13657" citStr="Sha and Pereira 2003" startWordPosition="2395" endWordPosition="2398">been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take advantage of the ever-growing amount of online text to model Chinese word segmentation, but usually are less accurate and more complicated than ours. 6. Conclusion With a virtually unlimited supply of raw corpus data, punctuation marks give us ample training examples and thus can be quite useful as implicit annotations for Chinese word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close analogy to word segmentation. Hence our method can potentially be applied to this task as well. Appendix A: Input to the Training Algorithm We give readers a feel for the input data used to train our probability models. First, to estimate Pr(L context), the input to the learning algorithm for the maximum entropy models looks like this: +L C0=FH C1=t C2=R� C0C1=FHt C1C2=ta +L C0=fTV C1=tXt C2=G C0C1=fTV#t C1C2=fktG -L C-2=t C-1=R� C0=f C-2C-1=tai C-1C0=a�f +L C-2=#t C-1=G C0=n C-2C-1=fktG C-1C0=Gn Whereas to estimate Pr(R I context), the input data are something like the following +</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha, Fei and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In NAACL ’03: Proceedings of the 2003 Conference of the North</rawString>
</citation>
<citation valid="false">
<title>American Chapter of the Association for Computational Linguistics on Human Language Technology,</title>
<pages>134--141</pages>
<location>Morristown, NJ.</location>
<marker></marker>
<rawString>American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 134–141, Morristown, NJ.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Maosong Sun</author>
</authors>
<title>Dayang Shen,</title>
<location>and</location>
<marker>Sun, </marker>
<rawString>Sun, Maosong, Dayang Shen, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin K Tsou</author>
</authors>
<title>Chinese word segmentation without using lexicon and hand-crafted training data.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>1265--1271</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="13105" citStr="Tsou 1998" startWordPosition="2310" endWordPosition="2311">ole in that method, and the model involves no punctuation marks. Chinese word segmentation based on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular well in this case thanks to the use of a huge Web corpus. In the past decade, much work has been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take advantage of the ever-growing amount of online text to model Chinese word segmentation, but usually are less accurate and more complicated than ours. 6. Conclusion With a virtually unlimited supply of raw corpus data, punctuation marks give us ample training examples and thus can be quite useful as implicit annotations for Chinese word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close analogy to word segmentation. Hence </context>
</contexts>
<marker>Tsou, 1998</marker>
<rawString>Benjamin K. Tsou. 1998. Chinese word segmentation without using lexicon and hand-crafted training data. In Proceedings of the 17th International Conference on Computational Linguistics, pages 1265–1271, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1206" citStr="Xue 2003" startWordPosition="182" endWordPosition="183">a paragraph closes, some sentence must finish. This observation is the basis of the sentence boundary detection method proposed by Riley (1989). Similarly, sentences consist of words. As a sentence begins or ends there must be word boundaries. Inspired by this notion, we invent a method to learn a Chinese word segmentation model with punctuation marks in a large raw corpus. The learning is guided by a segmented corpus (Section 3.2). Section 4 demonstrates that our method improves notably the recognition of out-of-vocabulary (OOV) words with respect to approaches which use only annotated data (Xue 2003; Low, Ng, and Guo 2005). This work has practical implications in that the OOV problem has long been a big challenge for the research community. 2. Segmentation as Tagging We call the first character of a Chinese word its left boundary L, and the last character its right boundary R. If we regard L and R as random events, then we can derive four events (or tags) from them: b = L·R, m = L · R, s = L·R, e = L · R * Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China. E-mail: eemath@gmail.com. ** Department of Computer Science and Technology, Tsinghua Universi</context>
<context position="3419" citStr="Xue (2003)" startWordPosition="573" endWordPosition="574"> the probability distribution of b, m, s, and e given context is adequate for carrying out Chinese word segmentation. The purpose of this article is to show that punctuation can play a major role in estimating this distribution. We use the maximum entropy approach to model the conditional probability Pr(y|x), which has the following parametric form according to Berger, Della Pietra, and Della Pietra (1996): Pr(y|x) = Z1x) expÃXλifi(x,y) (2) i X ÃX ! Z(x) = exp λifi(x, y) (3) y i For Chinese word segmentation, the binary valued functions fi are defined through the 10 features shown in Table 2. Xue (2003) explains how these features map to the feature functions in Equations (2) and (3). 3. Method Our key idea is to approximate probabilities of b, m, s, and e with those of L and R. To do this, we assume L and R are conditionally independent given context. Then we have Pr(b |context) = Pr(L ·R|context) (definition of b) = Pr(L |context) · Pr(R|context) (independence) = Pr(L |context) · (1 − Pr(R |context)) (4) Probabilities for m, s, and e can be derived in the same way and so their derivations are not provided here. As mentioned earlier, these probabilities are sufficient for Chinese word segme</context>
<context position="12640" citStr="Xue (2003)" startWordPosition="2233" endWordPosition="2234">4.5 86.3 510 Li and Sun Punctuation as Implicit Annotations mining the Web. It tends to segment long multiword expressions into their component words. This is not a disadvantage as long as the result is consistent. 5. Related Work Punctuation gives naturally occurring unambiguous word boundaries. Gao et al. (2005) described how to remove overlapping ambiguities in an annotated corpus to train a model for resolving these ambiguities. A raw corpus doesn’t play a role in that method, and the model involves no punctuation marks. Chinese word segmentation based on position tagging was initiated by Xue (2003). This method and its subsequent developments have achieved state-of-the-art performance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005; Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously unknown words, whereas our method performs particular well in this case thanks to the use of a huge Web corpus. In the past decade, much work has been done in unsupervised word segmentation (Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al. 2004; Goldwater, Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could a</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Xue, Nianwen. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT2003),</booktitle>
<pages>195--206</pages>
<location>Nancy.</location>
<contexts>
<context position="6863" citStr="Yamada and Matsumoto (2003)" startWordPosition="1228" endWordPosition="1231"> given by punctuation. Those failing this test are negative training examples. It is test (5) that invokes the need of a manually segmented corpus. Table 3 Training examples derived from those in Table 2. We have 1→5, 2→6, 3→7, and 4→8. features of context No. label c_2 c_1 c0 c1 c2 c_1c1 c_2c_1 c_1c0 c0c1 c1c2 5 R ffl )&apos;E FA ffl)&apos;E )&apos;EFA 6 L š 0 Of FA0 )&apos;E� �š š0 0Of 7 R š 0 ~ # šVf FAš š0 0Of Offf 8 L f f ff Offf ffn� 3.3 Training In all, we collected 10 billion L-L and R-R examples, each from a comprehensive Web corpus.1 To cope with so much training data, we use the partitioning method of Yamada and Matsumoto (2003). An alternative is the Vowpal Wabbit (fast on-line learning) algorithm.2 Such an algorithm allows incremental training as more raw texts become available. 4. Evaluation We evaluate our method with the data and scoring script provided by the SIGHAN 2005 Bakeoff. The data sets of Academia Sinica and City University of Hong Kong, which are in Traditional Chinese, are not used here because the raw corpus is mainly in Simplified Chinese. Table 4 gives the evaluation results on the data from Microsoft Research (MSR) and Peking University (PKU). It seems our method is over 10% below state of the art</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT2003), pages 195–206, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>162--165</pages>
<location>Sydney.</location>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Zhao, Hai, Chang-Ning Huang, and Mu Li. 2006. An improved Chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162–165, Sydney.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>