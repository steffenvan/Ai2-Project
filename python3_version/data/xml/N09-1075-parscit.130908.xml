<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9994225">
A model of local coherence effects in human sentence processing
as consequences of updates from bottom-up prior to posterior beliefs
</title>
<author confidence="0.995488">
Klinton Bicknell and Roger Levy
</author>
<affiliation confidence="0.9984085">
Department of Linguistics
University of California, San Diego
</affiliation>
<address confidence="0.900334">
9500 Gilman Dr, La Jolla, CA 92093-0108
</address>
<email confidence="0.999805">
{kbicknell,rlevy}@ling.ucsd.edu
</email>
<sectionHeader confidence="0.996676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999773705882353">
Human sentence processing involves integrat-
ing probabilistic knowledge from a variety of
sources in order to incrementally determine
the hierarchical structure for the serial input
stream. While a large number of sentence pro-
cessing effects have been explained in terms of
comprehenders’ rational use of probabilistic
information, effects of local coherences have
not. We present here a new model of local
coherences, viewing them as resulting from a
belief-update process, and show that the rele-
vant probabilities in our model are calculable
from a probabilistic Earley parser. Finally, we
demonstrate empirically that an implemented
version of the model makes the correct predic-
tions for the materials from the original exper-
iment demonstrating local coherence effects.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999730145833334">
The task of human sentence processing, recovering
a hierarchical structure from a serial input fraught
with local ambiguities, is a complex and difficult
problem. There is ample evidence that comprehen-
ders understand sentences incrementally, construct-
ing interpretations of partial structure and expecta-
tions for future input (Tanenhaus et al., 1995; Alt-
mann and Kamide, 1999). Many of the main behav-
ioral findings in the study of human sentence pro-
cessing have now been explained computationally.
Using probabilistic models trained on large-scale
corpora, effects such as global and incremental dis-
ambiguation preferences have been shown to be a
result of the rational use of syntactic probabilities
(Jurafsky, 1996; Hale, 2001; Narayanan and Juraf-
sky, 2001; Levy, 2008b; Levy et al., 2009). Simi-
larly, a number of other effects in both comprehen-
sion and production have been modeled as resulting
from rational strategies of languages users that take
into account all the probabilistic information present
in the linguistic signal (Genzel and Charniak, 2002;
Genzel and Charniak, 2003; Keller, 2004; Levy and
Jaeger, 2007).
One class of results from the literature that has
not yet been explained in terms of a rational com-
prehender strategy is that of local coherence effects
(Taboret al., 2004; Gibson, 2006; Konieczny and
M¨uller, 2007), cases in which it appears that the
parser is systematically ignoring contextual infor-
mation about possible syntactic structures and pur-
suing analyses that are probable only locally. These
effects are problematic for rational models, because
of the apparent failure to use all of the available in-
formation. This paper describes a new model of lo-
cal coherence effects under rational syntactic com-
prehension, which proposes that they arise as a re-
sult of updating prior beliefs about the structures
that a given string of words is likely to have to pos-
terior beliefs about the likelihoods of those struc-
tures in context. The critical intuition embodied in
the model is that larger updates in probability distri-
butions should be more processing-intensive; hence,
the farther the posterior is from the prior, the more
radical the update required and the greater the pro-
cessing load. Section 2 describes the problem of lo-
cal coherences in detail and Section 3 describes ex-
isting models of the phenomenon. Following that,
Sections 4–5 describe our model and its computa-
</bodyText>
<page confidence="0.979787">
665
</page>
<note confidence="0.927487">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 665–673,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figureCaption confidence="0.986175">
Figure 1: The difficulty of explaining local-
coherence effects as traditional garden-pathing.
</figureCaption>
<bodyText confidence="0.999586714285714">
tion from a probabilistic Earley parser. Section 6
presents the results of an experiment showing that
our model makes the correct predictions for the lo-
cal coherence effects seen in the original paper by
Tabor et al. (2004). Finally, Section 7 concludes and
discusses the insight our model gives into human
performance.
</bodyText>
<sectionHeader confidence="0.983821" genericHeader="method">
2 Local coherences
</sectionHeader>
<bodyText confidence="0.9992016">
The first studies to report effects of local coherences
are described in Tabor et al. (2004). In Experiment
1, they use a self-paced reading task and materials
containing relative clauses (RCs) attached to nouns
in non-subject position as in (1).
</bodyText>
<listItem confidence="0.718132">
(1) a. The coach smiled at the player tossed a
</listItem>
<bodyText confidence="0.992766755102041">
frisbee by the opposing team.
b. The coach smiled at the player who was
tossed a frisbee by the opposing team.
c. The coach smiled at the player thrown a
frisbee by the opposing team.
d. The coach smiled at the player who was
thrown a frisbee by the opposing team.
Their experimental design crossed RC reduction
with verb ambiguity. RCs are either reduced (1a,1c)
or unreduced (1b,1d), and the RC verb is either lex-
ically ambiguous between a past tense active and a
past participle (1a–1b), or is unambiguously a past
participle (1c–1d).
Tabor et al. point out that in one of these four
conditions (1a) there is a locally coherent string the
player tossed a frisbee. Out of context (e.g., if it
were starting a sentence) this string would have a
likely parse in which tossed is a past tense active
verb, the player is its agent, and a frisbee is its
theme (Figure 1, left). The preceding context within
the sentence, however, should rule out this interpre-
tation because the player appears within a PP and
hence should not be able to be the subject of a new
sentence (Figure 1, right). That is, given the preced-
ing context, the player tossed a frisbee must begin
a reduced RC, such that there is no local ambiguity.
Thus, if comprehenders are making full use of the
linguistic context, (1a) should be no more difficult
than the other examples, except insofar as ambigu-
ous verbs are harder than unambiguous verbs, and
reduced RCs are harder than unreduced RCs, pre-
dicting there would be only the two main effects of
RC reduction and verb ambiguity on reading times
for the tossed a frisbee region.
Tabor et al., however, predict an interaction such
that (1a) will have added difficulty above and be-
yond these two effects, because of the interference
from the locally coherent parse of the player tossed a
frisbee. Concordant with their predictions, they find
an interaction in the tossed a frisbee region, such
that (1a) is super-additively difficult. Because this
result requires that an impossible parse influences a
word’s difficulty, it is in direct opposition to the pre-
dictions of theories of processing difficulty in which
the probability of a word given context is the pri-
mary source of parsing difficulty, and more gener-
ally appears to be in opposition to any rational the-
ory, in which comprehenders are making use of all
the information in the linguistic context.
</bodyText>
<sectionHeader confidence="0.979514" genericHeader="method">
3 Existing models
</sectionHeader>
<bodyText confidence="0.9999593125">
With the results showing local coherence effects
in mind, we can ask the question of what sorts
of theories do predict these effects. This section
briefly describes two recent examples of such the-
ories. The first involves dynamical systems models
to explain the effects and the second uses a mathe-
matical model of the combination of bottom-up and
top-down probabilistic information.
In Tabor and Hutchins’s (2004) SOPARSE (self-
organized parse) model, reading a word activates a
set of lexically anchored tree fragments. Through
spreading activation between compatible fragments
and inhibition between incompatible ones, these tree
fragments then compete in a process which is sen-
sitive only to the local environment, i.e., ignoring
the global grammatical context. Eventually, the sys-
</bodyText>
<figure confidence="0.998725206896552">
NP
VP
S
D
The
NP
D
the
N
coach
V
smiled
P
at
PP
N
player
S
VP
V
tossed
S
NP
Det
N
the player
VP
V
tossed
</figure>
<page confidence="0.996298">
666
</page>
<bodyText confidence="0.999698424242424">
tem stabilizes to the correct parse, and reading times
for each word are modeled as the time the system
takes to stabilize after reading a word. Stabilization
takes longer for locally coherent regions because the
locally coherent parse will be created and compete
with the globally grammatical parse.
There are, however, unresolved issues with this
model. The model has a number of free parameters,
relating to the equations used for the competition,
the method by which links between fragments are
formed, as well as the question of precisely what
tree fragments a given word will activate. While Ta-
bor and Hutchins (2004) work out these questions
in detail for the types of sentences they model, it is
unclear how the model could be scaled up to make
predictions for arbitrary types of sentences. That is,
there is no principled system for setting the three
types of parameters mentioned, and thus no clear in-
terpretation of their values. The model put forward
in this paper is an attempt to remedy this situation.
A recent proposal by Gibson (2006) can also ex-
plain some of the local coherence results. Gibson’s
proposal is that part-of-speech ambiguities have a
special status in parsing; in effect, lexical part-of-
speech ambiguities can be thought of as one-word
local coherences. In this model, a probability func-
tion P˜ is calculated over part-of-speech tags given
a word. This probability for tag ti and a word w,
P˜(ti|w), is proportional to the context-independent
probability of ti given the word w, P(ti|w) – the
bottom-up component – multiplied by a smoothed
probability P3 of the tag given the context – the top-
down component:
</bodyText>
<equation confidence="0.993474333333333">
P˜(ti|w) = P(ti|w)P3(ti|context) (1)
P(t|w)P3(t|context)
tET
</equation>
<bodyText confidence="0.999864">
Difficulty is predicted to be high when the probabil-
ity P˜ of the correct tag is low.
Because the top-down probabilities are smoothed
to allow for all possible parts-of-speech, any word
which is lexically ambiguous will be more difficult
to process, regardless of whether it is ambiguous or
not in its context. This can thus explain some of the
difference between the ambiguous and unambiguous
verbs in Tabor et al. (2004). It is not clear, however,
under such a model why the super-additive interac-
tion would obtain—that is, why (1a) should be so
much harder than (1b) starting at the word tossed.
In addition, Gibson’s model is a bit underspecified:
he does not discuss how the top-down probabilities
are calculated, nor what the precise linking hypothe-
sis is between the final P˜ and reading times. Finally,
it is not at all clear why the top-down expectations
should be smoothed, since the smoothing actually
has negative consequences on the processor’s per-
formance.
</bodyText>
<sectionHeader confidence="0.779058" genericHeader="method">
4 Parsing as belief update
</sectionHeader>
<bodyText confidence="0.999090181818182">
The basic intuition behind the model presented here
is that incrementally processing a sentence can be
conceptualized as a process of updating one’s be-
liefs. Such an analogy has been used to moti-
vate surprisal-based theories of sentence processing
(Hale, 2001; Levy, 2008a), where beliefs about the
structure of a sentence after seeing the first i − 1
words in the sentence, which we denote as wi−1
0 ,
are updated upon encountering wi. In this case, the
surprisal of a word (− log P(wi|wi−1
</bodyText>
<equation confidence="0.983741">
0 )) is equiva-
</equation>
<bodyText confidence="0.99934637037037">
lent to the Kullback-Leibler divergence of the beliefs
after wi0 from the beliefs after wi−1
0 (Levy, 2008a).
Our model focuses on another belief-update process
in sentence processing: updating beliefs about the
structures that a string of words is likely to have in-
dependent of context to beliefs about what structures
it is likely to have in context. A bit more formally, it
views the process of integrating a string of words
w i into a sentence as beginning with a ‘bottom-
up’ prior distribution of syntactic structures likely to
span w i and integrating that with ‘top-down’ knowl-
edge from the previous words in the sentence wi0 in
order to reach a posterior distribution conditioning
on w 0 over which structures actually can span w i.
This belief update process can be viewed as a ratio-
nal reconstruction of the Tabor and Hutchins (2004)
model, where – instead of the system dynamics of
competition between arbitrary tree fragments – dif-
ferences between prior and posterior probability dis-
tributions over syntactic structures determine pro-
cessing difficulty.
More formally still, when integrating w i into a
sentence, for each syntactic category X, we can de-
fine the prior probability conditioned only on w i that
w i will form the beginning of that category, i.e., that
an X exists which begins at index i and spans at least
</bodyText>
<page confidence="0.859346">
667
</page>
<equation confidence="0.927284333333333">
through j:
Prior: P(Xk≥j
i |wji) (2)
</equation>
<bodyText confidence="0.999894909090909">
It is important to note here that this prior probability
is conditional only on the value of wji and not the
values of i or j; that is, in the prior probability, i and
j should be interpreted merely as a way to coindex
the start and end points of the string of words being
integrated with a category X potentially spanning
them, and not as making reference to position in the
full sentence string.
For each category X, this prior probability will be
updated to the posterior probability of that category
spanning wji given all the words seen so far:
</bodyText>
<equation confidence="0.5210685">
Posterior: P(Xk≥j
i |wj 0) (3)
</equation>
<bodyText confidence="0.999965">
In the equation for the posterior, of course, the in-
dices i and j are positions in the sentence string, and
not merely coindices.
Given these prior and posterior beliefs, we pre-
dict difficulty to arise in cases where the prior re-
quires substantial modification to reach the poste-
rior, that is, cases in which the prior and poste-
rior make substantially different predictions for cat-
egories. A strong local coherence will have sharply
different prior and posterior distributions, causing
difficulty. We represent the prior and posterior be-
liefs as vectors of the probabilities of each syntactic
category spanning wji ,and measure Mij, the amount
of modification required, as the summed K-L diver-
gence of the prior from the posterior vector. That is,
if N is the set of nonterminals in the grammar, the
size of the belief update is modeled as1
</bodyText>
<equation confidence="0.925106333333333">
Mij =
def � D (P(Xk≥j |wj0)   ||P(Xk≥j  |wji ))
X∈N
</equation>
<bodyText confidence="0.966364">
In the remainder of the paper, we show how to com-
pute Mij by using Bayesian inference on quanti-
ties calculated in ordinary probabilistic incremen-
tal Earley parsing with a stochastic context-free
1Note that for each syntactic category X ∈ N, the proba-
bility distribution P(Xk≥j
i |I) for some information I is over a
binary random variable indicating the presence of X. The dif-
ferent syntactic categories X that could span from i to any k
are not mutually exclusive, hence we cannot define size of be-
lief update as a single K-L divergence defined over multinomial
distributions.
grammar (SCFG), and show that our model makes
the correct predictions using an SCFG for English
on the original local-coherences experiment of Ta-
bor et al. (2004).
</bodyText>
<sectionHeader confidence="0.550627" genericHeader="method">
5 Computing priors and posteriors
</sectionHeader>
<bodyText confidence="0.9997873">
For SCFGs, a probabilistic Earley parser (Earley,
1970; Stolcke, 1995) provides the basic quantities
we need to compute the prior (2) and posterior
(3) for each category X. Following Stolcke, we
use capital Latin characters to denote non-terminal
categories and use lowercase Greek characters to
denote (possibly null) sequences of terminals and
non-terminals. We write the probability that a non-
terminal X can be recursively rewritten by SCFG
rules as a certain series of symbols µ by
</bodyText>
<equation confidence="0.982659">
P(X ⇒∗ µ)
</equation>
<bodyText confidence="0.999452">
An edge built from the rule X → λµ where λ has
been recognized as beginning at position i and end-
ing at position j is denoted
</bodyText>
<equation confidence="0.938809">
j : Xi → λ.µ
</equation>
<bodyText confidence="0.937142">
The forward probability of that edge at position j,
αj, is defined to be the joint probability that the root
node will generate all words recognized so far wj0 as
well as the edge
αj(Xi → λ.µ)
With this terminology, we are now in a position to
describe how we calculate the posterior and prior
probability vectors for our model.
</bodyText>
<subsectionHeader confidence="0.997006">
5.1 Calculating the posterior
</subsectionHeader>
<bodyText confidence="0.999964">
To calculate the posterior, we first use the definition
of conditional probability to rewrite it as
</bodyText>
<equation confidence="0.984994">
P(Xk≥j 0)
i |wj 0) = P (Xk≥j
P(wj0)
i , wj
</equation>
<bodyText confidence="0.99064275">
In a context-free grammar, given the syntactic cat-
egory that dominates a string of words, the words’
probability is independent from everything outside
the category. Thus, this is equivalent to
</bodyText>
<equation confidence="0.9971912">
i |wj 0) = P(wi o, Xi)P(wj i |Xk≥j
P(Xk≥j i )
P(wj0)
= P(S ∗⇒ wi0Xν)P(X ⇒∗ wjiµ)
P(S ⇒∗ wj0λ)
</equation>
<page confidence="0.977446">
668
</page>
<subsubsectionHeader confidence="0.70996">
5.1.1 Posterior: the numerator’s first term
</subsubsectionHeader>
<bodyText confidence="0.999941666666667">
The first term in the numerator P(S ∗ wi0Xν)
can be computed from a parse of wi0 by summing
forward probabilities of the form
</bodyText>
<equation confidence="0.997183">
αi(Xi — .µ) (4)
</equation>
<subsubsectionHeader confidence="0.82627">
5.1.2 Posterior: the denominator
</subsubsectionHeader>
<bodyText confidence="0.999722333333333">
Similarly, the denominator P(S ∗ wj0λ) can be
computed from a parse of wj0 by summing forward
probabilities of the form
</bodyText>
<equation confidence="0.998884">
αj(Y — λwjj−1.µ) (5)
</equation>
<bodyText confidence="0.999277">
for all Y. This is because the forward probability of
a state is conditioned on generating all the previous
words.
</bodyText>
<subsubsectionHeader confidence="0.928095">
5.1.3 Posterior: the numerator’s second term
</subsubsectionHeader>
<bodyText confidence="0.999754">
The second term in the numerator P(X ∗wjiµ)
for an arbitrary category X cannot necessarily be
calculated from a probabilistic Earley parse of the
sentence, because the parser does not construct
states that are not potentially useful in forming a sen-
tence (i.e., states that would have a forward proba-
bility of zero.) However, to calculate the probability
of X generating words wji we can parse wji sepa-
rately with a goal category of X. From this parse,
we can extract the probability of wji being generated
from X in the same way as we extracted the proba-
bility of wj0 being generated from S, i.e., as a sum of
forward probabilities at j (Eq. 5).2
</bodyText>
<tableCaption confidence="0.996695">
Table 1: Event space for the prior
</tableCaption>
<bodyText confidence="0.285071">
Event Description
</bodyText>
<listItem confidence="0.782921666666667">
E0: There are at least i&apos; words |w |&gt; i0
E1: A category X begins at i&apos; Xi0
E2: An Xi, spans at least through j Xk≥j
i0
E3: There are at least j words |w |&gt; j
E4: Words wji, are these specific ˜wji, wji0 = ˜wji0
</listItem>
<bodyText confidence="0.9912652">
an arbitrary sentence, the next j — i words will be
wji, and P(Xk≥j
i ) denotes the probability that an ar-
bitrary point in an arbitrary sentence will be the left
edge of a category X that spans at least j — i words.
None of the three terms in Eq. 6 can be directly ob-
tained. However, we can obtain a very good approx-
imation of Eq. 6 as follows. First, we marginalize
over the position within a sentence with which the
left edge i might be identified:
</bodyText>
<equation confidence="0.998604333333333">
P(Xk≥j
i |wji) =
P(wji0|Xk≥j �P(i = i0)
i0 )P(Xk≥j (7)
i0 )
P(wji0)
</equation>
<bodyText confidence="0.834572777777778">
In Eq. 7, i0 is identified with the actual string position
within the sentence.
Second, we rewrite the first term in this sum with
event space notation, using the event space given in
Table 1.
i0=0,1,...
5.2 Calculating the prior P(wji0|Xk≥j = P (E0,3,4|E0...3)P (E0...3)
To calculate the prior, we first use Bayes rule to i0 )P(Xk≥j i0)
rewrite it as
</bodyText>
<equation confidence="0.9875966">
P(wji0) P(E0,3,4)
P(Xk≥j i )
i |wj i ) = P(wj i |Xk≥j
i )P(Xk≥j (6)
P(wji )
</equation>
<bodyText confidence="0.846435777777778">
Recall that at this point, i and j do not refer to in-
dex positions in the actual string but rather serve to
identify the substring wji of interest. That is, P(wji)
denotes the probability that at an arbitrary point in
2To calculate the posterior, it is not necessary to parse wZ
separately, since these states are only excluded from the parse
when their forward probability is zero, in which case the first
term in the numerator will also be zero. A separate parse is nec-
essary, however, when using this term to calculate the prior.
</bodyText>
<equation confidence="0.923770615384615">
P(E4|E0...3)P(Eo...3)
=
P(Eo,3,4)
Applying the chain rule, we can further simplify.
P(E4|E0...3)P(E1...3|E0)P(E0)
P(E3,4|E0)P(E0)
P(E4|E0...3)P(E1...3|E0)
P(E3,4|E0)
P(E2...4|E0, E1)P(E1|E0)
P(E3,4|E0)
=
=
=
</equation>
<page confidence="0.990996">
669
</page>
<bodyText confidence="0.9990755">
Switching back from event space notation and sub-
stituting this term into Eq. 7, we now have
</bodyText>
<equation confidence="0.9975786">
P(Xk�j
i |wji ) =
P(wji0|Xi0, E0)P(Xi0|E0)
P(wji0|E0)
(8)
</equation>
<bodyText confidence="0.999539">
Thus, by conditioning all terms on E0, the presence
of at least i&apos; words, we have transformed the proba-
bilities we need to calculate into these four terms,
which are easier to calculate from the parser. We
now consider how to calculate each of the terms.
</bodyText>
<subsubsectionHeader confidence="0.646791">
5.2.1 Prior: the numerator’s first term
</subsubsectionHeader>
<bodyText confidence="0.9990555">
The first term in the numerator can be simplified
because our grammar is context-free:
</bodyText>
<equation confidence="0.996248">
P(wji0|Xi0,E0) = P(wji0|Xi0)
= P(X ⇒* wji0)
</equation>
<subsectionHeader confidence="0.386494">
This can be computed as described in Section 5.1.3.
5.2.2 Prior: the numerator’s second term
</subsectionHeader>
<bodyText confidence="0.998747">
The second term in the numerator can be rewritten
as follows:
</bodyText>
<equation confidence="0.9999145">
P (Xi0, E0)
P(Xi0|E0) = P(E0)
P(S ⇒* ˚wi00 Xµ)
P(S ⇒* ˚wi00 µ)
</equation>
<bodyText confidence="0.999948">
where ˚wi00 denotes any sequence of i&apos; words. Given
a value i&apos; we can calculate both terms by parsing
the string ˚wi0X, where each word w˚ in ˚wi0X is a
special word that can freely act as any preterminal.
The denominator can then be calculated by summing
the forward probabilities of the last word ˚wii_1 as in
Eq. 5, and the numerator by summing the forward
probability of X, as in Eq. 4.
</bodyText>
<subsectionHeader confidence="0.648226">
5.2.3 Prior: the denominator
</subsectionHeader>
<bodyText confidence="0.999860666666667">
The denominator in the calculation of the prior
can be calculated in a way analogous to the numera-
tor’s second term (Section 5.2.2):
</bodyText>
<equation confidence="0.9978424">
i0, E0)
P(wj i0|E0) = P(wj
P(E0)
= P(S ⇒* ˚wi00 wji0µ)
P(S ⇒* ˚wi00 µ)
</equation>
<subsubsectionHeader confidence="0.85708">
5.2.4 Prior: starting position probability
</subsubsectionHeader>
<bodyText confidence="0.99982775">
Finally, we must calculate the second term in
Eq. 8, the probability of the starting position
P(i = i&apos;). Given that all our terms are conditional
on the existence of all words in the sentence up to
i&apos; (E0), the probability of a starting position P(i) is
the probability of drawing i&apos; randomly from the set
of positions in sentences generated by the grammar
such that all words up to that position exist. For most
language grammars, this distribution can be easily
approximated by a sample of sentences generated
from the SCFG, since most of the probability mass
is concentrated in small indices.
</bodyText>
<sectionHeader confidence="0.997526" genericHeader="evaluation">
6 Experiment
</sectionHeader>
<bodyText confidence="0.9999744">
We tested the predictions of an implemented ver-
sion of our model on the materials from Ta-
bor et al. (2004). To generate quantitative predic-
tions, we created a small grammar of relevant syn-
tactic rules, and estimated the rule probabilities from
syntactically annotated text. We calculated summed
K-L divergence of the prior from the posterior vector
for each word in the Tabor et al. items, and predict
this sum to be largest at the critical region when the
sentence has an effect of local coherence.
</bodyText>
<subsectionHeader confidence="0.990426">
6.1 Methods
6.1.1 Grammar
</subsectionHeader>
<bodyText confidence="0.99998075">
We defined a small SCFG for the problem, and es-
timated its rule probabilities using the parsed Brown
corpus. The resulting SCFG is identical to that used
in Levy (2008b) and is given in Table 2.
</bodyText>
<subsectionHeader confidence="0.954131">
6.1.2 Lexicon
</subsectionHeader>
<bodyText confidence="0.999780333333333">
Lexical rewrite probabilities for part-of-speech
tags were also estimated using the entire parsed
Brown corpus.
</bodyText>
<subsectionHeader confidence="0.872206">
6.1.3 Materials
</subsectionHeader>
<bodyText confidence="0.999959777777778">
The materials were taken from Experiment 1 of
Tabor et al. (2004). We removed 8 of their 20 items
for which our trained model either did not know the
critical verb or did not know the syntactic structure
of some part of the sentence. For the other 12 items,
we replaced unknown nouns (9 instances) and un-
known non-critical verbs (2 instances), changed one
plural noun to singular, and dropped one sentence-
initial prepositional phrase.
</bodyText>
<equation confidence="0.930833">
i0=0,1,...
�P(i = i&apos;)
</equation>
<page confidence="0.998299">
670
</page>
<tableCaption confidence="0.986198">
Table 2: The SCFG used in Experiment 3. Rule
weights given as negative log-probabilities in bits.
</tableCaption>
<table confidence="0.798689375">
Rule Weight
ROOT → S 0
S → S-base CC S-base 7.3
S → S-base 0.01
S-base → NP-base VP 0
NP → NP-base RC 4.1
NP → NP-base 0.5
NP → NP-base PP 2.0
</table>
<equation confidence="0.948392166666667">
NP-base → DT NN NN 4.7
NP-base → DT NN 1.9
NP-base → DT JJ NN 3.8
NP-base → PRP 1.0
NP-base → NNP 3.1
VP/NP → VBD NP 4.0
VP/NP → VBD 0.1
VP → VBD PP 2.0
VP → VBD NP 0.7
VP → VBD 2.9
RC → WP S/NP 0.5
RC → VP-pass/NP 2.0
RC → WP FinCop VP-pass/NP 4.9
PP → IN NP 0
S/NP → VP 0.7
S/NP → NP-base VP/NP 1.3
VP-pass/NP → VBN NP 2.2
VP-pass/NP → VBN 0.4
</equation>
<subsectionHeader confidence="0.998921">
6.2 Procedure
</subsectionHeader>
<bodyText confidence="0.950305833333333">
For these 12 items, we ran our model on the four
conditions in (1). For each word, we calculated
the prior and posterior vectors for substrings of
three lengths at wi. The summed K-L divergence
is reported for a substring length of 1 word us-
ing a prior of P(Xk≥i
</bodyText>
<equation confidence="0.4358062">
i−1 |wi i−1), for a length of 2
using P(Xk≥i
i−2 |wi i−2), and for a length of 3 us-
ing P(Xk≥i
i−3 |wi i−3). For all lengths, we predict the
</equation>
<bodyText confidence="0.996327375">
summed divergence to be greater at critical words
for the part-of-speech ambiguous conditions (1a,1b)
than for unambiguous (1c,1d), because the part-of-
speech unambiguous verbs cannot give rise to a prior
that predicts for a sentence to begin. For a substring
length of 3, we also predict that the divergence is
superadditively greatest in the ambiguous reduced
condition (1a), because of the possibility of starting
</bodyText>
<figureCaption confidence="0.976204">
Figure 2: Summed K-L divergence of the prior from
the posterior vectors at each word: Substring length
</figureCaption>
<figure confidence="0.946565">
1
</figure>
<figureCaption confidence="0.9763805">
Figure 3: Summed K-L divergence of the prior from
the posterior vectors at each word: Substring length
</figureCaption>
<bodyText confidence="0.9592465">
2
a sentence with the player tossed.
</bodyText>
<subsectionHeader confidence="0.908481">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999973153846154">
The results of the experiment are shown in Figures
2–4. For all three substring lengths, the model pre-
dicts difficulty to be greater in the ambiguous condi-
tions at the critical words (tossed/thrown a frisbee).
For 1-word substrings, the effect is localized on the
critical verb (tossed/thrown), for 2-word substrings
it is localized on the word directly following the
critical verb (tossed/thrown a), and for 3-word sub-
strings there are two effects: one on the critical verb
(the player tossed/thrown) and one two words later
(tossed/thrown a frisbee). Furthermore, for 3-word
substrings, the effect is superadditively greatest for
the player tossed. These results thus nicely confirm
</bodyText>
<figure confidence="0.99376921875">
at the player who was tossed/ a frisbee
thrown
Summed K−L Divergence (bits)
0 5 10 15 20 25
●
●
●
tossed
who was tossed
thrown
who was thrown
● ●
● ● ●
●
●
●
at the player who was tossed/ a frisbee
thrown
tossed
who was tossed
thrown
who was thrown
●
● ● ●
●
● ●
●
0 5 10 15 20 25
●
●
Summed K−L Divergence (bits)
●
</figure>
<page confidence="0.78051">
671
</page>
<figureCaption confidence="0.775271333333333">
Figure 4: Summed K-L divergence of the prior from
the posterior vectors at each word: Substring length
3
</figureCaption>
<bodyText confidence="0.99997875">
both of our predictions and demonstrate that a model
in which large belief updates from a bottom-up prior
to a posterior induce difficulty is capable of account-
ing for effects of local coherences.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999995298507463">
This paper has described a model of local coherence
effects in sentence processing, which views the pro-
cess of integrating a string of words wz into a sen-
tence as a process of updating prior beliefs about
the structures spanning those words to posterior be-
liefs. These prior beliefs are simply the probabilities
of those structures given only the words being inte-
grated, and the posterior beliefs are the probabilities
given the entire sentence processed thus far. Diffi-
culty is predicted to result whenever this update is
large – which we model in terms of a large summed
K-L divergence of the prior from the posterior vec-
tor. We demonstrated a method of normatively cal-
culating these probabilities from probabilistic Ear-
ley parses and used this implemented model to make
predictions for the materials for the original experi-
mental result of effects of local coherences (Taboret
al., 2004). Our results demonstrated that the model
predicts difficulty to occur at the correct part of the
sentence in the correct condition.
We improve on existing models in two ways.
First, we make predictions for where local coher-
ences should obtain for an arbitrary SCFG, not just
one particular class of sentences. This allows the
model to scale up for use with a broad coverage
grammar and to make predictions for arbitrary sen-
tences, which was not possible with a model such as
Tabor &amp; Hutchins (2004).
Second, our model gives a rational basis to an ef-
fect which has typically been seen to result from ir-
rationality of the human sentence processor. Specif-
ically, the cost that our model describes of updating
bottom-up prior beliefs to in-context posterior be-
liefs can be viewed as resulting from a rational pro-
cess in the case that the bottom-up prior is available
to the human sentence processor more rapidly than
the in-context posterior. Interestingly, the fact that
the prior is actually more difficult to compute than
the posterior suggests that the only way it would be
available more rapidly is if it is precomputed. Thus,
our model provides the insight that, to the extent
that comprehenders are behaving rationally in pro-
ducing effects of local coherences, this may indi-
cate that they have precomputed the likely syntac-
tic structures of short sequences of words. While it
may be unlikely that they calculate these probabil-
ities for sequences directly from their grammar as
we do in this paper, there could be a number of ways
to approximate this prior: for example, given a large
enough corpus, these probabilities could be approx-
imated for any string of words that appears suffi-
ciently often by merely tracking the structures the
string has each time it occurs. Such a hypothesis for
how comprehenders approximate the prior could be
tested by manipulating the frequency of the relevant
substrings in sentences with local coherences.
This work can be extended in a number of ways.
As already mentioned, one logical step is using
a broad-coverage grammar. Another possibility re-
lates to the problem of correlations between the dif-
ferent components of the prior and posterior vec-
tors. For example, in our small grammar, whenever a
ROOT category begins, so does an S, an S-base, and
an NP-base. Dimensionality reduction techniques on
our vectors may be able to remove such correlations.
These steps and more exhaustive evaluation of a va-
riety of datasets remain for the future.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.974213">
This research was supported by NIH Training Grant
T32-DC000041 from the Center for Research in
Language at UCSD to the first author.
</bodyText>
<figure confidence="0.981051416666667">
at the player who was tossed/ a frisbee
thrown
tossed
who was tossed
thrown
who was thrown
●
●
0 5 10 15 20 25
Summed K−L Divergence (bits)
●
●
</figure>
<page confidence="0.988437">
672
</page>
<sectionHeader confidence="0.992396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999687425">
Gerry T.M. Altmann and Yuki Kamide. 1999. Incremen-
tal interpretation at verbs: Restricting the domain of
subsequent reference. Cognition, 73:247–264.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In Proceedings of the 40th
annual meeting of the Association for Computational
Linguistics, pages 199–206, Philadelphia, July. Asso-
ciation for Computational Linguistics.
Dmitriy Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function of
the sentence number. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 65–72, Sapporo, Japan. Association for
Computational Linguistics.
Edward Gibson. 2006. The interaction of top-down and
bottom-up statistics in the resolution of syntactic cat-
egory ambiguity. Journal of Memory and Language,
54:363–388.
John Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, volume 2, pages
159–166, New Brunswick, NJ. Associate for Compu-
tational Linguistics.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20:137–194.
Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In Dekang Lin and Dekai Wu, edi-
tors, Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing, pages 317–
324, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Lars Konieczny and Daniel M¨uller. 2007. Local co-
herence interpretation in written and spoken language.
Presented at the 20th Annual CUNY Conference on
Human Sentence Processing. La Jolla, CA.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 849–856, Cambridge, MA. MIT Press.
Roger Levy, Florencia Reali, and Thomas L. Griffiths.
2009. Modeling the effects of memory on human on-
line sentence processing with particle filters. In Pro-
ceedings of NIPS.
Roger Levy. 2008a. Expectation-based syntactic com-
prehension. Cognition, 106:1126–1177.
Roger Levy. 2008b. A noisy-channel model of rational
human sentence comprehension under uncertain input.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 234–
243, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Srini Narayanan and Daniel Jurafsky. 2001. A Bayesian
model predicts human parse preference and read-
ing time in sentence processing. In T.G. Dietterich,
S Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems 14, pages 59–
65, Cambridge, MA. MIT Press.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2):165–
201.
Whitney Tabor and Sean Hutchins. 2004. Evidence
for self-organized sentence processing: Digging-in ef-
fects. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 30(2):431–450.
Whitney Tabor, Bruno Galantucci, and Daniel Richard-
son. 2004. Effects of merely local syntactic coher-
ence on sentence processing. Journal of Memory and
Language, 50:355–370.
Michael K Tanenhaus, Michael J Spivey-Knowlton,
Kathleen M Eberhard, and Julie C Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632–1634.
</reference>
<page confidence="0.999274">
673
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.665480">
<title confidence="0.7938135">A model of local coherence effects in human sentence as consequences of updates from bottom-up prior to posterior beliefs</title>
<author confidence="0.966166">Bicknell</author>
<affiliation confidence="0.9769795">Department of University of California, San</affiliation>
<address confidence="0.999489">9500 Gilman Dr, La Jolla, CA</address>
<abstract confidence="0.999129944444445">Human sentence processing involves integrating probabilistic knowledge from a variety of sources in order to incrementally determine the hierarchical structure for the serial input stream. While a large number of sentence processing effects have been explained in terms of comprehenders’ rational use of probabilistic information, effects of local coherences have not. We present here a new model of local coherences, viewing them as resulting from a belief-update process, and show that the relevant probabilities in our model are calculable from a probabilistic Earley parser. Finally, we demonstrate empirically that an implemented version of the model makes the correct predictions for the materials from the original experiment demonstrating local coherence effects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gerry T M Altmann</author>
<author>Yuki Kamide</author>
</authors>
<title>Incremental interpretation at verbs: Restricting the domain of subsequent reference.</title>
<date>1999</date>
<journal>Cognition,</journal>
<pages>73--247</pages>
<contexts>
<context position="1471" citStr="Altmann and Kamide, 1999" startWordPosition="209" endWordPosition="213">able from a probabilistic Earley parser. Finally, we demonstrate empirically that an implemented version of the model makes the correct predictions for the materials from the original experiment demonstrating local coherence effects. 1 Introduction The task of human sentence processing, recovering a hierarchical structure from a serial input fraught with local ambiguities, is a complex and difficult problem. There is ample evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the pro</context>
</contexts>
<marker>Altmann, Kamide, 1999</marker>
<rawString>Gerry T.M. Altmann and Yuki Kamide. 1999. Incremental interpretation at verbs: Restricting the domain of subsequent reference. Cognition, 73:247–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="14582" citStr="Earley, 1970" startWordPosition="2415" endWordPosition="2416">tactic category X ∈ N, the probability distribution P(Xk≥j i |I) for some information I is over a binary random variable indicating the presence of X. The different syntactic categories X that could span from i to any k are not mutually exclusive, hence we cannot define size of belief update as a single K-L divergence defined over multinomial distributions. grammar (SCFG), and show that our model makes the correct predictions using an SCFG for English on the original local-coherences experiment of Tabor et al. (2004). 5 Computing priors and posteriors For SCFGs, a probabilistic Earley parser (Earley, 1970; Stolcke, 1995) provides the basic quantities we need to compute the prior (2) and posterior (3) for each category X. Following Stolcke, we use capital Latin characters to denote non-terminal categories and use lowercase Greek characters to denote (possibly null) sequences of terminals and non-terminals. We write the probability that a nonterminal X can be recursively rewritten by SCFG rules as a certain series of symbols µ by P(X ⇒∗ µ) An edge built from the rule X → λµ where λ has been recognized as beginning at position i and ending at position j is denoted j : Xi → λ.µ The forward probabi</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
<author>Eugene Charniak</author>
</authors>
<title>Entropy rate constancy in text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>199--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia,</location>
<contexts>
<context position="2153" citStr="Genzel and Charniak, 2002" startWordPosition="316" endWordPosition="319">man sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears that the parser is systematically ignoring contextual information about possible syntactic structures and pursuing analyses that are probable only locally. These effects are problematic for rational models, because of the apparent failure to use all of the available information. This paper</context>
</contexts>
<marker>Genzel, Charniak, 2002</marker>
<rawString>Dmitriy Genzel and Eugene Charniak. 2002. Entropy rate constancy in text. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 199–206, Philadelphia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
<author>Eugene Charniak</author>
</authors>
<title>Variation of entropy and parse trees of sentences as a function of the sentence number.</title>
<date>2003</date>
<booktitle>In Michael Collins and Mark Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>65--72</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2180" citStr="Genzel and Charniak, 2003" startWordPosition="320" endWordPosition="323">e now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears that the parser is systematically ignoring contextual information about possible syntactic structures and pursuing analyses that are probable only locally. These effects are problematic for rational models, because of the apparent failure to use all of the available information. This paper describes a new model of l</context>
</contexts>
<marker>Genzel, Charniak, 2003</marker>
<rawString>Dmitriy Genzel and Eugene Charniak. 2003. Variation of entropy and parse trees of sentences as a function of the sentence number. In Michael Collins and Mark Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 65–72, Sapporo, Japan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>The interaction of top-down and bottom-up statistics in the resolution of syntactic category ambiguity.</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<pages>54--363</pages>
<contexts>
<context position="2405" citStr="Gibson, 2006" startWordPosition="360" endWordPosition="361">es (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears that the parser is systematically ignoring contextual information about possible syntactic structures and pursuing analyses that are probable only locally. These effects are problematic for rational models, because of the apparent failure to use all of the available information. This paper describes a new model of local coherence effects under rational syntactic comprehension, which proposes that they arise as a result of updating prior beliefs about the structures that a given string of words is likely to have to posterior beliefs abou</context>
<context position="8754" citStr="Gibson (2006)" startWordPosition="1423" endWordPosition="1424">competition, the method by which links between fragments are formed, as well as the question of precisely what tree fragments a given word will activate. While Tabor and Hutchins (2004) work out these questions in detail for the types of sentences they model, it is unclear how the model could be scaled up to make predictions for arbitrary types of sentences. That is, there is no principled system for setting the three types of parameters mentioned, and thus no clear interpretation of their values. The model put forward in this paper is an attempt to remedy this situation. A recent proposal by Gibson (2006) can also explain some of the local coherence results. Gibson’s proposal is that part-of-speech ambiguities have a special status in parsing; in effect, lexical part-ofspeech ambiguities can be thought of as one-word local coherences. In this model, a probability function P˜ is calculated over part-of-speech tags given a word. This probability for tag ti and a word w, P˜(ti|w), is proportional to the context-independent probability of ti given the word w, P(ti|w) – the bottom-up component – multiplied by a smoothed probability P3 of the tag given the context – the topdown component: P˜(ti|w) =</context>
</contexts>
<marker>Gibson, 2006</marker>
<rawString>Edward Gibson. 2006. The interaction of top-down and bottom-up statistics in the resolution of syntactic category ambiguity. Journal of Memory and Language, 54:363–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<institution>Associate for Computational Linguistics.</institution>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="1823" citStr="Hale, 2001" startWordPosition="266" endWordPosition="267">guities, is a complex and difficult problem. There is ample evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨</context>
<context position="10662" citStr="Hale, 2001" startWordPosition="1733" endWordPosition="1734">ied: he does not discuss how the top-down probabilities are calculated, nor what the precise linking hypothesis is between the final P˜ and reading times. Finally, it is not at all clear why the top-down expectations should be smoothed, since the smoothing actually has negative consequences on the processor’s performance. 4 Parsing as belief update The basic intuition behind the model presented here is that incrementally processing a sentence can be conceptualized as a process of updating one’s beliefs. Such an analogy has been used to motivate surprisal-based theories of sentence processing (Hale, 2001; Levy, 2008a), where beliefs about the structure of a sentence after seeing the first i − 1 words in the sentence, which we denote as wi−1 0 , are updated upon encountering wi. In this case, the surprisal of a word (− log P(wi|wi−1 0 )) is equivalent to the Kullback-Leibler divergence of the beliefs after wi0 from the beliefs after wi−1 0 (Levy, 2008a). Our model focuses on another belief-update process in sentence processing: updating beliefs about the structures that a string of words is likely to have independent of context to beliefs about what structures it is likely to have in context. </context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, volume 2, pages 159–166, New Brunswick, NJ. Associate for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<pages>20--137</pages>
<contexts>
<context position="1811" citStr="Jurafsky, 1996" startWordPosition="264" endWordPosition="265"> with local ambiguities, is a complex and difficult problem. There is ample evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Koni</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>Daniel Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20:137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
</authors>
<title>The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>317--324</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2194" citStr="Keller, 2004" startWordPosition="324" endWordPosition="325">ationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears that the parser is systematically ignoring contextual information about possible syntactic structures and pursuing analyses that are probable only locally. These effects are problematic for rational models, because of the apparent failure to use all of the available information. This paper describes a new model of local coherence</context>
</contexts>
<marker>Keller, 2004</marker>
<rawString>Frank Keller. 2004. The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data. In Dekang Lin and Dekai Wu, editors, Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 317– 324, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Konieczny</author>
<author>Daniel M¨uller</author>
</authors>
<title>Local coherence interpretation in written and spoken language.</title>
<date>2007</date>
<booktitle>Presented at the 20th Annual CUNY Conference on Human Sentence Processing.</booktitle>
<location>La Jolla, CA.</location>
<marker>Konieczny, M¨uller, 2007</marker>
<rawString>Lars Konieczny and Daniel M¨uller. 2007. Local coherence interpretation in written and spoken language. Presented at the 20th Annual CUNY Conference on Human Sentence Processing. La Jolla, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>T Florian Jaeger</author>
</authors>
<title>Speakers optimize information density through syntactic reduction.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>849--856</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2218" citStr="Levy and Jaeger, 2007" startWordPosition="326" endWordPosition="329">ng probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears that the parser is systematically ignoring contextual information about possible syntactic structures and pursuing analyses that are probable only locally. These effects are problematic for rational models, because of the apparent failure to use all of the available information. This paper describes a new model of local coherence effects under rational </context>
</contexts>
<marker>Levy, Jaeger, 2007</marker>
<rawString>Roger Levy and T. Florian Jaeger. 2007. Speakers optimize information density through syntactic reduction. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 849–856, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Florencia Reali</author>
<author>Thomas L Griffiths</author>
</authors>
<title>Modeling the effects of memory on human online sentence processing with particle filters.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1886" citStr="Levy et al., 2009" startWordPosition="275" endWordPosition="278">mple evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears that the parser is syst</context>
</contexts>
<marker>Levy, Reali, Griffiths, 2009</marker>
<rawString>Roger Levy, Florencia Reali, and Thomas L. Griffiths. 2009. Modeling the effects of memory on human online sentence processing with particle filters. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>106--1126</pages>
<contexts>
<context position="1865" citStr="Levy, 2008" startWordPosition="273" endWordPosition="274">m. There is ample evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears th</context>
<context position="10674" citStr="Levy, 2008" startWordPosition="1735" endWordPosition="1736"> not discuss how the top-down probabilities are calculated, nor what the precise linking hypothesis is between the final P˜ and reading times. Finally, it is not at all clear why the top-down expectations should be smoothed, since the smoothing actually has negative consequences on the processor’s performance. 4 Parsing as belief update The basic intuition behind the model presented here is that incrementally processing a sentence can be conceptualized as a process of updating one’s beliefs. Such an analogy has been used to motivate surprisal-based theories of sentence processing (Hale, 2001; Levy, 2008a), where beliefs about the structure of a sentence after seeing the first i − 1 words in the sentence, which we denote as wi−1 0 , are updated upon encountering wi. In this case, the surprisal of a word (− log P(wi|wi−1 0 )) is equivalent to the Kullback-Leibler divergence of the beliefs after wi0 from the beliefs after wi−1 0 (Levy, 2008a). Our model focuses on another belief-update process in sentence processing: updating beliefs about the structures that a string of words is likely to have independent of context to beliefs about what structures it is likely to have in context. A bit more f</context>
<context position="21844" citStr="Levy (2008" startWordPosition="3712" endWordPosition="3713">rom Tabor et al. (2004). To generate quantitative predictions, we created a small grammar of relevant syntactic rules, and estimated the rule probabilities from syntactically annotated text. We calculated summed K-L divergence of the prior from the posterior vector for each word in the Tabor et al. items, and predict this sum to be largest at the critical region when the sentence has an effect of local coherence. 6.1 Methods 6.1.1 Grammar We defined a small SCFG for the problem, and estimated its rule probabilities using the parsed Brown corpus. The resulting SCFG is identical to that used in Levy (2008b) and is given in Table 2. 6.1.2 Lexicon Lexical rewrite probabilities for part-of-speech tags were also estimated using the entire parsed Brown corpus. 6.1.3 Materials The materials were taken from Experiment 1 of Tabor et al. (2004). We removed 8 of their 20 items for which our trained model either did not know the critical verb or did not know the syntactic structure of some part of the sentence. For the other 12 items, we replaced unknown nouns (9 instances) and unknown non-critical verbs (2 instances), changed one plural noun to singular, and dropped one sentenceinitial prepositional phr</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008a. Expectation-based syntactic comprehension. Cognition, 106:1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>A noisy-channel model of rational human sentence comprehension under uncertain input.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>234--243</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1865" citStr="Levy, 2008" startWordPosition="273" endWordPosition="274">m. There is ample evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which it appears th</context>
<context position="10674" citStr="Levy, 2008" startWordPosition="1735" endWordPosition="1736"> not discuss how the top-down probabilities are calculated, nor what the precise linking hypothesis is between the final P˜ and reading times. Finally, it is not at all clear why the top-down expectations should be smoothed, since the smoothing actually has negative consequences on the processor’s performance. 4 Parsing as belief update The basic intuition behind the model presented here is that incrementally processing a sentence can be conceptualized as a process of updating one’s beliefs. Such an analogy has been used to motivate surprisal-based theories of sentence processing (Hale, 2001; Levy, 2008a), where beliefs about the structure of a sentence after seeing the first i − 1 words in the sentence, which we denote as wi−1 0 , are updated upon encountering wi. In this case, the surprisal of a word (− log P(wi|wi−1 0 )) is equivalent to the Kullback-Leibler divergence of the beliefs after wi0 from the beliefs after wi−1 0 (Levy, 2008a). Our model focuses on another belief-update process in sentence processing: updating beliefs about the structures that a string of words is likely to have independent of context to beliefs about what structures it is likely to have in context. A bit more f</context>
<context position="21844" citStr="Levy (2008" startWordPosition="3712" endWordPosition="3713">rom Tabor et al. (2004). To generate quantitative predictions, we created a small grammar of relevant syntactic rules, and estimated the rule probabilities from syntactically annotated text. We calculated summed K-L divergence of the prior from the posterior vector for each word in the Tabor et al. items, and predict this sum to be largest at the critical region when the sentence has an effect of local coherence. 6.1 Methods 6.1.1 Grammar We defined a small SCFG for the problem, and estimated its rule probabilities using the parsed Brown corpus. The resulting SCFG is identical to that used in Levy (2008b) and is given in Table 2. 6.1.2 Lexicon Lexical rewrite probabilities for part-of-speech tags were also estimated using the entire parsed Brown corpus. 6.1.3 Materials The materials were taken from Experiment 1 of Tabor et al. (2004). We removed 8 of their 20 items for which our trained model either did not know the critical verb or did not know the syntactic structure of some part of the sentence. For the other 12 items, we replaced unknown nouns (9 instances) and unknown non-critical verbs (2 instances), changed one plural noun to singular, and dropped one sentenceinitial prepositional phr</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008b. A noisy-channel model of rational human sentence comprehension under uncertain input. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 234– 243, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Daniel Jurafsky</author>
</authors>
<title>A Bayesian model predicts human parse preference and reading time in sentence processing.</title>
<date>2001</date>
<booktitle>Advances in Neural Information Processing Systems 14,</booktitle>
<pages>59--65</pages>
<editor>In T.G. Dietterich, S Becker, and Z. Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1853" citStr="Narayanan and Jurafsky, 2001" startWordPosition="268" endWordPosition="272">a complex and difficult problem. There is ample evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). One class of results from the literature that has not yet been explained in terms of a rational comprehender strategy is that of local coherence effects (Taboret al., 2004; Gibson, 2006; Konieczny and M¨uller, 2007), cases in which i</context>
</contexts>
<marker>Narayanan, Jurafsky, 2001</marker>
<rawString>Srini Narayanan and Daniel Jurafsky. 2001. A Bayesian model predicts human parse preference and reading time in sentence processing. In T.G. Dietterich, S Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 59– 65, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>201</pages>
<contexts>
<context position="14598" citStr="Stolcke, 1995" startWordPosition="2417" endWordPosition="2418">y X ∈ N, the probability distribution P(Xk≥j i |I) for some information I is over a binary random variable indicating the presence of X. The different syntactic categories X that could span from i to any k are not mutually exclusive, hence we cannot define size of belief update as a single K-L divergence defined over multinomial distributions. grammar (SCFG), and show that our model makes the correct predictions using an SCFG for English on the original local-coherences experiment of Tabor et al. (2004). 5 Computing priors and posteriors For SCFGs, a probabilistic Earley parser (Earley, 1970; Stolcke, 1995) provides the basic quantities we need to compute the prior (2) and posterior (3) for each category X. Following Stolcke, we use capital Latin characters to denote non-terminal categories and use lowercase Greek characters to denote (possibly null) sequences of terminals and non-terminals. We write the probability that a nonterminal X can be recursively rewritten by SCFG rules as a certain series of symbols µ by P(X ⇒∗ µ) An edge built from the rule X → λµ where λ has been recognized as beginning at position i and ending at position j is denoted j : Xi → λ.µ The forward probability of that edg</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Whitney Tabor</author>
<author>Sean Hutchins</author>
</authors>
<title>Evidence for self-organized sentence processing: Digging-in effects.</title>
<date>2004</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="8326" citStr="Tabor and Hutchins (2004)" startWordPosition="1345" endWordPosition="1349">6 tem stabilizes to the correct parse, and reading times for each word are modeled as the time the system takes to stabilize after reading a word. Stabilization takes longer for locally coherent regions because the locally coherent parse will be created and compete with the globally grammatical parse. There are, however, unresolved issues with this model. The model has a number of free parameters, relating to the equations used for the competition, the method by which links between fragments are formed, as well as the question of precisely what tree fragments a given word will activate. While Tabor and Hutchins (2004) work out these questions in detail for the types of sentences they model, it is unclear how the model could be scaled up to make predictions for arbitrary types of sentences. That is, there is no principled system for setting the three types of parameters mentioned, and thus no clear interpretation of their values. The model put forward in this paper is an attempt to remedy this situation. A recent proposal by Gibson (2006) can also explain some of the local coherence results. Gibson’s proposal is that part-of-speech ambiguities have a special status in parsing; in effect, lexical part-ofspee</context>
<context position="11751" citStr="Tabor and Hutchins (2004)" startWordPosition="1924" endWordPosition="1927">tures that a string of words is likely to have independent of context to beliefs about what structures it is likely to have in context. A bit more formally, it views the process of integrating a string of words w i into a sentence as beginning with a ‘bottomup’ prior distribution of syntactic structures likely to span w i and integrating that with ‘top-down’ knowledge from the previous words in the sentence wi0 in order to reach a posterior distribution conditioning on w 0 over which structures actually can span w i. This belief update process can be viewed as a rational reconstruction of the Tabor and Hutchins (2004) model, where – instead of the system dynamics of competition between arbitrary tree fragments – differences between prior and posterior probability distributions over syntactic structures determine processing difficulty. More formally still, when integrating w i into a sentence, for each syntactic category X, we can define the prior probability conditioned only on w i that w i will form the beginning of that category, i.e., that an X exists which begins at index i and spans at least 667 through j: Prior: P(Xk≥j i |wji) (2) It is important to note here that this prior probability is conditiona</context>
<context position="26861" citStr="Tabor &amp; Hutchins (2004)" startWordPosition="4613" endWordPosition="4616">o make predictions for the materials for the original experimental result of effects of local coherences (Taboret al., 2004). Our results demonstrated that the model predicts difficulty to occur at the correct part of the sentence in the correct condition. We improve on existing models in two ways. First, we make predictions for where local coherences should obtain for an arbitrary SCFG, not just one particular class of sentences. This allows the model to scale up for use with a broad coverage grammar and to make predictions for arbitrary sentences, which was not possible with a model such as Tabor &amp; Hutchins (2004). Second, our model gives a rational basis to an effect which has typically been seen to result from irrationality of the human sentence processor. Specifically, the cost that our model describes of updating bottom-up prior beliefs to in-context posterior beliefs can be viewed as resulting from a rational process in the case that the bottom-up prior is available to the human sentence processor more rapidly than the in-context posterior. Interestingly, the fact that the prior is actually more difficult to compute than the posterior suggests that the only way it would be available more rapidly i</context>
</contexts>
<marker>Tabor, Hutchins, 2004</marker>
<rawString>Whitney Tabor and Sean Hutchins. 2004. Evidence for self-organized sentence processing: Digging-in effects. Journal of Experimental Psychology: Learning, Memory, and Cognition, 30(2):431–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Whitney Tabor</author>
<author>Bruno Galantucci</author>
<author>Daniel Richardson</author>
</authors>
<title>Effects of merely local syntactic coherence on sentence processing.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<pages>50--355</pages>
<contexts>
<context position="4011" citStr="Tabor et al. (2004)" startWordPosition="613" endWordPosition="616">escribes existing models of the phenomenon. Following that, Sections 4–5 describe our model and its computa665 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 665–673, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Figure 1: The difficulty of explaining localcoherence effects as traditional garden-pathing. tion from a probabilistic Earley parser. Section 6 presents the results of an experiment showing that our model makes the correct predictions for the local coherence effects seen in the original paper by Tabor et al. (2004). Finally, Section 7 concludes and discusses the insight our model gives into human performance. 2 Local coherences The first studies to report effects of local coherences are described in Tabor et al. (2004). In Experiment 1, they use a self-paced reading task and materials containing relative clauses (RCs) attached to nouns in non-subject position as in (1). (1) a. The coach smiled at the player tossed a frisbee by the opposing team. b. The coach smiled at the player who was tossed a frisbee by the opposing team. c. The coach smiled at the player thrown a frisbee by the opposing team. d. The</context>
<context position="9827" citStr="Tabor et al. (2004)" startWordPosition="1595" endWordPosition="1598">rd w, P(ti|w) – the bottom-up component – multiplied by a smoothed probability P3 of the tag given the context – the topdown component: P˜(ti|w) = P(ti|w)P3(ti|context) (1) P(t|w)P3(t|context) tET Difficulty is predicted to be high when the probability P˜ of the correct tag is low. Because the top-down probabilities are smoothed to allow for all possible parts-of-speech, any word which is lexically ambiguous will be more difficult to process, regardless of whether it is ambiguous or not in its context. This can thus explain some of the difference between the ambiguous and unambiguous verbs in Tabor et al. (2004). It is not clear, however, under such a model why the super-additive interaction would obtain—that is, why (1a) should be so much harder than (1b) starting at the word tossed. In addition, Gibson’s model is a bit underspecified: he does not discuss how the top-down probabilities are calculated, nor what the precise linking hypothesis is between the final P˜ and reading times. Finally, it is not at all clear why the top-down expectations should be smoothed, since the smoothing actually has negative consequences on the processor’s performance. 4 Parsing as belief update The basic intuition behi</context>
<context position="14492" citStr="Tabor et al. (2004)" startWordPosition="2399" endWordPosition="2403">y probabilistic incremental Earley parsing with a stochastic context-free 1Note that for each syntactic category X ∈ N, the probability distribution P(Xk≥j i |I) for some information I is over a binary random variable indicating the presence of X. The different syntactic categories X that could span from i to any k are not mutually exclusive, hence we cannot define size of belief update as a single K-L divergence defined over multinomial distributions. grammar (SCFG), and show that our model makes the correct predictions using an SCFG for English on the original local-coherences experiment of Tabor et al. (2004). 5 Computing priors and posteriors For SCFGs, a probabilistic Earley parser (Earley, 1970; Stolcke, 1995) provides the basic quantities we need to compute the prior (2) and posterior (3) for each category X. Following Stolcke, we use capital Latin characters to denote non-terminal categories and use lowercase Greek characters to denote (possibly null) sequences of terminals and non-terminals. We write the probability that a nonterminal X can be recursively rewritten by SCFG rules as a certain series of symbols µ by P(X ⇒∗ µ) An edge built from the rule X → λµ where λ has been recognized as be</context>
<context position="21257" citStr="Tabor et al. (2004)" startWordPosition="3610" endWordPosition="3614">i = i&apos;). Given that all our terms are conditional on the existence of all words in the sentence up to i&apos; (E0), the probability of a starting position P(i) is the probability of drawing i&apos; randomly from the set of positions in sentences generated by the grammar such that all words up to that position exist. For most language grammars, this distribution can be easily approximated by a sample of sentences generated from the SCFG, since most of the probability mass is concentrated in small indices. 6 Experiment We tested the predictions of an implemented version of our model on the materials from Tabor et al. (2004). To generate quantitative predictions, we created a small grammar of relevant syntactic rules, and estimated the rule probabilities from syntactically annotated text. We calculated summed K-L divergence of the prior from the posterior vector for each word in the Tabor et al. items, and predict this sum to be largest at the critical region when the sentence has an effect of local coherence. 6.1 Methods 6.1.1 Grammar We defined a small SCFG for the problem, and estimated its rule probabilities using the parsed Brown corpus. The resulting SCFG is identical to that used in Levy (2008b) and is giv</context>
</contexts>
<marker>Tabor, Galantucci, Richardson, 2004</marker>
<rawString>Whitney Tabor, Bruno Galantucci, and Daniel Richardson. 2004. Effects of merely local syntactic coherence on sentence processing. Journal of Memory and Language, 50:355–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J Spivey-Knowlton</author>
<author>Kathleen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="1444" citStr="Tanenhaus et al., 1995" startWordPosition="205" endWordPosition="208"> in our model are calculable from a probabilistic Earley parser. Finally, we demonstrate empirically that an implemented version of the model makes the correct predictions for the materials from the original experiment demonstrating local coherence effects. 1 Introduction The task of human sentence processing, recovering a hierarchical structure from a serial input fraught with local ambiguities, is a complex and difficult problem. There is ample evidence that comprehenders understand sentences incrementally, constructing interpretations of partial structure and expectations for future input (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Many of the main behavioral findings in the study of human sentence processing have now been explained computationally. Using probabilistic models trained on large-scale corpora, effects such as global and incremental disambiguation preferences have been shown to be a result of the rational use of syntactic probabilities (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that ta</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Michael K Tanenhaus, Michael J Spivey-Knowlton, Kathleen M Eberhard, and Julie C Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>