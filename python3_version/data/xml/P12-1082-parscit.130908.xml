<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000431">
<title confidence="0.99292">
Semi-supervised Dependency Parsing using Lexical Affinities
</title>
<author confidence="0.74032">
Seyed Abolghasem Mirroshandel†,* Alexis Nasr† Joseph Le Roux*
</author>
<affiliation confidence="0.678162">
†Laboratoire d’Informatique Fondamentale de Marseille- CNRS - UMR 7279
Universit´e Aix-Marseille, Marseille, France
*LIPN, Universit´e Paris Nord &amp; CNRS,Villetaneuse, France
*Computer Engineering Department, Sharif university of Technology, Tehran, Iran
</affiliation>
<email confidence="0.7033695">
(ghasem.mirroshandel@lif.univ-mrs.fr, alexis.nasr@lif.univ-mrs.fr,
leroux@univ-paris13.fr)
</email>
<sectionHeader confidence="0.983326" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99941375">
Treebanks are not large enough to reliably
model precise lexical phenomena. This de-
ficiency provokes attachment errors in the
parsers trained on such data. We propose
in this paper to compute lexical affinities,
on large corpora, for specific lexico-syntactic
configurations that are hard to disambiguate
and introduce the new information in a parser.
Experiments on the French Treebank showed
a relative decrease of the error rate of 7.1% La-
beled Accuracy Score yielding the best pars-
ing results on this treebank.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991196875">
Probabilistic parsers are usually trained on treebanks
composed of few thousands sentences. While this
amount of data seems reasonable for learning syn-
tactic phenomena and, to some extent, very frequent
lexical phenomena involving closed parts of speech
(POS), it proves inadequate when modeling lexical
dependencies between open POS, such as nouns,
verbs and adjectives. This fact was first recognized
by (Bikel, 2004) who showed that bilexical depen-
dencies were barely used in Michael Collins’ parser.
The work reported in this paper aims at a better
modeling of such phenomena by using a raw corpus
that is several orders of magnitude larger than the
treebank used for training the parser. The raw cor-
pus is first parsed and the computed lexical affinities
between lemmas, in specific lexico-syntactic config-
urations, are then injected back in the parser. Two
outcomes are expected from this procedure, the first
is, as mentioned above, a better modeling of bilexi-
cal dependencies and the second is a method to adapt
a parser to new domains.
The paper is organized as follows. Section 2 re-
views some work on the same topic and highlights
their differences with ours. In section 3, we describe
the parser that we use in our experiments and give
a detailed description of the frequent attachment er-
rors. Section 4 describes how lexical affinities be-
tween lemmas are calculated and their impact is then
evaluated with respect to the attachment errors made
by the parser. Section 5 describes three ways to in-
tegrate the lexical affinities in the parser and reports
the results obtained with the three methods.
</bodyText>
<sectionHeader confidence="0.996191" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999258555555556">
Coping with lexical sparsity of treebanks using raw
corpora has been an active direction of research for
many years.
One simple and effective way to tackle this prob-
lem is to put together words that share, in a large
raw corpus, similar linear contexts, into word clus-
ters. The word occurrences of the training treebank
are then replaced by their cluster identifier and a new
parser is trained on the transformed treebank. Us-
ing such techniques (Koo et al., 2008) report signi-
ficative improvement on the Penn Treebank (Marcus
et al., 1993) and so do (Candito and Seddah, 2010;
Candito and Crabb´e, 2009) on the French Treebank
(Abeill´e et al., 2003).
Another series of papers (Volk, 2001; Nakov
and Hearst, 2005; Pitler et al., 2010; Zhou et al.,
2011) directly model word co-occurrences. Co-
occurrences of pairs of words are first collected in a
</bodyText>
<page confidence="0.981046">
777
</page>
<note confidence="0.985805">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999946823529412">
raw corpus or internet n-grams. Based on the counts
produced, lexical affinity scores are computed. The
detection of pairs of words co-occurrences is gen-
erally very simple, it is either based on the direct
adjacency of the words in the string or their co-
occurrence in a window of a few words. (Bansal
and Klein, 2011; Nakov and Hearst, 2005) rely on
the same sort of techniques but use more sophisti-
cated patterns, based on simple paraphrase rules, for
identifying co-occurrences.
Our work departs from those approaches by the
fact that we do not extract the lexical information
directly on a raw corpus, but we first parse it and
then extract the co-occurrences on the parse trees,
based on some predetermined lexico-syntactic pat-
terns. The first reason for this choice is that the lin-
guistic phenomena that we are interested in, such as
as PP attachment, coordination, verb subject and ob-
ject can range over long distances, beyond what is
generally taken into account when working on lim-
ited windows. The second reason for this choice was
to show that the performances that the NLP commu-
nity has reached on parsing, combined with the use
of confidence measures allow to use parsers to ex-
tract accurate lexico-syntactic information, beyond
what can be found in limited annotated corpora.
Our work can also be compared with self train-
ing approaches to parsing (McClosky et al., 2006;
Suzuki et al., 2009; Steedman et al., 2003; Sagae
and Tsujii, 2007) where a parser is first trained on
a treebank and then used to parse a large raw cor-
pus. The parses produced are then added to the ini-
tial treebank and a new parser is trained. The main
difference between these approaches and ours is that
we do not directly add the output of the parser to the
training corpus, but extract precise lexical informa-
tion that is then re-injected in the parser. In the self
training approach, (Chen et al., 2009) is quite close
to our work: instead of adding new parses to the tree-
bank, the occurrence of simple interesting subtrees
are detected in the parses and introduced as new fea-
tures in the parser.
The way we introduce lexical affinity measures in
the parser, in 5.1, shares some ideas with (Anguiano
and Candito, 2011), who modify some attachments
in the parser output, based on lexical information.
The main difference is that we only take attachments
that appear in an n-best parse list into account, while
they consider the first best parse and compute all po-
tential alternative attachments, that may not actually
occur in the n-best forests.
</bodyText>
<sectionHeader confidence="0.968234" genericHeader="method">
3 The Parser
</sectionHeader>
<bodyText confidence="0.999907375">
The parser used in this work is the second order
graph based parser (McDonald et al., 2005; K¨ubler
et al., 2009) implementation of (Bohnet, 2010). The
parser was trained on the French Treebank (Abeill´e
et al., 2003) which was transformed into dependency
trees by (Candito et al., 2009). The size of the tree-
bank and its decomposition into train, development
and test sets is represented in table 1.
</bodyText>
<table confidence="0.983556">
nb of sentences nb of words
FTB TRAIN 9 881 278 083
FTB DEV 1 239 36 508
FTB TEST 1 235 36 340
</table>
<tableCaption confidence="0.999756">
Table 1: Size and decomposition of the French Treebank
</tableCaption>
<bodyText confidence="0.9964396">
The part of speech tagging was performed with
the MELT tagger (Denis and Sagot, 2010) and lem-
matized with the MACAON tool suite (Nasr et al.,
2011). The parser gave state of the art results for
parsing of French, reported in table 2.
</bodyText>
<table confidence="0.833239">
pred. POS tags gold POS tags
punct no punct punct no punct
LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
</table>
<tableCaption confidence="0.960285333333333">
Table 2: Labeled and unlabeled accuracy score for auto-
matically predicted and gold POS tags with and without
taking into account punctuation on FTB TEST.
</tableCaption>
<bodyText confidence="0.993117615384615">
Figure 1 shows the distribution of the 100 most
common error types made by the parser. In this
figure, x axis shows the error types and y axis
shows the error ratio of the related error type
number of errors of the specific type
( total number of errors ). We define an error
type by the POS tag of the governor and the POS
tag of the dependent. The figure presents a typical
Zipfian distribution with a low number of frequent
error types and a large number of unfrequent error
types. The shape of the curve shows that concen-
trating on some specific frequent errors in order to
increase the parser accuracy is a good strategy.
</bodyText>
<page confidence="0.996602">
778
</page>
<table confidence="0.986000785714286">
dependency freq. acc. contrib. name
N-*N 1.50 72.23 2.91
V -* a` 0.88 69.11 2.53 VaN
V—suj -* N 3.43 93.03 2.53 SBJ
N CC 0.77 69.78 2.05 NcN
N de 3.70 92.07 2.05 NdeN
V de 0.66 74.68 1.62 VdeN
V—obj -* N 2.74 90.43 1.60 OBJ
V en 0.66 81.20 1.24
V pour 0.46 67.78 1.10
N ADJ 6.18 96.60 0.96 ADJ
N -* a` 0.29 70.64 0.72 NaN
N pour 0.12 38.64 0.67
N en 0.15 47.69 0.57
</table>
<figure confidence="0.978705272727273">
error ratio
0.14
0.12
0.08
0.06
0.04
0.02
0.1
0
10 20 30 40 50 60 70 80 90 100
Error Type
</figure>
<figureCaption confidence="0.999998">
Figure 1: Distribution of the types of errors
</figureCaption>
<bodyText confidence="0.999945181818182">
Table 3 gives a finer description of the most com-
mon types of error made by the parser. Here we
define more precise patterns for errors, where some
lexical values are specified (for prepositions) and, in
some cases, the nature of the dependency is taken
into account. Every line of the table corresponds to
one type of error. The first column describes the
error type. The second column indicates the fre-
quency of this type of dependency in the corpus. The
third one displays the accuracy for this type of de-
pendency (the number of dependencies of this type
correctly analyzed by the parser divided by the to-
tal number of dependencies of this type). The fourth
column shows the contribution of the errors made on
this type of dependency to the global error rate. The
last column associates a name with some of the error
types that will prove useful in the remainder of the
paper to refer to the error type.
Table 3 shows two different kinds of errors that
impact the global error rate. The first one concerns
very common dependencies that have a high accu-
racy but, due to their frequency, hurt the global er-
ror rate of the parser. The second one concerns low
frequency, low accuracy dependency types. Lines 2
and 3, respectively attachment of the preposition a` to
a verb and the subject dependency illustrate such a
contrast. They both impact the total error rate in the
same way (2.53% of the errors). But the first one
is a low frequency low accuracy type (respectively
0.88% and 69.11%) while the second is a high fre-
quency high accuracy type (respectively 3.43% and
93.03%). We will see in 4.2.2 that our method be-
haves quite differently on these two types of error.
</bodyText>
<tableCaption confidence="0.996218">
Table 3: The 13 most common error types
</tableCaption>
<sectionHeader confidence="0.966167" genericHeader="method">
4 Creating the Lexical Resource
</sectionHeader>
<bodyText confidence="0.995864241379311">
The lexical resource is a collection of tuples
(C, g, d, s) where C is a lexico-syntactic configu-
ration, g is a lemma, called the governor of the
configuration, d is another lemma called the depen-
dent and s is a numerical value between 0 and 1,
called the lexical affinity score, which accounts for
the strength of the association between g and d in
obj
the context C. For example the tuple ((V,g) -*
(N, d), eat, oyster, 0.23) defines a simple configu-
ration (V, g) obj-* (N, d) that is an object depen-
dency between verb g and noun d. When replac-
ing variables g and d in C respectively with eat
and oyster, we obtain the fully specified lexico syn-
tactic pattern(V, eat) obj-* (N, oyster), that we call
an instantiated configuration. The numerical value
0.23 accounts for how much eat and oyster like
to co-occur in the verb-object configuration. Con-
figurations can be of arbitrary complexity but they
have to be generic enough in order to occur fre-
quently in a corpus yet be specific enough to model
a precise lexico syntactic phenomenon. The context
(*, g) �(*, d), for example is very generic but does
not model a precise linguistic phenomenon, as selec-
tional preferences of a verb, for example. Moreover,
configurations need to be error-prone. In the per-
spective of increasing a parser performances, there
is no point in computing lexical affinity scores be-
tween words that appear in a configuration for which
</bodyText>
<page confidence="0.991027">
779
</page>
<bodyText confidence="0.999800777777778">
the parser never makes mistakes.
The creation of the lexical resource is a three stage
process. The first step is the definition of configura-
tions, the second one is the collection of raw counts
from the machine parsed corpora and the third one is
the computation of lexical affinities based on the raw
counts. The three steps are described in the follow-
ing subsection while the evaluation of the created
resource is reported in subsection 4.2.
</bodyText>
<subsectionHeader confidence="0.999825">
4.1 Computing Lexical Affinities
</subsectionHeader>
<bodyText confidence="0.999925625">
A set of 9 configurations have been defined. Their
selection is a manual process based on the analysis
of the errors made by the parser, described in sec-
tion 3, as well as on the linguistic phenomena they
model. The list of the 9 configurations is described
in Table 4. As one can see on this table, configu-
rations are usually simple, made up of one or two
dependencies. Linguistically, configurations OBJ
and SBJ concern subject and object attachments,
configuration ADJ is related to attachments of ad-
jectives to nouns and configurations NdeN, VdeN,
VaN, and NaN indicate prepositional attachments.
We have restricted ourselves here to two common
French prepositions a� and de. Configurations NcN
and VcV deal respectively with noun and verb coor-
dination.
</bodyText>
<table confidence="0.874528333333333">
Name Description
OBJ (V, g) ��� (N, d)
__+
SBJ (V, g) s��� (N, d)
__+
ADJ (N, g) __+ ADJ
NdeN (N, g) (P, de) (N, d)
VdeN (V, g) (P, de) (N, d)
NaN (N, g) (P, a) (N, d)
VaN (V, g) (P, h) (N, d)
NcN (N, g) (CC, *) (N, d)
VcV (V, g) (CC, *) (V, d)
</table>
<tableCaption confidence="0.998177">
Table 4: List of the 9 configurations.
</tableCaption>
<bodyText confidence="0.997629833333333">
The computation of the number of occurrences of
an instantiated configuration in the corpus is quite
straightforward, it consists in traversing the depen-
dency trees produced by the parser and detect the
occurrences of this configuration.
At the end of the counts collection, we have gath-
</bodyText>
<table confidence="0.984694">
CORPUS Sent. nb. Tokens nb.
AFP 1 024 797 31 486 618
EST REP 1 103 630 19 635 985
WIKI 1 592 035 33 821 460
TOTAL 3 720 462 84 944 063
</table>
<tableCaption confidence="0.999179">
Table 5: sizes of the corpora used to gather lexical counts
</tableCaption>
<bodyText confidence="0.993876533333333">
ered for every lemma l its number of occurrences as
governor (resp. dependent) of configuration C in the
corpus, noted C(C,l, *) (resp. C(C, *,l)), as well as
the number of occurrences of configuration C with
lemma lg as a governor and lemma ld as a depen-
dent, noted C(C, lg, ld). We are now in a position
to compute the score s(C, lg, ld). This score should
reflect the tendency of lg and ld to appear together
in configuration C. It should be maximal if when-
ever lg occurs as the governor of configuration C,
the dependent position is occupied by ld and, sym-
metrically, if whenever ld occurs as the dependent of
configuration C, the governor position is occupied
by lg. A function that conforms such a behavior is
the following:
</bodyText>
<equation confidence="0.997994">
(C(C, lg, ld) �
1 C(C, lg, ld)
s(C, lg, ld) = 2 C(C, lg, *) + C(C, *, ld)
</equation>
<bodyText confidence="0.9999388">
it takes its values between 0 (lg and ld never
co-occur) and 1 (g and d always co-occur). This
function is close to pointwise mutual information
(Church and Hanks, 1990) but takes its values be-
tween 0 and 1.
</bodyText>
<subsectionHeader confidence="0.919297">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999846714285714">
Lexical affinities were computed on three corpora of
slightly different genres. The first one, is a collection
of news report of the French press agency Agence
France Presse, the second is a collection of news-
paper articles from a local French newspaper : l’Est
Republicain. The third one is a collection of articles
from the French Wikipedia. The size of the different
corpora are detailed in table 5. The corpus was first
POS tagged, lemmatized and parsed in order to get
the 50 best parses for every sentence. Then the lexi-
cal resource was built, based on the 9 configurations
described in table 4.
The lexical resource has been evaluated on
FTB DEV with respect to two measures: coverage
</bodyText>
<page confidence="0.976146">
780
</page>
<bodyText confidence="0.9973795">
and correction rate, described in the next two sec-
tions.
</bodyText>
<subsectionHeader confidence="0.559628">
4.2.1 Coverage
</subsectionHeader>
<bodyText confidence="0.998123863636364">
Coverage measures the instantiated configura-
tions present in the evaluation corpus that are in the
resource. The results are presented in table 6. Every
line represents a configuration, the second column
indicates the number of different instantiations of
this configuration in the evaluation corpus, the third
one indicates the number of instantiated configura-
tions that were actually found in the lexical resource
and the fourth column shows the coverage for this
configuration, which is the ratio third column over
the second. Last column represents the coverage of
the training corpus (the lexical resource is extracted
on the training corpus) and the last line represents
the same quantities computed on all configurations.
Table 6 shows two interesting results: firstly the
high variability of coverage with respect to configu-
rations, and secondly the low coverage when the lex-
ical resource is computed on the training corpus, this
fact being consistent with the conclusions of (Bikel,
2004). A parser trained on a treebank cannot be ex-
pected to reliably select the correct governor in lex-
ically sensitive cases.
</bodyText>
<table confidence="0.998337818181818">
Conf. occ. pres. cov. T cov.
OBJ 1017 709 0.70 0.21
SBJ 1210 825 0.68 0.24
ADJ 1791 1239 0.69 0.33
NdeN 1909 1287 0.67 0.31
VdeN 189 107 0.57 0.16
NaN 123 61 0.50 0.20
VaN 422 273 0.65 0.23
NcN 220 55 0.25 0.10
VcV 165 93 0.56 0.04
E 7046 4649 0.66 0.27
</table>
<tableCaption confidence="0.998128">
Table 6: Coverage of the lexical resource over FTB DEV.
</tableCaption>
<subsubsectionHeader confidence="0.796884">
4.2.2 Correction Rate
</subsubsectionHeader>
<bodyText confidence="0.995884321428571">
While coverage measures how many instantiated
configurations that occur in the treebank are actu-
ally present in the lexical resource, it does not mea-
sure if the information present in the lexical resource
can actually help correcting the errors made by the
parser.
We define Correction Rate (CR) as a way to ap-
proximate the usefulness of the data. Given a word
d present in a sentence S and a configuration C, the
set of all potential governors of d in configuration
C, in all the n-best parses produced by the parser is
computed. This set is noted G = {gl, ... , gj}. Let
us note GL the element of G that maximizes the lex-
ical affinity score. When the lexical resource gives
no score to any of the elements of G, GL is left un-
specified.
Ideally, G should not be the set of governors in
the n-best parses but the set of all possible governors
for d in sentence S. Since we have no simple way
to compute the latter, we will content ourselves with
the former as an approximation of the latter.
Let us note GH the governor of d in the (first)
best parse produced and GR the governor of d in the
correct parse. CR measures the effect of replacing
GH with GL.
We have represented in table 7 the different sce-
narios that can happen when comparing GH, GR
and GL.
</bodyText>
<table confidence="0.924834">
GH = GR GL = GR or GL unspec. CC
GL =� GR CE
GH =� GR GL = GR EC
GL =� GR or GL unspec. EE
GREG NA
</table>
<tableCaption confidence="0.8431305">
Table 7: Five possible scenarios when comparing the
governor of a word produced by the parser (GH), in
the reference parse (GR) and according to the lexical re-
source (GL).
</tableCaption>
<bodyText confidence="0.999962933333333">
In scenarios CC and CE, the parser did not make
a mistake (the first letter, C, stands for correct). In
scenario CC, the lexical affinity score was compat-
ible with the choice of the parser or the lexical re-
source did not select any candidate. In scenario CE,
the lexical resource introduced an error. In scenar-
ios EC and EE, the parser made an error. In EC,
the error was corrected by the lexical resource while
in EE, it wasn’t. Either because the lexical resource
candidate was not the correct governor or it was un-
specified. The last case, NA, indicates that the cor-
rect governor does not appear in any of the n-best
parses. Technically this case could be integrated in
EE (an error made by the parser was not corrected
by the lexical resource) but we chose to keep it apart
</bodyText>
<page confidence="0.994824">
781
</page>
<bodyText confidence="0.999571833333333">
since it represents a case where the right solution
could not be found in the n-best parse list (the cor-
rect governor is not a member of set G).
Let’s note nS the number of occurrences of sce-
nario S for a given configuration. We compute CR
for this configuration in the following way:
</bodyText>
<equation confidence="0.973722">
old error number - new error number
CR =
old error number
nEC − nCE
=
nEE + nEC + nNA
</equation>
<bodyText confidence="0.877738333333333">
When CR is equal to 0, the correction did not have
any impact on the error rate. When CR &gt; 0, the error
rate is reduced and if CR &lt; 0 it is increased&apos;.
CR for each configuration is reported in table 8.
The counts of the different scenarios have also been
reported.
</bodyText>
<table confidence="0.999014">
Conf. nCC nCE nEC nEE nNA CR
OBJ 992 30 51 5 17 0.29
SBJ 1131 35 61 16 34 0.23
ADJ 2220 42 16 20 6 -0.62
NdeN 2083 93 42 44 21 -0.48
VdeN 150 2 49 1 13 0.75
NaN 89 5 21 10 2 0.48
VaN 273 19 132 8 11 0.75
NcN 165 17 12 31 12 -0.09
VcN 120 21 14 11 5 -0.23
E 7223 264 398 146 121 0.20
</table>
<tableCaption confidence="0.9883985">
Table 8: Correction Rate of the lexical resource with re-
spect to FTB DEV.
</tableCaption>
<bodyText confidence="0.987353954545454">
Table 8 shows very different results among con-
figurations. Results for PP attachments VdeN, VaN
and NaN are quite good (a CR of 75% for a given
configuration, as VdeN indicates that the number of
errors on such a configuration is decreased by 25%).
It is interesting to note that the parser behaves quite
badly on these attachments: their accuracy (as re-
ported in table 3) is, respectively 74.68, 69.1 and
70.64. Lexical affinity helps in such cases. On
the other hand, some attachments like configuration
ADJ and NdeN, for which the parser showed very
good accuracy (96.6 and 92.2) show very poor per-
formances. In such cases, taking into account lexical
affinity creates new errors.
&apos;One can note, that contrary to coverage, CR does not mea-
sure a characteristic of the lexical resource alone, but the lexical
resource combined with a parser.
On average, using the lexical resource with this
simple strategy of systematically replacing GH with
GL allows to decrease by 20% the errors made on
our 9 configurations and by 2.5% the global error
rate of the parser.
</bodyText>
<subsectionHeader confidence="0.993407">
4.3 Filtering Data with Ambiguity Threshold
</subsectionHeader>
<bodyText confidence="0.999997">
The data used to extract counts is noisy: it con-
tains errors made by the parser. Ideally, we would
like to take into account only non ambiguous sen-
tences, for which the parser outputs a single parse
hypothesis, hopefully the good one. Such an ap-
proach is obviously doomed to fail since almost ev-
ery sentence will be associated to several parses.
Another solution would be to select sentences for
which the parser has a high confidence, using confi-
dence measures as proposed in (S´anchez-S´aez et al.,
2009; Hwa, 2004). But since we are only interested
in some parts of sentences (usually one attachment),
we don’t need high confidence for the whole sen-
tence. We have instead used a parameter, defined on
single dependencies, called the ambiguity measure.
Given the n best parses of a sentence and a depen-
dency S, present in at least one of the n best parses,
let us note C(S) the number of occurrences of S in
the n best parse set. We note AM(S) the ambiguity
measure associated to S. It is computed as follows:
</bodyText>
<equation confidence="0.917963666666667">
C(S)
AM(S) = 1 −
n
</equation>
<bodyText confidence="0.9999895">
An ambiguity measure of 0 indicates that S is non
ambiguous in the set of the n best parses (the word
that constitutes the dependent in S is attached to the
word that constitutes the governor in S in all the n-
best analyses). When n gets large enough this mea-
sure approximates the non ambiguity of a depen-
dency in a given sentence.
Ambiguity measure is used to filter the data when
counting the number of occurrences of a configura-
tion: only occurrences that are made of dependen-
cies S such that AM(S) &lt; z are taken into account.
z is called the ambiguity threshold.
The results of coverage and CR given above were
computed for z equal to 1, which means that, when
collecting counts, all the dependencies are taken into
account whatever their ambiguity is. Table 9 shows
coverage and CR for different values of z. As ex-
pected, coverage decreases with z. But, interest-
</bodyText>
<page confidence="0.99269">
782
</page>
<bodyText confidence="0.99992375">
ingly, decreasing z, from 1 down to 0.2 has a posi-
tive influence on CR. Ambiguity threshold plays the
role we expected: it allows to reduce noise in the
data, and corrects more errors.
</bodyText>
<table confidence="0.998951083333333">
r = 1.0 r = 0.4 r = 0.2 r = 0.0
cov/CR cov/CR cov/CR cov/CR
OBJ 0.70/0.29 0.58/0.36 0.52/0.36 0.35/0.38
SBJ 0.68/0.23 0.64/0.23 0.62/0.23 0.52/0.23
ADJ 0.69/-0.62 0.61/-0.52 0.56/-0.52 0.43/-0.38
NdeN 0.67/-0.48 0.58/-0.53 0.52/-0.52 0.38/-0.41
VdeN 0.57/0.75 0.44/0.73 0.36/0.73 0.20/0.30
NaN 0.50/0.48 0.34/0.42 0.28/0.45 0.15/0.48
VaN 0.65/0.75 0.50/0.8 0.41/0.80 0.26/0.48
NcN 0.25/-0.09 0.19/0 0.16/0.02 0.07/0.13
VcV 0.56/-0.23 0.42/-0.07 0.28/0.03 0.08/0.07
Avg 0.66/0.2 0.57/0.23 0.51/0.24 0.38/0.17
</table>
<tableCaption confidence="0.998566">
Table 9: Coverage and Correction Rate on FTB DEV for
several values of ambiguity threshold.
</tableCaption>
<sectionHeader confidence="0.959414" genericHeader="method">
5 Integrating Lexical Affinity in the Parser
</sectionHeader>
<bodyText confidence="0.999996777777778">
We have devised three methods for taking into ac-
count lexical affinity scores in the parser. The first
two are post-processing methods, that take as input
the n-best parses produced by the parser and mod-
ify some attachments with respect to the information
given by the lexical resource. The third method in-
troduces the lexical affinity scores as new features in
the parsing model. The three methods are described
in 5.1, 5.2 and 5.3. They are evaluated in 5.4.
</bodyText>
<subsectionHeader confidence="0.99683">
5.1 Post Processing Method
</subsectionHeader>
<bodyText confidence="0.999961909090909">
The post processing method is quite simple. It is
very close to the method that was used to compute
the Correction Rate of the lexical resource, in 4.2.2:
it takes as input the n-best parses produced by the
parser and, for every configuration occurrence C
found in the first best parse, the set (G) of all po-
tential governors of C, in the n-best parses, is com-
puted and among them, the word that maximizes the
lexical affinity score (GL) is identified.
Once GL is identified, one can replace the choice
of the parser (GH) with GL. This method is quite
crude since it does not take into account the confi-
dence the parser has in the solution proposed. We
observed, in 4.2.2 that CR was very low for configu-
rations for which the parser achieves good accuracy.
In order to introduce the parser confidence in the fi-
nal choice of a governor, we compute C(GH) and
C(GL) which respectively represent the number of
times GH and GL appear as the governor of config-
uration C. The choice of the final governor, noted
G, depends on the ratio of C(GH) and C(GL). The
complete selection strategy is the following:
</bodyText>
<equation confidence="0.5662975">
1. if GH = GL or GL is unspecified, G� = GH.
configuration.
</equation>
<table confidence="0.9952835">
Conf. OBJ SUJ ADJ NdeN VdeN
CR 0.45 0.46 0.14 0.05 0.73
Conf. NaN VaN NcN VcV E
CR 0.12 0.8 0.12 0.1 0.4
</table>
<tableCaption confidence="0.996323">
Table 10: Correction Rate on FTB DEV when taking into
account parser confidence.
</tableCaption>
<bodyText confidence="0.999962333333333">
We have reported, in table 10 the values of CR,
for the 9 different features, using this strategy, for
z = 1. We do not report the values of CR for other
values of z since they are very close to each other.
The table shows several noticeable facts. First, the
new strategy performs much better than the former
one (crudely replacing GH by GL), the value of CR
increased from 0.2 to 0.4, which means that the er-
rors made on the nine configurations are now de-
creased by 40%. Second, CR is now positive for ev-
ery configuration: the number of errors is decreased
for every
</bodyText>
<subsectionHeader confidence="0.999517">
5.2 Double Parsing Method
</subsectionHeader>
<bodyText confidence="0.999872166666667">
The post processing method performs better than the
naive strategy that was used in 4.2.2. But it has an
important drawback: it creates inconsistent parses.
Recall that the parser we are using is based on a sec-
ond order model, which means that the score of a de-
pendency depends on some neighbori
ng ones. Since
with the post processing method only a subset of the
dependencies are modified, the resulting parse is in-
consistent: the score of some dependencies is com-
puted on the basis of other dependencies that have
been modified.
</bodyText>
<figure confidence="0.939322714285714">
2. if GH =� GL, G� is determined as follows:
�
GH if C(GH)
G� = C(G�) � α
where α is a coefficient that is optimized on the
development data set.
GL otherwise
</figure>
<page confidence="0.994795">
783
</page>
<bodyText confidence="0.999944">
In order to compute a new optimal parse tree
that preserves the modified dependencies, we have
used a technique proposed in (Mirroshandel and
Nasr, 2011) that modifies the scoring function of the
parser in such a way that the dependencies that we
want to keep in the parser output get better scores
than all competing dependencies.
The double parsing method is therefore a three
stage method. First, sentence S is parsed, producing
the n-best parses. Then, the post processing method
is used, modifying the first best parse. Let’s note
D the set of dependencies that were changed in this
process. In the last stage, a new parse is produced,
that preserves D.
</bodyText>
<subsectionHeader confidence="0.984693">
5.3 Feature Based Method
</subsectionHeader>
<bodyText confidence="0.999981692307692">
In the feature based method, new features are
added to the parser that rely on lexical affinity
scores. These features are of the following form:
(C, l9, ld, JC(s)), where C is a configuration num-
ber, s is the lexical affinity score (s = s(C, l9, ld))
and S,(·) is a discretization function.
Discretization of the lexical affinity scores is nec-
essary in order to fight against data sparseness. In
this work, we have used Weka software (Hall et al.,
2009) to discretize the scores with unsupervised bin-
ning. Binning is a simple process which divides
the range of possible values a parameter can take
into subranges called bins. Two methods are im-
plemented in Weka to find the optimal number of
bins: equal-frequency and equal-width. In equal-
frequency binning, the range of possible values are
divided into k bins, each of which holds the same
number of instances. In equal-width binning, which
is the method we have used, the range are divided
into k subranges of the same size. The optimal num-
ber of bins is the one that minimizes the entropy of
the data. Weka computes different number of bins
for different configurations, ranging from 4 to 10.
The number of new features added to the parser is
equal to EC B(C) where C is a configuration and
B(C) is the number of bins for configuration C.
</bodyText>
<subsectionHeader confidence="0.825611">
5.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999971684210526">
The three methods described above have been evalu-
ated on FTB TEST. Results are reported in table 11.
The three methods outperformed the baseline (the
state of the art parser for French which is a second
order graph based method) (Bohnet, 2010). The best
performances were obtained by the Double Parsing
method that achieved a labeled relative error reduc-
tion of 7,1% on predicted POS tags, yielding the
best parsing results on the French Treebank. It per-
forms better than the Post Processing method, which
means that the second parsing stage corrects some
inconsistencies introduced in the Post Processing
method. The performances of the Feature Based
method are disappointing, it achieves an error reduc-
tion of 1.4%. This result is not easy to interpret. It
is probably due to the limited number of new fea-
tures introduced in the parser. These new features
probably have a hard time competing with the large
number of other features in the training process.
</bodyText>
<table confidence="0.550567">
BL
PP
DP
FB
</table>
<tableCaption confidence="0.924333">
Table 11: Parser accuracy on FTB TEST using the
standard parser (BL) the post processing method (PP),
the double parsing method (DP) and the feature based
method.
</tableCaption>
<sectionHeader confidence="0.997693" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999907666666667">
Computing lexical affinities, on large corpora, for
specific lexico-syntactic configurations that are hard
to disambiguate has shown to be an effective way
to increase the performances of a parser. We have
proposed in this paper one method to compute lexi-
cal affinity scores as well as three ways to introduce
this new information in a parser. Experiments on a
French corpus showed a relative decrease of the er-
ror rate of 7.1% Labeled Accuracy Score.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9987965">
This work has been funded by the French Agence
Nationale pour la Recherche, through the projects
SEQUOIA (ANR-08-EMER-013) and EDYLEX
(ANR-08-CORD-009).
</bodyText>
<table confidence="0.9949385">
pred. POS tags gold POS tags
punct no punct punct no punct
LAS 88.02 90.24 88.88 91.12
UAS 90.02 92.50 90.71 93.20
LAS 88.45 90.73 89.46 91.78
UAS 90.61 93.20 91.44 93.86
LAS 88.87 91.10 89.72 91.90
UAS 90.84 93.30 91.58 93.99
LAS 88.19 90.33 89.29 91.43
UAS 90.22 92.62 91.09 93.46
</table>
<page confidence="0.997646">
784
</page>
<sectionHeader confidence="0.995874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913459183673">
A. Abeill´e, L. Cl´ement, and F. Toussenel. 2003. Building
a treebank for french. In Anne Abeill´e, editor, Tree-
banks. Kluwer, Dordrecht.
E.H. Anguiano and M. Candito. 2011. Parse correction
with specialized models for difficult attachment types.
In Proceedings of EMNLP.
M. Bansal and D. Klein. 2011. Web-scale features for
full-scale parsing. In Proceedings of ACL, pages 693–
702.
D. Bikel. 2004. Intricacies of Collins’ parsing model.
Computational Linguistics, 30(4):479–511.
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of ACL, pages 89–97.
M. Candito and B. Crabb´e. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In Proceedings of the 11th International Confer-
ence on Parsing Technologies, pages 138–141.
M. Candito and D. Seddah. 2010. Parsing word clusters.
In Proceedings of the NAACL HLT Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 76–84.
M. Candito, B. Crabb´e, P. Denis, and F. Gu´erin. 2009.
Analyse syntaxique du franc¸ais : des constituants aux
d´ependances. In Proceedings of Traitement Automa-
tique des Langues Naturelles.
W. Chen, J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570–579.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational linguistics, 16(1):22–29.
P. Denis and B. Sagot. 2010. Exploitation d’une
ressource lexicale pour la construction d’un ´etiqueteur
morphosyntaxique ´etat-de-l’art du franc¸ais. In Pro-
ceedings of Traitement Automatique des Langues Na-
turelles.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10–18.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253–276.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
ACL HLT, pages 595–603.
S. K¨ubler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 1(1):1–127.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313–330.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
HLT NAACL, pages 152–159.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT-EMNLP,
pages 523–530.
S.A. Mirroshandel and A. Nasr. 2011. Active learning
for dependency parsing using partially annotated sen-
tences. In Proceedings of International Conference on
Parsing Technologies.
P. Nakov and M. Hearst. 2005. Using the web as an
implicit training set: application to structural ambigu-
ity resolution. In Proceedings of HLT-EMNLP, pages
835–842.
A. Nasr, F. B´echet, J-F. Rey, B. Favre, and Le Roux J.
2011. MACAON: An NLP tool suite for processing
word lattices. In Proceedings of ACL.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886–
894.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In Proceedings of the CoNLL shared task session
of EMNLP-CoNLL, volume 7, pages 1044–1050.
R. S´anchez-S´aez, J.A. S´anchez, and J.M. Benedi. 2009.
Statistical confidence measures for probabilistic pars-
ing. In Proceedings of RANLP, pages 388–392.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of EACL, pages 331–338.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551–560.
M. Volk. 2001. Exploiting the WWW as a corpus to
resolve PP attachment ambiguities. In Proceedings of
Corpus Linguistics.
G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting
web-derived selectional preference to improve statisti-
cal dependency parsing. In Proceedings of HLT-ACL,
pages 1556–1565.
</reference>
<page confidence="0.998502">
785
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.396334">
<title confidence="0.999529">Semi-supervised Dependency Parsing using Lexical Affinities</title>
<author confidence="0.99959">Abolghasem Alexis Le</author>
<affiliation confidence="0.90728425">d’Informatique Fondamentale de Marseille- CNRS - UMR Universit´e Aix-Marseille, Marseille, Universit´e Paris Nord &amp; CNRS,Villetaneuse, Engineering Department, Sharif university of Technology, Tehran, Iran</affiliation>
<email confidence="0.799232">(ghasem.mirroshandel@lif.univ-mrs.fr,leroux@univ-paris13.fr)</email>
<abstract confidence="0.998907538461539">Treebanks are not large enough to reliably model precise lexical phenomena. This deficiency provokes attachment errors in the parsers trained on such data. We propose in this paper to compute lexical affinities, on large corpora, for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser. Experiments on the French Treebank showed relative decrease of the error rate of Labeled Accuracy Score yielding the best parsing results on this treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeill´e</author>
<author>L Cl´ement</author>
<author>F Toussenel</author>
</authors>
<title>Building a treebank for french.</title>
<date>2003</date>
<editor>In Anne Abeill´e, editor, Treebanks.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Abeill´e, Cl´ement, Toussenel, 2003</marker>
<rawString>A. Abeill´e, L. Cl´ement, and F. Toussenel. 2003. Building a treebank for french. In Anne Abeill´e, editor, Treebanks. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Anguiano</author>
<author>M Candito</author>
</authors>
<title>Parse correction with specialized models for difficult attachment types.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5884" citStr="Anguiano and Candito, 2011" startWordPosition="947" endWordPosition="950">e initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected in the parses and introduced as new features in the parser. The way we introduce lexical affinity measures in the parser, in 5.1, shares some ideas with (Anguiano and Candito, 2011), who modify some attachments in the parser output, based on lexical information. The main difference is that we only take attachments that appear in an n-best parse list into account, while they consider the first best parse and compute all potential alternative attachments, that may not actually occur in the n-best forests. 3 The Parser The parser used in this work is the second order graph based parser (McDonald et al., 2005; K¨ubler et al., 2009) implementation of (Bohnet, 2010). The parser was trained on the French Treebank (Abeill´e et al., 2003) which was transformed into dependency tre</context>
</contexts>
<marker>Anguiano, Candito, 2011</marker>
<rawString>E.H. Anguiano and M. Candito. 2011. Parse correction with specialized models for difficult attachment types. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bansal</author>
<author>D Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>693--702</pages>
<contexts>
<context position="3992" citStr="Bansal and Klein, 2011" startWordPosition="620" endWordPosition="623"> et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in a window of a few words. (Bansal and Klein, 2011; Nakov and Hearst, 2005) rely on the same sort of techniques but use more sophisticated patterns, based on simple paraphrase rules, for identifying co-occurrences. Our work departs from those approaches by the fact that we do not extract the lexical information directly on a raw corpus, but we first parse it and then extract the co-occurrences on the parse trees, based on some predetermined lexico-syntactic patterns. The first reason for this choice is that the linguistic phenomena that we are interested in, such as as PP attachment, coordination, verb subject and object can range over long d</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>M. Bansal and D. Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of ACL, pages 693– 702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1425" citStr="Bikel, 2004" startWordPosition="190" endWordPosition="191">n in a parser. Experiments on the French Treebank showed a relative decrease of the error rate of 7.1% Labeled Accuracy Score yielding the best parsing results on this treebank. 1 Introduction Probabilistic parsers are usually trained on treebanks composed of few thousands sentences. While this amount of data seems reasonable for learning syntactic phenomena and, to some extent, very frequent lexical phenomena involving closed parts of speech (POS), it proves inadequate when modeling lexical dependencies between open POS, such as nouns, verbs and adjectives. This fact was first recognized by (Bikel, 2004) who showed that bilexical dependencies were barely used in Michael Collins’ parser. The work reported in this paper aims at a better modeling of such phenomena by using a raw corpus that is several orders of magnitude larger than the treebank used for training the parser. The raw corpus is first parsed and the computed lexical affinities between lemmas, in specific lexico-syntactic configurations, are then injected back in the parser. Two outcomes are expected from this procedure, the first is, as mentioned above, a better modeling of bilexical dependencies and the second is a method to adapt</context>
<context position="16368" citStr="Bikel, 2004" startWordPosition="2832" endWordPosition="2833"> found in the lexical resource and the fourth column shows the coverage for this configuration, which is the ratio third column over the second. Last column represents the coverage of the training corpus (the lexical resource is extracted on the training corpus) and the last line represents the same quantities computed on all configurations. Table 6 shows two interesting results: firstly the high variability of coverage with respect to configurations, and secondly the low coverage when the lexical resource is computed on the training corpus, this fact being consistent with the conclusions of (Bikel, 2004). A parser trained on a treebank cannot be expected to reliably select the correct governor in lexically sensitive cases. Conf. occ. pres. cov. T cov. OBJ 1017 709 0.70 0.21 SBJ 1210 825 0.68 0.24 ADJ 1791 1239 0.69 0.33 NdeN 1909 1287 0.67 0.31 VdeN 189 107 0.57 0.16 NaN 123 61 0.50 0.20 VaN 422 273 0.65 0.23 NcN 220 55 0.25 0.10 VcV 165 93 0.56 0.04 E 7046 4649 0.66 0.27 Table 6: Coverage of the lexical resource over FTB DEV. 4.2.2 Correction Rate While coverage measures how many instantiated configurations that occur in the treebank are actually present in the lexical resource, it does not </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="6371" citStr="Bohnet, 2010" startWordPosition="1030" endWordPosition="1031">ser. The way we introduce lexical affinity measures in the parser, in 5.1, shares some ideas with (Anguiano and Candito, 2011), who modify some attachments in the parser output, based on lexical information. The main difference is that we only take attachments that appear in an n-best parse list into account, while they consider the first best parse and compute all potential alternative attachments, that may not actually occur in the n-best forests. 3 The Parser The parser used in this work is the second order graph based parser (McDonald et al., 2005; K¨ubler et al., 2009) implementation of (Bohnet, 2010). The parser was trained on the French Treebank (Abeill´e et al., 2003) which was transformed into dependency trees by (Candito et al., 2009). The size of the treebank and its decomposition into train, development and test sets is represented in table 1. nb of sentences nb of words FTB TRAIN 9 881 278 083 FTB DEV 1 239 36 508 FTB TEST 1 235 36 340 Table 1: Size and decomposition of the French Treebank The part of speech tagging was performed with the MELT tagger (Denis and Sagot, 2010) and lemmatized with the MACAON tool suite (Nasr et al., 2011). The parser gave state of the art results for p</context>
<context position="29318" citStr="Bohnet, 2010" startWordPosition="5199" endWordPosition="5200">ided into k subranges of the same size. The optimal number of bins is the one that minimizes the entropy of the data. Weka computes different number of bins for different configurations, ranging from 4 to 10. The number of new features added to the parser is equal to EC B(C) where C is a configuration and B(C) is the number of bins for configuration C. 5.4 Evaluation The three methods described above have been evaluated on FTB TEST. Results are reported in table 11. The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). The best performances were obtained by the Double Parsing method that achieved a labeled relative error reduction of 7,1% on predicted POS tags, yielding the best parsing results on the French Treebank. It performs better than the Post Processing method, which means that the second parsing stage corrects some inconsistencies introduced in the Post Processing method. The performances of the Feature Based method are disappointing, it achieves an error reduction of 1.4%. This result is not easy to interpret. It is probably due to the limited number of new features introduced in the parser. Thes</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>B. Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of ACL, pages 89–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabb´e</author>
</authors>
<title>Improving generative statistical parsing with semi-supervised word clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>138--141</pages>
<marker>Candito, Crabb´e, 2009</marker>
<rawString>M. Candito and B. Crabb´e. 2009. Improving generative statistical parsing with semi-supervised word clustering. In Proceedings of the 11th International Conference on Parsing Technologies, pages 138–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>D Seddah</author>
</authors>
<title>Parsing word clusters.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>76--84</pages>
<contexts>
<context position="3206" citStr="Candito and Seddah, 2010" startWordPosition="492" endWordPosition="495">obtained with the three methods. 2 Previous Work Coping with lexical sparsity of treebanks using raw corpora has been an active direction of research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of word</context>
</contexts>
<marker>Candito, Seddah, 2010</marker>
<rawString>M. Candito and D. Seddah. 2010. Parsing word clusters. In Proceedings of the NAACL HLT Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 76–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabb´e</author>
<author>P Denis</author>
<author>F Gu´erin</author>
</authors>
<title>Analyse syntaxique du franc¸ais : des constituants aux d´ependances.</title>
<date>2009</date>
<booktitle>In Proceedings of Traitement Automatique des Langues Naturelles.</booktitle>
<marker>Candito, Crabb´e, Denis, Gu´erin, 2009</marker>
<rawString>M. Candito, B. Crabb´e, P. Denis, and F. Gu´erin. 2009. Analyse syntaxique du franc¸ais : des constituants aux d´ependances. In Proceedings of Traitement Automatique des Langues Naturelles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chen</author>
<author>J Kazama</author>
<author>K Uchimoto</author>
<author>K Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>570--579</pages>
<contexts>
<context position="5569" citStr="Chen et al., 2009" startWordPosition="892" endWordPosition="895">ated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected in the parses and introduced as new features in the parser. The way we introduce lexical affinity measures in the parser, in 5.1, shares some ideas with (Anguiano and Candito, 2011), who modify some attachments in the parser output, based on lexical information. The main difference is that we only take attachments that appear in an n-best parse list into account, while they consider the first best parse and compute all potential alternative attachments, that may</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>W. Chen, J. Kazama, K. Uchimoto, and K. Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In Proceedings of EMNLP, pages 570–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="14549" citStr="Church and Hanks, 1990" startWordPosition="2533" endWordPosition="2536"> should reflect the tendency of lg and ld to appear together in configuration C. It should be maximal if whenever lg occurs as the governor of configuration C, the dependent position is occupied by ld and, symmetrically, if whenever ld occurs as the dependent of configuration C, the governor position is occupied by lg. A function that conforms such a behavior is the following: (C(C, lg, ld) � 1 C(C, lg, ld) s(C, lg, ld) = 2 C(C, lg, *) + C(C, *, ld) it takes its values between 0 (lg and ld never co-occur) and 1 (g and d always co-occur). This function is close to pointwise mutual information (Church and Hanks, 1990) but takes its values between 0 and 1. 4.2 Evaluation Lexical affinities were computed on three corpora of slightly different genres. The first one, is a collection of news report of the French press agency Agence France Presse, the second is a collection of newspaper articles from a local French newspaper : l’Est Republicain. The third one is a collection of articles from the French Wikipedia. The size of the different corpora are detailed in table 5. The corpus was first POS tagged, lemmatized and parsed in order to get the 50 best parses for every sentence. Then the lexical resource was bui</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>B Sagot</author>
</authors>
<title>Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais.</title>
<date>2010</date>
<booktitle>In Proceedings of Traitement Automatique des Langues Naturelles.</booktitle>
<contexts>
<context position="6861" citStr="Denis and Sagot, 2010" startWordPosition="1119" endWordPosition="1122">d in this work is the second order graph based parser (McDonald et al., 2005; K¨ubler et al., 2009) implementation of (Bohnet, 2010). The parser was trained on the French Treebank (Abeill´e et al., 2003) which was transformed into dependency trees by (Candito et al., 2009). The size of the treebank and its decomposition into train, development and test sets is represented in table 1. nb of sentences nb of words FTB TRAIN 9 881 278 083 FTB DEV 1 239 36 508 FTB TEST 1 235 36 340 Table 1: Size and decomposition of the French Treebank The part of speech tagging was performed with the MELT tagger (Denis and Sagot, 2010) and lemmatized with the MACAON tool suite (Nasr et al., 2011). The parser gave state of the art results for parsing of French, reported in table 2. pred. POS tags gold POS tags punct no punct punct no punct LAS 88.02 90.24 88.88 91.12 UAS 90.02 92.50 90.71 93.20 Table 2: Labeled and unlabeled accuracy score for automatically predicted and gold POS tags with and without taking into account punctuation on FTB TEST. Figure 1 shows the distribution of the 100 most common error types made by the parser. In this figure, x axis shows the error types and y axis shows the error ratio of the related er</context>
</contexts>
<marker>Denis, Sagot, 2010</marker>
<rawString>P. Denis and B. Sagot. 2010. Exploitation d’une ressource lexicale pour la construction d’un ´etiqueteur morphosyntaxique ´etat-de-l’art du franc¸ais. In Proceedings of Traitement Automatique des Langues Naturelles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="28221" citStr="Hall et al., 2009" startWordPosition="5005" endWordPosition="5008">rse. Let’s note D the set of dependencies that were changed in this process. In the last stage, a new parse is produced, that preserves D. 5.3 Feature Based Method In the feature based method, new features are added to the parser that rely on lexical affinity scores. These features are of the following form: (C, l9, ld, JC(s)), where C is a configuration number, s is the lexical affinity score (s = s(C, l9, ld)) and S,(·) is a discretization function. Discretization of the lexical affinity scores is necessary in order to fight against data sparseness. In this work, we have used Weka software (Hall et al., 2009) to discretize the scores with unsupervised binning. Binning is a simple process which divides the range of possible values a parameter can take into subranges called bins. Two methods are implemented in Weka to find the optimal number of bins: equal-frequency and equal-width. In equalfrequency binning, the range of possible values are divided into k bins, each of which holds the same number of instances. In equal-width binning, which is the method we have used, the range are divided into k subranges of the same size. The optimal number of bins is the one that minimizes the entropy of the data</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="21747" citStr="Hwa, 2004" startWordPosition="3860" endWordPosition="3861">and by 2.5% the global error rate of the parser. 4.3 Filtering Data with Ambiguity Threshold The data used to extract counts is noisy: it contains errors made by the parser. Ideally, we would like to take into account only non ambiguous sentences, for which the parser outputs a single parse hypothesis, hopefully the good one. Such an approach is obviously doomed to fail since almost every sentence will be associated to several parses. Another solution would be to select sentences for which the parser has a high confidence, using confidence measures as proposed in (S´anchez-S´aez et al., 2009; Hwa, 2004). But since we are only interested in some parts of sentences (usually one attachment), we don’t need high confidence for the whole sentence. We have instead used a parameter, defined on single dependencies, called the ambiguity measure. Given the n best parses of a sentence and a dependency S, present in at least one of the n best parses, let us note C(S) the number of occurrences of S in the n best parse set. We note AM(S) the ambiguity measure associated to S. It is computed as follows: C(S) AM(S) = 1 − n An ambiguity measure of 0 indicates that S is non ambiguous in the set of the n best p</context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>R. Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30(3):253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL HLT,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="3094" citStr="Koo et al., 2008" startWordPosition="473" endWordPosition="476">Section 5 describes three ways to integrate the lexical affinities in the parser and reports the results obtained with the three methods. 2 Previous Work Coping with lexical sparsity of treebanks using raw corpora has been an active direction of research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or inte</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In Proceedings of the ACL HLT, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<date>2009</date>
<booktitle>Dependency parsing. Synthesis Lectures on Human Language Technologies,</booktitle>
<volume>1</volume>
<issue>1</issue>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>S. K¨ubler, R. McDonald, and J. Nivre. 2009. Dependency parsing. Synthesis Lectures on Human Language Technologies, 1(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="3170" citStr="Marcus et al., 1993" startWordPosition="485" endWordPosition="488"> parser and reports the results obtained with the three methods. 2 Previous Work Coping with lexical sparsity of treebanks using raw corpora has been an active direction of research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are comp</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT NAACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="5058" citStr="McClosky et al., 2006" startWordPosition="798" endWordPosition="801">is choice is that the linguistic phenomena that we are interested in, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Effective self-training for parsing. In Proceedings of HLT NAACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT-EMNLP, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Mirroshandel</author>
<author>A Nasr</author>
</authors>
<title>Active learning for dependency parsing using partially annotated sentences.</title>
<date>2011</date>
<booktitle>In Proceedings of International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="27236" citStr="Mirroshandel and Nasr, 2011" startWordPosition="4833" endWordPosition="4836"> model, which means that the score of a dependency depends on some neighbori ng ones. Since with the post processing method only a subset of the dependencies are modified, the resulting parse is inconsistent: the score of some dependencies is computed on the basis of other dependencies that have been modified. 2. if GH =� GL, G� is determined as follows: � GH if C(GH) G� = C(G�) � α where α is a coefficient that is optimized on the development data set. GL otherwise 783 In order to compute a new optimal parse tree that preserves the modified dependencies, we have used a technique proposed in (Mirroshandel and Nasr, 2011) that modifies the scoring function of the parser in such a way that the dependencies that we want to keep in the parser output get better scores than all competing dependencies. The double parsing method is therefore a three stage method. First, sentence S is parsed, producing the n-best parses. Then, the post processing method is used, modifying the first best parse. Let’s note D the set of dependencies that were changed in this process. In the last stage, a new parse is produced, that preserves D. 5.3 Feature Based Method In the feature based method, new features are added to the parser tha</context>
</contexts>
<marker>Mirroshandel, Nasr, 2011</marker>
<rawString>S.A. Mirroshandel and A. Nasr. 2011. Active learning for dependency parsing using partially annotated sentences. In Proceedings of International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>M Hearst</author>
</authors>
<title>Using the web as an implicit training set: application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>835--842</pages>
<contexts>
<context position="3343" citStr="Nakov and Hearst, 2005" startWordPosition="514" endWordPosition="517">f research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>P. Nakov and M. Hearst. 2005. Using the web as an implicit training set: application to structural ambiguity resolution. In Proceedings of HLT-EMNLP, pages 835–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nasr</author>
<author>F B´echet</author>
<author>J-F Rey</author>
<author>B Favre</author>
<author>J Le Roux</author>
</authors>
<title>MACAON: An NLP tool suite for processing word lattices.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Nasr, B´echet, Rey, Favre, Le Roux, 2011</marker>
<rawString>A. Nasr, F. B´echet, J-F. Rey, B. Favre, and Le Roux J. 2011. MACAON: An NLP tool suite for processing word lattices. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>S Bergsma</author>
<author>D Lin</author>
<author>K Church</author>
</authors>
<title>Using web-scale N-grams to improve base NP parsing performance.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>886--894</pages>
<contexts>
<context position="3364" citStr="Pitler et al., 2010" startWordPosition="518" endWordPosition="521">s. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in a window of a few wo</context>
</contexts>
<marker>Pitler, Bergsma, Lin, Church, 2010</marker>
<rawString>E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Using web-scale N-grams to improve base NP parsing performance. In Proceedings of COLING, pages 886– 894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL shared task session of EMNLP-CoNLL,</booktitle>
<volume>7</volume>
<pages>1044--1050</pages>
<contexts>
<context position="5127" citStr="Sagae and Tsujii, 2007" startWordPosition="810" endWordPosition="813">n, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected in the parses and introd</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In Proceedings of the CoNLL shared task session of EMNLP-CoNLL, volume 7, pages 1044–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S´anchez-S´aez</author>
<author>J A S´anchez</author>
<author>J M Benedi</author>
</authors>
<title>Statistical confidence measures for probabilistic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>388--392</pages>
<marker>S´anchez-S´aez, S´anchez, Benedi, 2009</marker>
<rawString>R. S´anchez-S´aez, J.A. S´anchez, and J.M. Benedi. 2009. Statistical confidence measures for probabilistic parsing. In Proceedings of RANLP, pages 388–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>S Clark</author>
<author>R Hwa</author>
<author>J Hockenmaier</author>
<author>P Ruhlen</author>
<author>S Baker</author>
<author>J Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>331--338</pages>
<contexts>
<context position="5102" citStr="Steedman et al., 2003" startWordPosition="806" endWordPosition="809">hat we are interested in, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interesting subtrees are detected</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proceedings of EACL, pages 331–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="5079" citStr="Suzuki et al., 2009" startWordPosition="802" endWordPosition="805">inguistic phenomena that we are interested in, such as as PP attachment, coordination, verb subject and object can range over long distances, beyond what is generally taken into account when working on limited windows. The second reason for this choice was to show that the performances that the NLP community has reached on parsing, combined with the use of confidence measures allow to use parsers to extract accurate lexico-syntactic information, beyond what can be found in limited annotated corpora. Our work can also be compared with self training approaches to parsing (McClosky et al., 2006; Suzuki et al., 2009; Steedman et al., 2003; Sagae and Tsujii, 2007) where a parser is first trained on a treebank and then used to parse a large raw corpus. The parses produced are then added to the initial treebank and a new parser is trained. The main difference between these approaches and ours is that we do not directly add the output of the parser to the training corpus, but extract precise lexical information that is then re-injected in the parser. In the self training approach, (Chen et al., 2009) is quite close to our work: instead of adding new parses to the treebank, the occurrence of simple interestin</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proceedings of EMNLP, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Volk</author>
</authors>
<title>Exploiting the WWW as a corpus to resolve PP attachment ambiguities.</title>
<date>2001</date>
<booktitle>In Proceedings of Corpus Linguistics.</booktitle>
<contexts>
<context position="3319" citStr="Volk, 2001" startWordPosition="512" endWordPosition="513"> direction of research for many years. One simple and effective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string </context>
</contexts>
<marker>Volk, 2001</marker>
<rawString>M. Volk. 2001. Exploiting the WWW as a corpus to resolve PP attachment ambiguities. In Proceedings of Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Zhao</author>
<author>K Liu</author>
<author>L Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of HLT-ACL,</booktitle>
<pages>1556--1565</pages>
<contexts>
<context position="3384" citStr="Zhou et al., 2011" startWordPosition="522" endWordPosition="525">ective way to tackle this problem is to put together words that share, in a large raw corpus, similar linear contexts, into word clusters. The word occurrences of the training treebank are then replaced by their cluster identifier and a new parser is trained on the transformed treebank. Using such techniques (Koo et al., 2008) report significative improvement on the Penn Treebank (Marcus et al., 1993) and so do (Candito and Seddah, 2010; Candito and Crabb´e, 2009) on the French Treebank (Abeill´e et al., 2003). Another series of papers (Volk, 2001; Nakov and Hearst, 2005; Pitler et al., 2010; Zhou et al., 2011) directly model word co-occurrences. Cooccurrences of pairs of words are first collected in a 777 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 777–785, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics raw corpus or internet n-grams. Based on the counts produced, lexical affinity scores are computed. The detection of pairs of words co-occurrences is generally very simple, it is either based on the direct adjacency of the words in the string or their cooccurrence in a window of a few words. (Bansal and Kle</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>G. Zhou, J. Zhao, K. Liu, and L. Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings of HLT-ACL, pages 1556–1565.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>