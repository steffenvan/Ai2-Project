<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022891">
<title confidence="0.917768">
NoWaC: a large web-based corpus for Norwegian
</title>
<author confidence="0.986748">
Emiliano Guevara
</author>
<affiliation confidence="0.990365">
Tekstlab,
Institute for Linguistics and Nordic Studies,
University of Oslo
</affiliation>
<email confidence="0.991954">
e.r.guevara@iln.uio.no
</email>
<sectionHeader confidence="0.99864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996043952380953">
In this paper we introduce the first version
of noWaC, a large web-based corpus of Bok-
mål Norwegian currently containing about
700 million tokens. The corpus has been
built by crawling, downloading and process-
ing web documents in the .no top-level in-
ternet domain. The procedure used to col-
lect the noWaC corpus is largely based on
the techniques described by Ferraresi et al.
(2008). In brief, first a set of “seed” URLs
containing documents in the target language
is collected by sending queries to commer-
cial search engines (Google and Yahoo). The
obtained seeds (overall 6900 URLs) are then
used to start a crawling job using the Heritrix
web-crawler limited to the .no domain. The
downloaded documents are then processed in
various ways in order to build a linguistic cor-
pus (e.g. filtering by document size, language
identification, duplicate and near duplicate de-
tection, etc.).
</bodyText>
<sectionHeader confidence="0.995188" genericHeader="categories and subject descriptors">
1 Introduction and motivations
</sectionHeader>
<bodyText confidence="0.999923090909091">
The development, training and testing of NLP tools
requires suitable electronic sources of linguistic data
(corpora, lexica, treebanks, ontological databases,
etc.), which demand a great deal of work in or-
der to be built and are, very often copyright pro-
tected. Furthermore, the ever growing importance of
heavily data-intensive NLP techniques for strategic
tasks such as machine translation and information
retrieval, has created the additional requirement that
these electronic resources be very large and general
in scope.
</bodyText>
<page confidence="0.845784">
1
</page>
<bodyText confidence="0.999965566666667">
Since most of the current work in NLP is carried
out with data from the economically most impact-
ing languages (and especially with English data),
an amazing wealth of tools and resources is avail-
able for them. However, researchers interested in
“smaller” languages (whether by the number of
speakers or by their market relevance in the NLP
industry) must struggle to transfer and adapt the
available technologies because the suitable sources
of data are lacking. Using the web as corpus is a
promising option for the latter case, since it can pro-
vide with reasonably large and reliable amounts of
data in a relatively short time and with a very low
production cost.
In this paper we present the first version of
noWaC, a large web-based corpus of Bokmål Nor-
wegian, a language with a limited web presence,
built by crawling the .no internet top level domain.
The computational procedure used to collect the
noWaC corpus is by and large based on the tech-
niques described by Ferraresi et al. (2008). Our
initiative was originally aimed at collecting a 1.5–2
billion word general-purpose corpus comparable to
the corpora made available by the WaCky initiative
(http://wacky.sslmit.unibo.it). How-
ever, carrying out this project on a language with a
relatively small online presence such as Bokmål has
lead to results which differ from previously reported
similar projects. In its current, first version, noWaC
contains about 700 million tokens.
</bodyText>
<note confidence="0.40715">
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 1–7,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.9102275">
1.1 Norwegian: linguistic situation and
available corpora
</subsectionHeader>
<bodyText confidence="0.999914342105263">
Norway is a country with a population of ca. 4.8 mil-
lion inhabitants that has two official national written
standards: Bokmål and Nynorsk (respectively, ‘book
language’ and ‘new Norwegian’). Of the two stan-
dards, Bokmål is the most widely used, being ac-
tively written by about 85% of the country’s popu-
lation (cf. http://www.sprakrad.no/ for de-
tailed up to date statistics). The two written stan-
dards are extremely similar, especially from the
point of view of their orthography. In addition, Nor-
way recognizes a number of regional minority lan-
guages (the largest of which, North Sami, has ca.
15,000 speakers).
While the written language is generally standard-
ized, the spoken language in Norway is not, and
using one’s dialect in any occasion is tolerated and
even encouraged. This tolerance is rapidly extend-
ing to informal writing, especially in modern means
of communication and media such as internet fo-
rums, social networks, etc.
There is a fairly large number of corpora of the
Norwegian language, both spoken and written (in
both standards). However, most of them are of a
limited size (under 50 million words, cf. http://
www.hf.uio.no/tekstlab/ for an overview).
To our knowledge, the largest existing written cor-
pus of Norwegian is the Norsk Aviskorpus (Hofland
2000, cf. http://avis.uib.no/), an expand-
ing newspaper-based corpus currently containing
700 million words. However, the Norsk Aviskor-
pus is only available though a dedicated web inter-
face for non commercial use, and advanced research
tasks cannot be freely carried out on its contents.
Even though we have only worked on building a
web corpus for Bokmål Norwegian, we intend to
apply the same procedures to create web-corpora
also for Nynorsk and North Sami, thus covering the
whole spectrum of written languages in Norway.
</bodyText>
<subsectionHeader confidence="0.998841">
1.2 Obtaining legal clearance
</subsectionHeader>
<bodyText confidence="0.993099166666667">
The legal status of openly accessible web-
documents is not clear. In practice, when one
visits a web page with a browsing program, an
electronic exact copy of the remote document is
created locally; this logically implies that any online
document must be, at least to a certain extent,
copyright-free if it is to be visited/viewed at all.
This is a major difference with respect to other types
of documents (e.g. printed materials, films, music
records) which cannot be copied at all.
However, when building a web corpus, we do not
only wish to visit (i.e. download) web documents,
but we would like to process them in various ways,
index them and, finally, make them available to other
researchers and users in general. All of this would
ideally require clearance from the copyright holders
of each single document in the corpus, something
which is simply impossible to realize for corpora
that contain millions of different documents.1
In short, web corpora are, from the legal point of
view, still a very dark spot in the field of computa-
tional linguistics. In most countries, there is simply
no legal background to refer to, and the internet is a
sort of no-man’s land.
Norway is a special case: while the law explicitly
protects online content as intellectual property,
there is rather new piece of legislation in Forskrift
til åndsverkloven av 21.12 2001 nr. 1563, § 1-4
that allows universities and other research insti-
tutions to ask for permission from the Ministry
of Culture and Church in order to use copyright
protected documents for research purposes that
do not cause conflict with the right holders’
own use or their economic interests (cf. http:
//www.lovdata.no/cgi-wift/ldles?
ltdoc=/for/ff-20011221-1563.html).
We have been officially granted this permission for
this project, and we can proudly say that noWaC
is a totally legal and recognized initiative. The
results of this work will be legally made available
free of charge for research (i.e. non commercial)
purposes. NoWaC will be distributed in association
with the WaCky initiative and also directly from the
University of Oslo.
1Search engines are in a clear contradiction to the copyright
policies in most countries: they crawl, download and index bil-
lions of documents with no clearance whatsoever, and also re-
distribute whole copies of the cached documents.
</bodyText>
<page confidence="0.966796">
2
</page>
<bodyText confidence="0.8673545">
2 Building a corpus of Bolimål by
web-crawling
</bodyText>
<subsectionHeader confidence="0.990125">
2.1 Methods and tools
</subsectionHeader>
<bodyText confidence="0.999987733333333">
In this project we decided to follow the methods
used to build the WaCky corpora, and to use the re-
lated tools as much as possible (e.g. the BootCaT
tools). In particular, we tried to reproduce the pro-
cedures described by Ferraresi et al. (2008) and Ba-
roni et al. (2009). The methodology has already
produced web-corpora ranging from 1.7 to 2.6 bil-
lion tokens (German, Italian, British English). How-
ever, most of the steps needed some adaptation, fine-
tuning and some extra programming. In particu-
lar, given the relatively complex linguistic situation
in Norway, a step dedicated to document language
identification was added.
In short, the building and processing chain used
for noWaC comprises the following steps:
</bodyText>
<listItem confidence="0.984520666666667">
1. Extraction of list of mid-frequency Bokmål
words from Wikipedia and building query
strings
2. Retrieval of seed URLs from search engines by
sending automated queries, limited to the .no
top-level domain
3. Crawling the web using the seed URLS, limited
to the .no top-level domain
4. Removing HTML boilerplate and filtering doc-
uments by size
5. Removing duplicate and near-duplicate docu-
ments
6. Language identification and filtering
7. Tokenisation
8. POS-tagging
</listItem>
<bodyText confidence="0.985152666666667">
At the time of writing, the first version of noWaC is
being POS-tagged and will be made available in the
course of the next weeks.
</bodyText>
<subsectionHeader confidence="0.999503">
2.2 Retrieving seed URLs from search engines
</subsectionHeader>
<bodyText confidence="0.9999672">
We started by obtaining the Wikipedia text dumps
for Bokmål Norwegian and related languages
(Nynorsk, Danish, Swedish and Icelandic) and se-
lecting the 2000 most frequent words that are unique
to Bokmål. We then sent queries of 2 randomly
selected Bokmål words though search engine APIs
(Google and Yahoo!). A maximum of ten seed
URLs were saved for each query, and the retrieved
URLs were collapsed in a single list of root URLs,
deduplicated and filtered, only keeping those in the
.no top level domain.
After one week of automated queries (limited
to 1000 queries per day per search engine by the
respective APIs) we had about 6900 filtered seed
URLs.
</bodyText>
<subsectionHeader confidence="0.997181">
2.3 Crawling
</subsectionHeader>
<bodyText confidence="0.999988642857143">
We used the Heritrix open-source, web-scale
crawler (http://crawler.archive.org/)
seeded with the 6900 URLs we obtained to tra-
verse the internet .no domain and to download
only HTML documents (all other document types
were discarded from the archive). We instructed
the crawler to use a multi-threaded breadth-first
strategy, and to follow a very strict politeness policy,
respecting all robots.txt exclusion directives
while downloading pages at a moderate rate (90
second pause before retrying any URL) in order not
to disrupt normal website activity.
The final crawling job was stopped after 15 days.
In this period of time, a total size of 1 terabyte
was crawled, with approximately 90 million URLs
being processed by the crawler. Circa 17 million
HTML documents were downloaded, adding up to
an overall archive size of 550 gigabytes. Only
about 13.5 million documents were successfully re-
trieved pages (the rest consisting of various “page
not found” replies and other server-side error mes-
sages).
The documents in the archive were filtered by
size, keeping only those documents that were be-
tween 5Kb and 200Kb in size (following Ferraresi
et al. 2008 and Baroni et al. 2009). This resulted
in a reduced archive of 11.4 million documents for
post-processing.
</bodyText>
<subsectionHeader confidence="0.77578">
2.4 Post-processing: removing HTML
boilerplate and de-duplication
</subsectionHeader>
<bodyText confidence="0.99987725">
At this point of the process, the archive contained
raw HTML documents, still very far from being a
linguistic corpus. We used the BootCaT toolkit (Ba-
roni and Bernardini 2004, cf. http://sslmit.
unibo.it/~baroni/bootcat.html) to per-
form the major operations to clean our archive.
First, every document was processed with the
HTML boilerplate removal tool in order to select
</bodyText>
<page confidence="0.993545">
3
</page>
<bodyText confidence="0.999873357142857">
only the linguistically interesting portions of text
while removing all HTML, Javascript and CSS code
and non-linguistic material (made mainly of HTML
tags, visual formatting, tables, navigation links, etc.)
Then, the archive was processed with the dupli-
cate and near-duplicate detecting script in the the
BootCaT toolkit, based on a 5-gram model. This is
a very drastic strategy leading to a huge reduction in
the number of kept documents: any two documents
sharing more than 1/25 5-grams were considered du-
plicates, and both documents were discarded. The
overall number of documents in the archive went
down from 11.40 to 1.17 million after duplicate re-
moval.2
</bodyText>
<subsectionHeader confidence="0.992683">
2.5 Language identification and filtering
</subsectionHeader>
<bodyText confidence="0.976918121212121">
The complex linguistic situation in Norway makes
us expect that the Norwegian internet be at least a
bilingual domain (Bokmål and Nynorsk). In addi-
tion, we also expect a number of other languages to
be present to a lesser degree.
We used Damir Cavar’s tri-gram algorithm
for language identification (cf. http://ling.
unizd.hr/~dcavar/LID/), training 16 lan-
guage models on Wikipedia text from languages that
are closely related to, or that have contact with Bok-
mål (Bokmål, Danish, Dutch, English, Faeroese,
Finnish, French, German, Icelandic, Italian, North-
ern Sami, Nynorsk, Polish, Russian, Spanish and
Swedish). The best models were trained on 1Mb
of random Wikipedia lines and evaluated against a
database of one hundred 5 Kb article excerpts for
each language. The models performed very well,
often approaching 100% accuracy; however, the ex-
tremely similar orthography of Bokmål and Nynorsk
make them the most difficult pair of languages to
spot for the system, one being often misclassified as
the other. In any case, our results were relatively
good: Bokmål Precision = 1.00, Recall = 0.89, F-
measure = 0.94, Nynorsk Precision = 0.90, Recall =
1.00, F-measure = 0.95.
The language identifying filter was applied on a
document basis, recognizing about 3 out of 4 docu-
2As pointed out by an anonymous reviewer, this drastic re-
duction in number of documents may be due to faults in the
boilerplate removal phase, leading to 5-grams of HTML or sim-
ilar code counting as real text. We are aware of this issue, and
the future versions of noWaC will be revised to this effect.
ments as Bokmål:
</bodyText>
<listItem confidence="0.9994398">
• 72.25% Bokmål
• 16.00% Nynorsk
• 05.80% English
• 02.43% Danish
• 01.95% Swedish
</listItem>
<bodyText confidence="0.934793666666667">
This filter produced another sensible drop in the
overall number of kept documents: from 1.17 to 0.85
million.
</bodyText>
<subsectionHeader confidence="0.996435">
2.6 POS-tagging and lemmatization
</subsectionHeader>
<bodyText confidence="0.99997347368421">
At the time of writing noWaC is in the process of be-
ing POS-tagged. This is not at all an easy task, since
the best and most widely used tagger for Norwe-
gian (the Oslo-Bergen tagger, cf. Hagen et al. 2000)
is available as a binary distribution which, besides
not being open to modifications, is fairly slow and
does not handle large text files. A number of statisti-
cal taggers have been trained, but we are still unde-
cided about which system to use because the avail-
able training materials for Bokmål are rather lim-
ited (about 120,000 words). The tagging accuracy
we have obtained so far is still not comparable to
the state-of-the-art (94.32% with TnT, 94.40% with
SVMT). In addition, we are also working on creat-
ing a large list of tagged lemmas to be used with
noWaC. We estimate that a final POS-tagged and
lemmatized version of the corpus will be available
in the next few weeks (in any case, before the WAC6
workshop).
</bodyText>
<sectionHeader confidence="0.963634" genericHeader="general terms">
3 Comparing results
</sectionHeader>
<bodyText confidence="0.9998076">
While it is still too early for us to carry out a
fully fledged qualitative evaluation of noWaC, we
are able to compare our results with previous pub-
lished work, especially with the WaCky corpora we
tried to emulate.
</bodyText>
<subsectionHeader confidence="0.937393">
3.1 NoWaC and the WaCky corpora
</subsectionHeader>
<bodyText confidence="0.999928571428571">
As we stated above, we tried to follow the WaCky
methodology as closely as possible, in the hopes that
we could obtain a very large corpus (we aimed at
collecting above 1 billion tokens). However, even
though our crawling job produced a much bigger
initial archive than those reported for German, Ital-
ian and British English in Baroni et al. (2009), and
</bodyText>
<page confidence="0.995648">
4
</page>
<bodyText confidence="0.999993472222222">
even though after document size filtering was ap-
plied our archive contained roughly twice as many
documents, our final figures (number of tokens and
number of documents) only amount to about half the
size reported for the WaCky corpora (cf. table 1).
In particular, we observe that the most significant
drop in size and in number of documents took place
during the detection of duplicate and near-duplicate
documents (drastically dropping from 11.4 million
documents to 1.17 million documents after duplicate
filtering). This indicates that, even if a huge num-
ber of documents in Bokmål Norwegian are present
in the internet, a large portion of them must be
machine generated content containing repeated n-
grams that the duplicate removal tool successfully
identifies and discards.3
These figures, although unexpected by us, may
actually have a reasonable explanation. If we con-
sider that Bokmål Norwegian has about 4.8 million
potential content authors (assuming that every Nor-
wegian inhabitant is able to produce web documents
in Bokmål), and given that our final corpus contains
0.85 million documents, this means that we have so
far sampled roughly one document every five poten-
tial writers: as good as it may sound, it is a highly
unrealistic projection, and a great deal of noise and
possibly also machine generated content must still
be present in the corpus. The duplicate removal tools
are only helping us understand that a speaker com-
munity can only produce a limited amount of lin-
guistically relevant online content. We leave the in-
teresting task of estimating the size of this content
and its growth rate for further research. The Norwe-
gian case, being a relatively small but highly devel-
oped information society, might prove to be a good
starting point.
</bodyText>
<subsectionHeader confidence="0.916286">
3.2 Scaling noWaC: how much Bokmål is
there? How much did we get?
</subsectionHeader>
<bodyText confidence="0.9965080625">
The question arises immediately. We want to know
how representative our corpus is, in spite of the fact
that we now know that it must still contain a great
deal of noise and that a great deal of documents were
plausibly not produced by human speakers.
To this effect, we applied the scaling factors
3Although we are aware that the process of duplicate re-
moval in noWaC must be refined further, constituting in itself
an interesting research area.
methodology used by Kilgarrif (2007) to estimate
the size of the Italian and German internet on the
basis of the WaCky corpora. The method consists
in comparing document hits for a sample of mid-
frequency words in Google and in our corpus before
and after duplicate removal. The method assumes
that Google does indeed apply duplicate removal to
some extent, though less drastically than we have.
Cf. table 2 for some example figures.
From this document hit comparison, two scaling
factors are extracted. The scaling ratio tells us how
much smaller our corpus is compared to the Google
index for Norwegian (including duplicates and non-
running-text). The duplicate ratio gives us an idea
of how much duplicated material was found in our
archive.
Since we do not know exactly how much dupli-
cate detection Google performs, we will multiply
the duplicate ratio by a weight of 0.1, 0.25 and 0.5
(these weights, in turn, assume that Google discards
10 times less, 4 times less and half what our dupli-
cate removal has done – the latter hypothesis is used
by Kilgarriff 2007).
</bodyText>
<listItem confidence="0.967393333333333">
• Scaling ratio (average):
Google frq. / noWaC raw frq. = 24.9
• Duplicate ratio (average):
</listItem>
<bodyText confidence="0.982287263157895">
noWaC raw frq. / dedup. frq. = 7.8
We can then multiply the number of tokens in our
final cleaned corpus by the scaling ratio and by the
duplicate ratio (weighted) in order to obtain a rough
estimate of how much Norwegian text is contained
in the Google index. We can also estimate how much
of this amount is present in noWaC. Cf. table 3.
Using exactly the same procedure as Kilgarrif
(2007) leads us to conclude that noWaC should
contain over 15% of the Bokmål text indexed by
Google. A much more restrictive estimate gives us
about 3%. More precise estimates are extremely
difficult to make, and these results should be taken
only as rough approximations. In any case, noWaC
certainly is a reasonably representative web-corpus
containing between 3% and 15% of all the currently
indexed online Bokmål (Kilgarriff reports an esti-
mate of 3% for German and 7% for Italian in the
WaCky corpora).
</bodyText>
<page confidence="0.978805">
5
</page>
<table confidence="0.999877727272727">
deWaC itWaC ukWaC noWaC
N. of seed pairs 1,653 1,000 2,000 1,000
N. of seed URLs 8,626 5,231 6,528 6,891
Raw crawl size 398GB 379GB 351GB 550GB
Size after document size filter 20GB 19GB 19GB 22GB
N. of docs after document size filter 4.86M 4.43M 5.69M 11.4M
Size after near-duplicate filter 13GB 10GB 12GB 5GB
N. of docs after near-duplicate filter 1.75M 1.87M 2.69M 1.17M
N. of docs after lang-ID – – – 0.85M
N. of tokens 1.27 Bn 1.58 Bn 1.91 Bn 0.69 Bn
N. of types 9.3M 3.6M 3.8M 6.0M
</table>
<tableCaption confidence="0.985855">
Table 1: Figure comparison of noWaC and the published WaCky corpora (German, Italian and British English data
from Baroni et al. 2009)
</tableCaption>
<table confidence="0.99728075">
Word Google frq. noWaC raw frq. noWaC dedup. frq.
bilavgifter 33700 1637 314
mekanikk 82900 3266 661
musikkpris 16700 570 171
</table>
<tableCaption confidence="0.969499">
Table 2: Sample of Google and noWaC document frequencies before and after duplicate removal.
</tableCaption>
<table confidence="0.99920775">
noWaC Scaling ratio Dup. ratio (weight) Google estimate % in noWaC
0.78 (0.10) 21.8 bn 3.15%
0.69 bn 24.9 1.97 (0.25) 8.7 bn 7.89%
3.94 (0.50) 4.3 bn 15.79%
</table>
<tableCaption confidence="0.9901155">
Table 3: Estimating the size of the Bokmål Norwegian internet as indexed by Google in three different settings (method
from Kilgarriff 2007)
</tableCaption>
<sectionHeader confidence="0.989441" genericHeader="conclusions">
4 Concluding remarks
</sectionHeader>
<bodyText confidence="0.9999098">
Building large web-corpora for languages with a
relatively small internet presence and with a lim-
ited speaker population presents problems and chal-
lenges that have not been found in previous work.
In particular, the amount of data that can be col-
lected with similar efforts is considerably smaller. In
our experience, following as closely as possible the
WaCky corpora methodology yielded a corpus that
is roughly between one half and one third the size of
the published comparable Italian, German and En-
glish corpora.
In any case, the experience has been very success-
ful so far, and the first version of the noWaC cor-
pus is about the same size than the largest currently
available corpus of Norwegian (i.e. Norske Avisko-
rpus, 700 million tokens), and it has been created in
just a minimal fraction of the time it took to build it.
Furthermore, the scaling experiments showed that
noWaC is a very representative web-corpus contain-
ing a significant portion of all the online content in
Bokmål Norwegian, in spite of our extremely drastic
cleaning and filtering strategies.
There is clearly a great margin for improvement
in almost every processing step we applied in this
work. And there is clearly a lot to be done in or-
der to qualitatively assess the created corpus. In the
future, we intend to pursue this activity by carrying
out an even greater crawling job in order to obtain a
larger corpus, possibly containing over 1 billion to-
kens. Moreover, we shall reproduce this corpus cre-
ation process with the remaining two largest written
languages of Norway, Nynorsk and North Sami. All
of these resources will soon be publicly and freely
available both for the general public and for the re-
search community.
</bodyText>
<page confidence="0.998837">
6
</page>
<sectionHeader confidence="0.999097" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99968">
Building noWaC has been possible thanks to NO-
TUR advanced user support and assistance from the
Research Computing Services group (Vitenskapelig
Databehandling) at USIT, University of Oslo. Many
thanks are due to Eros Zanchetta (U. of Bologna),
Adriano Ferraresi (U. of Bologna) and Marco Ba-
roni (U. of Trento) and two anonymous reviewers
for their helpful comments and help.
</bodyText>
<sectionHeader confidence="0.999402" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999741717948718">
M. Baroni and S. Bernardini. 2004. Bootcat: Bootstrap-
ping corpora and terms from the web. In Proceedings
of LREC 2004, pages 1313–1316, Lisbon. ELDA.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226, 09.
David Crystal. 2001. Language and the Internet. Cam-
bridge University Press, Cambridge.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini.
2008. Introducing and evaluating ukWaC, a very large
web-derived corpus of English. In Proceedings of the
WAC4 Workshop at LREC 2008.
R. Ghani, R. Jones, and D. Mladenic. 2001. Mining the
web to create minority language corpora. In Proceed-
ings of the tenth international conference on Informa-
tion and knowledge management, pages 279–286.
Johannessen J.B. Nøklestad A. Hagen, K. 2000. A
constraint-based tagger for norwegian. Odense Work-
ing Papers in Language and Communication, 19(I).
Knut Hofland. 2000. A self-expanding corpus based on
newspapers on the web. In Proceedings of the Sec-
ond International Language Resources and Evaluation
Conference, Paris. European Language Resources As-
sociation.
Adam Kilgarriff and Marco Baroni, editors. 2006. Pro-
ceedings of the 2nd International Workshop on the Web
as Corpus (EACL 2006 SIGWAC Workshop). Associa-
tion for Computational Linguistics, East Stroudsburg,
PA.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333–348.
A. Kilgarriff. 2007. Googleology is bad science. Com-
putational Linguistics, 33(1):147–151.
S. Sharoff. 2005. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Cor-
pus Linguistics, (11):435–462.
</reference>
<page confidence="0.99952">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422299">
<title confidence="0.99976">NoWaC: a large web-based corpus for Norwegian</title>
<author confidence="0.974895">Emiliano</author>
<affiliation confidence="0.997541">Institute for Linguistics and Nordic University of</affiliation>
<email confidence="0.926286">e.r.guevara@iln.uio.no</email>
<abstract confidence="0.974643909090909">In this paper we introduce the first version a large web-based corpus of Bokmål Norwegian currently containing about 700 million tokens. The corpus has been built by crawling, downloading and processweb documents in the internet domain. The procedure used to colthe is largely based on the techniques described by Ferraresi et al. (2008). In brief, first a set of “seed” URLs containing documents in the target language is collected by sending queries to commercial search engines (Google and Yahoo). The obtained seeds (overall 6900 URLs) are then used to start a crawling job using the Heritrix limited to the The downloaded documents are then processed in various ways in order to build a linguistic corpus (e.g. filtering by document size, language identification, duplicate and near duplicate detection, etc.).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
</authors>
<title>Bootcat: Bootstrapping corpora and terms from the web.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<pages>1313--1316</pages>
<location>Lisbon. ELDA.</location>
<contexts>
<context position="11040" citStr="Baroni and Bernardini 2004" startWordPosition="1766" endWordPosition="1770">successfully retrieved pages (the rest consisting of various “page not found” replies and other server-side error messages). The documents in the archive were filtered by size, keeping only those documents that were between 5Kb and 200Kb in size (following Ferraresi et al. 2008 and Baroni et al. 2009). This resulted in a reduced archive of 11.4 million documents for post-processing. 2.4 Post-processing: removing HTML boilerplate and de-duplication At this point of the process, the archive contained raw HTML documents, still very far from being a linguistic corpus. We used the BootCaT toolkit (Baroni and Bernardini 2004, cf. http://sslmit. unibo.it/~baroni/bootcat.html) to perform the major operations to clean our archive. First, every document was processed with the HTML boilerplate removal tool in order to select 3 only the linguistically interesting portions of text while removing all HTML, Javascript and CSS code and non-linguistic material (made mainly of HTML tags, visual formatting, tables, navigation links, etc.) Then, the archive was processed with the duplicate and near-duplicate detecting script in the the BootCaT toolkit, based on a 5-gram model. This is a very drastic strategy leading to a huge </context>
</contexts>
<marker>Baroni, Bernardini, 2004</marker>
<rawString>M. Baroni and S. Bernardini. 2004. Bootcat: Bootstrapping corpora and terms from the web. In Proceedings of LREC 2004, pages 1313–1316, Lisbon. ELDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<volume>43</volume>
<issue>3</issue>
<pages>09</pages>
<contexts>
<context position="7780" citStr="Baroni et al. (2009)" startWordPosition="1245" endWordPosition="1249">nd also directly from the University of Oslo. 1Search engines are in a clear contradiction to the copyright policies in most countries: they crawl, download and index billions of documents with no clearance whatsoever, and also redistribute whole copies of the cached documents. 2 2 Building a corpus of Bolimål by web-crawling 2.1 Methods and tools In this project we decided to follow the methods used to build the WaCky corpora, and to use the related tools as much as possible (e.g. the BootCaT tools). In particular, we tried to reproduce the procedures described by Ferraresi et al. (2008) and Baroni et al. (2009). The methodology has already produced web-corpora ranging from 1.7 to 2.6 billion tokens (German, Italian, British English). However, most of the steps needed some adaptation, finetuning and some extra programming. In particular, given the relatively complex linguistic situation in Norway, a step dedicated to document language identification was added. In short, the building and processing chain used for noWaC comprises the following steps: 1. Extraction of list of mid-frequency Bokmål words from Wikipedia and building query strings 2. Retrieval of seed URLs from search engines by sending aut</context>
<context position="10716" citStr="Baroni et al. 2009" startWordPosition="1718" endWordPosition="1721">al crawling job was stopped after 15 days. In this period of time, a total size of 1 terabyte was crawled, with approximately 90 million URLs being processed by the crawler. Circa 17 million HTML documents were downloaded, adding up to an overall archive size of 550 gigabytes. Only about 13.5 million documents were successfully retrieved pages (the rest consisting of various “page not found” replies and other server-side error messages). The documents in the archive were filtered by size, keeping only those documents that were between 5Kb and 200Kb in size (following Ferraresi et al. 2008 and Baroni et al. 2009). This resulted in a reduced archive of 11.4 million documents for post-processing. 2.4 Post-processing: removing HTML boilerplate and de-duplication At this point of the process, the archive contained raw HTML documents, still very far from being a linguistic corpus. We used the BootCaT toolkit (Baroni and Bernardini 2004, cf. http://sslmit. unibo.it/~baroni/bootcat.html) to perform the major operations to clean our archive. First, every document was processed with the HTML boilerplate removal tool in order to select 3 only the linguistically interesting portions of text while removing all HT</context>
<context position="15308" citStr="Baroni et al. (2009)" startWordPosition="2482" endWordPosition="2485">). 3 Comparing results While it is still too early for us to carry out a fully fledged qualitative evaluation of noWaC, we are able to compare our results with previous published work, especially with the WaCky corpora we tried to emulate. 3.1 NoWaC and the WaCky corpora As we stated above, we tried to follow the WaCky methodology as closely as possible, in the hopes that we could obtain a very large corpus (we aimed at collecting above 1 billion tokens). However, even though our crawling job produced a much bigger initial archive than those reported for German, Italian and British English in Baroni et al. (2009), and 4 even though after document size filtering was applied our archive contained roughly twice as many documents, our final figures (number of tokens and number of documents) only amount to about half the size reported for the WaCky corpora (cf. table 1). In particular, we observe that the most significant drop in size and in number of documents took place during the detection of duplicate and near-duplicate documents (drastically dropping from 11.4 million documents to 1.17 million documents after duplicate filtering). This indicates that, even if a huge number of documents in Bokmål Norwe</context>
<context position="20271" citStr="Baroni et al. 2009" startWordPosition="3332" endWordPosition="3335">WaC itWaC ukWaC noWaC N. of seed pairs 1,653 1,000 2,000 1,000 N. of seed URLs 8,626 5,231 6,528 6,891 Raw crawl size 398GB 379GB 351GB 550GB Size after document size filter 20GB 19GB 19GB 22GB N. of docs after document size filter 4.86M 4.43M 5.69M 11.4M Size after near-duplicate filter 13GB 10GB 12GB 5GB N. of docs after near-duplicate filter 1.75M 1.87M 2.69M 1.17M N. of docs after lang-ID – – – 0.85M N. of tokens 1.27 Bn 1.58 Bn 1.91 Bn 0.69 Bn N. of types 9.3M 3.6M 3.8M 6.0M Table 1: Figure comparison of noWaC and the published WaCky corpora (German, Italian and British English data from Baroni et al. 2009) Word Google frq. noWaC raw frq. noWaC dedup. frq. bilavgifter 33700 1637 314 mekanikk 82900 3266 661 musikkpris 16700 570 171 Table 2: Sample of Google and noWaC document frequencies before and after duplicate removal. noWaC Scaling ratio Dup. ratio (weight) Google estimate % in noWaC 0.78 (0.10) 21.8 bn 3.15% 0.69 bn 24.9 1.97 (0.25) 8.7 bn 7.89% 3.94 (0.50) 4.3 bn 15.79% Table 3: Estimating the size of the Bokmål Norwegian internet as indexed by Google in three different settings (method from Kilgarriff 2007) 4 Concluding remarks Building large web-corpora for languages with a relatively sm</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226, 09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
</authors>
<title>Language and the Internet.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Crystal, 2001</marker>
<rawString>David Crystal. 2001. Language and the Internet. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ferraresi</author>
<author>E Zanchetta</author>
<author>M Baroni</author>
<author>S Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the WAC4 Workshop at LREC</booktitle>
<contexts>
<context position="2610" citStr="Ferraresi et al. (2008)" startWordPosition="416" endWordPosition="419">fer and adapt the available technologies because the suitable sources of data are lacking. Using the web as corpus is a promising option for the latter case, since it can provide with reasonably large and reliable amounts of data in a relatively short time and with a very low production cost. In this paper we present the first version of noWaC, a large web-based corpus of Bokmål Norwegian, a language with a limited web presence, built by crawling the .no internet top level domain. The computational procedure used to collect the noWaC corpus is by and large based on the techniques described by Ferraresi et al. (2008). Our initiative was originally aimed at collecting a 1.5–2 billion word general-purpose corpus comparable to the corpora made available by the WaCky initiative (http://wacky.sslmit.unibo.it). However, carrying out this project on a language with a relatively small online presence such as Bokmål has lead to results which differ from previously reported similar projects. In its current, first version, noWaC contains about 700 million tokens. Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 1–7, Los Angeles, California, June 2010. c�2010 Association for Computational Linguis</context>
<context position="7755" citStr="Ferraresi et al. (2008)" startWordPosition="1240" endWordPosition="1243"> with the WaCky initiative and also directly from the University of Oslo. 1Search engines are in a clear contradiction to the copyright policies in most countries: they crawl, download and index billions of documents with no clearance whatsoever, and also redistribute whole copies of the cached documents. 2 2 Building a corpus of Bolimål by web-crawling 2.1 Methods and tools In this project we decided to follow the methods used to build the WaCky corpora, and to use the related tools as much as possible (e.g. the BootCaT tools). In particular, we tried to reproduce the procedures described by Ferraresi et al. (2008) and Baroni et al. (2009). The methodology has already produced web-corpora ranging from 1.7 to 2.6 billion tokens (German, Italian, British English). However, most of the steps needed some adaptation, finetuning and some extra programming. In particular, given the relatively complex linguistic situation in Norway, a step dedicated to document language identification was added. In short, the building and processing chain used for noWaC comprises the following steps: 1. Extraction of list of mid-frequency Bokmål words from Wikipedia and building query strings 2. Retrieval of seed URLs from sear</context>
<context position="10692" citStr="Ferraresi et al. 2008" startWordPosition="1713" endWordPosition="1716"> website activity. The final crawling job was stopped after 15 days. In this period of time, a total size of 1 terabyte was crawled, with approximately 90 million URLs being processed by the crawler. Circa 17 million HTML documents were downloaded, adding up to an overall archive size of 550 gigabytes. Only about 13.5 million documents were successfully retrieved pages (the rest consisting of various “page not found” replies and other server-side error messages). The documents in the archive were filtered by size, keeping only those documents that were between 5Kb and 200Kb in size (following Ferraresi et al. 2008 and Baroni et al. 2009). This resulted in a reduced archive of 11.4 million documents for post-processing. 2.4 Post-processing: removing HTML boilerplate and de-duplication At this point of the process, the archive contained raw HTML documents, still very far from being a linguistic corpus. We used the BootCaT toolkit (Baroni and Bernardini 2004, cf. http://sslmit. unibo.it/~baroni/bootcat.html) to perform the major operations to clean our archive. First, every document was processed with the HTML boilerplate removal tool in order to select 3 only the linguistically interesting portions of te</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the WAC4 Workshop at LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ghani</author>
<author>R Jones</author>
<author>D Mladenic</author>
</authors>
<title>Mining the web to create minority language corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the tenth international conference on Information and knowledge management,</booktitle>
<pages>279--286</pages>
<marker>Ghani, Jones, Mladenic, 2001</marker>
<rawString>R. Ghani, R. Jones, and D. Mladenic. 2001. Mining the web to create minority language corpora. In Proceedings of the tenth international conference on Information and knowledge management, pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannessen J B Nøklestad A Hagen</author>
<author>K</author>
</authors>
<title>A constraint-based tagger for norwegian. Odense Working Papers in Language and Communication,</title>
<date>2000</date>
<marker>Hagen, K, 2000</marker>
<rawString>Johannessen J.B. Nøklestad A. Hagen, K. 2000. A constraint-based tagger for norwegian. Odense Working Papers in Language and Communication, 19(I).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knut Hofland</author>
</authors>
<title>A self-expanding corpus based on newspapers on the web.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Language Resources and Evaluation Conference, Paris. European Language Resources Association.</booktitle>
<contexts>
<context position="4550" citStr="Hofland 2000" startWordPosition="719" endWordPosition="720"> the spoken language in Norway is not, and using one’s dialect in any occasion is tolerated and even encouraged. This tolerance is rapidly extending to informal writing, especially in modern means of communication and media such as internet forums, social networks, etc. There is a fairly large number of corpora of the Norwegian language, both spoken and written (in both standards). However, most of them are of a limited size (under 50 million words, cf. http:// www.hf.uio.no/tekstlab/ for an overview). To our knowledge, the largest existing written corpus of Norwegian is the Norsk Aviskorpus (Hofland 2000, cf. http://avis.uib.no/), an expanding newspaper-based corpus currently containing 700 million words. However, the Norsk Aviskorpus is only available though a dedicated web interface for non commercial use, and advanced research tasks cannot be freely carried out on its contents. Even though we have only worked on building a web corpus for Bokmål Norwegian, we intend to apply the same procedures to create web-corpora also for Nynorsk and North Sami, thus covering the whole spectrum of written languages in Norway. 1.2 Obtaining legal clearance The legal status of openly accessible webdocument</context>
</contexts>
<marker>Hofland, 2000</marker>
<rawString>Knut Hofland. 2000. A self-expanding corpus based on newspapers on the web. In Proceedings of the Second International Language Resources and Evaluation Conference, Paris. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Marco Baroni</author>
<author>editors</author>
</authors>
<date>2006</date>
<booktitle>Proceedings of the 2nd International Workshop on the Web as Corpus (EACL 2006 SIGWAC Workshop). Association for Computational Linguistics,</booktitle>
<location>East Stroudsburg, PA.</location>
<marker>Kilgarriff, Baroni, editors, 2006</marker>
<rawString>Adam Kilgarriff and Marco Baroni, editors. 2006. Proceedings of the 2nd International Workshop on the Web as Corpus (EACL 2006 SIGWAC Workshop). Association for Computational Linguistics, East Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Introduction to the special issue on the web as corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue on the web as corpus. Computational Linguistics, 29(3):333–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Googleology is bad science.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="18660" citStr="Kilgarriff 2007" startWordPosition="3047" endWordPosition="3048">nt hit comparison, two scaling factors are extracted. The scaling ratio tells us how much smaller our corpus is compared to the Google index for Norwegian (including duplicates and nonrunning-text). The duplicate ratio gives us an idea of how much duplicated material was found in our archive. Since we do not know exactly how much duplicate detection Google performs, we will multiply the duplicate ratio by a weight of 0.1, 0.25 and 0.5 (these weights, in turn, assume that Google discards 10 times less, 4 times less and half what our duplicate removal has done – the latter hypothesis is used by Kilgarriff 2007). • Scaling ratio (average): Google frq. / noWaC raw frq. = 24.9 • Duplicate ratio (average): noWaC raw frq. / dedup. frq. = 7.8 We can then multiply the number of tokens in our final cleaned corpus by the scaling ratio and by the duplicate ratio (weighted) in order to obtain a rough estimate of how much Norwegian text is contained in the Google index. We can also estimate how much of this amount is present in noWaC. Cf. table 3. Using exactly the same procedure as Kilgarrif (2007) leads us to conclude that noWaC should contain over 15% of the Bokmål text indexed by Google. A much more restric</context>
<context position="20788" citStr="Kilgarriff 2007" startWordPosition="3420" endWordPosition="3421"> and the published WaCky corpora (German, Italian and British English data from Baroni et al. 2009) Word Google frq. noWaC raw frq. noWaC dedup. frq. bilavgifter 33700 1637 314 mekanikk 82900 3266 661 musikkpris 16700 570 171 Table 2: Sample of Google and noWaC document frequencies before and after duplicate removal. noWaC Scaling ratio Dup. ratio (weight) Google estimate % in noWaC 0.78 (0.10) 21.8 bn 3.15% 0.69 bn 24.9 1.97 (0.25) 8.7 bn 7.89% 3.94 (0.50) 4.3 bn 15.79% Table 3: Estimating the size of the Bokmål Norwegian internet as indexed by Google in three different settings (method from Kilgarriff 2007) 4 Concluding remarks Building large web-corpora for languages with a relatively small internet presence and with a limited speaker population presents problems and challenges that have not been found in previous work. In particular, the amount of data that can be collected with similar efforts is considerably smaller. In our experience, following as closely as possible the WaCky corpora methodology yielded a corpus that is roughly between one half and one third the size of the published comparable Italian, German and English corpora. In any case, the experience has been very successful so far</context>
</contexts>
<marker>Kilgarriff, 2007</marker>
<rawString>A. Kilgarriff. 2007. Googleology is bad science. Computational Linguistics, 33(1):147–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sharoff</author>
</authors>
<title>Open-source corpora: Using the net to fish for linguistic data.</title>
<date>2005</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>11--435</pages>
<marker>Sharoff, 2005</marker>
<rawString>S. Sharoff. 2005. Open-source corpora: Using the net to fish for linguistic data. International Journal of Corpus Linguistics, (11):435–462.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>