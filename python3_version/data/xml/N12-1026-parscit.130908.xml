<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.642088">
Optimized Online Rank Learning for Machine Translation
</title>
<author confidence="0.890099">
Taro Watanabe
</author>
<affiliation confidence="0.899313">
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.852387">
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 JAPAN
</address>
<email confidence="0.998744">
{taro.watanabe}@nict.go.jp
</email>
<sectionHeader confidence="0.99666" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999672588235294">
We present an online learning algorithm for
statistical machine translation (SMT) based on
stochastic gradient descent (SGD). Under the
online setting of rank learning, a corpus-wise
loss has to be approximated by a batch lo-
cal loss when optimizing for evaluation mea-
sures that cannot be linearly decomposed into
a sentence-wise loss, such as BLEU. We pro-
pose a variant of SGD with a larger batch size
in which the parameter update in each iteration
is further optimized by a passive-aggressive
algorithm. Learning is efficiently parallelized
and line search is performed in each round
when merging parameters across parallel jobs.
Experiments on the NIST Chinese-to-English
Open MT task indicate significantly better
translation results.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939083333333">
The advancement of statistical machine translation
(SMT) relies on efficient tuning of several or many
parameters in a model. One of the standards for such
tuning is minimum error rate training (MERT) (Och,
2003), which directly minimize the loss of transla-
tion evaluation measures, i.e. BLEU (Papineni et al.,
2002). MERT has been successfully used in prac-
tical applications, although, it is known to be un-
stable (Clark et al., 2011). To overcome this insta-
bility, it requires multiple runs from random start-
ing points and directions (Moore and Quirk, 2008),
or a computationally expensive procedure by linear
programming and combinatorial optimization (Gal-
ley and Quirk, 2011).
Many alternative methods have been proposed
based on the algorithms in machine learning, such as
averaged perceptron (Liang et al., 2006), maximum
entropy (Och and Ney, 2002; Blunsom et al., 2008),
Margin Infused Relaxed Algorithm (MIRA) (Watan-
abe et al., 2007; Chiang et al., 2008b), or pairwise
rank optimization (PRO) (Hopkins and May, 2011).
They primarily differ in the mode of training; on-
line or MERT-like batch, and in their objectives;
max-margin (Taskar et al., 2004), conditional log-
likelihood (or softmax loss) (Berger et al., 1996),
risk (Smith and Eisner, 2006; Li and Eisner, 2009),
or ranking (Herbrich et al., 1999).
We present an online learning algorithm based
on stochastic gradient descent (SGD) with a larger
batch size (Shalev-Shwartz et al., 2007). Like Hop-
kins and May (2011), we optimize ranking in n-
best lists, but learn parameters in an online fash-
ion. As proposed by Haddow et al. (2011), BLEU
is approximately computed in the local batch, since
BLEU is not linearly decomposed into a sentence-
wise score (Chiang et al., 2008a), and optimization
for sentence-BLEU does not always achieve opti-
mal parameters for corpus-BLEU. Setting the larger
batch size implies the more accurate corpus-BLEU,
but at the cost of slower convergence of SGD. There-
fore, we propose an optimized update method in-
spired by the passive-aggressive algorithm (Cram-
mer et al., 2006), in which each parameter update is
further rescaled considering the tradeoff between the
amount of updates to the parameters and the ranking
loss. Learning is efficiently parallelized by splitting
training data among shards and by merging parame-
ters in each round (McDonald et al., 2010). Instead
</bodyText>
<page confidence="0.657414333333333">
253
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 253–262,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999706857142857">
of simple averaging, we perform an additional line
search step to find the optimal merging across paral-
lel jobs.
Experiments were carried out on the NIST 2008
Chinese-to-English Open MT task. We found signif-
icant gains over traditional MERT and other tuning
algorithms, such as MIRA and PRO.
</bodyText>
<sectionHeader confidence="0.982688" genericHeader="method">
2 Statistical Machine Translation
</sectionHeader>
<bodyText confidence="0.98846975">
SMT can be formulated as a maximization problem
of finding the most likely translation e given an input
sentence f using a set of parameters θ (Brown et al.,
1993)
</bodyText>
<equation confidence="0.9935215">
e� = arg max p(e|f; θ). (1)
e
</equation>
<bodyText confidence="0.99928875">
Under this maximization setting, we assume that
p(·) is represented by a linear combination of fea-
ture functions h(f, e) which are scaled by a set of
parameters w (Och and Ney, 2002)
</bodyText>
<equation confidence="0.9923475">
e� = arg max wTh(f, e). (2)
e
</equation>
<bodyText confidence="0.9996911">
Each element of h(·) is a feature function which cap-
tures different aspects of translations, for instance,
log of n-gram language model probability, the num-
ber of translated words or log of phrasal probability.
In this paper, we concentrate on the problem of
learning w, which is referred to as tuning. One of
the standard methods for parameter tuning is mini-
mum error rate training (Och, 2003) (MERT) which
directly minimizes the task loss ℓ(·), i.e. negative
BLEU (Papineni et al., 2002), given training data
</bodyText>
<equation confidence="0.847263">
D = {(f1, e1), ..., (fN, eN)}, sets of paired source
sentence fi and its reference translations ei
1i=1N
ℓ(5 arg maxwTh(fi,e),{ei}N1).e
</equation>
<bodyText confidence="0.899008666666667">
(3)
The objective in Equation 3 is discontinuous and
non-convex, and it requires decoding of all the train-
ing data given w. Therefore, MERT relies on a
derivative-free unconstrained optimization method,
such as Powell’s method, which repeatedly chooses
one direction to optimize using a line search pro-
cedure as in Algorithm 1. Expensive decoding is
approximated by an n-best merging technique in
which decoding is carried out in each epoch of it-
erations t and the maximization in Eq. 3 is approxi-
Algorithm 1 MERT
</bodyText>
<listItem confidence="0.987231">
1: Initialize w1
2: for t = 1, ..., T do ▷ Or, until convergence
3: Generate n-bests using wt
4: Learn new wt+1 by Powell’s method
5: end for
6: return wT+1
</listItem>
<bodyText confidence="0.9877614">
mated by search over the n-bests merged across iter-
ations. The merged n-bests are also used in the line
search procedure to efficiently draw the error surface
for efficient computation of the outer minimization
of Eq. 3.
</bodyText>
<sectionHeader confidence="0.997988" genericHeader="method">
3 Online Rank Learning
</sectionHeader>
<subsectionHeader confidence="0.997821">
3.1 Rank Learning
</subsectionHeader>
<bodyText confidence="0.999298666666667">
Instead of the direct task loss minimization of Eq.
3, we would like to find w by solving the L2-
regularized constrained minimization problem
</bodyText>
<equation confidence="0.744187">
arg min
w λ 2 11w1122 + ℓ(w; D) (4)
</equation>
<bodyText confidence="0.999877">
where λ &gt; 0 is a hyperparameter controlling the fit-
ness to the data. The loss function ℓ(·) we consider
here is inspired by a pairwise ranking method (Hop-
kins and May, 2011) in which pairs of correct trans-
lation and incorrect translation are sampled from n-
bests and suffer a hinge loss
</bodyText>
<equation confidence="0.951782375">
M(w; D) (f,e)ED e*,e′
1 � � { }
max 0,1 − wTb(f,e*,e′)
(5)
where
e′ E NBEST(w; f) \ ORACLE(w; f, e)
e* E ORACLE(w; f, e)
b(f, e*, e′) = h(f, e*) − h(f, e′).
</equation>
<bodyText confidence="0.999277">
NBEST(·) is the n-best translations of f generated
with the parameter w, and ORACLE(·) is a set of
oracle translations chosen among NBEST(·). Note
that each e′ (and e*) implicitly represents a deriva-
tion consisting of a tuple (e′, ϕ), where ϕ is a latent
structure, i.e. phrases in a phrase-based SMT, but we
omit ϕ for brevity. M(·) is a normalization constant
which is equal to the number of paired loss terms
b(f, e*, e′) in Equation 5. Since it is impossible
</bodyText>
<equation confidence="0.9972865">
w� = arg min
w
</equation>
<page confidence="0.994555">
254
</page>
<bodyText confidence="0.999726125">
to enumerate all possible translations, we follow the
convention of approximating the domain of transla-
tion by n-bests. Unlike Hopkins and May (2011),
we do not randomly sample from all the pairs in the
n-best translations, but extract pairs by selecting one
oracle translation and one other translation in the n-
bests other than those in ORACLE(·). Oracle trans-
lations are selected by minimizing the task loss,
</bodyText>
<equation confidence="0.566583">
ℓ({e′ ∈ NBEST(w; fi)}Ni=1 , {ei}Ni=1)
</equation>
<bodyText confidence="0.999092">
i.e. negative BLEU, with respect to a set of ref-
erence translations e. In order to compute oracles
with corpus-BLEU, we apply a greedy search strat-
egy over n-bests (Venugopal, 2005). Equation 5 can
be easily interpreted as a constant loss “1” for choos-
ing a wrong translation under current parameters w,
which is in contrast with the direct task-loss used in
max-margin approach to structured output learning
(Taskar et al., 2004).
As an alternative, we would also consider a soft-
max loss (Collins and Koo, 2005) represented by
where
</bodyText>
<equation confidence="0.988222">
ZO(w; f, e) = ∑e*EORACLE(w;f,e) exp(wTf(f, e*))
ZN(w; f) = ∑e′ENBEST(w;f) exp(wTf(f, e′)).
</equation>
<bodyText confidence="0.9808086">
Equation 6 is a log-linear model used in common
NLP tasks such as tagging, chunking and named en-
tity recognition, but differ slightly in that multiple
correct translations are discriminated from the oth-
ers (Charniak and Johnson, 2005).
</bodyText>
<subsectionHeader confidence="0.996678">
3.2 Online Approximation
</subsectionHeader>
<bodyText confidence="0.999211272727273">
Hopkins and May (2011) applied a MERT-like pro-
cedure in Alg. 1 in which Equation 4 was solved
to obtain new parameters in each iteration. Here,
we employ stochastic gradient descent (SGD) meth-
ods as presented in Algorithm 2 motivated by Pega-
sos (Shalev-Shwartz et al., 2007). In each iteration,
we randomly permute D and choose a set of batches
Bt = {bt1, ..., btK} with each bt j consisting of N/K
training data. For each batch b in Bt, we generate
n-bests from the source sentences in b and compute
oracle translations from the newly created n-bests
</bodyText>
<listItem confidence="0.9142251">
Algorithm 2 Stochastic Gradient Descent
1: k = 1,w1 ← 0
2: for t = 1, ..., T do
3: Choose Bt = {bt1, ..., btK} from D
4: for b ∈ Bt do
5: Compute n-bests and oracles of b
6: Set learning rate ηk
7: wk+ 2 1 ← wk − ηk∇(wk; b)
▷ Our proposed algorithm solve lEq. 12 or 16
8: w
</listItem>
<equation confidence="0.976585">
k+ +— min { 1 1/,/λ }
1 l &apos;
11w 1112 JJJ Wk+2
</equation>
<listItem confidence="0.65031725">
9: k ← k + 1
10: end for
11: end for
12: return wk
</listItem>
<bodyText confidence="0.994537545454545">
(line 5) using a batch local corpus-BLEU (Haddow
et al., 2011). Then, we optimize an approximated
objective function
λ 2 ∥w∥�2 + ℓ(w; b) (7)
by replacing D with b in the objective of Eq. 4. The
parameters wk are updated by the sub-gradient of
Equation 7, ∇(wk; b), scaled by the learning rate
ηk (line 7). We use an exponential decayed learn-
ing rate ηk = η0αk/K, which converges very fast in
practice (Tsuruoka et al., 2009)1. The sub-gradient
of Eq.7 with the hinge loss of Eq. 5 is
</bodyText>
<equation confidence="0.9773765">
1 ∑λwk − M(wk; b) ∑ Φ(f, e*, e′) (8)
(f,e)Eb e*,e′
such that
1 − wTk Φ(f, e*, e′) &gt; 0. (9)
</equation>
<bodyText confidence="0.999491">
We found that the normalization term by M(·) was
very slow in convergence, thus, instead, we used
M′(w; b), which was the number of paired loss
terms satisfied the constraints in Equation 9. In the
case of the softmax loss objective of Eq. 6, the sub-
gradient is
</bodyText>
<equation confidence="0.770860333333333">
�
∂ �
∂wL(w; f, e) (10)
(f,e)Eb w=wk
1We set α = 0.85 and 770 = 0.2 which converged well in
our preliminary experiments.
1 ∑ ZO(w; f, e)
N log (6)
(f,e)ED ZN(w;f)
arg min
w
1 ∑ λwk − |b|
</equation>
<page confidence="0.989431">
255
</page>
<bodyText confidence="0.996593481481482">
where L(w; f, e) = log (ZO(w; f, e)/ZN(w; f)).
After the parameter update, wk+21 is projected
within the L2-norm ball (Shalev-Shwartz et al.,
2007).
Setting smaller batch size implies frequent up-
dates to the parameters and a faster convergence.
However, as briefly mentioned in Haddow et al.
(2011), setting batch size to a smaller value, such as
|b |= 1, does not work well in practice, since BLEU
is devised for a corpus based evaluation, not for an
individual sentence-wise evaluation, and it is not lin-
early decomposed into a sentence-wise score (Chi-
ang et al., 2008a). Thus, the smaller batch size may
also imply less accurate batch-local corpus-BLEU
and incorrect oracle translation selections, which
may lead to incorrect sub-gradient estimations or
slower convergence. In the next section we propose
an optimized parameter update which works well
when setting a smaller batch size is impractical due
to its task loss setting.
The problem is inspired by the passive-aggressive
algorithm (Crammer et al., 2006) in which new pa-
rameters are derived through the tradeoff between
the amount of updates to the parameters and the
margin-based loss. Note that the objective in MIRA
is represented by
arg min
</bodyText>
<equation confidence="0.9724495">
w λ ∑2∥w − wk∥22 + ξf,e∗,e′ (13)
(f,e)∈b,e∗,e′
</equation>
<bodyText confidence="0.835717">
If we treat wk+1 as our previous parameters and set
</bodyText>
<page confidence="0.439958">
4
</page>
<bodyText confidence="0.997959833333333">
λ = 1/ηk, they are very similar. Unlike MIRA, the
learning rate ηk is directly used as a tradeoff param-
eter which decays as training proceeds, and the sub-
gradient of the global L2 regularization term is also
combined in the problem through wk+14 .
The Lagrangian dual of Equation 12 is
</bodyText>
<equation confidence="0.895207166666667">
arg min
τe∗�e′ 2∥
1 ∑ Te*,e&apos;�(f� e*� E ) 11 2
(f,e)∈b,e∗,e′
Te*,e/ 2
∑−
</equation>
<sectionHeader confidence="0.987228" genericHeader="method">
4 Optimized Online Rank Learning
</sectionHeader>
<subsectionHeader confidence="0.991741">
4.1 Optimized Parameter Update
</subsectionHeader>
<bodyText confidence="0.99996725">
In line 7 of Algorithm 2, parameters are updated by
the sub-gradient of each training instance in a batch
b. When the sub-gradient in Equation 8 is employed,
the update procedure can be rearranged as
</bodyText>
<equation confidence="0.9959846">
τe∗ e′ { 1 − W +1 -b(f, e∗, e′) } (14)
ll 4
τe∗,e′ ≤ ηk.
(f,e)∈b,e∗,e′
subject to
∑
(f,e)∈b,e∗,e′
∑ ηk
wk+2 1 ← (1−ληk)wk+ M(wk; b)4b(f, e∗, e′)
(f,e)∈b,e∗,e′
</equation>
<bodyText confidence="0.91923">
(11)
in which each individual loss term 4b(·) is scaled uni-
formly by a constant ηk/M(·).
Instead of the uniform scaling, we propose to up-
date the parameters in two steps: First, we suffer the
sub-gradient from the L2 regularization
</bodyText>
<equation confidence="0.757377142857143">
wk+14 ←(1 − ληk)wk.
Second, we solve the following problem
arg min 1 ∑ 2∥w−wk+1 4 ∥22+ηk ξf,e∗,e′ (12)
w (f,e)∈b,e∗,e′
such that
w⊤4b(f, e∗, e′) ≥ 1 − ξf,e∗,e′
ξf,e∗,e′ ≥ �.
</equation>
<bodyText confidence="0.999950416666667">
We used a dual coordinate descent algorithm (Hsieh
et al., 2008)2 to efficiently solve the quadratic pro-
gram (QP) in Equation 14, leading to an update
When compared with Equation 11, the update pro-
cedure in Equation 15 rescales the contribution from
each sub-gradient through the Lagrange multipliers
τe∗,e′. Note that if we set τe∗,e′ = ηk/M(·), we sat-
isfy the constraints in Eq. 14, and recover the update
in Eq. 11.
In the same manner as Eq. 12, we derive an opti-
mized update procedure for the softmax loss, which
replaces the update with Equation 10, by solving the
</bodyText>
<equation confidence="0.819106">
2Specifically, each parameter is bound constrained 0 &lt; T &lt;
rlk but is not summation constrained ∑ T &lt; 77k. Thus, we re-
normalize T after optimization.
∑
wk+1← wk+1 +
2 4
τe∗,e′4b(f, e∗, e′). (15)
(f,e)∈b,e∗,e′
</equation>
<page confidence="0.973984">
256
</page>
<bodyText confidence="0.9623095">
following problem
such that
</bodyText>
<equation confidence="0.983859">
w⊤IF(wk; f, e) ≥ −L(wk; f, e) − ξf
ξf ≥ 0
</equation>
<bodyText confidence="0.999265625">
in which IF(w′; f, e)= awL(w; f, e) Iw=w′. Equa-
tion 16 can be interpreted as a cutting-plane approx-
imation for the objective of Eq. 7, in which the orig-
inal objective of Eq. 7 with the softmax loss in Eq.
6 is approximated by |b |linear constraints derived
from the sub-gradients at point wk (Teo et al., 2010).
Eq. 16 is efficiently solved by its Lagrange dual,
leading to an update
</bodyText>
<equation confidence="0.72806775">
∑ τfIF(wk; f, e) (17)
wk+ 1 ← wk+ 1 +
2 4
(f,e)Eb
</equation>
<bodyText confidence="0.99580125">
subject to ∑(f,e)Eb τf ≤ ηk. Similar to Eq. 15, the
parameter update by IF(·) is rescaled by its Lagrange
multipliers τf in place of the uniform scale of 1/|b|
in the sub-gradient of Eq. 10.
</bodyText>
<subsectionHeader confidence="0.959078">
4.2 Line Search for Parameter Mixing
</subsectionHeader>
<bodyText confidence="0.999044684210526">
For faster training, we employ an efficient paral-
lel training strategy proposed by McDonald et al.
(2010). The training data D is split into S disjoint
shards, {D1, ..., DS}. Each shard learns its own pa-
rameters in each single epoch t and performs param-
eter mixing by averaging parameters across shards.
We propose an optimized parallel training in Al-
gorithm 3 which performs better mixing with re-
spect to the task loss, i.e. negative BLEU. In line
5, wt+21 is computed by averaging
wt+1,s from all
Then, the new parameters wt+1 are obtained by
linearly interpolating with the parameters from the
previous epoch wt. The linear interpolation weight
is efficiently computed by a line search proce-
dure which directly minimizes the negative corpus-
BLEU. The procedure is exactly the same as the line
search strategy employed in MERT using wt as our
starti
</bodyText>
<equation confidence="0.3371315">
Ds.
ρ
</equation>
<bodyText confidence="0.992827666666667">
ng point with the direction wt+21 − wt. The
idea of using the line search procedure is to find the
optimum parameters under corpus-BLEU without a
</bodyText>
<figure confidence="0.95102476">
Algorithm 3 Distri
2: for t =
T do
wt
Distribute parameters
4: Each shard learns wt+1,s using
Line
in Alg. 2
wt+1,s
Mixing
(1
+
w1 ← 0
1, ...,
wt,s←
▷
Ds▷
3–10
wt+21←1/S∑s
▷
wt+1←
−ρ)wt
ρwt+21▷ Line search
7: end for
8: return wT+1
</figure>
<bodyText confidence="0.854951428571429">
Thus, the optimum
obtained by the line search may
be suboptimal in terms of the training objective, but
potentially better than
ρ
averaging for minimizing the
final task loss.
</bodyText>
<sectionHeader confidence="0.996581" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.980813142857143">
the shards after local training using their own data
buted training with line search
batch-local approximation. Unlike MERT, however,
we do not memorize nor merge all the n-bests gener-
ated across iterations, but keep only n-bests in each
iteration for faster training and for memory saving.
Experiments were carried out on the NIST 2008
Chinese-to-English Open MT task. The training
data consists of nearly 5.6 million bilingual sen-
tences and additional monolingual data, English
Gigaword, for 5-gram language model estimation.
and
were used as our tuning and devel-
opment testing, and MT08 as our final testing with
all data consisting of four reference translations.
We use an in-house developed hypergraph-based
toolkit for training and decoding with synchronous-
CFGs (SCFG) for hierarchical phrase-bassed SMT
(Chiang, 2007). The system employs 14 features,
consisting of standard Hiero-style features (Chiang,
2007), and a set of indicator features, such as the
number of synchronous-rules in a derivation. Two
5-gram language models are also included, one from
the English-side of bitexts and the other from En-
glish Gigaword, with features counting the number
of out-of-vocabulary words in each model (Dyer et
al., 2011). For faster experiments, we precomputed
translation forests inspired by Xiao et al. (2011). In-
stead of generating forests from bitexts in each it-
eration, we construct and save translation forests by
intersecting the source side of SCFG with input sen-
tences an
MT02
MT06
d by keeping the target side of the inter-
</bodyText>
<figure confidence="0.819442166666667">
arg min
w
∑2∥w−wk+14∥22+ηk
(f,e)Eb
ξf (16)
1
</figure>
<page confidence="0.989499">
257
</page>
<bodyText confidence="0.999909825">
sected rules. n-bests are generated from the pre-
computed forests on the fly using the forest rescor-
ing framework (Huang and Chiang, 2007) with ad-
ditional non-local features, such as 5-gram language
models.
We compared four algorithms, MERT, PRO,
MIRA and our proposed online settings, online rank
optimization (ORO). Note that ORO without our op-
timization methods in Section 4 is essentially the
same as Pegasos, but differs in that we employ the
algorithm for ranking structured outputs with var-
ied objectives, hinge loss or softmax loss3. MERT
learns parameters from forests (Kumar et al., 2009)
with 4 restarts and 8 random directions in each it-
eration. We experimented on a variant of PRO4, in
which the objective in Eq. 4 with the hinge loss of
Eq. 5 was solved in each iteration in line 4 of Alg. 1
using an off-the-shelf solver5. Our MIRA solves the
problem in Equation 13 in line 7 of Alg. 2. For a sys-
tematic comparison, we used our exhaustive oracle
translation selection method in Section 3 for PRO,
MIRA and ORO. For each learning algorithm, we
ran 30 iterations and generated duplicate removed
1,000-best translations in each iteration. The hyper-
parameter A for PRO and ORO was set to 10−5, se-
lected from among {10−3,10−4,10−5}, and 102 for
MIRA, chosen from {10,102,103} by preliminary
testing on MT06. Both decoding and learning are
parallelized and run on 8 cores. Each online learn-
ing took roughly 12 hours, and PRO took one day. It
took roughly 3 days for MERT with 20 iterations.
Translation results are measured by case sensitive
BLEU.
Table 1 presents our main results. Among the pa-
rameters from multiple iterations, we report the out-
puts that performed the best on MT06. With Moses
(Koehn et al., 2007), we achieved 30.36 and 23.64
BLEU for MT06 and MT08, respectively. We de-
note the “O-” prefix for the optimized parameter up-
dates discussed in Section 4.1, and the “-L” suffix
</bodyText>
<footnote confidence="0.96805075">
3The other major difference is the use of a simpler learning
rate, ak , which was very slow in our preliminary studies.
4Hopkins and May (2011) minimized logistic loss sampled
from the merged n-bests, and sentence-BLEU was used for de-
termining ranks.
5We used liblinear (Fan et al., 2008) at http://www.
csie.ntu.edu.tw/˜cjlin/liblinear with the solver
type of 3.
</footnote>
<table confidence="0.992163125">
MT06 MT08
MERT 31.45† 24.13†
PRO 31.76† 24.43†
MIRA-L 31.42† 24.15†
ORO-Lhinge 29.76 21.96
O-ORO-Lhinge 32.06 24.95
ORO-Lsoftmax 30.77 23.07
O-ORO-Lsoftmax 31.16† 23.20
</table>
<tableCaption confidence="0.998999">
Table 1: Translation results by BLEU. Results with-
</tableCaption>
<bodyText confidence="0.90623">
out significant differences from the MERT baseline
are marked †. The numbers in boldface are signif-
icantly better than the MERT baseline (both mea-
sured by the bootstrap resampling (Koehn, 2004)
with p &gt; 0.05).
</bodyText>
<figure confidence="0.9215745">
0 5 10 15 20 25 30
iteration
</figure>
<figureCaption confidence="0.991229">
Figure 1: Learning curves for three algorithms,
MIRA-L, ORO-Lhinge and O-ORO-Lhinge.
</figureCaption>
<bodyText confidence="0.979886866666667">
for parameter mixing by line search as described in
Section 4.2. The batch size was set to 16 for MIRA
and ORO. In general, our PRO and MIRA settings
achieved the results very comparable to MERT. The
hinge-loss and softmax objective OROs were lower
than those of the three baselines. The softmax ob-
jective with the optimized update (O-ORO-Lsoftmax)
performed better than the non-optimized version,
but it was still lower than our baselines. In the case
of the hinge-loss objective with the optimized update
(O-ORO-Lhinge), the gain in MT08 was significant,
and achieved the best BLEU.
Figure 1 presents the learning curves for three al-
gorithms MIRA-L, ORO-Lhinge and O-ORO-Lhinge,
in which the performance is measured by BLEU
</bodyText>
<figure confidence="0.962303738095238">
BLEU
40
35
30
25
20
15
10
5
0
MIRA-L MT02
MT08
ORO-L MT02
MT08
O-ORO-L MT02
MT08
258
MT06 MT08
MIRA 30.95 23.06
MIRA-L 31.42† 24.15†
OROhinge 29.09 21.93
ORO-Lhinge 29.76 21.96
OROsoftmax 30.80 23.06
ORO-Lsoftmax 30.77 23.07
O-OROhinge 31.15† 23.20
O-ORO-Lhinge 32.06 24.95
O-OROsoftmax 31.40† 23.93†
O-ORO-Lsoftmax 31.16† 23.20
35
BLEU
30
25
ORO-L batch-16
batch-8
batch-4
O-ORO-L batch-16
batch-8
batch-4
0 5 10 15 20 25 30
iteration
40
20
</figure>
<tableCaption confidence="0.984848">
Table 2: Parameter mixing by line search.
</tableCaption>
<bodyText confidence="0.999941878787879">
on the training data (MT02) and on the test data
(MT08). MIRA-L quickly converges and is slightly
unstable in the test set, while ORO-Lhinge is very sta-
ble and slow to converge, but with low performance
on the training and test data. The stable learning
curve in ORO-Lhinge is probably influenced by our
learning rate parameter qo = 0.2, which will be
investigated in future work. O-ORO-Lhinge is less
stable in several iterations, but steadily improves its
BLEU. The behavior is justified by our optimized
update procedure, in which the learning rate q� is
used as a tradeoff parameter. Thus, it tries a very
aggressive update at the early stage of training, but
eventually becomes conservative in updating param-
eters.
Next, we compare the effect of line search for pa-
rameter mixing in Table 2. Line search was very
effective for MIRA and O-OROhinge, but less effec-
tive for the others. Since the line search procedure
directly minimizes a task loss, not objectives, this
may hurt the performance for the softmax objective,
where the margins between the correct and incorrect
translations are softly penalized.
Finally, Table 3 shows the effect of batch size se-
lected from 11, 4, 8,16}. There seems to be no clear
trends in MIRA, and we achieved BLEU score of
24.58 by setting the batch size to 8. Clearly, set-
ting smaller batch size is better for ORO, but it is
the reverse for the optimized variants of both the
hinge and softmax objectives. Figure 2 compares
ORO-Lhinge and O-ORO-Lhinge on MT02 with dif-
ferent batch size settings. ORO-Lhinge converges
faster when the batch size is smaller and fine tun-
</bodyText>
<figureCaption confidence="0.8944205">
Figure 2: Learning curves on MT02 for ORO-Lhinge
and O-ORO-Lhinge with different batch size.
</figureCaption>
<bodyText confidence="0.999867272727273">
ing of the learning rate parameter will be required
for a larger batch size. As discussed in Section 3,
the smaller batch size means frequent updates to pa-
rameters and a faster convergence, but potentially
leads to a poor performance since the corpus-BLEU
is approximately computed in a local batch. Our op-
timized update algorithms address the problem by
adjusting the tradeoff between the amount of up-
date to parameters and the loss, and perform better
for larger batch sizes with a more accurate corpus-
BLEU.
</bodyText>
<sectionHeader confidence="0.999863" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999959111111111">
Our work is largely inspired by pairwise rank op-
timization (Hopkins and May, 2011), but runs in
an online fashion similar to (Watanabe et al., 2007;
Chiang et al., 2008b). Major differences come from
the corpus-BLEU computation used to select oracle
translations. Instead of the sentence-BLEU used by
Hopkins and May (2011) or the corpus-BLEU statis-
tics accumulated from previous translations gener-
ated by different parameters (Watanabe et al., 2007;
Chiang et al., 2008b), we used a simple batch lo-
cal corpus-BLEU (Haddow et al., 2011) in the same
way as an online approximation to the objectives.
An alternative is the use of a Taylor series approxi-
mation (Smith and Eisner, 2006; Rosti et al., 2011),
which was not investigated in this paper.
Training is performed by SGD with a parame-
ter projection method (Shalev-Shwartz et al., 2007).
Slower training incurred by the larger batch size
</bodyText>
<page confidence="0.995056">
259
</page>
<table confidence="0.997722142857143">
MT06 MT08
batch size 1 4 8 16 1 4 8 16
MIRA-L 31.281 31.531 31.631 31.421 23.46 23.971 24.58 24.151
ORO-Lhinge 31.321 30.69 29.61 29.76 23.63 23.12 22.07 21.96
O-ORO-Lhinge 31.441 31.541 31.351 32.06 23.72 24.021 24.281 24.95
ORO-Lsoftmax 25.10 31.661 31.311 30.77 19.27 23.59 23.50 23.07
O-ORO-Lsoftmax 31.151 31.171 30.90 31.161 23.62 23.31 23.03 23.20
</table>
<tableCaption confidence="0.999891">
Table 3: Translation results with varied batch size.
</tableCaption>
<bodyText confidence="0.99997662962963">
for more accurate corpus-BLEU is addressed by
optimally scaling parameter updates in the spirit
of a passive-aggressive algorithm (Crammer et al.,
2006). The derived algorithm is very similar to
MIRA, but differs in that the learning rate is em-
ployed as a hyperparameter for controlling the fit-
ness to training data which decays when training
proceeds. The non-uniform sub-gradient based up-
date is also employed in an exponentiated gradient
(EG) algorithm (Kivinen and Warmuth, 1997; Kivi-
nen and Warmuth, 2001) in which parameter updates
are maximum-likely estimated using an exponen-
tially combined sub-gradients. In contrast, our ap-
proach relies on an ultraconservative update which
tradeoff between the amount of updates performed
to the parameters and the progress made for the ob-
jectives by solving a QP subproblem.
Unlike a complex parallelization by Chiang et
al. (2008b), in which support vectors are asyn-
chronously exchanged among parallel jobs, train-
ing is efficiently and easily carried out by distribut-
ing training data among shards and by mixing pa-
rameters in each iteration (McDonald et al., 2010).
Rather than simple averaging, new parameters are
derived by linearly interpolating with the previously
mixed parameters, and its weight is determined by
the line search algorithm employed in (Och, 2003).
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999826185185186">
We proposed a variant of an online learning al-
gorithm inspired by a batch learning algorithm of
(Hopkins and May, 2011). Training is performed by
SGD with a parameter projection (Shalev-Shwartz et
al., 2007) using a larger batch size for a more accu-
rate batch local corpus-BLEU estimation. Parameter
updates in each iteration is further optimized using
an idea from a passive-aggressive algorithm (Cram-
mer et al., 2006). Learning is efficiently parallelized
(McDonald et al., 2010) and the locally learned pa-
rameters are mixed by an additional line search step.
Experiments indicate that better performance was
achieved by our optimized updates and by the more
sophisticated parameter mixing.
In future work, we would like to investigate other
objectives with a more direct task loss, such as max-
margin (Taskar et al., 2004), risk (Smith and Eisner,
2006) or softmax-loss (Gimpel and Smith, 2010),
and different regularizers, such as Li-norm for a
sparse solution. Instead of n-best approximations,
we may directly employ forests for a better con-
ditional log-likelihood estimation (Li and Eisner,
2009). We would also like to explore other mix-
ing strategies for parallel training which can directly
minimize the training objectives like those proposed
for a cutting-plane algorithm (Franc and Sonnen-
burg, 2008).
</bodyText>
<sectionHeader confidence="0.997563" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999788">
We would like to thank anonymous reviewers and
our colleagues for helpful comments and discussion.
</bodyText>
<sectionHeader confidence="0.997934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999374666666666">
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22:39–71, March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL-08: HLT, pages
200–208, Columbus, Ohio, June.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19:263–311, June.
</reference>
<page confidence="0.979722">
260
</page>
<reference confidence="0.999483897196261">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. ofACL 2005, pages 173–180, Ann Arbor,
Michigan, June.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient al-
gorithms. In Proc. of EMNLP 2008, pages 610–619,
Honolulu, Hawaii, October.
David Chiang, Yuval Marton, and Philip Resnik. 2008b.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP 2008,
pages 224–233, Honolulu, Hawaii, October.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. ofACL 2011, pages 176–181, Portland,
Oregon, USA, June.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25–70, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585, March.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The cmu-ark german-english
translation system. In Proc. of SMT 2011, pages 337–
343, Edinburgh, Scotland, July.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874, June.
Vojtˇech Franc and Soeren Sonnenburg. 2008. Optimized
cutting plane algorithm for support vector machines.
In Proc. of ICML ’08, pages 320–327, Helsinki, Fin-
land.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proc. of EMNLP
2011, pages 38–49, Edinburgh, Scotland, UK., July.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
margin crfs: Training log-linear models with cost
functions. In Proc. of NAACL-HLT 2010, pages 733–
736, Los Angeles, California, June.
Barry Haddow, Abhishek Arun, and Philipp Koehn.
2011. Samplerank training for phrase-based machine
translation. In Proc. of SMT 2011, pages 261–271, Ed-
inburgh, Scotland, July.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support vector learning for ordinal regression.
In In Proc. of International Conference on Artificial
Neural Networks, pages 97–102.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proc. of EMNLP 2011, pages 1352–1362, Ed-
inburgh, Scotland, UK., July.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In Proc. of
ICML ’08, pages 408–415, Helsinki, Finland.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language models.
In Proc. of ACL 2007, pages 144–151, Prague, Czech
Republic, June.
Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132(1):1–
63, January.
J. Kivinen and M. K. Warmuth. 2001. Relative
loss bounds for multidimensional regression problems.
Machine Learning, 45(3):301–329, December.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Procc of
ACL 2007, pages 177–180, Prague, Czech Republic,
June.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388–395, Barcelona, Spain, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proc. of ACL-IJCNLP
2009, pages 163–171, Suntec, Singapore, August.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of EMNLP
2009, pages 40–51, Singapore, August.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of COL-
ING/ACL 2006, pages 761–768, Sydney, Australia,
July.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proc. of NAACL-HLT 2010, pages 456–
464, Los Angeles, California, June.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proc. of COLING 2008, pages
585–592, Manchester, UK, August.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
</reference>
<page confidence="0.963708">
261
</page>
<reference confidence="0.999929854166667">
tical machine translation. In Proc. of ACL 2002, pages
295–302, Philadelphia, July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160–167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. of ACL 2002,
pages 311–318, Philadelphia, Pennsylvania, USA,
July.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected bleu training for
graphs: Bbn system description for wmt11 system
combination task. In Proc. of SMT 2011, pages 159–
165, Edinburgh, Scotland, July.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proc. of ICML ’07, pages 807–814, Cor-
valis, Oregon.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In Proc.
of COLING/ACL 2006, pages 787–794, Sydney, Aus-
tralia, July.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. of EMNLP 2004, pages 1–8, Barcelona, Spain,
July.
Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, and
Quoc V. Le. 2010. Bundle methods for regularized
risk minimization. Journal of Machine Learning Re-
search, 11:311–365, March.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proc. of ACL-IJCNLP 2009, pages 477–
485, Suntec, Singapore, August.
Ashish Venugopal. 2005. Considerations in maximum
mutual information and minimum classification error
training for statistical machine translation. In Proc. of
EAMT-05, page 3031.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of EMNLP-CoNLL
2007, pages 764–773, Prague, Czech Republic, June.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Fast generation of translation forest for large-
scale smt discriminative training. In Proc. of EMNLP
2011, pages 880–888, Edinburgh, Scotland, UK., July.
</reference>
<page confidence="0.997295">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.763517">
<title confidence="0.999186">Optimized Online Rank Learning for Machine Translation</title>
<author confidence="0.825245">Taro</author>
<affiliation confidence="0.955867">National Institute of Information and Communications</affiliation>
<address confidence="0.952885">3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289</address>
<email confidence="0.980481">{taro.watanabe}@nict.go.jp</email>
<abstract confidence="0.997551222222222">We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--39</pages>
<contexts>
<context position="2207" citStr="Berger et al., 1996" startWordPosition="329" endWordPosition="332">linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpu</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>200--208</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1859" citStr="Blunsom et al., 2008" startWordPosition="274" endWordPosition="277">lation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL-08: HLT, pages 200–208, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4061" citStr="Brown et al., 1993" startWordPosition="624" endWordPosition="627">: Human Language Technologies, pages 253–262, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics of simple averaging, we perform an additional line search step to find the optimal merging across parallel jobs. Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task. We found significant gains over traditional MERT and other tuning algorithms, such as MIRA and PRO. 2 Statistical Machine Translation SMT can be formulated as a maximization problem of finding the most likely translation e given an input sentence f using a set of parameters θ (Brown et al., 1993) e� = arg max p(e|f; θ). (1) e Under this maximization setting, we assume that p(·) is represented by a linear combination of feature functions h(f, e) which are scaled by a set of parameters w (Och and Ney, 2002) e� = arg max wTh(f, e). (2) e Each element of h(·) is a feature function which captures different aspects of translations, for instance, log of n-gram language model probability, the number of translated words or log of phrasal probability. In this paper, we concentrate on the problem of learning w, which is referred to as tuning. One of the standard methods for parameter tuning is m</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL 2005,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="8297" citStr="Charniak and Johnson, 2005" startWordPosition="1369" endWordPosition="1372">t loss “1” for choosing a wrong translation under current parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning (Taskar et al., 2004). As an alternative, we would also consider a softmax loss (Collins and Koo, 2005) represented by where ZO(w; f, e) = ∑e*EORACLE(w;f,e) exp(wTf(f, e*)) ZN(w; f) = ∑e′ENBEST(w;f) exp(wTf(f, e′)). Equation 6 is a log-linear model used in common NLP tasks such as tagging, chunking and named entity recognition, but differ slightly in that multiple correct translations are discriminated from the others (Charniak and Johnson, 2005). 3.2 Online Approximation Hopkins and May (2011) applied a MERT-like procedure in Alg. 1 in which Equation 4 was solved to obtain new parameters in each iteration. Here, we employ stochastic gradient descent (SGD) methods as presented in Algorithm 2 motivated by Pegasos (Shalev-Shwartz et al., 2007). In each iteration, we randomly permute D and choose a set of batches Bt = {bt1, ..., btK} with each bt j consisting of N/K training data. For each batch b in Bt, we generate n-bests from the source sentences in b and compute oracle translations from the newly created n-bests Algorithm 2 Stochasti</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proc. ofACL 2005, pages 173–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Decomposability of translation metrics for improved evaluation and efficient algorithms.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>610--619</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1944" citStr="Chiang et al., 2008" startWordPosition="288" endWordPosition="291">ly used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. </context>
<context position="10845" citStr="Chiang et al., 2008" startWordPosition="1867" endWordPosition="1871"> (6) (f,e)ED ZN(w;f) arg min w 1 ∑ λwk − |b| 255 where L(w; f, e) = log (ZO(w; f, e)/ZN(w; f)). After the parameter update, wk+21 is projected within the L2-norm ball (Shalev-Shwartz et al., 2007). Setting smaller batch size implies frequent updates to the parameters and a faster convergence. However, as briefly mentioned in Haddow et al. (2011), setting batch size to a smaller value, such as |b |= 1, does not work well in practice, since BLEU is devised for a corpus based evaluation, not for an individual sentence-wise evaluation, and it is not linearly decomposed into a sentence-wise score (Chiang et al., 2008a). Thus, the smaller batch size may also imply less accurate batch-local corpus-BLEU and incorrect oracle translation selections, which may lead to incorrect sub-gradient estimations or slower convergence. In the next section we propose an optimized parameter update which works well when setting a smaller batch size is impractical due to its task loss setting. The problem is inspired by the passive-aggressive algorithm (Crammer et al., 2006) in which new parameters are derived through the tradeoff between the amount of updates to the parameters and the margin-based loss. Note that the objecti</context>
<context position="23702" citStr="Chiang et al., 2008" startWordPosition="4064" endWordPosition="4067">e. As discussed in Section 3, the smaller batch size means frequent updates to parameters and a faster convergence, but potentially leads to a poor performance since the corpus-BLEU is approximately computed in a local batch. Our optimized update algorithms address the problem by adjusting the tradeoff between the amount of update to parameters and the loss, and perform better for larger batch sizes with a more accurate corpusBLEU. 6 Related Work Our work is largely inspired by pairwise rank optimization (Hopkins and May, 2011), but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. Training is performed </context>
<context position="25712" citStr="Chiang et al. (2008" startWordPosition="4382" endWordPosition="4385">hyperparameter for controlling the fitness to training data which decays when training proceeds. The non-uniform sub-gradient based update is also employed in an exponentiated gradient (EG) algorithm (Kivinen and Warmuth, 1997; Kivinen and Warmuth, 2001) in which parameter updates are maximum-likely estimated using an exponentially combined sub-gradients. In contrast, our approach relies on an ultraconservative update which tradeoff between the amount of updates performed to the parameters and the progress made for the objectives by solving a QP subproblem. Unlike a complex parallelization by Chiang et al. (2008b), in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration (McDonald et al., 2010). Rather than simple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in (Och, 2003). 7 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of (Hopkins and May, 2011). Training is performed by </context>
</contexts>
<marker>Chiang, DeNeefe, Chan, Ng, 2008</marker>
<rawString>David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng. 2008a. Decomposability of translation metrics for improved evaluation and efficient algorithms. In Proc. of EMNLP 2008, pages 610–619, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>224--233</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1944" citStr="Chiang et al., 2008" startWordPosition="288" endWordPosition="291">ly used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. </context>
<context position="10845" citStr="Chiang et al., 2008" startWordPosition="1867" endWordPosition="1871"> (6) (f,e)ED ZN(w;f) arg min w 1 ∑ λwk − |b| 255 where L(w; f, e) = log (ZO(w; f, e)/ZN(w; f)). After the parameter update, wk+21 is projected within the L2-norm ball (Shalev-Shwartz et al., 2007). Setting smaller batch size implies frequent updates to the parameters and a faster convergence. However, as briefly mentioned in Haddow et al. (2011), setting batch size to a smaller value, such as |b |= 1, does not work well in practice, since BLEU is devised for a corpus based evaluation, not for an individual sentence-wise evaluation, and it is not linearly decomposed into a sentence-wise score (Chiang et al., 2008a). Thus, the smaller batch size may also imply less accurate batch-local corpus-BLEU and incorrect oracle translation selections, which may lead to incorrect sub-gradient estimations or slower convergence. In the next section we propose an optimized parameter update which works well when setting a smaller batch size is impractical due to its task loss setting. The problem is inspired by the passive-aggressive algorithm (Crammer et al., 2006) in which new parameters are derived through the tradeoff between the amount of updates to the parameters and the margin-based loss. Note that the objecti</context>
<context position="23702" citStr="Chiang et al., 2008" startWordPosition="4064" endWordPosition="4067">e. As discussed in Section 3, the smaller batch size means frequent updates to parameters and a faster convergence, but potentially leads to a poor performance since the corpus-BLEU is approximately computed in a local batch. Our optimized update algorithms address the problem by adjusting the tradeoff between the amount of update to parameters and the loss, and perform better for larger batch sizes with a more accurate corpusBLEU. 6 Related Work Our work is largely inspired by pairwise rank optimization (Hopkins and May, 2011), but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. Training is performed </context>
<context position="25712" citStr="Chiang et al. (2008" startWordPosition="4382" endWordPosition="4385">hyperparameter for controlling the fitness to training data which decays when training proceeds. The non-uniform sub-gradient based update is also employed in an exponentiated gradient (EG) algorithm (Kivinen and Warmuth, 1997; Kivinen and Warmuth, 2001) in which parameter updates are maximum-likely estimated using an exponentially combined sub-gradients. In contrast, our approach relies on an ultraconservative update which tradeoff between the amount of updates performed to the parameters and the progress made for the objectives by solving a QP subproblem. Unlike a complex parallelization by Chiang et al. (2008b), in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration (McDonald et al., 2010). Rather than simple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in (Och, 2003). 7 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of (Hopkins and May, 2011). Training is performed by </context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008b. Online large-margin training of syntactic and structural translation features. In Proc. of EMNLP 2008, pages 224–233, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="16535" citStr="Chiang, 2007" startWordPosition="2866" endWordPosition="2867">keep only n-bests in each iteration for faster training and for memory saving. Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task. The training data consists of nearly 5.6 million bilingual sentences and additional monolingual data, English Gigaword, for 5-gram language model estimation. and were used as our tuning and development testing, and MT08 as our final testing with all data consisting of four reference translations. We use an in-house developed hypergraph-based toolkit for training and decoding with synchronousCFGs (SCFG) for hierarchical phrase-bassed SMT (Chiang, 2007). The system employs 14 features, consisting of standard Hiero-style features (Chiang, 2007), and a set of indicator features, such as the number of synchronous-rules in a derivation. Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model (Dyer et al., 2011). For faster experiments, we precomputed translation forests inspired by Xiao et al. (2011). Instead of generating forests from bitexts in each iteration, we construct and save translation forests by in</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. ofACL 2011,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1417" citStr="Clark et al., 2011" startWordPosition="207" endWordPosition="210">ed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They pri</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. ofACL 2011, pages 176–181, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--25</pages>
<contexts>
<context position="7950" citStr="Collins and Koo, 2005" startWordPosition="1315" endWordPosition="1318">RACLE(·). Oracle translations are selected by minimizing the task loss, ℓ({e′ ∈ NBEST(w; fi)}Ni=1 , {ei}Ni=1) i.e. negative BLEU, with respect to a set of reference translations e. In order to compute oracles with corpus-BLEU, we apply a greedy search strategy over n-bests (Venugopal, 2005). Equation 5 can be easily interpreted as a constant loss “1” for choosing a wrong translation under current parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning (Taskar et al., 2004). As an alternative, we would also consider a softmax loss (Collins and Koo, 2005) represented by where ZO(w; f, e) = ∑e*EORACLE(w;f,e) exp(wTf(f, e*)) ZN(w; f) = ∑e′ENBEST(w;f) exp(wTf(f, e′)). Equation 6 is a log-linear model used in common NLP tasks such as tagging, chunking and named entity recognition, but differ slightly in that multiple correct translations are discriminated from the others (Charniak and Johnson, 2005). 3.2 Online Approximation Hopkins and May (2011) applied a MERT-like procedure in Alg. 1 in which Equation 4 was solved to obtain new parameters in each iteration. Here, we employ stochastic gradient descent (SGD) methods as presented in Algorithm 2 mo</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31:25–70, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="3046" citStr="Crammer et al., 2006" startWordPosition="466" endWordPosition="470">l., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm (Crammer et al., 2006), in which each parameter update is further rescaled considering the tradeoff between the amount of updates to the parameters and the ranking loss. Learning is efficiently parallelized by splitting training data among shards and by merging parameters in each round (McDonald et al., 2010). Instead 253 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 253–262, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics of simple averaging, we perform an additional line search step to find the</context>
<context position="11291" citStr="Crammer et al., 2006" startWordPosition="1934" endWordPosition="1937">LEU is devised for a corpus based evaluation, not for an individual sentence-wise evaluation, and it is not linearly decomposed into a sentence-wise score (Chiang et al., 2008a). Thus, the smaller batch size may also imply less accurate batch-local corpus-BLEU and incorrect oracle translation selections, which may lead to incorrect sub-gradient estimations or slower convergence. In the next section we propose an optimized parameter update which works well when setting a smaller batch size is impractical due to its task loss setting. The problem is inspired by the passive-aggressive algorithm (Crammer et al., 2006) in which new parameters are derived through the tradeoff between the amount of updates to the parameters and the margin-based loss. Note that the objective in MIRA is represented by arg min w λ ∑2∥w − wk∥22 + ξf,e∗,e′ (13) (f,e)∈b,e∗,e′ If we treat wk+1 as our previous parameters and set 4 λ = 1/ηk, they are very similar. Unlike MIRA, the learning rate ηk is directly used as a tradeoff parameter which decays as training proceeds, and the subgradient of the global L2 regularization term is also combined in the problem through wk+14 . The Lagrangian dual of Equation 12 is arg min τe∗�e′ 2∥ 1 ∑ </context>
<context position="24989" citStr="Crammer et al., 2006" startWordPosition="4268" endWordPosition="4271">2007). Slower training incurred by the larger batch size 259 MT06 MT08 batch size 1 4 8 16 1 4 8 16 MIRA-L 31.281 31.531 31.631 31.421 23.46 23.971 24.58 24.151 ORO-Lhinge 31.321 30.69 29.61 29.76 23.63 23.12 22.07 21.96 O-ORO-Lhinge 31.441 31.541 31.351 32.06 23.72 24.021 24.281 24.95 ORO-Lsoftmax 25.10 31.661 31.311 30.77 19.27 23.59 23.50 23.07 O-ORO-Lsoftmax 31.151 31.171 30.90 31.161 23.62 23.31 23.03 23.20 Table 3: Translation results with varied batch size. for more accurate corpus-BLEU is addressed by optimally scaling parameter updates in the spirit of a passive-aggressive algorithm (Crammer et al., 2006). The derived algorithm is very similar to MIRA, but differs in that the learning rate is employed as a hyperparameter for controlling the fitness to training data which decays when training proceeds. The non-uniform sub-gradient based update is also employed in an exponentiated gradient (EG) algorithm (Kivinen and Warmuth, 1997; Kivinen and Warmuth, 2001) in which parameter updates are maximum-likely estimated using an exponentially combined sub-gradients. In contrast, our approach relies on an ultraconservative update which tradeoff between the amount of updates performed to the parameters a</context>
<context position="26585" citStr="Crammer et al., 2006" startWordPosition="4519" endWordPosition="4523">ple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in (Och, 2003). 7 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of (Hopkins and May, 2011). Training is performed by SGD with a parameter projection (Shalev-Shwartz et al., 2007) using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggressive algorithm (Crammer et al., 2006). Learning is efficiently parallelized (McDonald et al., 2010) and the locally learned parameters are mixed by an additional line search step. Experiments indicate that better performance was achieved by our optimized updates and by the more sophisticated parameter mixing. In future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin (Taskar et al., 2004), risk (Smith and Eisner, 2006) or softmax-loss (Gimpel and Smith, 2010), and different regularizers, such as Li-norm for a sparse solution. Instead of n-best approximations, we may directly empl</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Jonathan H Clark</author>
<author>Noah A Smith</author>
</authors>
<title>The cmu-ark german-english translation system.</title>
<date>2011</date>
<booktitle>In Proc. of SMT 2011,</booktitle>
<pages>337--343</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="16933" citStr="Dyer et al., 2011" startWordPosition="2926" endWordPosition="2929">sting with all data consisting of four reference translations. We use an in-house developed hypergraph-based toolkit for training and decoding with synchronousCFGs (SCFG) for hierarchical phrase-bassed SMT (Chiang, 2007). The system employs 14 features, consisting of standard Hiero-style features (Chiang, 2007), and a set of indicator features, such as the number of synchronous-rules in a derivation. Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model (Dyer et al., 2011). For faster experiments, we precomputed translation forests inspired by Xiao et al. (2011). Instead of generating forests from bitexts in each iteration, we construct and save translation forests by intersecting the source side of SCFG with input sentences an MT02 MT06 d by keeping the target side of the interarg min w ∑2∥w−wk+14∥22+ηk (f,e)Eb ξf (16) 1 257 sected rules. n-bests are generated from the precomputed forests on the fly using the forest rescoring framework (Huang and Chiang, 2007) with additional non-local features, such as 5-gram language models. We compared four algorithms, MERT</context>
</contexts>
<marker>Dyer, Gimpel, Clark, Smith, 2011</marker>
<rawString>Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and Noah A. Smith. 2011. The cmu-ark german-english translation system. In Proc. of SMT 2011, pages 337– 343, Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="19482" citStr="Fan et al., 2008" startWordPosition="3364" endWordPosition="3367">s our main results. Among the parameters from multiple iterations, we report the outputs that performed the best on MT06. With Moses (Koehn et al., 2007), we achieved 30.36 and 23.64 BLEU for MT06 and MT08, respectively. We denote the “O-” prefix for the optimized parameter updates discussed in Section 4.1, and the “-L” suffix 3The other major difference is the use of a simpler learning rate, ak , which was very slow in our preliminary studies. 4Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. 5We used liblinear (Fan et al., 2008) at http://www. csie.ntu.edu.tw/˜cjlin/liblinear with the solver type of 3. MT06 MT08 MERT 31.45† 24.13† PRO 31.76† 24.43† MIRA-L 31.42† 24.15† ORO-Lhinge 29.76 21.96 O-ORO-Lhinge 32.06 24.95 ORO-Lsoftmax 30.77 23.07 O-ORO-Lsoftmax 31.16† 23.20 Table 1: Translation results by BLEU. Results without significant differences from the MERT baseline are marked †. The numbers in boldface are significantly better than the MERT baseline (both measured by the bootstrap resampling (Koehn, 2004) with p &gt; 0.05). 0 5 10 15 20 25 30 iteration Figure 1: Learning curves for three algorithms, MIRA-L, ORO-Lhinge</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vojtˇech Franc</author>
<author>Soeren Sonnenburg</author>
</authors>
<title>Optimized cutting plane algorithm for support vector machines.</title>
<date>2008</date>
<booktitle>In Proc. of ICML ’08,</booktitle>
<pages>320--327</pages>
<location>Helsinki, Finland.</location>
<marker>Franc, Sonnenburg, 2008</marker>
<rawString>Vojtˇech Franc and Soeren Sonnenburg. 2008. Optimized cutting plane algorithm for support vector machines. In Proc. of ICML ’08, pages 320–327, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Chris Quirk</author>
</authors>
<title>Optimal search for minimum error rate training.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP 2011,</booktitle>
<pages>38--49</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1661" citStr="Galley and Quirk, 2011" startWordPosition="242" endWordPosition="246">) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), </context>
</contexts>
<marker>Galley, Quirk, 2011</marker>
<rawString>Michel Galley and Chris Quirk. 2011. Optimal search for minimum error rate training. In Proc. of EMNLP 2011, pages 38–49, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Softmaxmargin crfs: Training log-linear models with cost functions.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT 2010,</booktitle>
<pages>733--736</pages>
<location>Los Angeles, California,</location>
<marker>Gimpel, Smith, 2010</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2010. Softmaxmargin crfs: Training log-linear models with cost functions. In Proc. of NAACL-HLT 2010, pages 733– 736, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
<author>Abhishek Arun</author>
<author>Philipp Koehn</author>
</authors>
<title>Samplerank training for phrase-based machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of SMT 2011,</booktitle>
<pages>261--271</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2579" citStr="Haddow et al. (2011)" startWordPosition="393" endWordPosition="396"> rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm (Crammer et al., 2006), in which each parameter update is further rescaled considering the tradeoff between the amount of updates to the parameters and the</context>
<context position="9309" citStr="Haddow et al., 2011" startWordPosition="1576" endWordPosition="1579"> with each bt j consisting of N/K training data. For each batch b in Bt, we generate n-bests from the source sentences in b and compute oracle translations from the newly created n-bests Algorithm 2 Stochastic Gradient Descent 1: k = 1,w1 ← 0 2: for t = 1, ..., T do 3: Choose Bt = {bt1, ..., btK} from D 4: for b ∈ Bt do 5: Compute n-bests and oracles of b 6: Set learning rate ηk 7: wk+ 2 1 ← wk − ηk∇(wk; b) ▷ Our proposed algorithm solve lEq. 12 or 16 8: w k+ +— min { 1 1/,/λ } 1 l &apos; 11w 1112 JJJ Wk+2 9: k ← k + 1 10: end for 11: end for 12: return wk (line 5) using a batch local corpus-BLEU (Haddow et al., 2011). Then, we optimize an approximated objective function λ 2 ∥w∥�2 + ℓ(w; b) (7) by replacing D with b in the objective of Eq. 4. The parameters wk are updated by the sub-gradient of Equation 7, ∇(wk; b), scaled by the learning rate ηk (line 7). We use an exponential decayed learning rate ηk = η0αk/K, which converges very fast in practice (Tsuruoka et al., 2009)1. The sub-gradient of Eq.7 with the hinge loss of Eq. 5 is 1 ∑λwk − M(wk; b) ∑ Φ(f, e*, e′) (8) (f,e)Eb e*,e′ such that 1 − wTk Φ(f, e*, e′) &gt; 0. (9) We found that the normalization term by M(·) was very slow in convergence, thus, instea</context>
<context position="10573" citStr="Haddow et al. (2011)" startWordPosition="1819" endWordPosition="1822"> paired loss terms satisfied the constraints in Equation 9. In the case of the softmax loss objective of Eq. 6, the subgradient is � ∂ � ∂wL(w; f, e) (10) (f,e)Eb w=wk 1We set α = 0.85 and 770 = 0.2 which converged well in our preliminary experiments. 1 ∑ ZO(w; f, e) N log (6) (f,e)ED ZN(w;f) arg min w 1 ∑ λwk − |b| 255 where L(w; f, e) = log (ZO(w; f, e)/ZN(w; f)). After the parameter update, wk+21 is projected within the L2-norm ball (Shalev-Shwartz et al., 2007). Setting smaller batch size implies frequent updates to the parameters and a faster convergence. However, as briefly mentioned in Haddow et al. (2011), setting batch size to a smaller value, such as |b |= 1, does not work well in practice, since BLEU is devised for a corpus based evaluation, not for an individual sentence-wise evaluation, and it is not linearly decomposed into a sentence-wise score (Chiang et al., 2008a). Thus, the smaller batch size may also imply less accurate batch-local corpus-BLEU and incorrect oracle translation selections, which may lead to incorrect sub-gradient estimations or slower convergence. In the next section we propose an optimized parameter update which works well when setting a smaller batch size is imprac</context>
<context position="24070" citStr="Haddow et al., 2011" startWordPosition="4120" endWordPosition="4123">rform better for larger batch sizes with a more accurate corpusBLEU. 6 Related Work Our work is largely inspired by pairwise rank optimization (Hopkins and May, 2011), but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. Training is performed by SGD with a parameter projection method (Shalev-Shwartz et al., 2007). Slower training incurred by the larger batch size 259 MT06 MT08 batch size 1 4 8 16 1 4 8 16 MIRA-L 31.281 31.531 31.631 31.421 23.46 23.971 24.58 24.151 ORO-Lhinge 31.321 30.69 29.61 29.76 23.63 23.12 22.07 21.96 O-ORO-Lhinge 31.441 31.541 31.351 32.06 23.72 24.021 24.281 24.95 ORO-Lsoftmax 25</context>
</contexts>
<marker>Haddow, Arun, Koehn, 2011</marker>
<rawString>Barry Haddow, Abhishek Arun, and Philipp Koehn. 2011. Samplerank training for phrase-based machine translation. In Proc. of SMT 2011, pages 261–271, Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
<author>Klaus Obermayer</author>
</authors>
<title>Support vector learning for ordinal regression. In</title>
<date>1999</date>
<booktitle>In Proc. of International Conference on Artificial Neural Networks,</booktitle>
<pages>97--102</pages>
<contexts>
<context position="2295" citStr="Herbrich et al., 1999" startWordPosition="344" endWordPosition="347">native methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the </context>
</contexts>
<marker>Herbrich, Graepel, Obermayer, 1999</marker>
<rawString>Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 1999. Support vector learning for ordinal regression. In In Proc. of International Conference on Artificial Neural Networks, pages 97–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP 2011,</booktitle>
<pages>1352--1362</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2007" citStr="Hopkins and May, 2011" startWordPosition="297" endWordPosition="300">be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately comp</context>
<context position="6237" citStr="Hopkins and May, 2011" startWordPosition="1008" endWordPosition="1012">rn wT+1 mated by search over the n-bests merged across iterations. The merged n-bests are also used in the line search procedure to efficiently draw the error surface for efficient computation of the outer minimization of Eq. 3. 3 Online Rank Learning 3.1 Rank Learning Instead of the direct task loss minimization of Eq. 3, we would like to find w by solving the L2- regularized constrained minimization problem arg min w λ 2 11w1122 + ℓ(w; D) (4) where λ &gt; 0 is a hyperparameter controlling the fitness to the data. The loss function ℓ(·) we consider here is inspired by a pairwise ranking method (Hopkins and May, 2011) in which pairs of correct translation and incorrect translation are sampled from nbests and suffer a hinge loss M(w; D) (f,e)ED e*,e′ 1 � � { } max 0,1 − wTb(f,e*,e′) (5) where e′ E NBEST(w; f) \ ORACLE(w; f, e) e* E ORACLE(w; f, e) b(f, e*, e′) = h(f, e*) − h(f, e′). NBEST(·) is the n-best translations of f generated with the parameter w, and ORACLE(·) is a set of oracle translations chosen among NBEST(·). Note that each e′ (and e*) implicitly represents a derivation consisting of a tuple (e′, ϕ), where ϕ is a latent structure, i.e. phrases in a phrase-based SMT, but we omit ϕ for brevity. M</context>
<context position="8346" citStr="Hopkins and May (2011)" startWordPosition="1376" endWordPosition="1379">ent parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning (Taskar et al., 2004). As an alternative, we would also consider a softmax loss (Collins and Koo, 2005) represented by where ZO(w; f, e) = ∑e*EORACLE(w;f,e) exp(wTf(f, e*)) ZN(w; f) = ∑e′ENBEST(w;f) exp(wTf(f, e′)). Equation 6 is a log-linear model used in common NLP tasks such as tagging, chunking and named entity recognition, but differ slightly in that multiple correct translations are discriminated from the others (Charniak and Johnson, 2005). 3.2 Online Approximation Hopkins and May (2011) applied a MERT-like procedure in Alg. 1 in which Equation 4 was solved to obtain new parameters in each iteration. Here, we employ stochastic gradient descent (SGD) methods as presented in Algorithm 2 motivated by Pegasos (Shalev-Shwartz et al., 2007). In each iteration, we randomly permute D and choose a set of batches Bt = {bt1, ..., btK} with each bt j consisting of N/K training data. For each batch b in Bt, we generate n-bests from the source sentences in b and compute oracle translations from the newly created n-bests Algorithm 2 Stochastic Gradient Descent 1: k = 1,w1 ← 0 2: for t = 1, </context>
<context position="19337" citStr="Hopkins and May (2011)" startWordPosition="3341" endWordPosition="3344">rs, and PRO took one day. It took roughly 3 days for MERT with 20 iterations. Translation results are measured by case sensitive BLEU. Table 1 presents our main results. Among the parameters from multiple iterations, we report the outputs that performed the best on MT06. With Moses (Koehn et al., 2007), we achieved 30.36 and 23.64 BLEU for MT06 and MT08, respectively. We denote the “O-” prefix for the optimized parameter updates discussed in Section 4.1, and the “-L” suffix 3The other major difference is the use of a simpler learning rate, ak , which was very slow in our preliminary studies. 4Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. 5We used liblinear (Fan et al., 2008) at http://www. csie.ntu.edu.tw/˜cjlin/liblinear with the solver type of 3. MT06 MT08 MERT 31.45† 24.13† PRO 31.76† 24.43† MIRA-L 31.42† 24.15† ORO-Lhinge 29.76 21.96 O-ORO-Lhinge 32.06 24.95 ORO-Lsoftmax 30.77 23.07 O-ORO-Lsoftmax 31.16† 23.20 Table 1: Translation results by BLEU. Results without significant differences from the MERT baseline are marked †. The numbers in boldface are significantly better than the MERT baseline (both measured by the b</context>
<context position="23616" citStr="Hopkins and May, 2011" startWordPosition="4048" endWordPosition="4051">nt batch size. ing of the learning rate parameter will be required for a larger batch size. As discussed in Section 3, the smaller batch size means frequent updates to parameters and a faster convergence, but potentially leads to a poor performance since the corpus-BLEU is approximately computed in a local batch. Our optimized update algorithms address the problem by adjusting the tradeoff between the amount of update to parameters and the loss, and perform better for larger batch sizes with a more accurate corpusBLEU. 6 Related Work Our work is largely inspired by pairwise rank optimization (Hopkins and May, 2011), but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006;</context>
<context position="26285" citStr="Hopkins and May, 2011" startWordPosition="4473" endWordPosition="4476"> a complex parallelization by Chiang et al. (2008b), in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration (McDonald et al., 2010). Rather than simple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in (Och, 2003). 7 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of (Hopkins and May, 2011). Training is performed by SGD with a parameter projection (Shalev-Shwartz et al., 2007) using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggressive algorithm (Crammer et al., 2006). Learning is efficiently parallelized (McDonald et al., 2010) and the locally learned parameters are mixed by an additional line search step. Experiments indicate that better performance was achieved by our optimized updates and by the more sophisticated parameter mixing. In future work, we would l</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proc. of EMNLP 2011, pages 1352–1362, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cho-Jui Hsieh</author>
<author>Kai-Wei Chang</author>
<author>Chih-Jen Lin</author>
<author>S Sathiya Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear svm.</title>
<date>2008</date>
<booktitle>In Proc. of ICML ’08,</booktitle>
<pages>408--415</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="12834" citStr="Hsieh et al., 2008" startWordPosition="2216" endWordPosition="2219"> − W +1 -b(f, e∗, e′) } (14) ll 4 τe∗,e′ ≤ ηk. (f,e)∈b,e∗,e′ subject to ∑ (f,e)∈b,e∗,e′ ∑ ηk wk+2 1 ← (1−ληk)wk+ M(wk; b)4b(f, e∗, e′) (f,e)∈b,e∗,e′ (11) in which each individual loss term 4b(·) is scaled uniformly by a constant ηk/M(·). Instead of the uniform scaling, we propose to update the parameters in two steps: First, we suffer the sub-gradient from the L2 regularization wk+14 ←(1 − ληk)wk. Second, we solve the following problem arg min 1 ∑ 2∥w−wk+1 4 ∥22+ηk ξf,e∗,e′ (12) w (f,e)∈b,e∗,e′ such that w⊤4b(f, e∗, e′) ≥ 1 − ξf,e∗,e′ ξf,e∗,e′ ≥ �. We used a dual coordinate descent algorithm (Hsieh et al., 2008)2 to efficiently solve the quadratic program (QP) in Equation 14, leading to an update When compared with Equation 11, the update procedure in Equation 15 rescales the contribution from each sub-gradient through the Lagrange multipliers τe∗,e′. Note that if we set τe∗,e′ = ηk/M(·), we satisfy the constraints in Eq. 14, and recover the update in Eq. 11. In the same manner as Eq. 12, we derive an optimized update procedure for the softmax loss, which replaces the update with Equation 10, by solving the 2Specifically, each parameter is bound constrained 0 &lt; T &lt; rlk but is not summation constraine</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear svm. In Proc. of ICML ’08, pages 408–415, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17431" citStr="Huang and Chiang, 2007" startWordPosition="3011" endWordPosition="3014">e other from English Gigaword, with features counting the number of out-of-vocabulary words in each model (Dyer et al., 2011). For faster experiments, we precomputed translation forests inspired by Xiao et al. (2011). Instead of generating forests from bitexts in each iteration, we construct and save translation forests by intersecting the source side of SCFG with input sentences an MT02 MT06 d by keeping the target side of the interarg min w ∑2∥w−wk+14∥22+ηk (f,e)Eb ξf (16) 1 257 sected rules. n-bests are generated from the precomputed forests on the fly using the forest rescoring framework (Huang and Chiang, 2007) with additional non-local features, such as 5-gram language models. We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO without our optimization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with varied objectives, hinge loss or softmax loss3. MERT learns parameters from forests (Kumar et al., 2009) with 4 restarts and 8 random directions in each iteration. We experimented on a variant of PRO4, in which the objective in Eq. 4 with the h</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. of ACL 2007, pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jyrki Kivinen</author>
<author>Manfred K Warmuth</author>
</authors>
<title>Exponentiated gradient versus gradient descent for linear predictors.</title>
<date>1997</date>
<journal>Information and Computation,</journal>
<volume>132</volume>
<issue>1</issue>
<pages>63</pages>
<contexts>
<context position="25319" citStr="Kivinen and Warmuth, 1997" startWordPosition="4321" endWordPosition="4324">19.27 23.59 23.50 23.07 O-ORO-Lsoftmax 31.151 31.171 30.90 31.161 23.62 23.31 23.03 23.20 Table 3: Translation results with varied batch size. for more accurate corpus-BLEU is addressed by optimally scaling parameter updates in the spirit of a passive-aggressive algorithm (Crammer et al., 2006). The derived algorithm is very similar to MIRA, but differs in that the learning rate is employed as a hyperparameter for controlling the fitness to training data which decays when training proceeds. The non-uniform sub-gradient based update is also employed in an exponentiated gradient (EG) algorithm (Kivinen and Warmuth, 1997; Kivinen and Warmuth, 2001) in which parameter updates are maximum-likely estimated using an exponentially combined sub-gradients. In contrast, our approach relies on an ultraconservative update which tradeoff between the amount of updates performed to the parameters and the progress made for the objectives by solving a QP subproblem. Unlike a complex parallelization by Chiang et al. (2008b), in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each itera</context>
</contexts>
<marker>Kivinen, Warmuth, 1997</marker>
<rawString>Jyrki Kivinen and Manfred K. Warmuth. 1997. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1– 63, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kivinen</author>
<author>M K Warmuth</author>
</authors>
<title>Relative loss bounds for multidimensional regression problems.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="25347" citStr="Kivinen and Warmuth, 2001" startWordPosition="4325" endWordPosition="4329">RO-Lsoftmax 31.151 31.171 30.90 31.161 23.62 23.31 23.03 23.20 Table 3: Translation results with varied batch size. for more accurate corpus-BLEU is addressed by optimally scaling parameter updates in the spirit of a passive-aggressive algorithm (Crammer et al., 2006). The derived algorithm is very similar to MIRA, but differs in that the learning rate is employed as a hyperparameter for controlling the fitness to training data which decays when training proceeds. The non-uniform sub-gradient based update is also employed in an exponentiated gradient (EG) algorithm (Kivinen and Warmuth, 1997; Kivinen and Warmuth, 2001) in which parameter updates are maximum-likely estimated using an exponentially combined sub-gradients. In contrast, our approach relies on an ultraconservative update which tradeoff between the amount of updates performed to the parameters and the progress made for the objectives by solving a QP subproblem. Unlike a complex parallelization by Chiang et al. (2008b), in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration (McDonald et al., 2010)</context>
</contexts>
<marker>Kivinen, Warmuth, 2001</marker>
<rawString>J. Kivinen and M. K. Warmuth. 2001. Relative loss bounds for multidimensional regression problems. Machine Learning, 45(3):301–329, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Procc of ACL</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="19018" citStr="Koehn et al., 2007" startWordPosition="3284" endWordPosition="3287">d 1,000-best translations in each iteration. The hyperparameter A for PRO and ORO was set to 10−5, selected from among {10−3,10−4,10−5}, and 102 for MIRA, chosen from {10,102,103} by preliminary testing on MT06. Both decoding and learning are parallelized and run on 8 cores. Each online learning took roughly 12 hours, and PRO took one day. It took roughly 3 days for MERT with 20 iterations. Translation results are measured by case sensitive BLEU. Table 1 presents our main results. Among the parameters from multiple iterations, we report the outputs that performed the best on MT06. With Moses (Koehn et al., 2007), we achieved 30.36 and 23.64 BLEU for MT06 and MT08, respectively. We denote the “O-” prefix for the optimized parameter updates discussed in Section 4.1, and the “-L” suffix 3The other major difference is the use of a simpler learning rate, ak , which was very slow in our preliminary studies. 4Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. 5We used liblinear (Fan et al., 2008) at http://www. csie.ntu.edu.tw/˜cjlin/liblinear with the solver type of 3. MT06 MT08 MERT 31.45† 24.13† PRO 31.76† 24.43† MIRA-L 31.42†</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Procc of ACL 2007, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="19970" citStr="Koehn, 2004" startWordPosition="3437" endWordPosition="3438">oss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. 5We used liblinear (Fan et al., 2008) at http://www. csie.ntu.edu.tw/˜cjlin/liblinear with the solver type of 3. MT06 MT08 MERT 31.45† 24.13† PRO 31.76† 24.43† MIRA-L 31.42† 24.15† ORO-Lhinge 29.76 21.96 O-ORO-Lhinge 32.06 24.95 ORO-Lsoftmax 30.77 23.07 O-ORO-Lsoftmax 31.16† 23.20 Table 1: Translation results by BLEU. Results without significant differences from the MERT baseline are marked †. The numbers in boldface are significantly better than the MERT baseline (both measured by the bootstrap resampling (Koehn, 2004) with p &gt; 0.05). 0 5 10 15 20 25 30 iteration Figure 1: Learning curves for three algorithms, MIRA-L, ORO-Lhinge and O-ORO-Lhinge. for parameter mixing by line search as described in Section 4.2. The batch size was set to 16 for MIRA and ORO. In general, our PRO and MIRA settings achieved the results very comparable to MERT. The hinge-loss and softmax objective OROs were lower than those of the three baselines. The softmax objective with the optimized update (O-ORO-Lsoftmax) performed better than the non-optimized version, but it was still lower than our baselines. In the case of the hinge-los</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP</booktitle>
<pages>163--171</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="17891" citStr="Kumar et al., 2009" startWordPosition="3084" endWordPosition="3087">(f,e)Eb ξf (16) 1 257 sected rules. n-bests are generated from the precomputed forests on the fly using the forest rescoring framework (Huang and Chiang, 2007) with additional non-local features, such as 5-gram language models. We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO without our optimization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with varied objectives, hinge loss or softmax loss3. MERT learns parameters from forests (Kumar et al., 2009) with 4 restarts and 8 random directions in each iteration. We experimented on a variant of PRO4, in which the objective in Eq. 4 with the hinge loss of Eq. 5 was solved in each iteration in line 4 of Alg. 1 using an off-the-shelf solver5. Our MIRA solves the problem in Equation 13 in line 7 of Alg. 2. For a systematic comparison, we used our exhaustive oracle translation selection method in Section 3 for PRO, MIRA and ORO. For each learning algorithm, we ran 30 iterations and generated duplicate removed 1,000-best translations in each iteration. The hyperparameter A for PRO and ORO was set to</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proc. of ACL-IJCNLP 2009, pages 163–171, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>40--51</pages>
<location>Singapore,</location>
<contexts>
<context position="2259" citStr="Li and Eisner, 2009" startWordPosition="338" endWordPosition="341">alley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the mo</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proc. of EMNLP 2009, pages 40–51, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<pages>761--768</pages>
<location>Sydney, Australia,</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of COLING/ACL 2006, pages 761–768, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT 2010,</booktitle>
<pages>456--464</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="3334" citStr="McDonald et al., 2010" startWordPosition="512" endWordPosition="515">2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm (Crammer et al., 2006), in which each parameter update is further rescaled considering the tradeoff between the amount of updates to the parameters and the ranking loss. Learning is efficiently parallelized by splitting training data among shards and by merging parameters in each round (McDonald et al., 2010). Instead 253 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 253–262, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics of simple averaging, we perform an additional line search step to find the optimal merging across parallel jobs. Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task. We found significant gains over traditional MERT and other tuning algorithms, such as MIRA and PRO. 2 Statistical Machine Translation SMT can be formulated as a maximizat</context>
<context position="14390" citStr="McDonald et al. (2010)" startWordPosition="2505" endWordPosition="2508">he original objective of Eq. 7 with the softmax loss in Eq. 6 is approximated by |b |linear constraints derived from the sub-gradients at point wk (Teo et al., 2010). Eq. 16 is efficiently solved by its Lagrange dual, leading to an update ∑ τfIF(wk; f, e) (17) wk+ 1 ← wk+ 1 + 2 4 (f,e)Eb subject to ∑(f,e)Eb τf ≤ ηk. Similar to Eq. 15, the parameter update by IF(·) is rescaled by its Lagrange multipliers τf in place of the uniform scale of 1/|b| in the sub-gradient of Eq. 10. 4.2 Line Search for Parameter Mixing For faster training, we employ an efficient parallel training strategy proposed by McDonald et al. (2010). The training data D is split into S disjoint shards, {D1, ..., DS}. Each shard learns its own parameters in each single epoch t and performs parameter mixing by averaging parameters across shards. We propose an optimized parallel training in Algorithm 3 which performs better mixing with respect to the task loss, i.e. negative BLEU. In line 5, wt+21 is computed by averaging wt+1,s from all Then, the new parameters wt+1 are obtained by linearly interpolating with the parameters from the previous epoch wt. The linear interpolation weight is efficiently computed by a line search procedure which </context>
<context position="25947" citStr="McDonald et al., 2010" startWordPosition="4420" endWordPosition="4423">nen and Warmuth, 2001) in which parameter updates are maximum-likely estimated using an exponentially combined sub-gradients. In contrast, our approach relies on an ultraconservative update which tradeoff between the amount of updates performed to the parameters and the progress made for the objectives by solving a QP subproblem. Unlike a complex parallelization by Chiang et al. (2008b), in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration (McDonald et al., 2010). Rather than simple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in (Och, 2003). 7 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of (Hopkins and May, 2011). Training is performed by SGD with a parameter projection (Shalev-Shwartz et al., 2007) using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggre</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Proc. of NAACL-HLT 2010, pages 456– 464, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Random restarts in minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>585--592</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="1541" citStr="Moore and Quirk, 2008" startWordPosition="227" endWordPosition="230">dicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004),</context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>Robert C. Moore and Chris Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In Proc. of COLING 2008, pages 585–592, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>295--302</pages>
<location>Philadelphia,</location>
<contexts>
<context position="1836" citStr="Och and Ney, 2002" startWordPosition="270" endWordPosition="273">e the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). </context>
<context position="4274" citStr="Och and Ney, 2002" startWordPosition="665" endWordPosition="668">erging across parallel jobs. Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task. We found significant gains over traditional MERT and other tuning algorithms, such as MIRA and PRO. 2 Statistical Machine Translation SMT can be formulated as a maximization problem of finding the most likely translation e given an input sentence f using a set of parameters θ (Brown et al., 1993) e� = arg max p(e|f; θ). (1) e Under this maximization setting, we assume that p(·) is represented by a linear combination of feature functions h(f, e) which are scaled by a set of parameters w (Och and Ney, 2002) e� = arg max wTh(f, e). (2) e Each element of h(·) is a feature function which captures different aspects of translations, for instance, log of n-gram language model probability, the number of translated words or log of phrasal probability. In this paper, we concentrate on the problem of learning w, which is referred to as tuning. One of the standard methods for parameter tuning is minimum error rate training (Och, 2003) (MERT) which directly minimizes the task loss ℓ(·), i.e. negative BLEU (Papineni et al., 2002), given training data D = {(f1, e1), ..., (fN, eN)}, sets of paired source sente</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL 2002, pages 295–302, Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1195" citStr="Och, 2003" startWordPosition="172" endWordPosition="173">ose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., </context>
<context position="4699" citStr="Och, 2003" startWordPosition="742" endWordPosition="743"> e Under this maximization setting, we assume that p(·) is represented by a linear combination of feature functions h(f, e) which are scaled by a set of parameters w (Och and Ney, 2002) e� = arg max wTh(f, e). (2) e Each element of h(·) is a feature function which captures different aspects of translations, for instance, log of n-gram language model probability, the number of translated words or log of phrasal probability. In this paper, we concentrate on the problem of learning w, which is referred to as tuning. One of the standard methods for parameter tuning is minimum error rate training (Och, 2003) (MERT) which directly minimizes the task loss ℓ(·), i.e. negative BLEU (Papineni et al., 2002), given training data D = {(f1, e1), ..., (fN, eN)}, sets of paired source sentence fi and its reference translations ei 1i=1N ℓ(5 arg maxwTh(fi,e),{ei}N1).e (3) The objective in Equation 3 is discontinuous and non-convex, and it requires decoding of all the training data given w. Therefore, MERT relies on a derivative-free unconstrained optimization method, such as Powell’s method, which repeatedly chooses one direction to optimize using a line search procedure as in Algorithm 1. Expensive decoding </context>
<context position="26151" citStr="Och, 2003" startWordPosition="4452" endWordPosition="4453"> amount of updates performed to the parameters and the progress made for the objectives by solving a QP subproblem. Unlike a complex parallelization by Chiang et al. (2008b), in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration (McDonald et al., 2010). Rather than simple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in (Och, 2003). 7 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of (Hopkins and May, 2011). Training is performed by SGD with a parameter projection (Shalev-Shwartz et al., 2007) using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggressive algorithm (Crammer et al., 2006). Learning is efficiently parallelized (McDonald et al., 2010) and the locally learned parameters are mixed by an additional line search step. Experiments indicate th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="1299" citStr="Papineni et al., 2002" startWordPosition="186" endWordPosition="189">n is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA</context>
<context position="4794" citStr="Papineni et al., 2002" startWordPosition="755" endWordPosition="758">combination of feature functions h(f, e) which are scaled by a set of parameters w (Och and Ney, 2002) e� = arg max wTh(f, e). (2) e Each element of h(·) is a feature function which captures different aspects of translations, for instance, log of n-gram language model probability, the number of translated words or log of phrasal probability. In this paper, we concentrate on the problem of learning w, which is referred to as tuning. One of the standard methods for parameter tuning is minimum error rate training (Och, 2003) (MERT) which directly minimizes the task loss ℓ(·), i.e. negative BLEU (Papineni et al., 2002), given training data D = {(f1, e1), ..., (fN, eN)}, sets of paired source sentence fi and its reference translations ei 1i=1N ℓ(5 arg maxwTh(fi,e),{ei}N1).e (3) The objective in Equation 3 is discontinuous and non-convex, and it requires decoding of all the training data given w. Therefore, MERT relies on a derivative-free unconstrained optimization method, such as Powell’s method, which repeatedly chooses one direction to optimize using a line search procedure as in Algorithm 1. Expensive decoding is approximated by an n-best merging technique in which decoding is carried out in each epoch o</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL 2002, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Expected bleu training for graphs: Bbn system description for wmt11 system combination task.</title>
<date>2011</date>
<booktitle>In Proc. of SMT 2011,</booktitle>
<pages>159--165</pages>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="24236" citStr="Rosti et al., 2011" startWordPosition="4150" endWordPosition="4153"> but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. Training is performed by SGD with a parameter projection method (Shalev-Shwartz et al., 2007). Slower training incurred by the larger batch size 259 MT06 MT08 batch size 1 4 8 16 1 4 8 16 MIRA-L 31.281 31.531 31.631 31.421 23.46 23.971 24.58 24.151 ORO-Lhinge 31.321 30.69 29.61 29.76 23.63 23.12 22.07 21.96 O-ORO-Lhinge 31.441 31.541 31.351 32.06 23.72 24.021 24.281 24.95 ORO-Lsoftmax 25.10 31.661 31.311 30.77 19.27 23.59 23.50 23.07 O-ORO-Lsoftmax 31.151 31.171 30.90 31.161 23.62 23.31 23.03 23.20 Table 3: Translation results with varied batch size.</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2011</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2011. Expected bleu training for graphs: Bbn system description for wmt11 system combination task. In Proc. of SMT 2011, pages 159– 165, Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Nathan Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proc. of ICML ’07,</booktitle>
<pages>807--814</pages>
<location>Corvalis, Oregon.</location>
<contexts>
<context position="2434" citStr="Shalev-Shwartz et al., 2007" startWordPosition="365" endWordPosition="368">mum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm (Crammer e</context>
<context position="8598" citStr="Shalev-Shwartz et al., 2007" startWordPosition="1419" endWordPosition="1422">e ZO(w; f, e) = ∑e*EORACLE(w;f,e) exp(wTf(f, e*)) ZN(w; f) = ∑e′ENBEST(w;f) exp(wTf(f, e′)). Equation 6 is a log-linear model used in common NLP tasks such as tagging, chunking and named entity recognition, but differ slightly in that multiple correct translations are discriminated from the others (Charniak and Johnson, 2005). 3.2 Online Approximation Hopkins and May (2011) applied a MERT-like procedure in Alg. 1 in which Equation 4 was solved to obtain new parameters in each iteration. Here, we employ stochastic gradient descent (SGD) methods as presented in Algorithm 2 motivated by Pegasos (Shalev-Shwartz et al., 2007). In each iteration, we randomly permute D and choose a set of batches Bt = {bt1, ..., btK} with each bt j consisting of N/K training data. For each batch b in Bt, we generate n-bests from the source sentences in b and compute oracle translations from the newly created n-bests Algorithm 2 Stochastic Gradient Descent 1: k = 1,w1 ← 0 2: for t = 1, ..., T do 3: Choose Bt = {bt1, ..., btK} from D 4: for b ∈ Bt do 5: Compute n-bests and oracles of b 6: Set learning rate ηk 7: wk+ 2 1 ← wk − ηk∇(wk; b) ▷ Our proposed algorithm solve lEq. 12 or 16 8: w k+ +— min { 1 1/,/λ } 1 l &apos; 11w 1112 JJJ Wk+2 9:</context>
<context position="10422" citStr="Shalev-Shwartz et al., 2007" startWordPosition="1795" endWordPosition="1798">− wTk Φ(f, e*, e′) &gt; 0. (9) We found that the normalization term by M(·) was very slow in convergence, thus, instead, we used M′(w; b), which was the number of paired loss terms satisfied the constraints in Equation 9. In the case of the softmax loss objective of Eq. 6, the subgradient is � ∂ � ∂wL(w; f, e) (10) (f,e)Eb w=wk 1We set α = 0.85 and 770 = 0.2 which converged well in our preliminary experiments. 1 ∑ ZO(w; f, e) N log (6) (f,e)ED ZN(w;f) arg min w 1 ∑ λwk − |b| 255 where L(w; f, e) = log (ZO(w; f, e)/ZN(w; f)). After the parameter update, wk+21 is projected within the L2-norm ball (Shalev-Shwartz et al., 2007). Setting smaller batch size implies frequent updates to the parameters and a faster convergence. However, as briefly mentioned in Haddow et al. (2011), setting batch size to a smaller value, such as |b |= 1, does not work well in practice, since BLEU is devised for a corpus based evaluation, not for an individual sentence-wise evaluation, and it is not linearly decomposed into a sentence-wise score (Chiang et al., 2008a). Thus, the smaller batch size may also imply less accurate batch-local corpus-BLEU and incorrect oracle translation selections, which may lead to incorrect sub-gradient estim</context>
<context position="24373" citStr="Shalev-Shwartz et al., 2007" startWordPosition="4172" endWordPosition="4175">BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. Training is performed by SGD with a parameter projection method (Shalev-Shwartz et al., 2007). Slower training incurred by the larger batch size 259 MT06 MT08 batch size 1 4 8 16 1 4 8 16 MIRA-L 31.281 31.531 31.631 31.421 23.46 23.971 24.58 24.151 ORO-Lhinge 31.321 30.69 29.61 29.76 23.63 23.12 22.07 21.96 O-ORO-Lhinge 31.441 31.541 31.351 32.06 23.72 24.021 24.281 24.95 ORO-Lsoftmax 25.10 31.661 31.311 30.77 19.27 23.59 23.50 23.07 O-ORO-Lsoftmax 31.151 31.171 30.90 31.161 23.62 23.31 23.03 23.20 Table 3: Translation results with varied batch size. for more accurate corpus-BLEU is addressed by optimally scaling parameter updates in the spirit of a passive-aggressive algorithm (Cramm</context>
<context position="26373" citStr="Shalev-Shwartz et al., 2007" startWordPosition="4486" endWordPosition="4489">asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration (McDonald et al., 2010). Rather than simple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in (Och, 2003). 7 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of (Hopkins and May, 2011). Training is performed by SGD with a parameter projection (Shalev-Shwartz et al., 2007) using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggressive algorithm (Crammer et al., 2006). Learning is efficiently parallelized (McDonald et al., 2010) and the locally learned parameters are mixed by an additional line search step. Experiments indicate that better performance was achieved by our optimized updates and by the more sophisticated parameter mixing. In future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin (Tas</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proc. of ICML ’07, pages 807–814, Corvalis, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<pages>787--794</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2237" citStr="Smith and Eisner, 2006" startWordPosition="334" endWordPosition="337">natorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger bat</context>
<context position="24215" citStr="Smith and Eisner, 2006" startWordPosition="4146" endWordPosition="4149">(Hopkins and May, 2011), but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. Training is performed by SGD with a parameter projection method (Shalev-Shwartz et al., 2007). Slower training incurred by the larger batch size 259 MT06 MT08 batch size 1 4 8 16 1 4 8 16 MIRA-L 31.281 31.531 31.631 31.421 23.46 23.971 24.58 24.151 ORO-Lhinge 31.321 30.69 29.61 29.76 23.63 23.12 22.07 21.96 O-ORO-Lhinge 31.441 31.541 31.351 32.06 23.72 24.021 24.281 24.95 ORO-Lsoftmax 25.10 31.661 31.311 30.77 19.27 23.59 23.50 23.07 O-ORO-Lsoftmax 31.151 31.171 30.90 31.161 23.62 23.31 23.03 23.20 Table 3: Translation results wi</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proc. of COLING/ACL 2006, pages 787–794, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Mike Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP 2004,</booktitle>
<pages>1--8</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2140" citStr="Taskar et al., 2004" startWordPosition="319" endWordPosition="322">oore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for</context>
<context position="7868" citStr="Taskar et al., 2004" startWordPosition="1300" endWordPosition="1303">oracle translation and one other translation in the nbests other than those in ORACLE(·). Oracle translations are selected by minimizing the task loss, ℓ({e′ ∈ NBEST(w; fi)}Ni=1 , {ei}Ni=1) i.e. negative BLEU, with respect to a set of reference translations e. In order to compute oracles with corpus-BLEU, we apply a greedy search strategy over n-bests (Venugopal, 2005). Equation 5 can be easily interpreted as a constant loss “1” for choosing a wrong translation under current parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning (Taskar et al., 2004). As an alternative, we would also consider a softmax loss (Collins and Koo, 2005) represented by where ZO(w; f, e) = ∑e*EORACLE(w;f,e) exp(wTf(f, e*)) ZN(w; f) = ∑e′ENBEST(w;f) exp(wTf(f, e′)). Equation 6 is a log-linear model used in common NLP tasks such as tagging, chunking and named entity recognition, but differ slightly in that multiple correct translations are discriminated from the others (Charniak and Johnson, 2005). 3.2 Online Approximation Hopkins and May (2011) applied a MERT-like procedure in Alg. 1 in which Equation 4 was solved to obtain new parameters in each iteration. Here, </context>
<context position="26990" citStr="Taskar et al., 2004" startWordPosition="4584" endWordPosition="4587">07) using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggressive algorithm (Crammer et al., 2006). Learning is efficiently parallelized (McDonald et al., 2010) and the locally learned parameters are mixed by an additional line search step. Experiments indicate that better performance was achieved by our optimized updates and by the more sophisticated parameter mixing. In future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin (Taskar et al., 2004), risk (Smith and Eisner, 2006) or softmax-loss (Gimpel and Smith, 2010), and different regularizers, such as Li-norm for a sparse solution. Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner, 2009). We would also like to explore other mixing strategies for parallel training which can directly minimize the training objectives like those proposed for a cutting-plane algorithm (Franc and Sonnenburg, 2008). Acknowledgments We would like to thank anonymous reviewers and our colleagues for helpful comments and discussio</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In Proc. of EMNLP 2004, pages 1–8, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Choon Hui Teo</author>
<author>S V N Vishwanthan</author>
<author>Alex J Smola</author>
<author>Quoc V Le</author>
</authors>
<title>Bundle methods for regularized risk minimization.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--311</pages>
<contexts>
<context position="13933" citStr="Teo et al., 2010" startWordPosition="2419" endWordPosition="2422">tion 10, by solving the 2Specifically, each parameter is bound constrained 0 &lt; T &lt; rlk but is not summation constrained ∑ T &lt; 77k. Thus, we renormalize T after optimization. ∑ wk+1← wk+1 + 2 4 τe∗,e′4b(f, e∗, e′). (15) (f,e)∈b,e∗,e′ 256 following problem such that w⊤IF(wk; f, e) ≥ −L(wk; f, e) − ξf ξf ≥ 0 in which IF(w′; f, e)= awL(w; f, e) Iw=w′. Equation 16 can be interpreted as a cutting-plane approximation for the objective of Eq. 7, in which the original objective of Eq. 7 with the softmax loss in Eq. 6 is approximated by |b |linear constraints derived from the sub-gradients at point wk (Teo et al., 2010). Eq. 16 is efficiently solved by its Lagrange dual, leading to an update ∑ τfIF(wk; f, e) (17) wk+ 1 ← wk+ 1 + 2 4 (f,e)Eb subject to ∑(f,e)Eb τf ≤ ηk. Similar to Eq. 15, the parameter update by IF(·) is rescaled by its Lagrange multipliers τf in place of the uniform scale of 1/|b| in the sub-gradient of Eq. 10. 4.2 Line Search for Parameter Mixing For faster training, we employ an efficient parallel training strategy proposed by McDonald et al. (2010). The training data D is split into S disjoint shards, {D1, ..., DS}. Each shard learns its own parameters in each single epoch t and performs </context>
</contexts>
<marker>Teo, Vishwanthan, Smola, Le, 2010</marker>
<rawString>Choon Hui Teo, S.V.N. Vishwanthan, Alex J. Smola, and Quoc V. Le. 2010. Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 11:311–365, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP 2009,</booktitle>
<pages>477--485</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="9671" citStr="Tsuruoka et al., 2009" startWordPosition="1643" endWordPosition="1646"> Set learning rate ηk 7: wk+ 2 1 ← wk − ηk∇(wk; b) ▷ Our proposed algorithm solve lEq. 12 or 16 8: w k+ +— min { 1 1/,/λ } 1 l &apos; 11w 1112 JJJ Wk+2 9: k ← k + 1 10: end for 11: end for 12: return wk (line 5) using a batch local corpus-BLEU (Haddow et al., 2011). Then, we optimize an approximated objective function λ 2 ∥w∥�2 + ℓ(w; b) (7) by replacing D with b in the objective of Eq. 4. The parameters wk are updated by the sub-gradient of Equation 7, ∇(wk; b), scaled by the learning rate ηk (line 7). We use an exponential decayed learning rate ηk = η0αk/K, which converges very fast in practice (Tsuruoka et al., 2009)1. The sub-gradient of Eq.7 with the hinge loss of Eq. 5 is 1 ∑λwk − M(wk; b) ∑ Φ(f, e*, e′) (8) (f,e)Eb e*,e′ such that 1 − wTk Φ(f, e*, e′) &gt; 0. (9) We found that the normalization term by M(·) was very slow in convergence, thus, instead, we used M′(w; b), which was the number of paired loss terms satisfied the constraints in Equation 9. In the case of the softmax loss objective of Eq. 6, the subgradient is � ∂ � ∂wL(w; f, e) (10) (f,e)Eb w=wk 1We set α = 0.85 and 770 = 0.2 which converged well in our preliminary experiments. 1 ∑ ZO(w; f, e) N log (6) (f,e)ED ZN(w;f) arg min w 1 ∑ λwk − |b| </context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proc. of ACL-IJCNLP 2009, pages 477– 485, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
</authors>
<title>Considerations in maximum mutual information and minimum classification error training for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of EAMT-05,</booktitle>
<pages>3031</pages>
<contexts>
<context position="7619" citStr="Venugopal, 2005" startWordPosition="1261" endWordPosition="1262">ll possible translations, we follow the convention of approximating the domain of translation by n-bests. Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the nbests other than those in ORACLE(·). Oracle translations are selected by minimizing the task loss, ℓ({e′ ∈ NBEST(w; fi)}Ni=1 , {ei}Ni=1) i.e. negative BLEU, with respect to a set of reference translations e. In order to compute oracles with corpus-BLEU, we apply a greedy search strategy over n-bests (Venugopal, 2005). Equation 5 can be easily interpreted as a constant loss “1” for choosing a wrong translation under current parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning (Taskar et al., 2004). As an alternative, we would also consider a softmax loss (Collins and Koo, 2005) represented by where ZO(w; f, e) = ∑e*EORACLE(w;f,e) exp(wTf(f, e*)) ZN(w; f) = ∑e′ENBEST(w;f) exp(wTf(f, e′)). Equation 6 is a log-linear model used in common NLP tasks such as tagging, chunking and named entity recognition, but differ slightly in that multiple corre</context>
</contexts>
<marker>Venugopal, 2005</marker>
<rawString>Ashish Venugopal. 2005. Considerations in maximum mutual information and minimum classification error training for statistical machine translation. In Proc. of EAMT-05, page 3031.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL 2007,</booktitle>
<pages>764--773</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1923" citStr="Watanabe et al., 2007" startWordPosition="283" endWordPosition="287">ERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters i</context>
<context position="23681" citStr="Watanabe et al., 2007" startWordPosition="4060" endWordPosition="4063"> for a larger batch size. As discussed in Section 3, the smaller batch size means frequent updates to parameters and a faster convergence, but potentially leads to a poor performance since the corpus-BLEU is approximately computed in a local batch. Our optimized update algorithms address the problem by adjusting the tradeoff between the amount of update to parameters and the loss, and perform better for larger batch sizes with a more accurate corpusBLEU. 6 Related Work Our work is largely inspired by pairwise rank optimization (Hopkins and May, 2011), but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. T</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. of EMNLP-CoNLL 2007, pages 764–773, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Fast generation of translation forest for largescale smt discriminative training.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP 2011,</booktitle>
<pages>880--888</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="17024" citStr="Xiao et al. (2011)" startWordPosition="2939" endWordPosition="2942"> hypergraph-based toolkit for training and decoding with synchronousCFGs (SCFG) for hierarchical phrase-bassed SMT (Chiang, 2007). The system employs 14 features, consisting of standard Hiero-style features (Chiang, 2007), and a set of indicator features, such as the number of synchronous-rules in a derivation. Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model (Dyer et al., 2011). For faster experiments, we precomputed translation forests inspired by Xiao et al. (2011). Instead of generating forests from bitexts in each iteration, we construct and save translation forests by intersecting the source side of SCFG with input sentences an MT02 MT06 d by keeping the target side of the interarg min w ∑2∥w−wk+14∥22+ηk (f,e)Eb ξf (16) 1 257 sected rules. n-bests are generated from the precomputed forests on the fly using the forest rescoring framework (Huang and Chiang, 2007) with additional non-local features, such as 5-gram language models. We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO</context>
</contexts>
<marker>Xiao, Liu, Liu, Lin, 2011</marker>
<rawString>Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin. 2011. Fast generation of translation forest for largescale smt discriminative training. In Proc. of EMNLP 2011, pages 880–888, Edinburgh, Scotland, UK., July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>