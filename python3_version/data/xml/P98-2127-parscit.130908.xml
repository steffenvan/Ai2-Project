<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.840143">
Automatic Retrieval and Clustering of Similar Words
</title>
<author confidence="0.991371">
Dekang Lin
</author>
<affiliation confidence="0.924034">
Department of Computer Science
University of Manitoba
Winnipeg, Manitoba, Canada R3T 2N2
</affiliation>
<email confidence="0.987298">
lindek@cs.umanitoba.ca
</email>
<sectionHeader confidence="0.99461" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995852">
Bootstrapping semantics from text is one of the
greatest challenges in natural language learning.
We first define a word similarity measure based on
the distributional pattern of words. The similarity
measure allows us to construct a thesaurus using a
parsed corpus. We then present a new evaluation
methodology for the automatically constructed the-
saurus. The evaluation results show that the the-
saurus is significantly closer to WordNet than Roget
Thesaurus is.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999826333333333">
The meaning of an unknown word can often be
inferred from its context. Consider the following
(slightly modified) example in (Nida, 1975, p.167):
</bodyText>
<listItem confidence="0.794998">
(1) A bottle of tezgiiino is on the table.
</listItem>
<bodyText confidence="0.999955112903226">
Everyone likes tezgiiino.
Tezgiiino makes you drunk.
We make tezgiiino out of corn.
The contexts in which the word tezgiiino is used
suggest that tezgiiino may be a kind of alcoholic
beverage made from corn mash.
Bootstrapping semantics from text is one of the
greatest challenges in natural language learning. It
has been argued that similarity plays an important
role in word acquisition (Gentner, 1982). Identify-
ing similar words is an initial step in learning the
definition of a word. This paper presents a method
for making this first step. For example, given a cor-
pus that includes the sentences in (1), our goal is to
be able to infer that tezgiiino is similar to &amp;quot;beer&amp;quot;,
&amp;quot;wine&amp;quot;, &amp;quot;vodka&amp;quot;, etc.
In addition to the long-term goal of bootstrap-
ping semantics from text, automatic identification
of similar words has many immediate applications.
The most obvious one is thesaurus construction. An
automatically created thesaurus offers many advan-
tages over manually constructed thesauri. Firstly,
the terms can be corpus- or genre-specific. Man-
ually constructed general-purpose dictionaries and
thesauri include many usages that are very infre-
quent in a particular corpus or genre of documents.
For example, one of the 8 senses of &amp;quot;company&amp;quot; in
WordNet 1.5 is a &amp;quot;visitor/visitant&amp;quot;, which is a hy-
ponym of &amp;quot;person&amp;quot;. This usage of the word is prac-
tically never used in newspaper articles. However,
its existance may prevent a co-reference recognizer
to rule out the possiblity for personal pronouns to
refer to &amp;quot;company&amp;quot;. Secondly, certain word us-
ages may be particular to a period of time, which
are unlikely to be captured by manually compiled
lexicons. For example, among 274 occurrences of
the word &amp;quot;westerner&amp;quot; in a 45 million word San Jose
Mercury corpus, 55% of them refer to hostages. If
one needs to search hostage-related articles, &amp;quot;west-
erner&amp;quot; may well be a good search term.
Another application of automatically extracted
similar words is to help solve the problem of data
sparseness in statistical natural language process-
ing (Dagan et al., 1994; Essen and Steinbiss, 1992).
When the frequency of a word does not warrant reli-
able maximum likelihood estimation, its probability
can be computed as a weighted sum of the probabil-
ities of words that are similar to it. It was shown in
(Dagan et al., 1997) that a similarity-based smooth-
ing method achieved much better results than back-
off smoothing methods in word sense disambigua-
tion.
The remainder of the paper is organized as fol-
lows. The next section is concerned with similari-
ties between words based on their distributional pat-
terns. The similarity measure can then be used to
create a thesaurus. In Section 3, we evaluate the
constructed thesauri by computing the similarity be-
tween their entries and entries in manually created
thesauri. Section 4 briefly discuss future work in
clustering similar words. Finally, Section 5 reviews
related work and summarize our contributions.
</bodyText>
<page confidence="0.997659">
768
</page>
<sectionHeader confidence="0.922265" genericHeader="method">
2 Word Similarity
</sectionHeader>
<bodyText confidence="0.995914583333333">
Our similarity measure is based on a proposal in
(Lin, 1997), where the similarity between two ob-
jects is defined to be the amount of information con-
tained in the commonality between the objects di-
vided by the amount of information in the descrip-
tions of the objects.
We use a broad-coverage parser (Lin, 1993; Lin,
1994) to extract dependency triples from the text
corpus. A dependency triple consists of two words
and the grammatical relationship between them in
the input sentence. For example, the triples ex-
tracted from the sentence &amp;quot;I have a brown dog&amp;quot; are:
</bodyText>
<listItem confidence="0.973941333333333">
(2) (have subj I), (I subj-of have), (dog obj-of
have), (dog adj-mod brown), (brown
adj-mod-of dog), (dog det a), (a det-of dog)
</listItem>
<bodyText confidence="0.997228176470588">
We use the notation 11w, r, w&apos; II to denote the fre-
quency count of the dependency triple (w, r, w&apos;) in
the parsed corpus. When w, r, or w&apos; is the wild
card (*), the frequency counts of all the depen-
dency triples that matches the rest of the pattern are
summed up. For example, kook, obj, *11 is the to-
tal occurrences of cook—object relationships in the
parsed corpus, and 11*,*,*11 is the total number of
dependency triples extracted from the parsed cor-
pus.
The description of a word w consists of the fre-
quency counts of all the dependency triples that
matches the pattern (w, *, *). The commonality be-
tween two words consists of the dependency triples
that appear in the descriptions of both words. For
example, (3) is the the description of the word
&amp;quot;cell&amp;quot;.
</bodyText>
<listItem confidence="0.647288">
(3) lIcell, subj-of, absorb11=1
kelt, subj-of, adapt11=1
&apos;ken, subj-of, behavel1=1
</listItem>
<bodyText confidence="0.921407625">
II cell, obj-of, contain11=4
obj-of, decorate11=2
Icell, nmod, bacteriall=3
I cell, nmod, blood vessell1=1
I cell, nmod, body11=2
Icell, nmod, bone marrow 11=2
Icell, nmod, burial 11=1
J cell, nmod, chameleon11=1
Assuming that the frequency counts of the depen-
dency triples are independent of each other, the in-
formation contained in the description of a word is
the sum of the information contained in each indi-
vidual frequency count.
To measure the information contained in the
statement Ilw, r, Il=c, we first measure the amount
of information in the statement that a randomly se-
lected dependency triple is (w, r, w&apos;) when we do
not know the value of 11w, r, will. We then mea-
sure the amount of information in the same state-
ment when we do know the value of11w, r, w&apos; II. The
difference between these two amounts is taken to be
the information contained in 11w, r, w&apos; II =c•
An occurrence of a dependency triple (w, r, w&apos;)
can be regarded as the co-occurrence of three
events:
A: a randomly selected word is w;
B: a randomly selected dependency type is r;
C: a randomly selected word is w1.
When the value of II w, r, w&apos; II is unknown, we
assume that A and C are conditionally indepen-
dent given B. The probability of A, B and C co-
occurring is estimated by
</bodyText>
<equation confidence="0.742843">
PMLE (B)PmLE(A1B)P,E(C1B),
</equation>
<bodyText confidence="0.9943825">
where PALE is the maximum likelihood estimation
of a probability distribution and
</bodyText>
<equation confidence="0.675492857142857">
lIcell, pobj-of, in11=159 PmLE(B) —
pobj-of, insidell=16 PmE(A1B) =
lIcell, pobj -of, intoll=30 PmLE(CIB)
II cell, nmod-of, abnormality I I =3
II cell, nmod-of, anemiall=8
I I cell, nmod-of, architecture II =1
I I cell, obj -of, attackl1=6
</equation>
<bodyText confidence="0.9407244">
II cell, obj -of, bludgeon11=1
lIcell, obj-of, cal111.11
cell, obj-of, come from11=3
When the value of II w, r, w&apos; II is known, we can
obtain PALE (A, B, C) directly:
</bodyText>
<equation confidence="0.626253">
PmLE(A, B, C) = iiw,r,w11111*,*, *II
</equation>
<bodyText confidence="0.984142">
Let /(w, r, w&apos;) denote the amount information
contained in 11w, r, w1 II=c. Its value can be corn-
</bodyText>
<page confidence="0.86146">
769
</page>
<equation confidence="0.9551984375">
SilnHindie (W1, W2) = E(r,w)ET(wo N
nT(w2)Arels..of.obrof} ming(wi, r, w), i(wz, r, w))
SiMH indler (w1, w2) = &gt;(r,w)ET(wiriT(w2) ming(wi , r, w), /(w2 r, w))
Silricosine(W1, W2) = VIT(tvi)I x IT(w2)1
SiinDice (W1, W2/ = IT(D1)1+1T(w2)
2x IT(wi )nT(w2
IT(wi)nT(w2)
SiMJacard(W1, W2) = 1T(wi)1+1T(w2)i—IT(w11)nT(w2)1
Figure 1: Other Similarity Measures
IT(wOnT(w2)1
puted as follows:
/(w, r, w&apos;)
= — log(P.,E(B)P1,,,,E(A1B)PkILE(C1B))
— (— log 11,,LE(A, B, C))
= Iltv,r,w11x11*,r,*11
a
</equation>
<bodyText confidence="0.941556428571429">
It is worth noting that 1(w,r,w&apos;) is equal to
the mutual information between w and w&apos; (Hindle,
1990).
Let T(W) be the set of pairs (r, w&apos;) such that
log x 1,r,is positive. We define the sim-
ilarity sim(wi , w2) between two words w1 and w2
as follows:
</bodyText>
<equation confidence="0.951368">
E(r,w)ET(wonT(w2)(/(w1, r, w) /(w2, r, w))
74-,(r,w)ET(w2) /(W2&apos; r&apos; w)
</equation>
<bodyText confidence="0.999991571428571">
We parsed a 64-million-word corpus consisting
of the Wall Street Journal (24 million words), San
Jose Mercury (21 million words) and AP Newswire
(19 million words). From the parsed corpus, we
extracted 56.5 million dependency triples (8.7 mil-
lion unique). In the parsed corpus, there are 5469
nouns, 2173 verbs, and 2632 adjectives/adverbs that
occurred at least 100 times. We computed the pair-
wise similarity between all the nouns, all the verbs
and all the adjectives/adverbs, using the above sim-
ilarity measure. For each word, we created a the-
saurus entry which contains the top-N1 words that
are most similar to it.2 The thesaurus entry for word
w has the following format:
</bodyText>
<equation confidence="0.438483">
w (Pos) : wi, si, wz, sz, • • • , WN, SN
</equation>
<bodyText confidence="0.999872">
where pos is a part of speech, we is a word,
se=sim(w, ) and se&apos;s are ordered in descending
</bodyText>
<footnote confidence="0.619692333333333">
&apos;We used N=200 in our experiments
2The resulting thesaurus is available at:
http://www.cs.umanitoba.caflindek/sims.htm.
</footnote>
<bodyText confidence="0.999317542857143">
order. For example, the top-10 words in the noun,
verb, and adjective entries for the word &amp;quot;brief&amp;quot; are
shown below:
brief (noun): affidavit 0.13, petition 0.05, memo-
randum 0.05, motion 0.05, lawsuit 0.05, depo-
sition 0.05, slight 0.05, prospectus 0.04, docu-
ment 0.04 paper 0.04, ...
brief (verb): tell 0.09, urge 0.07, ask 0.07, meet
0.06, appoint 0.06, elect 0.05, name 0.05, em-
power 0.05, summon 0.05, overrule 0.04, ...
brief (adjective): lengthy 0.13, short 0.12, recent
0.09, prolonged 0.09, long 0.09, extended 0.09,
daylong 0.08, scheduled 0.08, stormy 0.07,
planned 0.06, ...
Two words are a pair of respective nearest neigh-
bors (RNNs) if each is the other&apos;s most similar
word. Our program found 543 pairs of RNN nouns,
212 pairs of RNN verbs and 382 pairs of RNN
adjectives/adverbs in the automatically created the-
saurus. Appendix A lists every 10th of the RNNs.
The result looks very strong. Few pairs of RNNs in
Appendix A have clearly better alternatives.
We also constructed several other thesauri us-
ing the same corpus, but with the similarity mea-
sures in Figure 1. The measure SirriHindie is the
same as the similarity measure proposed in (Hin-
dle, 1990), except that it does not use dependency
triples with negative mutual information. The mea-
sure simHindi„ is the same as sim H indle except that
all types of dependency relationships are used, in-
stead of just subject and object relationships. The
measures simeosine, simdice and SiMJacard are ver-
sions of similarity measures commonly used in in-
formation retrieval (Frakes and Baeza-Yates, 1992).
Unlike sim, sim H indle and si -MHindler, they only
</bodyText>
<page confidence="0.703379">
770
</page>
<equation confidence="0.995896333333333">
2 log P(c)
SiMw N(wi,w2) = MaXc1ES(w1)Ac2ES(w2)(MaXcEsuper(cOnsuper(c2) log P(ci Wog P(c2)
SiMRoget (W1) 1-02) = 12Z7;1)-rflIRRZ
</equation>
<bodyText confidence="0.999567">
where S(w) is the set of senses of w in the WordNet, super(c) is the set of (possibly indirect)
superclasses of concept c in the WordNet, R(w) is the set of words that belong to a same Roget
category as w.
</bodyText>
<figureCaption confidence="0.99464">
Figure 2: Word similarity measures based on WordNet and Roget
</figureCaption>
<bodyText confidence="0.8058255">
make use of the unique dependency triples and ig-
nore their frequency counts.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99974825">
In this section, we present an evaluation of automat-
ically constructed thesauri with two manually com-
piled thesauri, namely, WordNet1.5 (Miller et al.,
1990) and Roget Thesaurus. We first define two
word similarity measures that are based on the struc-
tures of WordNet and Roget (Figure 2). The simi-
larity measure simwN is based on the proposal in
(Lin, 1997). The similarity measure sim
--Roget treats
all the words in Roget as features. A word w pos-
sesses the feature f if f and w belong to a same
Roget category. The similarity between two words
is then defined as the cosine coefficient of the two
feature vectors.
With simwN and siMRoget, we transform Word-
Net and Roget into the same format as the automat-
ically constructed thesauri in the previous section.
We now discuss how to measure the similarity be-
tween two thesaurus entries. Suppose two thesaurus
entries for the same word are as follows:
</bodyText>
<equation confidence="0.954010666666667">
W st, w2, s2, • • • , wN, sN
W • &apos;4&apos;11°11 u&apos;21°21 • • • &amp;quot;-&apos;NI&apos;N
Their similarity is defined as:
(4)
Ewi=oi sis;
V(En 4)(E7-1 s?)
</equation>
<bodyText confidence="0.7419365">
For example, (5) is the entry for &amp;quot;brief (noun)&amp;quot; in
our automatically generated thesaurus and (6) and
(7) are corresponding entries in WordNet thesaurus
and Roget thesaurus.
(5) brief (noun): affidavit 0.13, petition 0.05,
memorandum 0.05, motion 0.05, lawsuit 0.05,
deposition 0.05, slight 0.05, prospectus 0.04,
document 0.04 paper 0.04.
</bodyText>
<listItem confidence="0.927294454545455">
(6) brief (noun): outline 0.96, instrument 0.84,
summary 0.84, affidavit 0.80, deposition
0.80, law 0.77, survey 0.74, sketch 0.74,
resume 0.74, argument 0.74.
(7) brief (noun): recital 0.77, saga 0.77,
autobiography 0.77, anecdote 0.77, novel
0.77, novelist 0.77, tradition 0.70, historian
0.70, tale 0.64.
According to (4), the similarity between (5) and
(6) is 0.297, whereas the similarities between (5)
and (7) and between (6) and (7) are 0.
</listItem>
<bodyText confidence="0.981379307692308">
Our evaluation was conducted with 4294 nouns
that occurred at least 100 times in the parsed cor-
pus and are found in both WordNet1.5 and the Ro-
get Thesaurus. Table 1 shows the average similarity
between corresponding entries in different thesauri
and the standard deviation of the average, which
is the standard deviation of the data items divided
by the square root of the number of data items.
Since the differences among sim
SiMJacard cosine, SiMdzce and
are very small, we only included the re-
sults for sitncosi, in Table 1 for the sake of brevity.
It can be seen that sim, Hindle, and cosine are
significantly more similar to WordNet than Roget
is, but are significantly less similar to Roget than
WordNet is. The differences between Hindle and
Hindle,. clearly demonstrate that the use of other
types of dependencies in addition to subject and ob-
ject relationships is very beneficial.
The performance of sim, Hindle, and cosine are
quite close. To determine whether or not the dif-
ferences are statistically significant, we computed
their differences in similarities to WordNet and Ro-
get thesaurus for each individual entry. Table 2
shows the average and standard deviation of the av-
erage difference. Since the 95% confidence inter-
</bodyText>
<page confidence="0.999057">
771
</page>
<tableCaption confidence="0.999981">
Table 1: Evaluation with WordNet and Roget
</tableCaption>
<table confidence="0.996959285714286">
WordNet
average Gra vg
Roget 0.178397 0.001636
sim 0.212199 0.001484
Hindle, 0.204179 0.001424
cosine 0.199402 0.001352
Hindle 0.164716 0.001200
Roget
average aavg
WordNet 0.178397 0.001636
sim 0.149045 0.001429
Hindle, 0.14663 0.001383
cosine 0.135697 0.001275
Hindle 0.115489 0.001140
</table>
<bodyText confidence="0.98785475">
vals of all the differences in Table 2 are on the posi-
tive side, one can draw the statistical conclusion that
simis better than simHzndle, which is better than
sinacosine.
</bodyText>
<tableCaption confidence="0.927949">
Table 2: Distribution of Differences
</tableCaption>
<bodyText confidence="0.6612893">
WordNet
average Gra vp
sim- Hindle, 0.008021 0.000428
sim-cosine 0.012798 0.000386
Hindle, -cosine 0.004777 0.000561
Roget
average Cravg
sim-Hindle„ 0.002415 0.000401
sim -cosine 0.013349 0.000375
Hindle, -cosine 0.010933 0.000509
</bodyText>
<sectionHeader confidence="0.99877" genericHeader="method">
4 Future Work
</sectionHeader>
<bodyText confidence="0.995817666666667">
Reliable extraction of similar words from text cor-
pus opens up many possibilities for future work. For
example, one can go a step further by constructing a
tree structure among the most similar words so that
different senses of a given word can be identified
with different subtrees. Let w1, , wr, be a list of
words in descending order of their similarity to a
given word w. The similarity tree for w is created
as follows:
</bodyText>
<listItem confidence="0.979939">
• Initialize the similarity tree to consist of a sin-
gle node w.
• For i=1, 2, ..., n, insert wi as a child of wi
such that w3 is the most similar one to wi
</listItem>
<bodyText confidence="0.993644833333333">
among lw, w1, wz-11.
For example, Figure 3 shows the similarity tree for
the top-40 most similar words to duty. The first
number behind a word is the similarity of the word
to its parent. The second number is the similarity of
the word to the root node of the tree.
</bodyText>
<table confidence="0.993214585365854">
duty
___responsibility 0.21 0.21
role 0.12 0.11
action 0.11 0.10
___change 0.24 0.08
I-rule 0.16 0.08
I___restriction 0.27 0.08
I I ___ban 0.30 0.08
I I ___sanction 0.19 0.08
I___schedule 0.11 0.07
I___regulation 0.37 0.07
___challenge 0.13 0.07
I-issue 0.13 0.07
I reason 0.14 0.07
I matter 0.28 0.07
measure 0.22 0.07 &apos;
___obligation 0.12 0.10
___power 0.17 0.08
I___jurisdiction 0.13 0.08
I___right 0.12 0.07
I control 0.20 0.07
I-ground 0.08 0.07
___accountability 0.14 0.08
___experience 0.12 0.07
post 0.14 0.14
job 0.17 0.10
I___work 0.17 0.10
I__training 0.11 0.07
___position 0.25 0.10
task 0.10 0.10
I___chore 0.11 0.07
operation 0.10 0.10
I___function 0.10 0.08
I___mission 0.12 0.07
I I._patrol 0.07 0.07
I staff 0.10 0.07
penalty 0.09 0.09
I fee 0.17 0.08
I___tariff 0.13 0.08
I_tax 0.19 0.07
reservist 0.07 0.07
</table>
<figureCaption confidence="0.99446">
Figure 3: Similarity tree for &amp;quot;duty&amp;quot;
</figureCaption>
<bodyText confidence="0.999924333333333">
Inspection of sample outputs shows that this al-
gorithm works well. However, formal evaluation of
its accuracy remains to be future work.
</bodyText>
<sectionHeader confidence="0.998467" genericHeader="related work">
5 Related Work and Conclusion
</sectionHeader>
<bodyText confidence="0.9960625">
There have been many approaches to automatic de-
tection of similar words from text corpora. Ours is
</bodyText>
<page confidence="0.99295">
772
</page>
<bodyText confidence="0.999906633333334">
similar to (Grefenstette, 1994; Hindle, 1990; Ruge,
1992) in the use of dependency relationship as the
word features, based on which word similarities are
computed.
Evaluation of automatically generated lexical re-
sources is a difficult problem. In (Hindle, 1990),
a small set of sample results are presented. In
(Smadja, 1993), automatically extracted colloca-
tions are judged by a lexicographer. In (Dagan et
al., 1993) and (Pereira et al., 1993), clusters of sim-
ilar words are evaluated by how well they are able
to recover data items that are removed from the in-
put corpus one at a time. In (Alshawi and Carter,
1994), the collocations and their associated scores
were evaluated indirectly by their use in parse tree
selection. The merits of different measures for as-
sociation strength are judged by the differences they
make in the precision and the recall of the parser
outputs.
The main contribution of this paper is a new eval-
uation methodology for automatically constructed
thesaurus. While previous methods rely on indirect
tasks or subjective judgments, our method allows
direct and objective comparison between automati-
cally and manually constructed thesauri. The results
show that our automatically created thesaurus is sig-
nificantly closer to WordNet than Roget Thesaurus
is. Our experiments also surpasses previous experi-
ments on automatic thesaurus construction in scale
and (possibly) accuracy.
</bodyText>
<sectionHeader confidence="0.751866" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.970400666666667">
This research has also been partially supported by
NSERC Research Grant 0GP121338 and by the In-
stitute for Robotics and Intelligent Systems.
</bodyText>
<sectionHeader confidence="0.978063" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.980635822580645">
Hiyan Alshawi and David Carter. 1994. Training
and scaling preference functions for disambiguation.
Computational Linguistics, 20(4):635-648, Decem-
ber.
Ido Dagan, Shaul Marcus, and Shaul Markovitch. 1993.
Contextual word similarity and estimation from sparse
data. In Proceedings of ACL-93, pages 164-171,
Columbus, Ohio, June.
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of the 32nd Annual
Meeting of the ACL, pages 272-278, Las Cruces, NM.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based method for word sense disambigua-
tion. In Proceedings of the 35th Annual Meeting of
the ACL, pages 56-63, Madrid, Spain.
Ute Essen and Volker Steinbiss. 1992. Cooccurrence
smoothing for stochastic language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 161-164.
W. B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structure and Algorithms.
Prentice Hall.
D. Gentner. 1982. Why nouns are learned before verbs:
Linguistic relativity versus natural partitioning. In
S. A. Kuczaj, editor, Language development: Vol. 2.
Language, thought, and culture, pages 301-334. Erl-
baum, Hillsdale, NJ.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Press,
Boston, MA.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of
ACL-90, pages 268-275, Pittsburg, Pennsylvania,
June.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. In Proceedings of ACL-93, pages
112-120, Columbus, Ohio.
Dekang Lin. 1994. Principar—an efficient, broad-
coverage, principle-based parser. In Proceedings of
COLING-94, pages 482-488. Kyoto, Japan.
Dekang Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of ACL/EACL-97, pages 64-71, Madrid, Spain,
July.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography, 3(4):235-244.
George A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235-312.
Eugene A. Nida. 1975. Componential Analysis of Mean-
ing. The Hague, Mouton.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
Clustering of English Words. In Proceedings of ACL-
93, pages 183-190, Ohio State University, Columbus,
Ohio.
Gerda Ruge. 1992. Experiments on linguistically based
term associations. Information Processing &amp; Man-
agement, 28(3):317-332.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143-178.
</reference>
<page confidence="0.998363">
773
</page>
<table confidence="0.996732097560976">
Appendix A: Respective Nearest Neighbors
Nouns
Rank Respective Nearest Neighbors Similarity
1 earnings profit 0.572525
11 plan proposal 0.47475
21 employee worker 0.413936
31 battle fight 0.389776
41 airline carrier 0.370589
51 share stock 0.351294
61 rumor speculation 0.327266
71 outlay spending 0.320535
81 accident incident 0.310121
91 facility plant 0.284845
101 charge count 0.278339
111 baby infant 0.268093
121 actor actress 0.255098
131 chance likelihood 0.248942
141 catastrophe disaster 0.241986
151 fine penalty 0.237606
161 legislature parliament 0.231528
171 oil petroleum 0.227277
181 strength weakness 0.218027
191 radio television 0.215043
201 coupe sedan 0.209631
211 turmoil upheaval 0.205841
221 music song 0.202102
231 bomb grenade 0.198707
241 gallery museum 0.194591
251 leaf leave 0.192483
261 fuel gasoline 0.186045
271 door window 0.181301
281 emigration immigration 0.176331
291 espionage treason 0.17262
301 peril pitfall 0.169587
311 surcharge surtax 0.166831
321 ability credibility 0.163301
331 pub tavern 0.158815
341 license permit 0.156963
351 excerpt transcript 0.150941
361 dictatorship regime 0.148837
371 lake river 0.145586
381 disc disk 0.142733
391 interpreter translator 0.138778
401 bacteria organism 0.135539
411 ballet symphony 0.131688
421 silk wool 0.128999
431 intent intention 0.125236
441 waiter waitress 0.122373
451 blood urine 0.118063
461 mosquito tick 0.115499
471 fervor zeal 0.112087
481 equal equivalent 0.107159
491 freezer refrigerator 0.103777
501 humor wit 0.0991108
511 cushion pillow 0.0944567
521 purse wallet 0.0914273
531 learning listening 0.0859118
541 clown cowboy 0.0714762
Verbs
Rank Respective Nearest Neighbors Similarity
1 fall rise 0.674113
11 injure kill 0.378254
21 concern worry 0.340122
31 convict sentence 0.289678
41 limit restrict 0.271588
51 narrow widen 0.258385
61 attract draw 0.242331
71 discourage encourage 0.234425
81 hit strike 0.22171
91 disregard ignore 0.21027
101 overstate understate 0.199197
111 affirm reaffirm 0.182765
121 inform notify 0.170477
131 differ vary 0.161821
141 scream yell 0.150168
151 laugh smile 0.142951
161 compete cope 0.135869
171 add whisk 0.129205
181 blossom mature 0.123351
191 smell taste 0.112418
201 bark howl 0.101566
211 black white 0.0694954
Adjective/Adverbs
Rank Respective Nearest Neighbors Similarity
1 high low 0.580408
11 bad good 0.376744
21 extremely very 0.357606
31 deteriorating improving 0.332664
41 alleged suspected 0.317163
51 clerical salaried 0.305448
61 often sometimes 0.281444
71 bleak gloomy 0.275557
81 adequate inadequate 0.263136
91 affiliated merged 0.257666
101 stormy turbulent 0.252846
111 paramilitary uniformed 0.246638
121 sharp steep 0.240788
131 communist leftist 0.232518
141 indoor outdoor 0.224183
151 changed changing 0.219697
161 defensive offensive 0.211062
171 sad tragic 0.206688
181 enormously tremendously 0.199936
191 defective faulty 0.193863
201 concerned worried 0.186899
211 dropped fell 0.184768
221 bloody violent 0.183058
231 favorite popular 0.179234
241 permanently temporarily 0.174361
251 confidential secret 0.17022
261 privately publicly 0.165313
271 operating sales 0.162894
281 annually apiece 0.159883
291 gentle kind 0.154554
301 losing winning 0.149447
311 experimental test 0.146435
321 designer dress 0.142552
331 dormant inactive 0.137002
341 commercially domestically 0.132918
351 complimentary free 0.128117
361 constantly continually 0.122342
371 hardy resistant 0.112133
381 anymore anyway 0.103241
</table>
<page confidence="0.995264">
774
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801989">
<title confidence="0.999723">Automatic Retrieval and Clustering of Similar Words</title>
<author confidence="0.998392">Dekang Lin</author>
<affiliation confidence="0.999877">Department of Computer Science University of Manitoba</affiliation>
<address confidence="0.998591">Winnipeg, Manitoba, Canada R3T 2N2</address>
<email confidence="0.997744">lindek@cs.umanitoba.ca</email>
<abstract confidence="0.981836636363636">Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>David Carter</author>
</authors>
<title>Training and scaling preference functions for disambiguation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="17549" citStr="Alshawi and Carter, 1994" startWordPosition="2922" endWordPosition="2925">s 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a new evaluation methodology for automatically constructed thesaurus. While previous methods rely on indirect tasks or subjective judgments, our method allows direct and objective comparison between automatically and manually constructed thesauri. The results show that our automatically created t</context>
</contexts>
<marker>Alshawi, Carter, 1994</marker>
<rawString>Hiyan Alshawi and David Carter. 1994. Training and scaling preference functions for disambiguation. Computational Linguistics, 20(4):635-648, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Shaul Marcus</author>
<author>Shaul Markovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL-93,</booktitle>
<pages>164--171</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="17349" citStr="Dagan et al., 1993" startWordPosition="2883" endWordPosition="2886">ever, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a new evaluation methodology for automatically constructed thesaurus. While previous methods rely</context>
</contexts>
<marker>Dagan, Marcus, Markovitch, 1993</marker>
<rawString>Ido Dagan, Shaul Marcus, and Shaul Markovitch. 1993. Contextual word similarity and estimation from sparse data. In Proceedings of ACL-93, pages 164-171, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Fernando Pereira</author>
<author>Lillian Lee</author>
</authors>
<title>Similarity-based estimation of word cooccurrence probabilities.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the ACL,</booktitle>
<pages>272--278</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="2907" citStr="Dagan et al., 1994" startWordPosition="458" endWordPosition="461"> recognizer to rule out the possiblity for personal pronouns to refer to &amp;quot;company&amp;quot;. Secondly, certain word usages may be particular to a period of time, which are unlikely to be captured by manually compiled lexicons. For example, among 274 occurrences of the word &amp;quot;westerner&amp;quot; in a 45 million word San Jose Mercury corpus, 55% of them refer to hostages. If one needs to search hostage-related articles, &amp;quot;westerner&amp;quot; may well be a good search term. Another application of automatically extracted similar words is to help solve the problem of data sparseness in statistical natural language processing (Dagan et al., 1994; Essen and Steinbiss, 1992). When the frequency of a word does not warrant reliable maximum likelihood estimation, its probability can be computed as a weighted sum of the probabilities of words that are similar to it. It was shown in (Dagan et al., 1997) that a similarity-based smoothing method achieved much better results than backoff smoothing methods in word sense disambiguation. The remainder of the paper is organized as follows. The next section is concerned with similarities between words based on their distributional patterns. The similarity measure can then be used to create a thesau</context>
</contexts>
<marker>Dagan, Pereira, Lee, 1994</marker>
<rawString>Ido Dagan, Fernando Pereira, and Lillian Lee. 1994. Similarity-based estimation of word cooccurrence probabilities. In Proceedings of the 32nd Annual Meeting of the ACL, pages 272-278, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based method for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>56--63</pages>
<location>Madrid,</location>
<contexts>
<context position="3163" citStr="Dagan et al., 1997" startWordPosition="504" endWordPosition="507"> the word &amp;quot;westerner&amp;quot; in a 45 million word San Jose Mercury corpus, 55% of them refer to hostages. If one needs to search hostage-related articles, &amp;quot;westerner&amp;quot; may well be a good search term. Another application of automatically extracted similar words is to help solve the problem of data sparseness in statistical natural language processing (Dagan et al., 1994; Essen and Steinbiss, 1992). When the frequency of a word does not warrant reliable maximum likelihood estimation, its probability can be computed as a weighted sum of the probabilities of words that are similar to it. It was shown in (Dagan et al., 1997) that a similarity-based smoothing method achieved much better results than backoff smoothing methods in word sense disambiguation. The remainder of the paper is organized as follows. The next section is concerned with similarities between words based on their distributional patterns. The similarity measure can then be used to create a thesaurus. In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work a</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1997</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1997. Similarity-based method for word sense disambiguation. In Proceedings of the 35th Annual Meeting of the ACL, pages 56-63, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ute Essen</author>
<author>Volker Steinbiss</author>
</authors>
<title>Cooccurrence smoothing for stochastic language modeling.</title>
<date>1992</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<volume>1</volume>
<pages>161--164</pages>
<contexts>
<context position="2935" citStr="Essen and Steinbiss, 1992" startWordPosition="462" endWordPosition="465">out the possiblity for personal pronouns to refer to &amp;quot;company&amp;quot;. Secondly, certain word usages may be particular to a period of time, which are unlikely to be captured by manually compiled lexicons. For example, among 274 occurrences of the word &amp;quot;westerner&amp;quot; in a 45 million word San Jose Mercury corpus, 55% of them refer to hostages. If one needs to search hostage-related articles, &amp;quot;westerner&amp;quot; may well be a good search term. Another application of automatically extracted similar words is to help solve the problem of data sparseness in statistical natural language processing (Dagan et al., 1994; Essen and Steinbiss, 1992). When the frequency of a word does not warrant reliable maximum likelihood estimation, its probability can be computed as a weighted sum of the probabilities of words that are similar to it. It was shown in (Dagan et al., 1997) that a similarity-based smoothing method achieved much better results than backoff smoothing methods in word sense disambiguation. The remainder of the paper is organized as follows. The next section is concerned with similarities between words based on their distributional patterns. The similarity measure can then be used to create a thesaurus. In Section 3, we evalua</context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>Ute Essen and Volker Steinbiss. 1992. Cooccurrence smoothing for stochastic language modeling. In Proceedings of ICASSP, volume 1, pages 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Frakes</author>
<author>R Baeza-Yates</author>
<author>editors</author>
</authors>
<date>1992</date>
<booktitle>Information Retrieval, Data Structure and Algorithms.</booktitle>
<publisher>Prentice Hall.</publisher>
<marker>Frakes, Baeza-Yates, editors, 1992</marker>
<rawString>W. B. Frakes and R. Baeza-Yates, editors. 1992. Information Retrieval, Data Structure and Algorithms. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gentner</author>
</authors>
<title>Why nouns are learned before verbs: Linguistic relativity versus natural partitioning.</title>
<date>1982</date>
<booktitle>Language development: Vol. 2. Language, thought, and culture,</booktitle>
<pages>301--334</pages>
<editor>In S. A. Kuczaj, editor,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="1257" citStr="Gentner, 1982" startWordPosition="191" endWordPosition="192"> is. 1 Introduction The meaning of an unknown word can often be inferred from its context. Consider the following (slightly modified) example in (Nida, 1975, p.167): (1) A bottle of tezgiiino is on the table. Everyone likes tezgiiino. Tezgiiino makes you drunk. We make tezgiiino out of corn. The contexts in which the word tezgiiino is used suggest that tezgiiino may be a kind of alcoholic beverage made from corn mash. Bootstrapping semantics from text is one of the greatest challenges in natural language learning. It has been argued that similarity plays an important role in word acquisition (Gentner, 1982). Identifying similar words is an initial step in learning the definition of a word. This paper presents a method for making this first step. For example, given a corpus that includes the sentences in (1), our goal is to be able to infer that tezgiiino is similar to &amp;quot;beer&amp;quot;, &amp;quot;wine&amp;quot;, &amp;quot;vodka&amp;quot;, etc. In addition to the long-term goal of bootstrapping semantics from text, automatic identification of similar words has many immediate applications. The most obvious one is thesaurus construction. An automatically created thesaurus offers many advantages over manually constructed thesauri. Firstly, the t</context>
</contexts>
<marker>Gentner, 1982</marker>
<rawString>D. Gentner. 1982. Why nouns are learned before verbs: Linguistic relativity versus natural partitioning. In S. A. Kuczaj, editor, Language development: Vol. 2. Language, thought, and culture, pages 301-334. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Press,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="16960" citStr="Grefenstette, 1994" startWordPosition="2825" endWordPosition="2826"> 0.10 I__training 0.11 0.07 ___position 0.25 0.10 task 0.10 0.10 I___chore 0.11 0.07 operation 0.10 0.10 I___function 0.10 0.08 I___mission 0.12 0.07 I I._patrol 0.07 0.07 I staff 0.10 0.07 penalty 0.09 0.09 I fee 0.17 0.08 I___tariff 0.13 0.08 I_tax 0.19 0.07 reservist 0.07 0.07 Figure 3: Similarity tree for &amp;quot;duty&amp;quot; Inspection of sample outputs shows that this algorithm works well. However, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collo</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Press, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90,</booktitle>
<pages>268--275</pages>
<location>Pittsburg, Pennsylvania,</location>
<contexts>
<context position="7852" citStr="Hindle, 1990" startWordPosition="1303" endWordPosition="1304">c. Its value can be corn769 SilnHindie (W1, W2) = E(r,w)ET(wo N nT(w2)Arels..of.obrof} ming(wi, r, w), i(wz, r, w)) SiMH indler (w1, w2) = &gt;(r,w)ET(wiriT(w2) ming(wi , r, w), /(w2 r, w)) Silricosine(W1, W2) = VIT(tvi)I x IT(w2)1 SiinDice (W1, W2/ = IT(D1)1+1T(w2) 2x IT(wi )nT(w2 IT(wi)nT(w2) SiMJacard(W1, W2) = 1T(wi)1+1T(w2)i—IT(w11)nT(w2)1 Figure 1: Other Similarity Measures IT(wOnT(w2)1 puted as follows: /(w, r, w&apos;) = — log(P.,E(B)P1,,,,E(A1B)PkILE(C1B)) — (— log 11,,LE(A, B, C)) = Iltv,r,w11x11*,r,*11 a It is worth noting that 1(w,r,w&apos;) is equal to the mutual information between w and w&apos; (Hindle, 1990). Let T(W) be the set of pairs (r, w&apos;) such that log x 1,r,is positive. We define the similarity sim(wi , w2) between two words w1 and w2 as follows: E(r,w)ET(wonT(w2)(/(w1, r, w) /(w2, r, w)) 74-,(r,w)ET(w2) /(W2&apos; r&apos; w) We parsed a 64-million-word corpus consisting of the Wall Street Journal (24 million words), San Jose Mercury (21 million words) and AP Newswire (19 million words). From the parsed corpus, we extracted 56.5 million dependency triples (8.7 million unique). In the parsed corpus, there are 5469 nouns, 2173 verbs, and 2632 adjectives/adverbs that occurred at least 100 times. We co</context>
<context position="10171" citStr="Hindle, 1990" startWordPosition="1696" endWordPosition="1698">ormy 0.07, planned 0.06, ... Two words are a pair of respective nearest neighbors (RNNs) if each is the other&apos;s most similar word. Our program found 543 pairs of RNN nouns, 212 pairs of RNN verbs and 382 pairs of RNN adjectives/adverbs in the automatically created thesaurus. Appendix A lists every 10th of the RNNs. The result looks very strong. Few pairs of RNNs in Appendix A have clearly better alternatives. We also constructed several other thesauri using the same corpus, but with the similarity measures in Figure 1. The measure SirriHindie is the same as the similarity measure proposed in (Hindle, 1990), except that it does not use dependency triples with negative mutual information. The measure simHindi„ is the same as sim H indle except that all types of dependency relationships are used, instead of just subject and object relationships. The measures simeosine, simdice and SiMJacard are versions of similarity measures commonly used in information retrieval (Frakes and Baeza-Yates, 1992). Unlike sim, sim H indle and si -MHindler, they only 770 2 log P(c) SiMw N(wi,w2) = MaXc1ES(w1)Ac2ES(w2)(MaXcEsuper(cOnsuper(c2) log P(ci Wog P(c2) SiMRoget (W1) 1-02) = 12Z7;1)-rflIRRZ where S(w) is the se</context>
<context position="16974" citStr="Hindle, 1990" startWordPosition="2827" endWordPosition="2828">11 0.07 ___position 0.25 0.10 task 0.10 0.10 I___chore 0.11 0.07 operation 0.10 0.10 I___function 0.10 0.08 I___mission 0.12 0.07 I I._patrol 0.07 0.07 I staff 0.10 0.07 penalty 0.09 0.09 I fee 0.17 0.08 I___tariff 0.13 0.08 I_tax 0.19 0.07 reservist 0.07 0.07 Figure 3: Similarity tree for &amp;quot;duty&amp;quot; Inspection of sample outputs shows that this algorithm works well. However, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and th</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun classification from predicate-argument structures. In Proceedings of ACL-90, pages 268-275, Pittsburg, Pennsylvania, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL-93,</booktitle>
<pages>112--120</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="4126" citStr="Lin, 1993" startWordPosition="664" endWordPosition="665">ction 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Word Similarity Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence &amp;quot;I have a brown dog&amp;quot; are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) We use the notation 11w, r, w&apos; II to denote the frequency count of the dependency triple (w, r, w&apos;) in the parsed corpus. When w, r, or w&apos; is the wild card (*), the frequency counts of all the dependency triples that</context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>Dekang Lin. 1993. Principle-based parsing without overgeneration. In Proceedings of ACL-93, pages 112-120, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Principar—an efficient, broadcoverage, principle-based parser.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<pages>482--488</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="4138" citStr="Lin, 1994" startWordPosition="666" endWordPosition="667"> evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Word Similarity Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence &amp;quot;I have a brown dog&amp;quot; are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) We use the notation 11w, r, w&apos; II to denote the frequency count of the dependency triple (w, r, w&apos;) in the parsed corpus. When w, r, or w&apos; is the wild card (*), the frequency counts of all the dependency triples that matches the</context>
</contexts>
<marker>Lin, 1994</marker>
<rawString>Dekang Lin. 1994. Principar—an efficient, broadcoverage, principle-based parser. In Proceedings of COLING-94, pages 482-488. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL-97,</booktitle>
<pages>64--71</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="3877" citStr="Lin, 1997" startWordPosition="620" endWordPosition="621"> word sense disambiguation. The remainder of the paper is organized as follows. The next section is concerned with similarities between words based on their distributional patterns. The similarity measure can then be used to create a thesaurus. In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Word Similarity Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence &amp;quot;I have a brown dog&amp;quot; are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of d</context>
<context position="11467" citStr="Lin, 1997" startWordPosition="1912" endWordPosition="1913">es of concept c in the WordNet, R(w) is the set of words that belong to a same Roget category as w. Figure 2: Word similarity measures based on WordNet and Roget make use of the unique dependency triples and ignore their frequency counts. 3 Evaluation In this section, we present an evaluation of automatically constructed thesauri with two manually compiled thesauri, namely, WordNet1.5 (Miller et al., 1990) and Roget Thesaurus. We first define two word similarity measures that are based on the structures of WordNet and Roget (Figure 2). The similarity measure simwN is based on the proposal in (Lin, 1997). The similarity measure sim --Roget treats all the words in Roget as features. A word w possesses the feature f if f and w belong to a same Roget category. The similarity between two words is then defined as the cosine coefficient of the two feature vectors. With simwN and siMRoget, we transform WordNet and Roget into the same format as the automatically constructed thesauri in the previous section. We now discuss how to measure the similarity between two thesaurus entries. Suppose two thesaurus entries for the same word are as follows: W st, w2, s2, • • • , wN, sN W • &apos;4&apos;11°11 u&apos;21°21 • • • </context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>Dekang Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of ACL/EACL-97, pages 64-71, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<date>1990</date>
<contexts>
<context position="11266" citStr="Miller et al., 1990" startWordPosition="1874" endWordPosition="1877"> = MaXc1ES(w1)Ac2ES(w2)(MaXcEsuper(cOnsuper(c2) log P(ci Wog P(c2) SiMRoget (W1) 1-02) = 12Z7;1)-rflIRRZ where S(w) is the set of senses of w in the WordNet, super(c) is the set of (possibly indirect) superclasses of concept c in the WordNet, R(w) is the set of words that belong to a same Roget category as w. Figure 2: Word similarity measures based on WordNet and Roget make use of the unique dependency triples and ignore their frequency counts. 3 Evaluation In this section, we present an evaluation of automatically constructed thesauri with two manually compiled thesauri, namely, WordNet1.5 (Miller et al., 1990) and Roget Thesaurus. We first define two word similarity measures that are based on the structures of WordNet and Roget (Figure 2). The similarity measure simwN is based on the proposal in (Lin, 1997). The similarity measure sim --Roget treats all the words in Roget as features. A word w possesses the feature f if f and w belong to a same Roget category. The similarity between two words is then defined as the cosine coefficient of the two feature vectors. With simwN and siMRoget, we transform WordNet and Roget into the same format as the automatically constructed thesauri in the previous sect</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990.</rawString>
</citation>
<citation valid="false">
<title>Introduction to WordNet: An on-line lexical database.</title>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<marker></marker>
<rawString>Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<pages>3--4</pages>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene A Nida</author>
</authors>
<title>Componential Analysis of Meaning. The Hague,</title>
<date>1975</date>
<location>Mouton.</location>
<contexts>
<context position="799" citStr="Nida, 1975" startWordPosition="116" endWordPosition="117">trapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is. 1 Introduction The meaning of an unknown word can often be inferred from its context. Consider the following (slightly modified) example in (Nida, 1975, p.167): (1) A bottle of tezgiiino is on the table. Everyone likes tezgiiino. Tezgiiino makes you drunk. We make tezgiiino out of corn. The contexts in which the word tezgiiino is used suggest that tezgiiino may be a kind of alcoholic beverage made from corn mash. Bootstrapping semantics from text is one of the greatest challenges in natural language learning. It has been argued that similarity plays an important role in word acquisition (Gentner, 1982). Identifying similar words is an initial step in learning the definition of a word. This paper presents a method for making this first step. </context>
</contexts>
<marker>Nida, 1975</marker>
<rawString>Eugene A. Nida. 1975. Componential Analysis of Meaning. The Hague, Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional Clustering of English Words.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL93,</booktitle>
<pages>183--190</pages>
<institution>Ohio State University,</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="17376" citStr="Pereira et al., 1993" startWordPosition="2888" endWordPosition="2891">f its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a new evaluation methodology for automatically constructed thesaurus. While previous methods rely on indirect tasks or subje</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional Clustering of English Words. In Proceedings of ACL93, pages 183-190, Ohio State University, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerda Ruge</author>
</authors>
<title>Experiments on linguistically based term associations.</title>
<date>1992</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>28--3</pages>
<contexts>
<context position="16987" citStr="Ruge, 1992" startWordPosition="2829" endWordPosition="2830">ition 0.25 0.10 task 0.10 0.10 I___chore 0.11 0.07 operation 0.10 0.10 I___function 0.10 0.08 I___mission 0.12 0.07 I I._patrol 0.07 0.07 I staff 0.10 0.07 penalty 0.09 0.09 I fee 0.17 0.08 I___tariff 0.13 0.08 I_tax 0.19 0.07 reservist 0.07 0.07 Figure 3: Similarity tree for &amp;quot;duty&amp;quot; Inspection of sample outputs shows that this algorithm works well. However, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associate</context>
</contexts>
<marker>Ruge, 1992</marker>
<rawString>Gerda Ruge. 1992. Experiments on linguistically based term associations. Information Processing &amp; Management, 28(3):317-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<booktitle>Xtract. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<contexts>
<context position="17256" citStr="Smadja, 1993" startWordPosition="2871" endWordPosition="2872"> tree for &amp;quot;duty&amp;quot; Inspection of sample outputs shows that this algorithm works well. However, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a ne</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Frank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143-178.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>