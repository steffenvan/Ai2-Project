<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.971526">
Unsupervised Translation Sense Clustering
</title>
<author confidence="0.571423">
Mohit Bansal* John DeNero Dekang Lin
</author>
<affiliation confidence="0.349382">
UC Berkeley Google Google
</affiliation>
<email confidence="0.992686">
mbansal@cs.berkeley.edu denero@google.com lindek@google.com
</email>
<sectionHeader confidence="0.993753" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894058823529">
We propose an unsupervised method for clus-
tering the translations of a word, such that
the translations in each cluster share a com-
mon semantic sense. Words are assigned to
clusters based on their usage distribution in
large monolingual and parallel corpora using
the soft K-Means algorithm. In addition to de-
scribing our approach, we formalize the task
of translation sense clustering and describe a
procedure that leverages WordNet for evalu-
ation. By comparing our induced clusters to
reference clusters generated from WordNet,
we demonstrate that our method effectively
identifies sense-based translation clusters and
benefits from both monolingual and parallel
corpora. Finally, we describe a method for an-
notating clusters with usage examples.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.885705904761905">
The ability to learn a bilingual lexicon from a
parallel corpus was an early and influential area
of success for statistical modeling techniques in
natural language processing. Probabilistic word
alignment models can induce bilexical distributions
over target-language translations of source-language
words (Brown et al., 1993). However, word-to-word
correspondences do not capture the full structure of
a bilingual lexicon. Consider the example bilingual
dictionary entry in Figure 1; in addition to enumerat-
ing the translations of a word, the dictionary author
has grouped those translations into three sense clus-
ters. Inducing such a clustering would prove use-
ful in generating bilingual dictionaries automatically
or building tools to assist bilingual lexicographers.
*Author was a summer intern with Google Research while
conducting this research project.
Colocar [co-lo-car´], va. 1. To arrange, to put in
due place or order. 2. To place, to put in any place,
rank condition or office, to provide a place or em-
ployment. 3. To collocate, to locate, to lay.
</bodyText>
<figureCaption confidence="0.98339425">
Figure 1: This excerpt from a bilingual dictionary groups
English translations of the polysemous Spanish word colocar
into three clusters that correspond to different word senses
(Vel´azquez de la Cadena et al., 1965).
</figureCaption>
<bodyText confidence="0.999964846153846">
This paper formalizes the task of clustering a set
of translations by sense, as might appear in a pub-
lished bilingual dictionary, and proposes an unsu-
pervised method for inducing such clusters. We also
show how to add usage examples for the translation
sense clusters, hence providing complete structure
to a bilingual dictionary.
The input to this task is a set of source words and
a set of target translations for each source word. Our
proposed method clusters these translations in two
steps. First, we induce a global clustering of the en-
tire target vocabulary using the soft K-Means algo-
rithm, which identifies groups of words that appear
in similar contexts (in a monolingual corpus) and are
translated in similar ways (in a parallel corpus). Sec-
ond, we derive clusters over the translations of each
source word by projecting the global clusters.
We evaluate these clusters by comparing them to
reference clusters with the overlapping BCubed met-
ric (Amigo et al., 2009). We propose a clustering cri-
terion that allows us to derive reference clusters from
the synonym groups of WordNet @ (Miller, 1995).1
Our experiments using Spanish-English and
Japanese-English datasets demonstrate that the au-
tomatically generated clusters produced by our
method are substantially more similar to the
</bodyText>
<footnote confidence="0.999777">
1WordNet is used only for evaluation; our sense clustering
method is fully unsupervised and language-independent.
</footnote>
<page confidence="0.956212">
773
</page>
<note confidence="0.684042857142857">
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773–782,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
Sense cluster WordNet sense description Usage example
collocate group or chunk together in a certain colocar juntas todas los libros
order or place side by side collocate all the books
invest, place, put make an investment capitales para colocar
capital to invest
</note>
<bodyText confidence="0.318097333333333">
locate, place assign a location to colocar el n´umero de serie
locate the serial number
place, position, put put into a certain place or abstract colocar en un lugar
</bodyText>
<equation confidence="0.445486">
location put in a place
</equation>
<figureCaption confidence="0.998075666666667">
Figure 2: Correct sense clusters for the translations of Spanish verb s = colocar, assuming that it has translation set T. =
{collocate, invest, locate, place, position, put}. Only the sense clusters are outputs of the translation sense clustering task; the
additional columns are presented for clarity.
</figureCaption>
<bodyText confidence="0.999724625">
WordNet-based reference clusters than naive base-
lines. Moreover, we show that bilingual features
collected from parallel corpora improve clustering
accuracy over monolingual distributional similarity
features alone.
Finally, we present a method for annotating clus-
ters with usage examples, which enrich our automat-
ically generated bilingual dictionary entries.
</bodyText>
<sectionHeader confidence="0.988153" genericHeader="introduction">
2 Task Description
</sectionHeader>
<bodyText confidence="0.997493">
We consider a three-step pipeline for generating
structured bilingual dictionary entries automatically.
</bodyText>
<listItem confidence="0.978630666666667">
(1) The first step is to identify a set of high-quality
target-side translations for source lexical items. In
our experiments, we ask bilingual human annota-
tors to create these translation sets.2 We restrict our
present study to word-level translations, disallowing
multi-word phrases, in order to leverage existing lex-
ical resources for evaluation.
(2) The second step is to cluster translations of each
word according to common word senses. This clus-
tering task is the primary focus of the paper, and we
formalize it in this section.
(3) The final step annotates clusters with usage ex-
amples to enrich the structure of the output. Sec-
tion 7 describes a method of identifying cluster-
specific usage examples.
</listItem>
<bodyText confidence="0.977597333333333">
In the task of translation sense clustering, the
second step, we assume a fixed set of source lexi-
cal items of interest 5, each with a single part of
</bodyText>
<footnote confidence="0.993005">
2We do not use automatically extracted translation sets in
our experiments, in order to isolate the clustering task on clean
input.
</footnote>
<bodyText confidence="0.9639435">
speech3, and for each s E 5 a set T3 of target trans-
lations. Moreover, we assume that each target word
t E T3 has a set of senses in common with s. These
senses may also be shared among different target
words. That is, each target word may have multiple
senses and each sense may be expressed by multiple
words.
Given a translation set T3, we define a cluster G C_
T3 to be a correct sense cluster if it is both coherent
and complete.
</bodyText>
<listItem confidence="0.988918571428571">
• A sense cluster G is coherent if and only if
there exists some sense B shared by all of the
target words in G.
• A sense cluster G is complete if and only if, for
every sense B shared by all words in G, there
is no other word in T3 but not in G that also
shares that sense.
</listItem>
<bodyText confidence="0.999211533333333">
The full set of correct clusters for a set of translations
consists of all sense clusters that are both coherent
and complete.
The example translation set for the Spanish word
colocar in Figure 2 is shown with four correct sense
clusters. For descriptive purposes, these clusters are
annotated by WordNet senses and bilingual usage
examples. However, the task we have defined does
not require the WordNet sense or usage example
to be identified: we must only produce the correct
sense clusters within a set of translations. In fact, a
cluster may correspond to more than one sense.
Our definition of correct sense clusters has sev-
eral appealing properties. First, we do not attempt
to enumerate all senses of the source word. Sense
</bodyText>
<footnote confidence="0.8827275">
3A noun and verb that share the same word form would con-
stitute two different source lexical items.
</footnote>
<page confidence="0.996834">
774
</page>
<figure confidence="0.9734492">
Notation
T3 : The set of target-language translations (given)
Dt : The set of synsets in which t appears (given)
C : A synset; a set of target-language words
B : A source-specific synset; a subset of T3
B : A set of source-specific synsets
G : A set of correct sense clusters for T3
The Cluster Projection Algorithm:
B ← {C ∩ T3 : C ∈ � }
t�Ts Dt
G ← ∅
for B ∈ B do
if �B&apos; ∈ B such that B ⊂ B&apos; then
add B to G
return G
</figure>
<figureCaption confidence="0.6163226">
Figure 3: The Cluster Projection (CP) algorithm projects
language-level synsets (C) to source-specific synsets (B) and
then filters the set of synsets for redundant subsets to produce
the complete set of source-specific synsets that are both coher-
ent and complete (9).
</figureCaption>
<bodyText confidence="0.998749764705882">
distinctions are only made when they affect cross-
lingual lexical choice. If a source word has many
fine-grained senses but translates in the same way
regardless of the sense intended, then there is only
one correct sense cluster for that translation.
Second, no correct sense cluster can be a super-
set of another, because the subset would violate the
completeness condition. This criterion encourages
larger clusters that are easier to interpret, as their
unifying senses can be identified as the intersection
of senses of the translations in the cluster.
Third, the correct clusters need not form a parti-
tion of the input translations. It is common in pub-
lished bilingual dictionaries for a translation to ap-
pear in multiple sense clusters. In our example, the
polysemous English verbs place and put appear in
multiple clusters.
</bodyText>
<sectionHeader confidence="0.956105" genericHeader="method">
3 Generating Reference Clusters
</sectionHeader>
<bodyText confidence="0.998036">
To construct a reference set for the translation
sense clustering task, we first collected English
translations of Spanish and Japanese nouns, verbs,
and adverbs. Translation sets were curated by hu-
man annotators to keep only high-quality single-
word translations.
Rather than gathering reference clusters via an ad-
ditional annotation effort, we leverage WordNet, a
large database of English lexical semantics (Miller,
1995). WordNet groups words into sets of cogni-
</bodyText>
<figure confidence="0.7544719">
Words
collocate
invest
locate
Synsets
collocate
collocate, lump, chunk
invest, put, commit, place
invest, clothe, adorn
invest, vest, enthrone
É
locate, turn up
situate, locate
locate, place, site
É
Sense Clusters
collocate
invest, place, put
locate, place
place, position, put
</figure>
<figureCaption confidence="0.638976125">
place put, set, place, pose, position, lay
position rate, rank, range, order, grade, place
put locate, place, site
invest, put, commit, place
É
position
put, set, place, pose, position, lay
put, set, place, pose, position, lay
put
frame, redact, cast, put, couch
invest, put, commit, place
Figure 4: An example of cluster projection on WordNet, for the
Spanish source word colocar. We show the target translation
words to be clustered, their WordNet synsets (with words not in
the translation set grayed out), and the final set of correct sense
clusters.
</figureCaption>
<bodyText confidence="0.979995739130435">
tive synonyms called synsets, each expressing a dis-
tinct concept. We use WordNet version 2.1, which
has wide coverage of nouns, verbs, and adverbs, but
sparser coverage of adjectives and prepositions.4
Reference clusters for the set of translations T3
of some source word s are generated algorithmi-
cally from WordNet synsets via the Cluster Projec-
tion (CP) algorithm defined in Figure 3. An input
to the CP algorithm is the translation set T3 of some
source word s. Also, each translation t ∈ T3 be-
longs to some set of synsets Dt, where each synset
C ∈ Dt contains target-language words that may
or may not be translations of s. First, the CP algo-
rithm constructs a source-specific synset B for each
C, which contains only translations of s. Second,
it identifies all correct sense clusters G that are both
coherent and complete with respect to the source-
specific senses B. A sense cluster must correspond
to some synset B ∈ B to be coherent, and it must
4WordNet version 2.1 is almost identical to ver-
sion 3.0, for Unix-like systems, as described in
http://wordnetcode.princeton.edu/3.0/CHANGES. The lat-
est version 3.1 is not yet available for download.
</bodyText>
<page confidence="0.990777">
775
</page>
<bodyText confidence="0.99994725">
not have a proper superset in B to be complete.5
Figure 4 illustrates the CP algorithm for the trans-
lations of the Spanish source word colocar that ap-
pear in our input dataset.
</bodyText>
<sectionHeader confidence="0.5116" genericHeader="method">
4 Clustering with K-Means
</sectionHeader>
<bodyText confidence="0.99996525">
In this section, we describe an unsupervised method
for inducing translation sense clusters from the us-
age statistics of words in large monolingual and par-
allel corpora. Our method is language independent.
</bodyText>
<subsectionHeader confidence="0.993398">
4.1 Distributed Soft K-Means Clustering
</subsectionHeader>
<bodyText confidence="0.999964615384615">
As a first step, we cluster all words in the target-
language vocabulary in a way that relates words that
have similar distributional features. Several methods
exist for this task, such as the K-Means algorithm
(MacQueen, 1967), the Brown algorithm (Brown
et al., 1992) and the exchange algorithm (Kneser
and Ney, 1993; Martin et al., 1998; Uszkoreit and
Brants, 2008). We use a distributed implementa-
tion of the “soft” K-Means clustering algorithm de-
scribed in Lin and Wu (2009). Given a feature vec-
tor for each element (a word type) and the number
of desired clusters K, the K-Means algorithm pro-
ceeds as follows:
</bodyText>
<listItem confidence="0.9273311">
1. Select K elements as the initial centroids for
K clusters.
repeat
2. Assign each element to the top M clusters
with the nearest centroid, according to a simi-
larity function in feature space.
3. Recompute each cluster’s centroid by aver-
aging the feature vectors of the elements in that
cluster.
until convergence
</listItem>
<subsectionHeader confidence="0.998226">
4.2 Monolingual Features
</subsectionHeader>
<bodyText confidence="0.998320823529412">
Following Lin and Wu (2009), each word to be clus-
tered is represented as a feature vector describing the
distributional context of that word. In our setup, the
5One possible shortcoming of our approach to constructing
reference sets for translation sense clustering is that a cluster
may correspond to a sense that is not shared by the original
source word used to generate the translation set. All translations
must share some sense with the source word, but they may not
share all senses with the source word. It is possible that two
translations are synonymous in a sense that is not shared by the
source. However, we did not observe this problem in practice.
context of a word w consists of the words immedi-
ately to the left and right of w. The context feature
vector of w is constructed by first aggregating the
frequency counts of each word f in the context of
each w. We then compute point-wise mutual infor-
mation (PMI) features from the frequency counts:
</bodyText>
<equation confidence="0.778826333333333">
c(w, f)
PMI(w, f) = log
c(w)c(f)
</equation>
<bodyText confidence="0.99988425">
where w is a word, f is a neighboring word, and
c(·) is the count of a word or word pair in the cor-
pus.6 A feature vector for w contains a PMI feature
for each word type f (with relative position left or
right) for all words that appears a sufficient number
of times as a neighbor of w. The similarity of two
feature vectors is the cosine of the angle between the
vectors. We follow Lin and Wu (2009) in applying
various thresholds during K-Means, such as a fre-
quency threshold for the initial vocabulary, a total-
count threshold for the feature vectors, and a thresh-
old for PMI scores.
</bodyText>
<subsectionHeader confidence="0.999086">
4.3 Bilingual Features
</subsectionHeader>
<bodyText confidence="0.999363142857143">
In addition to the features described in Lin and Wu
(2009), we introduce features from a bilingual par-
allel corpus that encode reverse-translation informa-
tion from the source-language (Spanish or Japanese
in our experiments). We have two types of bilin-
gual features: unigram features capture source-side
reverse-translations of w, while bigram features cap-
ture both the reverse-translations and source-side
neighboring context words to the left and right. Fea-
tures are expressed again as PMI computed from
frequency counts of aligned phrase pairs in a par-
allel corpus. For example, one unigram feature for
place would be the PMI computed from the number
of times that place was in the target side of a phrase
pair whose source side was the unigram lugar. Sim-
ilarly, a bigram feature for place would be the PMI
computed from the number of times that place was
in the target side of a phrase pair whose source side
was the bigram lugar de. These features characterize
the way in which a word is translated, an indication
of its meaning.
</bodyText>
<footnote confidence="0.9988805">
6PMI is typically defined in terms of probabilities, but has
proven effective previously when defined in terms of counts.
</footnote>
<page confidence="0.989705">
776
</page>
<subsectionHeader confidence="0.998229">
4.4 Predicting Translation Clusters
</subsectionHeader>
<bodyText confidence="0.999400333333333">
As a result of soft K-Means clustering, each word in
the target-language vocabulary is assigned to a list of
up to M clusters. To predict the sense clusters for a
set of translations of a source word, we apply the CP
algorithm (Figure 3), treating the K-Means clusters
as synsets (Dt).
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999883916666667">
To our knowledge, the translation sense clustering
task has not been explored previously. However,
much prior work has explored the related task of
monolingual word and phrase clustering. Uszkor-
eit and Brants (2008) uses an exchange algorithm
to cluster words in a language model, Lin and Wu
(2009) uses distributed K-Means to cluster phrases
for various discriminative classification tasks, Vla-
chos et al. (2009) uses Dirichlet Process Mixture
Models for verb clustering, and Sun and Korhonen
(2011) uses a hierarchical Levin-style clustering to
cluster verbs.
Previous word sense induction work (Diab and
Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis
et al., 2004; Apidianaki, 2009) relates to our work
in that these approaches discover word senses au-
tomatically through clustering, even using multilin-
gual parallel corpora. However, our task of clus-
tering multiple words produces a different type of
output from the standard word sense induction task
of clustering in-context uses of a single word. The
underlying notion of “sense” is shared across these
tasks, but the way in which we use and evaluate in-
duced senses is novel.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99980375">
The purpose of our experiments is to assess whether
our unsupervised soft K-Means clustering method
can effectively recover the reference sense clusters
derived from WordNet.
</bodyText>
<subsectionHeader confidence="0.953652">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9997254">
We conduct experiments using two bilingual
datasets: Spanish-to-English (S→E) and Japanese-
to-English (J→E). Table 1 shows, for each dataset,
the number of source words and the total number
of target words in their translation sets. The datasets
</bodyText>
<table confidence="0.902833333333333">
Dataset No. of src-words Total no. of tgt-words
S→E 52 230
J→E 369 1639
</table>
<tableCaption confidence="0.9797995">
Table 1: Sizes of the Spanish-to-English (SSE) and Japanese-
to-English (JOE) datasets.
</tableCaption>
<bodyText confidence="0.999640541666667">
are limited in size because we solicited human anno-
tators to filter the set of translations for each source
word. The S→E dataset has 52 source-words with a
part-of-speech-tag distribution of 38 nouns, 10 verbs
and 4 adverbs. The J→E dataset has 369 source-
words with 319 nouns, 38 verbs and 12 adverbs. We
included only these parts of speech because Word-
Net version 2.1 has adequate coverage for them.
Most source words have 3 to 5 translations each.
Monolingual features for K-Means clustering
were computed from an English corpus of Web
documents with 700 billion tokens of text. Bilin-
gual features were computed from 0.78 (S→E) and
1.04 (J→E) billion tokens of parallel text, primar-
ily extracted from the Web using automated paral-
lel document identification (Uszkoreit et al., 2010).
Word alignments were induced from the HMM-
based alignment model (Vogel et al., 1996), initial-
ized with the bilexical parameters of IBM Model 1
(Brown et al., 1993). Both models were trained us-
ing 2 iterations of the expectation maximization al-
gorithm. Phrase pairs were extracted from aligned
sentence pairs in the same manner used in phrase-
based machine translation (Koehn et al., 2003).
</bodyText>
<subsectionHeader confidence="0.999826">
6.2 Clustering Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9998888">
The quality of text clustering algorithms can be eval-
uated using a wide set of metrics. For evaluation
by set matching, the popular measures are Purity
(Zhao and Karypis, 2001) and Inverse Purity and
their harmonic mean (F measure, see Van Rijsber-
gen (1974)). For evaluation by counting pairs, the
popular metrics are the Rand Statistic and Jaccard
Coefficient (Halkidi et al., 2001; Meila, 2003).
Metrics based on entropy include Cluster Entropy
(Steinbach et al., 2000), Class Entropy (Bakus et al.,
2002), VI-measure (Meila, 2003), Qo (Dom, 2001),
V-measure (Rosenberg and Hirschberg, 2007) and
Mutual Information (Xu et al., 2003). Lastly, there
exist the BCubed metrics (Bagga and Baldwin,
1998), a family of metrics that decompose the clus-
</bodyText>
<page confidence="0.993213">
777
</page>
<bodyText confidence="0.995566921052631">
tering evaluation by estimating precision and recall
for each item in the distribution.
Amigo et al. (2009) compares the various clus-
tering metrics mentioned above and their properties.
They define four formal but intuitive constraints on
such metrics that explain which aspects of clustering
quality are captured by the different metric families.
Their analysis shows that of the wide range of met-
rics, only BCubed satisfies those constraints. After
defining each constraint below, we briefly describe
its relevance to the translation sense clustering task.
Homogeneity: In a cluster, we should not mix items
belonging to different categories.
Relevance: All words in a proposed cluster should
share some common WordNet sense.
Completeness: Items belonging to the same cate-
gory should be grouped in the same cluster.
Relevance: All words that share some common
WordNet sense should appear in the same cluster.
Rag Bag: Introducing disorder into a disordered
cluster is less harmful than introducing disorder into
a clean cluster.
Relevance: We prefer to maximize the number of
error-free clusters, because these are most easily in-
terpreted and therefore most useful.
Cluster Size vs. Quantity: A small error in a big
cluster is preferable to a large number of small er-
rors in small clusters.
Relevance: We prefer to minimize the total number
of erroneous clusters in a dictionary.
Amigo et al. (2009) also show that BCubed ex-
tends cleanly to settings with overlapping clusters,
where an element can simultaneously belong to
more than one cluster. For these reasons, we focus
on BCubed for cluster similarity evaluation.7
The BCubed metric for scoring overlapping clus-
ters is computed from the pair-wise precision and
recall between pairs of items:
</bodyText>
<equation confidence="0.999741">
P(e, e0) = min(|C(e) ∩ C(e0)|, |L(e) ∩ L(e0)|)
|C(e) ∩ C(e0)|
R(e, e0) = min(|C(e) ∩ C(e0)|, |L(e) ∩ L(e0)|)
|L(e) ∩ L(e0)|
</equation>
<bodyText confidence="0.9991105">
where e and e0 are two items, L(e) is the set of ref-
erence clusters for e and C(e) is the set of predicted
</bodyText>
<footnote confidence="0.870173333333333">
7An evaluation using purity and inverse purity (extended to
overlapping clusters) has been omitted for space, but leads to
the same conclusions as the evaluation using BCubed.
</footnote>
<bodyText confidence="0.999448111111111">
clusters for e (i.e., clusters to which e belongs). Note
that P(e, e0) is defined only when e and e0 share
some predicted cluster, and R(e, e0) when e and e0
share some reference cluster.
The BCubed precision associated to one item is its
averaged pair-wise precision over other items shar-
ing some of its predicted clusters, and likewise for
recall8; and the overall BCubed precision (or recall)
is the averaged precision (or recall) of all items:
</bodyText>
<equation confidence="0.999803">
PB3 = Avge[Avge&apos;s.t.C(e)∩C(e&apos;)6�∅[P(e, e0)]]
RB3 = Avge[Avge&apos;s.t.L(e)∩L(e&apos;)6�∅[R(e, e0)]]
</equation>
<subsectionHeader confidence="0.56021">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.921303">
Figure 5 shows the F,3-score for various β values:
</bodyText>
<equation confidence="0.993930666666667">
= (1+02)
1+02) · PB3 · RB3
F,3 β2 · PB3 + RB3
</equation>
<bodyText confidence="0.966990615384615">
This graph gives us a trade-off between precision
and recall (β = 0 is exact precision and β → ∞
tends to exact recall).9
Each curve in Figure 5 represents a particular
clustering method. We include three naive baselines:
ewnc: Each word in its own cluster
aw1c: All words in one cluster
Random: Each target word is assigned M random
cluster id’s in the range 1 to K, then translation
sets are clustered with the CP algorithm.
The curves for K-Means clustering include one
condition with monolingual features alone and two
curves that include bilingual features as well.10 The
bilingual curves correspond to two different feature
sets: the first includes only unigram features (t1),
while the second includes both unigram and bigram
features (t1t2).
Each point on an F3 curve in Figure 5 (including
the baseline curves) represents a maximum over two
8The metric does include in this computation the relation of
each item with itself.
9Note that we use the micro-averaged version of F-score
where we first compute PB3 and RB3 for each source-word,
then compute the average PB3 and RB3 over all source-words,
and finally compute the F-score using these averaged PB3 and
RB3.
</bodyText>
<footnote confidence="0.558552666666667">
10All bilingual K-Means experiments include monolingual
features also. K-Means with only bilingual features does not
produce accurate clusters.
</footnote>
<page confidence="0.986803">
778
</page>
<figure confidence="0.999384368421053">
FR score
0.95
0.85
0.75
0.65
0.9
0.8
0.7
Spanish-English BCubed Results
ewnc
aw1c
Random
Kmeans-monolingual
Kmeans-bilingual-t1
Kmeans-bilingual-t1t2
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
R
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
R
</figure>
<figureCaption confidence="0.999765">
Figure 5: BCubed Fp plot for the Spanish-English dataset (top) and Japanese-English dataset (bottom).
</figureCaption>
<table confidence="0.999310666666667">
Source word: ayudar
Monolingual [[aid], [assist, help]] P=1.0, R=0.56
Bilingual [[aid, assist, help]] P=1.0, R=1.0
Source word: concurso
Monolingual [[competition, contest, match], [concourse], [contest, meeting]] P=0.58, R=1.0
Bilingual [[competition, contest], [concourse], [match], [meeting]] P=1.0, R=1.0
</table>
<tableCaption confidence="0.8451345">
Table 2: Examples showing improvements in clustering when we move from K-Means clustering with only monolingual features
to clustering with additional bilingual features.
</tableCaption>
<figure confidence="0.9908104">
FR score
0.95
0.85
0.75
0.9
0.8
0.7
Japanese-English BCubed Results
ewnc
aw1c
Random
Kmeans-monolingual
Kmeans-bilingual-t1
Kmeans-bilingual-t1t2
779
Recall
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Precision
Japanese-English BCubed Results
0.99
0.94
Random
Kmeans-monolingual
Kmeans-bilingual-t1
Kmeans-bilingual-t1t2
ewnc
aw1c
0.89
0.84
0.79
</figure>
<figureCaption confidence="0.9978895">
Figure 6: BCubed Precision-Recall scatter plot for the Japanese-English dataset. Each point represents a particular choice of cluster
count K and clusters per word M.
</figureCaption>
<bodyText confidence="0.999924095238095">
parameters: K, the number of clusters created in the
whole corpus and M, the number of clusters allowed
per word (in M-best soft K-Means). As both the
random baseline and proposed clustering methods
can be tuned to favor precision or recall, we show
the best result from each technique across this spec-
trum of F,a metrics. We vary Q to highlight different
potential objectives of translation sense clustering.
An application that focuses on synonym discovery
would favor recall, while an application portraying
highly granular sense distinctions would favor pre-
cision.
Clustering accuracy improves over the baselines
with monolingual features alone, and it improves
further with the addition of bilingual features, for a
wide range of Q values. Our unsupervised approach
with bilingual features achieves up to 6-8% absolute
improvement over the random baseline, and is par-
ticularly effective for recall-weighted metrics.11 As
an example, in a S—*E experiment with a K-Means
setting of K = 4096 : M = 3, the overall F1.5 score
</bodyText>
<footnote confidence="0.733851875">
11It is not surprising that a naive baseline like random clus-
tering can achieve a high precision: BCubed counts each word
itself as correctly clustered, and so even trivial techniques that
create many singleton clusters will have high precision. High
recall (without very low precision) is harder to achieve, because
it requires positing larger clusters, and it is for recall-focused
objectives that our technique substantially outperforms the ran-
dom baseline.
</footnote>
<bodyText confidence="0.9998645">
increases from 80.58% to 86.12% upon adding bilin-
gual features. Table 2 shows two examples from that
experiment for which bilingual features improve the
output clusters.
The parameter values we use in our experiments
are K E 123, 24, ... , 2121 and M E 11, 2, 3, 4, 51.
To provide additional detail, Figure 6 shows the
BCubed precision and recall for each induced clus-
tering, as the values of K and M vary, for Japanese-
English.12 Each point in this scatter plot represents a
clustering methodology and a particular value for K
and M. Soft K-Means with bilingual features pro-
vides the strongest performance across a broad range
of cluster parameters.
</bodyText>
<subsectionHeader confidence="0.978498">
6.4 Evaluation Details
</subsectionHeader>
<bodyText confidence="0.999921285714286">
Certain special cases needed to be addressed in order
to complete this evaluation.
Target words not in WordNet: Words that did not
have any synset in WordNet were each assigned to a
singleton reference cluster.13 The S—*E dataset has
only 2 out of 225 target types missing in WordNet
and the J—*E dataset has only 55 out of 1351 target
</bodyText>
<footnote confidence="0.9516714">
12Spanish-English precision-recall results are omitted due to
space constraints, but depict similar trends.
13Note that certain words with WordNet synsets also end up
in their own singleton cluster because all other words in their
cluster are not in the translation set.
</footnote>
<page confidence="0.994035">
780
</page>
<bodyText confidence="0.997277090909091">
types missing.
Target words not clustered by K-Means: The K-
Means algorithm applies various thresholds during
different parts of the process. As a result, there
are some target word types that are not assigned
any cluster at the end of the algorithm. For ex-
ample, in the J—*E experiment with K = 4096
and with bilingual (t1 only) features, only 49 out
of 1351 target-types are not assigned any cluster by
K-Means. These unclustered words were each as-
signed to a singleton cluster in post-processing.
</bodyText>
<sectionHeader confidence="0.98918" genericHeader="method">
7 Identifying Usage Examples
</sectionHeader>
<bodyText confidence="0.999888083333334">
We now briefly consider the task of automatically
extracting usage examples for each predicted clus-
ter. We identify these examples among the extracted
phrase pairs of a parallel corpus.
Let P3 be the set of source phrases containing
source word s, and let At be the set of source phrases
that align to target phrases containing target word
t. For a source word s and target sense cluster G,
we identify source phrases that contain s and trans-
late to all words in G. That is, we collect the set
of phrases P3 n ntCG At. We use the same parallel
corpus as we used to compute bilingual features.
For example, if we consider the cluster [place, po-
sition, put] for the Spanish word colocar, then we
find Spanish phrases that contain colocar and also
align to English phrases containing place, position,
and put somewhere in the parallel corpus. Sample
usage examples extracted by this approach appear in
Figure 7. We have not performed a quantitative eval-
uation of these extracted examples, although quali-
tatively we have found that the technique surfaces
useful phrases. We look forward to future research
that further explores this important sub-task of auto-
matically generating bilingual dictionaries.
</bodyText>
<sectionHeader confidence="0.997664" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999486125">
We presented the task of translation sense clustering,
a critical second step to follow translation extraction
in a pipeline for generating well-structured bilingual
dictionaries automatically. We introduced a method
of projecting language-level clusters into clusters for
specific translation sets using the CP algorithm. We
used this technique both for constructing reference
clusters, via WordNet synsets, and constructing pre-
</bodyText>
<figure confidence="0.95226052173913">
debajo
[&amp;quot;below&amp;quot;,&amp;quot;beneath&amp;quot;] → debajo de la superficie (below the surface)
[&amp;quot;below&amp;quot;,&amp;quot;under&amp;quot;] → debajo de la l’nea (below the line)
[&amp;quot;underneath&amp;quot;] → debajo de la piel (under the skin)
*�
[&amp;quot;break&amp;quot;] → — Floy k b-G * T.5 05 61 4H1,g TT .
(I worked hard and I deserve a good break.)
[&amp;quot;recreation&amp;quot;] → �� 05 8 -V- * )&apos;&amp;
(Traditional healing and recreation activities)
[&amp;quot;rest&amp;quot;] → �rF T *4 T.5 i&apos;:It T 8h 1T .
(Bed rest is the only treatment required.)
V1A
[&amp;quot;application&amp;quot;] → n/ts—3+— V1A WN
(Computer-aided technique)
[&amp;quot;use&amp;quot;,&amp;quot;utilization&amp;quot;] → ±4 05 4M V1A �k 0� T.5
(Promote effective use of land)
91C
[&amp;quot;draw&amp;quot;,&amp;quot;pull&amp;quot;] → ji— r/ �k 91C
(Draw the curtain)
[&amp;quot;subtract&amp;quot;] → A b-G B �k 91C
(Subtract B from A)
[&amp;quot;tug&amp;quot;] → M ;k Coyt 91C
(Tug at someone&apos;s sleeve)
</figure>
<figureCaption confidence="0.997171">
Figure 7: Usage examples for Spanish and Japanese words and
their English sense clusters. Our approach extracts multiple
examples per cluster, but we show only one. We also show
the translation of the examples back into English produced by
Google Translate.
</figureCaption>
<bodyText confidence="0.999469722222222">
dicted clusters from the output of a vocabulary-level
clustering algorithm.
Our experiments demonstrated that the soft K-
Means clustering algorithm, trained using distribu-
tional features from very large monolingual and
bilingual corpora, recovered a substantial portion of
the structure of reference clusters, as measured by
the BCubed clustering metric. The addition of bilin-
gual features improved clustering results over mono-
lingual features alone; these features could prove
useful for other clustering tasks as well. Finally, we
annotated our clusters with usage examples.
In future work, we hope to combine our cluster-
ing method with a system for automatically gen-
erating translation sets. In doing so, we will de-
velop a system that can automatically induce high-
quality, human-readable bilingual dictionaries from
large corpora using unsupervised learning methods.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999848666666666">
We would like to thank Jakob Uszkoreit, Adam
Pauls, and the anonymous reviewers for their helpful
suggestions.
</bodyText>
<page confidence="0.996611">
781
</page>
<sectionHeader confidence="0.995855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998772857142857">
Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461486.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of EACL.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of COLING-ACL.
J. Bakus, M. F. Hussin, and M. Kamel. 2002. A SOM-
based document clustering using phrases. In Proceed-
ings of ICONIP.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
B.E. Dom. 2001. An information-theoretic external
cluster-validity measure. In IBM Technical Report RJ-
10219.
M. Halkidi, Y. Batistakis, and M. Vazirgiannis. 2001. On
clustering validation techniques. Journal of Intelligent
Information Systems, 17(2-3):107–145.
Hiroyuki Kaji. 2003. Word sense acquisition from bilin-
gual comparable corpora. In Proceedings of NAACL.
Reinherd Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings ofACL.
J. B. MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of 5th Berkeley Symposium on Mathematical
Statistics and Probability.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19–37.
M. Meila. 2003. Comparing clusterings by the variation
of information. In Proceedings of COLT.
George A. Miller. 1995. Wordnet: A lexical database for
English. In Communications of the ACM.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of EMNLP.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. In Proceedings of KDD Workshop on Text
Mining.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP.
Dan Tufis, Radu Ion, and Nancy Ide. 2004. Fine-grained
word sense disambiguation based on parallel corpora,
word alignment, word clustering and aligned word-
nets. In Proceedings of COLING.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of COLING.
C. Van Rijsbergen. 1974. Foundation of evaluation.
Journal of Documentation, 30(4):365–373.
Mariano Vel´azquez de la Cadena, Edward Gray, and
Juan L. Iribas. 1965. New Revised Vel´azques Spanish
and English Dictionary. Follet Publishing Company.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained Dirichlet
process mixture models for verb clustering. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
W. Xu, X. Liu, and Y. Gong. 2003. Document-clustering
based on non-negative matrix factorization. In Pro-
ceedings of SIGIR.
Y. Zhao and G. Karypis. 2001. Criterion functions for
document clustering: Experiments and analysis. In
Technical Report TR 01-40, Department of Computer
Science, University of Minnesota, Minneapolis, MN.
</reference>
<page confidence="0.99741">
782
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762914">
<title confidence="0.998894">Unsupervised Translation Sense Clustering</title>
<author confidence="0.887999">DeNero Dekang Lin UC Berkeley Google Google</author>
<email confidence="0.995898">mbansal@cs.berkeley.edudenero@google.comlindek@google.com</email>
<abstract confidence="0.998910333333333">We propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using soft algorithm. In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation. By comparing our induced clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora. Finally, we describe a method for annotating clusters with usage examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amigo</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="3194" citStr="Amigo et al., 2009" startWordPosition="485" endWordPosition="488"> a set of source words and a set of target translations for each source word. Our proposed method clusters these translations in two steps. First, we induce a global clustering of the entire target vocabulary using the soft K-Means algorithm, which identifies groups of words that appear in similar contexts (in a monolingual corpus) and are translated in similar ways (in a parallel corpus). Second, we derive clusters over the translations of each source word by projecting the global clusters. We evaluate these clusters by comparing them to reference clusters with the overlapping BCubed metric (Amigo et al., 2009). We propose a clustering criterion that allows us to derive reference clusters from the synonym groups of WordNet @ (Miller, 1995).1 Our experiments using Spanish-English and Japanese-English datasets demonstrate that the automatically generated clusters produced by our method are substantially more similar to the 1WordNet is used only for evaluation; our sense clustering method is fully unsupervised and language-independent. 773 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773–782, Montr´eal, Canada, June 3</context>
<context position="19949" citStr="Amigo et al. (2009)" startWordPosition="3269" endWordPosition="3272">an Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. After defining each constraint below, we briefly describe its relevance to the translation sense clustering task. Homogeneity: In a cluster, we should not mix items belonging to different categories. Relevance: All words in a proposed cluster should share some c</context>
<context position="21243" citStr="Amigo et al. (2009)" startWordPosition="3473" endWordPosition="3476">uld be grouped in the same cluster. Relevance: All words that share some common WordNet sense should appear in the same cluster. Rag Bag: Introducing disorder into a disordered cluster is less harmful than introducing disorder into a clean cluster. Relevance: We prefer to maximize the number of error-free clusters, because these are most easily interpreted and therefore most useful. Cluster Size vs. Quantity: A small error in a big cluster is preferable to a large number of small errors in small clusters. Relevance: We prefer to minimize the total number of erroneous clusters in a dictionary. Amigo et al. (2009) also show that BCubed extends cleanly to settings with overlapping clusters, where an element can simultaneously belong to more than one cluster. For these reasons, we focus on BCubed for cluster similarity evaluation.7 The BCubed metric for scoring overlapping clusters is computed from the pair-wise precision and recall between pairs of items: P(e, e0) = min(|C(e) ∩ C(e0)|, |L(e) ∩ L(e0)|) |C(e) ∩ C(e0)| R(e, e0) = min(|C(e) ∩ C(e0)|, |L(e) ∩ L(e0)|) |L(e) ∩ L(e0)| where e and e0 are two items, L(e) is the set of reference clusters for e and C(e) is the set of predicted 7An evaluation using </context>
</contexts>
<marker>Amigo, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4):461486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marianna Apidianaki</author>
</authors>
<title>Data-driven semantic analysis for multilingual WSD and lexical selection in translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="16832" citStr="Apidianaki, 2009" startWordPosition="2770" endWordPosition="2771">xplored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively recover the reference s</context>
</contexts>
<marker>Apidianaki, 2009</marker>
<rawString>Marianna Apidianaki. 2009. Data-driven semantic analysis for multilingual WSD and lexical selection in translation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based crossdocument coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="19793" citStr="Bagga and Baldwin, 1998" startWordPosition="3243" endWordPosition="3246">of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. After defining each constraint below, we briefly describe its relevance to the translation sense clusterin</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based crossdocument coreferencing using the vector space model. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bakus</author>
<author>M F Hussin</author>
<author>M Kamel</author>
</authors>
<title>A SOMbased document clustering using phrases.</title>
<date>2002</date>
<booktitle>In Proceedings of ICONIP.</booktitle>
<contexts>
<context position="19600" citStr="Bakus et al., 2002" startWordPosition="3215" endWordPosition="3218">the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their anal</context>
</contexts>
<marker>Bakus, Hussin, Kamel, 2002</marker>
<rawString>J. Bakus, M. F. Hussin, and M. Kamel. 2002. A SOMbased document clustering using phrases. In Proceedings of ICONIP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="12355" citStr="Brown et al., 1992" startWordPosition="2003" endWordPosition="2006">anslations of the Spanish source word colocar that appear in our input dataset. 4 Clustering with K-Means In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the featu</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="1265" citStr="Brown et al., 1993" startWordPosition="174" endWordPosition="177">ed clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora. Finally, we describe a method for annotating clusters with usage examples. 1 Introduction The ability to learn a bilingual lexicon from a parallel corpus was an early and influential area of success for statistical modeling techniques in natural language processing. Probabilistic word alignment models can induce bilexical distributions over target-language translations of source-language words (Brown et al., 1993). However, word-to-word correspondences do not capture the full structure of a bilingual lexicon. Consider the example bilingual dictionary entry in Figure 1; in addition to enumerating the translations of a word, the dictionary author has grouped those translations into three sense clusters. Inducing such a clustering would prove useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers. *Author was a summer intern with Google Research while conducting this research project. Colocar [co-lo-car´], va. 1. To arrange, to put in due place or or</context>
<context position="18833" citStr="Brown et al., 1993" startWordPosition="3093" endWordPosition="3096">s of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16764" citStr="Diab and Resnik, 2002" startWordPosition="2756" endWordPosition="2759">k To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised so</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B E Dom</author>
</authors>
<title>An information-theoretic external cluster-validity measure.</title>
<date>2001</date>
<booktitle>In IBM</booktitle>
<tech>Technical Report RJ10219.</tech>
<contexts>
<context position="19642" citStr="Dom, 2001" startWordPosition="3223" endWordPosition="3224">tion (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metri</context>
</contexts>
<marker>Dom, 2001</marker>
<rawString>B.E. Dom. 2001. An information-theoretic external cluster-validity measure. In IBM Technical Report RJ10219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halkidi</author>
<author>Y Batistakis</author>
<author>M Vazirgiannis</author>
</authors>
<title>On clustering validation techniques.</title>
<date>2001</date>
<journal>Journal of Intelligent Information Systems,</journal>
<pages>17--2</pages>
<contexts>
<context position="19475" citStr="Halkidi et al., 2001" startWordPosition="3196" endWordPosition="3199">ined using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constrain</context>
</contexts>
<marker>Halkidi, Batistakis, Vazirgiannis, 2001</marker>
<rawString>M. Halkidi, Y. Batistakis, and M. Vazirgiannis. 2001. On clustering validation techniques. Journal of Intelligent Information Systems, 17(2-3):107–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Kaji</author>
</authors>
<title>Word sense acquisition from bilingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="16776" citStr="Kaji, 2003" startWordPosition="2760" endWordPosition="2761"> translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means c</context>
</contexts>
<marker>Kaji, 2003</marker>
<rawString>Hiroyuki Kaji. 2003. Word sense acquisition from bilingual comparable corpora. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinherd Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved clustering techniques for class-based statistical language modelling.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="12404" citStr="Kneser and Ney, 1993" startWordPosition="2011" endWordPosition="2014">at appear in our input dataset. 4 Clustering with K-Means In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>Reinherd Kneser and Hermann Ney. 1993. Improved clustering techniques for class-based statistical language modelling. In Proceedings of the 3rd European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="19057" citStr="Koehn et al., 2003" startWordPosition="3129" endWordPosition="3132">700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Ro</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Xiaoyun Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="12565" citStr="Lin and Wu (2009)" startWordPosition="2038" endWordPosition="2041"> statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional con</context>
<context position="14438" citStr="Lin and Wu (2009)" startWordPosition="2376" endWordPosition="2379"> constructed by first aggregating the frequency counts of each word f in the context of each w. We then compute point-wise mutual information (PMI) features from the frequency counts: c(w, f) PMI(w, f) = log c(w)c(f) where w is a word, f is a neighboring word, and c(·) is the count of a word or word pair in the corpus.6 A feature vector for w contains a PMI feature for each word type f (with relative position left or right) for all words that appears a sufficient number of times as a neighbor of w. The similarity of two feature vectors is the cosine of the angle between the vectors. We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a totalcount threshold for the feature vectors, and a threshold for PMI scores. 4.3 Bilingual Features In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). We have two types of bilingual features: unigram features capture source-side reverse-translations of w, while bigram features capture both the reverse-translations </context>
<context position="16443" citStr="Lin and Wu (2009)" startWordPosition="2710" endWordPosition="2713">nslation Clusters As a result of soft K-Means clustering, each word in the target-language vocabulary is assigned to a list of up to M clusters. To predict the sense clusters for a set of translations of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different </context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering for discriminative learning. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B MacQueen</author>
</authors>
<title>Some methods for classification and analysis of multivariate observations.</title>
<date>1967</date>
<booktitle>In Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability.</booktitle>
<contexts>
<context position="12313" citStr="MacQueen, 1967" startWordPosition="1998" endWordPosition="1999">llustrates the CP algorithm for the translations of the Spanish source word colocar that appear in our input dataset. 4 Clustering with K-Means In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each</context>
</contexts>
<marker>MacQueen, 1967</marker>
<rawString>J. B. MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jorg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. Speech Communication,</title>
<date>1998</date>
<pages>24--19</pages>
<contexts>
<context position="12425" citStr="Martin et al., 1998" startWordPosition="2015" endWordPosition="2018"> dataset. 4 Clustering with K-Means In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Mono</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, Jorg Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. Speech Communication, 24:19–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meila</author>
</authors>
<title>Comparing clusterings by the variation of information.</title>
<date>2003</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="19489" citStr="Meila, 2003" startWordPosition="3200" endWordPosition="3201">s of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such met</context>
</contexts>
<marker>Meila, 2003</marker>
<rawString>M. Meila. 2003. Comparing clusterings by the variation of information. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for English.</title>
<date>1995</date>
<booktitle>In Communications of the ACM.</booktitle>
<contexts>
<context position="3325" citStr="Miller, 1995" startWordPosition="509" endWordPosition="510">s. First, we induce a global clustering of the entire target vocabulary using the soft K-Means algorithm, which identifies groups of words that appear in similar contexts (in a monolingual corpus) and are translated in similar ways (in a parallel corpus). Second, we derive clusters over the translations of each source word by projecting the global clusters. We evaluate these clusters by comparing them to reference clusters with the overlapping BCubed metric (Amigo et al., 2009). We propose a clustering criterion that allows us to derive reference clusters from the synonym groups of WordNet @ (Miller, 1995).1 Our experiments using Spanish-English and Japanese-English datasets demonstrate that the automatically generated clusters produced by our method are substantially more similar to the 1WordNet is used only for evaluation; our sense clustering method is fully unsupervised and language-independent. 773 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773–782, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics Sense cluster WordNet sense description Usage example collocate group or</context>
<context position="9606" citStr="Miller, 1995" startWordPosition="1553" endWordPosition="1554"> dictionaries for a translation to appear in multiple sense clusters. In our example, the polysemous English verbs place and put appear in multiple clusters. 3 Generating Reference Clusters To construct a reference set for the translation sense clustering task, we first collected English translations of Spanish and Japanese nouns, verbs, and adverbs. Translation sets were curated by human annotators to keep only high-quality singleword translations. Rather than gathering reference clusters via an additional annotation effort, we leverage WordNet, a large database of English lexical semantics (Miller, 1995). WordNet groups words into sets of cogniWords collocate invest locate Synsets collocate collocate, lump, chunk invest, put, commit, place invest, clothe, adorn invest, vest, enthrone É locate, turn up situate, locate locate, place, site É Sense Clusters collocate invest, place, put locate, place place, position, put place put, set, place, pose, position, lay position rate, rank, range, order, grade, place put locate, place, site invest, put, commit, place É position put, set, place, pose, position, lay put, set, place, pose, position, lay put frame, redact, cast, put, couch invest, put, commi</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for English. In Communications of the ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Bin Wang</author>
<author>Yee Seng Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: An empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="16793" citStr="Ng et al., 2003" startWordPosition="2762" endWordPosition="2765"> sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method </context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploiting parallel texts for word sense disambiguation: An empirical study. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="19686" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="3226" endWordPosition="3229">3). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints.</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Steinbach</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<booktitle>In Proceedings of KDD Workshop on Text Mining.</booktitle>
<contexts>
<context position="19564" citStr="Steinbach et al., 2000" startWordPosition="3209" endWordPosition="3212">xtracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the d</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>Michael Steinbach, George Karypis, and Vipin Kumar. 2000. A comparison of document clustering techniques. In Proceedings of KDD Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Hierarchical verb clustering using graph factorization.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="16645" citStr="Sun and Korhonen (2011)" startWordPosition="2739" endWordPosition="2742">ions of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and </context>
</contexts>
<marker>Sun, Korhonen, 2011</marker>
<rawString>Lin Sun and Anna Korhonen. 2011. Hierarchical verb clustering using graph factorization. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis</author>
<author>Radu Ion</author>
<author>Nancy Ide</author>
</authors>
<title>Fine-grained word sense disambiguation based on parallel corpora, word alignment, word clustering and aligned wordnets.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="16813" citStr="Tufis et al., 2004" startWordPosition="2766" endWordPosition="2769"> task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively reco</context>
</contexts>
<marker>Tufis, Ion, Ide, 2004</marker>
<rawString>Dan Tufis, Radu Ion, and Nancy Ide. 2004. Fine-grained word sense disambiguation based on parallel corpora, word alignment, word clustering and aligned wordnets. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="12454" citStr="Uszkoreit and Brants, 2008" startWordPosition="2019" endWordPosition="2022">g with K-Means In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Li</context>
<context position="16360" citStr="Uszkoreit and Brants (2008)" startWordPosition="2694" endWordPosition="2698">, but has proven effective previously when defined in terms of counts. 776 4.4 Predicting Translation Clusters As a result of soft K-Means clustering, each word in the target-language vocabulary is assigned to a list of up to M clusters. To predict the sense clusters for a set of translations of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual par</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay Ponte</author>
<author>Ashok Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<journal>Journal of Documentation,</journal>
<booktitle>In Proceedings of</booktitle>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="18669" citStr="Uszkoreit et al., 2010" startWordPosition="3065" endWordPosition="3068">of-speech-tag distribution of 38 nouns, 10 verbs and 4 adverbs. The J→E dataset has 369 sourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001)</context>
</contexts>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proceedings of COLING. C. Van Rijsbergen. 1974. Foundation of evaluation. Journal of Documentation, 30(4):365–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariano Vel´azquez de la Cadena</author>
<author>Edward Gray</author>
<author>Juan L Iribas</author>
</authors>
<title>New Revised Vel´azques Spanish and English Dictionary.</title>
<date>1965</date>
<publisher>Follet Publishing Company.</publisher>
<contexts>
<context position="2217" citStr="Cadena et al., 1965" startWordPosition="323" endWordPosition="326"> useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers. *Author was a summer intern with Google Research while conducting this research project. Colocar [co-lo-car´], va. 1. To arrange, to put in due place or order. 2. To place, to put in any place, rank condition or office, to provide a place or employment. 3. To collocate, to locate, to lay. Figure 1: This excerpt from a bilingual dictionary groups English translations of the polysemous Spanish word colocar into three clusters that correspond to different word senses (Vel´azquez de la Cadena et al., 1965). This paper formalizes the task of clustering a set of translations by sense, as might appear in a published bilingual dictionary, and proposes an unsupervised method for inducing such clusters. We also show how to add usage examples for the translation sense clusters, hence providing complete structure to a bilingual dictionary. The input to this task is a set of source words and a set of target translations for each source word. Our proposed method clusters these translations in two steps. First, we induce a global clustering of the entire target vocabulary using the soft K-Means algorithm,</context>
</contexts>
<marker>Cadena, Gray, Iribas, 1965</marker>
<rawString>Mariano Vel´azquez de la Cadena, Edward Gray, and Juan L. Iribas. 1965. New Revised Vel´azques Spanish and English Dictionary. Follet Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and constrained Dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="16558" citStr="Vlachos et al. (2009)" startWordPosition="2725" endWordPosition="2729">ed to a list of up to M clusters. To predict the sense clusters for a set of translations of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The unde</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and constrained Dirichlet process mixture models for verb clustering. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Computational linguistics.</booktitle>
<contexts>
<context position="18754" citStr="Vogel et al., 1996" startWordPosition="3079" endWordPosition="3082">ourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). F</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the Conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>X Liu</author>
<author>Y Gong</author>
</authors>
<title>Document-clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="19727" citStr="Xu et al., 2003" startWordPosition="3233" endWordPosition="3236">t clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. After defining each constraint below, we</context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>W. Xu, X. Liu, and Y. Gong. 2003. Document-clustering based on non-negative matrix factorization. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Criterion functions for document clustering: Experiments and analysis. In</title>
<date>2001</date>
<tech>Technical Report TR 01-40,</tech>
<institution>Department of Computer Science, University of Minnesota,</institution>
<location>Minneapolis, MN.</location>
<contexts>
<context position="19269" citStr="Zhao and Karypis, 2001" startWordPosition="3163" endWordPosition="3166">Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Qo (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus777 tering evaluation by estima</context>
</contexts>
<marker>Zhao, Karypis, 2001</marker>
<rawString>Y. Zhao and G. Karypis. 2001. Criterion functions for document clustering: Experiments and analysis. In Technical Report TR 01-40, Department of Computer Science, University of Minnesota, Minneapolis, MN.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>