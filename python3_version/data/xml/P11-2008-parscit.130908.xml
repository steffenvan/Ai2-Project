<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012500">
<title confidence="0.996046">
Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments
</title>
<author confidence="0.9983645">
Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills,
Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith
</author>
<affiliation confidence="0.99731">
School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.6813585">
{kgimpel,nschneid,brenocon,dipanjan,dpmills,
jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.99378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998736777777778">
We address the problem of part-of-speech tag-
ging for English data from the popular micro-
blogging service Twitter. We develop a tagset,
annotate data, develop features, and report
tagging results nearing 90% accuracy. The
data and tools have been made available to the
research community with the goal of enabling
richer text analysis of Twitter and related so-
cial media data sets.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963076923077">
The growing popularity of social media and user-
created web content is producing enormous quanti-
ties of text in electronic form. The popular micro-
blogging service Twitter (twitter.com) is one
particularly fruitful source of user-created content,
and a flurry of recent research has aimed to under-
stand and exploit these data (Ritter et al., 2010; Shar-
ifi et al., 2010; Barbosa and Feng, 2010; Asur and
Huberman, 2010; O’Connor et al., 2010a; Thelwall
et al., 2011). However, the bulk of this work eschews
the standard pipeline of tools which might enable
a richer linguistic analysis; such tools are typically
trained on newstext and have been shown to perform
poorly on Twitter (Finin et al., 2010).
One of the most fundamental parts of the linguis-
tic pipeline is part-of-speech (POS) tagging, a basic
form of syntactic analysis which has countless appli-
cations in NLP. Most POS taggers are trained from
treebanks in the newswire domain, such as the Wall
Street Journal corpus of the Penn Treebank (PTB;
Marcus et al., 1993). Tagging performance degrades
on out-of-domain data, and Twitter poses additional
challenges due to the conversational nature of the
text, the lack of conventional orthography, and 140-
character limit of each message (“tweet”). Figure 1
shows three tweets which illustrate these challenges.
</bodyText>
<page confidence="0.968037">
42
</page>
<listItem confidence="0.99442575">
(a) @Gunservatively@ obozo∧ willV goV nutsA
whenR PA∧ electsV aD RepublicanA GovernorN
nextP Tue∧ ., CanV youO sayV redistrictingV ?,
(b) SpendingV theD dayN withhhP mommmaN !,
(c) lmao! ..., s/oV toP theD coolA assN asianA
officerN 4P #1$ notR runninV myD licenseN and&amp;
#2$ notR takinV druN booN toP jailN ., ThankV
uO God∧ ., #amen#
</listItem>
<figureCaption confidence="0.99720225">
Figure 1: Example tweets with gold annotations. Under-
lined tokens show tagger improvements due to features
detailed in Section 3 (respectively: TAGDICT, METAPH,
and DISTSIM).
</figureCaption>
<bodyText confidence="0.997193333333333">
In this paper, we produce an English POS tagger
that is designed especially for Twitter data. Our con-
tributions are as follows:
</bodyText>
<listItem confidence="0.907438">
• we developed a POS tagset for Twitter,
• we manually tagged 1,827 tweets,
</listItem>
<bodyText confidence="0.955995">
• we developed features for Twitter POS tagging
and conducted experiments to evaluate them, and
• we provide our annotated corpus and trained POS
tagger to the research community.
Beyond these specific contributions, we see this
work as a case study in how to rapidly engi-
neer a core NLP system for a new and idiosyn-
cratic dataset. This project was accomplished in
200 person-hours spread across 17 people and two
months. This was made possible by two things:
(1) an annotation scheme that fits the unique char-
acteristics of our data and provides an appropriate
level of linguistic detail, and (2) a feature set that
captures Twitter-specific properties and exploits ex-
isting resources such as tag dictionaries and phonetic
normalization. The success of this approach demon-
strates that with careful design, supervised machine
learning can be applied to rapidly produce effective
language technology in new domains.
</bodyText>
<note confidence="0.766426">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42–47,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.992991843137255">
Tag Description Examples %
Nominal, Nominal + Verbal
N common noun (NN, NNS) books someone 13.7
O pronoun (personal/WH; not it you u meeee 6.8
possessive; PRP, WP)
S nominal + possessive books’ someone’s 0.1
ˆ proper noun (NNP, NNPS) lebron usa iPad 6.4
Z proper noun + possessive America’s 0.2
L nominal + verbal he’s book’ll iono 1.6
(= I don’t know)
M proper noun + verbal Mark’ll 0.0
Other open-class words
V verb incl. copula, might gonna 15.1
auxiliaries (V*, MD) ought couldn’t is
eats
A adjective (J*) good fav lil 5.1
R adverb (R*, WRB) 2 (i.e., too) 4.6
! interjection (UH) lol haha FTW yea 2.6
right
Other closed-class words
D determiner (WDT, DT, the teh its it’s 6.5
WP$, PRP$)
P pre- or postposition, or while to for 2 (i.e., 8.7
subordinating conjunction to) 4 (i.e., for)
(IN, TO)
&amp; coordinating conjunction and n &amp; + BUT 1.7
(CC)
T verb particle (RP) out off Up UP 0.6
X existential there, both 0.1
predeterminers (EX, PDT)
Y X + verbal there’s all’s 0.0
Twitter/online-specific
# hashtag (indicates #acl 1.0
topic/category for tweet)
@ at-mention (indicates @BarackObama 4.9
another user as a recipient
of a tweet)
~ discourse marker, RT and : in retweet 3.4
indications of continuation construction RT
of a message across @user : hello
multiple tweets
U URL or email address http://bit.ly/xyz 1.6
E emoticon :-) :b (: &lt;3 o O 1.0
Miscellaneous
$ numeral (CD) 2010 four 9:30 1.5
, punctuation (#, $, &apos;&apos;, (, !!! .... M 11.6
), ,, ., :, ``)
G other abbreviations, foreign ily (I love you) wby 1.1
words, possessive endings, (what about you) ’s
symbols, garbage (FW, A --&gt;
POS, SYM, LS) awesome...I’m
</table>
<tableCaption confidence="0.9810045">
Table 1: The set of tags used to annotate tweets. The
last column indicates each tag’s relative frequency in the
full annotated data (26,435 tokens). (The rates for M and
Y are both &lt; 0.0005.)
</tableCaption>
<sectionHeader confidence="0.928549" genericHeader="introduction">
2 Annotation
</sectionHeader>
<bodyText confidence="0.999978378378378">
Annotation proceeded in three stages. For Stage 0,
we developed a set of 20 coarse-grained tags based
on several treebanks but with some additional cate-
gories specific to Twitter, including URLs and hash-
tags. Next, we obtained a random sample of mostly
American English1 tweets from October 27, 2010,
automatically tokenized them using a Twitter tok-
enizer (O’Connor et al., 2010b),2 and pre-tagged
them using the WSJ-trained Stanford POS Tagger
(Toutanova et al., 2003) in order to speed up man-
ual annotation. Heuristics were used to mark tokens
belonging to special Twitter categories, which took
precedence over the Stanford tags.
Stage 1 was a round of manual annotation: 17 re-
searchers corrected the automatic predictions from
Stage 0 via a custom Web interface. A total of
2,217 tweets were distributed to the annotators in
this stage; 390 were identified as non-English and
removed, leaving 1,827 annotated tweets (26,436 to-
kens).
The annotation process uncovered several situa-
tions for which our tagset, annotation guidelines,
and tokenization rules were deficient or ambiguous.
Based on these considerations we revised the tok-
enization and tagging guidelines, and for Stage 2,
two annotators reviewed and corrected all of the
English tweets tagged in Stage 1. A third anno-
tator read the annotation guidelines and annotated
72 tweets from scratch, for purposes of estimating
inter-annotator agreement. The 72 tweets comprised
1,021 tagged tokens, of which 80 differed from the
Stage 2 annotations, resulting in an agreement rate
of 92.2% and Cohen’s r. value of 0.914. A final
sweep was made by a single annotator to correct er-
rors and improve consistency of tagging decisions
across the corpus. The released data and tools use
the output of this final stage.
</bodyText>
<subsectionHeader confidence="0.943934">
2.1 Tagset
</subsectionHeader>
<bodyText confidence="0.99986325">
We set out to develop a POS inventory for Twitter
that would be intuitive and informative—while at
the same time simple to learn and apply—so as to
maximize tagging consistency within and across an-
</bodyText>
<footnote confidence="0.986327">
1We filtered to tweets sent via an English-localized user in-
terface set to a United States timezone.
2http://github.com/brendano/tweetmotif
</footnote>
<page confidence="0.999759">
43
</page>
<bodyText confidence="0.990907814285714">
notators. Thus, we sought to design a coarse tagset
that would capture standard parts of speech3 (noun,
verb, etc.) as well as categories for token varieties
seen mainly in social media: URLs and email ad-
dresses; emoticons; Twitter hashtags, of the form
#tagname, which the author may supply to catego-
rize a tweet; and Twitter at-mentions, of the form
@user, which link to other Twitter users from within
a tweet.
Hashtags and at-mentions can also serve as words
or phrases within a tweet; e.g. Is #qadaffi going down?.
When used in this way, we tag hashtags with their
appropriate part of speech, i.e., as if they did not start
with #. Of the 418 hashtags in our data, 148 (35%)
were given a tag other than #: 14% are proper nouns,
9% are common nouns, 5% are multi-word express-
sions (tagged as G), 3% are verbs, and 4% are some-
thing else. We do not apply this procedure to at-
mentions, as they are nearly always proper nouns.
Another tag, —, is used for tokens marking spe-
cific Twitter discourse functions. The most popular
of these is the RT (“retweet”) construction to publish
a message with attribution. For example,
RT @USER1 : LMBO !This man filed an
EMERGENCY Motion for Continuance on
account of the Rangers game tonight ! �
Wow lmao
indicates that the user @USER1 was originally the
source of the message following the colon. We ap-
ply — to the RT and : (which are standard), and
also «, which separates the author’s comment from
the retweeted material.4 Another common discourse
marker is ellipsis dots (... ) at the end of a tweet,
indicating a message has been truncated to fit the
140-character limit, and will be continued in a sub-
sequent tweet or at a specified URL.
Our first round of annotation revealed that, due to
nonstandard spelling conventions, tokenizing under
a traditional scheme would be much more difficult
3Our starting point was the cross-lingual tagset presented by
Petrov et al. (2011). Most of our tags are refinements of those
categories, which in turn are groupings of PTB WSJ tags (see
column 2 of Table 1). When faced with difficult tagging deci-
sions, we consulted the PTB and tried to emulate its conventions
as much as possible.
4These “iconic deictics” have been studied in other online
communities as well (Collister, 2010).
than for Standard English text. For example, apos-
trophes are often omitted, and there are frequently
words like ima (short for I’m gonna) that cut across
traditional POS categories. Therefore, we opted not
to split contractions or possessives, as is common
in English corpus preprocessing; rather, we intro-
duced four new tags for combined forms: {nominal,
proper noun} x {verb, possessive}.5
The final tagging scheme (Table 1) encompasses
25 tags. For simplicity, each tag is denoted with a
single ASCII character. The miscellaneous category
G includes multiword abbreviations that do not fit
in any of the other categories, like ily (I love you), as
well as partial words, artifacts of tokenization errors,
miscellaneous symbols, possessive endings,6 and ar-
rows that are not used as discourse markers.
Figure 2 shows where tags in our data tend to oc-
cur relative to the middle word of the tweet. We
see that Twitter-specific tags have strong positional
preferences: at-mentions (@) and Twitter discourse
markers (—) tend to occur towards the beginning of
messages, whereas URLs (U), emoticons (E), and
categorizing hashtags (#) tend to occur near the end.
</bodyText>
<sectionHeader confidence="0.998442" genericHeader="method">
3 System
</sectionHeader>
<bodyText confidence="0.972698052631579">
Our tagger is a conditional random field (CRF; Laf-
ferty et al., 2001), enabling the incorporation of ar-
bitrary local features in a log-linear model. Our
base features include: a feature for each word type,
a set of features that check whether the word con-
tains digits or hyphens, suffix features up to length 3,
and features looking at capitalization patterns in the
word. We then added features that leverage domain-
specific properties of our data, unlabeled in-domain
data, and external linguistic resources.
TWOxTH: Twitter orthography. We have features
for several regular expression-style rules that detect
at-mentions, hashtags, and URLs.
NAMES: Frequently-capitalized tokens. Micro-
bloggers are inconsistent in their use of capitaliza-
tion, so we compiled gazetteers of tokens which are
frequently capitalized. The likelihood of capital-
ization for a token is computed as Ncap��C
N�C , where
</bodyText>
<footnote confidence="0.99803025">
5The modified tokenizer is packaged with our tagger.
6Possessive endings only appear when a user or the tok-
enizer has separated the possessive ending from a possessor; the
tokenizer only does this when the possessor is an at-mention.
</footnote>
<page confidence="0.999091">
44
</page>
<figureCaption confidence="0.99884">
Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag. Most tags fall
between −1 and 1 on this scale; these are not shown.
</figureCaption>
<bodyText confidence="0.997904735294118">
N is the token count, Ncap is the capitalized to-
ken count, and a and C are the prior probability
and its prior weight.7 We compute features for
membership in the top N items by this metric, for
N E {1000, 2000, 3000, 5000,10000, 20000}.
TAGDICT: Traditional tag dictionary. We add
features for all coarse-grained tags that each word
occurs with in the PTB8 (conjoined with their fre-
quency rank). Unlike previous work that uses tag
dictionaries as hard constraints, we use them as soft
constraints since we expect lexical coverage to be
poor and the Twitter dialect of English to vary sig-
nificantly from the PTB domains. This feature may
be seen as a form of type-level domain adaptation.
DISTSIM: Distributional similarity. When train-
ing data is limited, distributional features from un-
labeled text can improve performance (Sch¨utze and
Pedersen, 1993). We used 1.9 million tokens from
134,000 unlabeled tweets to construct distributional
features from the successor and predecessor proba-
bilities for the 10,000 most common terms. The suc-
cessor and predecessor transition matrices are hori-
zontally concatenated into a sparse matrix M, which
we approximate using a truncated singular value de-
composition: M Pz� USVT, where U is limited to
50 columns. Each term’s feature vector is its row
in U; following Turian et al. (2010), we standardize
and scale the standard deviation to 0.1.
METAPH: Phonetic normalization. Since Twitter
includes many alternate spellings of words, we used
the Metaphone algorithm (Philips, 1990)9 to create
a coarse phonetic normalization of words to simpler
keys. Metaphone consists of 19 rules that rewrite
consonants and delete vowels. For example, in our
</bodyText>
<equation confidence="0.877968">
7α = 1
100, C = 10; this score is equivalent to the posterior
</equation>
<bodyText confidence="0.780602">
probability of capitalization with a Beta(0.1, 9.9) prior.
</bodyText>
<footnote confidence="0.9581705">
8Both WSJ and Brown corpora, no case normalization. We
also tried adding the WordNet (Fellbaum, 1998) and Moby
(Ward, 1996) lexicons, which increased lexical coverage but did
not seem to help performance.
9Via the Apache Commons implementation: http://
commons.apache.org/codec/
</footnote>
<bodyText confidence="0.999854416666667">
data, {thangs thanks thanksss thanx thinks thnx}
are mapped to 0NKS, and {lmao lmaoo lmaooooo}
map to LM. But it is often too coarse; e.g. {war we’re
wear were where worry} map to WR.
We include two types of features. First, we use
the Metaphone key for the current token, comple-
menting the base model’s word features. Second,
we use a feature indicating whether a tag is the most
frequent tag for PTB words having the same Meta-
phone key as the current token. (The second feature
was disabled in both −TAGDICT and −METAPH ab-
lation experiments.)
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999424608695652">
Our evaluation was designed to test the efficacy of
this feature set for part-of-speech tagging given lim-
ited training data. We randomly divided the set of
1,827 annotated tweets into a training set of 1,000
(14,542 tokens), a development set of 327 (4,770 to-
kens), and a test set of 500 (7,124 tokens). We com-
pare our system against the Stanford tagger. Due
to the different tagsets, we could not apply the pre-
trained Stanford tagger to our data. Instead, we re-
trained it on our labeled data, using a standard set
of features: words within a 5-word window, word
shapes in a 3-word window, and up to length-3
prefixes, length-3 suffixes, and prefix/suffix pairs.10
The Stanford system was regularized using a Gaus-
sian prior of a2 = 0.5 and our system with a Gaus-
sian prior of a2 = 5.0, tuned on development data.
The results are shown in Table 2. Our tagger with
the full feature set achieves a relative error reduction
of 25% compared to the Stanford tagger. We also
show feature ablation experiments, each of which
corresponds to removing one category of features
from the full set. In Figure 1, we show examples
that certain features help solve. Underlined tokens
</bodyText>
<footnote confidence="0.99869125">
10We used the following feature modules in the Stanford tag-
ger:bidirectional5words, naacl2003unknowns,
wordshapes(-3,3), prefix(3), suffix(3),
prefixsuffix(3).
</footnote>
<page confidence="0.994116">
45
</page>
<table confidence="0.999666818181818">
Dev. Test
Our tagger, all features 88.67 89.37
independent ablations:
−DISTSIM 87.88 88.31 (−1.06)
−TAGDICT 88.28 88.31 (−1.06)
−TWORTH 87.51 88.37 (−1.00)
−METAPH 88.18 88.95 (−0.42)
−NAMES 88.66 89.39 (+0.02)
Our tagger, base features 82.72 83.38
Stanford tagger 85.56 85.85
Annotator agreement 92.2
</table>
<tableCaption confidence="0.9710015">
Table 2: Tagging accuracies on development and test
data, including ablation experiments. Features are or-
dered by importance: test accuracy decrease due to ab-
lation (final column).
</tableCaption>
<table confidence="0.999441916666667">
Tag Acc. Confused Tag Acc. Confused
V 91 N ! 82 N
N 85 ∧ L 93 V
, 98 ~ &amp; 98 ∧
P 95 R U 97 ,
∧ 71 N $ 89 P
D 95 ∧ # 89 ∧
O 97 ∧ G 26 ,
A 79 N E 88 ,
R 83 A T 72 P
@ 99 V Z 45 ∧
~ 91 ,
</table>
<tableCaption confidence="0.93879775">
Table 3: Accuracy (recall) rates per class, in the test set
with the full model. (Omitting tags that occur less than
10 times in the test set.) For each gold category, the most
common confusion is shown.
</tableCaption>
<bodyText confidence="0.999808166666667">
are incorrect in a specific ablation, but are corrected
in the full system (i.e. when the feature is added).
The −TAGDICT ablation gets elects, Governor,
and next wrong in tweet (a). These words appear
in the PTB tag dictionary with the correct tags, and
thus are fixed by that feature. In (b), withhh is ini-
tially misclassified an interjection (likely caused by
interjections with the same suffix, like ohhh), but is
corrected by METAPH, because it is normalized to the
same equivalence class as with. Finally, s/o in tweet
(c) means “shoutout”, which appears only once in
the training data; adding DISTSIM causes it to be cor-
rectly identified as a verb.
Substantial challenges remain; for example, de-
spite the NAMES feature, the system struggles to
identify proper nouns with nonstandard capitaliza-
tion. This can be observed from Table 3, which
shows the recall of each tag type: the recall of proper
nouns (ˆ) is only 71%. The system also struggles
with the miscellaneous category (G), which covers
many rare tokens, including obscure symbols and ar-
tifacts of tokenization errors. Nonetheless, we are
encouraged by the success of our system on the
whole, leveraging out-of-domain lexical resources
(TAGDICT), in-domain lexical resources (DISTSIM),
and sublexical analysis (METAPH).
Finally, we note that, even though 1,000 train-
ing examples may seem small, the test set accuracy
when training on only 500 tweets drops to 87.66%,
a decrease of only 1.7% absolute.
</bodyText>
<sectionHeader confidence="0.998861" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999368">
We have developed a part-of-speech tagger for Twit-
ter and have made our data and tools available to the
research community at http://www.ark.cs.
cmu.edu/TweetNLP. More generally, we be-
lieve that our approach can be applied to address
other linguistic analysis needs as they continue to
arise in the era of social media and its rapidly chang-
ing linguistic conventions. We also believe that the
annotated data can be useful for research into do-
main adaptation and semi-supervised learning.
</bodyText>
<sectionHeader confidence="0.999405" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<copyright confidence="0.9764762">
We thank Desai Chen, Chris Dyer, Lori Levin, Behrang
Mohit, Bryan Routledge, Naomi Saphra, and Tae Yano
for assistance in annotating data. This research was sup-
ported in part by: the NSF through CAREER grant IIS-
1054319, the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
ber W911NF-10-1-0533, Sandia National Laboratories
(fellowship to K. Gimpel), and the U. S. Department of
Education under IES grant R305B040063 (fellowship to
M. Heilman).
</copyright>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981558363636364">
Sitaram Asur and Bernardo A. Huberman. 2010. Pre-
dicting the future with social media. In Proc. of WI-
IAT.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proc. of COLING.
Lauren Collister. 2010. Meaning variation of the iconic
deictics ˆ and &lt;— in an online community. In New
Ways of Analyzing Variation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
</reference>
<page confidence="0.989689">
46
</page>
<reference confidence="0.999047714285714">
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010. An-
notating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313–330.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proc. of ICWSM.
Brendan O’Connor, Michel Krieger, and David Ahn.
2010b. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proc. of ICWSM (demo
track).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Lawrence Philips. 1990. Hanging on the Metaphone.
Computer Language, 7(12).
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Proc.
of NAACL.
Hinrich Sch¨utze and Jan Pedersen. 1993. A vector model
for syntagmatic and paradigmatic relatedness. In Pro-
ceedings of the 9th Annual Conference of the UW Cen-
tre for the New OED and Text Research.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. of NAACL.
Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in Twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406–418.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc. of
HLT-NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proc. of ACL.
Grady Ward. 1996. Moby lexicon. http://icon.
shef.ac.uk/Moby.
</reference>
<page confidence="0.99949">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857542">
<title confidence="0.999932">Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</title>
<author confidence="0.990672">Kevin Gimpel</author>
<author confidence="0.990672">Nathan Schneider</author>
<author confidence="0.990672">Brendan O’Connor</author>
<author confidence="0.990672">Dipanjan Das</author>
<author confidence="0.990672">Daniel Mills</author>
<author confidence="0.990672">Michael Heilman Eisenstein</author>
<author confidence="0.990672">Dani Yogatama</author>
<author confidence="0.990672">Jeffrey Flanigan</author>
<author confidence="0.990672">A Smith</author>
<affiliation confidence="0.883806">School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA</affiliation>
<abstract confidence="0.9983458">We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sitaram Asur</author>
<author>Bernardo A Huberman</author>
</authors>
<title>Predicting the future with social media.</title>
<date>2010</date>
<booktitle>In Proc. of WIIAT.</booktitle>
<contexts>
<context position="1243" citStr="Asur and Huberman, 2010" startWordPosition="172" endWordPosition="175">esults nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. 1 Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et a</context>
</contexts>
<marker>Asur, Huberman, 2010</marker>
<rawString>Sitaram Asur and Bernardo A. Huberman. 2010. Predicting the future with social media. In Proc. of WIIAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on Twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="1218" citStr="Barbosa and Feng, 2010" startWordPosition="168" endWordPosition="171">es, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. 1 Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn T</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on Twitter from biased and noisy data. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauren Collister</author>
</authors>
<title>Meaning variation of the iconic deictics ˆ and &lt;— in an online community. In New Ways of Analyzing Variation.</title>
<date>2010</date>
<contexts>
<context position="10178" citStr="Collister, 2010" startWordPosition="1665" endWordPosition="1666">uent tweet or at a specified URL. Our first round of annotation revealed that, due to nonstandard spelling conventions, tokenizing under a traditional scheme would be much more difficult 3Our starting point was the cross-lingual tagset presented by Petrov et al. (2011). Most of our tags are refinements of those categories, which in turn are groupings of PTB WSJ tags (see column 2 of Table 1). When faced with difficult tagging decisions, we consulted the PTB and tried to emulate its conventions as much as possible. 4These “iconic deictics” have been studied in other online communities as well (Collister, 2010). than for Standard English text. For example, apostrophes are often omitted, and there are frequently words like ima (short for I’m gonna) that cut across traditional POS categories. Therefore, we opted not to split contractions or possessives, as is common in English corpus preprocessing; rather, we introduced four new tags for combined forms: {nominal, proper noun} x {verb, possessive}.5 The final tagging scheme (Table 1) encompasses 25 tags. For simplicity, each tag is denoted with a single ASCII character. The miscellaneous category G includes multiword abbreviations that do not fit in an</context>
</contexts>
<marker>Collister, 2010</marker>
<rawString>Lauren Collister. 2010. Meaning variation of the iconic deictics ˆ and &lt;— in an online community. In New Ways of Analyzing Variation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="14554" citStr="Fellbaum, 1998" startWordPosition="2370" endWordPosition="2371"> U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. METAPH: Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7α = 1 100, C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (Ward, 1996) lexicons, which increased lexical coverage but did not seem to help performance. 9Via the Apache Commons implementation: http:// commons.apache.org/codec/ data, {thangs thanks thanksss thanx thinks thnx} are mapped to 0NKS, and {lmao lmaoo lmaooooo} map to LM. But it is often too coarse; e.g. {war we’re wear were where worry} map to WR. We include two types of features. First, we use the Metaphone key for the current token, complementing the base model’s word features. Second, we use a feature indicating whether a tag is the most frequent tag for PTB words having the sam</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>Will Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in Twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="1526" citStr="Finin et al., 2010" startWordPosition="219" endWordPosition="222"> enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140- character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in Twitter data with crowdsourcing. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="11415" citStr="Lafferty et al., 2001" startWordPosition="1862" endWordPosition="1866">categories, like ily (I love you), as well as partial words, artifacts of tokenization errors, miscellaneous symbols, possessive endings,6 and arrows that are not used as discourse markers. Figure 2 shows where tags in our data tend to occur relative to the middle word of the tweet. We see that Twitter-specific tags have strong positional preferences: at-mentions (@) and Twitter discourse markers (—) tend to occur towards the beginning of messages, whereas URLs (U), emoticons (E), and categorizing hashtags (#) tend to occur near the end. 3 System Our tagger is a conditional random field (CRF; Lafferty et al., 2001), enabling the incorporation of arbitrary local features in a log-linear model. Our base features include: a feature for each word type, a set of features that check whether the word contains digits or hyphens, suffix features up to length 3, and features looking at capitalization patterns in the word. We then added features that leverage domainspecific properties of our data, unlabeled in-domain data, and external linguistic resources. TWOxTH: Twitter orthography. We have features for several regular expression-style rules that detect at-mentions, hashtags, and URLs. NAMES: Frequently-capital</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1852" citStr="Marcus et al., 1993" startWordPosition="274" endWordPosition="277">erman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140- character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate these challenges. 42 (a) @Gunservatively@ obozo∧ willV goV nutsA whenR PA∧ electsV aD RepublicanA GovernorN nextP Tue∧ ., CanV youO sayV redistrictingV ?, (b) SpendingV theD dayN withhhP mommmaN !, (c) lmao! ..., s/oV toP theD coolA assN asianA officerN 4P #1$ notR runninV myD licenseN and&amp; #2$ notR takinV druN booN toP jai</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proc. of ICWSM.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010a. From tweets to polls: Linking text sentiment to public opinion time series. In Proc. of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Michel Krieger</author>
<author>David Ahn</author>
</authors>
<title>TweetMotif: Exploratory search and topic summarization for Twitter.</title>
<date>2010</date>
<booktitle>In Proc. of ICWSM</booktitle>
<note>(demo track).</note>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>Brendan O’Connor, Michel Krieger, and David Ahn. 2010b. TweetMotif: Exploratory search and topic summarization for Twitter. In Proc. of ICWSM (demo track).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2011</date>
<journal>ArXiv:1104.2086.</journal>
<contexts>
<context position="9831" citStr="Petrov et al. (2011)" startWordPosition="1604" endWordPosition="1607">ce of the message following the colon. We apply — to the RT and : (which are standard), and also «, which separates the author’s comment from the retweeted material.4 Another common discourse marker is ellipsis dots (... ) at the end of a tweet, indicating a message has been truncated to fit the 140-character limit, and will be continued in a subsequent tweet or at a specified URL. Our first round of annotation revealed that, due to nonstandard spelling conventions, tokenizing under a traditional scheme would be much more difficult 3Our starting point was the cross-lingual tagset presented by Petrov et al. (2011). Most of our tags are refinements of those categories, which in turn are groupings of PTB WSJ tags (see column 2 of Table 1). When faced with difficult tagging decisions, we consulted the PTB and tried to emulate its conventions as much as possible. 4These “iconic deictics” have been studied in other online communities as well (Collister, 2010). than for Standard English text. For example, apostrophes are often omitted, and there are frequently words like ima (short for I’m gonna) that cut across traditional POS categories. Therefore, we opted not to split contractions or possessives, as is c</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. ArXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Philips</author>
</authors>
<date>1990</date>
<journal>Hanging on the Metaphone. Computer Language,</journal>
<volume>7</volume>
<issue>12</issue>
<contexts>
<context position="14168" citStr="Philips, 1990" startWordPosition="2306" endWordPosition="2307">weets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. The successor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M Pz� USVT, where U is limited to 50 columns. Each term’s feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. METAPH: Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7α = 1 100, C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (Ward, 1996) lexicons, which increased lexical coverage but did not seem to help performance. 9Via the Apache Commons implementation: http:// commons.apache.org/codec/ data, {thangs thanks thanksss thanx </context>
</contexts>
<marker>Philips, 1990</marker>
<rawString>Lawrence Philips. 1990. Hanging on the Metaphone. Computer Language, 7(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of Twitter conversations.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1172" citStr="Ritter et al., 2010" startWordPosition="159" endWordPosition="162">lop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. 1 Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such a</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of Twitter conversations. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
<author>Jan Pedersen</author>
</authors>
<title>A vector model for syntagmatic and paradigmatic relatedness.</title>
<date>1993</date>
<booktitle>In Proceedings of the 9th Annual Conference of the UW Centre for the New OED and Text Research.</booktitle>
<marker>Sch¨utze, Pedersen, 1993</marker>
<rawString>Hinrich Sch¨utze and Jan Pedersen. 1993. A vector model for syntagmatic and paradigmatic relatedness. In Proceedings of the 9th Annual Conference of the UW Centre for the New OED and Text Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beaux Sharifi</author>
<author>Mark-Anthony Hutton</author>
<author>Jugal Kalita</author>
</authors>
<title>Summarizing microblogs automatically.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="1194" citStr="Sharifi et al., 2010" startWordPosition="163" endWordPosition="167">e data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. 1 Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Jour</context>
</contexts>
<marker>Sharifi, Hutton, Kalita, 2010</marker>
<rawString>Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita. 2010. Summarizing microblogs automatically. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment in Twitter events.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>2</issue>
<contexts>
<context position="1291" citStr="Thelwall et al., 2011" startWordPosition="180" endWordPosition="183">ave been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. 1 Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-o</context>
</contexts>
<marker>Thelwall, Buckley, Paltoglou, 2011</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2011. Sentiment in Twitter events. Journal of the American Society for Information Science and Technology, 62(2):406–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="6266" citStr="Toutanova et al., 2003" startWordPosition="1004" endWordPosition="1007">e last column indicates each tag’s relative frequency in the full annotated data (26,435 tokens). (The rates for M and Y are both &lt; 0.0005.) 2 Annotation Annotation proceeded in three stages. For Stage 0, we developed a set of 20 coarse-grained tags based on several treebanks but with some additional categories specific to Twitter, including URLs and hashtags. Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation. Heuristics were used to mark tokens belonging to special Twitter categories, which took precedence over the Stanford tags. Stage 1 was a round of manual annotation: 17 researchers corrected the automatic predictions from Stage 0 via a custom Web interface. A total of 2,217 tweets were distributed to the annotators in this stage; 390 were identified as non-English and removed, leaving 1,827 annotated tweets (26,436 tokens). The annotation process uncovered several situations for which our tagset, annotation guidelines, and tokenization rules were deficie</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="13973" citStr="Turian et al. (2010)" startWordPosition="2277" endWordPosition="2280">utional similarity. When training data is limited, distributional features from unlabeled text can improve performance (Sch¨utze and Pedersen, 1993). We used 1.9 million tokens from 134,000 unlabeled tweets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. The successor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M Pz� USVT, where U is limited to 50 columns. Each term’s feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. METAPH: Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7α = 1 100, C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (Ward, 19</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grady Ward</author>
</authors>
<date>1996</date>
<note>Moby lexicon. http://icon. shef.ac.uk/Moby.</note>
<contexts>
<context position="14576" citStr="Ward, 1996" startWordPosition="2374" endWordPosition="2375">. (2010), we standardize and scale the standard deviation to 0.1. METAPH: Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7α = 1 100, C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (Ward, 1996) lexicons, which increased lexical coverage but did not seem to help performance. 9Via the Apache Commons implementation: http:// commons.apache.org/codec/ data, {thangs thanks thanksss thanx thinks thnx} are mapped to 0NKS, and {lmao lmaoo lmaooooo} map to LM. But it is often too coarse; e.g. {war we’re wear were where worry} map to WR. We include two types of features. First, we use the Metaphone key for the current token, complementing the base model’s word features. Second, we use a feature indicating whether a tag is the most frequent tag for PTB words having the same Metaphone key as the</context>
</contexts>
<marker>Ward, 1996</marker>
<rawString>Grady Ward. 1996. Moby lexicon. http://icon. shef.ac.uk/Moby.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>