<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.993447">
User Edits Classification Using Document Revision Histories
</title>
<author confidence="0.982273">
Amit Bronner Christof Monz
</author>
<affiliation confidence="0.9991945">
Informatics Institute Informatics Institute
University of Amsterdam University of Amsterdam
</affiliation>
<email confidence="0.994756">
a.bronner@uva.nl c.monz@uva.nl
</email>
<sectionHeader confidence="0.997309" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751714285714">
Document revision histories are a useful
and abundant source of data for natural
language processing, but selecting relevant
data for the task at hand is not trivial.
In this paper we introduce a scalable ap-
proach for automatically distinguishing be-
tween factual and fluency edits in document
revision histories. The approach is based
on supervised machine learning using lan-
guage model probabilities, string similar-
ity measured over different representations
of user edits, comparison of part-of-speech
tags and named entities, and a set of adap-
tive features extracted from large amounts
of unlabeled user edits. Applied to con-
tiguous edit segments, our method achieves
statistically significant improvements over
a simple yet effective edit-distance base-
line. It reaches high classification accuracy
(88%) and is shown to generalize to addi-
tional sets of unseen data.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999175">
Many online collaborative editing projects such as
Wikipedia1 keep track of complete revision histo-
ries. These contain valuable information about the
evolution of documents in terms of content as well
as language, style and form. Such data is publicly
available in large volumes and constantly grow-
ing. According to Wikipedia statistics, in August
2011 the English Wikipedia contained 3.8 million
articles with an average of 78.3 revisions per ar-
ticle. The average number of revision edits per
month is about 4 million in English and almost 11
million in total for all languages.2
</bodyText>
<footnote confidence="0.925285333333333">
1http://www.wikipedia.org
2Average for the 5 years period between August 2006
and August 2011. The count includes edits by registered
</footnote>
<bodyText confidence="0.983720135135135">
Exploiting document revision histories has
proven useful for a variety of natural language
processing (NLP) tasks, including sentence com-
pression (Nelken and Yamangil, 2008; Yamangil
and Nelken, 2008) and simplification (Yatskar et
al., 2010; Woodsend and Lapata, 2011), informa-
tion retrieval (Aji et al., 2010; Nunes et al., 2011),
textual entailment recognition (Zanzotto and Pen-
nacchiotti, 2010), and paraphrase extraction (Max
and Wisniewski, 2010; Dutrey et al., 2011).
The ability to distinguish between factual
changes or edits, which alter the meaning, and flu-
ency edits, which improve the style or readability,
is a crucial requirement for approaches exploit-
ing revision histories. The need for an automated
classification method has been identified (Nelken
and Yamangil, 2008; Max and Wisniewski, 2010),
but to the best of our knowledge has not been di-
rectly addressed. Previous approaches have either
applied simple heuristics (Yatskar et al., 2010;
Woodsend and Lapata, 2011) or manual annota-
tions (Dutrey et al., 2011) to restrict the data to
the type of edits relevant to the NLP task at hand.
The work described in this paper shows that it is
possible to automatically distinguish between fac-
tual and fluency edits. This is very desirable as
it does not rely on heuristics, which often gener-
alize poorly, and does not require manual anno-
tation beyond a small collection of training data,
thereby allowing for much larger data sets of re-
vision histories to be used for NLP research.
In this paper, we make the following novel con-
tributions:
We address the problem of automated classi-
fication of user edits as factual or fluency edits
users, anonymous users, software bots and reverts. Source:
http://stats.wikimedia.org.
</bodyText>
<page confidence="0.972226">
356
</page>
<note confidence="0.9768325">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 356–366,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943857142857">
by defining the scope of user edits, extracting a
large collection of such user edits from the En-
glish Wikipedia, constructing a manually labeled
dataset, and setting up a classification baseline.
A set of features is designed and integrated into
a supervised machine learning framework. It is
composed of language model probabilities and
string similarity measured over different represen-
tations, including part-of-speech tags and named
entities. Despite their relative simplicity, the fea-
tures achieve high classification accuracy when
applied to contiguous edit segments.
We go beyond labeled data and exploit large
amounts of unlabeled data. First, we demonstrate
that the trained classifier generalizes to thousands
of examples identified by user comments as spe-
cific types of fluency edits. Furthermore, we in-
troduce a new method for extracting features from
an evolving set of unlabeled user edits. This
method is successfully evaluated as an alternative
or supplement to the initial supervised approach.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999912052631579">
The need for user edits classification is implicit in
studies of Wikipedia edit histories. For example,
Viegas et al. (2004) use revision size as a simpli-
fied measure for the change of content, and Kittur
et al. (2007) use metadata features to predict user
edit conflicts.
Classification becomes an explicit requirement
when exploiting edit histories for NLP research.
Yamangil and Nelken (2008) use edits as train-
ing data for sentence compression. They make
the simplifying assumption that all selected edits
retain the core meaning. Zanzotto and Pennac-
chiotti (2010) use edits as training data for textual
entailment recognition. In addition to manually
labeled edits, they use Wikipedia user comments
and a co-training approach to leverage unlabeled
edits. Woodsend and Lapata (2011) and Yatskar
et al. (2010) use Wikipedia comments to identify
relevant edits for learning sentence simplification.
The work by Max and Wisniewski (2010) is
closely related to the approach proposed in this
paper. They extract a corpus of rewritings, dis-
tinguish between weak semantic differences and
strong semantic differences, and present a typol-
ogy of multiple subclasses. Spelling corrections
are heuristically identified but the task of auto-
matic classification is deferred. Follow-up work
by Dutrey et al. (2011) focuses on automatic para-
phrase identification using a rule based approach
and manually annotated examples.
Wikipedia vandalism detection is a user ed-
its classification problem addressed by a yearly
competition (since 2010) in conjunction with the
CLEF conference (Potthast et al., 2010; Potthast
and Holfeld, 2011). State-of-the-art solutions in-
volve supervised machine learning using various
content and metadata features. Content features
use spelling, grammar, character- and word-level
attributes. Many of them are relevant for our ap-
proach. Metadata features allow detection by pat-
terns of usage, time and place, which are gener-
ally useful for the detection of online malicious
activities (West et al., 2010; West and Lee, 2011).
We deliberately refrain from using such features.
A wide range of methods and approaches has
been applied to the similar tasks of textual en-
tailment and paraphrase recognition, see Androut-
sopoulos and Malakasiotis (2010) for a compre-
hensive review. These are all related because
paraphrases and bidirectional entailments repre-
sent types of fluency edits.
A different line of research uses classifiers to
predict sentence-level fluency (Zwarts and Dras,
2008; Chae and Nenkova, 2009). These could be
useful for fluency edits detection. Alternatively,
user edits could be a potential source of human-
produced training data for fluency models.
</bodyText>
<sectionHeader confidence="0.876414" genericHeader="method">
3 Definition of User Edits Scope
</sectionHeader>
<bodyText confidence="0.999977214285714">
Within our approach we distinguish between edit
segments, which represent the comparison (diff)
between two document revisions, and user edits,
which are the input for classification.
An edit segment is a contiguous sequence of
deleted, inserted or equal words. The difference
between two document revisions (vi, vj) is repre-
sented by a sequence of edit segments E. Each
edit segment (S, wm1 ) E E is a pair, where S E
{deleted, inserted, equal} and wm1 is a m-word
substring of vi, vj or both (respectively).
A user edit is a minimal set of sentences over-
lapping with deleted or inserted segments. Given
the two sets of revision sentences (Svz, Svj), let
</bodyText>
<equation confidence="0.503031">
O(S,wm1 ) = {s E Svz u Svj I wm1 n s =�0} (1)
</equation>
<bodyText confidence="0.9731035">
be the subset of sentences overlapping with a
given edit segment, and let
</bodyText>
<equation confidence="0.997911">
O(s) = {(S,wm1 ) E E I wm1 n s =�0} (2)
</equation>
<page confidence="0.958144">
357
</page>
<bodyText confidence="0.992093747252747">
be the subset of edit segments overlapping with a (1) Revisions 368209202 &amp; 378822230
given sentence.
A user edit is a pair (pre ⊆ Svi, post ⊆ Svj)
where
pre (“By the mid 1700s, Medzhybizh was the seat of
power in Podilia Province.”)
post (“By the mid 18th century, Medzhybizh was
the seat of power in Podilia Province.”)
∀s ∈ pre ∪ post ∀6 ∈ {deleted, inserted} ∀wm1 diff (equal , “By the mid”) , (deleted, “1700s”) ,
(6, wm1 ) ∈ 0(s) → 0(6, wm1 ) ⊆ pre ∪ post (3) (inserted , “18th century”) , (equal , “, Medzhy-
∃s ∈ pre ∪ post ∃6 ∈ {deleted, inserted} ∃wm1 bizh was the seat of power in Podilia Province.”)
(6, w1 m) ∈ 0(s) (4)
Table 1 illustrates different types of edit seg-
ments and user edits. The term replaced segment
refers to adjacent deleted and inserted segments.
Example (1) contains a replaced segment because
the deleted segment (“1700s”) is adjacent to the
inserted segment (“18th century”). Example (2)
contains an inserted segment (“and largest profes-
sional”), a replaced segment (“(est.” → “estab-
lished in”) and a deleted segment (“)”). User edits
of both examples consist of a single pre sentence
and a single post sentence because deleted and in-
serted segments do not cross any sentence bound-
ary. Example (3) contains a replaced segment (“.
He” → “who”). In this case the deleted segment
(“. He”) overlaps with two sentences and there-
fore the user edit consists of two pre sentences.
4 Features for Edits Classification
We design a set of features for supervised classi-
fication of user edits. The design is guided by two
main considerations: simplicity and interoperabil-
ity. Simplicity is important because there are po-
tentially hundreds of millions of user edits to be
classified. This amount continues to grow at rapid
pace and a scalable solution is required. Interop-
erability is important because millions of user ed-
its are available in multiple languages. Wikipedia
is a flagship project, but there are other collabora-
tive editing projects. The solution should prefer-
ably be language- and project-independent. Con-
sequently, we refrain from deeper syntactic pars-
ing, Wikipedia-specific features, and language re-
sources that are limited to English.
Our basic intuition is that longer edits are likely
to be factual and shorter edits are likely to be
fluency edits. The baseline method is therefore
character-level edit distance (Levenshtein, 1966)
between pre- and post-edited text.
Six feature categories are added to the baseline.
Most features take the form of threefold counts re-
ferring to deleted, inserted and equal elements of
(2) Revisions 148109085 &amp; 149440273
pre (“Original Society of Teachers of the Alexander
Technique (est. 1958).”)
post (“Original and largest professional Society of
Teachers of the Alexander Technique estab-
lished in 1958.”)
diff (equal , “Original”) , (inserted , “and largest
professional”) , (equal , “Society of Teachers of
the Alexander Technique”) , (deleted , “(est.”) ,
(inserted , “ established in”) , (equal , “1958”) ,
(deleted, “)”) , (equal, “.”)
(3) Revisions 61406809 &amp; 61746002
pre (“Fredrik Modin is a Swedish ice hockey left
winger.” , “He is known for having one of the
hardest slap shots in the NHL.”)
post (“Fredrik Modin is a Swedish ice hockey left
winger who is known for having one of the hard-
est slap shots in the NHL.”)
diff (equal , “Fredrik Modin is a Swedish ice hockey
left winger”) , (deleted , “. He”) , (inserted ,
“who”) , (equal , “is known for having one of
the hardest slap shots in the NHL.”)
Table 1: Examples of user edits and the corre-
sponding edit segments (revision numbers corre-
spond to the English Wikipedia).
each user edit. For instance, example (1) in Table
1 has one deleted token, two inserted tokens and
14 equal tokens. Many features use string similar-
ity calculated over alternative representations.
Character-level features include counts of
deleted, inserted and equal characters of different
types, such as word &amp; non-word characters or dig-
its &amp; non-digits. Character types may help iden-
tify edits types. For example, the change of dig-
its may suggest a factual edit while the change of
non-word characters may suggest a fluency edit.
Word-level features count deleted, inserted
and equal words using three parallel represen-
tations: original case, lower case, and lemmas.
</bodyText>
<table confidence="0.905023933333333">
Word-level edit distance is calculated for each
representation. Table 2 illustrates how edit dis-
tance may vary across different representations.
358
Rep. User Edit Dist
Words pre Branch lines were built in Kenya 4
post A branch line was built in Kenya
Lowcase pre branch lines were built in kenya 3
post a branch line was built in kenya
Lemmas pre branch line be build in Kenya 1
post a branch line be build in Kenya
PoS tags pre NN NNS VBD VBN IN NNP 2
post DT NN NN VBD VBN IN NNP
NE tags pre LOCATION 0
post LOCATION
</table>
<tableCaption confidence="0.97254">
Table 2: Word- and tag-level edit distance mea-
</tableCaption>
<bodyText confidence="0.9685244375">
sured over different representations (example
from Wikipedia revisions 2678278 &amp; 2682972).
Fluency edits may shift words, which sometimes
may be slightly modified. Fluency edits may also
add or remove words that already appear in con-
text. Optimal calculation of edit distance with
shifts is computationally expensive (Shapira and
Storer, 2002). Translation error rate (TER) pro-
vides an approximation but it is designed for the
needs of machine translation evaluation (Snover
et al., 2006). To have a more sensitive estima-
tion of the degree of edit, we compute the minimal
character-level edit distance between every pair of
words that belong to different edit segments. For
each pair of edit segments (δ, wm1 ), (δ&apos;, w&apos;k1) over-
lapping with a user edit, if δ =� δ&apos; we compute:
</bodyText>
<equation confidence="0.634227">
bw E wm1 : min
w/Ew/l EditDist(w, w&apos;) (5)
</equation>
<bodyText confidence="0.999955142857143">
Binned counts of the number of words with a min-
imal edit distance of 0, 1, 2, 3 or more charac-
ters are accumulated per edit segment type (equal,
deleted or inserted).
Part-of-speech (PoS) features include counts
of deleted, inserted and equal PoS tags (per tag)
and edit distance at the tag level between PoS tags
before and after the edit. Similarly, named-entity
(NE) features include counts of deleted, inserted
and equal NE tags (per tag, excluding OTHER)
and edit distance at the tag level between NE tags
before and after the edit. Table 2 illustrates the
edit distance at different levels of representation.
We assume that a deleted NE tag, e.g. PERSON
or LOCATION, could indicate a factual edit. It
could however be a fluency edit where the NE is
replaced by a co-referent like “she” or “it”. Even
if we encounter an inserted PRP PoS tag, the fea-
tures do not capture the explicit relation between
the deleted NE tag and the inserted PoS tag. This
is an inherent weakness of these features when
compared to parsing-based alternatives.
An additional set of counts, NE values, de-
scribes the number of deleted, inserted and equal
normalized values of numeric entities such as
numbers and dates. For instance, if the word
“100” is replaced by “200” and the respective nu-
meric values 100.0 and 200.0 are normalized, the
counts of deleted and inserted NE values will be
incremented and suggest a factual edit. If on the
other hand “100” is replaced by “hundred” and the
latter is normalized as having the numeric value
100.0, then the count of equal NE values will be
incremented, rather suggesting a fluency edit.
Acronym features count deleted, inserted and
equal acronyms. Potential acronyms are extracted
from word sequences that start with a capital letter
and from words that contain multiple capital let-
ters. If, for example, “UN” is replaced by “United
Nations”, “MicroSoft” by “MS” or “Jean Pierre”
by “J.P”, the count of equal acronyms will be in-
cremented, suggesting a fluency edit.
The last category, language model (LM) fea-
tures, takes a different approach. These features
look at n-gram based sentence probabilities be-
fore and after the edit, with and without normal-
ization with respect to sentence lengths. The ratio
of the two probabilities, Pratio(pre, post) is com-
puted as follows:
</bodyText>
<equation confidence="0.996622230769231">
P (wi|wi−1
i−n+1) (6)
M
�Pnorm(wm 1 ) = P� (wm 1 ) (7)
�Pnorm(post)
�Pratio(pre, post) = (8)
Pnorm(pre)
log pratio(pre, post) = log Pnorm(post) (9)
Pnorm(pre)
= log Pnorm(post) − log Pnorm(pre)
|post |log
1 P�(post) − 1
|pre |log P�(pre)
</equation>
<bodyText confidence="0.98039425">
Where P� is the sentence probability estimated as
a product of n-gram conditional probabilities and
Pnorm is the sentence probability normalized by
the sentence length. We hypothesize that the rel-
ative change of normalized sentence probabilities
is related to the edit type. As an additional feature,
the number of out of vocabulary (OOV) words be-
fore and after the edit is computed. The intuition
</bodyText>
<equation confidence="0.975372">
P�(wm1 ) � �m
i=1
</equation>
<page confidence="0.988135">
359
</page>
<table confidence="0.9957583">
Dataset Labeled Subset
Number of User Edits:
923,820 (100%) 2,008 (100%)
Edit Segments Distribution:
Replaced 535,402 (57.96%) 1,259 (62.70%)
Inserted 235,968 (25.54%) 471 (23.46%)
Deleted 152,450 (16.5%) 278 (13.84%)
Character-level Edit Distance Distribution:
1 202,882 (21.96%) 466 (23.21%)
2 81,388 (8.81%) 198 (9.86%)
3-10 296,841 (32.13%) 645 (32.12%)
11-100 342,709 (37.10%) 699 (34.81%)
Word-level Edit Distance Distribution:
1 493,095 (53.38%) 1,008 (54.18%)
2 182,770 (19.78%) 402 (20.02%)
3 77,603 (8.40%) 161 (8.02%)
4-10 170,352 (18.44%) 357 (17.78%)
Labels Distribution:
Fluency - 1,008 (50.2%)
Factual - 1,000 (49.8%)
</table>
<tableCaption confidence="0.995898">
Table 3: Dataset of nearly 1 million user edits
</tableCaption>
<bodyText confidence="0.979769166666667">
with single deleted, inserted or replaced segments,
of which 2K are labeled. The labels are almost
equally distributed. The distribution over edit seg-
ment types and edit distance intervals is detailed.
is that unknown words are more likely to be in-
dicative of factual edits.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.98948">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999940153846154">
First, we extract a large amount of user edits from
revision histories of the English Wikipedia.3 The
extraction process scans pairs of subsequent re-
visions of article pages and ignores any revision
that was reverted due to vandalism. It parses the
Wikitext and filters out markup, hyperlinks, tables
and templates. The process analyzes the clean text
of the two revisions4 and computes the difference
between them.5 The process identifies the overlap
between edit segments and sentence boundaries
and extracts user edits. Features are calculated
and user edits are stored and indexed. LM features
are calculated against a large English 4-gram lan-
</bodyText>
<footnote confidence="0.993222166666667">
3Dump of all pages with complete edit history as of Jan-
uary 15, 2011 (342GB bz2), http://dumps.wikimedia.org.
4Tokenization, sentence split, PoS &amp; NE tags by Stanford
CoreNLP, http://nlp.stanford.edu/software/corenlp.shtml.
5Myers’ O(ND) difference algorithm (Myers, 1986),
http://code.google.com/p/google-diff-match-patch.
</footnote>
<bodyText confidence="0.99991358">
guage model built by SRILM (Stolcke, 2002) with
modified interpolated Kneser-Ney smoothing us-
ing the AFP and Xinhua portions of the English
Gigaword corpus (LDC2003T05).
We extract a total of 4.3 million user edits of
which 2.52 million (almost 60%) are insertions
and deletions of complete sentences. Although
these may include fluency edits such as sentence
reordering or rewriting from scratch, we assume
that the large majority is factual. Of the remaining
1.78 million edits, the majority (64.5%) contains
single deleted, inserted or replaced segments. We
decide to focus on this subset because sentences
with multiple non-contiguous edit segments are
more likely to contain mixed cases of unrelated
factual and fluency edits, as illustrated by exam-
ple (2) in Table 1. Learning to classify contigu-
ous edit segments seems to be a reasonable way
of breaking down the problem into smaller parts.
We filter out user edits with edit distance longer
than 100 characters or 10 words that we assume to
be factual. The resulting dataset contains 923,820
user edits: 58% replaced segments, 25.5% in-
serted segments and 16.5% deleted segments.
Manual labeling of user edits is carried out by
a group of annotators with near native or native
level of English. All annotators receive the same
written guidelines. In short, fluency labels are
assigned to edits of letter case, spelling, gram-
mar, synonyms, paraphrases, co-referents, lan-
guage and style. Factual labels are assigned to
edits of dates, numbers and figures, named enti-
ties, semantic change or disambiguation, addition
or removal of content. A random set of 2,676 in-
stances is labeled: 2,008 instances with a majority
agreement of at least two annotators are selected
as training set, 270 instances are held out as de-
velopment set, 164 trivial fluency corrections of a
single letter’s case and 234 instances with no clear
agreement among annotators are excluded. The
last group (8.7%) emphasizes that the task is, to
a limited extent, subjective. It suggests that auto-
mated classification of certain user edits would be
difficult. Nevertheless, inter-rater agreement be-
tween annotators is high to very high. Kappa val-
ues between 0.74 to 0.84 are measured between
six pairs of annotators, each pair annotated a com-
mon subset of at least 100 instances. Table 3 de-
scribes the resulting dataset, which we also make
available to the research community.6
</bodyText>
<footnote confidence="0.953154">
6Available for download at http://staff.
</footnote>
<page confidence="0.974959">
360
</page>
<table confidence="0.9779555">
Character-level Edit Distance
/&lt;4 &gt;4\
Fluency (725) Factual (821)
Factual (179) Fluency (283)
</table>
<figureCaption confidence="0.989703">
Figure 1: A decision tree that uses character-level
edit distance as a sole feature. The tree correctly
classifies 76% of the labeled user edits.
</figureCaption>
<table confidence="0.999580555555555">
Feature set SVM RF Logit
Baseline 76.26% 76.26% 76.34%
+ Char-level 83.71%† 84.45%† 84.01%†
+ Word-level 78.38%†v 81.38%†^ 78.13%†v
+ PoS 76.58%v 76.97% 78.35%†^
+ NE 82.71%† 83.12%† 82.38%†
+ Acronyms 76.55% 76.61% 76.96%
+ LM 76.20% 77.42% 76.52%
All Features 87.14%†^ 87.14%† 85.64%†v
</table>
<tableCaption confidence="0.997289">
Table 4: Classification accuracy using the base-
</tableCaption>
<bodyText confidence="0.893224333333333">
line, each feature set added to the baseline, and
all features combined. Statistical significance at
p &lt; 0.05 is indicated by † w.r.t the baseline (us-
ing the same classifier), and by ^ w.r.t to another
classifier marked by v (using the same features).
Highest accuracy per classifier is marked in bold.
</bodyText>
<subsectionHeader confidence="0.998453">
5.2 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.986291611111111">
We experiment with three classifiers: Support
Vector Machines (SVM), Random Forests (RF)
and Logistic Regression (Logit).7 SVMs (Cortes
and Vapnik, 1995) and Logistic Regression (or
Maximum Entropy classifiers) are two widely
used machine learning techniques. SVMs have
been applied to many text classification problems
(Joachims, 1998). Maximum Entropy classifiers
have been applied to the similar tasks of para-
phrase recognition (Malakasiotis, 2009) and tex-
tual entailment (Hickl et al., 2006). Random
Forests (Breiman, 2001) as well as other decision
tree algorithms are successfully used for classi-
fying Wikipedia edits for the purpose of vandal-
ism detection (Potthast et al., 2010; Potthast and
Holfeld, 2011).
Experiments begin with the edit-distance base-
science.uva.nl/˜abronner/uec/data.
</bodyText>
<footnote confidence="0.766508">
7Using Weka classifiers: SMO (SVM), RandomForest &amp;
Logistic (Hall et al., 2009). Classifier’s parameters are tuned
using the held-out development set.
</footnote>
<table confidence="0.9979648">
Feature set SVM RF Logit
flu. /fac. flu. /fac. flu. /fac.
Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67
+ Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0.82
+ Word-level 0.88 / 0.69 0.81 / 0.82 0.86 / 0.70
+ PoS 0.85 / 0.68 0.78 / 0.76 0.84 / 0.72
+ NE 0.86 / 0.79 0.79 / 0.87 0.87 / 0.78
+ Acronyms 0.87 / 0.66 0.83 / 0.70 0.86 / 0.68
+ LM 0.85 / 0.67 0.79 / 0.76 0.84 / 0.69
All Features 0.88 / 0.86 0.86 / 0.88 0.87 / 0.84
</table>
<tableCaption confidence="0.995801">
Table 5: Fraction of correctly classified edits per
</tableCaption>
<bodyText confidence="0.977606710526316">
type: fluency edits (left) and factual edits (right),
using the baseline, each feature set added to the
baseline, and all features combined.
line. Then each one of the feature groups is sep-
arately added to the baseline. Finally, all features
are evaluated together. Table 4 reports the per-
centage of correctly classified edits (classifiers’
accuracy), and Table 5 reports the fraction of cor-
rectly classified edits per type. All results are for
10-fold cross validation. Statistical significance
against the baseline and between classifiers is cal-
culated at p &lt; 0.05 using paired t-test.
The first interesting result is the highly predic-
tive power of the single-feature baseline. It con-
firms the intuition that longer edits are mainly fac-
tual. Figure 1 shows that the edit distance of 72%
of the user edits labeled as fluency is between 1 to
4, while the edit distance of 82% of those labeled
as factual is greater than 4. The cut-off value is
found by a single-node decision tree that uses edit
distance as a sole feature. The tree correctly clas-
sifies 76% of the instances. This result implies
that the actual challenge is to correctly classify
short factual edits and long fluency edits.
Character-level features and named-entity fea-
tures lead to significant improvements over the
baseline for all classifiers. Their strength lies in
their ability to identify short factual edits such
as changes of numeric values or proper names.
Word-level features also significantly improve the
baseline but their contribution is smaller. PoS
and acronym features lead to small statistically-
insignificant improvements over the baseline.
The poor contribution of LM features is sur-
prising. It might be due to the limited context
of n-grams, but it might be that LM probabili-
ties are not a good predictor for the task. Re-
moving LM features from the set of all features
</bodyText>
<page confidence="0.996135">
361
</page>
<table confidence="0.839501888888889">
Fluency Edits Misclassified as Factual Correctly Classified Fluency Edits
Equivalent or redundant in context 14 “Adventure education makes intentional use of intention-
Paraphrases 13 ally uses challenging experiences for learning.”
Equivalent numeric patterns 7
Replacing first name with last name 4
Acronyms 4
Non specific adjectives or adverbs 3
Other 5
“He served as president from October 1 , 1985 and retired
through his retirement on June 30 , 2002.”
“In 1973, he helped organize assisted in organizing his
first ever visit to the West.”
Factual Edits Misclassified as Fluency
Short correction of content 35
Opposites 3
Similar names 3
Noise (unfiltered vandalism) 3
Other 6
</table>
<tableCaption confidence="0.966599333333333">
Table 6: Error types based on manual examina-
tion of 50 fluency edit misclassifications and 50
factual edit misclassifications.
</tableCaption>
<table confidence="0.483069857142857">
Correctly Classified Factual Edits
“Over the course of the next two years five months, the
unit completed a series of daring raids.”
“Scottish born David Tennant has reportedly said he
would like his Doctor to wear a kilt.”
“This family joined the strip in late 1990 around March
1991.”
</table>
<tableCaption confidence="0.875924666666667">
Table 7: Examples of correctly classified user ed-
its. Deleted segments are struck out, inserted are
bold (revision numbers are omitted for brevity).
</tableCaption>
<bodyText confidence="0.999563105263158">
leads to a small decrease in classification accu-
racy, namely 86.68% instead of 87.14% for SVM.
This decrease is not statistically significant.
The highest accuracy is achieved by both SVM
and RF and there are few significant differences
among the three classifiers. The fraction of cor-
rectly classified edits per type (Table 5) reveals
that for SVM and Logit, most fluency edits are
correctly classified by the baseline and most im-
provements over the baseline are attributed to bet-
ter classification of factual edits. This is not the
case for RF, where the fraction of correctly classi-
fied factual edits is higher and the fraction of cor-
rectly classified fluency edits is lower. This in-
sight motivates further experimentation. Repeat-
ing the experiment with a meta-classifier that uses
a majority voting scheme, achieves an improved
accuracy of 87.58%. This improvement is not sta-
tistically significant.
</bodyText>
<subsectionHeader confidence="0.989675">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999863375">
To have better understanding of errors made by
the classifier, 50 fluency edit misclassifications
and 50 factual edit misclassifications are ran-
domly selected and manually examined. The er-
rors are grouped into categories as summarized in
Table 6. These explain certain limitations of the
classifier and suggest possible improvements.
Fluency edit misclassifications: 14 instances
(28%) are phrases (often co-referents) that are ei-
ther equivalent or redundant in the given context.
For example: “in 1986” → “that year”, “when
she returned” → “when Ruffa returned” and “the
core member of the group are” → “the core mem-
bers are”. 13 (26%) are paraphrases misclassified
as factual edits. Examples are: “made cartoons”
→ “produced animated cartoons” and “with the
implication that they are similar to” → “imply-
ing a connection to”. 7 modify numeric patterns
that do not change the meaning such as the year
“37” → “1937”. 4 replace a first name of a per-
son with the last name. 4 contain acronyms, e.g.
“Display PostScript” → “Display PostScript (or
DPS)”. Acronym features are correctly identified
but the classifier fails to recognize a fluency edit.
3 modify adjectives or adverbs that do not change
the meaning such as “entirely” and “various”.
Factual edit misclassifications: the big major-
ity, 35 instances (70%), could be characterized as
short corrections, often replacing a similar word,
that make the content more accurate or more
precise. Examples (context is omitted): “city”
→ “village”, “emigrated” → “immigrated” and
“electrical” → “electromagnetic”. 3 are opposites
or antonyms such as “previous” → “next” and
“lived” → “died”. 3 are modifications of similar
person or entity names, e.g. “Kelly” → “Kate”.
3 are instances of unfiltered vandalism, i.e. noisy
examples. Other misclassifications include verb
tense modifications such as “is” → “was” and
“consists” → “consisted”. These are difficult to
</bodyText>
<page confidence="0.993538">
362
</page>
<table confidence="0.997985285714286">
Comment Test Set Classified as
Size Fluency Edits
“grammar” 1,122 88.9%
“spelling” 2,893 97.6%
“typo” 3,382 91.6%
“copyedit” 3,437 68.4%
Random set 5,000 49.4%
</table>
<tableCaption confidence="0.968568">
Table 8: Classifying unlabeled data selected by
user comments that suggest a fluency edit. The
SVM classifier is trained using the labeled data.
User comments are not used as features.
</tableCaption>
<bodyText confidence="0.995667272727273">
classify because the modification of verb tense in
a given context is sometimes factual and some-
times a fluency edit.
These findings agree with the feature analy-
sis. Fluency edit misclassifications are typically
longer phrases that carry the same meaning while
factual edit misclassifications are typically sin-
gle words or short phrases that carry different
meaning. The main conclusion is that the clas-
sifier should take into account explicit content
and context. Putting aside the consideration of
simplicity and interoperability, features based on
co-reference resolution and paraphrase recogni-
tion are likely to improve fluency edits classi-
fication, and features from language resources
that describe synonymy and antonymy relations
are likely to improve factual edits classification.
While this conclusion may come at no surprise, it
is important to highlight the high classification ac-
curacy that is achieved without such capabilities
and resources. Table 7 presents several examples
of correct classification produced by our classifier.
</bodyText>
<sectionHeader confidence="0.944914" genericHeader="method">
6 Exploiting Unlabeled Data
</sectionHeader>
<bodyText confidence="0.999978333333333">
We extracted a large set of user edits but our ap-
proach has been limited to a restricted number of
labeled examples. This section attempts to find
whether the classifier generalizes beyond labeled
data and whether unlabeled data could be used to
improve classification accuracy.
</bodyText>
<subsectionHeader confidence="0.999539">
6.1 Generalizing Beyond Labeled Data
</subsectionHeader>
<bodyText confidence="0.999962">
The aim of the next experiment is to test how well
the supervised classifier generalizes beyond the
labeled test set. The problem is the availability
of test data. There is no shared task for user ed-
its classification and no common test set to eval-
</bodyText>
<note confidence="0.53699">
Replaced by Frequency Edit class
</note>
<table confidence="0.9152526">
“second” 144 Factual
“First” 38 Fluency
“last” 31 Factual
“1st” 22 Fluency
“third” 22 Factaul
</table>
<tableCaption confidence="0.949194">
Table 9: User edits replacing the word “first” with
another single word: most frequent 5 out of 524.
</tableCaption>
<table confidence="0.997625333333333">
Replaced by Frequency Replaced by Frequency
“Adams” 7 “Squidward” 6
“Joseph” 7 “Alexander” 5
“Einstein” 6 “Davids” 5
“Galland” 6 “Haim” 5
“Lowe” 6 “Hickes” 5
</table>
<tableCaption confidence="0.794366">
Table 10: Fluency edits replacing the word “He”
with proper noun: most frequent 10 out of 1,381.
</tableCaption>
<bodyText confidence="0.999931166666667">
uate against. We resort to Wikipedia user com-
ments. It is a problematic option because it is un-
reliable. Users may add a comment when submit-
ting an edit, but it is not mandatory. The com-
ment is a free text with no predefined structure.
It could be meaningful or nonsense. The com-
ment is per revision. It may refer to one, some
or all edits submitted for a given revision. Nev-
ertheless, we identify several keywords that rep-
resent certain types of fluency edits: “grammar”,
“spelling”, “typo”, and “copyedit”. The first three
clearly indicate grammar and spelling corrections.
The last indicates a correction of format and style,
but also of accuracy of the text. Therefore it only
represents a bias towards fluency edits.
We extract unlabeled edits whose comment is
equal to one of the keywords and construct a test
set per keyword. An additional test set consists of
randomly selected unlabeled edits with any com-
ment. The five test sets are classified by the SVM
classifier trained using the labeled data and the set
of all features. To remove any doubt, user com-
ments are not part of any feature of the classifier.
The results in Table 8 show that most unlabeled
edits whose comments are “grammar”, “spelling”
or “typo” are indeed classified as fluency ed-
its. The classification of edits whose comment is
“copyedit” is biased towards fluency edits, but as
expected the result is less distinct. The classifica-
tion of the random set is balanced, as expected.
</bodyText>
<page confidence="0.996801">
363
</page>
<table confidence="0.999597166666667">
Feature set SVM RF Logit
Baseline 76.26% 76.26% 76.34%
All Features 87.14%†∧ 87.14%† 85.64%†∨
Unlabeled only 78.11%∨ 83.49%†∧ 78.78%†∨
Base + unlabeled 80.86%†∨ 85.45%†∧ 81.83%†∨
All + unlabeled 87.23% 88.35%‡†∧ 85.92%∨
</table>
<tableCaption confidence="0.982612">
Table 11: Classification accuracy using features
</tableCaption>
<bodyText confidence="0.967097333333333">
from unlabeled data. The first two rows are identi-
cal to Table 4. Statistical significance at p &lt; 0.05
is indicated by: † w.r.t the baseline; ‡ w.r.t all fea-
tures excluding features from unlabeled data; and
∧ w.r.t to another classifier marked by ∨ (using the
same features). The best result is marked in bold.
</bodyText>
<subsectionHeader confidence="0.964446">
6.2 Features from Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999992983870968">
The purpose of the last experiment is to exploit
unlabeled data in order to extract additional fea-
tures for the classifier. The underlying assumption
is that reoccurring patterns may indicate whether
a user edit is factual or a fluency edit.
We could assume that fluency edits would re-
occur across many revisions, while factual edits
would only appear in revisions of specific docu-
ments. However, this assumption does not nec-
essarily hold. Table 9 gives a simple example of
single word replacements for which the most re-
occurring edit is actually factual and other factual
and fluency edits reoccur in similar frequencies.
Finding user edits reoccurrence is not trivial.
We could rely on exact matches of surface forms,
but this may lead to data sparseness issues. Flu-
ency edits that exchange co-referents and proper
nouns, as illustrated by the example in Table 10,
may reoccur frequently but this fact could not
be revealed by exact matching of specific proper
nouns. On the other hand, using a bag of word
approach may find too many unrelated edits.
We introduce a two-step method that measures
the reoccurrence of edits in unlabeled data us-
ing exact and approximate matching over multi-
ple representations. The method provides a set of
frequencies that is fed into the classifier and al-
lows for learning subtle patterns of reoccurrence.
Staying consistent with our initial design consid-
erations, the method is simple and interoperable.
Given a user edit (pre, post), the method does
not compare pre with post in any way. It only
compares pre with pre-edited sentences of other
unlabeled edits and post with post-edited sen-
tences of other unlabeled edits. The first step is to
select candidates using a bag of words approach.
The second step is a comparison of the user edit
with each one of the candidates while increment-
ing counts of similarity measures. These account
for exact matches between different representa-
tions (original and low case, lemmas, PoS and NE
tags) as well as for approximate matches using
character- and word-level edit distance between
those representations. An additional feature is the
number of distinct documents in the candidate set.
We compute the set of features for the labeled
dataset based on the unlabeled data. The number
of candidates is set to 1,000 per user edit. We
re-train the classifiers using five configurations:
Baseline and All Features are identical to the first
experiment. Unlabeled only uses the new feature
set without any other feature. Base + Unlabeled
adds the new feature set to the baseline. All + Un-
labeled uses all available features. All results are
for 10-fold cross validation with statistical signif-
icance at p &lt; 0.05 by paired t-test, see Table 11.
We find that features extracted from unlabeled
data outperform the baseline and lead to statisti-
cally significant improvements when added to it.
The combination of all features allows Random
Forests to achieve the highest statistically signifi-
cant accuracy level of 88.35%.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9998959">
This work addresses the task of user edits clas-
sification as factual or fluency edits. It adopts
a supervised machine learning approach and
uses character- and word- level features, part-
of-speech tags, named entities, language model
probabilities, and a set of features extracted from
large amounts of unlabeled data. Our experiments
with contiguous user edits extracted from revision
histories of the English Wikipedia achieve high
classification accuracy and demonstrate general-
ization to data beyond labeled edits.
Our approach shows that machine learning
techniques can successfully distinguish between
user edit types, making them a favorable alterna-
tive to heuristic solutions. The simple and adap-
tive nature of our method allows for application to
large and evolving sets of user edits.
Acknowledgments. This research was funded
in part by the European Commission through the
CoSyne project FP7-ICT-4-248531.
</bodyText>
<page confidence="0.998144">
364
</page>
<sectionHeader confidence="0.996266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999603178571428">
A. Aji, Y. Wang, E. Agichtein, and E. Gabrilovich.
2010. Using the past to score the present: Extend-
ing term weighting models through revision history
analysis. In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, pages 629–638.
I. Androutsopoulos and P. Malakasiotis. 2010. A sur-
vey of paraphrasing and textual entailment meth-
ods. Journal of Artificial Intelligence Research,
38(1):135–187.
L. Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5–32.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case stud-
ies of machine translation and human-written text.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 139–147.
C. Cortes and V. Vapnik. 1995. Support-vector net-
works. Machine learning, 20(3):273–297.
C. Dutrey, D. Bernhard, H. Bouamor, and A. Max.
2011. Local modifications and paraphrases in
Wikipedia’s revision history. Procesamiento del
Lenguaje Natural, Revista no 46:51–58.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data
mining software: an update. ACM SIGKDD Explo-
rations Newsletter, 11(1):10–18.
A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,
and Y. Shi. 2006. Recognizing textual entailment
with LCCs GROUNDHOG system. In Proceedings
of the Second PASCAL Challenges Workshop.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. Machine Learning: ECML-98, pages 137–
142.
A. Kittur, B. Suh, B.A. Pendleton, and E.H. Chi. 2007.
He says, she says: Conflict and coordination in
Wikipedia. In Proceedings of the SIGCHI confer-
ence on Human factors in computing systems, pages
453–462.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707–710.
P. Malakasiotis. 2009. Paraphrase recognition using
machine learning to combine similarity measures.
In Proceedings of the ACL-IJCNLP 2009 Student
Research Workshop, pages 27–35.
A. Max and G. Wisniewski. 2010. Min-
ing naturally-occurring corrections and paraphrases
from Wikipedia’s revision history. In Proceedings
of LREC, pages 3143–3148.
E.W. Myers. 1986. An O(ND) difference algorithm
and its variations. Algorithmica, 1(1):251–266.
R. Nelken and E. Yamangil. 2008. Mining
Wikipedia’s article revision history for training
computational linguistics algorithms. In Proceed-
ings of the AAAI Workshop on Wikipedia and Arti-
ficial Intelligence: An Evolving Synergy, pages 31–
36.
S. Nunes, C. Ribeiro, and G. David. 2011. Term
weighting based on document revision history.
Journal of the American Society for Information
Science and Technology, 62(12):2471–2478.
M. Potthast and T. Holfeld. 2011. Overview of the 2nd
international competition on Wikipedia vandalism
detection. Notebook for PAN at CLEF 2011.
M. Potthast, B. Stein, and T. Holfeld. 2010. Overview
of the 1st international competition on Wikipedia
vandalism detection. Notebook Papers of CLEF,
pages 22–23.
D. Shapira and J. Storer. 2002. Edit distance with
move operations. In Combinatorial Pattern Match-
ing, pages 85–98.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Ameri-
cas, pages 223–231.
A. Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Proceedings of the interna-
tional conference on spoken language processing,
volume 2, pages 901–904.
F.B. Viegas, M. Wattenberg, and K. Dave. 2004.
Studying cooperation and conflict between authors
with history flow visualizations. In Proceedings of
the SIGCHI conference on Human factors in com-
puting systems, pages 575–582.
A.G. West and I. Lee. 2011. Multilingual vandalism
detection using language-independent &amp; ex post
facto evidence. Notebook for PAN at CLEF 2011.
A.G. West, S. Kannan, and I. Lee. 2010. Detecting
Wikipedia vandalism via spatio-temporal analysis
of revision metadata. In Proceedings of the Third
European Workshop on System Security, pages 22–
28.
K. Woodsend and M. Lapata. 2011. Learning to
simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 409–420.
E. Yamangil and R. Nelken. 2008. Mining Wikipedia
revision histories for improving sentence compres-
sion. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 137–140.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and
L. Lee. 2010. For the sake of simplicity: Unsu-
pervised extraction of lexical simplifications from
Wikipedia. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 365–368.
</reference>
<page confidence="0.986652">
365
</page>
<reference confidence="0.9993594">
F.M. Zanzotto and M. Pennacchiotti. 2010. Expand-
ing textual entailment corpora from Wikipedia us-
ing co-training. In Proceedings of the 2nd Work-
shop on Collaboratively Constructed Semantic Re-
sources, COLING 2010.
S. Zwarts and M. Dras. 2008. Choosing the right
translation: A syntactically informed classification
approach. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume
1, pages 1153–1160.
</reference>
<page confidence="0.999083">
366
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.914195">
<title confidence="0.999731">User Edits Classification Using Document Revision Histories</title>
<author confidence="0.992562">Amit Bronner Christof Monz</author>
<affiliation confidence="0.998767">Informatics Institute Informatics Institute University of Amsterdam University of Amsterdam</affiliation>
<email confidence="0.947716">a.bronner@uva.nlc.monz@uva.nl</email>
<abstract confidence="0.998497954545455">Document revision histories are a useful and abundant source of data for natural language processing, but selecting relevant data for the task at hand is not trivial. In this paper we introduce a scalable approach for automatically distinguishing between factual and fluency edits in document revision histories. The approach is based on supervised machine learning using language model probabilities, string similarity measured over different representations of user edits, comparison of part-of-speech tags and named entities, and a set of adaptive features extracted from large amounts of unlabeled user edits. Applied to contiguous edit segments, our method achieves statistically significant improvements over a simple yet effective edit-distance baseline. It reaches high classification accuracy (88%) and is shown to generalize to additional sets of unseen data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aji</author>
<author>Y Wang</author>
<author>E Agichtein</author>
<author>E Gabrilovich</author>
</authors>
<title>Using the past to score the present: Extending term weighting models through revision history analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>629--638</pages>
<contexts>
<context position="2129" citStr="Aji et al., 2010" startWordPosition="311" endWordPosition="314">h an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly addressed. Previous approaches have eithe</context>
</contexts>
<marker>Aji, Wang, Agichtein, Gabrilovich, 2010</marker>
<rawString>A. Aji, Y. Wang, E. Agichtein, and E. Gabrilovich. 2010. Using the past to score the present: Extending term weighting models through revision history analysis. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 629–638.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>P Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="7055" citStr="Androutsopoulos and Malakasiotis (2010)" startWordPosition="1064" endWordPosition="1068">State-of-the-art solutions involve supervised machine learning using various content and metadata features. Content features use spelling, grammar, character- and word-level attributes. Many of them are relevant for our approach. Metadata features allow detection by patterns of usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, see Androutsopoulos and Malakasiotis (2010) for a comprehensive review. These are all related because paraphrases and bidirectional entailments represent types of fluency edits. A different line of research uses classifiers to predict sentence-level fluency (Zwarts and Dras, 2008; Chae and Nenkova, 2009). These could be useful for fluency edits detection. Alternatively, user edits could be a potential source of humanproduced training data for fluency models. 3 Definition of User Edits Scope Within our approach we distinguish between edit segments, which represent the comparison (diff) between two document revisions, and user edits, whi</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>I. Androutsopoulos and P. Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research, 38(1):135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine learning,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="22792" citStr="Breiman, 2001" startWordPosition="3646" endWordPosition="3647">ame features). Highest accuracy per classifier is marked in bold. 5.2 Feature Analysis We experiment with three classifiers: Support Vector Machines (SVM), Random Forests (RF) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7Using Weka classifiers: SMO (SVM), RandomForest &amp; Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out development set. Feature set SVM RF Logit flu. /fac. flu. /fac. flu. /fac. Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67 + Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0.82 + Word-level 0.88 / 0.69 0.8</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>L. Breiman. 2001. Random forests. Machine learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chae</author>
<author>A Nenkova</author>
</authors>
<title>Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>139--147</pages>
<contexts>
<context position="7317" citStr="Chae and Nenkova, 2009" startWordPosition="1104" endWordPosition="1107"> usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, see Androutsopoulos and Malakasiotis (2010) for a comprehensive review. These are all related because paraphrases and bidirectional entailments represent types of fluency edits. A different line of research uses classifiers to predict sentence-level fluency (Zwarts and Dras, 2008; Chae and Nenkova, 2009). These could be useful for fluency edits detection. Alternatively, user edits could be a potential source of humanproduced training data for fluency models. 3 Definition of User Edits Scope Within our approach we distinguish between edit segments, which represent the comparison (diff) between two document revisions, and user edits, which are the input for classification. An edit segment is a contiguous sequence of deleted, inserted or equal words. The difference between two document revisions (vi, vj) is represented by a sequence of edit segments E. Each edit segment (S, wm1 ) E E is a pair, </context>
</contexts>
<marker>Chae, Nenkova, 2009</marker>
<rawString>J. Chae and A. Nenkova. 2009. Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 139–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="22418" citStr="Cortes and Vapnik, 1995" startWordPosition="3591" endWordPosition="3594">%† + Acronyms 76.55% 76.61% 76.96% + LM 76.20% 77.42% 76.52% All Features 87.14%†^ 87.14%† 85.64%†v Table 4: Classification accuracy using the baseline, each feature set added to the baseline, and all features combined. Statistical significance at p &lt; 0.05 is indicated by † w.r.t the baseline (using the same classifier), and by ^ w.r.t to another classifier marked by v (using the same features). Highest accuracy per classifier is marked in bold. 5.2 Feature Analysis We experiment with three classifiers: Support Vector Machines (SVM), Random Forests (RF) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-dista</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dutrey</author>
<author>D Bernhard</author>
<author>H Bouamor</author>
<author>A Max</author>
</authors>
<title>Local modifications and paraphrases in Wikipedia’s revision history. Procesamiento del Lenguaje Natural, Revista no 46:51–58.</title>
<date>2011</date>
<contexts>
<context position="2292" citStr="Dutrey et al., 2011" startWordPosition="334" endWordPosition="337"> languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly addressed. Previous approaches have either applied simple heuristics (Yatskar et al., 2010; Woodsend and Lapata, 2011) or manual annotations (Dutrey et al., 2011) to restrict the data to the type of edits</context>
<context position="6098" citStr="Dutrey et al. (2011)" startWordPosition="922" endWordPosition="925">ipedia user comments and a co-training approach to leverage unlabeled edits. Woodsend and Lapata (2011) and Yatskar et al. (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. The work by Max and Wisniewski (2010) is closely related to the approach proposed in this paper. They extract a corpus of rewritings, distinguish between weak semantic differences and strong semantic differences, and present a typology of multiple subclasses. Spelling corrections are heuristically identified but the task of automatic classification is deferred. Follow-up work by Dutrey et al. (2011) focuses on automatic paraphrase identification using a rule based approach and manually annotated examples. Wikipedia vandalism detection is a user edits classification problem addressed by a yearly competition (since 2010) in conjunction with the CLEF conference (Potthast et al., 2010; Potthast and Holfeld, 2011). State-of-the-art solutions involve supervised machine learning using various content and metadata features. Content features use spelling, grammar, character- and word-level attributes. Many of them are relevant for our approach. Metadata features allow detection by patterns of usa</context>
</contexts>
<marker>Dutrey, Bernhard, Bouamor, Max, 2011</marker>
<rawString>C. Dutrey, D. Bernhard, H. Bouamor, and A. Max. 2011. Local modifications and paraphrases in Wikipedia’s revision history. Procesamiento del Lenguaje Natural, Revista no 46:51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="23140" citStr="Hall et al., 2009" startWordPosition="3693" endWordPosition="3696">s. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7Using Weka classifiers: SMO (SVM), RandomForest &amp; Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out development set. Feature set SVM RF Logit flu. /fac. flu. /fac. flu. /fac. Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67 + Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0.82 + Word-level 0.88 / 0.69 0.81 / 0.82 0.86 / 0.70 + PoS 0.85 / 0.68 0.78 / 0.76 0.84 / 0.72 + NE 0.86 / 0.79 0.79 / 0.87 0.87 / 0.78 + Acronyms 0.87 / 0.66 0.83 / 0.70 0.86 / 0.68 + LM 0.85 / 0.67 0.79 / 0.76 0.84 / 0.69 All Features 0.88 / 0.86 0.86 / 0.88 0.87 / 0.84 Table 5: Fraction of correctly classified edits per type: fluency edits (left) and factual edits (right), u</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Williams</author>
<author>J Bensley</author>
<author>K Roberts</author>
<author>B Rink</author>
<author>Y Shi</author>
</authors>
<title>Recognizing textual entailment with LCCs GROUNDHOG system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="22760" citStr="Hickl et al., 2006" startWordPosition="3640" endWordPosition="3643">r classifier marked by v (using the same features). Highest accuracy per classifier is marked in bold. 5.2 Feature Analysis We experiment with three classifiers: Support Vector Machines (SVM), Random Forests (RF) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7Using Weka classifiers: SMO (SVM), RandomForest &amp; Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out development set. Feature set SVM RF Logit flu. /fac. flu. /fac. flu. /fac. Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67 + Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink, and Y. Shi. 2006. Recognizing textual entailment with LCCs GROUNDHOG system. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>Machine Learning: ECML-98,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="22601" citStr="Joachims, 1998" startWordPosition="3618" endWordPosition="3619"> and all features combined. Statistical significance at p &lt; 0.05 is indicated by † w.r.t the baseline (using the same classifier), and by ^ w.r.t to another classifier marked by v (using the same features). Highest accuracy per classifier is marked in bold. 5.2 Feature Analysis We experiment with three classifiers: Support Vector Machines (SVM), Random Forests (RF) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7Using Weka classifiers: SMO (SVM), RandomForest &amp; Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out develo</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. Machine Learning: ECML-98, pages 137– 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kittur</author>
<author>B Suh</author>
<author>B A Pendleton</author>
<author>E H Chi</author>
</authors>
<title>He says, she says: Conflict and coordination in Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>453--462</pages>
<contexts>
<context position="5013" citStr="Kittur et al. (2007)" startWordPosition="761" endWordPosition="764">s of unlabeled data. First, we demonstrate that the trained classifier generalizes to thousands of examples identified by user comments as specific types of fluency edits. Furthermore, we introduce a new method for extracting features from an evolving set of unlabeled user edits. This method is successfully evaluated as an alternative or supplement to the initial supervised approach. 2 Related Work The need for user edits classification is implicit in studies of Wikipedia edit histories. For example, Viegas et al. (2004) use revision size as a simplified measure for the change of content, and Kittur et al. (2007) use metadata features to predict user edit conflicts. Classification becomes an explicit requirement when exploiting edit histories for NLP research. Yamangil and Nelken (2008) use edits as training data for sentence compression. They make the simplifying assumption that all selected edits retain the core meaning. Zanzotto and Pennacchiotti (2010) use edits as training data for textual entailment recognition. In addition to manually labeled edits, they use Wikipedia user comments and a co-training approach to leverage unlabeled edits. Woodsend and Lapata (2011) and Yatskar et al. (2010) use W</context>
</contexts>
<marker>Kittur, Suh, Pendleton, Chi, 2007</marker>
<rawString>A. Kittur, B. Suh, B.A. Pendleton, and E.H. Chi. 2007. He says, she says: Conflict and coordination in Wikipedia. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 453–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="10699" citStr="Levenshtein, 1966" startWordPosition="1690" endWordPosition="1691">scalable solution is required. Interoperability is important because millions of user edits are available in multiple languages. Wikipedia is a flagship project, but there are other collaborative editing projects. The solution should preferably be language- and project-independent. Consequently, we refrain from deeper syntactic parsing, Wikipedia-specific features, and language resources that are limited to English. Our basic intuition is that longer edits are likely to be factual and shorter edits are likely to be fluency edits. The baseline method is therefore character-level edit distance (Levenshtein, 1966) between pre- and post-edited text. Six feature categories are added to the baseline. Most features take the form of threefold counts referring to deleted, inserted and equal elements of (2) Revisions 148109085 &amp; 149440273 pre (“Original Society of Teachers of the Alexander Technique (est. 1958).”) post (“Original and largest professional Society of Teachers of the Alexander Technique established in 1958.”) diff (equal , “Original”) , (inserted , “and largest professional”) , (equal , “Society of Teachers of the Alexander Technique”) , (deleted , “(est.”) , (inserted , “ established in”) , (eq</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V.I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Malakasiotis</author>
</authors>
<title>Paraphrase recognition using machine learning to combine similarity measures.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop,</booktitle>
<pages>27--35</pages>
<contexts>
<context position="22716" citStr="Malakasiotis, 2009" startWordPosition="3634" endWordPosition="3635">e same classifier), and by ^ w.r.t to another classifier marked by v (using the same features). Highest accuracy per classifier is marked in bold. 5.2 Feature Analysis We experiment with three classifiers: Support Vector Machines (SVM), Random Forests (RF) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7Using Weka classifiers: SMO (SVM), RandomForest &amp; Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out development set. Feature set SVM RF Logit flu. /fac. flu. /fac. flu. /fac. Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67 +</context>
</contexts>
<marker>Malakasiotis, 2009</marker>
<rawString>P. Malakasiotis. 2009. Paraphrase recognition using machine learning to combine similarity measures. In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Max</author>
<author>G Wisniewski</author>
</authors>
<title>Mining naturally-occurring corrections and paraphrases from Wikipedia’s revision history.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>3143--3148</pages>
<contexts>
<context position="2270" citStr="Max and Wisniewski, 2010" startWordPosition="330" endWordPosition="333">1 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly addressed. Previous approaches have either applied simple heuristics (Yatskar et al., 2010; Woodsend and Lapata, 2011) or manual annotations (Dutrey et al., 2011) to restrict the dat</context>
<context position="5733" citStr="Max and Wisniewski (2010)" startWordPosition="867" endWordPosition="870">rement when exploiting edit histories for NLP research. Yamangil and Nelken (2008) use edits as training data for sentence compression. They make the simplifying assumption that all selected edits retain the core meaning. Zanzotto and Pennacchiotti (2010) use edits as training data for textual entailment recognition. In addition to manually labeled edits, they use Wikipedia user comments and a co-training approach to leverage unlabeled edits. Woodsend and Lapata (2011) and Yatskar et al. (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. The work by Max and Wisniewski (2010) is closely related to the approach proposed in this paper. They extract a corpus of rewritings, distinguish between weak semantic differences and strong semantic differences, and present a typology of multiple subclasses. Spelling corrections are heuristically identified but the task of automatic classification is deferred. Follow-up work by Dutrey et al. (2011) focuses on automatic paraphrase identification using a rule based approach and manually annotated examples. Wikipedia vandalism detection is a user edits classification problem addressed by a yearly competition (since 2010) in conjunc</context>
</contexts>
<marker>Max, Wisniewski, 2010</marker>
<rawString>A. Max and G. Wisniewski. 2010. Mining naturally-occurring corrections and paraphrases from Wikipedia’s revision history. In Proceedings of LREC, pages 3143–3148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Myers</author>
</authors>
<title>An O(ND) difference algorithm and its variations.</title>
<date>1986</date>
<journal>Algorithmica,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="18868" citStr="Myers, 1986" startWordPosition="3035" endWordPosition="3036">mplates. The process analyzes the clean text of the two revisions4 and computes the difference between them.5 The process identifies the overlap between edit segments and sentence boundaries and extracts user edits. Features are calculated and user edits are stored and indexed. LM features are calculated against a large English 4-gram lan3Dump of all pages with complete edit history as of January 15, 2011 (342GB bz2), http://dumps.wikimedia.org. 4Tokenization, sentence split, PoS &amp; NE tags by Stanford CoreNLP, http://nlp.stanford.edu/software/corenlp.shtml. 5Myers’ O(ND) difference algorithm (Myers, 1986), http://code.google.com/p/google-diff-match-patch. guage model built by SRILM (Stolcke, 2002) with modified interpolated Kneser-Ney smoothing using the AFP and Xinhua portions of the English Gigaword corpus (LDC2003T05). We extract a total of 4.3 million user edits of which 2.52 million (almost 60%) are insertions and deletions of complete sentences. Although these may include fluency edits such as sentence reordering or rewriting from scratch, we assume that the large majority is factual. Of the remaining 1.78 million edits, the majority (64.5%) contains single deleted, inserted or replaced </context>
</contexts>
<marker>Myers, 1986</marker>
<rawString>E.W. Myers. 1986. An O(ND) difference algorithm and its variations. Algorithmica, 1(1):251–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nelken</author>
<author>E Yamangil</author>
</authors>
<title>Mining Wikipedia’s article revision history for training computational linguistics algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy,</booktitle>
<pages>31--36</pages>
<contexts>
<context position="1991" citStr="Nelken and Yamangil, 2008" startWordPosition="290" endWordPosition="293">in large volumes and constantly growing. According to Wikipedia statistics, in August 2011 the English Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Y</context>
</contexts>
<marker>Nelken, Yamangil, 2008</marker>
<rawString>R. Nelken and E. Yamangil. 2008. Mining Wikipedia’s article revision history for training computational linguistics algorithms. In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy, pages 31– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nunes</author>
<author>C Ribeiro</author>
<author>G David</author>
</authors>
<title>Term weighting based on document revision history.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>12</issue>
<contexts>
<context position="2150" citStr="Nunes et al., 2011" startWordPosition="315" endWordPosition="318">.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly addressed. Previous approaches have either applied simple heur</context>
</contexts>
<marker>Nunes, Ribeiro, David, 2011</marker>
<rawString>S. Nunes, C. Ribeiro, and G. David. 2011. Term weighting based on document revision history. Journal of the American Society for Information Science and Technology, 62(12):2471–2478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Potthast</author>
<author>T Holfeld</author>
</authors>
<title>Overview of the 2nd international competition on Wikipedia vandalism detection. Notebook for PAN at CLEF</title>
<date>2011</date>
<contexts>
<context position="6414" citStr="Potthast and Holfeld, 2011" startWordPosition="968" endWordPosition="971">aper. They extract a corpus of rewritings, distinguish between weak semantic differences and strong semantic differences, and present a typology of multiple subclasses. Spelling corrections are heuristically identified but the task of automatic classification is deferred. Follow-up work by Dutrey et al. (2011) focuses on automatic paraphrase identification using a rule based approach and manually annotated examples. Wikipedia vandalism detection is a user edits classification problem addressed by a yearly competition (since 2010) in conjunction with the CLEF conference (Potthast et al., 2010; Potthast and Holfeld, 2011). State-of-the-art solutions involve supervised machine learning using various content and metadata features. Content features use spelling, grammar, character- and word-level attributes. Many of them are relevant for our approach. Metadata features allow detection by patterns of usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, se</context>
<context position="22979" citStr="Potthast and Holfeld, 2011" startWordPosition="3674" endWordPosition="3677">) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7Using Weka classifiers: SMO (SVM), RandomForest &amp; Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out development set. Feature set SVM RF Logit flu. /fac. flu. /fac. flu. /fac. Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67 + Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0.82 + Word-level 0.88 / 0.69 0.81 / 0.82 0.86 / 0.70 + PoS 0.85 / 0.68 0.78 / 0.76 0.84 / 0.72 + NE 0.86 / 0.79 0.79 / 0.87 0.87 / 0.78 + Acronyms 0.87 / 0.66 0.83 / 0.70 0.86 / 0.68 + LM 0.85 / 0.67 0.79 / 0.76 0.84 / </context>
</contexts>
<marker>Potthast, Holfeld, 2011</marker>
<rawString>M. Potthast and T. Holfeld. 2011. Overview of the 2nd international competition on Wikipedia vandalism detection. Notebook for PAN at CLEF 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Potthast</author>
<author>B Stein</author>
<author>T Holfeld</author>
</authors>
<title>Overview of the 1st international competition on Wikipedia vandalism detection. Notebook Papers of CLEF,</title>
<date>2010</date>
<pages>22--23</pages>
<contexts>
<context position="6385" citStr="Potthast et al., 2010" startWordPosition="964" endWordPosition="967">oach proposed in this paper. They extract a corpus of rewritings, distinguish between weak semantic differences and strong semantic differences, and present a typology of multiple subclasses. Spelling corrections are heuristically identified but the task of automatic classification is deferred. Follow-up work by Dutrey et al. (2011) focuses on automatic paraphrase identification using a rule based approach and manually annotated examples. Wikipedia vandalism detection is a user edits classification problem addressed by a yearly competition (since 2010) in conjunction with the CLEF conference (Potthast et al., 2010; Potthast and Holfeld, 2011). State-of-the-art solutions involve supervised machine learning using various content and metadata features. Content features use spelling, grammar, character- and word-level attributes. Many of them are relevant for our approach. Metadata features allow detection by patterns of usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment a</context>
<context position="22950" citStr="Potthast et al., 2010" startWordPosition="3670" endWordPosition="3673">VM), Random Forests (RF) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7Using Weka classifiers: SMO (SVM), RandomForest &amp; Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out development set. Feature set SVM RF Logit flu. /fac. flu. /fac. flu. /fac. Baseline 0.85 / 0.67 0.74 / 0.79 0.85 / 0.67 + Char-level 0.85 / 0.82 0.83 / 0.86 0.86 / 0.82 + Word-level 0.88 / 0.69 0.81 / 0.82 0.86 / 0.70 + PoS 0.85 / 0.68 0.78 / 0.76 0.84 / 0.72 + NE 0.86 / 0.79 0.79 / 0.87 0.87 / 0.78 + Acronyms 0.87 / 0.66 0.83 / 0.70 0.86 / 0.68 + LM 0.</context>
</contexts>
<marker>Potthast, Stein, Holfeld, 2010</marker>
<rawString>M. Potthast, B. Stein, and T. Holfeld. 2010. Overview of the 1st international competition on Wikipedia vandalism detection. Notebook Papers of CLEF, pages 22–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shapira</author>
<author>J Storer</author>
</authors>
<title>Edit distance with move operations.</title>
<date>2002</date>
<booktitle>In Combinatorial Pattern Matching,</booktitle>
<pages>85--98</pages>
<contexts>
<context position="13522" citStr="Shapira and Storer, 2002" startWordPosition="2166" endWordPosition="2169">ilt in kenya 3 post a branch line was built in kenya Lemmas pre branch line be build in Kenya 1 post a branch line be build in Kenya PoS tags pre NN NNS VBD VBN IN NNP 2 post DT NN NN VBD VBN IN NNP NE tags pre LOCATION 0 post LOCATION Table 2: Word- and tag-level edit distance measured over different representations (example from Wikipedia revisions 2678278 &amp; 2682972). Fluency edits may shift words, which sometimes may be slightly modified. Fluency edits may also add or remove words that already appear in context. Optimal calculation of edit distance with shifts is computationally expensive (Shapira and Storer, 2002). Translation error rate (TER) provides an approximation but it is designed for the needs of machine translation evaluation (Snover et al., 2006). To have a more sensitive estimation of the degree of edit, we compute the minimal character-level edit distance between every pair of words that belong to different edit segments. For each pair of edit segments (δ, wm1 ), (δ&apos;, w&apos;k1) overlapping with a user edit, if δ =� δ&apos; we compute: bw E wm1 : min w/Ew/l EditDist(w, w&apos;) (5) Binned counts of the number of words with a minimal edit distance of 0, 1, 2, 3 or more characters are accumulated per edit s</context>
</contexts>
<marker>Shapira, Storer, 2002</marker>
<rawString>D. Shapira and J. Storer. 2002. Edit distance with move operations. In Combinatorial Pattern Matching, pages 85–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="13667" citStr="Snover et al., 2006" startWordPosition="2189" endWordPosition="2192">NS VBD VBN IN NNP 2 post DT NN NN VBD VBN IN NNP NE tags pre LOCATION 0 post LOCATION Table 2: Word- and tag-level edit distance measured over different representations (example from Wikipedia revisions 2678278 &amp; 2682972). Fluency edits may shift words, which sometimes may be slightly modified. Fluency edits may also add or remove words that already appear in context. Optimal calculation of edit distance with shifts is computationally expensive (Shapira and Storer, 2002). Translation error rate (TER) provides an approximation but it is designed for the needs of machine translation evaluation (Snover et al., 2006). To have a more sensitive estimation of the degree of edit, we compute the minimal character-level edit distance between every pair of words that belong to different edit segments. For each pair of edit segments (δ, wm1 ), (δ&apos;, w&apos;k1) overlapping with a user edit, if δ =� δ&apos; we compute: bw E wm1 : min w/Ew/l EditDist(w, w&apos;) (5) Binned counts of the number of words with a minimal edit distance of 0, 1, 2, 3 or more characters are accumulated per edit segment type (equal, deleted or inserted). Part-of-speech (PoS) features include counts of deleted, inserted and equal PoS tags (per tag) and edit</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on spoken language processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="18962" citStr="Stolcke, 2002" startWordPosition="3043" endWordPosition="3044">ce between them.5 The process identifies the overlap between edit segments and sentence boundaries and extracts user edits. Features are calculated and user edits are stored and indexed. LM features are calculated against a large English 4-gram lan3Dump of all pages with complete edit history as of January 15, 2011 (342GB bz2), http://dumps.wikimedia.org. 4Tokenization, sentence split, PoS &amp; NE tags by Stanford CoreNLP, http://nlp.stanford.edu/software/corenlp.shtml. 5Myers’ O(ND) difference algorithm (Myers, 1986), http://code.google.com/p/google-diff-match-patch. guage model built by SRILM (Stolcke, 2002) with modified interpolated Kneser-Ney smoothing using the AFP and Xinhua portions of the English Gigaword corpus (LDC2003T05). We extract a total of 4.3 million user edits of which 2.52 million (almost 60%) are insertions and deletions of complete sentences. Although these may include fluency edits such as sentence reordering or rewriting from scratch, we assume that the large majority is factual. Of the remaining 1.78 million edits, the majority (64.5%) contains single deleted, inserted or replaced segments. We decide to focus on this subset because sentences with multiple non-contiguous edi</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the international conference on spoken language processing, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F B Viegas</author>
<author>M Wattenberg</author>
<author>K Dave</author>
</authors>
<title>Studying cooperation and conflict between authors with history flow visualizations.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>575--582</pages>
<contexts>
<context position="4919" citStr="Viegas et al. (2004)" startWordPosition="743" endWordPosition="746">y when applied to contiguous edit segments. We go beyond labeled data and exploit large amounts of unlabeled data. First, we demonstrate that the trained classifier generalizes to thousands of examples identified by user comments as specific types of fluency edits. Furthermore, we introduce a new method for extracting features from an evolving set of unlabeled user edits. This method is successfully evaluated as an alternative or supplement to the initial supervised approach. 2 Related Work The need for user edits classification is implicit in studies of Wikipedia edit histories. For example, Viegas et al. (2004) use revision size as a simplified measure for the change of content, and Kittur et al. (2007) use metadata features to predict user edit conflicts. Classification becomes an explicit requirement when exploiting edit histories for NLP research. Yamangil and Nelken (2008) use edits as training data for sentence compression. They make the simplifying assumption that all selected edits retain the core meaning. Zanzotto and Pennacchiotti (2010) use edits as training data for textual entailment recognition. In addition to manually labeled edits, they use Wikipedia user comments and a co-training ap</context>
</contexts>
<marker>Viegas, Wattenberg, Dave, 2004</marker>
<rawString>F.B. Viegas, M. Wattenberg, and K. Dave. 2004. Studying cooperation and conflict between authors with history flow visualizations. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 575–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G West</author>
<author>I Lee</author>
</authors>
<title>Multilingual vandalism detection using language-independent &amp; ex post facto evidence. Notebook for PAN at CLEF</title>
<date>2011</date>
<contexts>
<context position="6833" citStr="West and Lee, 2011" startWordPosition="1031" endWordPosition="1034">ia vandalism detection is a user edits classification problem addressed by a yearly competition (since 2010) in conjunction with the CLEF conference (Potthast et al., 2010; Potthast and Holfeld, 2011). State-of-the-art solutions involve supervised machine learning using various content and metadata features. Content features use spelling, grammar, character- and word-level attributes. Many of them are relevant for our approach. Metadata features allow detection by patterns of usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, see Androutsopoulos and Malakasiotis (2010) for a comprehensive review. These are all related because paraphrases and bidirectional entailments represent types of fluency edits. A different line of research uses classifiers to predict sentence-level fluency (Zwarts and Dras, 2008; Chae and Nenkova, 2009). These could be useful for fluency edits detection. Alternatively, user edits could be a potential source of humanp</context>
</contexts>
<marker>West, Lee, 2011</marker>
<rawString>A.G. West and I. Lee. 2011. Multilingual vandalism detection using language-independent &amp; ex post facto evidence. Notebook for PAN at CLEF 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G West</author>
<author>S Kannan</author>
<author>I Lee</author>
</authors>
<title>Detecting Wikipedia vandalism via spatio-temporal analysis of revision metadata.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third European Workshop on System Security,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="6812" citStr="West et al., 2010" startWordPosition="1027" endWordPosition="1030">d examples. Wikipedia vandalism detection is a user edits classification problem addressed by a yearly competition (since 2010) in conjunction with the CLEF conference (Potthast et al., 2010; Potthast and Holfeld, 2011). State-of-the-art solutions involve supervised machine learning using various content and metadata features. Content features use spelling, grammar, character- and word-level attributes. Many of them are relevant for our approach. Metadata features allow detection by patterns of usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, see Androutsopoulos and Malakasiotis (2010) for a comprehensive review. These are all related because paraphrases and bidirectional entailments represent types of fluency edits. A different line of research uses classifiers to predict sentence-level fluency (Zwarts and Dras, 2008; Chae and Nenkova, 2009). These could be useful for fluency edits detection. Alternatively, user edits could be a poten</context>
</contexts>
<marker>West, Kannan, Lee, 2010</marker>
<rawString>A.G. West, S. Kannan, and I. Lee. 2010. Detecting Wikipedia vandalism via spatio-temporal analysis of revision metadata. In Proceedings of the Third European Workshop on System Security, pages 22– 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>409--420</pages>
<contexts>
<context position="2088" citStr="Woodsend and Lapata, 2011" startWordPosition="304" endWordPosition="307">nglish Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly </context>
<context position="5581" citStr="Woodsend and Lapata (2011)" startWordPosition="844" endWordPosition="847">easure for the change of content, and Kittur et al. (2007) use metadata features to predict user edit conflicts. Classification becomes an explicit requirement when exploiting edit histories for NLP research. Yamangil and Nelken (2008) use edits as training data for sentence compression. They make the simplifying assumption that all selected edits retain the core meaning. Zanzotto and Pennacchiotti (2010) use edits as training data for textual entailment recognition. In addition to manually labeled edits, they use Wikipedia user comments and a co-training approach to leverage unlabeled edits. Woodsend and Lapata (2011) and Yatskar et al. (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. The work by Max and Wisniewski (2010) is closely related to the approach proposed in this paper. They extract a corpus of rewritings, distinguish between weak semantic differences and strong semantic differences, and present a typology of multiple subclasses. Spelling corrections are heuristically identified but the task of automatic classification is deferred. Follow-up work by Dutrey et al. (2011) focuses on automatic paraphrase identification using a rule based approach and man</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>K. Woodsend and M. Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 409–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Yamangil</author>
<author>R Nelken</author>
</authors>
<title>Mining Wikipedia revision histories for improving sentence compression.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>137--140</pages>
<contexts>
<context position="2019" citStr="Yamangil and Nelken, 2008" startWordPosition="294" endWordPosition="297">ntly growing. According to Wikipedia statistics, in August 2011 the English Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisni</context>
<context position="5190" citStr="Yamangil and Nelken (2008)" startWordPosition="785" endWordPosition="788"> Furthermore, we introduce a new method for extracting features from an evolving set of unlabeled user edits. This method is successfully evaluated as an alternative or supplement to the initial supervised approach. 2 Related Work The need for user edits classification is implicit in studies of Wikipedia edit histories. For example, Viegas et al. (2004) use revision size as a simplified measure for the change of content, and Kittur et al. (2007) use metadata features to predict user edit conflicts. Classification becomes an explicit requirement when exploiting edit histories for NLP research. Yamangil and Nelken (2008) use edits as training data for sentence compression. They make the simplifying assumption that all selected edits retain the core meaning. Zanzotto and Pennacchiotti (2010) use edits as training data for textual entailment recognition. In addition to manually labeled edits, they use Wikipedia user comments and a co-training approach to leverage unlabeled edits. Woodsend and Lapata (2011) and Yatskar et al. (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. The work by Max and Wisniewski (2010) is closely related to the approach proposed in this pape</context>
</contexts>
<marker>Yamangil, Nelken, 2008</marker>
<rawString>E. Yamangil and R. Nelken. 2008. Mining Wikipedia revision histories for improving sentence compression. In Proceedings of ACL-08: HLT, Short Papers, pages 137–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yatskar</author>
<author>B Pang</author>
<author>C Danescu-Niculescu-Mizil</author>
<author>L Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>365--368</pages>
<contexts>
<context position="2060" citStr="Yatskar et al., 2010" startWordPosition="300" endWordPosition="303">, in August 2011 the English Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our know</context>
<context position="5607" citStr="Yatskar et al. (2010)" startWordPosition="849" endWordPosition="852">t, and Kittur et al. (2007) use metadata features to predict user edit conflicts. Classification becomes an explicit requirement when exploiting edit histories for NLP research. Yamangil and Nelken (2008) use edits as training data for sentence compression. They make the simplifying assumption that all selected edits retain the core meaning. Zanzotto and Pennacchiotti (2010) use edits as training data for textual entailment recognition. In addition to manually labeled edits, they use Wikipedia user comments and a co-training approach to leverage unlabeled edits. Woodsend and Lapata (2011) and Yatskar et al. (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. The work by Max and Wisniewski (2010) is closely related to the approach proposed in this paper. They extract a corpus of rewritings, distinguish between weak semantic differences and strong semantic differences, and present a typology of multiple subclasses. Spelling corrections are heuristically identified but the task of automatic classification is deferred. Follow-up work by Dutrey et al. (2011) focuses on automatic paraphrase identification using a rule based approach and manually annotated examples. </context>
</contexts>
<marker>Yatskar, Pang, Danescu-Niculescu-Mizil, Lee, 2010</marker>
<rawString>M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and L. Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 365–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>M Pennacchiotti</author>
</authors>
<title>Expanding textual entailment corpora from Wikipedia using co-training.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd Workshop on Collaboratively Constructed Semantic Resources, COLING</booktitle>
<contexts>
<context position="2217" citStr="Zanzotto and Pennacchiotti, 2010" startWordPosition="322" endWordPosition="326">ion edits per month is about 4 million in English and almost 11 million in total for all languages.2 1http://www.wikipedia.org 2Average for the 5 years period between August 2006 and August 2011. The count includes edits by registered Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly addressed. Previous approaches have either applied simple heuristics (Yatskar et al., 2010; Woodsend and Lapata, 2011) or manual </context>
<context position="5363" citStr="Zanzotto and Pennacchiotti (2010)" startWordPosition="811" endWordPosition="815">or supplement to the initial supervised approach. 2 Related Work The need for user edits classification is implicit in studies of Wikipedia edit histories. For example, Viegas et al. (2004) use revision size as a simplified measure for the change of content, and Kittur et al. (2007) use metadata features to predict user edit conflicts. Classification becomes an explicit requirement when exploiting edit histories for NLP research. Yamangil and Nelken (2008) use edits as training data for sentence compression. They make the simplifying assumption that all selected edits retain the core meaning. Zanzotto and Pennacchiotti (2010) use edits as training data for textual entailment recognition. In addition to manually labeled edits, they use Wikipedia user comments and a co-training approach to leverage unlabeled edits. Woodsend and Lapata (2011) and Yatskar et al. (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. The work by Max and Wisniewski (2010) is closely related to the approach proposed in this paper. They extract a corpus of rewritings, distinguish between weak semantic differences and strong semantic differences, and present a typology of multiple subclasses. Spellin</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, 2010</marker>
<rawString>F.M. Zanzotto and M. Pennacchiotti. 2010. Expanding textual entailment corpora from Wikipedia using co-training. In Proceedings of the 2nd Workshop on Collaboratively Constructed Semantic Resources, COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zwarts</author>
<author>M Dras</author>
</authors>
<title>Choosing the right translation: A syntactically informed classification approach.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>1153--1160</pages>
<contexts>
<context position="7292" citStr="Zwarts and Dras, 2008" startWordPosition="1100" endWordPosition="1103">etection by patterns of usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, see Androutsopoulos and Malakasiotis (2010) for a comprehensive review. These are all related because paraphrases and bidirectional entailments represent types of fluency edits. A different line of research uses classifiers to predict sentence-level fluency (Zwarts and Dras, 2008; Chae and Nenkova, 2009). These could be useful for fluency edits detection. Alternatively, user edits could be a potential source of humanproduced training data for fluency models. 3 Definition of User Edits Scope Within our approach we distinguish between edit segments, which represent the comparison (diff) between two document revisions, and user edits, which are the input for classification. An edit segment is a contiguous sequence of deleted, inserted or equal words. The difference between two document revisions (vi, vj) is represented by a sequence of edit segments E. Each edit segment </context>
</contexts>
<marker>Zwarts, Dras, 2008</marker>
<rawString>S. Zwarts and M. Dras. 2008. Choosing the right translation: A syntactically informed classification approach. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 1153–1160.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>