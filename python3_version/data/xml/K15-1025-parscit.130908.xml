<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000032">
<title confidence="0.9879615">
Structural and lexical factors in adjective placement in complex noun
phrases across Romance languages
</title>
<author confidence="0.996695">
Kristina Gulordava Paola Merlo
</author>
<affiliation confidence="0.999901">
University of Geneva University of Geneva
</affiliation>
<email confidence="0.953863">
Kristina.Gulordava@unige.ch Paola.Merlo@unige.ch
</email>
<sectionHeader confidence="0.996359" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969642857143">
One of the most common features across
all known languages is their variability
in word order. We show that differences
in the prenominal and postnominal place-
ment of adjectives in the noun phrase
across five main Romance languages is
not only subject to heaviness effects, as
previously reported, but also to subtler
structural interactions among dependen-
cies that are better explained as effects of
the principle of dependency length min-
imisation. These effects are almost purely
structural and show lexical conditioning
only in highly frequent collocations.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.968267372093024">
One of the most widely observed characteristics
of all languages is the variability in the linear or-
der of their words, both across and within a single
language. In this study, we concentrate on word
order alternations where one structure can be lin-
earised in two different ways. Consider, for exam-
ple, the case when a phrasal verb (V + particle)
has a direct object (NP), in English. Two alter-
native orders are possible: VP1 = V NP Prt, and
VP2 = V Prt NP. If the NP is heavy, as defined in
number of words or number of syllables, it will be
frequently placed after the Prt, yielding the V-Prt-
NP order. Compare, for instance Call me up! to
Call up the customer who called yesterday. This
tendency is also formulated as a Principle of End
Weight, where phrases are presented in order of
increasing weight (Wasow, 2002). Cases of heavy
NP-shift (Stallings et al., 1998), dative alternation
(Bresnan et al., 2007) and other alternation pref-
erences among verbal dependents are traditionally
evoked to argue in favour of the “heaviness” effect.
In this work, we study the alternations in the
noun-phrase domain, much less investigated in
Figure 1: Percent of postnominal simple (green)
and heavy (red) adjectives across seventeen lan-
guages.
connection with the heaviness effect. Abeill´e and
Godard (2000) introduce the heaviness of adjec-
tive phrases as a principle explaining their post-
nominal placement compared to ‘light’ adjectives
in French. Their observations have been recently
confirmed in a corpus study by Thuilier (2012).
Cross-linguistically, the data that we have col-
lected across many languages and several families,
presented in Figure 1, confirm the heaviness effect
for adjectives1. By extracting relevant statistics
from gold dependency annotated corpora, we can
observe that heavy adjectives (adjective phrases of
at least two words) appear more frequently post-
nominally than simple adjectives.
While the effect of size or heaviness is well-
documented, this statistics is very coarse and it
confounds various linguistic factors, such as types
</bodyText>
<footnote confidence="0.72684575">
1We use the following languages and treebanks: English,
Czech, Spanish, Chinese, Catalan, German, Italian (Hajiˇc et
al., 2009), Danish, Dutch, Portuguese, Swedish (Buchholz
and Marsi, 2006), Latin, Ancient Greek (Haug and Jøhndal,
2008), Hungarian (Csendes et al., 2005), Polish (Woli´nski et
al., 2011), Arabic (Zeman et al., 2012), French (McDonald et
al., 2013). The extraction is based on the conversion to the
universal part-of-speech tags (Petrov et al., 2012).
</footnote>
<page confidence="0.926704">
247
</page>
<note confidence="0.9804725">
Proceedings of the 19th Conference on Computational Language Learning, pages 247–257,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999673125">
of adjectives, and annotation conventions of dif-
ferent corpora. From a typological perspective,
the formulation needs to be refined from a pref-
erence of end weight to a preference for all el-
ements being closer to the governing head: lan-
guages with Verb-Object dominant order tend to
put constituents in ‘short before long’ order, while
Object-Verb languages, like Japanese or Korean,
do the reverse (Hawkins, 1994; Wasow, 2002).
A more general explanation for the weight effect
has been sought in a general tendency to minimise
the length of the dependency between two related
words, called Dependency Length Minimisation
(DLM, Temperley (2007), Gildea and Temperley
(2007)).
In this paper, we look at the structural factors,
such as DLM, and lexical factors that play a role
in adjective-noun word order alternations in Ro-
mance languages and the predictions they make
on prenominal or postnominal placement of adjec-
tives. We concentrate on a smaller set of languages
than those shown in Figure 1 to be able to study
finer-grained effects than what can be observed at
a very large scale and across many different cor-
pus annotation schemes. We choose Romance lan-
guages because they show a good amount of vari-
ation in the word order of the noun phrase.
The DLM principle can be stated as follows:
if there exist possible alternative orderings of a
phrase, the one with the shortest overall depen-
dency length (DL) is preferred.
Consider, again, the case when a phrasal verb
(verb + particle) has a direct object (NP). Two al-
ternative orders are possible: VP1 = V NP Prt,
whose length is DL1 and VP2 = V Prt NP, whose
length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _
INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _
|Prt |+ 1. If DL1 is bigger than DL2, then VP2
is preferred over VP1. Unlike the principle of End
Weight, this explanation applies also to languages
with a different word order than English.
The observation that human languages appear
to minimise the distance between related words is
well documented in sentence processing (Gibson,
1998; Hawkins, 1994; Hawkins, 2004), in cor-
pus properties of treebanks (Gildea and Temper-
ley, 2007; Futrell et al., 2015), in diachronic lan-
guage change (Tily, 2010). It is usually interpreted
as a means to reduce memory load and support ef-
ficient communication. Dependency length min-
imisation has been demonstrated on a large scale
in the verbal domain and at the sentence level, but
has not yet been investigated in the more limited
nominal domain, where dependencies are usually
shorter and might create lighter processing loads
that do not need to be minimised. In applying the
general principle of DLM to the dependency struc-
ture of noun phrases, our goal is to test to what
extent the DLM principle predicts the observed
adjective-noun word order alternation patterns.
In this paper, we develop and discuss a more
complex variant of a model described previously
(Gulordava et al., 2015) and extend its analysis.
First, we investigate whether the more complex
DLM principle is necessary to explain our findings
or if the simpler heaviness effect demonstrated for
many languages in Figure 1 is sufficient. The
answer is positive: the complexity introduced by
DLM is necessary. Then, we develop a more de-
tailed analysis of the only prediction of the model
that is only weakly confirmed, showing that this
result still holds under different definitions of de-
pendency length. We also present an in-depth
study to show that the DLM effect is structural, as
assumed, and not lexical. While it is well-known
that in French prenominal and post-nominal place-
ment of adjectives is sometimes lexically-specific
and meaning-dependent, this is not often the case
in other languages like Italian, and does not ex-
plain the extent of the variation.
</bodyText>
<sectionHeader confidence="0.8885795" genericHeader="introduction">
2 Dependency length minimisation in the
noun phrase
</sectionHeader>
<bodyText confidence="0.999359176470588">
In this section, we summarise the model in Gulor-
dava et al. (2015). In the next section we propose
a more complex model and study some factors in
depth. Gulordava et al. (2015) consider a proto-
typical noun phrase with an adjective phrase as a
modifier. They assume a simplified noun phrase
with only one adjective modifier adjacent to the
noun and two possible placements for an adjective
phrase: post-nominal and prenominal. The adjec-
tive modifier can be a complex phrase with both
left and right dependents (α and Q, respectively).
The noun phrase can have parents and right mod-
ifiers (X and Y, respectively). The structures for
the possible cases are shown in Figure 2.
These structures correspond to examples like
those shown in (1), in Italian (X=‘vede’,
Adj=‘bella’, N=‘casa’, Y= ‘al mare’).
</bodyText>
<page confidence="0.989348">
248
</page>
<figure confidence="0.983542181818182">
d0
1
DL1: X [NP [AP α Adj Q ] N Y ]
(a) Left external dependent, prenominal adjective
(c) Right external dependent, prenominal adjective
d00
1
d00
2
DL2: [NP N [AP α Adj Q ] Y ] X
(d) Right external dependent, postnominal adjective
</figure>
<figureCaption confidence="0.827474333333333">
Figure 2: Noun phrase structure variants and the
dependencies relevant for the DLM calculation
with right noun dependent Y.
</figureCaption>
<tableCaption confidence="0.759533666666667">
Table 1: Dependency length difference for differ-
ent types of noun phrases. By convention, we al-
ways calculate DL1 − DL2.
</tableCaption>
<bodyText confidence="0.96834826984127">
(1) a....vede la bella casa al mare.
(’..sees the beautiful house at the sea’)
b....vede la casa bella al mare.
(’..sees the house beautiful at the sea’)
c. La bella casa al mare e` vuota.
(’the beautiful house at the sea is empty.’)
d. La casa bella al mare e` vuota.
(’the house beautiful at the sea is empty.’)
The differences in dependency lengths pre-
dicted by DLM are summarized in Table 1. DLM
makes predictions on adjective placement with re-
spect to the noun —prenominal or postnominal—
given the dependents of the adjectives, α and Q,
and given the dependent of the noun Y.
The column RightNP=No shows the depen-
dency length difference for the two cases where
the noun does not have a right dependent Y. Given
that the calculation of DL differences is always
calculated as DL1 − DL2, the fact that the cell
(X=Left, RightNP=No) holds a positive value in-
dicates that DL1 &gt; DL2 and that the differ-
ence in length depends only on Q and not on
α. Conversely, the negative value of (X=Right,
RightNP=No) shows that DL1 &lt; DL2 and that
the difference in length does not depend on Q,
but only on α. This is not intuitive: intuitively,
one would expect that whether the Adjective is
left or right of the Noun depends on the relative
lengths of α and Q, but instead if we look at all
the dependencies that come into play for a noun
phrase in a larger structure, the adjective position
depends on only one of the two dependents. The
table also shows that, on average, across all the
cells, the weights of α are less than zero while the
weights of Q are greater than zero. This indicates
that DL1 &lt; DL2, which means that globally the
prenominal adjective order is preferred.
DLM also makes predictions on adjective place-
ment with respect to the noun given the depen-
dents of the noun. Here the predictions of DLM
are not intuitive. DLM predicts that when the ex-
ternal dependency is right (the dependency from
the noun to its parent, X=right), then the adjective
is prenominal, else it is postnominal. To spell this
out, DLM predicts that, for example, we should
find more prenominal adjectives in subject NPs
than in NPs in object position. We discuss this
odd prediction below.
Another prediction that will be investigated in
detail is that when the noun has a right depen-
dent, the prenominal adjective position is more
preferred than when there is no right dependent, as
evinced by the fact that the RightNP = Yes column
is always greater than the RightNP = No column.
Gulordava et al. (2015) develop a mixed-effects
model to test which of the fine-grained predictions
derived from DLM are confirmed by the data pro-
vided by the dependency annotated corpora of five
main Romance languages. The different elements
in the DLM configuration are encoded as four fac-
tors: corresponding to the factors illustrated in
Figure 2 and example (1), represented as binary
or real-valued variables: LeftAP - the cumulative
</bodyText>
<figure confidence="0.99073005">
d0
2 d0
3
d00
3
DL2: X [NP N [AP α Adj Q ] Y ]
(b) Left external dependent, postnominal adjective
DL1: [NP [AP α Adj Q ] N Y ] X
d00
d00 2
1
d0 d0
1 2
d0
3
d00
3
RightNP=Yes RightNP=No
X=Left |Q |− |α |2|Q |+ 1
X=Right −3|α |− 2 −2|α |− 1
</figure>
<page confidence="0.993666">
249
</page>
<bodyText confidence="0.999991382352941">
length (in words) of all left dependents of the ad-
jective, indicated as α in Figure 2; RightAP - the
cumulative length (in words) of all right depen-
dents of the adjective, indicated as Q in Figure 2;
ExtDep - the direction of the arc from the noun to
its parent X, an indicator variable; RightNP - the
indicator variable representing the presence or ab-
sence of the right dependent of the noun, indicated
as Y in Figure 2. 2
Their findings partly confirm the predictions
about adjective placement with respect to the noun
given the adjective dependents. The DLM predic-
tions about the position of the noun with respect
to its parent are instead not confirmed. Finally, the
prediction related to the presence of a right depen-
dent of the noun on the placement of the adjective
are confirmed.
In the next two sections, we replicate and inves-
tigate in more detail these results. First, we de-
velop and discuss a more detailed model, where
not only the factors, but also their interactions are
taken into account. Then, we compare the pre-
dictions of the DLM model to the predictions of
a simpler heaviness account, and confirm that the
complexity of DLM is needed, as a simpler model
based on heaviness of the adjective does not yield
the same effects. Then, we discuss the external
dependency factor, which, in the more complex
model with interactions, is a significant factor. Fi-
nally, the RightNP factor is significant in the fitted
model. The presence of a noun dependent on the
right of the noun favours a prenominal placement,
as predicted by DLM. We investigate the lexical
aspects of this result in a more detailed case study.
</bodyText>
<sectionHeader confidence="0.8931085" genericHeader="method">
3 Analysis of Dependency Minimisation
Factors
</sectionHeader>
<bodyText confidence="0.999976">
The analysis that we develop here is based on the
assumption that DLM is exhibited by the depen-
dencies in the avalailable dependency-annotated
corpora for the five Romance languages.
</bodyText>
<subsectionHeader confidence="0.998132">
3.1 Materials: Dependency treebanks
</subsectionHeader>
<bodyText confidence="0.9993354">
The dependency annotated corpora of five Ro-
mance languages are used: Catalan, Spanish, Ital-
ian (Hajiˇc et al., 2009), French (McDonald et
2In addition, to account for lexical variation, they include
adjective tokens (or lemmas when available) as grouping vari-
ables introducing random effects. For example, the instances
of adjective-noun order for a particular adjective will share
the same weight value γ for the adjective variable, but across
different adjectives this value can vary.
al., 2013), and Portuguese (Buchholz and Marsi,
2006).
Noun phrases containing adjectives are ex-
tracted using part-of-speech information and de-
pendency arcs from the gold annotation. Specif-
ically, all treebanks are converted to coarse uni-
versal part-of-speech tags, using existing conven-
tional mappings from the original tagset to the uni-
versal tagset (Petrov et al., 2012). All adjectives
are identified using the universal PoS tag ‘ADJ’,
whose dependency head is a noun, tagged using
the universal PoS tag ‘NOUN’. All elements of
the dependency subtree, the noun phrase, rooted
in this noun are collected. For all languages where
this information is available, we extract lemmas
of adjective and noun tokens. The only treebank
without lemma annotation is French, for which we
extract token forms.3 A total of around 64’000 in-
stances of adjectives in noun phrases is collected,
ranging from 2’800 for Italian to 20’000 for Span-
ish.
</bodyText>
<subsectionHeader confidence="0.999145">
3.2 Method: Mixed-Effects models
</subsectionHeader>
<bodyText confidence="0.999893571428571">
The interactions of several dependency factors are
analysed using a logit mixed effect models (Bates
et al., 2014). Mixed-effect logistic regression
models (logit models) are a type of Generalized
Linear Mixed Models with the logit link function
and are designed for binomially distributed out-
comes such as Order, in our case.
</bodyText>
<subsectionHeader confidence="0.997203">
3.3 Factors and their interactions
</subsectionHeader>
<bodyText confidence="0.993814095238095">
While the original model in Gulordava et al.
(2015) represents the four main factors involved
in DLM in the noun phrase — α, Q, RightNP and
ExtDep — the predictions described above often
mention interactions, which are not directly mod-
elled in the original proposal. We introduce inter-
actions, so that the model is more faithful to the
DLM predictions, as shown in (2) and in Table 2.
We do not take directly represent the interaction
between the LeftAP and RightAP because in our
corpora these two factors were both greater than
zero in only 1% of the cases.
3During preprocessing, we exclude all adjectives and
nouns with non-lower case and non-alphabetic symbols
which can include common names. Compounds (in Span-
ish and Catalan treebanks), and English borrowings are also
excluded. Neither do we include in our analysis noun phrases
which have their elements separated by punctuation (for ex-
ample, commas or parentheses) to ensure that the placement
of the adjective is not affected by an unusual structure of the
noun phrase.
</bodyText>
<page confidence="0.93151">
250
</page>
<table confidence="0.999826454545455">
Predictor Q SE Z p
Intercept -0.157 0.117 -1.33 0.182
LeftAP 2.129 0.183 11.63 &lt; .001
RightAP 0.887 0.091 9.79 &lt; .001
RightNP -0.794 0.056 -14.24 &lt; .001
ExtDep -0.243 0.118 -2.06 0.039
RightNP:ExtDep 0.296 0.149 1.98 0.047
RightAP:RightNP:ExtDep -0.639 0.353 -1.81 0.070
Random effects Var
Adjective 1.989
Language 0.023
</table>
<tableCaption confidence="0.779277">
Table 2: Summary of the fixed and random effects in the mixed-effects logit model with interactions
(N = 15842), shown in (2). Non-significant factors are not shown.
</tableCaption>
<table confidence="0.994453333333333">
Model Df AIC BIC
Without interactions 7 12137 12190
With interactions 14 12134 12241
logLik deviance χ2 Df p
-6061.3 12123
-6052.9 12106 16.847 7 0.018∗
</table>
<tableCaption confidence="0.964723">
Table 3: Comparison of the fits of two models: the model with interactions (2) and a simpler model
without any interactions between the factors RightAP, LeftAP, RightNP and ExtDep.
</tableCaption>
<table confidence="0.5187135">
(2) yij = (LeftAP + RightAP) · RightNP·
· ExtDep × β + -YAdji + -YLangj
</table>
<bodyText confidence="0.9995275">
Contrary to the model without interactions (Gu-
lordava et al., 2015), both the ExtDep factor and
its interaction with the RightNP factor are signifi-
cant. This interaction corresponds to the four dif-
ferent NP contexts presented in Table 1. Its sig-
nificance, then, can be taken as preliminary con-
firming evidence for the distinction of these con-
texts, as predicted by DLM. A direct comparison
of the two models, with and without interactions,
shows, however, that the effects of these interac-
tions are rather small (Table 3). The log-likelihood
test shows that the model with interactions fits the
data significantly better (χ2 = 16.8,p = 0.02),
but the comparison of the Bayesian Information
Criterion scores of the two models — criterion
which strongly penalises the number of parame-
ters — suggests that the model without interac-
tions should be preferred.
</bodyText>
<subsectionHeader confidence="0.7499425">
3.4 Comparison of DLM and heaviness
model
</subsectionHeader>
<bodyText confidence="0.999808904761905">
Dependency length minimisation was introduced,
as mentioned in the introduction, to better explain
processing effects at the sentence level for which
heaviness accounts were inadequate. However,
noun phrases are small and relatively simple do-
mains. We ask, then, if a model is sufficient where
the AP is not divided into LeftAP and RightAP, but
holistically represented by the size of the whole
AP.
Specifically, a simpler Heaviness model does
not make a difference between left and right de-
pendent of adjectives: all heavy adjectives are pre-
dicted to move post-nominally. Heaviness would
also not predict the interaction between placement
and the existence of a noun dependent to the right.
We compare, then, two minimally different
models. Since neither the external dependency
factor nor the interactions were shown to be highly
significant, we compare a simplified DLM model
shown in (3) to a model where AP is represented
only by its heaviness (number of words) as in (4).
</bodyText>
<equation confidence="0.80045225">
yij = LeftAP · QLAP + RightAP · QRAP
+ RightNP · QRNP + -YAdji + -YLangj
yij = SizeAP · QHV + RightNP · QRNP
+ -YAdji + -YLangj
</equation>
<bodyText confidence="0.9986664">
The DLM model that distinguishes LeftAP
from RightAP in (3) fits the data better than a
model where AP is represented only by its heav-
iness as in (4), as can be seen in Table 4 and
from the difference in AIC values of two mod-
els (DAIC = 146). This result confirms that the
complexity introduced by DLM minimisation is
needed, and confirms DLM as a property of lan-
guage, also in noun phrases. The main conceptual
difference between heaviness accounts and DLM
</bodyText>
<page confidence="0.994523">
251
</page>
<table confidence="0.719624">
Df AIC BIC logLik deviance x2 Df p
Model with SizeAP 5 12518 12557 -6254.1 12508
Model with LeftAP, RightAP 6 12372 12418 -6179.8 12360 148.5 1 &lt; .001
</table>
<tableCaption confidence="0.997225">
Table 4: Comparison of the simplified DLM model in (3) and the heaviness model in (4).
</tableCaption>
<bodyText confidence="0.999932693548387">
accounts resides in the fact that the former do
not take into account the structure and the nature
of the context of the heavy element, while DLM
does. This model comparison shows that adjective
placement is not independent of its context.
Prediction for External Dependencies The ex-
pected effect of the external dependency of the
noun predicted by the DLM is borne out only
marginally. This factor predicts a difference be-
tween noun phrases that precede their head, for ex-
ample subjects, and noun phrases that follow their
head, for example objects. The prediction is unex-
pected, while the result that the factor is not highly
significant less so, as it is not immediately clear
why nouns preceding heads should behave differ-
ently from nouns that follow heads.
A possible explanation for this mismatch of
the predictions and the observed data patterns lies
in the assumptions underlying the DLM princi-
ple. We have assumed a definition of dependency
length as the number of words between the head
and the dependent, as found in the corpus annota-
tion. Our data are annotated using a content-head
rule, which assumes that the noun is the head of
the noun phrase. Hawkins (1994), in his well-
developed variant of DLM, postulates that min-
imisation occurs on the dependencies between the
head and the edge of the dependent phrase. For
noun phrases, the relevant dependencies will span
between the determiner which unambiguously de-
fines the left edge of the noun phrase and the head
of NP (e.g., a verb). The predictions of Hawkins’
theory for adjective placement will therefore differ
from the DLM predictions based on our definition.
As can be observed from Figure 2, the di and d1
dependencies to the left edge of the NP will be
of equal length in cases (a) and (b) (similarly to
d2 and d2 in cases (c) and (d)). The external de-
pendency is predicted therefore not to affect the
resulting adjective placement, as observed in the
data. This result lends weak support to a theory
where in this case the relevant dependency is be-
tween the parent and the edge of the dependent.
A question remains of what dependencies are
minimised when the noun phrase does not have a
determiner and the left edge of the noun phrase is
ambiguous.4 This issue is difficult to test in prac-
tice in our corpora. First, there are many more
cases (84% versus 16%) with left ExtDep (X is
on the right, e.g. for object NPs) than with right
ExtDep (X is on the left, e.g. for subjects) in Ro-
mance languages. This is because all of them, ex-
cept French, can optionally omit subjects. More-
over, the function of the NP, subject or object, and
therefore the ExtDep variable, correlates with the
definiteness of the NP. NPs in object position take
an article 75% of time while NPs in subject po-
sition take an article 96% of time. Consequently,
NPs without articles and on the left of the head are
observed only 135 times in our data sample (across
all languages). This small number of cases did not
allow us to develop a model.
</bodyText>
<sectionHeader confidence="0.9854855" genericHeader="method">
4 In-depth study of the RightNP
dependency factor
</sectionHeader>
<bodyText confidence="0.999939222222223">
The most novel result of the model in Gulordava
et al. (2015), extended here to the more complex
model (2) concerns the interaction between the ad-
jective position and the RightNP. This effect would
not be predicted by a heaviness explanation and
even in the DLM framework it is surprising that
minimisation should apply to such a short depen-
dency. We investigate this interaction in more de-
tail and ask two questions: is this effect evenly
spread across different nouns and adjectives or is it
driven by some lexical outliers? what are the lexi-
cal effects of the noun and its dependent? We anal-
yse a large amount of data constructed to be a rep-
resentative sample of adjective variation for sev-
eral nouns (around thirty for each language) and
very many adjectives and investigate noun phrases
with a right dependent introduced by the preposi-
tion ‘de/di’5.
</bodyText>
<footnote confidence="0.9997085">
4In one of his analyses, Hawkins claims that adjectives
define unambiguously the left edge of the NP, but this as-
sumption is controversial.
5For Italian, the preposition is ‘di’, while for other three
languages it is ‘de’. We do not consider complex prepositions
such as ‘du’ in French or ‘do’ in Portuguese.
</footnote>
<page confidence="0.992771">
252
</page>
<subsectionHeader confidence="0.7037535">
4.1 Data extracted from large PoS-tagged
corpora
</subsectionHeader>
<bodyText confidence="0.999966763157894">
We extract the data by querying automatically
a collection of corpora brought together by the
SketchEngine project (Kilgarriff et al., 2014).
This web-interface-based collection allows par-
tially restricted access to billion-word corpora of
Italian (4 billions of words), French (11 billions),
Spanish (8.7 billions) and Portuguese (4.5 bil-
lions). The corpora are collected by web-crawling
and automatically PoS-tagged. A similar Catalan
corpus was not available through this service.
First, we define the set of seed nouns that will be
queried. For each language, we use our treebanks
to find the list of the two-hundred most frequent
nouns which take the ‘di/de’ preposition as a com-
plement. A noun has ‘di/de’ as its right dependent
if there is a direct head-dependent link between
these elements in the gold annotation. Nouns in
the list which could be ambiguous between dif-
ferent types of parts of speech are replaced man-
ually. We randomly sample around thirty nouns,
based on the percentage of their co-occurrence
with ‘di/de’. Given the list of seed nouns, we
automatically queried the four corpora with sim-
ple regular patterns containing these nouns to ex-
tract cases of prenominal and postnominal noun-
adjective co-occurrences.6
For each noun, we collected a maximum of
100’000 matches for each of the two patterns,
which is the SketchEngine service limit. These
matches include left and right contexts of the pat-
tern and allow to extract the token following the
pattern, which can be ‘di/de’ or nothing.
We modeled the data using the Logit mixed ef-
fect models, with the Order as a response vari-
able, one fixed effect (Di) and nouns and adjec-
tives as random effects. We fit the maximal model
with both slope and intercept parameters, as shown
in model (5).
</bodyText>
<listItem confidence="0.530888">
(5) y = Di · (QDi + QAdji + QNounj)
+ γAdji + γNounj
</listItem>
<bodyText confidence="0.9995865">
We fit our models on a sample of data of around
200’000 instances of adjective-noun alternations
for each language, equally balanced for noun
phrases with Di = True and Di = False.
</bodyText>
<footnote confidence="0.99435675">
6Our patterns were of the type ‘[tag=”ADJ”] noun’ and
‘noun [tag=”ADJ”]’, where the tag field is specified for the
PoS tag of a token. In our case, ‘A.’ was the tag for adjectives
in , and ‘ADJ’ in Italian, French and Spanish.
</footnote>
<figureCaption confidence="0.74069225">
Figure 3: Percent postnominal placement for the
thirty most frequent adjectives in French. (Noun
phrase has a right ‘de’-complement (green) and it
does not (red).
</figureCaption>
<subsectionHeader confidence="0.506157">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999908807692308">
The data shows that the Di effect is small, but
highly significant for all languages. The resulting
values are similar: for French QDi = −0.84, Por-
tuguese QDi = −0.95, Italian QDi = −1.14 and
Spanish QDi = −1.65 (all p &lt; 0.001).
Figure 3 illustrates the Di effect for French (cu-
mulative for all nouns). We observe that most
of the adjectives appear more frequently prenom-
inally in noun phrases with a ‘de’ complement
than in noun phrases without a ‘de’ complement
(green columns are smaller than corresponding red
columns). Importantly, we observe a very similar
picture cross-linguistically for all four languages
and for the adjective alternation across the major-
ity of the nouns (if considered independently), as
shown in Figure 4.
This result agrees with our predictions, and
shows that DLM effects show up even in short
spans, where they are not necessarily expected.
If a postnominal adjective intervenes between the
noun and the dependent, the dependency length in-
creases only by one word (with respect to the noun
phrase with the prenominal adjective). Our results
nevertheless suggest that even such short depen-
dencies are consistently minimised. This effect is
confirmed in all languages.
</bodyText>
<subsectionHeader confidence="0.994548">
4.3 Lexical effects on adjective placement
</subsectionHeader>
<bodyText confidence="0.9983275">
One of the lexical factors that could play a con-
founding role for the prenominal placement of ad-
</bodyText>
<page confidence="0.998658">
253
</page>
<figureCaption confidence="0.9904825">
Figure 4: Percent postnominal placement for the thirty most frequent adjectives in Italian, Spanish, and
Portuguese (in this order). (Noun phrase has a right ‘de/di’-complement (green) and it does not (red).
</figureCaption>
<bodyText confidence="0.999766133333333">
jectives in Di constructions is the strength of the
‘Noun + di/de + Complement’ collocation. For ex-
ample, in the French compound ‘taux de chomage’
(‘unemployement rate’) the placement of an ad-
jective after the compound — ‘taux de chomage
important’ — is preferred in our corpora to the
adjacent postnominal placement (‘taux important
de chomage’). In our analysis, we do not extract
these types of post-NP adjectives. From this per-
spective, a drop in the percentage of postnominal
adjectives in ‘di’ cases could indicate that adjec-
tives prefer not to intervene between nouns and
their complements. We hypothesize that this de-
pendency is more strongly minimised than other
dependencies in the noun phrase because of this
strong lexical link.
We confirm that the Di effect is an interaction of
the DLM principle and lexical properties of com-
pounds by a further preliminary analysis of col-
locations. From the French data, we selected a
subset with the most frequent ‘Noun + de + Com-
plement’ sequences (10 for each seed noun) and a
subset with infrequent sequences (100 random de-
complements for each seed noun). We assume that
the frequency of the sequence is an indicator of the
collocational strength, so that highly frequent se-
quences are collocations while low frequency se-
quences are non-collocational combinations. The
first subset has a proportion of 78% prenominal
and 22% postnominal adjectives, while the second
subset has 61% prenominal and 39% postnominal
adjectives. We confirm, then, that in the frequent
collocations there is a substantial lexical effect in
adjective placement. However, we also observe a
preference of prenominal placement for the infre-
quent ‘Noun + de + Complement’ sequences that
are not collocational combinations, since prenom-
inal placement is still much higher than what is
observed for French adjectives, on average (46%
prenominal and 54% postnominal in our sample
of data). These numbers suggest that the Di effect
reported in the previous section is not a result of
mere lexical collocation effects and that, for low
frequency combinations at least, DLM is at play.
A different kind of lexical effect is shown in
Figure 5. Here we plot the percent postnominal
placement of the adjective, if the noun has a com-
plement introduced by di (of), che (that), per (for),
in Italian. We notice that adjective placement is
no longer as majoritarily prenominal for the right
dependent introduced by che and per as it is for
di. The main difference between di (of) and che
(that), per (for) is that the former introduces a PP
that is inside the NP that selects it, while che and
per usually do not, they are adjuncts, or infiniti-
vals or clauses. In the linguistic literature, this is
a distinction between arguments and adjuncts of
the noun and it is represented structurally. This
distinction is, then, a lexically-induced structural
distinction, and not simply a collocation.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999928857142857">
Our work occupies the middle ground between
detailed linguistic investigations of weight effect
in chosen constructions of well-studied languages
and large scale demonstrations of the dependency
length minimisation principle.
Gildea and Temperley (2007) demonstrated that
DLM applies for the dependency annotated cor-
pora in English and German. They calculate ran-
dom and optimal dependency lengths for each
sentence given its unordered dependency tree
and compare these values to actual dependency
lengths. English lengths are shown to be close
to optimal, but for German this tendency is not as
clear. A recent study of Futrell et al. (2015) applies
</bodyText>
<page confidence="0.997046">
254
</page>
<figureCaption confidence="0.879895">
Figure 5: Percent postnominal placement for thirty most frequent adjectives in Italian, followed by
</figureCaption>
<bodyText confidence="0.976031516129032">
function word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green)
and it does not (red)).
this analysis on a large-scale, for more than thirty
languages that have dependency treebanks. Their
results also confirm the correspondence between
the dependency annotation and the experimental
data, something that has been reported previously
(Merlo, 1994; Roland and Jurafsky, 2002).
Much work in theoretical linguistics addresses
the adjective-noun order in Romance languages.
Such work typically concentrates on lexico-
semantic aspects of adjective placement (Cinque,
2010; Alexiadou, 2001). In our work, we account
for the strong lexical prenominal or postnominal
preferences of adjectives by including them as ran-
dom effects in our models.
Closest to our paper is the theoretical work of
Abeill´e and Godard (2000) on the placement of
adjective phrases in French and recent corpus-
based work by Fox and Thuilier (2012) and
Thuilier (2012). Fox and Thuilier (2012) use a
dependency annotated corpus of French to extract
cases of adjective-noun variation and their syntac-
tic contexts. They model the placement of an ad-
jective as a lexical, syntactic and semantic multi-
factorial variation. They find, for example, that
phonologically heavy simple adjectives tend to be
postnominal. This result highlights the distinc-
tion between phonological weight and syntactic
weight, a topic which we do not address in the cur-
rent work.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999628125">
In this paper, we have shown that differences in the
prenominal and postnominal placement of adjec-
tives in the noun phrase across five main Romance
languages is not only subject to heaviness effects,
but to subtler dependency length minimisation ef-
fects. These effects are almost purely structural
and show lexical conditioning only in highly fre-
quent collocations.
The subtle interactions found in this work raise
questions about the exact definition of what depen-
dencies are minimised and to what extent a given
dependency annotation captures these distinctions.
Future work will investigate more refined defini-
tions of dependency length minimisation, that dis-
tinguish different kinds of dependencies with dif-
ferent weights.
</bodyText>
<sectionHeader confidence="0.996922" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999964428571429">
We gratefully acknowledge the partial funding of
this work by the Swiss National Science Founda-
tion, under grant 144362. Part of this work was
conducted during the visit of The first author to the
Labex EFL in Paris and Alpage INRIA group. We
thank Benoit Crabb´e for the fruitful discussions
during this period.
</bodyText>
<sectionHeader confidence="0.992239" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9327021875">
Anne Abeill´e and Daniele Godard. 2000. French word
order and lexical weight. In Robert D. Borsley, edi-
tor, The nature and function of Syntactic Categories,
volume 32 of Syntax and Semantics, pages 325–360.
BRILL.
Artemis Alexiadou. 2001. Adjective syntax and noun
raising: word order asymmetries in the DP as the
result of adjective distribution. Studia linguistica,
55(3):217–248.
Douglas Bates, Martin Maechler, Ben Bolker, and
Steven Walker, 2014. lme4: Linear mixed-effects
models using Eigen and S4. R package version 1.1-
7.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Har-
ald Baayen. 2007. Predicting the dative alternation.
In G. Boume, I. Kraemer, and J. Zwarts, editors,
</reference>
<page confidence="0.989058">
255
</page>
<reference confidence="0.999480788990826">
Cognitive Foundations of Interpretation, pages 69–
94. Royal Netherlands Academy of Science, Ams-
terdam.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, CoNLL-X
’06, pages 149–164, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Guglielmo Cinque. 2010. The Syntax of Adjectives: A
Comparative Study. MIT Press.
D´ora Csendes, J´anos Csirik, Tibor Gyim´othy, and
Andr´as Kocsor. 2005. The Szeged treebank.
In Text, Speech and Dialogue, pages 123–131.
Springer.
Gwendoline Fox and Juliette Thuilier. 2012. Pre-
dicting the Position of Attributive Adjectives in
the French NP. In Daniel Lassiter and Marija
Slavkovik, editors, New Directions in Logic, Lan-
guage and Computation, Lecture Notes in Computer
Science, pages 1–15. Springer, April.
Richard Futrell, Kyle Mahowald, and Edward Gibson.
2015. Large-Scale Evidence of Dependency Length
Minimization in 37 Languages. (Submitted to Pro-
ceedings of the National Academy of Sciences of the
United States of America).
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1–
76.
Daniel Gildea and David Temperley. 2007. Optimiz-
ing Grammars for Minimum Dependency Length.
In Proceedings of the 45th Annual Conference
of the Association for Computational Linguistics
(ACL’07), pages 184–191, Prague, Czech Republic.
Kristina Gulordava, Paola Merlo, and Benoit Crabb´e.
2015. Dependency length minimisation effects in
short spans: a large-scale analysis of adjective place-
ment in complex noun phrases. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics: Short Papers (ACL’15).
Jan Haji&amp;quot;c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Marti, Lluis
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan &amp;quot;St&amp;quot;ep´anek, Pavel Stra&amp;quot;n´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning: Shared Task, CoNLL ’09,
pages 1–18, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dag T. T. Haug and Marius L. Jøhndal. 2008. Cre-
ating a Parallel Treebank of the Old Indo-European
Bible Translations. In Proceedings of the 2nd Work-
shop on Language Technology for Cultural Heritage
Data, pages 27–34, Marrakech, Morocco.
John A Hawkins. 1994. A performance theory of or-
der and constituency. Cambridge University Press,
Cambridge.
John A. Hawkins. 2004. Efficiency and Complexity in
Grammars. Oxford linguistics. Oxford University
Press, Oxford, UK.
Adam Kilgarriff, Vit Baisa, Jan Bu&amp;quot;sta, Milo&amp;quot;s
Jakubi&amp;quot;cek, Vojt&amp;quot;ech Kov´a&amp;quot;r, Jan Michelfeit, Pavel
Rychl`y, and Vit Suchomel. 2014. The sketch en-
gine: ten years on. Lexicography, 1(1):7–36.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria
Bertomeu Castell´o, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92–97. Association for
Computational Linguistics.
Paola Merlo. 1994. A corpus-based analysis of verb
continuation frequencies for syntactic processing.
Journal of Psycholinguistic Research, 23(6):435–
457.
Slav Petrov, Dipanjan Das, and Ryan T. McDonald.
2012. A Universal Part-of-Speech Tagset. In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12),
pages 2089–2096, Istanbul, Turkey.
Douglas Roland and Daniel Jurafsky. 2002. Verb sense
and verb subcategorization probabilities. In Paola
Merlo and Suzanne Stevenson, editors, The lexi-
cal basis of sentence processing: Formal, compu-
tational, and experimental issues. John Benjamins.
Lynne M Stallings, Maryellen C MacDonald, and
Padraig G O’Seaghdha. 1998. Phrasal ordering
constraints in sentence production: Phrase length
and verb disposition in heavy-NP shift. Journal of
Memory and Language, 39(3):392–417.
David Temperley. 2007. Minimization of dependency
length in written English. Cognition, 105(2):300–
333.
Juliette Thuilier. 2012. Contraintes pr´ef´erentielles et
ordre des mots en franc¸ais. Ph.D. Thesis, Universit´e
Paris-Diderot - Paris VII, Sep.
Harry Joel Tily. 2010. The role of processing com-
plexity in word order variation and change. Ph.D.
Thesis, Stanford University.
Thomas Wasow. 2002. Postverbal Behavior. CSLI
Publications.
Marcin Woli´nski, Katarzyna Głowi´nska, and Marek
´Swidzi´nski. 2011. A Preliminary Version of Sklad-
nica—a Treebank of Polish. In Zygmunt Vetulani,
</reference>
<page confidence="0.976">
256
</page>
<reference confidence="0.995254583333333">
editor, Proceedings of the 5th Language &amp; Technol-
ogy Conference: Human Language Technologies as
a Challenge for Computer Science and Linguistics,
pages 299–303, Poznan, Poland.
Daniel Zeman, David Mareˇcek, Martin Popel,
Loganathan Ramasamy, Jan ˇStˇep´anek, Zdenˇek
ˇZabokrtsk´y, and Jan Hajiˇc. 2012. HamleDT: To
Parse or Not to Parse? In Proceedings of the Eight
International Conference on Language Resources
and Evaluation (LREC’12), pages 23–25, Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
</reference>
<page confidence="0.997155">
257
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372074">
<title confidence="0.9963485">Structural and lexical factors in adjective placement in complex phrases across Romance languages</title>
<author confidence="0.999946">Kristina Gulordava Paola Merlo</author>
<affiliation confidence="0.999886">University of Geneva University of Geneva</affiliation>
<author confidence="0.377144">Kristina Gulordavaunige ch Paola Merlounige ch</author>
<abstract confidence="0.999524533333334">One of the most common features across all known languages is their variability in word order. We show that differences in the prenominal and postnominal placement of adjectives in the noun phrase across five main Romance languages is not only subject to heaviness effects, as previously reported, but also to subtler structural interactions among dependencies that are better explained as effects of the principle of dependency length minimisation. These effects are almost purely structural and show lexical conditioning only in highly frequent collocations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Daniele Godard</author>
</authors>
<title>French word order and lexical weight.</title>
<date>2000</date>
<booktitle>The nature and function of Syntactic Categories, volume 32 of Syntax and Semantics,</booktitle>
<pages>325--360</pages>
<editor>In Robert D. Borsley, editor,</editor>
<publisher>BRILL.</publisher>
<marker>Abeill´e, Godard, 2000</marker>
<rawString>Anne Abeill´e and Daniele Godard. 2000. French word order and lexical weight. In Robert D. Borsley, editor, The nature and function of Syntactic Categories, volume 32 of Syntax and Semantics, pages 325–360. BRILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Artemis Alexiadou</author>
</authors>
<title>Adjective syntax and noun raising: word order asymmetries in the DP as the result of adjective distribution.</title>
<date>2001</date>
<booktitle>Studia linguistica,</booktitle>
<pages>55--3</pages>
<contexts>
<context position="32588" citStr="Alexiadou, 2001" startWordPosition="5438" endWordPosition="5439">ction word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green) and it does not (red)). this analysis on a large-scale, for more than thirty languages that have dependency treebanks. Their results also confirm the correspondence between the dependency annotation and the experimental data, something that has been reported previously (Merlo, 1994; Roland and Jurafsky, 2002). Much work in theoretical linguistics addresses the adjective-noun order in Romance languages. Such work typically concentrates on lexicosemantic aspects of adjective placement (Cinque, 2010; Alexiadou, 2001). In our work, we account for the strong lexical prenominal or postnominal preferences of adjectives by including them as random effects in our models. Closest to our paper is the theoretical work of Abeill´e and Godard (2000) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier (2012) and Thuilier (2012). Fox and Thuilier (2012) use a dependency annotated corpus of French to extract cases of adjective-noun variation and their syntactic contexts. They model the placement of an adjective as a lexical, syntactic and semantic multifactorial variation. Th</context>
</contexts>
<marker>Alexiadou, 2001</marker>
<rawString>Artemis Alexiadou. 2001. Adjective syntax and noun raising: word order asymmetries in the DP as the result of adjective distribution. Studia linguistica, 55(3):217–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Bates</author>
<author>Martin Maechler</author>
<author>Ben Bolker</author>
<author>Steven Walker</author>
</authors>
<title>lme4: Linear mixed-effects models using Eigen and S4. R package version 1.1-7.</title>
<date>2014</date>
<contexts>
<context position="15213" citStr="Bates et al., 2014" startWordPosition="2535" endWordPosition="2538"> is a noun, tagged using the universal PoS tag ‘NOUN’. All elements of the dependency subtree, the noun phrase, rooted in this noun are collected. For all languages where this information is available, we extract lemmas of adjective and noun tokens. The only treebank without lemma annotation is French, for which we extract token forms.3 A total of around 64’000 instances of adjectives in noun phrases is collected, ranging from 2’800 for Italian to 20’000 for Spanish. 3.2 Method: Mixed-Effects models The interactions of several dependency factors are analysed using a logit mixed effect models (Bates et al., 2014). Mixed-effect logistic regression models (logit models) are a type of Generalized Linear Mixed Models with the logit link function and are designed for binomially distributed outcomes such as Order, in our case. 3.3 Factors and their interactions While the original model in Gulordava et al. (2015) represents the four main factors involved in DLM in the noun phrase — α, Q, RightNP and ExtDep — the predictions described above often mention interactions, which are not directly modelled in the original proposal. We introduce interactions, so that the model is more faithful to the DLM predictions,</context>
</contexts>
<marker>Bates, Maechler, Bolker, Walker, 2014</marker>
<rawString>Douglas Bates, Martin Maechler, Ben Bolker, and Steven Walker, 2014. lme4: Linear mixed-effects models using Eigen and S4. R package version 1.1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Anna Cueni</author>
<author>Tatiana Nikitina</author>
<author>Harald Baayen</author>
</authors>
<title>Predicting the dative alternation.</title>
<date>2007</date>
<booktitle>Cognitive Foundations of Interpretation,</booktitle>
<pages>69</pages>
<editor>In G. Boume, I. Kraemer, and J. Zwarts, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="1721" citStr="Bresnan et al., 2007" startWordPosition="274" endWordPosition="277">for example, the case when a phrasal verb (V + particle) has a direct object (NP), in English. Two alternative orders are possible: VP1 = V NP Prt, and VP2 = V Prt NP. If the NP is heavy, as defined in number of words or number of syllables, it will be frequently placed after the Prt, yielding the V-PrtNP order. Compare, for instance Call me up! to Call up the customer who called yesterday. This tendency is also formulated as a Principle of End Weight, where phrases are presented in order of increasing weight (Wasow, 2002). Cases of heavy NP-shift (Stallings et al., 1998), dative alternation (Bresnan et al., 2007) and other alternation preferences among verbal dependents are traditionally evoked to argue in favour of the “heaviness” effect. In this work, we study the alternations in the noun-phrase domain, much less investigated in Figure 1: Percent of postnominal simple (green) and heavy (red) adjectives across seventeen languages. connection with the heaviness effect. Abeill´e and Godard (2000) introduce the heaviness of adjective phrases as a principle explaining their postnominal placement compared to ‘light’ adjectives in French. Their observations have been recently confirmed in a corpus study by</context>
</contexts>
<marker>Bresnan, Cueni, Nikitina, Baayen, 2007</marker>
<rawString>Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald Baayen. 2007. Predicting the dative alternation. In G. Boume, I. Kraemer, and J. Zwarts, editors, Cognitive Foundations of Interpretation, pages 69– 94. Royal Netherlands Academy of Science, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3057" citStr="Buchholz and Marsi, 2006" startWordPosition="470" endWordPosition="473">ies, presented in Figure 1, confirm the heaviness effect for adjectives1. By extracting relevant statistics from gold dependency annotated corpora, we can observe that heavy adjectives (adjective phrases of at least two words) appear more frequently postnominally than simple adjectives. While the effect of size or heaviness is welldocumented, this statistics is very coarse and it confounds various linguistic factors, such as types 1We use the following languages and treebanks: English, Czech, Spanish, Chinese, Catalan, German, Italian (Hajiˇc et al., 2009), Danish, Dutch, Portuguese, Swedish (Buchholz and Marsi, 2006), Latin, Ancient Greek (Haug and Jøhndal, 2008), Hungarian (Csendes et al., 2005), Polish (Woli´nski et al., 2011), Arabic (Zeman et al., 2012), French (McDonald et al., 2013). The extraction is based on the conversion to the universal part-of-speech tags (Petrov et al., 2012). 247 Proceedings of the 19th Conference on Computational Language Learning, pages 247–257, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics of adjectives, and annotation conventions of different corpora. From a typological perspective, the formulation needs to be refined from a preferenc</context>
<context position="14186" citStr="Buchholz and Marsi, 2006" startWordPosition="2373" endWordPosition="2376">nnotated corpora for the five Romance languages. 3.1 Materials: Dependency treebanks The dependency annotated corpora of five Romance languages are used: Catalan, Spanish, Italian (Hajiˇc et al., 2009), French (McDonald et 2In addition, to account for lexical variation, they include adjective tokens (or lemmas when available) as grouping variables introducing random effects. For example, the instances of adjective-noun order for a particular adjective will share the same weight value γ for the adjective variable, but across different adjectives this value can vary. al., 2013), and Portuguese (Buchholz and Marsi, 2006). Noun phrases containing adjectives are extracted using part-of-speech information and dependency arcs from the gold annotation. Specifically, all treebanks are converted to coarse universal part-of-speech tags, using existing conventional mappings from the original tagset to the universal tagset (Petrov et al., 2012). All adjectives are identified using the universal PoS tag ‘ADJ’, whose dependency head is a noun, tagged using the universal PoS tag ‘NOUN’. All elements of the dependency subtree, the noun phrase, rooted in this noun are collected. For all languages where this information is a</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X ’06, pages 149–164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guglielmo Cinque</author>
</authors>
<title>The Syntax of Adjectives: A Comparative Study.</title>
<date>2010</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="32570" citStr="Cinque, 2010" startWordPosition="5436" endWordPosition="5437">ollowed by function word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green) and it does not (red)). this analysis on a large-scale, for more than thirty languages that have dependency treebanks. Their results also confirm the correspondence between the dependency annotation and the experimental data, something that has been reported previously (Merlo, 1994; Roland and Jurafsky, 2002). Much work in theoretical linguistics addresses the adjective-noun order in Romance languages. Such work typically concentrates on lexicosemantic aspects of adjective placement (Cinque, 2010; Alexiadou, 2001). In our work, we account for the strong lexical prenominal or postnominal preferences of adjectives by including them as random effects in our models. Closest to our paper is the theoretical work of Abeill´e and Godard (2000) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier (2012) and Thuilier (2012). Fox and Thuilier (2012) use a dependency annotated corpus of French to extract cases of adjective-noun variation and their syntactic contexts. They model the placement of an adjective as a lexical, syntactic and semantic multifacto</context>
</contexts>
<marker>Cinque, 2010</marker>
<rawString>Guglielmo Cinque. 2010. The Syntax of Adjectives: A Comparative Study. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D´ora Csendes</author>
<author>J´anos Csirik</author>
<author>Tibor Gyim´othy</author>
<author>Andr´as Kocsor</author>
</authors>
<title>The Szeged treebank.</title>
<date>2005</date>
<booktitle>In Text, Speech and Dialogue,</booktitle>
<pages>123--131</pages>
<publisher>Springer.</publisher>
<marker>Csendes, Csirik, Gyim´othy, Kocsor, 2005</marker>
<rawString>D´ora Csendes, J´anos Csirik, Tibor Gyim´othy, and Andr´as Kocsor. 2005. The Szeged treebank. In Text, Speech and Dialogue, pages 123–131. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gwendoline Fox</author>
<author>Juliette Thuilier</author>
</authors>
<title>Predicting the Position of Attributive Adjectives in the French NP.</title>
<date>2012</date>
<booktitle>In Daniel Lassiter and Marija Slavkovik, editors, New Directions in Logic, Language and Computation, Lecture Notes in Computer Science,</booktitle>
<pages>1--15</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="32917" citStr="Fox and Thuilier (2012)" startWordPosition="5492" endWordPosition="5495"> something that has been reported previously (Merlo, 1994; Roland and Jurafsky, 2002). Much work in theoretical linguistics addresses the adjective-noun order in Romance languages. Such work typically concentrates on lexicosemantic aspects of adjective placement (Cinque, 2010; Alexiadou, 2001). In our work, we account for the strong lexical prenominal or postnominal preferences of adjectives by including them as random effects in our models. Closest to our paper is the theoretical work of Abeill´e and Godard (2000) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier (2012) and Thuilier (2012). Fox and Thuilier (2012) use a dependency annotated corpus of French to extract cases of adjective-noun variation and their syntactic contexts. They model the placement of an adjective as a lexical, syntactic and semantic multifactorial variation. They find, for example, that phonologically heavy simple adjectives tend to be postnominal. This result highlights the distinction between phonological weight and syntactic weight, a topic which we do not address in the current work. 6 Conclusion In this paper, we have shown that differences in the prenominal and postnominal plac</context>
</contexts>
<marker>Fox, Thuilier, 2012</marker>
<rawString>Gwendoline Fox and Juliette Thuilier. 2012. Predicting the Position of Attributive Adjectives in the French NP. In Daniel Lassiter and Marija Slavkovik, editors, New Directions in Logic, Language and Computation, Lecture Notes in Computer Science, pages 1–15. Springer, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Futrell</author>
<author>Kyle Mahowald</author>
<author>Edward Gibson</author>
</authors>
<date>2015</date>
<booktitle>Large-Scale Evidence of Dependency Length Minimization in 37 Languages. (Submitted to Proceedings of the National Academy of Sciences of the United States of America).</booktitle>
<contexts>
<context position="5660" citStr="Futrell et al., 2015" startWordPosition="899" endWordPosition="902"> orders are possible: VP1 = V NP Prt, whose length is DL1 and VP2 = V Prt NP, whose length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _ INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _ |Prt |+ 1. If DL1 is bigger than DL2, then VP2 is preferred over VP1. Unlike the principle of End Weight, this explanation applies also to languages with a different word order than English. The observation that human languages appear to minimise the distance between related words is well documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004), in corpus properties of treebanks (Gildea and Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). It is usually interpreted as a means to reduce memory load and support efficient communication. Dependency length minimisation has been demonstrated on a large scale in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of DLM to the dependency structure of noun phrases, our goal is to test to what extent the DLM principle predicts the obs</context>
<context position="31856" citStr="Futrell et al. (2015)" startWordPosition="5333" endWordPosition="5336">e middle ground between detailed linguistic investigations of weight effect in chosen constructions of well-studied languages and large scale demonstrations of the dependency length minimisation principle. Gildea and Temperley (2007) demonstrated that DLM applies for the dependency annotated corpora in English and German. They calculate random and optimal dependency lengths for each sentence given its unordered dependency tree and compare these values to actual dependency lengths. English lengths are shown to be close to optimal, but for German this tendency is not as clear. A recent study of Futrell et al. (2015) applies 254 Figure 5: Percent postnominal placement for thirty most frequent adjectives in Italian, followed by function word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green) and it does not (red)). this analysis on a large-scale, for more than thirty languages that have dependency treebanks. Their results also confirm the correspondence between the dependency annotation and the experimental data, something that has been reported previously (Merlo, 1994; Roland and Jurafsky, 2002). Much work in theoretical linguistics addresses the adjective-noun order in </context>
</contexts>
<marker>Futrell, Mahowald, Gibson, 2015</marker>
<rawString>Richard Futrell, Kyle Mahowald, and Edward Gibson. 2015. Large-Scale Evidence of Dependency Length Minimization in 37 Languages. (Submitted to Proceedings of the National Academy of Sciences of the United States of America).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: Locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<volume>68</volume>
<issue>1</issue>
<pages>76</pages>
<contexts>
<context position="5543" citStr="Gibson, 1998" startWordPosition="882" endWordPosition="883">d. Consider, again, the case when a phrasal verb (verb + particle) has a direct object (NP). Two alternative orders are possible: VP1 = V NP Prt, whose length is DL1 and VP2 = V Prt NP, whose length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _ INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _ |Prt |+ 1. If DL1 is bigger than DL2, then VP2 is preferred over VP1. Unlike the principle of End Weight, this explanation applies also to languages with a different word order than English. The observation that human languages appear to minimise the distance between related words is well documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004), in corpus properties of treebanks (Gildea and Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). It is usually interpreted as a means to reduce memory load and support efficient communication. Dependency length minimisation has been demonstrated on a large scale in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of D</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Edward Gibson. 1998. Linguistic complexity: Locality of syntactic dependencies. Cognition, 68(1):1– 76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>David Temperley</author>
</authors>
<title>Optimizing Grammars for Minimum Dependency Length.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (ACL’07),</booktitle>
<pages>184--191</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4182" citStr="Gildea and Temperley (2007)" startWordPosition="640" endWordPosition="643">ifferent corpora. From a typological perspective, the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head: languages with Verb-Object dominant order tend to put constituents in ‘short before long’ order, while Object-Verb languages, like Japanese or Korean, do the reverse (Hawkins, 1994; Wasow, 2002). A more general explanation for the weight effect has been sought in a general tendency to minimise the length of the dependency between two related words, called Dependency Length Minimisation (DLM, Temperley (2007), Gildea and Temperley (2007)). In this paper, we look at the structural factors, such as DLM, and lexical factors that play a role in adjective-noun word order alternations in Romance languages and the predictions they make on prenominal or postnominal placement of adjectives. We concentrate on a smaller set of languages than those shown in Figure 1 to be able to study finer-grained effects than what can be observed at a very large scale and across many different corpus annotation schemes. We choose Romance languages because they show a good amount of variation in the word order of the noun phrase. The DLM principle can </context>
<context position="5637" citStr="Gildea and Temperley, 2007" startWordPosition="894" endWordPosition="898">object (NP). Two alternative orders are possible: VP1 = V NP Prt, whose length is DL1 and VP2 = V Prt NP, whose length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _ INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _ |Prt |+ 1. If DL1 is bigger than DL2, then VP2 is preferred over VP1. Unlike the principle of End Weight, this explanation applies also to languages with a different word order than English. The observation that human languages appear to minimise the distance between related words is well documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004), in corpus properties of treebanks (Gildea and Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). It is usually interpreted as a means to reduce memory load and support efficient communication. Dependency length minimisation has been demonstrated on a large scale in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of DLM to the dependency structure of noun phrases, our goal is to test to what extent the DLM pri</context>
<context position="31468" citStr="Gildea and Temperley (2007)" startWordPosition="5269" endWordPosition="5272">s a PP that is inside the NP that selects it, while che and per usually do not, they are adjuncts, or infinitivals or clauses. In the linguistic literature, this is a distinction between arguments and adjuncts of the noun and it is represented structurally. This distinction is, then, a lexically-induced structural distinction, and not simply a collocation. 5 Related work Our work occupies the middle ground between detailed linguistic investigations of weight effect in chosen constructions of well-studied languages and large scale demonstrations of the dependency length minimisation principle. Gildea and Temperley (2007) demonstrated that DLM applies for the dependency annotated corpora in English and German. They calculate random and optimal dependency lengths for each sentence given its unordered dependency tree and compare these values to actual dependency lengths. English lengths are shown to be close to optimal, but for German this tendency is not as clear. A recent study of Futrell et al. (2015) applies 254 Figure 5: Percent postnominal placement for thirty most frequent adjectives in Italian, followed by function word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green)</context>
</contexts>
<marker>Gildea, Temperley, 2007</marker>
<rawString>Daniel Gildea and David Temperley. 2007. Optimizing Grammars for Minimum Dependency Length. In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (ACL’07), pages 184–191, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Gulordava</author>
<author>Paola Merlo</author>
<author>Benoit Crabb´e</author>
</authors>
<title>Dependency length minimisation effects in short spans: a large-scale analysis of adjective placement in complex noun phrases.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics: Short Papers (ACL’15).</booktitle>
<marker>Gulordava, Merlo, Crabb´e, 2015</marker>
<rawString>Kristina Gulordava, Paola Merlo, and Benoit Crabb´e. 2015. Dependency length minimisation effects in short spans: a large-scale analysis of adjective placement in complex noun phrases. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics: Short Papers (ACL’15).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajic</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Marti</author>
<author>Lluis M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan Step´anek</author>
<author>Pavel Stran´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Hajic, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, Step´anek, Stran´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Haji&amp;quot;c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Marti, Lluis M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan &amp;quot;St&amp;quot;ep´anek, Pavel Stra&amp;quot;n´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09, pages 1–18, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dag T T Haug</author>
<author>Marius L Jøhndal</author>
</authors>
<title>Creating a Parallel Treebank of the Old Indo-European Bible Translations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd Workshop on Language Technology for Cultural Heritage Data,</booktitle>
<pages>27--34</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="3104" citStr="Haug and Jøhndal, 2008" startWordPosition="477" endWordPosition="480"> effect for adjectives1. By extracting relevant statistics from gold dependency annotated corpora, we can observe that heavy adjectives (adjective phrases of at least two words) appear more frequently postnominally than simple adjectives. While the effect of size or heaviness is welldocumented, this statistics is very coarse and it confounds various linguistic factors, such as types 1We use the following languages and treebanks: English, Czech, Spanish, Chinese, Catalan, German, Italian (Hajiˇc et al., 2009), Danish, Dutch, Portuguese, Swedish (Buchholz and Marsi, 2006), Latin, Ancient Greek (Haug and Jøhndal, 2008), Hungarian (Csendes et al., 2005), Polish (Woli´nski et al., 2011), Arabic (Zeman et al., 2012), French (McDonald et al., 2013). The extraction is based on the conversion to the universal part-of-speech tags (Petrov et al., 2012). 247 Proceedings of the 19th Conference on Computational Language Learning, pages 247–257, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics of adjectives, and annotation conventions of different corpora. From a typological perspective, the formulation needs to be refined from a preference of end weight to a preference for all element</context>
</contexts>
<marker>Haug, Jøhndal, 2008</marker>
<rawString>Dag T. T. Haug and Marius L. Jøhndal. 2008. Creating a Parallel Treebank of the Old Indo-European Bible Translations. In Proceedings of the 2nd Workshop on Language Technology for Cultural Heritage Data, pages 27–34, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>A performance theory of order and constituency.</title>
<date>1994</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="3922" citStr="Hawkins, 1994" startWordPosition="603" endWordPosition="604">ags (Petrov et al., 2012). 247 Proceedings of the 19th Conference on Computational Language Learning, pages 247–257, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics of adjectives, and annotation conventions of different corpora. From a typological perspective, the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head: languages with Verb-Object dominant order tend to put constituents in ‘short before long’ order, while Object-Verb languages, like Japanese or Korean, do the reverse (Hawkins, 1994; Wasow, 2002). A more general explanation for the weight effect has been sought in a general tendency to minimise the length of the dependency between two related words, called Dependency Length Minimisation (DLM, Temperley (2007), Gildea and Temperley (2007)). In this paper, we look at the structural factors, such as DLM, and lexical factors that play a role in adjective-noun word order alternations in Romance languages and the predictions they make on prenominal or postnominal placement of adjectives. We concentrate on a smaller set of languages than those shown in Figure 1 to be able to st</context>
<context position="5558" citStr="Hawkins, 1994" startWordPosition="884" endWordPosition="885">gain, the case when a phrasal verb (verb + particle) has a direct object (NP). Two alternative orders are possible: VP1 = V NP Prt, whose length is DL1 and VP2 = V Prt NP, whose length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _ INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _ |Prt |+ 1. If DL1 is bigger than DL2, then VP2 is preferred over VP1. Unlike the principle of End Weight, this explanation applies also to languages with a different word order than English. The observation that human languages appear to minimise the distance between related words is well documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004), in corpus properties of treebanks (Gildea and Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). It is usually interpreted as a means to reduce memory load and support efficient communication. Dependency length minimisation has been demonstrated on a large scale in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of DLM to the depen</context>
<context position="21267" citStr="Hawkins (1994)" startWordPosition="3559" endWordPosition="3560">tion is unexpected, while the result that the factor is not highly significant less so, as it is not immediately clear why nouns preceding heads should behave differently from nouns that follow heads. A possible explanation for this mismatch of the predictions and the observed data patterns lies in the assumptions underlying the DLM principle. We have assumed a definition of dependency length as the number of words between the head and the dependent, as found in the corpus annotation. Our data are annotated using a content-head rule, which assumes that the noun is the head of the noun phrase. Hawkins (1994), in his welldeveloped variant of DLM, postulates that minimisation occurs on the dependencies between the head and the edge of the dependent phrase. For noun phrases, the relevant dependencies will span between the determiner which unambiguously defines the left edge of the noun phrase and the head of NP (e.g., a verb). The predictions of Hawkins’ theory for adjective placement will therefore differ from the DLM predictions based on our definition. As can be observed from Figure 2, the di and d1 dependencies to the left edge of the NP will be of equal length in cases (a) and (b) (similarly to</context>
</contexts>
<marker>Hawkins, 1994</marker>
<rawString>John A Hawkins. 1994. A performance theory of order and constituency. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>Efficiency and Complexity in Grammars. Oxford linguistics.</title>
<date>2004</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="5574" citStr="Hawkins, 2004" startWordPosition="886" endWordPosition="887">when a phrasal verb (verb + particle) has a direct object (NP). Two alternative orders are possible: VP1 = V NP Prt, whose length is DL1 and VP2 = V Prt NP, whose length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _ INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _ |Prt |+ 1. If DL1 is bigger than DL2, then VP2 is preferred over VP1. Unlike the principle of End Weight, this explanation applies also to languages with a different word order than English. The observation that human languages appear to minimise the distance between related words is well documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004), in corpus properties of treebanks (Gildea and Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). It is usually interpreted as a means to reduce memory load and support efficient communication. Dependency length minimisation has been demonstrated on a large scale in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of DLM to the dependency structure </context>
</contexts>
<marker>Hawkins, 2004</marker>
<rawString>John A. Hawkins. 2004. Efficiency and Complexity in Grammars. Oxford linguistics. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Vit Baisa</author>
<author>Jan Busta</author>
<author>Milos Jakubicek</author>
<author>Vojtech Kov´ar</author>
<author>Jan Michelfeit</author>
<author>Pavel Rychl`y</author>
<author>Vit Suchomel</author>
</authors>
<title>The sketch engine: ten years on.</title>
<date>2014</date>
<journal>Lexicography,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Kilgarriff, Baisa, Busta, Jakubicek, Kov´ar, Michelfeit, Rychl`y, Suchomel, 2014</marker>
<rawString>Adam Kilgarriff, Vit Baisa, Jan Bu&amp;quot;sta, Milo&amp;quot;s Jakubi&amp;quot;cek, Vojt&amp;quot;ech Kov´a&amp;quot;r, Jan Michelfeit, Pavel Rychl`y, and Vit Suchomel. 2014. The sketch engine: ten years on. Lexicography, 1(1):7–36.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
<author>Yvonne QuirmbachBrundage</author>
<author>Yoav Goldberg</author>
<author>Dipanjan Das</author>
<author>Kuzman Ganchev</author>
<author>Keith Hall</author>
<author>Slav Petrov</author>
<author>Hao Zhang</author>
<author>Oscar T¨ackstr¨om</author>
<author>Claudia Bedini</author>
<author>N´uria Bertomeu Castell´o</author>
<author>Jungmee Lee</author>
</authors>
<title>Universal dependency annotation for multilingual parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>92--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>McDonald, Nivre, QuirmbachBrundage, Goldberg, Das, Ganchev, Hall, Petrov, Zhang, T¨ackstr¨om, Bedini, Castell´o, Lee, 2013</marker>
<rawString>Ryan McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria Bertomeu Castell´o, and Jungmee Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92–97. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
</authors>
<title>A corpus-based analysis of verb continuation frequencies for syntactic processing.</title>
<date>1994</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>23</volume>
<issue>6</issue>
<pages>457</pages>
<contexts>
<context position="32351" citStr="Merlo, 1994" startWordPosition="5407" endWordPosition="5408"> shown to be close to optimal, but for German this tendency is not as clear. A recent study of Futrell et al. (2015) applies 254 Figure 5: Percent postnominal placement for thirty most frequent adjectives in Italian, followed by function word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green) and it does not (red)). this analysis on a large-scale, for more than thirty languages that have dependency treebanks. Their results also confirm the correspondence between the dependency annotation and the experimental data, something that has been reported previously (Merlo, 1994; Roland and Jurafsky, 2002). Much work in theoretical linguistics addresses the adjective-noun order in Romance languages. Such work typically concentrates on lexicosemantic aspects of adjective placement (Cinque, 2010; Alexiadou, 2001). In our work, we account for the strong lexical prenominal or postnominal preferences of adjectives by including them as random effects in our models. Closest to our paper is the theoretical work of Abeill´e and Godard (2000) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier (2012) and Thuilier (2012). Fox and Thui</context>
</contexts>
<marker>Merlo, 1994</marker>
<rawString>Paola Merlo. 1994. A corpus-based analysis of verb continuation frequencies for syntactic processing. Journal of Psycholinguistic Research, 23(6):435– 457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan T McDonald</author>
</authors>
<title>A Universal Part-of-Speech Tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>2089--2096</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="3334" citStr="Petrov et al., 2012" startWordPosition="513" endWordPosition="516">ives. While the effect of size or heaviness is welldocumented, this statistics is very coarse and it confounds various linguistic factors, such as types 1We use the following languages and treebanks: English, Czech, Spanish, Chinese, Catalan, German, Italian (Hajiˇc et al., 2009), Danish, Dutch, Portuguese, Swedish (Buchholz and Marsi, 2006), Latin, Ancient Greek (Haug and Jøhndal, 2008), Hungarian (Csendes et al., 2005), Polish (Woli´nski et al., 2011), Arabic (Zeman et al., 2012), French (McDonald et al., 2013). The extraction is based on the conversion to the universal part-of-speech tags (Petrov et al., 2012). 247 Proceedings of the 19th Conference on Computational Language Learning, pages 247–257, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics of adjectives, and annotation conventions of different corpora. From a typological perspective, the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head: languages with Verb-Object dominant order tend to put constituents in ‘short before long’ order, while Object-Verb languages, like Japanese or Korean, do the reverse (Hawkins, 1994; Wasow, 200</context>
<context position="14506" citStr="Petrov et al., 2012" startWordPosition="2421" endWordPosition="2424">e) as grouping variables introducing random effects. For example, the instances of adjective-noun order for a particular adjective will share the same weight value γ for the adjective variable, but across different adjectives this value can vary. al., 2013), and Portuguese (Buchholz and Marsi, 2006). Noun phrases containing adjectives are extracted using part-of-speech information and dependency arcs from the gold annotation. Specifically, all treebanks are converted to coarse universal part-of-speech tags, using existing conventional mappings from the original tagset to the universal tagset (Petrov et al., 2012). All adjectives are identified using the universal PoS tag ‘ADJ’, whose dependency head is a noun, tagged using the universal PoS tag ‘NOUN’. All elements of the dependency subtree, the noun phrase, rooted in this noun are collected. For all languages where this information is available, we extract lemmas of adjective and noun tokens. The only treebank without lemma annotation is French, for which we extract token forms.3 A total of around 64’000 instances of adjectives in noun phrases is collected, ranging from 2’800 for Italian to 20’000 for Spanish. 3.2 Method: Mixed-Effects models The int</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan T. McDonald. 2012. A Universal Part-of-Speech Tagset. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 2089–2096, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Roland</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Verb sense and verb subcategorization probabilities.</title>
<date>2002</date>
<editor>In Paola Merlo and Suzanne Stevenson, editors,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="32379" citStr="Roland and Jurafsky, 2002" startWordPosition="5409" endWordPosition="5412">close to optimal, but for German this tendency is not as clear. A recent study of Futrell et al. (2015) applies 254 Figure 5: Percent postnominal placement for thirty most frequent adjectives in Italian, followed by function word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green) and it does not (red)). this analysis on a large-scale, for more than thirty languages that have dependency treebanks. Their results also confirm the correspondence between the dependency annotation and the experimental data, something that has been reported previously (Merlo, 1994; Roland and Jurafsky, 2002). Much work in theoretical linguistics addresses the adjective-noun order in Romance languages. Such work typically concentrates on lexicosemantic aspects of adjective placement (Cinque, 2010; Alexiadou, 2001). In our work, we account for the strong lexical prenominal or postnominal preferences of adjectives by including them as random effects in our models. Closest to our paper is the theoretical work of Abeill´e and Godard (2000) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier (2012) and Thuilier (2012). Fox and Thuilier (2012) use a dependency</context>
</contexts>
<marker>Roland, Jurafsky, 2002</marker>
<rawString>Douglas Roland and Daniel Jurafsky. 2002. Verb sense and verb subcategorization probabilities. In Paola Merlo and Suzanne Stevenson, editors, The lexical basis of sentence processing: Formal, computational, and experimental issues. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne M Stallings</author>
<author>Maryellen C MacDonald</author>
<author>Padraig G O’Seaghdha</author>
</authors>
<title>Phrasal ordering constraints in sentence production: Phrase length and verb disposition in heavy-NP shift.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<volume>39</volume>
<issue>3</issue>
<marker>Stallings, MacDonald, O’Seaghdha, 1998</marker>
<rawString>Lynne M Stallings, Maryellen C MacDonald, and Padraig G O’Seaghdha. 1998. Phrasal ordering constraints in sentence production: Phrase length and verb disposition in heavy-NP shift. Journal of Memory and Language, 39(3):392–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Temperley</author>
</authors>
<title>Minimization of dependency length in written English.</title>
<date>2007</date>
<journal>Cognition,</journal>
<volume>105</volume>
<issue>2</issue>
<pages>333</pages>
<contexts>
<context position="4153" citStr="Temperley (2007)" startWordPosition="638" endWordPosition="639">n conventions of different corpora. From a typological perspective, the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head: languages with Verb-Object dominant order tend to put constituents in ‘short before long’ order, while Object-Verb languages, like Japanese or Korean, do the reverse (Hawkins, 1994; Wasow, 2002). A more general explanation for the weight effect has been sought in a general tendency to minimise the length of the dependency between two related words, called Dependency Length Minimisation (DLM, Temperley (2007), Gildea and Temperley (2007)). In this paper, we look at the structural factors, such as DLM, and lexical factors that play a role in adjective-noun word order alternations in Romance languages and the predictions they make on prenominal or postnominal placement of adjectives. We concentrate on a smaller set of languages than those shown in Figure 1 to be able to study finer-grained effects than what can be observed at a very large scale and across many different corpus annotation schemes. We choose Romance languages because they show a good amount of variation in the word order of the noun p</context>
<context position="5637" citStr="Temperley, 2007" startWordPosition="896" endWordPosition="898">. Two alternative orders are possible: VP1 = V NP Prt, whose length is DL1 and VP2 = V Prt NP, whose length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _ INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _ |Prt |+ 1. If DL1 is bigger than DL2, then VP2 is preferred over VP1. Unlike the principle of End Weight, this explanation applies also to languages with a different word order than English. The observation that human languages appear to minimise the distance between related words is well documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004), in corpus properties of treebanks (Gildea and Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). It is usually interpreted as a means to reduce memory load and support efficient communication. Dependency length minimisation has been demonstrated on a large scale in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of DLM to the dependency structure of noun phrases, our goal is to test to what extent the DLM pri</context>
<context position="31468" citStr="Temperley (2007)" startWordPosition="5271" endWordPosition="5272"> is inside the NP that selects it, while che and per usually do not, they are adjuncts, or infinitivals or clauses. In the linguistic literature, this is a distinction between arguments and adjuncts of the noun and it is represented structurally. This distinction is, then, a lexically-induced structural distinction, and not simply a collocation. 5 Related work Our work occupies the middle ground between detailed linguistic investigations of weight effect in chosen constructions of well-studied languages and large scale demonstrations of the dependency length minimisation principle. Gildea and Temperley (2007) demonstrated that DLM applies for the dependency annotated corpora in English and German. They calculate random and optimal dependency lengths for each sentence given its unordered dependency tree and compare these values to actual dependency lengths. English lengths are shown to be close to optimal, but for German this tendency is not as clear. A recent study of Futrell et al. (2015) applies 254 Figure 5: Percent postnominal placement for thirty most frequent adjectives in Italian, followed by function word di, che, per, in this order. (Noun phrase has a right ‘di/per/che’-complement (green)</context>
</contexts>
<marker>Temperley, 2007</marker>
<rawString>David Temperley. 2007. Minimization of dependency length in written English. Cognition, 105(2):300– 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juliette Thuilier</author>
</authors>
<title>Contraintes pr´ef´erentielles et ordre des mots en franc¸ais.</title>
<date>2012</date>
<tech>Ph.D. Thesis,</tech>
<institution>Universit´e Paris-Diderot - Paris VII,</institution>
<contexts>
<context position="2337" citStr="Thuilier (2012)" startWordPosition="368" endWordPosition="369">and other alternation preferences among verbal dependents are traditionally evoked to argue in favour of the “heaviness” effect. In this work, we study the alternations in the noun-phrase domain, much less investigated in Figure 1: Percent of postnominal simple (green) and heavy (red) adjectives across seventeen languages. connection with the heaviness effect. Abeill´e and Godard (2000) introduce the heaviness of adjective phrases as a principle explaining their postnominal placement compared to ‘light’ adjectives in French. Their observations have been recently confirmed in a corpus study by Thuilier (2012). Cross-linguistically, the data that we have collected across many languages and several families, presented in Figure 1, confirm the heaviness effect for adjectives1. By extracting relevant statistics from gold dependency annotated corpora, we can observe that heavy adjectives (adjective phrases of at least two words) appear more frequently postnominally than simple adjectives. While the effect of size or heaviness is welldocumented, this statistics is very coarse and it confounds various linguistic factors, such as types 1We use the following languages and treebanks: English, Czech, Spanish</context>
<context position="32917" citStr="Thuilier (2012)" startWordPosition="5494" endWordPosition="5495">ng that has been reported previously (Merlo, 1994; Roland and Jurafsky, 2002). Much work in theoretical linguistics addresses the adjective-noun order in Romance languages. Such work typically concentrates on lexicosemantic aspects of adjective placement (Cinque, 2010; Alexiadou, 2001). In our work, we account for the strong lexical prenominal or postnominal preferences of adjectives by including them as random effects in our models. Closest to our paper is the theoretical work of Abeill´e and Godard (2000) on the placement of adjective phrases in French and recent corpusbased work by Fox and Thuilier (2012) and Thuilier (2012). Fox and Thuilier (2012) use a dependency annotated corpus of French to extract cases of adjective-noun variation and their syntactic contexts. They model the placement of an adjective as a lexical, syntactic and semantic multifactorial variation. They find, for example, that phonologically heavy simple adjectives tend to be postnominal. This result highlights the distinction between phonological weight and syntactic weight, a topic which we do not address in the current work. 6 Conclusion In this paper, we have shown that differences in the prenominal and postnominal plac</context>
</contexts>
<marker>Thuilier, 2012</marker>
<rawString>Juliette Thuilier. 2012. Contraintes pr´ef´erentielles et ordre des mots en franc¸ais. Ph.D. Thesis, Universit´e Paris-Diderot - Paris VII, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Joel Tily</author>
</authors>
<title>The role of processing complexity in word order variation and change.</title>
<date>2010</date>
<tech>Ph.D. Thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="5704" citStr="Tily, 2010" startWordPosition="908" endWordPosition="909">DL1 and VP2 = V Prt NP, whose length is DL2. DL1 is DL(V-NP)+DL(V-Prt) _ INPI + 1; DL2 is DL(V-NP) + DL(V-Prt) _ |Prt |+ 1. If DL1 is bigger than DL2, then VP2 is preferred over VP1. Unlike the principle of End Weight, this explanation applies also to languages with a different word order than English. The observation that human languages appear to minimise the distance between related words is well documented in sentence processing (Gibson, 1998; Hawkins, 1994; Hawkins, 2004), in corpus properties of treebanks (Gildea and Temperley, 2007; Futrell et al., 2015), in diachronic language change (Tily, 2010). It is usually interpreted as a means to reduce memory load and support efficient communication. Dependency length minimisation has been demonstrated on a large scale in the verbal domain and at the sentence level, but has not yet been investigated in the more limited nominal domain, where dependencies are usually shorter and might create lighter processing loads that do not need to be minimised. In applying the general principle of DLM to the dependency structure of noun phrases, our goal is to test to what extent the DLM principle predicts the observed adjective-noun word order alternation </context>
</contexts>
<marker>Tily, 2010</marker>
<rawString>Harry Joel Tily. 2010. The role of processing complexity in word order variation and change. Ph.D. Thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Wasow</author>
</authors>
<title>Postverbal Behavior.</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="1628" citStr="Wasow, 2002" startWordPosition="262" endWordPosition="263">alternations where one structure can be linearised in two different ways. Consider, for example, the case when a phrasal verb (V + particle) has a direct object (NP), in English. Two alternative orders are possible: VP1 = V NP Prt, and VP2 = V Prt NP. If the NP is heavy, as defined in number of words or number of syllables, it will be frequently placed after the Prt, yielding the V-PrtNP order. Compare, for instance Call me up! to Call up the customer who called yesterday. This tendency is also formulated as a Principle of End Weight, where phrases are presented in order of increasing weight (Wasow, 2002). Cases of heavy NP-shift (Stallings et al., 1998), dative alternation (Bresnan et al., 2007) and other alternation preferences among verbal dependents are traditionally evoked to argue in favour of the “heaviness” effect. In this work, we study the alternations in the noun-phrase domain, much less investigated in Figure 1: Percent of postnominal simple (green) and heavy (red) adjectives across seventeen languages. connection with the heaviness effect. Abeill´e and Godard (2000) introduce the heaviness of adjective phrases as a principle explaining their postnominal placement compared to ‘ligh</context>
<context position="3936" citStr="Wasow, 2002" startWordPosition="605" endWordPosition="606">al., 2012). 247 Proceedings of the 19th Conference on Computational Language Learning, pages 247–257, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics of adjectives, and annotation conventions of different corpora. From a typological perspective, the formulation needs to be refined from a preference of end weight to a preference for all elements being closer to the governing head: languages with Verb-Object dominant order tend to put constituents in ‘short before long’ order, while Object-Verb languages, like Japanese or Korean, do the reverse (Hawkins, 1994; Wasow, 2002). A more general explanation for the weight effect has been sought in a general tendency to minimise the length of the dependency between two related words, called Dependency Length Minimisation (DLM, Temperley (2007), Gildea and Temperley (2007)). In this paper, we look at the structural factors, such as DLM, and lexical factors that play a role in adjective-noun word order alternations in Romance languages and the predictions they make on prenominal or postnominal placement of adjectives. We concentrate on a smaller set of languages than those shown in Figure 1 to be able to study finer-grai</context>
</contexts>
<marker>Wasow, 2002</marker>
<rawString>Thomas Wasow. 2002. Postverbal Behavior. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Woli´nski</author>
<author>Katarzyna Głowi´nska</author>
<author>Marek ´Swidzi´nski</author>
</authors>
<title>A Preliminary Version of Skladnica—a Treebank of Polish.</title>
<date>2011</date>
<booktitle>In Zygmunt Vetulani, editor, Proceedings of the 5th Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics,</booktitle>
<pages>299--303</pages>
<location>Poznan, Poland.</location>
<marker>Woli´nski, Głowi´nska, ´Swidzi´nski, 2011</marker>
<rawString>Marcin Woli´nski, Katarzyna Głowi´nska, and Marek ´Swidzi´nski. 2011. A Preliminary Version of Skladnica—a Treebank of Polish. In Zygmunt Vetulani, editor, Proceedings of the 5th Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, pages 299–303, Poznan, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>David Mareˇcek</author>
<author>Martin Popel</author>
<author>Loganathan Ramasamy</author>
<author>Jan ˇStˇep´anek</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Jan Hajiˇc</author>
</authors>
<title>HamleDT: To Parse or Not to Parse?</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>23--25</pages>
<location>Istanbul, Turkey,</location>
<marker>Zeman, Mareˇcek, Popel, Ramasamy, ˇStˇep´anek, ˇZabokrtsk´y, Hajiˇc, 2012</marker>
<rawString>Daniel Zeman, David Mareˇcek, Martin Popel, Loganathan Ramasamy, Jan ˇStˇep´anek, Zdenˇek ˇZabokrtsk´y, and Jan Hajiˇc. 2012. HamleDT: To Parse or Not to Parse? In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 23–25, Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>