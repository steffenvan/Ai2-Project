<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013079">
<note confidence="0.4273205">
Case Revisited: In the Shadow of Automatic Processing of Machine-Readable
Dictionaries
</note>
<author confidence="0.771633666666667">
Fuliang Weng
Computing Research Lab, New Mexico State University
Las Cruces, NM 88003
</author>
<bodyText confidence="0.997759166666667">
This paper discusses the work of automat-
ically extracting Case Frames from Machine-
Readable Dictionaries based on a three layer
a posteriori Case Theory[5].
The theory is intended to deal with two
problems:
</bodyText>
<listItem confidence="0.953443684210526">
1. To dynamically adjust grains of Cases.
This is where a posteriori comes from.
2. To provide a procedure to determine
Cases. This is where three layer comes from.
The three layers are:
1. base layer: This layer is intended to ac-
complish transformations of words to concepts
by explicating language and word specific im-
plicants. e.g., for the verb eat in the intran-
sitive case, its subject is eater, while for verb
break in the intransitive case, its subject is the
broken.
2. default layer: in this layer, implicit as-
sumptions of naive theories are made explicit,
e.g., for concept see, there are two different
views towards its subject: if a person who uses
this concept believes that seeing is just a pro-
cess of passive perception, then this person will
assign to its subject, a passive&apos; Case such as
</listItem>
<bodyText confidence="0.882995133333333">
*I would like to express thanks to Dr. L. Guthrie,
Dr. D. Farwell and Prof. Y. Wilks for comments and
encouragement. This project is supported in part by
CRL. Some of the ideas were developed during my stay
in CS/Fudan and CMT/CMU.
1The words passive/active are used to indicate dif-
ferent levels of activeness. In what follows, Cases
such as agent and instrument have somewhat different
meanings than the conventional ones. We use them
just for referring to a group of phenomena which are
related to their names.
experiencer; if a person who uses this concept
believes that seeing is a process of active selec-
tion, then this person will assign to its subject,
an active Case such as agent.
</bodyText>
<listItem confidence="0.551551666666667">
3. context layer: in this layer, Cases
are further clarified upon any requests from
current tasks, associated context and personal
belief systems (knowledge), e.g., in sentence
The commander forced the soldier to break the
door., whether the soldier should be assigned
agent, instrument, active, or something else,
should be decided by both contextual infor-
mation and needs.
</listItem>
<bodyText confidence="0.998388">
Arguments for the three layer theory can be
found in[5].
Relevant knowledge sources for arriving at
different layers are:
</bodyText>
<listItem confidence="0.997083882352941">
1. Formation of the base layer: the for-
mation is based on knowledge sources which
mainly come from syntactic codes and def-
initions in LDOCE (Longman Dictionary of
Contemporary English). Examples in LDOCE
also contribute to this process [1].
2. Formation of the default layer: the for-
mation is based on the assumption that naive
theories are weakly consistent, which implies
that certain semantic classifications may be
consistent with certain naive theories: verb,
noun, preposition and adjective classifications
based on semantic and pragmatic codes in
LDOCE, and examples in LDOCE can help
to obtain such theories.
3. Formation of the context layer: the
unification of the base layer and the de-
</listItem>
<page confidence="0.994133">
337
</page>
<bodyText confidence="0.967963153846154">
fault layer forms an initial representation of
the context layer, its further development
mainly depends on task, contextual needs and
personal belief systems. The initial repre-
sentation is a tuple with three components:
entity-role, environment and endurance. An
example of an initial representation for break
is: ((±) (u -) (0)) break ((-) (u -) (0)), where
(+) stands for active, (-) for passive, (u -) for
indexing of the internal environment, (0) for
duration. If the task is MT, the requirement
for understanding could be shallow as pointed
out by Wilks [7], although he did not discuss
any dynamic grain adjustment. Contextual in-
formation can be conveyed by active features
[5].
Following the boot-strapping principle, we
are starting with 750 genus verbs in the defin-
ing word list of LDOCE, then gradually ex-
panding them to all the verbs defined in
LDOCE.
There are various subtasks associated with
this work:
1. Dynamically adjusting classifications of
relational concepts (mainly reflected by verbs):
we are trying to get a set of core verbs as proto-
types of classes based on certain statistics and
genus verb sense nets (the latter is being con-
structed by G. Stein). A primary set of core
verbs have been chosen, functional verbs are
carefully prevented. The criterion for dynam-
ically adjusting verb classes is: Ci(d)= {y:
Y--x ii&lt; d,xE Ci}, where Ci are core classes
and II • II is defined as: II y—x 11= mini{ i is the
numbers of links on P, P is any path connect-
ing x and y }. We can select a reasonable dis-
tance for Ci(d) by detecting slopes with points
in the distribution of members. Classification
can also be done within connectionist models.
</bodyText>
<listItem confidence="0.9675292">
2. From the prototypes, naive theories may
be formed, and then converted into represen-
tations in the default layer.
3. Dynamic creation of Cases. Initial rep-
resentations in the context layer may be ad-
</listItem>
<bodyText confidence="0.9917042">
justed and new Cases be created according to
a set of contextual conditions (mainly when
mismatches happen).
4. A set of rules can be constructed to get
the conventional Cases for typical situations.
Many Case Theories are focused on verbs.
In our situation, all the four major cate-
gories (verb, noun, adjective and preposition)
must be paid enough attention to, since there
are many verbs defined by verb phrases in
LDOCE. e.g., a definition entry of verb take
in LDOCE contains get possession of. In or-
der to select a right Case frame and verb class
for each verb, we need something beyond what
we have presented although it does not con-
flict with what we have proposed and it is very
plausible that the procedure used here may be
adapted to establish Case frames for nouns,
adjectives and prepositions. This task may be
benefited from [2].
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.999477210526316">
[1] B. Atkins et al, Explicit and Implicit Information
in Dictionaries, CSL Report 5, Princeton Univer-
sity, 1986.
[2] R.. Bruce and L. Guthrie, Genus Disambiguation:
A Study in Weighted Preference, MCCS-91-207,
CRL/NMSU, 1991.
[3] C. Fillmore, The Case for Case,in Universals in
Linguistic Theory, E. Bach and R. Harm (eds.),
Holt, Rinehart, and Winston, 1968.
[4] R.. Schank, Conceptual Information Processing,
North-Holland Publishing Co., 1975.
[5] F. Weng, A Three-Layer a posteriori Case The-
ory, in preparation, 1991.
[6) W. Wilkins, Syntax and Semantics, Academic
Press, Inc., California, 1988.
[7] Y. Wilks, An Artificial Intelligence Approach
to Machine Translation, in Computer Models of
Thought and Language, R.Schank and K.Colby
(eds.), W.H.Freeman Co., 1973.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.058958">
<title confidence="0.9961805">Case Revisited: In the Shadow of Automatic Processing of Machine-Readable Dictionaries</title>
<author confidence="0.98267">Fuliang Weng</author>
<affiliation confidence="0.999975">Computing Research Lab, New Mexico State University</affiliation>
<address confidence="0.995961">Las Cruces, NM 88003</address>
<abstract confidence="0.994161119402985">This paper discusses the work of automatically extracting Case Frames from Machine- Readable Dictionaries based on a three layer Theory[5]. The theory is intended to deal with two problems: 1. To dynamically adjust grains of Cases. is where a from. 2. To provide a procedure to determine This is where layer from. The three layers are: 1. base layer: This layer is intended to accomplish transformations of words to concepts by explicating language and word specific ime.g., for the verb the intrancase, its subject is for verb the intransitive case, its subject is broken. 2. default layer: in this layer, implicit assumptions of naive theories are made explicit, for concept are two different views towards its subject: if a person who uses concept believes that is a process of passive perception, then this person will to its subject, a such as *I would like to express thanks to Dr. L. Guthrie, Dr. D. Farwell and Prof. Y. Wilks for comments and encouragement. This project is supported in part by CRL. Some of the ideas were developed during my stay in CS/Fudan and CMT/CMU. words used to indicate different levels of activeness. In what follows, Cases such as agent and instrument have somewhat different meanings than the conventional ones. We use them just for referring to a group of phenomena which are related to their names. if person who uses this concept that is process of active selection, then this person will assign to its subject, such as 3. context layer: in this layer, Cases are further clarified upon any requests from current tasks, associated context and personal belief systems (knowledge), e.g., in sentence The commander forced the soldier to break the soldier be assigned instrument, active, something else, should be decided by both contextual information and needs. Arguments for the three layer theory can be found in[5]. Relevant knowledge sources for arriving at different layers are: 1. Formation of the base layer: the formation is based on knowledge sources which mainly come from syntactic codes and definitions in LDOCE (Longman Dictionary of Contemporary English). Examples in LDOCE also contribute to this process [1]. 2. Formation of the default layer: the formation is based on the assumption that naive theories are weakly consistent, which implies that certain semantic classifications may be consistent with certain naive theories: verb, noun, preposition and adjective classifications based on semantic and pragmatic codes in LDOCE, and examples in LDOCE can help to obtain such theories. 3. Formation of the context layer: the of the base layer and the de- 337 fault layer forms an initial representation of the context layer, its further development mainly depends on task, contextual needs and personal belief systems. The initial representation is a tuple with three components: entity-role, environment and endurance. An of an initial representation for ((±) -) (0)) break ((-) (u -) (0)), where (+) stands for active, (-) for passive, (u -) for indexing of the internal environment, (0) for duration. If the task is MT, the requirement for understanding could be shallow as pointed out by Wilks [7], although he did not discuss any dynamic grain adjustment. Contextual information can be conveyed by active features Following the boot-strapping principle, we are starting with 750 genus verbs in the defining word list of LDOCE, then gradually expanding them to all the verbs defined in LDOCE. There are various subtasks associated with this work: 1. Dynamically adjusting classifications of relational concepts (mainly reflected by verbs): we are trying to get a set of core verbs as prototypes of classes based on certain statistics and genus verb sense nets (the latter is being constructed by G. Stein). A primary set of core verbs have been chosen, functional verbs are carefully prevented. The criterion for dynamadjusting verb classes is: ii&lt; d,xE Ci are core classes and II • II is defined as: II y—x 11= mini{ i is the numbers of links on P, P is any path connecting x and y }. We can select a reasonable disfor detecting slopes with points in the distribution of members. Classification can also be done within connectionist models. 2. From the prototypes, naive theories may be formed, and then converted into representations in the default layer. 3. Dynamic creation of Cases. Initial representations in the context layer may be adjusted and new Cases be created according to contextual conditions (mainly when mismatches happen). 4. A set of rules can be constructed to get the conventional Cases for typical situations. Many Case Theories are focused on verbs. In our situation, all the four major categories (verb, noun, adjective and preposition) must be paid enough attention to, since there are many verbs defined by verb phrases in e.g., a definition entry of verb LDOCE contains possession of. order to select a right Case frame and verb class for each verb, we need something beyond what we have presented although it does not conflict with what we have proposed and it is very plausible that the procedure used here may be adapted to establish Case frames for nouns, adjectives and prepositions. This task may be benefited from [2].</abstract>
<note confidence="0.8538771">References B. Atkins et al, Explicit Implicit Information Dictionaries, Report 5, Princeton University, 1986. R.. Bruce and L. Guthrie, Disambiguation: Study in Weighted Preference, CRL/NMSU, 1991. C. Fillmore, Case for Case,in Universals in Theory, Bach and R. Harm (eds.), Holt, Rinehart, and Winston, 1968. R.. Schank, Information Processing, North-Holland Publishing Co., 1975. F. Weng, Three-Layer a posteriori Case Thepreparation, 1991. W. Wilkins, and Semantics, Press, Inc., California, 1988. Wilks, An Intelligence Approach to Machine Translation, in Computer Models of and Language, and K.Colby (eds.), W.H.Freeman Co., 1973.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Atkins</author>
</authors>
<title>Explicit and Implicit Information in Dictionaries,</title>
<date>1986</date>
<journal>CSL Report</journal>
<volume>5</volume>
<institution>Princeton University,</institution>
<contexts>
<context position="2610" citStr="[1]" startWordPosition="432" endWordPosition="432">lief systems (knowledge), e.g., in sentence The commander forced the soldier to break the door., whether the soldier should be assigned agent, instrument, active, or something else, should be decided by both contextual information and needs. Arguments for the three layer theory can be found in[5]. Relevant knowledge sources for arriving at different layers are: 1. Formation of the base layer: the formation is based on knowledge sources which mainly come from syntactic codes and definitions in LDOCE (Longman Dictionary of Contemporary English). Examples in LDOCE also contribute to this process [1]. 2. Formation of the default layer: the formation is based on the assumption that naive theories are weakly consistent, which implies that certain semantic classifications may be consistent with certain naive theories: verb, noun, preposition and adjective classifications based on semantic and pragmatic codes in LDOCE, and examples in LDOCE can help to obtain such theories. 3. Formation of the context layer: the unification of the base layer and the de337 fault layer forms an initial representation of the context layer, its further development mainly depends on task, contextual needs and pers</context>
</contexts>
<marker>[1]</marker>
<rawString>B. Atkins et al, Explicit and Implicit Information in Dictionaries, CSL Report 5, Princeton University, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>L Guthrie</author>
</authors>
<title>Genus Disambiguation: A Study</title>
<date>1991</date>
<booktitle>in Weighted Preference,</booktitle>
<pages>91--207</pages>
<location>CRL/NMSU,</location>
<marker>[2]</marker>
<rawString>R.. Bruce and L. Guthrie, Genus Disambiguation: A Study in Weighted Preference, MCCS-91-207, CRL/NMSU, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fillmore</author>
</authors>
<title>The Case for Case,in Universals in Linguistic Theory,</title>
<date>1968</date>
<editor>E. Bach and R. Harm (eds.),</editor>
<location>Holt, Rinehart, and Winston,</location>
<marker>[3]</marker>
<rawString>C. Fillmore, The Case for Case,in Universals in Linguistic Theory, E. Bach and R. Harm (eds.), Holt, Rinehart, and Winston, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
</authors>
<title>Conceptual Information Processing,</title>
<date>1975</date>
<publisher>North-Holland Publishing Co.,</publisher>
<marker>[4]</marker>
<rawString>R.. Schank, Conceptual Information Processing, North-Holland Publishing Co., 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Weng</author>
</authors>
<title>A Three-Layer a posteriori Case Theory, in preparation,</title>
<date>1991</date>
<booktitle>[6) W. Wilkins, Syntax and Semantics,</booktitle>
<publisher>Academic Press, Inc.,</publisher>
<location>California,</location>
<contexts>
<context position="2304" citStr="[5]" startWordPosition="384" endWordPosition="384">riencer; if a person who uses this concept believes that seeing is a process of active selection, then this person will assign to its subject, an active Case such as agent. 3. context layer: in this layer, Cases are further clarified upon any requests from current tasks, associated context and personal belief systems (knowledge), e.g., in sentence The commander forced the soldier to break the door., whether the soldier should be assigned agent, instrument, active, or something else, should be decided by both contextual information and needs. Arguments for the three layer theory can be found in[5]. Relevant knowledge sources for arriving at different layers are: 1. Formation of the base layer: the formation is based on knowledge sources which mainly come from syntactic codes and definitions in LDOCE (Longman Dictionary of Contemporary English). Examples in LDOCE also contribute to this process [1]. 2. Formation of the default layer: the formation is based on the assumption that naive theories are weakly consistent, which implies that certain semantic classifications may be consistent with certain naive theories: verb, noun, preposition and adjective classifications based on semantic an</context>
<context position="3755" citStr="[5]" startWordPosition="616" endWordPosition="616">elopment mainly depends on task, contextual needs and personal belief systems. The initial representation is a tuple with three components: entity-role, environment and endurance. An example of an initial representation for break is: ((±) (u -) (0)) break ((-) (u -) (0)), where (+) stands for active, (-) for passive, (u -) for indexing of the internal environment, (0) for duration. If the task is MT, the requirement for understanding could be shallow as pointed out by Wilks [7], although he did not discuss any dynamic grain adjustment. Contextual information can be conveyed by active features [5]. Following the boot-strapping principle, we are starting with 750 genus verbs in the defining word list of LDOCE, then gradually expanding them to all the verbs defined in LDOCE. There are various subtasks associated with this work: 1. Dynamically adjusting classifications of relational concepts (mainly reflected by verbs): we are trying to get a set of core verbs as prototypes of classes based on certain statistics and genus verb sense nets (the latter is being constructed by G. Stein). A primary set of core verbs have been chosen, functional verbs are carefully prevented. The criterion for </context>
</contexts>
<marker>[5]</marker>
<rawString>F. Weng, A Three-Layer a posteriori Case Theory, in preparation, 1991. [6) W. Wilkins, Syntax and Semantics, Academic Press, Inc., California, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>An Artificial Intelligence Approach to Machine Translation,</title>
<date>1973</date>
<booktitle>in Computer Models of Thought and Language, R.Schank and K.Colby (eds.), W.H.Freeman Co.,</booktitle>
<contexts>
<context position="3634" citStr="[7]" startWordPosition="597" endWordPosition="597">ication of the base layer and the de337 fault layer forms an initial representation of the context layer, its further development mainly depends on task, contextual needs and personal belief systems. The initial representation is a tuple with three components: entity-role, environment and endurance. An example of an initial representation for break is: ((±) (u -) (0)) break ((-) (u -) (0)), where (+) stands for active, (-) for passive, (u -) for indexing of the internal environment, (0) for duration. If the task is MT, the requirement for understanding could be shallow as pointed out by Wilks [7], although he did not discuss any dynamic grain adjustment. Contextual information can be conveyed by active features [5]. Following the boot-strapping principle, we are starting with 750 genus verbs in the defining word list of LDOCE, then gradually expanding them to all the verbs defined in LDOCE. There are various subtasks associated with this work: 1. Dynamically adjusting classifications of relational concepts (mainly reflected by verbs): we are trying to get a set of core verbs as prototypes of classes based on certain statistics and genus verb sense nets (the latter is being constructed</context>
</contexts>
<marker>[7]</marker>
<rawString>Y. Wilks, An Artificial Intelligence Approach to Machine Translation, in Computer Models of Thought and Language, R.Schank and K.Colby (eds.), W.H.Freeman Co., 1973.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>