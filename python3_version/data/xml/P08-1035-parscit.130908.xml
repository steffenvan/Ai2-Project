<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.996086">
A Generic Sentence Trimmer with CRFs
</title>
<author confidence="0.995757">
Tadashi Nomoto
</author>
<affiliation confidence="0.998893">
National Institute of Japanese Literature
</affiliation>
<address confidence="0.5640595">
10-3, Midori Tachikawa
Tokyo, 190-0014, Japan
</address>
<email confidence="0.99381">
nomoto@acm.org
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877357142857">
The paper presents a novel sentence trimmer
in Japanese, which combines a non-statistical
yet generic tree generation model and Con-
ditional Random Fields (CRFs), to address
improving the grammaticality of compres-
sion while retaining its relevance. Experi-
ments found that the present approach out-
performs in grammaticality and in relevance
a dependency-centric approach (Oguro et al.,
2000; Morooka et al., 2004; Yamagata et al.,
2006; Fukutomi et al., 2007) − the only line of
work in prior literature (on Japanese compres-
sion) we are aware of that allows replication
and permits a direct comparison.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999437960784314">
For better or worse, much of prior work on sentence
compression (Riezler et al., 2003; McDonald, 2006;
Turner and Charniak, 2005) turned to a single cor-
pus developed by Knight and Marcu (2002) (K&amp;M,
henceforth) for evaluating their approaches.
The K&amp;M corpus is a moderately sized corpus
consisting of 1,087 pairs of sentence and compres-
sion, which account for about 2% of a Ziff-Davis
collection from which it was derived. Despite its
limited scale, prior work in sentence compression
relied heavily on this particular corpus for establish-
ing results (Turner and Charniak, 2005; McDonald,
2006; Clarke and Lapata, 2006; Galley and McKe-
own, 2007). It was not until recently that researchers
started to turn attention to an alternative approach
which does not require supervised data (Turner and
Charniak, 2005).
Our approach is broadly in line with prior work
(Jing, 2000; Dorr et al., 2003; Riezler et al., 2003;
Clarke and Lapata, 2006), in that we make use of
some form of syntactic knowledge to constrain com-
pressions we generate. What sets this work apart
from them, however, is a novel use we make of
Conditional Random Fields (CRFs) to select among
possible compressions (Lafferty et al., 2001; Sut-
ton and McCallum, 2006). An obvious benefit of
using CRFs for sentence compression is that the
model provides a general (and principled) proba-
bilistic framework which permits information from
various sources to be integrated towards compress-
ing sentence, a property K&amp;M do not share.
Nonetheless, there is some cost that comes with
the straightforward use of CRFs as a discriminative
classifier in sentence compression; its outputs are
often ungrammatical and it allows no control over
the length of compression they generates (Nomoto,
2007). We tackle the issues by harnessing CRFs
with what we might call dependency truncation,
whose goal is to restrict CRFs to working with can-
didates that conform to the grammar.
Thus, unlike McDonald (2006), Clarke and Lap-
ata (2006) and Cohn and Lapata (2007), we do not
insist on finding a globally optimal solution in the
space of 2&apos; possible compressions for an n word
long sentence. Rather we insist on finding a most
plausible compression among those that are explic-
itly warranted by the grammar.
Later in the paper, we will introduce an approach
called the ‘Dependency Path Model’ (DPM) from
the previous literature (Section 4), which purports to
provide a robust framework for sentence compres-
</bodyText>
<page confidence="0.981246">
299
</page>
<note confidence="0.710029">
Proceedings of ACL-08: HLT, pages 299–307,
</note>
<page confidence="0.464716">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.7509465">
sion in Japanese. We will look at how the present
approach compares with that of DPM in Section 6.
</bodyText>
<page confidence="0.491673">
2 A Sentence Trimmer with CRFs
</page>
<figureCaption confidence="0.999744">
Figure 1: Syntactic structure in Japanese
</figureCaption>
<bodyText confidence="0.978146666666667">
Our idea on how to make CRFs comply with gram-
mar is quite simple: we focus on only those la-
bel sequences that are associated with grammati-
cally correct compressions, by making CRFs look
at only those that comply with some grammatical
constraints G, and ignore others, regardless of how
probable they are.1 But how do we find compres-
sions that are grammatical? To address the issue,
rather than resort to statistical generation models as
in the previous literature (Cohn and Lapata, 2007;
Galley and McKeown, 2007), we pursue a particular
rule-based approach we call a ‘dependency trunca-
tion,’ which as we will see, gives us a greater control
over the form that compression takes.
Let us denote a set of label assignments for 5 that
satisfy constraints, by G(5).2 We seek to solve the
following,
Consider the following.
</bodyText>
<listItem confidence="0.612235">
(3) Mushoku-no John -ga takai kuruma
</listItem>
<bodyText confidence="0.5984552">
unemployed John SBJ expensive car
-wo kat-ta.
ACC buy PAST
‘John, who is unemployed, bought an
expensive car.’
</bodyText>
<equation confidence="0.942013">
y⋆ = arg max p(y|x;o). (2)
Y∈G(S)
</equation>
<bodyText confidence="0.999919090909091">
There would be a number of ways to go about the
problem. In the context of sentence compression, a
linear programming based approach such as Clarke
and Lapata (2006) is certainly one that deserves con-
sideration. In this paper, however, we will explore a
much simpler approach which does not require as
involved formulation as Clarke and Lapata (2006)
do.
We approach the problem extentionally, i.e.,
through generating sentences that are grammatical,
or that conform to whatever constraints there are.
</bodyText>
<equation confidence="0.9668106">
1Assume as usual that CRFs take the form,
p(y|x) a
(/moexp k,j Ajfj(yk, yk−1, x) + /moi pigi(xk, yk, x)
= exp[w⊤f(x, y)]
(1)
</equation>
<bodyText confidence="0.6332239">
fj and gi are ‘features’ associated with edges and vertices, re-
spectively, and k E C, where C denotes a set of cliques in CRFs.
Aj and pi are the weights for corresponding features. w and f
are vector representations of weights and features, respectively
(Tasker, 2004).
2Note that a sentence compression can be represented as an
array of binary labels, one of them marking words to be retained
in compression and the other those to be dropped.
whose grammatically legitimate compressions
would include:
</bodyText>
<listItem confidence="0.986382625">
(4) (a) John -ga takai kuruma -wo kat-ta.
‘John bought an expensive car.’
(b) John -ga kuruma -wo kat-ta.
‘John bought a car.’
(c) Mushoku-no John -ga kuruma -wo kat-ta.
‘John, who is unemployed, bought a car.
(d) John -ga kat-ta.
‘John bought.’
(e) Mushoku-no John -ga kat-ta.
‘John, who is unemployed, bought.’
(f) Takai kuruma-wo kat-ta.
‘ Bought an expensive car.’
(g) Kuruma-wo kat-ta.
‘ Bought a car.’
(h) Kat-ta.
‘ Bought.’
</listItem>
<bodyText confidence="0.999856">
This would give us G(5)={a, b, c, d, e, f, g, h}, for
the input 3. Whatever choice we make for compres-
sion among candidates in G(5), should be gram-
matical, since they all are. One linguistic feature
</bodyText>
<page confidence="0.997644">
300
</page>
<figureCaption confidence="0.9999925">
Figure 2: Compressing an NP chunk
Figure 3: Trimming TDPs
</figureCaption>
<bodyText confidence="0.9999257">
of the Japanese language we need to take into ac-
count when generating compressions, is that the sen-
tence, which is free of word order and verb-final,
typically takes a left-branching structure as in Fig-
ure 1, consisting of an array of morphological units
called bunsetsu (BS, henceforth). A BS, which we
might regard as an inflected form (case marked in the
case of nouns) of verb, adjective, and noun, could
involve one or more independent linguistic elements
such as noun, case particle, but acts as a morpholog-
ical atom, in that it cannot be torn apart, or partially
deleted, without compromising the grammaticality.3
Noting that a Japanese sentence typically consists
of a sequence of case marked NPs and adjuncts, fol-
lowed by a main verb at the end (or what would
be called ‘matrix verb’ in linguistics), we seek to
compress each of the major chunks in the sentence,
leaving untouched the matrix verb, as its removal of-
ten leaves the sentence unintelligible. In particular,
starting with the leftmost BS in a major constituent,
</bodyText>
<footnote confidence="0.738192">
3Example 3 could be broken into BSs: /Mushuku -no /John
-ga / takai / kuruma -wo / kat-ta /.
</footnote>
<bodyText confidence="0.999623">
we work up the tree by pruning BSs on our way up,
which in general gives rise to grammatically legiti-
mate compressions of various lengths (Figure 2).
More specifically, we take the following steps to
construct G(5). Let 5 = ABCDE. Assume that
it has a dependency structure as in Figure 3. We
begin by locating terminal nodes, i.e., those which
have no incoming edges, depicted as filled circles
in Figure 3, and find a dependency (singly linked)
path from each terminal node to the root, or a node
labeled ‘E’ here, which would give us two paths
p1 = A-C-D-E and p2 = B-C-D-E (call them ter-
minating dependency paths, or TDPs). Now create
a set T of all trimmings, or suffixes of each TDP,
including an empty string:
</bodyText>
<equation confidence="0.9917725">
T (p1) = {&lt;A C D E&gt;, &lt;C D E&gt;, &lt;D E&gt;, &lt;E&gt;, &lt;&gt;}
T (p2) = {&lt;B C D E&gt;, &lt;C D E&gt;, &lt;D E&gt;, &lt;E&gt;, &lt;&gt;}
</equation>
<bodyText confidence="0.993639866666667">
Then we merge subpaths from the two sets in every
possible way, i.e., for any two subpaths t1 E T (p1)
and t2 E T (p2), we take a union over nodes in t1 and
t2; Figure 4 shows how this might done. We remove
duplicates if any. This would give us G(5)={{A B C
D E}, {A C D E}, {B C D E}, {C D E}, {D E}, {E},
{}}, a set of compressions over 5 based on TDPs.
What is interesting about the idea is that creating
G(5) does not involve much of anything that is spe-
cific to a given language. Indeed this could be done
on English as well. Take for instance a sentence at
the top of Table 1, which is a slightly modified lead
sentence from an article in the New York Times. As-
sume that we have a relevant dependency structure
as shown in Figure 5, where we have three TDPs,
i.e., one with southern, one with British and one with
lethal. Then G(5) would include those listed in Ta-
ble 1. A major difference from Japanese lies in the
direction in which a tree is branching out: right ver-
sus left.4
Having said this, we need to address some lan-
guage specific constraints: in Japanese, for instance,
we should keep a topic marked NP in compression
as its removal often leads to a decreased readability;
and also it is grammatically wrong to start any com-
pressed segment with sentence nominalizers such as
4We stand in a marked contrast to previous ‘grafting’ ap-
proaches which more or less rely on an ad-hoc collection
of transformation rules to generate candidates (Riezler et al.,
2003).
</bodyText>
<page confidence="0.999118">
301
</page>
<tableCaption confidence="0.999852">
Table 1: Hedge-clipping English
</tableCaption>
<table confidence="0.994189727272727">
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British
troops in southern Iraq
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British
troops in Iraq
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British
troops
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on troops
An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks
An official was quoted yesterday as accusing Iran of supplying explosive technology used in attacks
An official was quoted yesterday as accusing Iran of supplying explosive technology
An official was quoted yesterday as accusing Iran of supplying technology
</table>
<figureCaption confidence="0.9999955">
Figure 5: An English dependency structure and TDPs
Figure 4: Combining TDP suffixes
</figureCaption>
<bodyText confidence="0.999210285714285">
-koto and -no. In English, we should keep a prepo-
sition from being left dangling, as in An official was
quoted yesterday as accusing Iran of supplying tech-
nology used in. In any case, we need some extra
rules on G(5) to take care of language specific is-
sues (cf. Vandeghinste and Pan (2004) for English).
An important point about the dependency truncation
is that for most of the time, a compression it gener-
ates comes out reasonably grammatical, so the num-
ber of ‘extras’ should be small.
Finally, in order for CRFs to work with the com-
pressions, we need to translate them into a sequence
of binary labels, which involves labeling an element
token, bunsetsu or a word, with some label, e.g., 0
for ’remove’ and 1 for ‘retain,’ as in Figure 6.
Consider following compressions y1 to y4 for
x = 010203040506. 0i denotes a bunsetsu (BS).
‘0’ marks a BS to be removed and ‘1’ that to be re-
tained.
01 02 03 04 05 06
y1 0 1 1 1 1 1
y2 0 0 1 1 1 1
y3 0 0 0 0 0 1
y4 0 0 1 0 0 0
Assume that G(5) = {y1, y2, y3}. Because y4
is not part of G(5), it is not considered a candidate
for a compression for y, even if its likelihood may
exceed those of others in G(5). We note that the
approach here does not rely on so much of CRFs
as a discriminative classifier as CRFs as a strategy
for ranking among a limited set of label sequences
which correspond to syntactically plausible simpli-
fications of input sentence.
Furthermore, we could dictate the length of com-
pression by putbting an additional constraint on out-
</bodyText>
<page confidence="0.994243">
302
</page>
<figureCaption confidence="0.999702">
Figure 6: Compression in binary representation.
</figureCaption>
<bodyText confidence="0.591306">
put, as in:
</bodyText>
<equation confidence="0.994829">
y⋆ = arg max p(y|x;B), (5)
YEG′(S)
</equation>
<bodyText confidence="0.998844">
where G′(5) = {y : y E G(5), R(y, x) = r}.
R(y, x) denotes a compression rate r for which y is
desired, where r = # of 1 in y The constraint forces
length of x.
the trimmer to look for the best solution among can-
didates that satisfy the constraint, ignoring those that
do not.5
Another point to note is that G(5) is finite and rel-
atively small − it was found, for our domain, G(5)
usually runs somewhere between a few hundred and
ten thousand in length − so in practice it suffices
that we visit each compression in G(5), and select
one that gives the maximum value for the objective
function. We will have more to say about the size of
the search space in Section 6.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="method">
3 Features in CRFs
</sectionHeader>
<bodyText confidence="0.999948416666667">
We use an array of features in CRFs which are ei-
ther derived or borrowed from the taxonomy that
a Japanese tokenizer called JUMAN and KNP,6 a
Japanese dependency parser (aka Kurohashi-Nagao
Parser), make use of in characterizing the output
they produce: both JUMAN and KNP are part of the
compression model we build.
Features come in three varieties: semantic, mor-
phological and syntactic. Semantic features are used
for classifying entities into semantic types such as
name of person, organization, or place, while syn-
tactic features characterize the kinds of dependency
</bodyText>
<footnote confidence="0.999337">
5It is worth noting that the present approach can be recast
into one based on ‘constraint relaxation’ (Tromble and Eisner,
2006).
6http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html
</footnote>
<bodyText confidence="0.999403227272727">
relations that hold among BSs such as whether a BS
is of the type that combines with the verb (renyou),
or of the type that combines with the noun (rentai),
etc.
A morphological feature could be thought of as
something that broadly corresponds to an English
POS, marking for some syntactic or morphological
category such as noun, verb, numeral, etc. Also
we included ngram features to encode the lexi-
cal context in which a given morpheme appears.
Thus we might have something like: for some
words (morphemes) w1, w2, and w3, fw1·w2(w3) =
1 if w3 is preceded by w1, w2; otherwise, 0. In ad-
dition, we make use of an IR-related feature, whose
job is to indicate whether a given morpheme in the
input appears in the title of an associated article.
The motivation for the feature is obviously to iden-
tify concepts relevant to, or unique to the associ-
ated article. Also included was a feature on tfidf,
to mark words that are conceptually more important
than others. The number of features came to around
80,000 for the corpus we used in the experiment.
</bodyText>
<sectionHeader confidence="0.992633" genericHeader="method">
4 The Dependency Path Model
</sectionHeader>
<bodyText confidence="0.99900925">
In what follows, we will describe somewhat in
detail a prior approach to sentence compression
in Japanese which we call the ”dependency path
model,” or DPM. DPM was first introduced in
(Oguro et al., 2000), later explored by a number of
people (Morooka et al., 2004; Yamagata et al., 2006;
Fukutomi et al., 2007).7
DPM has the form:
</bodyText>
<equation confidence="0.999677">
h(y) = αf(y) + (1 − α)g(y), (6)
</equation>
<bodyText confidence="0.998796">
where y = Q0, Q1, ... , Qn−1, i.e., a compression
consisting of any number of bunsetsu’s, or phrase-
like elements. f(�) measures the relevance of con-
tent in y; and g(�) the fluency of text. α is to provide
a way of weighing up contributions from each com-
ponent.
We further define:
</bodyText>
<equation confidence="0.9961002">
q(Qi), (7)
7Kikuchi et al. (2003) explore an approach similar to DPM.
n−1∑
i=0
f(y) =
</equation>
<page confidence="0.994515">
303
</page>
<figureCaption confidence="0.840881916666667">
represent bunsestu’s that the edge spans. Cs(β) de-
notes the class of a bunsetsu where the edge starts
and Ce(β) that of a bunsetsu where the edge ends.
What we mean by ‘class of bunsetsu’ is some sort of
a classificatory scheme that concerns linguistic char-
acteristics of bunsetsu, such as a part-of-speech of
the head, whether it has an inflection, and if it does,
what type of inflection it has, etc. Moreover, DPM
uses two separate classificatory schemes for Cs(β)
and Ce(β).
In DPM, we define the connectivity strength p by:
Figure 7: A dependency structure
</figureCaption>
<equation confidence="0.771103428571429">
r log S(t) if DL(βi, βj) 7�
Sl
otherwise
−oo
p(βi, 0j) = oo (9)
and
p(βi, βs(i)). (8)
</equation>
<bodyText confidence="0.96927940625">
q(·) is meant to quantify how worthy of inclusion
in compression, a given bunsetsu is; and p(βi,βj)
represents the connectivity strength of dependency
relation between βi and βj. s(·) is a linking function
that associates with a bunsetsu any one of those that
follows it. g(y) thus represents a set of linked edges
that, if combined, give the largest probability for y.
Dependency path length (DL) refers to the num-
ber of (singly linked) dependency relations (or
edges) that span two bunsetsu’s. Consider the de-
pendency tree in Figure 7, which corresponds to
a somewhat contrived sentence ’Three-legged dogs
disappeared from sight.’ Take an English word for a
bunsetsu here. We have
DL(three-legged, dogs) = 1
DL(three-legged, disappeared) = 2
DL(three-legged, from) = oo
DL(three-legged, sight) = oo
Since dogs is one edge away from three-legged, DL
for them is 1; and we have DL of two for three-
legged and disappeared, as we need to cross two
edges in the direction of arrow to get from the for-
mer to the latter. In case there is no path between
words as in the last two cases above, we take the DL
to be infinite.
DPM takes a dependency tree to be a set of
linked edges. Each edge is expressed as a triple
&lt; Cs(βi), Ce(βj), DL(βi, βj) &gt;, where βi and βj
where t =&lt; Cs(βi),Ce(βj),DL(βi,βj) &gt;, and
S(t) is the probability of t occurring in a compres-
sion, which is given by:
# of t’s found in compressions
</bodyText>
<equation confidence="0.971326">
S(t) = (10)
</equation>
<bodyText confidence="0.9439515">
# of triples found in the training data
We complete the DPM formulation with:
</bodyText>
<equation confidence="0.998108">
q(β) = log pc(β) + tfidf(β) (11)
</equation>
<bodyText confidence="0.999886">
pc(β) denotes the probability of having bunsetsu β
in compression, calculated analogously to Eq. 10,8
and tfidf(β) obviously denotes the tfidf value of β.
In DPM, a compression of a given sentence can be
obtained by finding arg maxy h(y), where y ranges
over possible candidate compressions of a particular
length one may derive from that sentence. In the
experiment described later, we set α = 0.1 for DPM,
following Morooka et al. (2004), who found the best
performance with that setting for α.
</bodyText>
<sectionHeader confidence="0.996039" genericHeader="method">
5 Evaluation Setup
</sectionHeader>
<bodyText confidence="0.989763">
We created a corpus of sentence summaries based
on email news bulletins we had received over five
to six months from an on-line news provider called
Nikkei Net, which mostly deals with finance and
politics.9 Each bulletin consists of six to seven news
briefs, each with a few sentences. Since a news brief
contains nothing to indicate what its longer version
8DPM puts bunsetsu’s into some groups based on linguis-
tic features associated with them, and uses the statistics of the
groups for P. rather than that of bunsetsu’s that actually appear
in text.
</bodyText>
<equation confidence="0.9061592">
9http://www.nikkei.co.jp
g(y) = max
s
n−2∑
i=0
</equation>
<page confidence="0.999518">
304
</page>
<tableCaption confidence="0.999387">
Table 2: The rating scale on fluency
</tableCaption>
<table confidence="0.4535786">
RATING EXPLANATION
1 makes no sense
2 only partially intelligible/grammatical
3 makes sense; seriously flawed in gram-
mar
</table>
<tableCaption confidence="0.98870025">
4 makes good sense; only slightly flawed
in grammar
5 makes perfect sense; no grammar flaws
Table 3: The rating scale on content overlap
</tableCaption>
<figure confidence="0.660890333333333">
RATING EXPLANATION
1 no overlap with reference
2 poor or marginal overlap w. ref.
3 moderate overlap w. ref.
4 significant overlap w. ref.
5 perfect overlap w. ref.
</figure>
<bodyText confidence="0.994560285714286">
might look like, we manually searched the news site
for a full-length article that might reasonably be con-
sidered a long version of that brief.
We extracted lead sentences both from the brief
and from its source article, and aligned them, us-
ing what is known as the Smith-Waterman algorithm
(Smith and Waterman, 1981), which produced 1,401
pairs of summary and source sentence.10 For the
ease of reference, we call the corpus so produced
‘NICOM’ for the rest of the paper. A part of our sys-
tem makes use of a modeling toolkit called GRMM
(Sutton et al., 2004; Sutton, 2006). Throughout the
experiments, we call our approach ‘Generic Sen-
tence Trimmer’ or GST.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999914333333333">
We ran DPM and GST on NICOM in the 10-fold
cross validation format where we break the data into
10 blocks, use 9 of them for training and test on the
remaining block. In addition, we ran the test at three
different compression rates, 50%, 60% and 70%, to
learn how they affect the way the models perform.
This means that for each input sentence in NICOM,
we have three versions of its compression created,
corresponding to a particular rate at which the sen-
tence is compressed. We call a set of compressions
so generated ‘NICOM-g.’
In order to evaluate the quality of outputs GST
and DPM generate, we asked 6 people, all Japanese
natives, to make an intuitive judgment on how each
compression fares in fluency and relevance to gold
</bodyText>
<footnote confidence="0.55898975">
10The Smith-Waterman algorithm aims at finding a best
match between two sequences which may include gaps, such
as A-C-D-E and A-B-C-D-E. The algorithm is based on an idea
rather akin to dynamic programming.
</footnote>
<bodyText confidence="0.995400945945946">
standards (created by humans), on a scale of 1 to 5.
To this end, we conducted evaluation in two sepa-
rate formats; one concerns fluency and the other rel-
evance. The fluency test consisted of a set of com-
pressions which we created by randomly selecting
200 of them from NICOM-g, for each model at com-
pression rates 50%, 60%, and 70%; thus we have
200 samples for each model and each compression
rate.11 The total number of test compressions came
to 1,200.
The relevance test, on the other hand, consisted of
paired compressions along with the associated gold
standard compressions. Each pair contains compres-
sions both from DPM and from GST at a given com-
pression rate. We randomly picked 200 of them from
NICOM-g, at each compression rate, and asked the
participants to make a subjective judgment on how
much of the content in a compression semantically
overlap with that of the gold standard, on a scale of
1 to 5 (Table 3). Also included in the survey are 200
gold standard compressions, to get some idea of how
fluent “ideal” compressions are, compared to those
generated by machine.
Tables 4 and 5 summarize the results. Table 4
looks at the fluency of compressions generated by
each of the models; Table 5 looks at how much of
the content in reference is retained in compressions.
In either table, CR stands for compression rate. All
the results are averaged over samples.
We find in Table 4 a clear superiority of GST over
DPM at every compression rate examined, with flu-
ency improved by as much as 60% at 60%. How-
ever, GST fell short of what human compressions
achieved in fluency − an issue we need to address
11As stated elsewhere, by compression rate, we mean r =
# of 1 in y
length of x.
</bodyText>
<page confidence="0.999273">
305
</page>
<tableCaption confidence="0.997181">
Table 4: Fluency (Average)
</tableCaption>
<table confidence="0.99787825">
MODEL/CR 50% 60% 70%
GST 3.430 3.820 3.810
DPM 2.222 2.372 2.660
Human − 4.45 −
</table>
<tableCaption confidence="0.9798">
Table 5: Semantic (Content) Overlap (Average)
</tableCaption>
<table confidence="0.945797666666667">
MODEL/CR 50% 60% 70%
GST 2.720 3.181 3.405
DPM 2.210 2.548 2.890
Frequency 0 400 800 1200
0 500 1500 2500
Number of Candidates
</table>
<bodyText confidence="0.999569481481482">
in the future. Since the average CR of gold standard
compressions was 60%, we report their fluency at
that rate only.
Table 5 shows the results in relevance of con-
tent. Again GST marks a superior performance over
DPM, beating it at every compression rate. It is in-
teresting to observe that GST manages to do well
in the semantic overlap, despite the cutback on the
search space we forced on GST.
As for fluency, we suspect that the superior per-
formance of GST is largely due to the depen-
dency truncation the model is equipped with; and
its performance in content overlap owes a lot to
CRFs. However, just how much improvement GST
achieved over regular CRFs (with no truncation) in
fluency and in relevance is something that remains
to be seen, as the latter do not allow for variable
length compression, which prohibits a straightfor-
ward comparison between the two kinds of models.
We conclude the section with a few words on the
size of |G(S)|, i.e., the number of candidates gener-
ated per run of compression with GST.
Figure 8 shows the distribution of the numbers of
candidates generated per compression, which looks
like the familiar scale-free power curve. Over 99%
of the time, the number of candidates or |G(S) |is
found to be less than 500.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.991212">
This paper introduced a novel approach to sentence
compression in Japanese, which combines a syntac-
tically motivated generation model and CRFs, in or-
</bodyText>
<figureCaption confidence="0.999512">
Figure 8: The distribution of IG(S)
</figureCaption>
<bodyText confidence="0.999978318181818">
der to address fluency and relevance of compres-
sions we generate. What distinguishes this work
from prior research is its overt withdrawal from a
search for global optima to a search for local optima
that comply with grammar.
We believe that our idea was empirically borne
out, as the experiments found that our approach out-
performs, by a large margin, a previously known
method called DPM, which employs a global search
strategy. The results on semantic overlap indicates
that the narrowing down of compressions we search
obviously does not harm their relevance to refer-
ences.
An interesting future exercise would be to explore
whether it is feasible to rewrite Eq. 5 as a linear inte-
ger program. If it is, the whole scheme of ours would
fall under what is known as ‘Linear Programming
CRFs’ (Tasker, 2004; Roth and Yih, 2005). What re-
mains to be seen, however, is whether GST is trans-
ferrable to languages other than Japanese, notably,
English. The answer is likely to be yes, but details
have yet to be worked out.
</bodyText>
<sectionHeader confidence="0.994546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.531369">
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer programming
</reference>
<page confidence="0.996651">
306
</page>
<reference confidence="0.999655028571428">
approach. In Proceedings of the COLING/ACL 2006,
pages 144–151.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of the 2007Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 73–82, Prague, June.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generataion. In Proceedings of the HLT-NAACL
Text Summarization Workshop and Document Under-
standing Conderence (DUC03), pages 1–8, Edmon-
ton, Canada.
Satoshi Fukutomi, Kazuyuki Takagi, and Kazuhiko
Ozeki. 2007. Japanese Sentence Compression using
Probabilistic Approach. In Proceedings of the 13th
Annual Meeting of the Association for Natural Lan-
guage Processing Japan.
Michel Galley and Kathleen McKeown. 2007. Lexical-
ized Markov grammars for sentence compression. In
Proceedings of the HLT-NAACL 2007, pages 180–187.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th Confer-
ence on Applied Natural Language Processing, pages
310–315.
Tomonori Kikuchi, Sadaoki Furui, and Chiori Hori.
2003. Two-stage automatic speech summarization by
sentence extraction and compaction. In Proceedings
of ICASSP 2003.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91–107.
John Lafferty, Andrew MacCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference
on Machine Learning (ICML-2001).
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of the 11th Conference of EACL, pages 297–304.
Yuhei Morooka, Makoto Esaki, Kazuyuki Takagi, and
Kazuhiko Ozeki. 2004. Automatic summarization of
news articles using sentence compaction and extrac-
tion. In Proceedings of the 10th Annual Meeting of
Natural Language Processing, pages 436–439, March.
(In Japanese).
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Information
Processing and Management, 43:1571 – 1587.
Rei Oguro, Kazuhiko Ozeki, Yujie Zhang, and Kazuyuki
Takagi. 2000. An efficient algorithm for Japanese
sentence compaction based on phrase importance
and inter-phrase dependency. In Proceedings of
TSD 2000 (Lecture Notes in Artificial Intelligence
1902,Springer-Verlag), pages 65–81, Brno, Czech Re-
public.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical functional grammar. In Pro-
ceedings of HLT-NAACL 2003, pages 118–125, Ed-
monton.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd International Conference on
Machine Learning (ICML 05).
T. F. Smith and M. S. Waterman. 1981. Identification of
common molecular subsequence. Journal of Molecu-
lar Biology, 147:195–197.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic labeling and segment-
ing sequence data. In Proceedings of the 21st In-
ternational Conference on Machine Learning, Banff,
Canada.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Ben Tasker. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
Roy W. Tromble and Jason Eisner. 2006. A fast finite-
state relaxation method for enforcing global constraint
on sequence decoding. In Proceeings of the NAACL,
pages 423–430.
Jenie Turner and Eugen Charniak. 2005. Supervised and
unsupervised learning for sentence compression. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 290–297, Ann Arbor, June.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automatic subtitling: A hybrid approach.
In Proceedings of the ACL workshop on Text Summa-
rization, Barcelona.
Kiwamu Yamagata, Satoshi Fukutomi, Kazuyuki Takagi,
and Kzauhiko Ozeki. 2006. Sentence compression
using statistical information about dependency path
length. In Proceedings of TSD 2006 (Lecture Notes in
Computer Science, Vol. 4188/2006), pages 127–134,
Brno, Czech Republic.
</reference>
<page confidence="0.998599">
307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931160">
<title confidence="0.999707">A Generic Sentence Trimmer with CRFs</title>
<author confidence="0.986502">Tadashi Nomoto</author>
<affiliation confidence="0.99939">National Institute of Japanese Literature</affiliation>
<address confidence="0.9836105">10-3, Midori Tachikawa Tokyo, 190-0014, Japan</address>
<email confidence="0.993371">nomoto@acm.org</email>
<abstract confidence="0.998736666666667">The paper presents a novel sentence trimmer in Japanese, which combines a non-statistical yet generic tree generation model and Conditional Random Fields (CRFs), to address improving the grammaticality of compression while retaining its relevance. Experiments found that the present approach outperforms in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., Fukutomi et al., 2007) only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Constraintbased sentence compression: An integer programming approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL</booktitle>
<pages>144--151</pages>
<contexts>
<context position="1399" citStr="Clarke and Lapata, 2006" startWordPosition="212" endWordPosition="215">ion For better or worse, much of prior work on sentence compression (Riezler et al., 2003; McDonald, 2006; Turner and Charniak, 2005) turned to a single corpus developed by Knight and Marcu (2002) (K&amp;M, henceforth) for evaluating their approaches. The K&amp;M corpus is a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data (Turner and Charniak, 2005). Our approach is broadly in line with prior work (Jing, 2000; Dorr et al., 2003; Riezler et al., 2003; Clarke and Lapata, 2006), in that we make use of some form of syntactic knowledge to constrain compressions we generate. What sets this work apart from them, however, is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions (Lafferty et al., 2001; Sutton and McCall</context>
<context position="2756" citStr="Clarke and Lapata (2006)" startWordPosition="432" endWordPosition="436">tic framework which permits information from various sources to be integrated towards compressing sentence, a property K&amp;M do not share. Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007). We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar. Thus, unlike McDonald (2006), Clarke and Lapata (2006) and Cohn and Lapata (2007), we do not insist on finding a globally optimal solution in the space of 2&apos; possible compressions for an n word long sentence. Rather we insist on finding a most plausible compression among those that are explicitly warranted by the grammar. Later in the paper, we will introduce an approach called the ‘Dependency Path Model’ (DPM) from the previous literature (Section 4), which purports to provide a robust framework for sentence compres299 Proceedings of ACL-08: HLT, pages 299–307, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics sion</context>
<context position="4690" citStr="Clarke and Lapata (2006)" startWordPosition="758" endWordPosition="761">-based approach we call a ‘dependency truncation,’ which as we will see, gives us a greater control over the form that compression takes. Let us denote a set of label assignments for 5 that satisfy constraints, by G(5).2 We seek to solve the following, Consider the following. (3) Mushoku-no John -ga takai kuruma unemployed John SBJ expensive car -wo kat-ta. ACC buy PAST ‘John, who is unemployed, bought an expensive car.’ y⋆ = arg max p(y|x;o). (2) Y∈G(S) There would be a number of ways to go about the problem. In the context of sentence compression, a linear programming based approach such as Clarke and Lapata (2006) is certainly one that deserves consideration. In this paper, however, we will explore a much simpler approach which does not require as involved formulation as Clarke and Lapata (2006) do. We approach the problem extentionally, i.e., through generating sentences that are grammatical, or that conform to whatever constraints there are. 1Assume as usual that CRFs take the form, p(y|x) a (/moexp k,j Ajfj(yk, yk−1, x) + /moi pigi(xk, yk, x) = exp[w⊤f(x, y)] (1) fj and gi are ‘features’ associated with edges and vertices, respectively, and k E C, where C denotes a set of cliques in CRFs. Aj and pi </context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Constraintbased sentence compression: An integer programming approach. In Proceedings of the COLING/ACL 2006, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Large margin synchronous generation and its application to sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>73--82</pages>
<location>Prague,</location>
<contexts>
<context position="2783" citStr="Cohn and Lapata (2007)" startWordPosition="438" endWordPosition="441">nformation from various sources to be integrated towards compressing sentence, a property K&amp;M do not share. Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007). We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar. Thus, unlike McDonald (2006), Clarke and Lapata (2006) and Cohn and Lapata (2007), we do not insist on finding a globally optimal solution in the space of 2&apos; possible compressions for an n word long sentence. Rather we insist on finding a most plausible compression among those that are explicitly warranted by the grammar. Later in the paper, we will introduce an approach called the ‘Dependency Path Model’ (DPM) from the previous literature (Section 4), which purports to provide a robust framework for sentence compres299 Proceedings of ACL-08: HLT, pages 299–307, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics sion in Japanese. We will look </context>
<context position="4010" citStr="Cohn and Lapata, 2007" startWordPosition="642" endWordPosition="645"> the present approach compares with that of DPM in Section 6. 2 A Sentence Trimmer with CRFs Figure 1: Syntactic structure in Japanese Our idea on how to make CRFs comply with grammar is quite simple: we focus on only those label sequences that are associated with grammatically correct compressions, by making CRFs look at only those that comply with some grammatical constraints G, and ignore others, regardless of how probable they are.1 But how do we find compressions that are grammatical? To address the issue, rather than resort to statistical generation models as in the previous literature (Cohn and Lapata, 2007; Galley and McKeown, 2007), we pursue a particular rule-based approach we call a ‘dependency truncation,’ which as we will see, gives us a greater control over the form that compression takes. Let us denote a set of label assignments for 5 that satisfy constraints, by G(5).2 We seek to solve the following, Consider the following. (3) Mushoku-no John -ga takai kuruma unemployed John SBJ expensive car -wo kat-ta. ACC buy PAST ‘John, who is unemployed, bought an expensive car.’ y⋆ = arg max p(y|x;o). (2) Y∈G(S) There would be a number of ways to go about the problem. In the context of sentence c</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Large margin synchronous generation and its application to sentence compression. In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 73–82, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generataion.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL Text Summarization Workshop and Document Understanding Conderence (DUC03),</booktitle>
<pages>1--8</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1670" citStr="Dorr et al., 2003" startWordPosition="257" endWordPosition="260">y sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data (Turner and Charniak, 2005). Our approach is broadly in line with prior work (Jing, 2000; Dorr et al., 2003; Riezler et al., 2003; Clarke and Lapata, 2006), in that we make use of some form of syntactic knowledge to constrain compressions we generate. What sets this work apart from them, however, is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions (Lafferty et al., 2001; Sutton and McCallum, 2006). An obvious benefit of using CRFs for sentence compression is that the model provides a general (and principled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property K&amp;M do not share. N</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generataion. In Proceedings of the HLT-NAACL Text Summarization Workshop and Document Understanding Conderence (DUC03), pages 1–8, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Fukutomi</author>
<author>Kazuyuki Takagi</author>
<author>Kazuhiko Ozeki</author>
</authors>
<title>Japanese Sentence Compression using Probabilistic Approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Association for Natural Language Processing Japan.</booktitle>
<contexts>
<context position="15188" citStr="Fukutomi et al., 2007" startWordPosition="2627" endWordPosition="2630"> feature is obviously to identify concepts relevant to, or unique to the associated article. Also included was a feature on tfidf, to mark words that are conceptually more important than others. The number of features came to around 80,000 for the corpus we used in the experiment. 4 The Dependency Path Model In what follows, we will describe somewhat in detail a prior approach to sentence compression in Japanese which we call the ”dependency path model,” or DPM. DPM was first introduced in (Oguro et al., 2000), later explored by a number of people (Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007).7 DPM has the form: h(y) = αf(y) + (1 − α)g(y), (6) where y = Q0, Q1, ... , Qn−1, i.e., a compression consisting of any number of bunsetsu’s, or phraselike elements. f(�) measures the relevance of content in y; and g(�) the fluency of text. α is to provide a way of weighing up contributions from each component. We further define: q(Qi), (7) 7Kikuchi et al. (2003) explore an approach similar to DPM. n−1∑ i=0 f(y) = 303 represent bunsestu’s that the edge spans. Cs(β) denotes the class of a bunsetsu where the edge starts and Ce(β) that of a bunsetsu where the edge ends. What we mean by ‘class of</context>
</contexts>
<marker>Fukutomi, Takagi, Ozeki, 2007</marker>
<rawString>Satoshi Fukutomi, Kazuyuki Takagi, and Kazuhiko Ozeki. 2007. Japanese Sentence Compression using Probabilistic Approach. In Proceedings of the 13th Annual Meeting of the Association for Natural Language Processing Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>180--187</pages>
<contexts>
<context position="1426" citStr="Galley and McKeown, 2007" startWordPosition="216" endWordPosition="220">much of prior work on sentence compression (Riezler et al., 2003; McDonald, 2006; Turner and Charniak, 2005) turned to a single corpus developed by Knight and Marcu (2002) (K&amp;M, henceforth) for evaluating their approaches. The K&amp;M corpus is a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data (Turner and Charniak, 2005). Our approach is broadly in line with prior work (Jing, 2000; Dorr et al., 2003; Riezler et al., 2003; Clarke and Lapata, 2006), in that we make use of some form of syntactic knowledge to constrain compressions we generate. What sets this work apart from them, however, is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions (Lafferty et al., 2001; Sutton and McCallum, 2006). An obvious benef</context>
<context position="4037" citStr="Galley and McKeown, 2007" startWordPosition="646" endWordPosition="649">ompares with that of DPM in Section 6. 2 A Sentence Trimmer with CRFs Figure 1: Syntactic structure in Japanese Our idea on how to make CRFs comply with grammar is quite simple: we focus on only those label sequences that are associated with grammatically correct compressions, by making CRFs look at only those that comply with some grammatical constraints G, and ignore others, regardless of how probable they are.1 But how do we find compressions that are grammatical? To address the issue, rather than resort to statistical generation models as in the previous literature (Cohn and Lapata, 2007; Galley and McKeown, 2007), we pursue a particular rule-based approach we call a ‘dependency truncation,’ which as we will see, gives us a greater control over the form that compression takes. Let us denote a set of label assignments for 5 that satisfy constraints, by G(5).2 We seek to solve the following, Consider the following. (3) Mushoku-no John -ga takai kuruma unemployed John SBJ expensive car -wo kat-ta. ACC buy PAST ‘John, who is unemployed, bought an expensive car.’ y⋆ = arg max p(y|x;o). (2) Y∈G(S) There would be a number of ways to go about the problem. In the context of sentence compression, a linear progra</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In Proceedings of the HLT-NAACL 2007, pages 180–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing,</booktitle>
<pages>310--315</pages>
<contexts>
<context position="1651" citStr="Jing, 2000" startWordPosition="255" endWordPosition="256"> a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data (Turner and Charniak, 2005). Our approach is broadly in line with prior work (Jing, 2000; Dorr et al., 2003; Riezler et al., 2003; Clarke and Lapata, 2006), in that we make use of some form of syntactic knowledge to constrain compressions we generate. What sets this work apart from them, however, is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions (Lafferty et al., 2001; Sutton and McCallum, 2006). An obvious benefit of using CRFs for sentence compression is that the model provides a general (and principled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property </context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In Proceedings of the 6th Conference on Applied Natural Language Processing, pages 310–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomonori Kikuchi</author>
<author>Sadaoki Furui</author>
<author>Chiori Hori</author>
</authors>
<title>Two-stage automatic speech summarization by sentence extraction and compaction.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="15554" citStr="Kikuchi et al. (2003)" startWordPosition="2698" endWordPosition="2701">ior approach to sentence compression in Japanese which we call the ”dependency path model,” or DPM. DPM was first introduced in (Oguro et al., 2000), later explored by a number of people (Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007).7 DPM has the form: h(y) = αf(y) + (1 − α)g(y), (6) where y = Q0, Q1, ... , Qn−1, i.e., a compression consisting of any number of bunsetsu’s, or phraselike elements. f(�) measures the relevance of content in y; and g(�) the fluency of text. α is to provide a way of weighing up contributions from each component. We further define: q(Qi), (7) 7Kikuchi et al. (2003) explore an approach similar to DPM. n−1∑ i=0 f(y) = 303 represent bunsestu’s that the edge spans. Cs(β) denotes the class of a bunsetsu where the edge starts and Ce(β) that of a bunsetsu where the edge ends. What we mean by ‘class of bunsetsu’ is some sort of a classificatory scheme that concerns linguistic characteristics of bunsetsu, such as a part-of-speech of the head, whether it has an inflection, and if it does, what type of inflection it has, etc. Moreover, DPM uses two separate classificatory schemes for Cs(β) and Ce(β). In DPM, we define the connectivity strength p by: Figure 7: A de</context>
</contexts>
<marker>Kikuchi, Furui, Hori, 2003</marker>
<rawString>Tomonori Kikuchi, Sadaoki Furui, and Chiori Hori. 2003. Two-stage automatic speech summarization by sentence extraction and compaction. In Proceedings of ICASSP 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<pages>139--91</pages>
<contexts>
<context position="972" citStr="Knight and Marcu (2002)" startWordPosition="146" endWordPosition="149">e grammaticality of compression while retaining its relevance. Experiments found that the present approach outperforms in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007) − the only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison. 1 Introduction For better or worse, much of prior work on sentence compression (Riezler et al., 2003; McDonald, 2006; Turner and Charniak, 2005) turned to a single corpus developed by Knight and Marcu (2002) (K&amp;M, henceforth) for evaluating their approaches. The K&amp;M corpus is a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data (Turner a</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139:91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew MacCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning (ICML-2001).</booktitle>
<contexts>
<context position="1980" citStr="Lafferty et al., 2001" startWordPosition="310" endWordPosition="313"> McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data (Turner and Charniak, 2005). Our approach is broadly in line with prior work (Jing, 2000; Dorr et al., 2003; Riezler et al., 2003; Clarke and Lapata, 2006), in that we make use of some form of syntactic knowledge to constrain compressions we generate. What sets this work apart from them, however, is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions (Lafferty et al., 2001; Sutton and McCallum, 2006). An obvious benefit of using CRFs for sentence compression is that the model provides a general (and principled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property K&amp;M do not share. Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007). We tackle the issues by harnessing CRFs with what we</context>
</contexts>
<marker>Lafferty, MacCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew MacCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of EACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="881" citStr="McDonald, 2006" startWordPosition="132" endWordPosition="133">ree generation model and Conditional Random Fields (CRFs), to address improving the grammaticality of compression while retaining its relevance. Experiments found that the present approach outperforms in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007) − the only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison. 1 Introduction For better or worse, much of prior work on sentence compression (Riezler et al., 2003; McDonald, 2006; Turner and Charniak, 2005) turned to a single corpus developed by Knight and Marcu (2002) (K&amp;M, henceforth) for evaluating their approaches. The K&amp;M corpus is a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to</context>
<context position="2730" citStr="McDonald (2006)" startWordPosition="430" endWordPosition="431">ipled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property K&amp;M do not share. Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007). We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar. Thus, unlike McDonald (2006), Clarke and Lapata (2006) and Cohn and Lapata (2007), we do not insist on finding a globally optimal solution in the space of 2&apos; possible compressions for an n word long sentence. Rather we insist on finding a most plausible compression among those that are explicitly warranted by the grammar. Later in the paper, we will introduce an approach called the ‘Dependency Path Model’ (DPM) from the previous literature (Section 4), which purports to provide a robust framework for sentence compres299 Proceedings of ACL-08: HLT, pages 299–307, Columbus, Ohio, USA, June 2008. c�2008 Association for Comp</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In Proceedings of the 11th Conference of EACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhei Morooka</author>
<author>Makoto Esaki</author>
<author>Kazuyuki Takagi</author>
<author>Kazuhiko Ozeki</author>
</authors>
<title>Automatic summarization of news articles using sentence compaction and extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th Annual Meeting of Natural Language Processing,</booktitle>
<pages>436--439</pages>
<note>(In Japanese).</note>
<contexts>
<context position="15141" citStr="Morooka et al., 2004" startWordPosition="2619" endWordPosition="2622">an associated article. The motivation for the feature is obviously to identify concepts relevant to, or unique to the associated article. Also included was a feature on tfidf, to mark words that are conceptually more important than others. The number of features came to around 80,000 for the corpus we used in the experiment. 4 The Dependency Path Model In what follows, we will describe somewhat in detail a prior approach to sentence compression in Japanese which we call the ”dependency path model,” or DPM. DPM was first introduced in (Oguro et al., 2000), later explored by a number of people (Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007).7 DPM has the form: h(y) = αf(y) + (1 − α)g(y), (6) where y = Q0, Q1, ... , Qn−1, i.e., a compression consisting of any number of bunsetsu’s, or phraselike elements. f(�) measures the relevance of content in y; and g(�) the fluency of text. α is to provide a way of weighing up contributions from each component. We further define: q(Qi), (7) 7Kikuchi et al. (2003) explore an approach similar to DPM. n−1∑ i=0 f(y) = 303 represent bunsestu’s that the edge spans. Cs(β) denotes the class of a bunsetsu where the edge starts and Ce(β) that of a bunsetsu</context>
<context position="18227" citStr="Morooka et al. (2004)" startWordPosition="3174" endWordPosition="3177">ression, which is given by: # of t’s found in compressions S(t) = (10) # of triples found in the training data We complete the DPM formulation with: q(β) = log pc(β) + tfidf(β) (11) pc(β) denotes the probability of having bunsetsu β in compression, calculated analogously to Eq. 10,8 and tfidf(β) obviously denotes the tfidf value of β. In DPM, a compression of a given sentence can be obtained by finding arg maxy h(y), where y ranges over possible candidate compressions of a particular length one may derive from that sentence. In the experiment described later, we set α = 0.1 for DPM, following Morooka et al. (2004), who found the best performance with that setting for α. 5 Evaluation Setup We created a corpus of sentence summaries based on email news bulletins we had received over five to six months from an on-line news provider called Nikkei Net, which mostly deals with finance and politics.9 Each bulletin consists of six to seven news briefs, each with a few sentences. Since a news brief contains nothing to indicate what its longer version 8DPM puts bunsetsu’s into some groups based on linguistic features associated with them, and uses the statistics of the groups for P. rather than that of bunsetsu’s</context>
</contexts>
<marker>Morooka, Esaki, Takagi, Ozeki, 2004</marker>
<rawString>Yuhei Morooka, Makoto Esaki, Kazuyuki Takagi, and Kazuhiko Ozeki. 2004. Automatic summarization of news articles using sentence compaction and extraction. In Proceedings of the 10th Annual Meeting of Natural Language Processing, pages 436–439, March. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>Discriminative sentence compression with conditional random fields.</title>
<date>2007</date>
<booktitle>Information Processing and Management, 43:1571 –</booktitle>
<pages>1587</pages>
<contexts>
<context position="2526" citStr="Nomoto, 2007" startWordPosition="396" endWordPosition="397">(CRFs) to select among possible compressions (Lafferty et al., 2001; Sutton and McCallum, 2006). An obvious benefit of using CRFs for sentence compression is that the model provides a general (and principled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property K&amp;M do not share. Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007). We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar. Thus, unlike McDonald (2006), Clarke and Lapata (2006) and Cohn and Lapata (2007), we do not insist on finding a globally optimal solution in the space of 2&apos; possible compressions for an n word long sentence. Rather we insist on finding a most plausible compression among those that are explicitly warranted by the grammar. Later in the paper, we will introduce an approach called the ‘Dependency Path Model’ (DPM) from the </context>
</contexts>
<marker>Nomoto, 2007</marker>
<rawString>Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Processing and Management, 43:1571 – 1587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rei Oguro</author>
<author>Kazuhiko Ozeki</author>
<author>Yujie Zhang</author>
<author>Kazuyuki Takagi</author>
</authors>
<title>An efficient algorithm for Japanese sentence compaction based on phrase importance and inter-phrase dependency.</title>
<date>2000</date>
<booktitle>In Proceedings of TSD 2000 (Lecture Notes in Artificial Intelligence 1902,Springer-Verlag),</booktitle>
<pages>65--81</pages>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="15081" citStr="Oguro et al., 2000" startWordPosition="2608" endWordPosition="2611">ther a given morpheme in the input appears in the title of an associated article. The motivation for the feature is obviously to identify concepts relevant to, or unique to the associated article. Also included was a feature on tfidf, to mark words that are conceptually more important than others. The number of features came to around 80,000 for the corpus we used in the experiment. 4 The Dependency Path Model In what follows, we will describe somewhat in detail a prior approach to sentence compression in Japanese which we call the ”dependency path model,” or DPM. DPM was first introduced in (Oguro et al., 2000), later explored by a number of people (Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007).7 DPM has the form: h(y) = αf(y) + (1 − α)g(y), (6) where y = Q0, Q1, ... , Qn−1, i.e., a compression consisting of any number of bunsetsu’s, or phraselike elements. f(�) measures the relevance of content in y; and g(�) the fluency of text. α is to provide a way of weighing up contributions from each component. We further define: q(Qi), (7) 7Kikuchi et al. (2003) explore an approach similar to DPM. n−1∑ i=0 f(y) = 303 represent bunsestu’s that the edge spans. Cs(β) denotes the class of a</context>
</contexts>
<marker>Oguro, Ozeki, Zhang, Takagi, 2000</marker>
<rawString>Rei Oguro, Kazuhiko Ozeki, Yujie Zhang, and Kazuyuki Takagi. 2000. An efficient algorithm for Japanese sentence compaction based on phrase importance and inter-phrase dependency. In Proceedings of TSD 2000 (Lecture Notes in Artificial Intelligence 1902,Springer-Verlag), pages 65–81, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Annie Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical functional grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>118--125</pages>
<location>Edmonton.</location>
<contexts>
<context position="865" citStr="Riezler et al., 2003" startWordPosition="128" endWordPosition="131">tistical yet generic tree generation model and Conditional Random Fields (CRFs), to address improving the grammaticality of compression while retaining its relevance. Experiments found that the present approach outperforms in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007) − the only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison. 1 Introduction For better or worse, much of prior work on sentence compression (Riezler et al., 2003; McDonald, 2006; Turner and Charniak, 2005) turned to a single corpus developed by Knight and Marcu (2002) (K&amp;M, henceforth) for evaluating their approaches. The K&amp;M corpus is a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that resear</context>
<context position="9755" citStr="Riezler et al., 2003" startWordPosition="1672" endWordPosition="1675">those listed in Table 1. A major difference from Japanese lies in the direction in which a tree is branching out: right versus left.4 Having said this, we need to address some language specific constraints: in Japanese, for instance, we should keep a topic marked NP in compression as its removal often leads to a decreased readability; and also it is grammatically wrong to start any compressed segment with sentence nominalizers such as 4We stand in a marked contrast to previous ‘grafting’ approaches which more or less rely on an ad-hoc collection of transformation rules to generate candidates (Riezler et al., 2003). 301 Table 1: Hedge-clipping English An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British troops in southern Iraq An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British troops in Iraq An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on British troops An official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on troops An official was quoted yesterday as accusing </context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>Stefan Riezler, Tracy H. King, Richard Crouch, and Annie Zaenen. 2003. Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical functional grammar. In Proceedings of HLT-NAACL 2003, pages 118–125, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning (ICML 05).</booktitle>
<marker>Roth, Yih, 2005</marker>
<rawString>Dan Roth and Wen-tau Yih. 2005. Integer linear programming inference for conditional random fields. In Proceedings of the 22nd International Conference on Machine Learning (ICML 05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T F Smith</author>
<author>M S Waterman</author>
</authors>
<title>Identification of common molecular subsequence.</title>
<date>1981</date>
<journal>Journal of Molecular Biology,</journal>
<pages>147--195</pages>
<contexts>
<context position="19686" citStr="Smith and Waterman, 1981" startWordPosition="3421" endWordPosition="3424">ammar 4 makes good sense; only slightly flawed in grammar 5 makes perfect sense; no grammar flaws Table 3: The rating scale on content overlap RATING EXPLANATION 1 no overlap with reference 2 poor or marginal overlap w. ref. 3 moderate overlap w. ref. 4 significant overlap w. ref. 5 perfect overlap w. ref. might look like, we manually searched the news site for a full-length article that might reasonably be considered a long version of that brief. We extracted lead sentences both from the brief and from its source article, and aligned them, using what is known as the Smith-Waterman algorithm (Smith and Waterman, 1981), which produced 1,401 pairs of summary and source sentence.10 For the ease of reference, we call the corpus so produced ‘NICOM’ for the rest of the paper. A part of our system makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006). Throughout the experiments, we call our approach ‘Generic Sentence Trimmer’ or GST. 6 Results and Discussion We ran DPM and GST on NICOM in the 10-fold cross validation format where we break the data into 10 blocks, use 9 of them for training and test on the remaining block. In addition, we ran the test at three different compression rates,</context>
</contexts>
<marker>Smith, Waterman, 1981</marker>
<rawString>T. F. Smith and M. S. Waterman. 1981. Identification of common molecular subsequence. Journal of Molecular Biology, 147:195–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields for relational learning.</title>
<date>2006</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<note>To appear.</note>
<contexts>
<context position="2008" citStr="Sutton and McCallum, 2006" startWordPosition="314" endWordPosition="318"> and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an alternative approach which does not require supervised data (Turner and Charniak, 2005). Our approach is broadly in line with prior work (Jing, 2000; Dorr et al., 2003; Riezler et al., 2003; Clarke and Lapata, 2006), in that we make use of some form of syntactic knowledge to constrain compressions we generate. What sets this work apart from them, however, is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions (Lafferty et al., 2001; Sutton and McCallum, 2006). An obvious benefit of using CRFs for sentence compression is that the model provides a general (and principled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property K&amp;M do not share. Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007). We tackle the issues by harnessing CRFs with what we might call dependency trunc</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Charles Sutton and Andrew McCallum. 2006. An introduction to conditional random fields for relational learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning,</booktitle>
<location>Banff, Canada.</location>
<contexts>
<context position="19927" citStr="Sutton et al., 2004" startWordPosition="3466" endWordPosition="3469">ref. 4 significant overlap w. ref. 5 perfect overlap w. ref. might look like, we manually searched the news site for a full-length article that might reasonably be considered a long version of that brief. We extracted lead sentences both from the brief and from its source article, and aligned them, using what is known as the Smith-Waterman algorithm (Smith and Waterman, 1981), which produced 1,401 pairs of summary and source sentence.10 For the ease of reference, we call the corpus so produced ‘NICOM’ for the rest of the paper. A part of our system makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006). Throughout the experiments, we call our approach ‘Generic Sentence Trimmer’ or GST. 6 Results and Discussion We ran DPM and GST on NICOM in the 10-fold cross validation format where we break the data into 10 blocks, use 9 of them for training and test on the remaining block. In addition, we ran the test at three different compression rates, 50%, 60% and 70%, to learn how they affect the way the models perform. This means that for each input sentence in NICOM, we have three versions of its compression created, corresponding to a particular rate at which the sentence is compress</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic labeling and segmenting sequence data. In Proceedings of the 21st International Conference on Machine Learning, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
</authors>
<title>GRMM: A graphical models toolkit.</title>
<date>2006</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="19942" citStr="Sutton, 2006" startWordPosition="3470" endWordPosition="3471">erlap w. ref. 5 perfect overlap w. ref. might look like, we manually searched the news site for a full-length article that might reasonably be considered a long version of that brief. We extracted lead sentences both from the brief and from its source article, and aligned them, using what is known as the Smith-Waterman algorithm (Smith and Waterman, 1981), which produced 1,401 pairs of summary and source sentence.10 For the ease of reference, we call the corpus so produced ‘NICOM’ for the rest of the paper. A part of our system makes use of a modeling toolkit called GRMM (Sutton et al., 2004; Sutton, 2006). Throughout the experiments, we call our approach ‘Generic Sentence Trimmer’ or GST. 6 Results and Discussion We ran DPM and GST on NICOM in the 10-fold cross validation format where we break the data into 10 blocks, use 9 of them for training and test on the remaining block. In addition, we ran the test at three different compression rates, 50%, 60% and 70%, to learn how they affect the way the models perform. This means that for each input sentence in NICOM, we have three versions of its compression created, corresponding to a particular rate at which the sentence is compressed. We call a s</context>
</contexts>
<marker>Sutton, 2006</marker>
<rawString>Charles Sutton. 2006. GRMM: A graphical models toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Tasker</author>
</authors>
<title>Learning Structured Prediction Models: A Large Margin Approach.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="5421" citStr="Tasker, 2004" startWordPosition="882" endWordPosition="883">oes not require as involved formulation as Clarke and Lapata (2006) do. We approach the problem extentionally, i.e., through generating sentences that are grammatical, or that conform to whatever constraints there are. 1Assume as usual that CRFs take the form, p(y|x) a (/moexp k,j Ajfj(yk, yk−1, x) + /moi pigi(xk, yk, x) = exp[w⊤f(x, y)] (1) fj and gi are ‘features’ associated with edges and vertices, respectively, and k E C, where C denotes a set of cliques in CRFs. Aj and pi are the weights for corresponding features. w and f are vector representations of weights and features, respectively (Tasker, 2004). 2Note that a sentence compression can be represented as an array of binary labels, one of them marking words to be retained in compression and the other those to be dropped. whose grammatically legitimate compressions would include: (4) (a) John -ga takai kuruma -wo kat-ta. ‘John bought an expensive car.’ (b) John -ga kuruma -wo kat-ta. ‘John bought a car.’ (c) Mushoku-no John -ga kuruma -wo kat-ta. ‘John, who is unemployed, bought a car. (d) John -ga kat-ta. ‘John bought.’ (e) Mushoku-no John -ga kat-ta. ‘John, who is unemployed, bought.’ (f) Takai kuruma-wo kat-ta. ‘ Bought an expensive ca</context>
</contexts>
<marker>Tasker, 2004</marker>
<rawString>Ben Tasker. 2004. Learning Structured Prediction Models: A Large Margin Approach. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>A fast finitestate relaxation method for enforcing global constraint on sequence decoding.</title>
<date>2006</date>
<booktitle>In Proceeings of the NAACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="13744" citStr="Tromble and Eisner, 2006" startWordPosition="2381" endWordPosition="2384">om the taxonomy that a Japanese tokenizer called JUMAN and KNP,6 a Japanese dependency parser (aka Kurohashi-Nagao Parser), make use of in characterizing the output they produce: both JUMAN and KNP are part of the compression model we build. Features come in three varieties: semantic, morphological and syntactic. Semantic features are used for classifying entities into semantic types such as name of person, organization, or place, while syntactic features characterize the kinds of dependency 5It is worth noting that the present approach can be recast into one based on ‘constraint relaxation’ (Tromble and Eisner, 2006). 6http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.html relations that hold among BSs such as whether a BS is of the type that combines with the verb (renyou), or of the type that combines with the noun (rentai), etc. A morphological feature could be thought of as something that broadly corresponds to an English POS, marking for some syntactic or morphological category such as noun, verb, numeral, etc. Also we included ngram features to encode the lexical context in which a given morpheme appears. Thus we might have something like: for some words (morphemes) w1, w2, and w3, fw1·w2(w3) = 1 if w</context>
</contexts>
<marker>Tromble, Eisner, 2006</marker>
<rawString>Roy W. Tromble and Jason Eisner. 2006. A fast finitestate relaxation method for enforcing global constraint on sequence decoding. In Proceeings of the NAACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenie Turner</author>
<author>Eugen Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>290--297</pages>
<location>Ann Arbor,</location>
<contexts>
<context position="909" citStr="Turner and Charniak, 2005" startWordPosition="134" endWordPosition="137">odel and Conditional Random Fields (CRFs), to address improving the grammaticality of compression while retaining its relevance. Experiments found that the present approach outperforms in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007) − the only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison. 1 Introduction For better or worse, much of prior work on sentence compression (Riezler et al., 2003; McDonald, 2006; Turner and Charniak, 2005) turned to a single corpus developed by Knight and Marcu (2002) (K&amp;M, henceforth) for evaluating their approaches. The K&amp;M corpus is a moderately sized corpus consisting of 1,087 pairs of sentence and compression, which account for about 2% of a Ziff-Davis collection from which it was derived. Despite its limited scale, prior work in sentence compression relied heavily on this particular corpus for establishing results (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2006; Galley and McKeown, 2007). It was not until recently that researchers started to turn attention to an altern</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenie Turner and Eugen Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd Annual Meeting of the ACL, pages 290–297, Ann Arbor, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Vandeghinste</author>
<author>Yi Pan</author>
</authors>
<title>Sentence compression for automatic subtitling: A hybrid approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL workshop on Text Summarization,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="11049" citStr="Vandeghinste and Pan (2004)" startWordPosition="1878" endWordPosition="1881">ficial was quoted yesterday as accusing Iran of supplying explosive technology used in attacks An official was quoted yesterday as accusing Iran of supplying explosive technology An official was quoted yesterday as accusing Iran of supplying technology Figure 5: An English dependency structure and TDPs Figure 4: Combining TDP suffixes -koto and -no. In English, we should keep a preposition from being left dangling, as in An official was quoted yesterday as accusing Iran of supplying technology used in. In any case, we need some extra rules on G(5) to take care of language specific issues (cf. Vandeghinste and Pan (2004) for English). An important point about the dependency truncation is that for most of the time, a compression it generates comes out reasonably grammatical, so the number of ‘extras’ should be small. Finally, in order for CRFs to work with the compressions, we need to translate them into a sequence of binary labels, which involves labeling an element token, bunsetsu or a word, with some label, e.g., 0 for ’remove’ and 1 for ‘retain,’ as in Figure 6. Consider following compressions y1 to y4 for x = 010203040506. 0i denotes a bunsetsu (BS). ‘0’ marks a BS to be removed and ‘1’ that to be retaine</context>
</contexts>
<marker>Vandeghinste, Pan, 2004</marker>
<rawString>Vincent Vandeghinste and Yi Pan. 2004. Sentence compression for automatic subtitling: A hybrid approach. In Proceedings of the ACL workshop on Text Summarization, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiwamu Yamagata</author>
<author>Satoshi Fukutomi</author>
<author>Kazuyuki Takagi</author>
<author>Kzauhiko Ozeki</author>
</authors>
<title>Sentence compression using statistical information about dependency path length.</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,</journal>
<booktitle>In Proceedings of TSD</booktitle>
<volume>4188</volume>
<pages>127--134</pages>
<location>Brno, Czech Republic.</location>
<contexts>
<context position="15164" citStr="Yamagata et al., 2006" startWordPosition="2623" endWordPosition="2626"> The motivation for the feature is obviously to identify concepts relevant to, or unique to the associated article. Also included was a feature on tfidf, to mark words that are conceptually more important than others. The number of features came to around 80,000 for the corpus we used in the experiment. 4 The Dependency Path Model In what follows, we will describe somewhat in detail a prior approach to sentence compression in Japanese which we call the ”dependency path model,” or DPM. DPM was first introduced in (Oguro et al., 2000), later explored by a number of people (Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007).7 DPM has the form: h(y) = αf(y) + (1 − α)g(y), (6) where y = Q0, Q1, ... , Qn−1, i.e., a compression consisting of any number of bunsetsu’s, or phraselike elements. f(�) measures the relevance of content in y; and g(�) the fluency of text. α is to provide a way of weighing up contributions from each component. We further define: q(Qi), (7) 7Kikuchi et al. (2003) explore an approach similar to DPM. n−1∑ i=0 f(y) = 303 represent bunsestu’s that the edge spans. Cs(β) denotes the class of a bunsetsu where the edge starts and Ce(β) that of a bunsetsu where the edge ends. W</context>
</contexts>
<marker>Yamagata, Fukutomi, Takagi, Ozeki, 2006</marker>
<rawString>Kiwamu Yamagata, Satoshi Fukutomi, Kazuyuki Takagi, and Kzauhiko Ozeki. 2006. Sentence compression using statistical information about dependency path length. In Proceedings of TSD 2006 (Lecture Notes in Computer Science, Vol. 4188/2006), pages 127–134, Brno, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>