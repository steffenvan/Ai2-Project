<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.99341">
Personalized, Interactive Question Answering on the Web
</title>
<author confidence="0.996284">
Silvia Quarteroni
</author>
<affiliation confidence="0.997287">
University of Trento
</affiliation>
<address confidence="0.8892995">
Via Sommarive 14
38100 Povo (TN), Italy
</address>
<email confidence="0.994193">
silviaq@disi.unitn.it
</email>
<sectionHeader confidence="0.993736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978825">
Two of the current issues of Question Answer-
ing (QA) systems are the lack of personaliza-
tion to the individual users’ needs, and the lack
of interactivity by which at the end of each Q/A
session the context of interaction is lost.
We address these issues by designing and im-
plementing a model of personalized, interac-
tive QA based on a User Modelling component
and on a conversational interface. Our eval-
uation with respect to a baseline QA system
yields encouraging results in both personaliza-
tion and interactivity.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999697961538461">
Information overload, i.e. the presence of an exces-
sive amount of data from which to search for relevant
information, is a common problem to Information Re-
trieval (IR) and its subdiscipline of Question Answering
(QA), that aims at finding concise answers to questions
in natural language. In Web-based QA in particular, this
problem affects the relevance of results with respect to
the users’ needs, as queries can be ambiguous and even
answers extracted from documents with relevant con-
tent but expressed in a difficult language may be ill-
received by users.
While the need for user personalization has been ad-
dressed by the IR community for a long time (Belkin
and Croft, 1992), very little effort has been carried out
up to now in the QA community in this direction. In-
deed, personalized Question Answering has been ad-
vocated in TREC-QA starting from 2003 (Voorhees,
2003); however, the issue was solved rather expedi-
tiously by designing a scenario where an “average news
reader” was imagined to submit the 2003 task’s defini-
tion questions.
Moreover, a commonly observed behavior in users
of IR systems is that they often issue queries not as
standalone questions but in the context of a wider in-
formation need, for instance when researching a spe-
cific topic. Recently, a new research direction has
</bodyText>
<footnote confidence="0.9030795">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.9992654375">
been proposed, which involves the integration of QA
systems with dialogue interfaces in order to encour-
age and accommodate the submission of multiple re-
lated questions and handle the user’s requests for clar-
ification in a less artificial setting (Maybury, 2002);
however, Interactive QA (IQA) systems are still at an
early stage or applied to closed domains (Small et al.,
2003; Kato et al., 2006). Also, the “complex, inter-
active QA” TREC track (www.umiacs.umd.edu/
˜jimmylin/ciqa/) has been organized, but here
the interactive aspect refers to the evaluators being en-
abled to interact with the systems rather than to dia-
logue per se.
In this paper, we first present an adaptation of User
Modelling (Kobsa, 2001) to the design of personalized
QA, and secondly we design and implement an inter-
active open-domain QA system, YourQA. Section 2
briefly introduces the baseline architecture of YourQA.
In Section 3, we show how a model of the user’s read-
ing abilities and personal interests can be used to effi-
ciently improve the quality of the information returned
by a QA system. We provide an extensive evaluation
methodology to assess such efficiency by improving on
our previous work in this area (Quarteroni and Manand-
har, 2007b).
Moreover, we discuss our design of interactive QA
in Section 4 and conduct a more rigorous evaluation of
the interactive version of YourQA by comparing it to
the baseline version on a set of TREC-QA questions,
obtaining encouraging results. Finally, a unified model
of personalized, interactive QA is described in Section
5.
</bodyText>
<sectionHeader confidence="0.98428" genericHeader="method">
2 Baseline System Architecture
</sectionHeader>
<bodyText confidence="0.998969">
The baseline version of our system, YourQA, is able to
extract answers to both factoid and non-factoid ques-
tions from the Web. As most QA systems (Kwok et al.,
2001), it is organized according to three phases:
</bodyText>
<listItem confidence="0.992396333333333">
• Question Processing: The query is classified and
the two top expected answer types are estimated; it
is then submitted to the underlying search engine;
• Document Retrieval: The top n documents are
retrieved from the search engine (Google, www.
google.com) and split into sentences;
</listItem>
<page confidence="0.994927">
33
</page>
<note confidence="0.7446855">
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 33–40
Manchester, August 2008
</note>
<listItem confidence="0.992763333333333">
• Answer Extraction:
1. A sentence-level similarity metric combining
lexical, syntactic and semantic criteria is ap-
plied to the query and to each retrieved doc-
ument sentence to identify candidate answer
sentences;
2. Candidate answers are ordered by relevance
to the query; the Google rank of the answer
source document is used as a tie-breaking cri-
terion.
3. The list of top ranked answers is then re-
turned to the user in an HTML page.
</listItem>
<bodyText confidence="0.999845142857143">
Note that our answers are in the form of sentences with
relevant words or phrases highlighted (as visible in Fig-
ure 2) and surrounded by their original passage. This
is for two reasons: we believe that providing a con-
text to the exact answer is important and we have been
mostly focusing on non-factoids, such as definitions,
which it makes sense to provide in the form of a sen-
tence. A thorough evaluation of YourQA is reported in
e.g. (Moschitti et al., 2007); it shows an F1 of 48±.7
for non-factoids on Web data, further improved by a
SVM-based re-ranker.
In the following sections, we describe how the base-
line architecture is enhanced to accommodate personal-
ization and interactivity.
</bodyText>
<sectionHeader confidence="0.983733" genericHeader="method">
3 User Modelling for Personalization
</sectionHeader>
<bodyText confidence="0.999636666666667">
Our model of personalization is centered on a User
Model which represents students searching for informa-
tion on the Web according to three attributes:
</bodyText>
<listItem confidence="0.97987825">
1. age range a E {7 − 10,11 − 16, adult},
2. reading level r E {basic, medium, advanced};
3. profile p, a set of textual documents, bookmarks
and Web pages of interest.
</listItem>
<bodyText confidence="0.999264">
Users’ age1 and browsing history are typical UM
components in news recommender systems (Magnini
and Strapparava, 2001); personalized search systems
such as (Teevan et al., 2005) also construct UMs based
on the user’s documents and Web pages of interest.
</bodyText>
<subsectionHeader confidence="0.999768">
3.1 Reading Level Estimation
</subsectionHeader>
<bodyText confidence="0.999754285714286">
We approach reading level estimation as a supervised
learning task, where representative documents for each
of the three UM reading levels are collected to be la-
belled training instances and used to classify previously
unseen documents.
Our training instances consist of about 180 HTML
documents from a collection of Web portals2 where
</bodyText>
<footnote confidence="0.9992316">
1Although the reading level can be modelled separately
from the age range, for simplicity we here assume that these
are paired in a reading level component.
2Such Web portals include: bbc.co.uk/schools,
www.think-energy.com,kids.msfc.nasa.gov.
</footnote>
<bodyText confidence="0.997514105263158">
pages are explicitly annotated by the publishers ac-
cording to the three reading levels above. As a learn-
ing model, we use unigram language modelling in-
troduced in (Collins-Thompson and Callan, 2004) to
model the reading level of subjects in primary and sec-
ondary school.
Given a set of documents, a unigram language model
represents such a set as the vector of all the words ap-
pearing in the component documents associated with
their corresponding probabilities of occurrence within
the set.
In the test phase of the learning process, for each un-
classified document D, a unigram language model is
built (as done for the training documents). The esti-
mated reading level of D is the language model lmi
maximizing the likelihood that D has been generated
by lmi (In our case, three language models lmi are de-
fined, where i E {basic, medium, advanced}.). Such
likelihood is estimated using the function:
</bodyText>
<equation confidence="0.9994545">
L(lmi|D) = � C(w, D) · log[P(w|lmi)], (1)
w∈D
</equation>
<bodyText confidence="0.9998925">
where w is a word in the document, C(w, d) represents
the number of occurrences of w in D and P(w|lmi) is
the probability that w occurs in lmi (approximated by
its frequency).
</bodyText>
<subsectionHeader confidence="0.999658">
3.2 Profile Estimation
</subsectionHeader>
<bodyText confidence="0.999940761904762">
Information extraction from the user’s documents as a
means of representation of the user’s interests, such as
his/her desktop files, is a well-established technique for
personalized IR (Teevan et al., 2005).
Profile estimation in YourQA is based on key-phrase
extraction, a technique previously employed in several
natural language tasks (Frank et al., 1999).
For this purpose, we use Kea (Witten et al., 1999),
which splits documents into phrases and chooses some
of the phrases as be key-phrases based on two criteria:
the first index of their occurrence in the source doc-
uments and their TF x IDF score3 with respect to
the current document collection. Kea outputs for each
document in the set a ranked list where the candidate
key-phrases are in decreasing order; after experiment-
ing with several values, we chose to use the top 6 as
key-phrases for each document.
The profile resulting from the extracted key-phrases
is the base for all the subsequent QA activity: any ques-
tion the user submits to the QA system is answered by
taking such profile into account, as illustrated below.
</bodyText>
<subsectionHeader confidence="0.998718">
3.3 Personalized QA Algorithm
</subsectionHeader>
<bodyText confidence="0.99941125">
The interaction between the UM component and the
core QA component modifies the standard QA process
at the Answer Extraction phase, which is modified as
follows:
</bodyText>
<footnote confidence="0.847193666666667">
3The TF x IDF of a term t in document D within a col-
lection S is: TF xIDF(t, D, S) = P(t E D)x−logP(t E
[S/D]).
</footnote>
<page confidence="0.996047">
34
</page>
<listItem confidence="0.998176722222222">
1. The retrieved documents’ reading levels are esti-
mated;
2. Documents having a different reading level from
the user are discarded; if the remaining documents
are insufficient, part of the incompatible docu-
ments having a close reading level are kept;
3. From the documents remaining from step 2, key-
phrases are extracted using Kea;
4. The remaining documents are split into sentences;
5. Document topics are matched with the topics in
the UM that represent the user’s interests;
6. Candidate answers are extracted from the docu-
ments and ordered by relevance to the query;
7. As an additional answer relevance criterion, the
degree of match between the candidate answer
document topics and the user’s topics of interest is
used and a new ranking is computed on the initial
list of candidate answers.
</listItem>
<bodyText confidence="0.992623227272727">
Step 7 deserves some deeper explanation. For each
document composing the UM profile and the retrieved
document set, a ranked list of key-phrases is available
from the previous steps. Both key-phrase sets are rep-
resented by YourQA as arrays, where each row corre-
sponds to one document and each column corresponds
to the rank within such document of the key-phrase in
the corresponding cell.
As an illustrative example, a basic user profile, cre-
ated from two documents about Italian cuisine and the
movie “Ginger and Fred”, respectively, might result in
the following array:
rpizza lasagne tiramisu recipe chef egg 1
Lfred ginger film music movie review J
The arrays of UM profile and retrieved document
key-phrases are named P and Retr, respectively. We
call Retri the document represented in the i-th row in
Retr, and Pn the one represented in the n-th row of P
4. Given kij, i.e. the j-th key-phrase extracted from
Retri, and Pn, i.e. the n-th document in P, we call
w(kij, Pn) the relevance of kij with respect to Pn. We
define
</bodyText>
<equation confidence="0.750156333333333">
� |Retri|_j k.. E P
w(kij, Pn) =  |Retri |,z3 n
0, otherwise
</equation>
<bodyText confidence="0.9942612">
where |Retri |is the number of key-phrases of Retri.
The total relevance of document Retri with respect to
P, wP(Retri), is defined as the maximal sum of the
relevance of its key-phrases, obtained for all the rows
in P:
</bodyText>
<equation confidence="0.9487185">
1] wP(Retri) = maxnEP w(kij, Pn). (3)
kijERetri
</equation>
<bodyText confidence="0.991095095238095">
4Note that, while column index reflects a ranking based
on the relevance of a key-phrase to its source document, row
index only depends on the name of such document.
The personalized answer ranking takes wP into ac-
count as a secondary ranking criterion with respect
to the baseline system’s similarity score; as before,
Google rank of the source document is used as further
a tie-breaking criterion.
Notice that our approach to User Modelling can be
seen as a form of implicit (or quasi-implicit) relevance
feedback, i.e. feedback not explicitly obtained from the
user but inferred from latent information in the user’s
documents. Indeed, we take inspiration from (Teevan
et al., 2005)’s approach to personalized search, comput-
ing the relevance of unseen documents (such as those
retrieved for a query) as a function of the presence and
frequency of the same terms in a second set of docu-
ments on whose relevance the user has provided feed-
back.
Our approaches to personalization are evaluated in
Section 3.4.
</bodyText>
<subsectionHeader confidence="0.990626">
3.4 Evaluating Personalization
</subsectionHeader>
<bodyText confidence="0.999992333333333">
The evaluation of our personalized QA algorithms as-
sessed the contributions of the reading level attribute
and of the profile attribute of the User Model.
</bodyText>
<subsectionHeader confidence="0.968899">
3.4.1 Reading Level Evaluation
</subsectionHeader>
<bodyText confidence="0.999996807692308">
Reading level estimation was evaluated by first as-
sessing the robustness of the unigram language models
by running 10-fold cross-validation on the set of doc-
uments used to create such models, and averaging the
ratio of correctly classified documents with respect to
the total number of documents for each fold. Our re-
sults gave a very high accuracy, i.e. 94.23% f 1.98
standard deviation.
However, this does not prove a direct effect on the
user’s perception of such levels. For this purpose, we
defined Reading level agreement (Ar) as the percentage
of documents rated by the users as suitable to the read-
ing level to which they were assigned. We performed
a second experiment with 20 subjects aged between 16
and 52 and with a self-assessed good or medium En-
glish reading level. They evaluated the answers re-
turned by the system to 24 questions into 3 groups (ba-
sic, medium and advanced reading levels), by assessing
whether they agreed that the given answer was assigned
to the correct reading level.
Our results show that altogether, evaluators found an-
swers appropriate for the reading levels to which they
were assigned. The agreement decreased from 94% for
Aadv to 85% for Amed to 72% for Abas; this was pre-
dictable as it is more constraining to conform to a lower
reading level than to a higher one.
</bodyText>
<subsectionHeader confidence="0.58232">
3.4.2 Profile Evaluation
</subsectionHeader>
<bodyText confidence="0.9995635">
The impact of the UM profile was tested by us-
ing as a baseline the standard version of YourQA,
where the UM component is inactive. Ten adult par-
ticipants from various backgrounds took part in the
experiment; they were invited to form an individual
profile by brainstorming key-phrases for 2-3 topics of
</bodyText>
<figure confidence="0.325562">
(2)
</figure>
<page confidence="0.989403">
35
</page>
<bodyText confidence="0.995246705882353">
their interest chosen from the Yahoo! directory (dir.
yahoo.com): examples were “ballet”, “RPGs” and
“dog health”.
For each user, we created the following 3 questions
so that he/she would submit them to the QA system:
Qper, related to the user’s profile, for answering which
the personalized version of YourQA would be used;
Qbas, related to the user’s profile, for which the base-
line version of the system would be used; and Qunr,
unrelated to the user’s profile, hence not affected by
personalization. The reason why we handcrafted ques-
tions rather than letting users spontaneously interact
with YourQA’s two versions is that we wanted the re-
sults of the two versions to be different in order to mea-
sure a preference. After examining the top 5 results to
each question, users had to answer the following ques-
tionnaire5:
</bodyText>
<listItem confidence="0.835747">
• For each of the five results separately:
</listItem>
<figure confidence="0.798183">
TEST1: This result is useful to me:
5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)
Not at all
TEST2: This result is related to my profile:
5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)
Not at all
• For the five results taken as a whole:
TEST3: Finding the info I wanted in the result page
took:
1) Too long, 2) Quite long, 3) Not too long, 4)
Quite little, 5) Very little
TEST4: For this query, the system results were sensi-
tive to my profile:
</figure>
<figureCaption confidence="0.420862">
5) Yes, 4) Mostly yes, 3) Maybe, 2) Mostly not, 1)
Not at all
</figureCaption>
<tableCaption confidence="0.987558">
The experiment results are summarized in Table 1. The
Table 1: Profile evaluation results (avg ± st. dev.)
</tableCaption>
<table confidence="0.9995506">
Measurement Qrel Qbas Qunr
TEST1 3.6 ±0.4 2.3 ±0.3 3.3 ±0.3
TEST2 4.0 ±0.5 2.2 ±0.3 1.7 ±0.1
TEST3 3.1 ±1.1 2.7 ±1.3 3.4 ±1.4
TEST4 3.9 ±0.7 2.5 ±1.1 1.8 ±1.2
</table>
<bodyText confidence="0.9992798">
first row reports a remarkable difference between the
perceived usefulness for question Qrel with respect to
question Qbas (answers to TEST1).
The results were compared by carrying out a one-
way analysis of variance (ANOVA) and performing the
Fischer test using the usefulness as factor (with the
5The adoption of a Likert scale made it possible to com-
pute the average and standard deviations of the user comments
with respect to each answer among the top five returned by the
system. It was therefore possible to replace the binary mea-
surement of perceived usefulness, relatedness and sensitivity
used in (Quarteroni and Manandhar, 2007b) in terms of to-
tal number of users with a more fine-grained one in terms of
average computed over the users.
three queries as levels) at a 95% level of confidence.
The test revealed an overall significant difference be-
tween factors, confirming that users are positively bi-
ased towards questions related to their own profile when
it comes to perceived utility.
To analyze the answers to TEST2 (Table 1, row 2),
which measured the perceived relatedness of each an-
swer to the current profile, we used ANOVA again and
and obtained an overall significant difference. Hence,
answers obtained without using the users’ profile were
perceived as significantly less related to those obtained
using their own profile, i.e. there is a significant differ-
ence between Qrel and Qbas. As expected, the differ-
ence between Qrel and Qunr is even more significant.
Thirdly, the ANOVA table computed using average
perceived time (TEST3) as variable and the three ques-
tions as factors did not give any significance, nor did
any of the paired t-tests computed over each result pair.
We concluded that apparently, the time spent browsing
results is not directly correlated to the personalization
of results.
Finally, the average sensitivity of the five answers al-
together (TEST4) computed over the ten participants
for each query shows an overall significant difference in
perceived sensitivity between the answers to question
Qrel (3.9±0.7) and those to question Qbas (2.5±1.1)
and Qunr (1.8±1.2).
To conclude, our experience with profile evaluation
shows that personalized QA techniques yield answers
that are indeed perceived as more satisfying to users in
terms of usefulness and relatedness to their own profile.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="method">
4 Interactivity
</sectionHeader>
<bodyText confidence="0.9927348">
Making a QA system interactive implies maintaining
and efficiently using the current dialogue context and
the ability to converse with the user in a natural manner.
Our implementation of IQA is guided by the following
conversation scenario:
</bodyText>
<listItem confidence="0.9418">
1. An optional reciprocal greeting, followed by a
question q from the user;
2. q is analyzed to detect whether it is related to pre-
vious questions or not;
3. (a) If q is unrelated to the preceding questions, it
is submitted to the QA component;
(b) If q is related to the preceding questions
</listItem>
<bodyText confidence="0.97720675">
(follow-up question), it is interpreted by the
system in the context of previous queries;
a revised version of q, q’, is either directly
submitted to the QA component or a request
for confirmation (grounding) is issued to the
user; if he/she does not agree, the system asks
the user to reformulate the question until it
can be interpreted by the QA component;
</bodyText>
<listItem confidence="0.9511545">
4. As soon as the QA component results are avail-
able, an answer a is provided;
</listItem>
<page confidence="0.881549">
36
</page>
<listItem confidence="0.938802">
5. The system enquires whether the user is interested
in submitting new queries;
6. Whenever the user wants to terminate the interac-
tion, a final greeting is exchanged.
</listItem>
<subsectionHeader confidence="0.99825">
4.1 Choosing a Dialogue Manager
</subsectionHeader>
<bodyText confidence="0.999947419354838">
Among traditional methods for implementing
information-seeking dialogue management, Finite-
State (FS) approaches are the simplest. Here, the
dialogue manager is represented as a Finite-State
machine, where each state models a separate phase
of the conversation, and each dialogue move encodes
a transition to a subsequent state (Sutton, 1998).
However, an issue with FS models is that they allow
very limited freedom in the range of user utterances:
since each dialogue move must be pre-encoded in the
models, there is a scalability issue when addressing
open domain dialogue.
On the other hand, we believe that other dialogue ap-
proaches such as the Information State (IS) (Larsson et
al., 2000) are primarily suited to applications requiring
a planning component such as closed-domain dialogue
systems and to a lesser extent to open-domain QA.
As an alternative approach, we studied conversa-
tional agents (“chatbots”) based on AIML (Artificial
Intelligence Markup Language), such as ALICE6. Chat-
bots are based on the pattern matching technique, which
consists in matching the last user utterance against a
range of dialogue patterns known to the system. A co-
herent answer is created by following a range of “tem-
plate” responses associated with such patterns.
As its primary application is small-talk, chatbot di-
alogue appears more natural than in FS and IS sys-
tems. Moreover, since chatbots support a limited no-
tion of context, they can handle follow-up recognition
and other dialogue phenomena not easily covered using
standard FS models.
</bodyText>
<subsectionHeader confidence="0.987185">
4.2 A Wizard-of-Oz Experiment
</subsectionHeader>
<bodyText confidence="0.9999751875">
To assess the utility of a chatbot-based dialogue man-
ager in an open-domain QA application, we conducted
an exploratory Wizard of Oz experiment.
Wizard-of-Oz (WOz) experiments are usually de-
ployed for natural language systems to obtain initial
data when a full-fledged prototype is not yet available
(Dahlbaeck et al., 1993) and consist in “hiding” a hu-
man operator behind a computer interface to simulate a
conversation with the user, who believes to be interact-
ing with a fully automated prototype.
We designed six tasks reflecting the intended typical
usage of the system (e.g.: “Find out who painted Guer-
nica and ask the system for more information about the
artist”) to be carried out by 7 users by interacting with
an instant messaging platform, which they were told to
be the system interface.
</bodyText>
<footnote confidence="0.843323">
6www.alicebot.org/
</footnote>
<bodyText confidence="0.999959428571428">
The role of the Wizard was to simulate a limited
range of utterances and conversational situations han-
dled by a chatbot.
User feedback was collected mainly by using a
post-hoc questionnaire inspired by the experiment in
(Munteanu and Boldea, 2000), which consists of ques-
tions Q1 to Q6 in Table 2, col. 1, to be answered using
a scale from 1=“Not at all” to 5=“Yes, absolutely”.
From the WOz results, reported in Table 2, col.
“WOz”, users appear to be generally very satisfied with
the system’s performances: Q6 obtained an average of
4.5±.5. None of the users had difficulties in reformu-
lating their questions when this was requested: Q4 ob-
tained 3.8±.5. For the remaining questions, satisfaction
levels were high: users generally thought that the sys-
tem understood their information needs (Q2 obtained 4)
and were able to obtain such information (Q1 obtained
4.3±.5).
The dialogue manager and interface of YourQA were
implemented based on the dialogue scenario and the
successful outcome of the WOz experiment.
</bodyText>
<subsectionHeader confidence="0.996657">
4.3 Dialogue Management Algorithms
</subsectionHeader>
<bodyText confidence="0.999962666666667">
As chatbot dialogue follows a pattern-matching ap-
proach, it is not constrained by a notion of “state”:
when a user utterance is issued, the chatbot’s strategy is
to look for a pattern matching it and fire the correspond-
ing template response. Our main focus of attention
in terms of dialogue manager design was therefore di-
rected to the dialogue tasks invoking external resources,
such as handling follow-up questions, and tasks involv-
ing the QA component.
</bodyText>
<subsectionHeader confidence="0.900816">
4.3.1 Handling follow-up questions
</subsectionHeader>
<bodyText confidence="0.971660363636364">
For the detection of follow-up questions, the algo-
rithm in (De Boni and Manandhar, 2005) is used, which
uses features such as the presence of pronouns and
the absence of verbs in the current question and word
repetitions with the n previous questions to determine
whether qi is a follow-up question with respect to the
current context. If the question q is not identified as a
follow-up question, it is submitted to the QA compo-
nent. Otherwise, the reference resolution strategy be-
low is applied on q, drawing on the stack S of previous
user questions:
</bodyText>
<listItem confidence="0.892364416666667">
1. If q is elliptic (i.e. contains no verbs), its keywords
are completed with the keywords extracted by the
QA component from the previous question in S
for which there exists an answer. The completed
query is submitted to the QA component;
2. If q contains pronoun/adjective anaphora, a chun-
ker is used to find the most recent compatible an-
tecedent in S. This must be a NP compatible in
number with the referent.
3. If q contains NP anaphora, the first NP in S con-
taining all the words in the referent is used to re-
place the latter in q. When no antecedent can be
</listItem>
<page confidence="0.998805">
37
</page>
<bodyText confidence="0.999842428571429">
found, a clarification request is issued by the sys-
tem until a resolved query can be submitted to the
QA component.
When the QA process is terminated, a message direct-
ing the user to the HTML answer frame (see Figure 1) is
returned and a follow-up proposal or an enquiry about
user satisfaction is optionally issued.
</bodyText>
<subsectionHeader confidence="0.996823">
4.4 Implementation
</subsectionHeader>
<bodyText confidence="0.980832964285714">
To implement the dialogue manager and allow a seam-
less integration with our Java-based QA system, we ex-
tended the Java-based AIML interpreter Chatterbean7.
We started by augmenting the default AIML tag set
(including tags such as &lt;srai&gt; and &lt;that&gt;) with
two tags: &lt;query&gt;, to seamlessly invoke the core QA
module, and &lt;clarify&gt;, to support follow-up detec-
tion and resolution.
Moreover, the interpreter allows to instantiate and
update a set of variables, represented as context prop-
erties. Among others, we defined:
a) userID, which is matched against a list of known
user IDs to select a UM profile for answer extraction
(see Section 5);
b) the current query, which is used to dynamically up-
date the stack of recent user questions used by the clar-
ification request detection module to perform reference
resolution;
c) the topic of conversation, i.e. the keywords of the
last question issued by the user which received an an-
swer. The latter is used to clarify elliptic questions, by
augmenting the current query keywords with those in
the topic when ellipsis is detected.
Figure 1 illustrates YourQA’s interactive version,
which is accessible from the Web. As in a normal chat
application, users write in a text field and the current
session history as well as the interlocutor replies are vi-
sualized in a text area.
</bodyText>
<subsectionHeader confidence="0.952984">
4.5 Interactive QA evaluation
</subsectionHeader>
<bodyText confidence="0.999917411764706">
For the evaluation of interactivity, we built on our pre-
vious results from a Wizard-of-Oz experiment and an
initial evaluation conducted on a limited set of hand-
crafted questions (Quarteroni and Manandhar, 2007a).
We chose 9 question series from the TREC-QA 2007
campaign8. Three questions were retained per series to
make each evaluation balanced. For instance, the three
following questions were used to form one task: 266.1:
“When was Rafik Hariri born?”, 266.2: “To what reli-
gion did he belong (including sect)?” and 266.4: “At
what time in the day was he assassinated?”.
Twelve users were invited to find answers to the
questions to one of them by using the standard version
of the system and to the second by using the interactive
version. Each series was evaluated at least once using
both versions of the system. At the end of the exper-
iment, users had to give feedback about both versions
</bodyText>
<footnote confidence="0.701554">
7chatterbean.bitoflife.cjb.net.
8trec.nist.gov
</footnote>
<tableCaption confidence="0.734148666666667">
Table 2: Interactive QA evaluation results obtained for
the WOz, Standard and Interactive versions of YourQA.
Average ± st. dev. are reported.
</tableCaption>
<table confidence="0.645371153846154">
Question
Q1 Did you get all the in-
formation you wanted
using YourQA?
Q2 Do you think YourQA
understood what you
asked?
Q3 How easy was it to
obtain the information
you wanted?
Q4 Was it difficult to re-
formulate your ques-
tions when requested?
Q5 Do you think you
would use YourQA
again?
Q6 Overall, are you satis-
fied with YourQA?
Q7 Was the pace of inter-
action with YourQA
appropriate?
Q8 How often was
YourQA sluggish in
replying to you?
Q9 Which interface did
you prefer?
</table>
<bodyText confidence="0.999632172413793">
of the system by filling in the satisfaction questionnaire
reported in Table 2.
Although the paired t-test conducted to compare
questionnaire replies to the standard and interactive ver-
sions did not register statistical significance, we believe
that the evidence we collected suggests a few interest-
ing interpretations.
First, a good overall satisfaction appears with both
versions of the system (Qs), with a slight difference in
favor of the interactive version. The two versions of
the system seem to offer different advantages: while
the ease of use of the standard version was rated higher
(Q3), probably because the system’s reformulation re-
quests added a challenge to users used to search engine
interaction, users felt they obtained more information
using the interactive version (Q1).
Concerning interaction comfort, users seemed to feel
that the interactive version understood better their re-
quests than the standard one (Q2); they also found it
easy to reformulate questions when the former asked
to (Qs). However, while the pace of interaction was
judged slightly more appropriate in the interactive case
(Q7), interaction was considered faster when using the
standard version (Q4). This partly explains the fact that
users seemed more ready to use again the standard ver-
sion of the system (QS).
7 out of 12 users (58.3%) answered the “preference”
question Q9 by saying that they preferred the inter-
active version. The reasons given by users in their
</bodyText>
<table confidence="0.9570293">
WOz Stand Interact
4.3±.5 4.1±1 4.3±.7
4.0 3.4±1.3 3.8±1.1
4.0±.8 3.9±1.1 3.7±1
3.8±.5 - 3.9±.6
4.1±.6 3.3±1.6 3.1±1.4
4.5±.5 3.7±1.2 3.8±1.2
- 3.2±1.2 3.3±1.2
- 2.7±1.1 2.5±1.2
- 41.7% 58.3%
</table>
<page confidence="0.99389">
38
</page>
<figureCaption confidence="0.999859">
Figure 1: YourQA’s interactive interface
</figureCaption>
<bodyText confidence="0.999994857142857">
comments were mixed: while some of them were en-
thusiastic about the chatbot’s small-talk features, oth-
ers clearly said that they felt more comfortable with a
search engine-like interface. Most of the critical aspects
emerging from our overall satisfactory evaluation de-
pend on the specific system we have tested rather than
on the nature of interactive QA, to which none of such
results appear to be detrimental.
We believe that the search-engine-style use and in-
terpretation of QA systems are due to the fact that QA
is still a very little known technology. It is a challenge
for both developers and the larger public to cooperate
in designing and discovering applications that take ad-
vantage of the potentials of interactivity.
</bodyText>
<sectionHeader confidence="0.999171" genericHeader="method">
5 A Unified Model
</sectionHeader>
<bodyText confidence="0.999954333333333">
Our research so far has demonstrated the utility of per-
sonalization and interactivity in a QA system. It is
thus inevitable to regard the formulation of a unified
model of personalized, interactive QA as a valuable by-
product of these two technologies. In this perspective,
we propose the following dialogue scenario:
</bodyText>
<listItem confidence="0.989202944444444">
1. The user interacts with the dialogue interface for-
mulating an utterance q;
2. If q is recognized as a question, it is analyzed by
the dialogue manager (DM) to detect and resolve
multiple and follow-up questions;
3. As soon as a resolved version q&apos; of q is available,
the DM passes q&apos; to the QA module; the latter pro-
cesses q&apos; and retrieves a set Retr(q&apos;) of relevant
documents;
4. The QA module exchanges information with the
UM component which is responsible of maintain-
ing and updating the User Model of the current
user, u; Based on u, the QA module extracts a list
L(q&apos;, u) of personalized results from Retr(q&apos;);
5. The DM produces a reply r, which is returned
along with L(q&apos;, u) to the user via the dialogue in-
terface;
6. Once terminated, the current QA session is logged
</listItem>
<bodyText confidence="0.974360533333333">
into the dialogue history H(u), that will be used
to update u;
Concerning step 4, an efficient strategy for eliciting
the User Model from the user is yet to be specified at
this stage: the current one relies on the definition of
a context variable userzD in the dialogue manager,
which at the moment corresponds to the user’s name. A
number of AIML categories are created are created for
YourQA to explicitly ask for the user’s name, whihc is
then assigned to the userzD variable.
Figure 2 illustrates an example of a personalized, QA
session in YourQA where the user’s name is associated
with a basic reading level UM. This affects the docu-
ment retrieval phase, where only documents with sim-
ple words are retained for answer extraction.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999042">
In this paper, we present an efficient and light-weight
method to personalize the results of a Web-based QA
system based on a User Model representing individual
users’ reading level, age range and interests. Our results
show the efficiency of reading level estimation, and a
</bodyText>
<page confidence="0.999115">
39
</page>
<figureCaption confidence="0.8489675">
Figure 2: Screenshot from a personalized, interactive QA session. Here, the user’s name (“Kid”) is associated with
a UM requiring a basic reading level, hence the candidate answer documents are filtered accordingly.
</figureCaption>
<bodyText confidence="0.999436666666667">
significant improvement in satisfaction when filtering
answers based on the users’ profile with respect to the
baseline version of our system. Moreover, we introduce
a dialogue management model for interactive QA based
on a chat interface and evaluate it with optimistic con-
clusions.
In the future, we plan to study efficient strategies for
bootstrapping User Models based on current and past
conversations with the present user. Another problem
to be solved is updating user interests and reading lev-
els based on the dialogue history, in order to make the
system fully adaptive.
</bodyText>
<sectionHeader confidence="0.996059" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.990523333333333">
The research reported here was mainly conducted at the Com-
puter Science Department of the University of York, UK, un-
der the supervision of Suresh Manandhar.
</bodyText>
<sectionHeader confidence="0.998112" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995054357142857">
Belkin, N. J. and W.B. Croft. 1992. Information filter-
ing and information retrieval: Two sides of the same
coin? Comm. ACM, 35(12):29–38.
Collins-Thompson, K. and J. P. Callan. 2004. A lan-
guage modeling approach to predicting reading diffi-
culty. In HLT/NAACL’04.
Dahlbaeck, N., A. Jonsson, and L. Ahrenberg. 1993.
Wizard of Oz studies: why and how. In IUI ’93.
De Boni, M. and S. Manandhar. 2005. Implement-
ing clarification dialogue in open-domain question
answering. JNLE, 11.
Frank, E., G. W. Paynter, I. H. Witten, C. Gutwin,
and C. G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In IJCAI ’99.
Kato, T., J. Fukumoto, F.Masui, and N. Kando. 2006.
Woz simulation of interactive question answering. In
IQA’06.
Kobsa, A. 2001. Generic user modeling systems.
UMUAI, 11:49–63.
Kwok, C. T., O. Etzioni, and D. S. Weld. 2001. Scaling
question answering to the web. In WWW’01.
Larsson, S., P. Ljungl¨of, R. Cooper, E. Engdahl, and
S. Ericsson. 2000. GoDiS—an accommodating di-
alogue system. In ANLP/NAACL’00 WS on Conver-
sational Systems.
Magnini, B. and C. Strapparava. 2001. Improving user
modelling with content-based techniques. In UM’01.
Maybury, M. T. 2002. Towards a question answering
roadmap. Technical report, MITRE Corporation.
Moschitti, A., S. Quarteroni, R. Basili, and S. Man-
andhar. 2007. Exploiting syntactic and shallow se-
mantic kernels for question/answer classification. In
ACL’07.
Munteanu, C. and M. Boldea. 2000. Mdwoz: A wizard
of oz environment for dialog systems development.
In LREC’00.
Quarteroni, S. and S. Manandhar. 2007a. A chatbot-
based interactive question answering system. In
DECALOG’07, Rovereto, Italy.
Quarteroni, S. and S. Manandhar. 2007b. User
modelling for personalized question answering. In
AI*IA’07, Rome, Italy.
Small, S., T. Liu, N. Shimizu, and T. Strzalkowski.
2003. HITIQA: an interactive question answering
system- a preliminary report. In ACL’03 WS on Mul-
tilingual summarization and QA.
Sutton, S. 1998. Universal speech tools: the CSLU
toolkit. In ICSLP’98.
Teevan, J., S. T. Dumais, and E. Horvitz. 2005. Per-
sonalizing search via automated analysis of interests
and activities. In SIGIR ’05.
Voorhees, E. M. 2003. Overview of the TREC 2003
Question Answering Track. In TREC’03.
Witten, I. H., G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. KEA: Practical auto-
matic keyphrase extraction. In ACM DL.
</reference>
<page confidence="0.998638">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.458193">
<title confidence="0.997523">Personalized, Interactive Question Answering on the Web</title>
<author confidence="0.975379">Silvia</author>
<affiliation confidence="0.746385">University of Via Sommarive</affiliation>
<address confidence="0.998521">38100 Povo (TN), Italy</address>
<email confidence="0.998746">silviaq@disi.unitn.it</email>
<abstract confidence="0.996472538461538">Two of the current issues of Question Answering (QA) systems are the lack of personalization to the individual users’ needs, and the lack of interactivity by which at the end of each Q/A session the context of interaction is lost. We address these issues by designing and implementing a model of personalized, interactive QA based on a User Modelling component and on a conversational interface. Our evaluation with respect to a baseline QA system yields encouraging results in both personalization and interactivity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N J Belkin</author>
<author>W B Croft</author>
</authors>
<title>Information filtering and information retrieval: Two sides of the same coin?</title>
<date>1992</date>
<journal>Comm. ACM,</journal>
<volume>35</volume>
<issue>12</issue>
<contexts>
<context position="1377" citStr="Belkin and Croft, 1992" startWordPosition="220" endWordPosition="223"> amount of data from which to search for relevant information, is a common problem to Information Retrieval (IR) and its subdiscipline of Question Answering (QA), that aims at finding concise answers to questions in natural language. In Web-based QA in particular, this problem affects the relevance of results with respect to the users’ needs, as queries can be ambiguous and even answers extracted from documents with relevant content but expressed in a difficult language may be illreceived by users. While the need for user personalization has been addressed by the IR community for a long time (Belkin and Croft, 1992), very little effort has been carried out up to now in the QA community in this direction. Indeed, personalized Question Answering has been advocated in TREC-QA starting from 2003 (Voorhees, 2003); however, the issue was solved rather expeditiously by designing a scenario where an “average news reader” was imagined to submit the 2003 task’s definition questions. Moreover, a commonly observed behavior in users of IR systems is that they often issue queries not as standalone questions but in the context of a wider information need, for instance when researching a specific topic. Recently, a new </context>
</contexts>
<marker>Belkin, Croft, 1992</marker>
<rawString>Belkin, N. J. and W.B. Croft. 1992. Information filtering and information retrieval: Two sides of the same coin? Comm. ACM, 35(12):29–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J P Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In HLT/NAACL’04.</booktitle>
<contexts>
<context position="6939" citStr="Collins-Thompson and Callan, 2004" startWordPosition="1114" endWordPosition="1117">ollected to be labelled training instances and used to classify previously unseen documents. Our training instances consist of about 180 HTML documents from a collection of Web portals2 where 1Although the reading level can be modelled separately from the age range, for simplicity we here assume that these are paired in a reading level component. 2Such Web portals include: bbc.co.uk/schools, www.think-energy.com,kids.msfc.nasa.gov. pages are explicitly annotated by the publishers according to the three reading levels above. As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school. Given a set of documents, a unigram language model represents such a set as the vector of all the words appearing in the component documents associated with their corresponding probabilities of occurrence within the set. In the test phase of the learning process, for each unclassified document D, a unigram language model is built (as done for the training documents). The estimated reading level of D is the language model lmi maximizing the likelihood that D has been generated by lmi (In our case, three language models lmi</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>Collins-Thompson, K. and J. P. Callan. 2004. A language modeling approach to predicting reading difficulty. In HLT/NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dahlbaeck</author>
<author>A Jonsson</author>
<author>L Ahrenberg</author>
</authors>
<title>Wizard of Oz studies: why and how.</title>
<date>1993</date>
<booktitle>In IUI ’93.</booktitle>
<contexts>
<context position="21373" citStr="Dahlbaeck et al., 1993" startWordPosition="3540" endWordPosition="3543">ry application is small-talk, chatbot dialogue appears more natural than in FS and IS systems. Moreover, since chatbots support a limited notion of context, they can handle follow-up recognition and other dialogue phenomena not easily covered using standard FS models. 4.2 A Wizard-of-Oz Experiment To assess the utility of a chatbot-based dialogue manager in an open-domain QA application, we conducted an exploratory Wizard of Oz experiment. Wizard-of-Oz (WOz) experiments are usually deployed for natural language systems to obtain initial data when a full-fledged prototype is not yet available (Dahlbaeck et al., 1993) and consist in “hiding” a human operator behind a computer interface to simulate a conversation with the user, who believes to be interacting with a fully automated prototype. We designed six tasks reflecting the intended typical usage of the system (e.g.: “Find out who painted Guernica and ask the system for more information about the artist”) to be carried out by 7 users by interacting with an instant messaging platform, which they were told to be the system interface. 6www.alicebot.org/ The role of the Wizard was to simulate a limited range of utterances and conversational situations handl</context>
</contexts>
<marker>Dahlbaeck, Jonsson, Ahrenberg, 1993</marker>
<rawString>Dahlbaeck, N., A. Jonsson, and L. Ahrenberg. 1993. Wizard of Oz studies: why and how. In IUI ’93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M De Boni</author>
<author>S Manandhar</author>
</authors>
<title>Implementing clarification dialogue in open-domain question answering.</title>
<date>2005</date>
<journal>JNLE,</journal>
<volume>11</volume>
<marker>De Boni, Manandhar, 2005</marker>
<rawString>De Boni, M. and S. Manandhar. 2005. Implementing clarification dialogue in open-domain question answering. JNLE, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>G W Paynter</author>
<author>I H Witten</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>Domain-specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In IJCAI ’99.</booktitle>
<contexts>
<context position="8245" citStr="Frank et al., 1999" startWordPosition="1332" endWordPosition="1335"> the function: L(lmi|D) = � C(w, D) · log[P(w|lmi)], (1) w∈D where w is a word in the document, C(w, d) represents the number of occurrences of w in D and P(w|lmi) is the probability that w occurs in lmi (approximated by its frequency). 3.2 Profile Estimation Information extraction from the user’s documents as a means of representation of the user’s interests, such as his/her desktop files, is a well-established technique for personalized IR (Teevan et al., 2005). Profile estimation in YourQA is based on key-phrase extraction, a technique previously employed in several natural language tasks (Frank et al., 1999). For this purpose, we use Kea (Witten et al., 1999), which splits documents into phrases and chooses some of the phrases as be key-phrases based on two criteria: the first index of their occurrence in the source documents and their TF x IDF score3 with respect to the current document collection. Kea outputs for each document in the set a ranked list where the candidate key-phrases are in decreasing order; after experimenting with several values, we chose to use the top 6 as key-phrases for each document. The profile resulting from the extracted key-phrases is the base for all the subsequent Q</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Frank, E., G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In IJCAI ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kato</author>
<author>J Fukumoto</author>
<author>F Masui</author>
<author>N Kando</author>
</authors>
<title>Woz simulation of interactive question answering.</title>
<date>2006</date>
<booktitle>In IQA’06.</booktitle>
<contexts>
<context position="2575" citStr="Kato et al., 2006" startWordPosition="406" endWordPosition="409">ic. Recently, a new research direction has © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. been proposed, which involves the integration of QA systems with dialogue interfaces in order to encourage and accommodate the submission of multiple related questions and handle the user’s requests for clarification in a less artificial setting (Maybury, 2002); however, Interactive QA (IQA) systems are still at an early stage or applied to closed domains (Small et al., 2003; Kato et al., 2006). Also, the “complex, interactive QA” TREC track (www.umiacs.umd.edu/ ˜jimmylin/ciqa/) has been organized, but here the interactive aspect refers to the evaluators being enabled to interact with the systems rather than to dialogue per se. In this paper, we first present an adaptation of User Modelling (Kobsa, 2001) to the design of personalized QA, and secondly we design and implement an interactive open-domain QA system, YourQA. Section 2 briefly introduces the baseline architecture of YourQA. In Section 3, we show how a model of the user’s reading abilities and personal interests can be used</context>
</contexts>
<marker>Kato, Fukumoto, Masui, Kando, 2006</marker>
<rawString>Kato, T., J. Fukumoto, F.Masui, and N. Kando. 2006. Woz simulation of interactive question answering. In IQA’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kobsa</author>
</authors>
<title>Generic user modeling systems.</title>
<date>2001</date>
<pages>11--49</pages>
<publisher>UMUAI,</publisher>
<contexts>
<context position="2891" citStr="Kobsa, 2001" startWordPosition="458" endWordPosition="459">urage and accommodate the submission of multiple related questions and handle the user’s requests for clarification in a less artificial setting (Maybury, 2002); however, Interactive QA (IQA) systems are still at an early stage or applied to closed domains (Small et al., 2003; Kato et al., 2006). Also, the “complex, interactive QA” TREC track (www.umiacs.umd.edu/ ˜jimmylin/ciqa/) has been organized, but here the interactive aspect refers to the evaluators being enabled to interact with the systems rather than to dialogue per se. In this paper, we first present an adaptation of User Modelling (Kobsa, 2001) to the design of personalized QA, and secondly we design and implement an interactive open-domain QA system, YourQA. Section 2 briefly introduces the baseline architecture of YourQA. In Section 3, we show how a model of the user’s reading abilities and personal interests can be used to efficiently improve the quality of the information returned by a QA system. We provide an extensive evaluation methodology to assess such efficiency by improving on our previous work in this area (Quarteroni and Manandhar, 2007b). Moreover, we discuss our design of interactive QA in Section 4 and conduct a more</context>
</contexts>
<marker>Kobsa, 2001</marker>
<rawString>Kobsa, A. 2001. Generic user modeling systems. UMUAI, 11:49–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Kwok</author>
<author>O Etzioni</author>
<author>D S Weld</author>
</authors>
<title>Scaling question answering to the web. In WWW’01.</title>
<date>2001</date>
<contexts>
<context position="3931" citStr="Kwok et al., 2001" startWordPosition="628" endWordPosition="631">uch efficiency by improving on our previous work in this area (Quarteroni and Manandhar, 2007b). Moreover, we discuss our design of interactive QA in Section 4 and conduct a more rigorous evaluation of the interactive version of YourQA by comparing it to the baseline version on a set of TREC-QA questions, obtaining encouraging results. Finally, a unified model of personalized, interactive QA is described in Section 5. 2 Baseline System Architecture The baseline version of our system, YourQA, is able to extract answers to both factoid and non-factoid questions from the Web. As most QA systems (Kwok et al., 2001), it is organized according to three phases: • Question Processing: The query is classified and the two top expected answer types are estimated; it is then submitted to the underlying search engine; • Document Retrieval: The top n documents are retrieved from the search engine (Google, www. google.com) and split into sentences; 33 Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 33–40 Manchester, August 2008 • Answer Extraction: 1. A sentence-level similarity metric combining lexical, syntactic and semantic criteria is applied to the query and </context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>Kwok, C. T., O. Etzioni, and D. S. Weld. 2001. Scaling question answering to the web. In WWW’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
<author>P Ljungl¨of</author>
<author>R Cooper</author>
<author>E Engdahl</author>
<author>S Ericsson</author>
</authors>
<title>GoDiS—an accommodating dialogue system.</title>
<date>2000</date>
<booktitle>In ANLP/NAACL’00 WS on Conversational Systems.</booktitle>
<marker>Larsson, Ljungl¨of, Cooper, Engdahl, Ericsson, 2000</marker>
<rawString>Larsson, S., P. Ljungl¨of, R. Cooper, E. Engdahl, and S. Ericsson. 2000. GoDiS—an accommodating dialogue system. In ANLP/NAACL’00 WS on Conversational Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
</authors>
<title>Improving user modelling with content-based techniques.</title>
<date>2001</date>
<booktitle>In UM’01.</booktitle>
<contexts>
<context position="5996" citStr="Magnini and Strapparava, 2001" startWordPosition="972" endWordPosition="975"> a SVM-based re-ranker. In the following sections, we describe how the baseline architecture is enhanced to accommodate personalization and interactivity. 3 User Modelling for Personalization Our model of personalization is centered on a User Model which represents students searching for information on the Web according to three attributes: 1. age range a E {7 − 10,11 − 16, adult}, 2. reading level r E {basic, medium, advanced}; 3. profile p, a set of textual documents, bookmarks and Web pages of interest. Users’ age1 and browsing history are typical UM components in news recommender systems (Magnini and Strapparava, 2001); personalized search systems such as (Teevan et al., 2005) also construct UMs based on the user’s documents and Web pages of interest. 3.1 Reading Level Estimation We approach reading level estimation as a supervised learning task, where representative documents for each of the three UM reading levels are collected to be labelled training instances and used to classify previously unseen documents. Our training instances consist of about 180 HTML documents from a collection of Web portals2 where 1Although the reading level can be modelled separately from the age range, for simplicity we here a</context>
</contexts>
<marker>Magnini, Strapparava, 2001</marker>
<rawString>Magnini, B. and C. Strapparava. 2001. Improving user modelling with content-based techniques. In UM’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Maybury</author>
</authors>
<title>Towards a question answering roadmap.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>MITRE Corporation.</institution>
<contexts>
<context position="2439" citStr="Maybury, 2002" startWordPosition="384" endWordPosition="385">sue queries not as standalone questions but in the context of a wider information need, for instance when researching a specific topic. Recently, a new research direction has © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. been proposed, which involves the integration of QA systems with dialogue interfaces in order to encourage and accommodate the submission of multiple related questions and handle the user’s requests for clarification in a less artificial setting (Maybury, 2002); however, Interactive QA (IQA) systems are still at an early stage or applied to closed domains (Small et al., 2003; Kato et al., 2006). Also, the “complex, interactive QA” TREC track (www.umiacs.umd.edu/ ˜jimmylin/ciqa/) has been organized, but here the interactive aspect refers to the evaluators being enabled to interact with the systems rather than to dialogue per se. In this paper, we first present an adaptation of User Modelling (Kobsa, 2001) to the design of personalized QA, and secondly we design and implement an interactive open-domain QA system, YourQA. Section 2 briefly introduces t</context>
</contexts>
<marker>Maybury, 2002</marker>
<rawString>Maybury, M. T. 2002. Towards a question answering roadmap. Technical report, MITRE Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
<author>R Basili</author>
<author>S Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In ACL’07.</booktitle>
<contexts>
<context position="5291" citStr="Moschitti et al., 2007" startWordPosition="857" endWordPosition="860">he Google rank of the answer source document is used as a tie-breaking criterion. 3. The list of top ranked answers is then returned to the user in an HTML page. Note that our answers are in the form of sentences with relevant words or phrases highlighted (as visible in Figure 2) and surrounded by their original passage. This is for two reasons: we believe that providing a context to the exact answer is important and we have been mostly focusing on non-factoids, such as definitions, which it makes sense to provide in the form of a sentence. A thorough evaluation of YourQA is reported in e.g. (Moschitti et al., 2007); it shows an F1 of 48±.7 for non-factoids on Web data, further improved by a SVM-based re-ranker. In the following sections, we describe how the baseline architecture is enhanced to accommodate personalization and interactivity. 3 User Modelling for Personalization Our model of personalization is centered on a User Model which represents students searching for information on the Web according to three attributes: 1. age range a E {7 − 10,11 − 16, adult}, 2. reading level r E {basic, medium, advanced}; 3. profile p, a set of textual documents, bookmarks and Web pages of interest. Users’ age1 a</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Moschitti, A., S. Quarteroni, R. Basili, and S. Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In ACL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Munteanu</author>
<author>M Boldea</author>
</authors>
<title>Mdwoz: A wizard of oz environment for dialog systems development.</title>
<date>2000</date>
<booktitle>In LREC’00.</booktitle>
<contexts>
<context position="22116" citStr="Munteanu and Boldea, 2000" startWordPosition="3663" endWordPosition="3666">elieves to be interacting with a fully automated prototype. We designed six tasks reflecting the intended typical usage of the system (e.g.: “Find out who painted Guernica and ask the system for more information about the artist”) to be carried out by 7 users by interacting with an instant messaging platform, which they were told to be the system interface. 6www.alicebot.org/ The role of the Wizard was to simulate a limited range of utterances and conversational situations handled by a chatbot. User feedback was collected mainly by using a post-hoc questionnaire inspired by the experiment in (Munteanu and Boldea, 2000), which consists of questions Q1 to Q6 in Table 2, col. 1, to be answered using a scale from 1=“Not at all” to 5=“Yes, absolutely”. From the WOz results, reported in Table 2, col. “WOz”, users appear to be generally very satisfied with the system’s performances: Q6 obtained an average of 4.5±.5. None of the users had difficulties in reformulating their questions when this was requested: Q4 obtained 3.8±.5. For the remaining questions, satisfaction levels were high: users generally thought that the system understood their information needs (Q2 obtained 4) and were able to obtain such informatio</context>
</contexts>
<marker>Munteanu, Boldea, 2000</marker>
<rawString>Munteanu, C. and M. Boldea. 2000. Mdwoz: A wizard of oz environment for dialog systems development. In LREC’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Quarteroni</author>
<author>S Manandhar</author>
</authors>
<title>A chatbotbased interactive question answering system.</title>
<date>2007</date>
<booktitle>In DECALOG’07,</booktitle>
<location>Rovereto, Italy.</location>
<contexts>
<context position="3406" citStr="Quarteroni and Manandhar, 2007" startWordPosition="541" endWordPosition="545">stems rather than to dialogue per se. In this paper, we first present an adaptation of User Modelling (Kobsa, 2001) to the design of personalized QA, and secondly we design and implement an interactive open-domain QA system, YourQA. Section 2 briefly introduces the baseline architecture of YourQA. In Section 3, we show how a model of the user’s reading abilities and personal interests can be used to efficiently improve the quality of the information returned by a QA system. We provide an extensive evaluation methodology to assess such efficiency by improving on our previous work in this area (Quarteroni and Manandhar, 2007b). Moreover, we discuss our design of interactive QA in Section 4 and conduct a more rigorous evaluation of the interactive version of YourQA by comparing it to the baseline version on a set of TREC-QA questions, obtaining encouraging results. Finally, a unified model of personalized, interactive QA is described in Section 5. 2 Baseline System Architecture The baseline version of our system, YourQA, is able to extract answers to both factoid and non-factoid questions from the Web. As most QA systems (Kwok et al., 2001), it is organized according to three phases: • Question Processing: The que</context>
<context position="16591" citStr="Quarteroni and Manandhar, 2007" startWordPosition="2770" endWordPosition="2773">row reports a remarkable difference between the perceived usefulness for question Qrel with respect to question Qbas (answers to TEST1). The results were compared by carrying out a oneway analysis of variance (ANOVA) and performing the Fischer test using the usefulness as factor (with the 5The adoption of a Likert scale made it possible to compute the average and standard deviations of the user comments with respect to each answer among the top five returned by the system. It was therefore possible to replace the binary measurement of perceived usefulness, relatedness and sensitivity used in (Quarteroni and Manandhar, 2007b) in terms of total number of users with a more fine-grained one in terms of average computed over the users. three queries as levels) at a 95% level of confidence. The test revealed an overall significant difference between factors, confirming that users are positively biased towards questions related to their own profile when it comes to perceived utility. To analyze the answers to TEST2 (Table 1, row 2), which measured the perceived relatedness of each answer to the current profile, we used ANOVA again and and obtained an overall significant difference. Hence, answers obtained without usin</context>
<context position="26423" citStr="Quarteroni and Manandhar, 2007" startWordPosition="4390" endWordPosition="4393">r. The latter is used to clarify elliptic questions, by augmenting the current query keywords with those in the topic when ellipsis is detected. Figure 1 illustrates YourQA’s interactive version, which is accessible from the Web. As in a normal chat application, users write in a text field and the current session history as well as the interlocutor replies are visualized in a text area. 4.5 Interactive QA evaluation For the evaluation of interactivity, we built on our previous results from a Wizard-of-Oz experiment and an initial evaluation conducted on a limited set of handcrafted questions (Quarteroni and Manandhar, 2007a). We chose 9 question series from the TREC-QA 2007 campaign8. Three questions were retained per series to make each evaluation balanced. For instance, the three following questions were used to form one task: 266.1: “When was Rafik Hariri born?”, 266.2: “To what religion did he belong (including sect)?” and 266.4: “At what time in the day was he assassinated?”. Twelve users were invited to find answers to the questions to one of them by using the standard version of the system and to the second by using the interactive version. Each series was evaluated at least once using both versions of t</context>
</contexts>
<marker>Quarteroni, Manandhar, 2007</marker>
<rawString>Quarteroni, S. and S. Manandhar. 2007a. A chatbotbased interactive question answering system. In DECALOG’07, Rovereto, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Quarteroni</author>
<author>S Manandhar</author>
</authors>
<title>User modelling for personalized question answering.</title>
<date>2007</date>
<booktitle>In AI*IA’07,</booktitle>
<location>Rome, Italy.</location>
<contexts>
<context position="3406" citStr="Quarteroni and Manandhar, 2007" startWordPosition="541" endWordPosition="545">stems rather than to dialogue per se. In this paper, we first present an adaptation of User Modelling (Kobsa, 2001) to the design of personalized QA, and secondly we design and implement an interactive open-domain QA system, YourQA. Section 2 briefly introduces the baseline architecture of YourQA. In Section 3, we show how a model of the user’s reading abilities and personal interests can be used to efficiently improve the quality of the information returned by a QA system. We provide an extensive evaluation methodology to assess such efficiency by improving on our previous work in this area (Quarteroni and Manandhar, 2007b). Moreover, we discuss our design of interactive QA in Section 4 and conduct a more rigorous evaluation of the interactive version of YourQA by comparing it to the baseline version on a set of TREC-QA questions, obtaining encouraging results. Finally, a unified model of personalized, interactive QA is described in Section 5. 2 Baseline System Architecture The baseline version of our system, YourQA, is able to extract answers to both factoid and non-factoid questions from the Web. As most QA systems (Kwok et al., 2001), it is organized according to three phases: • Question Processing: The que</context>
<context position="16591" citStr="Quarteroni and Manandhar, 2007" startWordPosition="2770" endWordPosition="2773">row reports a remarkable difference between the perceived usefulness for question Qrel with respect to question Qbas (answers to TEST1). The results were compared by carrying out a oneway analysis of variance (ANOVA) and performing the Fischer test using the usefulness as factor (with the 5The adoption of a Likert scale made it possible to compute the average and standard deviations of the user comments with respect to each answer among the top five returned by the system. It was therefore possible to replace the binary measurement of perceived usefulness, relatedness and sensitivity used in (Quarteroni and Manandhar, 2007b) in terms of total number of users with a more fine-grained one in terms of average computed over the users. three queries as levels) at a 95% level of confidence. The test revealed an overall significant difference between factors, confirming that users are positively biased towards questions related to their own profile when it comes to perceived utility. To analyze the answers to TEST2 (Table 1, row 2), which measured the perceived relatedness of each answer to the current profile, we used ANOVA again and and obtained an overall significant difference. Hence, answers obtained without usin</context>
<context position="26423" citStr="Quarteroni and Manandhar, 2007" startWordPosition="4390" endWordPosition="4393">r. The latter is used to clarify elliptic questions, by augmenting the current query keywords with those in the topic when ellipsis is detected. Figure 1 illustrates YourQA’s interactive version, which is accessible from the Web. As in a normal chat application, users write in a text field and the current session history as well as the interlocutor replies are visualized in a text area. 4.5 Interactive QA evaluation For the evaluation of interactivity, we built on our previous results from a Wizard-of-Oz experiment and an initial evaluation conducted on a limited set of handcrafted questions (Quarteroni and Manandhar, 2007a). We chose 9 question series from the TREC-QA 2007 campaign8. Three questions were retained per series to make each evaluation balanced. For instance, the three following questions were used to form one task: 266.1: “When was Rafik Hariri born?”, 266.2: “To what religion did he belong (including sect)?” and 266.4: “At what time in the day was he assassinated?”. Twelve users were invited to find answers to the questions to one of them by using the standard version of the system and to the second by using the interactive version. Each series was evaluated at least once using both versions of t</context>
</contexts>
<marker>Quarteroni, Manandhar, 2007</marker>
<rawString>Quarteroni, S. and S. Manandhar. 2007b. User modelling for personalized question answering. In AI*IA’07, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
<author>T Liu</author>
<author>N Shimizu</author>
<author>T Strzalkowski</author>
</authors>
<title>HITIQA: an interactive question answering system- a preliminary report.</title>
<date>2003</date>
<booktitle>In ACL’03 WS on Multilingual summarization and QA.</booktitle>
<contexts>
<context position="2555" citStr="Small et al., 2003" startWordPosition="402" endWordPosition="405">ching a specific topic. Recently, a new research direction has © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. been proposed, which involves the integration of QA systems with dialogue interfaces in order to encourage and accommodate the submission of multiple related questions and handle the user’s requests for clarification in a less artificial setting (Maybury, 2002); however, Interactive QA (IQA) systems are still at an early stage or applied to closed domains (Small et al., 2003; Kato et al., 2006). Also, the “complex, interactive QA” TREC track (www.umiacs.umd.edu/ ˜jimmylin/ciqa/) has been organized, but here the interactive aspect refers to the evaluators being enabled to interact with the systems rather than to dialogue per se. In this paper, we first present an adaptation of User Modelling (Kobsa, 2001) to the design of personalized QA, and secondly we design and implement an interactive open-domain QA system, YourQA. Section 2 briefly introduces the baseline architecture of YourQA. In Section 3, we show how a model of the user’s reading abilities and personal i</context>
</contexts>
<marker>Small, Liu, Shimizu, Strzalkowski, 2003</marker>
<rawString>Small, S., T. Liu, N. Shimizu, and T. Strzalkowski. 2003. HITIQA: an interactive question answering system- a preliminary report. In ACL’03 WS on Multilingual summarization and QA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sutton</author>
</authors>
<title>Universal speech tools: the CSLU toolkit.</title>
<date>1998</date>
<booktitle>In ICSLP’98.</booktitle>
<contexts>
<context position="19820" citStr="Sutton, 1998" startWordPosition="3296" endWordPosition="3297">as the QA component results are available, an answer a is provided; 36 5. The system enquires whether the user is interested in submitting new queries; 6. Whenever the user wants to terminate the interaction, a final greeting is exchanged. 4.1 Choosing a Dialogue Manager Among traditional methods for implementing information-seeking dialogue management, FiniteState (FS) approaches are the simplest. Here, the dialogue manager is represented as a Finite-State machine, where each state models a separate phase of the conversation, and each dialogue move encodes a transition to a subsequent state (Sutton, 1998). However, an issue with FS models is that they allow very limited freedom in the range of user utterances: since each dialogue move must be pre-encoded in the models, there is a scalability issue when addressing open domain dialogue. On the other hand, we believe that other dialogue approaches such as the Information State (IS) (Larsson et al., 2000) are primarily suited to applications requiring a planning component such as closed-domain dialogue systems and to a lesser extent to open-domain QA. As an alternative approach, we studied conversational agents (“chatbots”) based on AIML (Artifici</context>
</contexts>
<marker>Sutton, 1998</marker>
<rawString>Sutton, S. 1998. Universal speech tools: the CSLU toolkit. In ICSLP’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Teevan</author>
<author>S T Dumais</author>
<author>E Horvitz</author>
</authors>
<title>Personalizing search via automated analysis of interests and activities.</title>
<date>2005</date>
<booktitle>In SIGIR ’05.</booktitle>
<contexts>
<context position="6055" citStr="Teevan et al., 2005" startWordPosition="981" endWordPosition="984">e baseline architecture is enhanced to accommodate personalization and interactivity. 3 User Modelling for Personalization Our model of personalization is centered on a User Model which represents students searching for information on the Web according to three attributes: 1. age range a E {7 − 10,11 − 16, adult}, 2. reading level r E {basic, medium, advanced}; 3. profile p, a set of textual documents, bookmarks and Web pages of interest. Users’ age1 and browsing history are typical UM components in news recommender systems (Magnini and Strapparava, 2001); personalized search systems such as (Teevan et al., 2005) also construct UMs based on the user’s documents and Web pages of interest. 3.1 Reading Level Estimation We approach reading level estimation as a supervised learning task, where representative documents for each of the three UM reading levels are collected to be labelled training instances and used to classify previously unseen documents. Our training instances consist of about 180 HTML documents from a collection of Web portals2 where 1Although the reading level can be modelled separately from the age range, for simplicity we here assume that these are paired in a reading level component. 2</context>
<context position="8093" citStr="Teevan et al., 2005" startWordPosition="1310" endWordPosition="1313"> has been generated by lmi (In our case, three language models lmi are defined, where i E {basic, medium, advanced}.). Such likelihood is estimated using the function: L(lmi|D) = � C(w, D) · log[P(w|lmi)], (1) w∈D where w is a word in the document, C(w, d) represents the number of occurrences of w in D and P(w|lmi) is the probability that w occurs in lmi (approximated by its frequency). 3.2 Profile Estimation Information extraction from the user’s documents as a means of representation of the user’s interests, such as his/her desktop files, is a well-established technique for personalized IR (Teevan et al., 2005). Profile estimation in YourQA is based on key-phrase extraction, a technique previously employed in several natural language tasks (Frank et al., 1999). For this purpose, we use Kea (Witten et al., 1999), which splits documents into phrases and chooses some of the phrases as be key-phrases based on two criteria: the first index of their occurrence in the source documents and their TF x IDF score3 with respect to the current document collection. Kea outputs for each document in the set a ranked list where the candidate key-phrases are in decreasing order; after experimenting with several value</context>
<context position="12125" citStr="Teevan et al., 2005" startWordPosition="1996" endWordPosition="1999">levance of a key-phrase to its source document, row index only depends on the name of such document. The personalized answer ranking takes wP into account as a secondary ranking criterion with respect to the baseline system’s similarity score; as before, Google rank of the source document is used as further a tie-breaking criterion. Notice that our approach to User Modelling can be seen as a form of implicit (or quasi-implicit) relevance feedback, i.e. feedback not explicitly obtained from the user but inferred from latent information in the user’s documents. Indeed, we take inspiration from (Teevan et al., 2005)’s approach to personalized search, computing the relevance of unseen documents (such as those retrieved for a query) as a function of the presence and frequency of the same terms in a second set of documents on whose relevance the user has provided feedback. Our approaches to personalization are evaluated in Section 3.4. 3.4 Evaluating Personalization The evaluation of our personalized QA algorithms assessed the contributions of the reading level attribute and of the profile attribute of the User Model. 3.4.1 Reading Level Evaluation Reading level estimation was evaluated by first assessing t</context>
</contexts>
<marker>Teevan, Dumais, Horvitz, 2005</marker>
<rawString>Teevan, J., S. T. Dumais, and E. Horvitz. 2005. Personalizing search via automated analysis of interests and activities. In SIGIR ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In TREC’03.</booktitle>
<contexts>
<context position="1573" citStr="Voorhees, 2003" startWordPosition="255" endWordPosition="256">uestions in natural language. In Web-based QA in particular, this problem affects the relevance of results with respect to the users’ needs, as queries can be ambiguous and even answers extracted from documents with relevant content but expressed in a difficult language may be illreceived by users. While the need for user personalization has been addressed by the IR community for a long time (Belkin and Croft, 1992), very little effort has been carried out up to now in the QA community in this direction. Indeed, personalized Question Answering has been advocated in TREC-QA starting from 2003 (Voorhees, 2003); however, the issue was solved rather expeditiously by designing a scenario where an “average news reader” was imagined to submit the 2003 task’s definition questions. Moreover, a commonly observed behavior in users of IR systems is that they often issue queries not as standalone questions but in the context of a wider information need, for instance when researching a specific topic. Recently, a new research direction has © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reser</context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Voorhees, E. M. 2003. Overview of the TREC 2003 Question Answering Track. In TREC’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>G W Paynter</author>
<author>E Frank</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>KEA: Practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In ACM DL.</booktitle>
<contexts>
<context position="8297" citStr="Witten et al., 1999" startWordPosition="1342" endWordPosition="1345">, (1) w∈D where w is a word in the document, C(w, d) represents the number of occurrences of w in D and P(w|lmi) is the probability that w occurs in lmi (approximated by its frequency). 3.2 Profile Estimation Information extraction from the user’s documents as a means of representation of the user’s interests, such as his/her desktop files, is a well-established technique for personalized IR (Teevan et al., 2005). Profile estimation in YourQA is based on key-phrase extraction, a technique previously employed in several natural language tasks (Frank et al., 1999). For this purpose, we use Kea (Witten et al., 1999), which splits documents into phrases and chooses some of the phrases as be key-phrases based on two criteria: the first index of their occurrence in the source documents and their TF x IDF score3 with respect to the current document collection. Kea outputs for each document in the set a ranked list where the candidate key-phrases are in decreasing order; after experimenting with several values, we chose to use the top 6 as key-phrases for each document. The profile resulting from the extracted key-phrases is the base for all the subsequent QA activity: any question the user submits to the QA </context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Witten, I. H., G. W. Paynter, E. Frank, C. Gutwin, and C. G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. In ACM DL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>