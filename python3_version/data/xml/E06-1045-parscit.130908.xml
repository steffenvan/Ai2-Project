<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021679">
<title confidence="0.986725">
Data-driven Generation of Emphatic Facial Displays
</title>
<author confidence="0.949246">
Mary Ellen Foster
</author>
<affiliation confidence="0.961378">
Department of Informatics, Technical University of Munich
</affiliation>
<address confidence="0.813888">
Boltzmannstraße 3, 85748 Garching, Germany
</address>
<email confidence="0.947432">
foster@in.tum.de
</email>
<author confidence="0.959452">
Jon Oberlander
</author>
<affiliation confidence="0.884062333333333">
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdom
</affiliation>
<email confidence="0.996099">
jon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983956521739">
We describe an implementation of data-
driven selection of emphatic facial dis-
plays for an embodied conversational
agent in a dialogue system. A corpus of
sentences in the domain of the target dia-
logue system was recorded, and the facial
displays used by the speaker were anno-
tated. The data from those recordings was
used in a range of models for generating
facial displays, each model making use of
a different amount of context or choosing
displays differently within a context. The
models were evaluated in two ways: by
cross-validation against the corpus, and by
asking users to rate the output. The predic-
tions of the cross-validation study differed
from the actual user ratings. While the
cross-validation gave the highest scores to
models making a majority choice within a
context, the user study showed a signifi-
cant preference for models that produced
more variation. This preference was espe-
cially strong among the female subjects.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949255319149">
It has long been documented that there are char-
acteristic facial displays that accompany the em-
phasised parts of spoken utterances. For example,
Ekman (1979) says that eyebrow raises “appear to
coincide with primary vocal stress, or more sim-
ply with a word that is spoken more loudly.” Cor-
relations have also been found between prosodic
features and events such as head nodding and the
amplitude of mouth movements. When Krah-
mer and Swerts (2004) performed an empirical,
cross-linguistic evaluation of the influence of brow
movements on the perception of prosodic stress,
they found that subjects preferred eyebrow move-
ments to be correlated with the most prominent
word in an utterance and that eyebrow movements
boosted the perceived prominence of the word
they were associated with.
While many facial displays have been shown
to co-occur with prosodic accents, the converse
is not true: in normal embodied speech, many
pitch accents and other prosodic events are unac-
companied by any facial display, and when dis-
plays are used, the selection varies widely. Cas-
sell and Th´orisson (1999) demonstrated that “en-
velope” facial displays related to the process of
conversation have a greater impact on successful
interaction with an embodied conversational agent
than do emotional displays. However, no descrip-
tion of face motion is sufficiently detailed that it
can be used as the basis for selecting emphatic fa-
cial displays for an agent. This is therefore a task
for which data-driven techniques are beneficial.
In this paper, we address the task of selecting
emphatic facial displays for the talking head in
the COMIC1 multimodal dialogue system. In the
basic COMIC process for generating multimodal
output (Foster et al., 2005), facial displays are se-
lected using simple rules based only on the pitch
accents specified by the text generation system. In
order to make a more sophisticated and naturalis-
tic selection of facial displays, we recorded a sin-
gle speaker reading a set of sentences drawn from
the COMIC domain, and annotated the facial dis-
plays that he used and the contexts in which he
used them. We then created models based on the
data from this corpus and used them to choose the
facial displays for the COMIC talking head.
</bodyText>
<footnote confidence="0.984365">
1http://www.hcrc.ed.ac.uk/comic/
</footnote>
<page confidence="0.998399">
353
</page>
<bodyText confidence="0.9971296">
The rest of this paper is arranged as follows.
First, in Section 2, we describe previous ap-
proaches to selecting non-verbal behaviour for
embodied conversational agents. In Section 3, we
then show how we collected and annotated a cor-
pus of facial displays, and give some generalisa-
tions about the range of displays found in the cor-
pus. After that, in Section 4, we outline how we
implemented a range of models for selecting be-
haviours for the COMIC agent using the corpus
data, using varying amounts of context and differ-
ent selection strategies within a context. Next, we
give the results of two evaluation studies compar-
ing the quality of the output generated by the var-
ious models: a cross-validation study against the
corpus (Section 5) and a direct user evaluation of
the output (Section 6). In Section 7, we discuss the
results of these two evaluations. Finally, in Sec-
tion 8, we draw some conclusions from the current
study and outline potential follow-up work.
</bodyText>
<sectionHeader confidence="0.690092" genericHeader="introduction">
2 Choosing Non-Verbal Behaviour for
Embodied Conversational Agents
</sectionHeader>
<bodyText confidence="0.999391208955224">
Embodied Conversational Agents (ECAs) are
computer interfaces that are represented as hu-
man bodies, and that use their face and body in
a human-like way in conversations with the user
(Cassell et al., 2000). The main benefit of ECAs
is that they allow users to interact with a computer
in the most natural possible setting: face-to-face
conversation. However, to realise this advantage
fully, the agent must produce high-quality output,
both verbal and non-verbal. A number of previous
systems have based the choice of non-verbal be-
haviours for an ECA on the behaviours of humans
in conversational situations. The implementations
vary as to how directly they use the human data.
In some systems, motion specifications for the
agent are created from scratch, using rules derived
from studying human behaviour. For the REA
agent (Cassell et al., 2001a), for example, ges-
turing behaviour was selected to perform particu-
lar communicative functions, using rules based on
studies of typical North American non-verbal dis-
plays. Similarly, the Greta agent (de Carolis et al.,
2002) selected its performative facial displays us-
ing hand-crafted rules to map from affective states
to facial motions. Such implementations do not
make direct use of any recorded human motions;
this means that they generate average behaviours
from a range of people, but it is difficult to adapt
them to reproduce the behaviour of an individual.
In contrast, other ECA implementations have
selected non-verbal behaviour based directly on
motion-capture recordings of humans. Stone et al.
(2004), for example, recorded an actor performing
scripted output in the domain of the target system.
They then segmented the recordings into coher-
ent phrases and annotated them with the relevant
semantic and pragmatic information, and com-
bined the segments at run-time to produce com-
plete performance specifications that were then
played back on the agent. Cunningham et al.
(2004) and Shimodaira et al. (2005) used similar
techniques to base the appearance and motions of
their talking heads directly on recordings of hu-
man faces. This technique is able to produce more
naturalistic output than the more rule-based sys-
tems described above; however, capturing the mo-
tion requires specialised hardware, and the agent
must be implemented in such a way that it can ex-
actly reproduce the human motions.
A middle ground is to use a purely synthetic
agent—one whose behaviour is controlled by
high-level instructions, rather than based directly
on human motions—but to create the instructions
for that agent using the data from an annotated cor-
pus of human behaviour. Like a motion-capture
implementation, this technique can also produce
increased naturalism in the output and also al-
lows choices to be based on the motions of a sin-
gle performer if necessary. However, annotating
a video corpus can be less technically demand-
ing than capturing and directly re-using real mo-
tions, especially when the corpus and the number
of features under consideration are small. This ap-
proach has been taken, for example, by Cassell
et al. (2001b) to choose posture shifts for REA,
and by Kipp (2004) to select gestures for agents,
and it is also the approach that we adopt here.
</bodyText>
<sectionHeader confidence="0.829358" genericHeader="method">
3 Recording and Annotation
</sectionHeader>
<bodyText confidence="0.9997529">
The recording script for the data collection con-
sisted of 444 sentences in the domain of the
COMIC multimodal dialogue system; all of the
sentences described one or more features of one or
more bathroom-tile designs. The sentences were
generated by the full COMIC output planner, and
were selected to provide coverage of all of the
syntactic patterns available to the system. In ad-
dition to the surface text, each sentence included
all of the contextual information from the COMIC
</bodyText>
<page confidence="0.998271">
354
</page>
<bodyText confidence="0.855489">
46. More about the current design
they dislike the first feature, but like the second one
There are GEOMETRIC SHAPES on the
decorative tiles, but the tiles ARE from the
ARMONIE series.
</bodyText>
<figureCaption confidence="0.997649">
Figure 1: Sample prompt slide
</figureCaption>
<bodyText confidence="0.999984689189189">
planner: the predicted pitch accents—selected ac-
cording to Steedman’s (2000) theory of informa-
tion structure and intonation—along with any in-
formation from the user model and dialogue his-
tory. The sentences were presented one at a time
to the speaker, who was instructed to read each
sentence out loud as expressively as possible while
looking into a camera directed at his face. The seg-
ments for which the presentation planner specified
pitch accents were highlighted, and any applicable
user-model and dialogue-history information was
included. Figure 1 shows a sample prompt slide.
The recorded videos were annotated by the first
author, using a purpose-built tool that allowed any
set of facial displays to be associated with any seg-
ment of the sentence. First, the video was split into
clips corresponding to each sentence. After that,
the facial displays in each clip were annotated.
The following were the displays that were consid-
ered: eyebrow raising and lowering; eye squinting;
head nodding (up, small down, large down); head
leaning (left and right); and head turning (left and
right). Figure 2 shows examples of two typical
display combinations. Any combination of these
facial displays could be associated with any of the
relevant segments in the text. The relevant seg-
ments included all mentions of tile-design prop-
erties (e.g., colours, designers), modifiers such
as once again and also, deictic determiners (this,
these), and verbs in contrastive contexts (e.g., are
in Figure 1). The annotation scheme treated all fa-
cial displays as batons rather than underliners (Ek-
man, 1979); that is, each display was associated
with a single segment. If a facial display spanned
a longer phrase in the speech, it was annotated as a
series of identical batons on each of the segments.
Any predicted pitch accents and dialogue-
history and user-model information from the
COMIC presentation planner were also associated
with each segment, as appropriate. We chose not
to restrict our annotation to those segments with
predicted pitch accents, because the speaker also
made a large number of facial displays on seg-
ments with no predicted pitch accent; instead, we
incorporated the predicted accent as an additional
contextual factor. For the most part, the pitch ac-
cents used by the speaker followed the specifica-
tions on the slides. We did not explicitly consider
the rhetorical or syntactic structure, as did, e.g.,
de Carolis et al. (2000); in general, the structure
was fully determined by the context.
There were a total of 1993 relevant segments in
the recorded sentences. Overall, the most frequent
display combination was a small downward nod
on its own, which occurred on just over 25% of the
segments. The second largest class was no motion
at all (20% of the segments), followed by down-
ward nods (large and small) accompanied by brow
raises. Further down the order, the various lateral
motions appear; for this speaker, these were pri-
marily turns to the right (Figure 2(a)) and leans to
the left (Figure 2(b)).
The distribution of facial displays in specific
contexts differed from the overall distribution. The
biggest influence was the user-model evaluation:
left leans, brow lowering, and eye squinting were
all relatively more frequent on objects with nega-
tive user-model evaluations, while right turns and
brow raises occurred more often in positive con-
texts. Other factors also had an influence: for ex-
ample, nodding and brow raises were both more
frequent on segments for which the COMIC plan-
ner specified a pitch accent. Foster (2006) gives a
detailed analysis of these recordings.
</bodyText>
<sectionHeader confidence="0.920238" genericHeader="method">
4 Modelling the Corpus Data
</sectionHeader>
<bodyText confidence="0.998072071428571">
We built a range of models using the data from
the annotated corpus to select facial displays to
accompany generated text. For each segment in
the text, a model selected a display combination
from among the displays used by the speaker in a
similar context. All of the models used the corpus
counts of displays associated with the segments di-
rectly, with no back-off or smoothing.
The models differed from one another in two
ways: the amount of context that they used, and
the way in which they made a selection within a
context. There were three levels of context:
No context These models used the overall corpus
counts for all segments.
</bodyText>
<page confidence="0.989676">
355
</page>
<figure confidence="0.997108">
(a) Right turn + brow raise (b) Left lean + brow lower
</figure>
<figureCaption confidence="0.999741">
Figure 2: Typical speaker motions from the recording
</figureCaption>
<bodyText confidence="0.999915514285715">
Surface only These models used only the context
provided by the word(s)—or, in some cases,
a domain-specific semantic class. For exam-
ple, a model would use the class DECORA-
TION rather than the specific word artwork.
Full context In addition to the surface form, these
models also used the pitch-accent specifica-
tions and contextual information supplied by
the COMIC presentation planner. The con-
textual information was associated with the
tile-design properties included in the sen-
tence and indicated (a) whether that property
had been mentioned before, (b) whether it
was explicitly contrasted with a property of
a previous design, and (c) the expected user
evaluation of that property.
Within a context, there were two strategies for se-
lecting a facial display:
Majority Choose the combination that occurred
the largest number of times in the context.
Weighted Make a random choice from all com-
binations seen in the context, weighting the
choice according to the relative frequency.
For example, in the no-context case, a majority-
choice model would choose the small downward
nod (the majority option) for every segment, while
a weighted-choice model would choose a small
downward nod with probability 0.25, no motion
with probability 0.2, and the other displays with
correspondingly decreasing probabilities.
These two factors produced a set of 6 models
in total (3 context levels × 2 selection strategies).
Throughout the rest of this paper, we will use two-
character labels to refer to the models. The first
character of each label indicates the amount of
</bodyText>
<figure confidence="0.99827875">
0.85 0.63 0.60 0.56 0.56
0.80
0.85 0.54
0.50 0.61
0.45
0.40
0.35
0.30
0.25
0.20
0. ,5
0. ,0
0.05
0.00
FM SM NM FW SW NW
Model
</figure>
<figureCaption confidence="0.999769">
Figure 3: Mean F score for all models
</figureCaption>
<bodyText confidence="0.99793975">
context that was used, while the second indicates
the selection method within that context: for ex-
ample, SM corresponds to a model that used the
surface form only and made a majority choice.
</bodyText>
<sectionHeader confidence="0.992117" genericHeader="method">
5 Evaluation 1: Cross-validation
</sectionHeader>
<bodyText confidence="0.968470578947368">
We first compared the performance of the models
using 10-fold cross-validation against the corpus.
For each fold, we built models using 90% of the
sentences in the corpus, and then used those mod-
els to predict the facial displays for the sentences
in the other 10% of the corpus. We measured the
recall and precision on a sentence by comparing
the predicted facial displays for each segment to
the actual displays used by the speaker and aver-
aging those scores across the sentence. We then
used the recall and precision scores for a sentence
to compute a sentence-level F score.
Averaged across all of the cross-validation
folds, the NM model had the highest recall score,
while the FM model scored highest for precision
and F score. Figure 3 shows the average sentence-
level F score for all of the models. All but one
of the differences shown are significant at the p &lt;
Mean F score
</bodyText>
<page confidence="0.932796">
356
</page>
<figure confidence="0.99759">
(a) Neutral (b) Right turn + brow raise (c) Left lean + brow lower
</figure>
<figureCaption confidence="0.999978">
Figure 4: Synthesised version of motions from Figure 2
</figureCaption>
<bodyText confidence="0.99989794117647">
0.01 level on a paired T test; the performance of
the NM and FW models was indistinguishable on
F score, although the FW model scored higher on
precision and the NM model on recall.
That the majority-choice models generally
scored better on this measure than the weighted-
choice models is not unexpected: a weighted-
choice model is more likely to choose a less-
common display, and if it chooses it in a context
where the speaker did not, the score for that sen-
tence is decreased. It is also not surprising that,
within a selection strategy, the models that take
into account more of the context did better than
those that use less of it; this is simply an indica-
tion that there are patterns in the corpus, and that
all of the contextual information contributes to the
selection of displays.
</bodyText>
<sectionHeader confidence="0.991322" genericHeader="method">
6 Evaluation 2: User Ratings
</sectionHeader>
<bodyText confidence="0.999838769230769">
The majority-choice models performed better on
the cross-validation study than the weighted-
choice ones did; however, this does not does not
mean that users will necessarily like their output
in practice. A large amount of the lateral motion
and eyebrow movements occurs in the second- or
third-largest class in a number of contexts, and is
therefore less likely to be selected by a majority-
choice model. If users like to see motion other
than simple nodding, it might be that the sched-
ules generated by the weighted-choice models are
actually preferred. To address this question, we
performed a user evaluation.
</bodyText>
<subsectionHeader confidence="0.98683">
6.1 Experiment Design
</subsectionHeader>
<bodyText confidence="0.999658828571429">
Materials For this study, we generated 30 new
sentences from the COMIC system. The sen-
tences were selected to ensure that they covered
the full range of syntactic structures available to
COMIC and that none of them was a duplicate
of anything from the recording script. We then
generated a facial schedule for each sentence us-
ing each of the six models. Note that, for some
of the sentences, more than one model produced
an identical sequence of facial displays, either be-
cause the majority choice in a broader context was
the same as in a more narrow context, or because
a weighted-choice model ended up selecting the
majority option in every case. All such identical
schedules were retained in the set of materials; in
Section 6.2, we discuss their impact on the results.
We then made videos of every schedule for ev-
ery sentence, using the Festival speech synthesiser
(Clark et al., 2004) and the RUTH talking head
(DeCarlo et al., 2004). Figure 4 shows synthesised
versions of the facial displays from Figure 2.
Procedure 33 subjects took part in the experi-
ment: 17 female subjects and 16 males. They
were primarily undergraduate students, between
20 and 24 years old, native speakers of English,
with an intermediate amount of computer experi-
ence. Each subject in the study was shown videos
of all 30 sentences in an individually-chosen ran-
dom order. For each sentence, the subject saw
two versions, each generated by a different model,
and was asked to choose which version they liked
better. The displayed versions were counterbal-
anced so that every subject performed each pair-
wise comparison of models twice, once in each
order. The study was run over the web.
</bodyText>
<subsectionHeader confidence="0.997224">
6.2 Results2
</subsectionHeader>
<bodyText confidence="0.998917">
Figure 5(a) shows the overall preference rates for
all of the models. For each model, the value shown
</bodyText>
<footnote confidence="0.489205666666667">
2 We do not include those trials where both videos were
identical; if these are included, the results are similar, but the
distinctions described here just fail to reach significance.
</footnote>
<page confidence="0.982736">
357
</page>
<figure confidence="0.999634527777778">
NW
FM ve FW SMve SW NMve NW Overall
0.6
0.55
0.55 0.52 0.5
0.5
0.45 0.42 0.43
0.4
0.35
0.3
0.25
0.2
0.75
0. 7
0.05
0 FM SM NM FW SW
Overall % chosen
0.56
Head-to-head choice
0.55
0.35
0.25
0.15
0.05
0.85
0.45
0.8
0.5
0.4
0.3
0.2
0.1
0
Majority Weight��
Model Amount of context
(a) Overall preference rates (b) Head-to-head preferences
</figure>
<figureCaption confidence="0.9993">
Figure 5: User evaluation results
</figureCaption>
<bodyText confidence="0.999927372093023">
on that graph indicates the proportion of the time
that model was chosen over any of the alterna-
tives. For example, in all of the trials where the
FW model was one of the options, it was chosen
over the alternative 55% of the time. Note that the
values on that graph should not be directly com-
pared against one another; instead, each should be
individually compared with 0.5 (the dotted line) to
determine whether it was chosen more or less fre-
quently than chance. A binomial test on these val-
ues indicates that both the FW and the NW mod-
els were chosen significantly above chance, while
those generated by the SM and NM models were
chosen significantly below chance (all p &lt; 0.05).
The choices on the FM and SW models were in-
distinguishable from chance.
If we examine the preferences within a context,
we also see a preference for the weighted-choice
models. Figure 5(b) shows the preferences for se-
lection strategy within each context. For example,
when choosing between schedules both generated
by models using the full context (FM vs. FW),
subjects chose the one generated by the FW model
60% of the time. The trend in both the full-context
and no-context cases is in favour of the weighted-
choice models, and the combined values over all
such trials (the rightmost pair of bars in the figure)
show a significant preference for weighted choice
over majority choice across all contexts (binomial
test; p &lt; 0.05).
Gender differences There was a large gender
effect on the users’ preferences: overall, the
male subjects (n = 16) tended to choose the ma-
jority and weighted versions with almost equal
probabilities, while the female subjects (n = 17)
strongly preferred the weighted versions in any
context, and chose the weighted versions signif-
icantly more often in head-to-head comparisons
(p &lt; 0.001). In fact, all of the overall prefer-
ence for weighted-choice models came from the
responses of the female subjects. The graphs in
Figure 6 show the head-to-head preferences in all
contexts for both groups of subjects.
</bodyText>
<sectionHeader confidence="0.997142" genericHeader="evaluation">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999920962962963">
The predicted rankings from the cross-validation
study differ from those in the human evalua-
tion: while the cross-validation gave the highest
scores to the majority-choice models, the human
judges actually showed an overall preference for
the weighted-choice models. This provides sup-
port for our hypothesis that humans would prefer
generated output that reproduced more of the vari-
ation in the corpus, even if the choices made on
specific sentences differ from those mode in the
corpus. When Belz and Reiter (2006) performed
a similar study comparing natural-language gen-
eration systems that used different text-planning
strategies, they also found similar results: auto-
mated measures tended to favour majority-choice
strategies, while human judges preferred those that
made weighted choices. In general, this sort of au-
tomated measure will always tend to favour strate-
gies that, on average, do not diverge far from what
is found in the corpus, which indicates a drawback
to using such measures alone to evaluate genera-
tion systems where variation is expected.
The current study also suggests a further draw-
back to corpus-based evaluation: users may vary
systematically amongst themselves in what they
prefer. All of the overall preference for weighted-
choice models came from the female subjects;
</bodyText>
<page confidence="0.986382">
358
</page>
<figure confidence="0.999645405405405">
Head-to-head choice
Head-to-head choice
7
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.
0.05
0
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Majority Weient�d
Majority WeleNed
FM ve FW SM ve SW NM ve NW Overall FM ve FW SM ve SW NM ve NW Overall
Amount of context Amount of context
(a) Male subjects (b) Female subjects
</figure>
<figureCaption confidence="0.999924">
Figure 6: Gender influence on head-to-head preferences
</figureCaption>
<bodyText confidence="0.99998032">
the male subjects did not express any significant
preference either way, but had a mild preference
for the majority-choice models. Previous stud-
ies on embodied conversational agents have ex-
hibited gender effects that appear related this re-
sult: Robertson et al. (2004) found that, among
schoolchildren, girls preferred a tutoring system
that included an animated agent, while boys pre-
ferred one that did not; White et al. (2005) found
that a more expressive talking head decreased
male subjects’ task performance when using the
full COMIC system; while Bickmore and Cassell
(2005) found that women trusted the REA agent
more in embodied mode, while men trusted her
more over the telephone. Taken together, these re-
sults imply that male users prefer and perform bet-
ter using an embodied agent that is less expressive
and that shows less variation in its motions, and
may even prefer a system that does not have an
agent at all. These results are independent of the
gender of the agent: the COMIC agent is male,
REA is female, while the gender of Robertson’s
agents was mixed. In any case, there is more gen-
eral evidence that females have superior abilities
in facial expression recognition (Hall, 1984).
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999641">
In this paper, we have demonstrated that there are
patterns in the facial displays that this speaker used
when giving different types of object descriptions
in the COMIC system. The findings from the cor-
pus analysis are compatible with previous find-
ings on emphatic facial displays in general, and
also provide a fine-grained analysis of the indi-
vidual displays used by this speaker. Basing the
recording scripts on the output of the presenta-
tion planner allowed full contextual information
to be included in the annotated corpus; indeed,
all of the contextual factors were found to influ-
ence the speaker’s use of facial displays. We have
also shown that a generation system that captures
and reproduces the corpus patterns for a synthetic
head can produce successful output. The results
of the evaluation also demonstrate that female sub-
jects are more receptive than male subjects to vari-
ation in facial displays; in combination with other
related results, this indicates that expressive con-
versational agents are more likely to be successful
with female users, regardless of the gender of the
agent. Finally, we have shown the potential draw-
backs of using a corpus to evaluate the output of a
generation system.
There are three directions in which the work de-
scribed here can be extended: improved corpus an-
notation, more sophisticated implementations, and
further evaluations. First, the annotation on the
corpus that was used here was done by a single an-
notator, in the context of a specific generation task.
The findings from the corpus analysis generally
agree with those of previous studies (e.g., the pre-
dicted pitch accent was correlated with nodding
and eyebrow raises), and the corpus as it stands
has proved useful for the task for which it was cre-
ated. However, to get a more definitive picture of
the patterns in the corpus, it should be re-annotated
by multiple coders, and the inter-annotator agree-
ment should be assessed. Possible extensions to
the annotation scheme include timing information
for the words and facial displays, and actual—as
opposed to predicted—prosodic contours.
In the implementation described here, we built
simple models based directly on the corpus counts
and used them to select facial displays to accom-
</bodyText>
<page confidence="0.996794">
359
</page>
<bodyText confidence="0.999972705882353">
pany previously-generated text; both of these as-
pects of the implementation can be extended in
future. If we build more sophisticated n-gram-
based models of the facial displays, using a full
language-modelling toolkit, we can take into ac-
count contextual information from words other
than those in a single segment, and back off
smoothly through different amounts of context.
Such models can also be integrated directly into
the OpenCCG surface realiser (White, 2005)—
which is already used as part of the COMIC
output-generation process, and which uses n-
grams to guide its search for a good realisation.
This will allow the system to choose the text and
facial displays in parallel rather than sequentially.
Such an integrated implementation has a better
chance at capturing the complex interactions be-
tween the two output channels.
Future evaluations should address several ques-
tions. First, we should gather users’ opinions of
the behaviours annotated in the corpus: it may be
that subjects actually prefer the generated facial
displays to the displays in the corpus, as was found
by Belz and Reiter (2006). As well, further stud-
ies should look in more detail at the exact nature of
the gender effect on user preferences, for instance
by systematically varying the motion on differ-
ent dimensions individually to see exactly which
types of facial displays are liked and disliked by
different demographic groups. Finally, if the ex-
tended n-gram-based model mentioned above is
implemented, its performance should be measured
and compared to that of the models described here,
through both cross-validation and user studies.
</bodyText>
<sectionHeader confidence="0.99655" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999799666666667">
Thanks to Matthew Stone, Michael White, and the
anonymous EACL reviewers for their useful com-
ments on previous versions of this paper.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916155844156">
A. Belz and E. Reiter. 2006. Comparing automatic and hu-
man evaluation of NLG systems. In Proc. EACL 2006.
T. Bickmore and J. Cassell. 2005. Social dialogue with em-
bodied conversational agents. In J. van Kuppevelt, L. Dy-
bkjær, and N. Bernsen, editors, Advances in Natural, Mul-
timodal Dialogue Systems. Kluwer, New York.
B. de Carolis, V. Carofiglio, and C. Pelachaud. 2002. From
discourse plans to believable behavior generation. In Proc.
INLG 2002.
B. de Carolis, C. Pelachaud, and I. Poggi. 2000. Verbal
and nonverbal discourse planning. In Proc. AAMAS 2000
Workshop “Achieving Human-Like Behavior in Interac-
tive Animated Agents”.
J. Cassell, T. Bickmore, H. Vilhj´almsson, and H. Yan. 2001a.
More than just a pretty face: Conversational protocols and
the affordances of embodiment. Knowledge-Based Sys-
tems, 14(1–2):55–64.
J. Cassell, Y. Nakano, T. W. Bickmore, C. L. Sidner, and
C. Rich. 2001b. Non-verbal cues for discourse structure.
In Proc. ACL 2001.
J. Cassell, J. Sullivan, S. Prevost, and E. Churchill. 2000. Em-
bodied Conversational Agents. MIT Press.
J. Cassell and K. R. Th´orisson. 1999. The power of a nod
and a glance: Envelope vs. emotional feedback in an-
imated conversational agents. Applied Artificial Intelli-
gence, 13(4–5):519–538.
R. A. J. Clark, K. Richmond, and S. King. 2004. Festival 2 –
build your own general purpose unit selection speech syn-
thesiser. In Proc. 5th ISCA Workshop on Speech Synthesis.
D. W. Cunningham, M. Kleiner, H. H. B¨ulthoff, and C. Wall-
raven. 2004. The components of conversational facial ex-
pressions. In Proc. APGV 2004, pages 143–150.
D. DeCarlo, M. Stone, C. Revilla, and J. Venditti. 2004. Spec-
ifying and animating facial signals for discourse in em-
bodied conversational agents. Computer Animation and
Virtual Worlds, 15(1):27–38.
P. Ekman. 1979. About brows: Emotional and conversational
signals. In M. von Cranach, K. Foppa, W. Lepenies, and
D. Ploog, editors, Human Ethology: Claims and limits of
a new discipline. Cambridge University Press.
M. E. Foster. 2006. Non-default choice in generation sys-
tems. Ph.D. thesis, School of Informatics, University of
Edinburgh. In preparation.
M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005.
Multimodal generation in the COMIC dialogue system. In
Proc. ACL 2005 Demo Session.
J. A. Hall. 1984. Nonverbal sex differences: Communication
accuracy and expressive style. The Johns Hopkins Uni-
versity Press.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disser-
tation.com.
E. Krahmer and M. Swerts. 2004. More about brows:
A cross-linguistic study via analysis-by-synthesis. In
C. Pelachaud and Z. Ruttkay, editors, From Brows
to Trust: Evaluating Embodied Conversational Agents,
pages 191–216. Kluwer.
J. Robertson, B. Cross, H. Macleod, and P. Wiemer-Hastings.
2004. Children’s interactions with animated agents in an
intelligent tutoring system. International Journal of Arti-
ficial Intelligence in Education, 14:335–357.
H. Shimodaira, K. Uematsu, S. Kawamoto, G. Hofer, and
M. Nakai. 2005. Analysis and synthesis of head motion
for lifelike conversational agents. In Proc. MLMI2005.
M. Steedman. 2000. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 31(4):649–689.
M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,
and C. Bregler. 2004. Speaking with hands: Creating an-
imated conversational characters from recordings of hu-
man performance. ACM Transactions on Graphics (TOG),
23(3):506–513.
M. White. 2005. Efficient realization of coordinate structures
in Combinatory Categorial Grammar. Research on Lan-
guage and Computation. To appear.
M. White, M. E. Foster, J. Oberlander, and A. Brown. 2005.
Using facial feedback to enhance turn-taking in a multi-
modal dialogue system. In Proc. HCI International 2005.
</reference>
<page confidence="0.998251">
360
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961479">
<title confidence="0.999453">Data-driven Generation of Emphatic Facial Displays</title>
<author confidence="0.999971">Mary Ellen Foster</author>
<affiliation confidence="0.999975">Department of Informatics, Technical University of Munich</affiliation>
<address confidence="0.995547">Boltzmannstraße 3, 85748 Garching, Germany</address>
<email confidence="0.999292">foster@in.tum.de</email>
<author confidence="0.9997">Jon Oberlander</author>
<affiliation confidence="0.999361">Institute for Communicating and Collaborative Systems School of Informatics, University of Edinburgh</affiliation>
<address confidence="0.980677">2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdom</address>
<email confidence="0.999056">jon@inf.ed.ac.uk</email>
<abstract confidence="0.999514833333333">We describe an implementation of datadriven selection of emphatic facial displays for an embodied conversational agent in a dialogue system. A corpus of sentences in the domain of the target dialogue system was recorded, and the facial displays used by the speaker were annotated. The data from those recordings was used in a range of models for generating facial displays, each model making use of a different amount of context or choosing displays differently within a context. The models were evaluated in two ways: by cross-validation against the corpus, and by asking users to rate the output. The predictions of the cross-validation study differed from the actual user ratings. While the cross-validation gave the highest scores to models making a majority choice within a context, the user study showed a significant preference for models that produced more variation. This preference was especially strong among the female subjects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proc. EACL</booktitle>
<contexts>
<context position="22272" citStr="Belz and Reiter (2006)" startWordPosition="3687" endWordPosition="3690">hs in Figure 6 show the head-to-head preferences in all contexts for both groups of subjects. 7 Discussion The predicted rankings from the cross-validation study differ from those in the human evaluation: while the cross-validation gave the highest scores to the majority-choice models, the human judges actually showed an overall preference for the weighted-choice models. This provides support for our hypothesis that humans would prefer generated output that reproduced more of the variation in the corpus, even if the choices made on specific sentences differ from those mode in the corpus. When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices. In general, this sort of automated measure will always tend to favour strategies that, on average, do not diverge far from what is found in the corpus, which indicates a drawback to using such measures alone to evaluate generation systems where variation is expected. The current study also suggests a further drawback to</context>
<context position="28050" citStr="Belz and Reiter (2006)" startWordPosition="4632" endWordPosition="4635">sed as part of the COMIC output-generation process, and which uses ngrams to guide its search for a good realisation. This will allow the system to choose the text and facial displays in parallel rather than sequentially. Such an integrated implementation has a better chance at capturing the complex interactions between the two output channels. Future evaluations should address several questions. First, we should gather users’ opinions of the behaviours annotated in the corpus: it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006). As well, further studies should look in more detail at the exact nature of the gender effect on user preferences, for instance by systematically varying the motion on different dimensions individually to see exactly which types of facial displays are liked and disliked by different demographic groups. Finally, if the extended n-gram-based model mentioned above is implemented, its performance should be measured and compared to that of the models described here, through both cross-validation and user studies. Acknowledgements Thanks to Matthew Stone, Michael White, and the anonymous EACL revie</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>A. Belz and E. Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proc. EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bickmore</author>
<author>J Cassell</author>
</authors>
<title>Social dialogue with embodied conversational agents.</title>
<date>2005</date>
<booktitle>Advances in Natural, Multimodal Dialogue Systems.</booktitle>
<editor>In J. van Kuppevelt, L. Dybkjær, and N. Bernsen, editors,</editor>
<publisher>Kluwer,</publisher>
<location>New York.</location>
<contexts>
<context position="24040" citStr="Bickmore and Cassell (2005)" startWordPosition="3975" endWordPosition="3978">: Gender influence on head-to-head preferences the male subjects did not express any significant preference either way, but had a mild preference for the majority-choice models. Previous studies on embodied conversational agents have exhibited gender effects that appear related this result: Robertson et al. (2004) found that, among schoolchildren, girls preferred a tutoring system that included an animated agent, while boys preferred one that did not; White et al. (2005) found that a more expressive talking head decreased male subjects’ task performance when using the full COMIC system; while Bickmore and Cassell (2005) found that women trusted the REA agent more in embodied mode, while men trusted her more over the telephone. Taken together, these results imply that male users prefer and perform better using an embodied agent that is less expressive and that shows less variation in its motions, and may even prefer a system that does not have an agent at all. These results are independent of the gender of the agent: the COMIC agent is male, REA is female, while the gender of Robertson’s agents was mixed. In any case, there is more general evidence that females have superior abilities in facial expression rec</context>
</contexts>
<marker>Bickmore, Cassell, 2005</marker>
<rawString>T. Bickmore and J. Cassell. 2005. Social dialogue with embodied conversational agents. In J. van Kuppevelt, L. Dybkjær, and N. Bernsen, editors, Advances in Natural, Multimodal Dialogue Systems. Kluwer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B de Carolis</author>
<author>V Carofiglio</author>
<author>C Pelachaud</author>
</authors>
<title>From discourse plans to believable behavior generation.</title>
<date>2002</date>
<booktitle>In Proc. INLG</booktitle>
<marker>de Carolis, Carofiglio, Pelachaud, 2002</marker>
<rawString>B. de Carolis, V. Carofiglio, and C. Pelachaud. 2002. From discourse plans to believable behavior generation. In Proc. INLG 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B de Carolis</author>
<author>C Pelachaud</author>
<author>I Poggi</author>
</authors>
<title>Verbal and nonverbal discourse planning.</title>
<date>2000</date>
<booktitle>In Proc. AAMAS 2000 Workshop “Achieving Human-Like Behavior in Interactive Animated Agents”.</booktitle>
<marker>de Carolis, Pelachaud, Poggi, 2000</marker>
<rawString>B. de Carolis, C. Pelachaud, and I. Poggi. 2000. Verbal and nonverbal discourse planning. In Proc. AAMAS 2000 Workshop “Achieving Human-Like Behavior in Interactive Animated Agents”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>T Bickmore</author>
<author>H Vilhj´almsson</author>
<author>H Yan</author>
</authors>
<title>More than just a pretty face: Conversational protocols and the affordances of embodiment. Knowledge-Based Systems,</title>
<date>2001</date>
<pages>14--1</pages>
<marker>Cassell, Bickmore, Vilhj´almsson, Yan, 2001</marker>
<rawString>J. Cassell, T. Bickmore, H. Vilhj´almsson, and H. Yan. 2001a. More than just a pretty face: Conversational protocols and the affordances of embodiment. Knowledge-Based Systems, 14(1–2):55–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>Y Nakano</author>
<author>T W Bickmore</author>
<author>C L Sidner</author>
<author>C Rich</author>
</authors>
<title>Non-verbal cues for discourse structure.</title>
<date>2001</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="5498" citStr="Cassell et al., 2001" startWordPosition="878" endWordPosition="881">ECAs is that they allow users to interact with a computer in the most natural possible setting: face-to-face conversation. However, to realise this advantage fully, the agent must produce high-quality output, both verbal and non-verbal. A number of previous systems have based the choice of non-verbal behaviours for an ECA on the behaviours of humans in conversational situations. The implementations vary as to how directly they use the human data. In some systems, motion specifications for the agent are created from scratch, using rules derived from studying human behaviour. For the REA agent (Cassell et al., 2001a), for example, gesturing behaviour was selected to perform particular communicative functions, using rules based on studies of typical North American non-verbal displays. Similarly, the Greta agent (de Carolis et al., 2002) selected its performative facial displays using hand-crafted rules to map from affective states to facial motions. Such implementations do not make direct use of any recorded human motions; this means that they generate average behaviours from a range of people, but it is difficult to adapt them to reproduce the behaviour of an individual. In contrast, other ECA implement</context>
<context position="7734" citStr="Cassell et al. (2001" startWordPosition="1234" endWordPosition="1237">tions, rather than based directly on human motions—but to create the instructions for that agent using the data from an annotated corpus of human behaviour. Like a motion-capture implementation, this technique can also produce increased naturalism in the output and also allows choices to be based on the motions of a single performer if necessary. However, annotating a video corpus can be less technically demanding than capturing and directly re-using real motions, especially when the corpus and the number of features under consideration are small. This approach has been taken, for example, by Cassell et al. (2001b) to choose posture shifts for REA, and by Kipp (2004) to select gestures for agents, and it is also the approach that we adopt here. 3 Recording and Annotation The recording script for the data collection consisted of 444 sentences in the domain of the COMIC multimodal dialogue system; all of the sentences described one or more features of one or more bathroom-tile designs. The sentences were generated by the full COMIC output planner, and were selected to provide coverage of all of the syntactic patterns available to the system. In addition to the surface text, each sentence included all of</context>
</contexts>
<marker>Cassell, Nakano, Bickmore, Sidner, Rich, 2001</marker>
<rawString>J. Cassell, Y. Nakano, T. W. Bickmore, C. L. Sidner, and C. Rich. 2001b. Non-verbal cues for discourse structure. In Proc. ACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>J Sullivan</author>
<author>S Prevost</author>
<author>E Churchill</author>
</authors>
<title>Embodied Conversational Agents.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4856" citStr="Cassell et al., 2000" startWordPosition="776" endWordPosition="779">ring the quality of the output generated by the various models: a cross-validation study against the corpus (Section 5) and a direct user evaluation of the output (Section 6). In Section 7, we discuss the results of these two evaluations. Finally, in Section 8, we draw some conclusions from the current study and outline potential follow-up work. 2 Choosing Non-Verbal Behaviour for Embodied Conversational Agents Embodied Conversational Agents (ECAs) are computer interfaces that are represented as human bodies, and that use their face and body in a human-like way in conversations with the user (Cassell et al., 2000). The main benefit of ECAs is that they allow users to interact with a computer in the most natural possible setting: face-to-face conversation. However, to realise this advantage fully, the agent must produce high-quality output, both verbal and non-verbal. A number of previous systems have based the choice of non-verbal behaviours for an ECA on the behaviours of humans in conversational situations. The implementations vary as to how directly they use the human data. In some systems, motion specifications for the agent are created from scratch, using rules derived from studying human behaviou</context>
</contexts>
<marker>Cassell, Sullivan, Prevost, Churchill, 2000</marker>
<rawString>J. Cassell, J. Sullivan, S. Prevost, and E. Churchill. 2000. Embodied Conversational Agents. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>K R Th´orisson</author>
</authors>
<title>The power of a nod and a glance: Envelope vs. emotional feedback in animated conversational agents.</title>
<date>1999</date>
<journal>Applied Artificial Intelligence,</journal>
<pages>13--4</pages>
<marker>Cassell, Th´orisson, 1999</marker>
<rawString>J. Cassell and K. R. Th´orisson. 1999. The power of a nod and a glance: Envelope vs. emotional feedback in animated conversational agents. Applied Artificial Intelligence, 13(4–5):519–538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A J Clark</author>
<author>K Richmond</author>
<author>S King</author>
</authors>
<title>Festival 2 – build your own general purpose unit selection speech synthesiser.</title>
<date>2004</date>
<booktitle>In Proc. 5th ISCA Workshop on Speech Synthesis.</booktitle>
<contexts>
<context position="18274" citStr="Clark et al., 2004" startWordPosition="3006" endWordPosition="3009">e then generated a facial schedule for each sentence using each of the six models. Note that, for some of the sentences, more than one model produced an identical sequence of facial displays, either because the majority choice in a broader context was the same as in a more narrow context, or because a weighted-choice model ended up selecting the majority option in every case. All such identical schedules were retained in the set of materials; in Section 6.2, we discuss their impact on the results. We then made videos of every schedule for every sentence, using the Festival speech synthesiser (Clark et al., 2004) and the RUTH talking head (DeCarlo et al., 2004). Figure 4 shows synthesised versions of the facial displays from Figure 2. Procedure 33 subjects took part in the experiment: 17 female subjects and 16 males. They were primarily undergraduate students, between 20 and 24 years old, native speakers of English, with an intermediate amount of computer experience. Each subject in the study was shown videos of all 30 sentences in an individually-chosen random order. For each sentence, the subject saw two versions, each generated by a different model, and was asked to choose which version they liked </context>
</contexts>
<marker>Clark, Richmond, King, 2004</marker>
<rawString>R. A. J. Clark, K. Richmond, and S. King. 2004. Festival 2 – build your own general purpose unit selection speech synthesiser. In Proc. 5th ISCA Workshop on Speech Synthesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Cunningham</author>
<author>M Kleiner</author>
<author>H H B¨ulthoff</author>
<author>C Wallraven</author>
</authors>
<title>The components of conversational facial expressions.</title>
<date>2004</date>
<booktitle>In Proc. APGV</booktitle>
<pages>143--150</pages>
<marker>Cunningham, Kleiner, B¨ulthoff, Wallraven, 2004</marker>
<rawString>D. W. Cunningham, M. Kleiner, H. H. B¨ulthoff, and C. Wallraven. 2004. The components of conversational facial expressions. In Proc. APGV 2004, pages 143–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D DeCarlo</author>
<author>M Stone</author>
<author>C Revilla</author>
<author>J Venditti</author>
</authors>
<title>Specifying and animating facial signals for discourse in embodied conversational agents. Computer Animation and Virtual Worlds,</title>
<date>2004</date>
<contexts>
<context position="18323" citStr="DeCarlo et al., 2004" startWordPosition="3015" endWordPosition="3018">tence using each of the six models. Note that, for some of the sentences, more than one model produced an identical sequence of facial displays, either because the majority choice in a broader context was the same as in a more narrow context, or because a weighted-choice model ended up selecting the majority option in every case. All such identical schedules were retained in the set of materials; in Section 6.2, we discuss their impact on the results. We then made videos of every schedule for every sentence, using the Festival speech synthesiser (Clark et al., 2004) and the RUTH talking head (DeCarlo et al., 2004). Figure 4 shows synthesised versions of the facial displays from Figure 2. Procedure 33 subjects took part in the experiment: 17 female subjects and 16 males. They were primarily undergraduate students, between 20 and 24 years old, native speakers of English, with an intermediate amount of computer experience. Each subject in the study was shown videos of all 30 sentences in an individually-chosen random order. For each sentence, the subject saw two versions, each generated by a different model, and was asked to choose which version they liked better. The displayed versions were counterbalanc</context>
</contexts>
<marker>DeCarlo, Stone, Revilla, Venditti, 2004</marker>
<rawString>D. DeCarlo, M. Stone, C. Revilla, and J. Venditti. 2004. Specifying and animating facial signals for discourse in embodied conversational agents. Computer Animation and Virtual Worlds, 15(1):27–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<title>About brows: Emotional and conversational signals.</title>
<date>1979</date>
<editor>In M. von Cranach, K. Foppa, W. Lepenies, and D. Ploog, editors, Human</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1495" citStr="Ekman (1979)" startWordPosition="225" endWordPosition="226">ated in two ways: by cross-validation against the corpus, and by asking users to rate the output. The predictions of the cross-validation study differed from the actual user ratings. While the cross-validation gave the highest scores to models making a majority choice within a context, the user study showed a significant preference for models that produced more variation. This preference was especially strong among the female subjects. 1 Introduction It has long been documented that there are characteristic facial displays that accompany the emphasised parts of spoken utterances. For example, Ekman (1979) says that eyebrow raises “appear to coincide with primary vocal stress, or more simply with a word that is spoken more loudly.” Correlations have also been found between prosodic features and events such as head nodding and the amplitude of mouth movements. When Krahmer and Swerts (2004) performed an empirical, cross-linguistic evaluation of the influence of brow movements on the perception of prosodic stress, they found that subjects preferred eyebrow movements to be correlated with the most prominent word in an utterance and that eyebrow movements boosted the perceived prominence of the wor</context>
<context position="10191" citStr="Ekman, 1979" startWordPosition="1635" endWordPosition="1637">ting; head nodding (up, small down, large down); head leaning (left and right); and head turning (left and right). Figure 2 shows examples of two typical display combinations. Any combination of these facial displays could be associated with any of the relevant segments in the text. The relevant segments included all mentions of tile-design properties (e.g., colours, designers), modifiers such as once again and also, deictic determiners (this, these), and verbs in contrastive contexts (e.g., are in Figure 1). The annotation scheme treated all facial displays as batons rather than underliners (Ekman, 1979); that is, each display was associated with a single segment. If a facial display spanned a longer phrase in the speech, it was annotated as a series of identical batons on each of the segments. Any predicted pitch accents and dialoguehistory and user-model information from the COMIC presentation planner were also associated with each segment, as appropriate. We chose not to restrict our annotation to those segments with predicted pitch accents, because the speaker also made a large number of facial displays on segments with no predicted pitch accent; instead, we incorporated the predicted acc</context>
</contexts>
<marker>Ekman, 1979</marker>
<rawString>P. Ekman. 1979. About brows: Emotional and conversational signals. In M. von Cranach, K. Foppa, W. Lepenies, and D. Ploog, editors, Human Ethology: Claims and limits of a new discipline. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Non-default choice in generation systems.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<note>In preparation.</note>
<contexts>
<context position="12150" citStr="Foster (2006)" startWordPosition="1957" endWordPosition="1958">e primarily turns to the right (Figure 2(a)) and leans to the left (Figure 2(b)). The distribution of facial displays in specific contexts differed from the overall distribution. The biggest influence was the user-model evaluation: left leans, brow lowering, and eye squinting were all relatively more frequent on objects with negative user-model evaluations, while right turns and brow raises occurred more often in positive contexts. Other factors also had an influence: for example, nodding and brow raises were both more frequent on segments for which the COMIC planner specified a pitch accent. Foster (2006) gives a detailed analysis of these recordings. 4 Modelling the Corpus Data We built a range of models using the data from the annotated corpus to select facial displays to accompany generated text. For each segment in the text, a model selected a display combination from among the displays used by the speaker in a similar context. All of the models used the corpus counts of displays associated with the segments directly, with no back-off or smoothing. The models differed from one another in two ways: the amount of context that they used, and the way in which they made a selection within a con</context>
</contexts>
<marker>Foster, 2006</marker>
<rawString>M. E. Foster. 2006. Non-default choice in generation systems. Ph.D. thesis, School of Informatics, University of Edinburgh. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
<author>M White</author>
<author>A Setzer</author>
<author>R Catizone</author>
</authors>
<title>Multimodal generation in the COMIC dialogue system.</title>
<date>2005</date>
<booktitle>In Proc. ACL</booktitle>
<location>Demo Session.</location>
<contexts>
<context position="3067" citStr="Foster et al., 2005" startWordPosition="475" endWordPosition="478">envelope” facial displays related to the process of conversation have a greater impact on successful interaction with an embodied conversational agent than do emotional displays. However, no description of face motion is sufficiently detailed that it can be used as the basis for selecting emphatic facial displays for an agent. This is therefore a task for which data-driven techniques are beneficial. In this paper, we address the task of selecting emphatic facial displays for the talking head in the COMIC1 multimodal dialogue system. In the basic COMIC process for generating multimodal output (Foster et al., 2005), facial displays are selected using simple rules based only on the pitch accents specified by the text generation system. In order to make a more sophisticated and naturalistic selection of facial displays, we recorded a single speaker reading a set of sentences drawn from the COMIC domain, and annotated the facial displays that he used and the contexts in which he used them. We then created models based on the data from this corpus and used them to choose the facial displays for the COMIC talking head. 1http://www.hcrc.ed.ac.uk/comic/ 353 The rest of this paper is arranged as follows. First,</context>
</contexts>
<marker>Foster, White, Setzer, Catizone, 2005</marker>
<rawString>M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005. Multimodal generation in the COMIC dialogue system. In Proc. ACL 2005 Demo Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hall</author>
</authors>
<title>Nonverbal sex differences: Communication accuracy and expressive style. The Johns Hopkins</title>
<date>1984</date>
<publisher>University Press.</publisher>
<contexts>
<context position="24661" citStr="Hall, 1984" startWordPosition="4086" endWordPosition="4087">t women trusted the REA agent more in embodied mode, while men trusted her more over the telephone. Taken together, these results imply that male users prefer and perform better using an embodied agent that is less expressive and that shows less variation in its motions, and may even prefer a system that does not have an agent at all. These results are independent of the gender of the agent: the COMIC agent is male, REA is female, while the gender of Robertson’s agents was mixed. In any case, there is more general evidence that females have superior abilities in facial expression recognition (Hall, 1984). 8 Conclusions and Future Work In this paper, we have demonstrated that there are patterns in the facial displays that this speaker used when giving different types of object descriptions in the COMIC system. The findings from the corpus analysis are compatible with previous findings on emphatic facial displays in general, and also provide a fine-grained analysis of the individual displays used by this speaker. Basing the recording scripts on the output of the presentation planner allowed full contextual information to be included in the annotated corpus; indeed, all of the contextual factors</context>
</contexts>
<marker>Hall, 1984</marker>
<rawString>J. A. Hall. 1984. Nonverbal sex differences: Communication accuracy and expressive style. The Johns Hopkins University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Gesture Generation by Imitation - From Human Behavior to Computer Character Animation.</title>
<date>2004</date>
<publisher>Dissertation.com.</publisher>
<contexts>
<context position="7789" citStr="Kipp (2004)" startWordPosition="1246" endWordPosition="1247"> the instructions for that agent using the data from an annotated corpus of human behaviour. Like a motion-capture implementation, this technique can also produce increased naturalism in the output and also allows choices to be based on the motions of a single performer if necessary. However, annotating a video corpus can be less technically demanding than capturing and directly re-using real motions, especially when the corpus and the number of features under consideration are small. This approach has been taken, for example, by Cassell et al. (2001b) to choose posture shifts for REA, and by Kipp (2004) to select gestures for agents, and it is also the approach that we adopt here. 3 Recording and Annotation The recording script for the data collection consisted of 444 sentences in the domain of the COMIC multimodal dialogue system; all of the sentences described one or more features of one or more bathroom-tile designs. The sentences were generated by the full COMIC output planner, and were selected to provide coverage of all of the syntactic patterns available to the system. In addition to the surface text, each sentence included all of the contextual information from the COMIC 354 46. More</context>
</contexts>
<marker>Kipp, 2004</marker>
<rawString>M. Kipp. 2004. Gesture Generation by Imitation - From Human Behavior to Computer Character Animation. Dissertation.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Swerts</author>
</authors>
<title>More about brows: A cross-linguistic study via analysis-by-synthesis.</title>
<date>2004</date>
<pages>191--216</pages>
<editor>In C. Pelachaud and Z. Ruttkay, editors, From</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="1784" citStr="Krahmer and Swerts (2004)" startWordPosition="272" endWordPosition="276">a context, the user study showed a significant preference for models that produced more variation. This preference was especially strong among the female subjects. 1 Introduction It has long been documented that there are characteristic facial displays that accompany the emphasised parts of spoken utterances. For example, Ekman (1979) says that eyebrow raises “appear to coincide with primary vocal stress, or more simply with a word that is spoken more loudly.” Correlations have also been found between prosodic features and events such as head nodding and the amplitude of mouth movements. When Krahmer and Swerts (2004) performed an empirical, cross-linguistic evaluation of the influence of brow movements on the perception of prosodic stress, they found that subjects preferred eyebrow movements to be correlated with the most prominent word in an utterance and that eyebrow movements boosted the perceived prominence of the word they were associated with. While many facial displays have been shown to co-occur with prosodic accents, the converse is not true: in normal embodied speech, many pitch accents and other prosodic events are unaccompanied by any facial display, and when displays are used, the selection v</context>
</contexts>
<marker>Krahmer, Swerts, 2004</marker>
<rawString>E. Krahmer and M. Swerts. 2004. More about brows: A cross-linguistic study via analysis-by-synthesis. In C. Pelachaud and Z. Ruttkay, editors, From Brows to Trust: Evaluating Embodied Conversational Agents, pages 191–216. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robertson</author>
<author>B Cross</author>
<author>H Macleod</author>
<author>P Wiemer-Hastings</author>
</authors>
<title>Children’s interactions with animated agents in an intelligent tutoring system.</title>
<date>2004</date>
<journal>International Journal of Artificial Intelligence in Education,</journal>
<pages>14--335</pages>
<contexts>
<context position="23728" citStr="Robertson et al. (2004)" startWordPosition="3926" endWordPosition="3929"> 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0. 0.05 0 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 Majority Weient�d Majority WeleNed FM ve FW SM ve SW NM ve NW Overall FM ve FW SM ve SW NM ve NW Overall Amount of context Amount of context (a) Male subjects (b) Female subjects Figure 6: Gender influence on head-to-head preferences the male subjects did not express any significant preference either way, but had a mild preference for the majority-choice models. Previous studies on embodied conversational agents have exhibited gender effects that appear related this result: Robertson et al. (2004) found that, among schoolchildren, girls preferred a tutoring system that included an animated agent, while boys preferred one that did not; White et al. (2005) found that a more expressive talking head decreased male subjects’ task performance when using the full COMIC system; while Bickmore and Cassell (2005) found that women trusted the REA agent more in embodied mode, while men trusted her more over the telephone. Taken together, these results imply that male users prefer and perform better using an embodied agent that is less expressive and that shows less variation in its motions, and ma</context>
</contexts>
<marker>Robertson, Cross, Macleod, Wiemer-Hastings, 2004</marker>
<rawString>J. Robertson, B. Cross, H. Macleod, and P. Wiemer-Hastings. 2004. Children’s interactions with animated agents in an intelligent tutoring system. International Journal of Artificial Intelligence in Education, 14:335–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shimodaira</author>
<author>K Uematsu</author>
<author>S Kawamoto</author>
<author>G Hofer</author>
<author>M Nakai</author>
</authors>
<title>Analysis and synthesis of head motion for lifelike conversational agents.</title>
<date>2005</date>
<booktitle>In Proc. MLMI2005.</booktitle>
<contexts>
<context position="6619" citStr="Shimodaira et al. (2005)" startWordPosition="1051" endWordPosition="1054">s difficult to adapt them to reproduce the behaviour of an individual. In contrast, other ECA implementations have selected non-verbal behaviour based directly on motion-capture recordings of humans. Stone et al. (2004), for example, recorded an actor performing scripted output in the domain of the target system. They then segmented the recordings into coherent phrases and annotated them with the relevant semantic and pragmatic information, and combined the segments at run-time to produce complete performance specifications that were then played back on the agent. Cunningham et al. (2004) and Shimodaira et al. (2005) used similar techniques to base the appearance and motions of their talking heads directly on recordings of human faces. This technique is able to produce more naturalistic output than the more rule-based systems described above; however, capturing the motion requires specialised hardware, and the agent must be implemented in such a way that it can exactly reproduce the human motions. A middle ground is to use a purely synthetic agent—one whose behaviour is controlled by high-level instructions, rather than based directly on human motions—but to create the instructions for that agent using th</context>
</contexts>
<marker>Shimodaira, Uematsu, Kawamoto, Hofer, Nakai, 2005</marker>
<rawString>H. Shimodaira, K. Uematsu, S. Kawamoto, G. Hofer, and M. Nakai. 2005. Analysis and synthesis of head motion for lifelike conversational agents. In Proc. MLMI2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Information structure and the syntaxphonology interface.</title>
<date>2000</date>
<journal>Linguistic Inquiry,</journal>
<volume>31</volume>
<issue>4</issue>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. Information structure and the syntaxphonology interface. Linguistic Inquiry, 31(4):649–689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>D DeCarlo</author>
<author>I Oh</author>
<author>C Rodriguez</author>
<author>A Lees</author>
<author>A Stere</author>
<author>C Bregler</author>
</authors>
<title>Speaking with hands: Creating animated conversational characters from recordings of human performance.</title>
<date>2004</date>
<journal>ACM Transactions on Graphics (TOG),</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="6214" citStr="Stone et al. (2004)" startWordPosition="987" endWordPosition="990">using rules based on studies of typical North American non-verbal displays. Similarly, the Greta agent (de Carolis et al., 2002) selected its performative facial displays using hand-crafted rules to map from affective states to facial motions. Such implementations do not make direct use of any recorded human motions; this means that they generate average behaviours from a range of people, but it is difficult to adapt them to reproduce the behaviour of an individual. In contrast, other ECA implementations have selected non-verbal behaviour based directly on motion-capture recordings of humans. Stone et al. (2004), for example, recorded an actor performing scripted output in the domain of the target system. They then segmented the recordings into coherent phrases and annotated them with the relevant semantic and pragmatic information, and combined the segments at run-time to produce complete performance specifications that were then played back on the agent. Cunningham et al. (2004) and Shimodaira et al. (2005) used similar techniques to base the appearance and motions of their talking heads directly on recordings of human faces. This technique is able to produce more naturalistic output than the more </context>
</contexts>
<marker>Stone, DeCarlo, Oh, Rodriguez, Lees, Stere, Bregler, 2004</marker>
<rawString>M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere, and C. Bregler. 2004. Speaking with hands: Creating animated conversational characters from recordings of human performance. ACM Transactions on Graphics (TOG), 23(3):506–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
</authors>
<title>Efficient realization of coordinate structures in Combinatory Categorial Grammar. Research on Language and Computation.</title>
<date>2005</date>
<note>To appear.</note>
<contexts>
<context position="27408" citStr="White, 2005" startWordPosition="4529" endWordPosition="4530">the implementation described here, we built simple models based directly on the corpus counts and used them to select facial displays to accom359 pany previously-generated text; both of these aspects of the implementation can be extended in future. If we build more sophisticated n-grambased models of the facial displays, using a full language-modelling toolkit, we can take into account contextual information from words other than those in a single segment, and back off smoothly through different amounts of context. Such models can also be integrated directly into the OpenCCG surface realiser (White, 2005)— which is already used as part of the COMIC output-generation process, and which uses ngrams to guide its search for a good realisation. This will allow the system to choose the text and facial displays in parallel rather than sequentially. Such an integrated implementation has a better chance at capturing the complex interactions between the two output channels. Future evaluations should address several questions. First, we should gather users’ opinions of the behaviours annotated in the corpus: it may be that subjects actually prefer the generated facial displays to the displays in the corp</context>
</contexts>
<marker>White, 2005</marker>
<rawString>M. White. 2005. Efficient realization of coordinate structures in Combinatory Categorial Grammar. Research on Language and Computation. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>M E Foster</author>
<author>J Oberlander</author>
<author>A Brown</author>
</authors>
<title>Using facial feedback to enhance turn-taking in a multimodal dialogue system.</title>
<date>2005</date>
<booktitle>In Proc. HCI International</booktitle>
<contexts>
<context position="23888" citStr="White et al. (2005)" startWordPosition="3952" endWordPosition="3955"> SM ve SW NM ve NW Overall FM ve FW SM ve SW NM ve NW Overall Amount of context Amount of context (a) Male subjects (b) Female subjects Figure 6: Gender influence on head-to-head preferences the male subjects did not express any significant preference either way, but had a mild preference for the majority-choice models. Previous studies on embodied conversational agents have exhibited gender effects that appear related this result: Robertson et al. (2004) found that, among schoolchildren, girls preferred a tutoring system that included an animated agent, while boys preferred one that did not; White et al. (2005) found that a more expressive talking head decreased male subjects’ task performance when using the full COMIC system; while Bickmore and Cassell (2005) found that women trusted the REA agent more in embodied mode, while men trusted her more over the telephone. Taken together, these results imply that male users prefer and perform better using an embodied agent that is less expressive and that shows less variation in its motions, and may even prefer a system that does not have an agent at all. These results are independent of the gender of the agent: the COMIC agent is male, REA is female, whi</context>
</contexts>
<marker>White, Foster, Oberlander, Brown, 2005</marker>
<rawString>M. White, M. E. Foster, J. Oberlander, and A. Brown. 2005. Using facial feedback to enhance turn-taking in a multimodal dialogue system. In Proc. HCI International 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>