<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007780">
<title confidence="0.9205345">
Can Markov Models Over Minimal Translation Units Help Phrase-Based
SMT?
</title>
<author confidence="0.993018">
Nadir Durrani Alexander Fraser Helmut Schmid
</author>
<affiliation confidence="0.996277">
University of Edinburgh Ludwig Maximilian University Munich
</affiliation>
<email confidence="0.977085">
dnadir@inf.ed.ac.uk fraser,schmid@cis.uni-muenchen.de
</email>
<author confidence="0.999397">
Hieu Hoang Philipp Koehn
</author>
<affiliation confidence="0.999157">
University of Edinburgh
</affiliation>
<email confidence="0.998308">
hieu.hoang,pkoehn@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993889" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853">
The phrase-based and N-gram-based
SMT frameworks complement each other.
While the former is better able to memo-
rize, the latter provides a more principled
model that captures dependencies across
phrasal boundaries. Some work has been
done to combine insights from these two
frameworks. A recent successful attempt
showed the advantage of using phrase-
based search on top of an N-gram-based
model. We probe this question in the
reverse direction by investigating whether
integrating N-gram-based translation and
reordering models into a phrase-based
decoder helps overcome the problematic
phrasal independence assumption. A large
scale evaluation over 8 language pairs
shows that performance does significantly
improve.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883703703704">
Phrase-based models (Koehn et al., 2003; Och
and Ney, 2004) learn local dependencies such as
reorderings, idiomatic collocations, deletions and
insertions by memorization. A fundamental draw-
back is that phrases are translated and reordered
independently of each other and contextual infor-
mation outside of phrasal boundaries is ignored.
The monolingual language model somewhat re-
duces this problem. However i) often the language
model cannot overcome the dispreference of the
translation model for nonlocal dependencies, ii)
source-side contextual dependencies are still ig-
nored and iii) generation of lexical translations and
reordering is separated.
The N-gram-based SMT framework addresses
these problems by learning Markov chains over se-
quences of minimal translation units (MTUs) also
known as tuples (Mari˜no et al., 2006) or over op-
erations coupling lexical generation and reorder-
ing (Durrani et al., 2011). Because the mod-
els condition the MTU probabilities on the previ-
ous MTUs, they capture non-local dependencies
and both source and target contextual information
across phrasal boundaries.
In this paper we study the effect of integrating
tuple-based N-gram models (TSM) and operation-
based N-gram models (OSM) into the phrase-
based model in Moses, a state-of-the-art phrase-
based system. Rather than using POS-based
rewrite rules (Crego and Mari˜no, 2006) to form
a search graph, we use the ability of the phrase-
based system to memorize larger translation units
to replicate the effect of source linearization as
done in the TSM model.
We also show that using phrase-based search
with MTU N-gram translation models helps to ad-
dress some of the search problems that are non-
trivial to handle when decoding with minimal
translation units. An important limitation of the
OSM N-gram model is that it does not handle un-
aligned or discontinuous target MTUs and requires
post-processing of the alignment to remove these.
Using phrases during search enabled us to make
novel changes to the OSM generative story (also
applicable to the TSM model) to handle unaligned
target words and to use target linearization to deal
with discontinuous target MTUs.
We performed an extensive evaluation, carrying
out translation experiments from French, Spanish,
Czech and Russian to English and in the opposite
direction. Our integration of the OSM model into
Moses and our modification of the OSM model to
deal with unaligned and discontinuous target to-
kens consistently improves BLEU scores over the
</bodyText>
<page confidence="0.988608">
399
</page>
<bodyText confidence="0.6512425">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399–405,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
baseline system, and shows statistically significant
improvements in seven out of eight cases.
</bodyText>
<sectionHeader confidence="0.988502" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999983953488372">
Several researchers have tried to combine the ideas
of phrase-based and N-gram-based SMT. Costa-
juss`a et al. (2007) proposed a method for combin-
ing the two approaches by applying sentence level
reranking. Feng et al. (2010) added a linearized
source-side language model in a phrase-based sys-
tem. Crego and Yvon (2010) modified the phrase-
based lexical reordering model of Tillman (2004)
for an N-gram-based system. Niehues et al. (2011)
integrated a bilingual language model based on
surface word forms and POS tags into a phrase-
based system. Zhang et al. (2013) explored multi-
ple decomposition structures for generating MTUs
in the task of lexical selection, and to rerank the
N-best candidate translations in the output of a
phrase-based. A drawback of the TSM model is
the assumption that source and target information
is generated monotonically. The process of re-
ordering is disconnected from lexical generation
which restricts the search to a small set of precom-
puted reorderings. Durrani et al. (2011) addressed
this problem by coupling lexical generation and
reordering information into a single generative
process and enriching the N-gram models to learn
lexical reordering triggers. Durrani et al. (2013)
showed that using larger phrasal units during de-
coding is superior to MTU-based decoding in an
N-gram-based system. However, they do not use
phrase-based models in their work, relying only
on the OSM model. This paper combines insights
from these recent pieces of work and show that
phrase-based search combined with N-gram-based
and phrase-based models in decoding is the over-
all best way to go. We integrate the two N-gram-
based models, TSM and OSM, into phrase-based
Moses and show that the translation quality is im-
proved by taking both translation and reordering
context into account. Other approaches that ex-
plored such models in syntax-based systems used
MTUs for sentence level reranking (Khalilov and
Fonollosa, 2009), in dependency translation mod-
els (Quirk and Menezes, 2006) and in target lan-
guage syntax systems (Vaswani et al., 2011).
</bodyText>
<sectionHeader confidence="0.95647" genericHeader="method">
3 Integration of N-gram Models
</sectionHeader>
<bodyText confidence="0.997012">
We now describe our integration of TSM and
OSM N-gram models into the phrase-based sys-
</bodyText>
<figureCaption confidence="0.850549">
Figure 1: Example (a) Word Alignments (b) Un-
folded MTU Sequence (c) Operation Sequence (d)
Step-wise Generation
</figureCaption>
<bodyText confidence="0.837783875">
tem. Given a bilingual sentence pair (F, E) and
its alignment (A), we first identify minimal trans-
lation units (MTUs) from it. An MTU is defined
as a translation rule that cannot be broken down
any further. The MTUs extracted from Figure 1(a)
are A → a,B → b,C ... H → c1 and D → d.
These units are then generated left-to-right in two
different ways, as we will describe next.
</bodyText>
<subsectionHeader confidence="0.997173">
3.1 Tuple Sequence Model (TSM)
</subsectionHeader>
<bodyText confidence="0.999505428571429">
The TSM translation model assumes that MTUs
are generated monotonically. To achieve this ef-
fect, we enumerate the MTUs in the target left-
to-right order. This process is also called source
linearization or tuple unfolding. The resulting se-
quence of monotonic MTUs is shown in Figure
1(b). We then define a TSM model over this se-
</bodyText>
<equation confidence="0.91756725">
quence (t1, t2, ... , tJ) as:
J
ptsm(F, E, A) = p(tj|tj−n+1, ..., tj−1)
j=1
</equation>
<bodyText confidence="0.999876666666667">
where n indicates the amount of context used. A
4-gram Kneser-Ney smoothed language model is
trained with SRILM (Stolcke, 2002).
Search: In previous work, the search graph in
TSM N-gram SMT was not built dynamically
like in the phrase-based system, but instead con-
structed as a preprocessing step using POS-based
rewrite rules (learned when linearizing the source
side). We do not adopt this framework. We use
</bodyText>
<footnote confidence="0.839798">
1We use ... to denote discontinuous MTUs.
</footnote>
<page confidence="0.992889">
400
</page>
<bodyText confidence="0.999991352941177">
phrase-based search which builds up the decoding
graph dynamically and searches through all pos-
sible reorderings within a fixed window. During
decoding we use the phrase-internal alignments to
perform source linearization. For example, if dur-
ing decoding we would like to apply the phrase
pair “C D H – d c”, a combination of t3 and t4 in
Figure 1(b), then we extract the MTUs from this
phrase-pair and linearize the source to be in the
order of the target. We then compute the TSM
probability given the n − 1 previous MTUs (in-
cluding MTUs occurring in the previous source
phrases). The idea is to replicate rewrite rules
with phrase-pairs to linearize the source. Previ-
ous work on N-gram-based models restricted the
length of the rewrite rules to be 7 or less POS tags.
We use phrases of length 6 and less.
</bodyText>
<subsectionHeader confidence="0.998093">
3.2 Operation Sequence Model (OSM)
</subsectionHeader>
<bodyText confidence="0.999791105263158">
The OSM model represents a bilingual sentence
pair and its alignment through a sequence of oper-
ations that generate the aligned sentence pair. An
operation either generates source and target words
or it performs reordering by inserting gaps and
jumping forward and backward. The MTUs are
generated in the target left-to-right order just as in
the TSM model. However rather than linearizing
the source-side, reordering operations (gaps and
jumps) are used to handle crossing alignments.
During training, each bilingual sentence pair is de-
terministically converted to a unique sequence of
operations.2 The example in Figure 1(a) is con-
verted to the sequence of operations shown in Fig-
ure 1(c). A step-wise generation of MTUs along
with reordering operations is shown in Figure 1(d).
We learn a Markov model over a sequence of oper-
ations (o1, o2, ... , oJ) that encapsulate MTUs and
reordering information which is defined as fol-
</bodyText>
<equation confidence="0.931429666666667">
lows: J
posm(F, E, A) = H p(oj|oj−n+1, ..., oj−1)
j=1
</equation>
<bodyText confidence="0.999634428571429">
A 9-gram Kneser-Ney smoothed language model
is trained with SRILM.3 By coupling reorder-
ing with lexical generation, each (translation or
reordering) decision conditions on n − 1 previ-
ous (translation and reordering) decisions span-
ning across phrasal boundaries. The reordering
decisions therefore influence lexical selection and
</bodyText>
<footnote confidence="0.983113">
2Please refer to Durrani et al. (2011) for a list of opera-
tions and the conversion algorithm.
3We also tried a 5-gram model, the performance de-
creased slightly in some cases.
</footnote>
<bodyText confidence="0.993666052631579">
vice versa. A heterogeneous mixture of translation
and reordering operations enables the OSM model
to memorize reordering patterns and lexicalized
triggers unlike the TSM model where translation
and reordering are modeled separately.
Search: We integrated the generative story of
the OSM model into the hypothesis extension pro-
cess of the phrase-based decoder. Each hypothesis
maintains the position of the source word covered
by the last generated MTU, the right-most source
word generated so far, the number of open gaps
and their relative indexes, etc. This information
is required to generate the operation sequence for
the MTUs in the hypothesized phrase-pair. After
the operation sequence is generated, we compute
its probability given the previous operations. We
define the main OSM feature, and borrow 4 sup-
portive features, the Gap, Open Gap, Gap-width
and Deletion penalties (Durrani et al., 2011).
</bodyText>
<subsectionHeader confidence="0.9581925">
3.3 Problem: Target Discontinuity and
Unaligned Words
</subsectionHeader>
<bodyText confidence="0.999953586206897">
Two issues that we have ignored so far are the han-
dling of MTUs which have discontinuous targets,
and the handling of unaligned target words. Both
TSM and OSM N-gram models generate MTUs
linearly in left-to-right order. This assumption be-
comes problematic in the cases of MTUs that have
target-side discontinuities (See Figure 2(a)). The
MTU A -+ g ... a can not be generated because of
the intervening MTUs B -+ b, C ... H -+ c and
D -+ d. In the original TSM model, such cases are
dealt with by merging all the intervening MTUs
to form a bigger unit ti in Figure 2(c). A solu-
tion that uses split-rules is proposed by Crego and
Yvon (2009) but has not been adopted in Ncode
(Crego et al., 2011), the state-of-the-art TSM N-
gram system. Durrani et al. (2011) dealt with
this problem by applying a post-processing (PP)
heuristic that modifies the alignments to remove
such cases. When a source word is aligned to a
discontinuous target-cept, first the link to the least
frequent target word is identified, and the group
of links containing this word is retained while the
others are deleted. The alignment in Figure 2(a),
for example, is transformed to that in Figure 2(b).
This allows OSM to extract the intervening MTUs
t2 ... t5 (Figure 2(c)). Note that this problem does
not exist when dealing with source-side disconti-
nuities: the TSM model linearizes discontinuous
source-side MTUs such as C ... H -+ c. The
</bodyText>
<page confidence="0.997195">
401
</page>
<figureCaption confidence="0.978107">
Figure 2: Example (a) Original Alignments (b)
Post-Processed Alignments (c) Extracted MTUs –
ti ... t3 (from (a)) and t1 ... t7 (from (b))
</figureCaption>
<bodyText confidence="0.999907684210526">
OSM model deals with such cases through Insert
Gap and Continue Cept operations.
The second problem is the unaligned target-side
MTUs such as ε -+ f in Figure 2(a). Inserting
target-side words “spuriously” during decoding is
a non-trival problem because there is no evidence
of when to hypothesize such words. These cases
are dealt with in N-gram-based SMT by merging
such MTUs to the MTU on the left or right based
on attachment counts (Durrani et al., 2011), lexical
probabilities obtained from IBM Model 1 (Mari˜no
et al., 2006), or POS entropy (Gispert and Mari˜no,
2006). Notice how ε -+ f (Figure 2(a)) is merged
with the neighboring MTU E -+ e to form a new
MTU E -+ ef (Figure 2 (c)). We initially used the
post-editing heuristic (PP) as defined by Durrani et
al. (2011) for both TSM and OSM N-gram mod-
els, but found that it lowers the translation quality
(See Row 2 in Table 2) in some language pairs.
</bodyText>
<subsectionHeader confidence="0.978884">
3.4 Solution: Insertion and Linearization
</subsectionHeader>
<bodyText confidence="0.9999846">
To deal with these problems, we made novel modi-
fications to the generative story of the OSM model.
Rather than merging the unaligned target MTU
such as ε − f, to its right or left MTU, we gen-
erate it through a new Generate Target Only (f)
operation. Orthogonal to its counterpart Generate
Source Only (I) operation (as used for MTU t7 in
Figure 2 (c)), this operation is generated as soon
as the MTU containing its previous target word
is generated. In Figure 2(a), ε − f is generated
immediately after MTU E − e is generated. In
a sequence of unaligned source and target MTUs,
unaligned source MTUs are generated before the
unaligned target MTUs. We do not modify the de-
coder to arbitrarily generate unaligned MTUs but
hypothesize these only when they appear within
an extracted phrase-pair. The constraint provided
by the phrase-based search makes the Generate
Target Only operation tractable. Using phrase-
based search therefore helps addressing some of
the problems that exist in the decoding framework
of N-gram SMT.
The remaining problem is the discontinuous tar-
get MTUs such as A -+ g ... a in Figure 2(a). We
handle this with target linearization similar to the
TSM source linearization. We collapse the target
words g and a in the MTU A -+ g ... a to occur
consecutively when generating the operation se-
quence. The conversion algorithm that generates
the operations thinks that g and a occurred adja-
cently. During decoding we use the phrasal align-
ments to linearize such MTUs within a phrasal
unit. This linearization is done only to compute
the OSM feature. Other features in the phrase-
based system (e.g., language model) work with the
target string in its original order. Notice again how
memorizing larger translation units using phrases
helps us reproduce such patterns. This is achieved
in the tuple N-gram model by using POS-based
split and rewrite rules.
</bodyText>
<sectionHeader confidence="0.998703" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.962228">
Corpus: We ran experiments with data made
available for the translation task of the Eighth
Workshop on Statistical Machine Translation. The
sizes of bitext used for the estimation of translation
and monolingual language models are reported in
</bodyText>
<tableCaption confidence="0.964954">
Table 1. All data is true-cased.
</tableCaption>
<table confidence="0.999882833333333">
Pair Parallel Monolingual Lang
fr–en P 39 M P 91 M fr
cs–en P 15.6 M P 43.4 M cs
es–en P 15.2 M P 65.7 M es
ru–en P 2 M P 21.7 M ru
P 287.3 M en
</table>
<tableCaption confidence="0.826391">
Table 1: Number of Sentences (in Millions) used
for Training
</tableCaption>
<bodyText confidence="0.9999047">
We follow the approach of Schwenk and Koehn
(2008) and trained domain-specific language mod-
els separately and then linearly interpolated them
using SRILM with weights optimized on the held-
out dev-set. We concatenated the news-test sets
from four years (2008-2011) to obtain a large dev-
setin order to obtain more stable weights (Koehn
and Haddow, 2012). For Russian-English and
English-Russian language pairs, we divided the
tuning-set news-test 2012 into two halves and used
</bodyText>
<page confidence="0.996887">
402
</page>
<table confidence="0.9994885">
No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru
Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88
1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05
1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96
1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22
1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25
</table>
<tableCaption confidence="0.999743">
Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline
</tableCaption>
<bodyText confidence="0.999667047619048">
the first half for tuning and second for test. We test
our systems on news-test 2012. We tune with the
k-best batch MIRA algorithm (Cherry and Foster,
2012).
Moses Baseline: We trained a Moses system
(Koehn et al., 2007) with the following settings:
maximum sentence length 80, grow-diag-final-
and symmetrization of GIZA++ alignments, an
interpolated Kneser-Ney smoothed 5-gram lan-
guage model with KenLM (Heafield, 2011) used at
runtime, msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), distortion limit of 6, 100-best
translation options, minimum bayes-risk decoding
(Kumar and Byrne, 2004), cube-pruning (Huang
and Chiang, 2007) and the no-reordering-over-
punctuation heuristic.
Results: Table 2 shows uncased BLEU scores
(Papineni et al., 2002) on the test set. Row 2 (+pp)
shows that the post-editing of alignments to re-
move unaligned and discontinuous target MTUs
decreases the performance in the case of ru-en, cs-
en and en-fr. Row 3 (+pp+tsm) shows that our in-
tegration of the TSM model slightly improves the
BLEU scores for en-fr, and es-en. Results drop
in ru-en and en-ru. Row 4 (+pp+osm) shows that
the OSM model consistently improves the BLEU
scores over the Baseline systems (Row 1) giving
significant improvements in half the cases. The
only result that is lower than the baseline system
is that of the ru-en experiment, because OSM is
built with PP alignments which particularly hurt
the performance for ru-en. Finally Row 5 (+osm*)
shows that our modifications to the OSM model
(Section 3.4) give the best result ranging from
[0.24−0.65] with statistically significant improve-
ments in seven out of eight cases. It also shows im-
provements over Row 4 (+pp+osm) even in some
cases where the PP heuristic doesn’t hurt. The
largest gains are obtained in the ru-en translation
task (where the PP heuristic inflicted maximum
damage).
</bodyText>
<sectionHeader confidence="0.970845" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999797965517241">
We have addressed the problem of the indepen-
dence assumption in PBSMT by integrating N-
gram-based models inside a phrase-based system
using a log-linear framework. We try to replicate
the effect of rewrite and split rules as used in the
TSM model through phrasal alignments. We pre-
sented a novel extension of the OSM model to
handle unaligned and discontinuous target MTUs
in the OSM model. Phrase-based search helps us
to address these problems that are non-trivial to
handle in the decoding frameworks of the N-gram-
based models. We tested our extentions and modi-
fications by evaluating against a competitive base-
line system over 8 language pairs. Our integra-
tion of TSM shows small improvements in a few
cases. The OSM model which takes both reorder-
ing and lexical context into consideration consis-
tently improves the performance of the baseline
system. Our modification to the OSM model pro-
duces the best results giving significant improve-
ments in most cases. Although our modifications
to the OSM model enables discontinuous MTUs,
we did not fully utilize these during decoding, as
Moses only uses continous phrases. The discon-
tinuous MTUs that span beyond a phrasal length
of 6 words are therefore never hypothesized. We
would like to explore this further by extending the
search to use discontinuous phrases (Galley and
Manning, 2010).
</bodyText>
<sectionHeader confidence="0.984425" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999976818181818">
We would like to thank the anonymous reviewers
for their helpful feedback and suggestions. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment n◦ 287658. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732. This
publication only reflects the authors views.
</bodyText>
<page confidence="0.998428">
403
</page>
<sectionHeader confidence="0.4706" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969505669724771">
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436, Montr´eal, Canada, June. Associa-
tion for Computational Linguistics.
Marta R. Costa-juss`a, Josep M. Crego, David Vilar,
Jos´e A.R. Fonollosa, Jos´e B. Mari˜no, and Her-
mann Ney. 2007. Analysis and System Combina-
tion of Phrase- and N-Gram-Based Statistical Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Compu-
tational Linguistics; Companion Volume, Short Pa-
pers, pages 137–140, Rochester, New York, April.
Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving
Statistical MT by Coupling Reordering and Decod-
ing. Machine Translation, 20(3):199–215.
Josep M. Crego and Franc¸ois Yvon. 2009. Gappy
Translation Units under Left-to-Right SMT Decod-
ing. In Proceedings of the Meeting of the European
Association for Machine Translation (EAMT), pages
66–73, Barcelona, Spain.
Josep M. Crego and Franc¸ois Yvon. 2010. Improv-
ing Reordering with Linguistically Informed Bilin-
gual N-Grams. In Coling 2010: Posters, pages 197–
205, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no.
2011. Ncode: an Open Source Bilingual N-gram
SMT Toolkit. The Prague Bulletin of Mathematical
Linguistics, 96:49–58.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045–1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model With Minimal Translation Units, But
Decode With Phrases. In The 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A Source-side Decoding Sequence Model for Statis-
tical Machine Translation. In Conference of the As-
sociation for Machine Translation in the Americas
2010, Denver, Colorado, USA, October.
Michel Galley and Christopher D. Manning. 2010.
Accurate Non-Hierarchical Phrase-Based Transla-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 966–974, Los Angeles, California, June. As-
sociation for Computational Linguistics.
Adri`a Gispert and Jos´e B. Mari˜no. 2006. Linguis-
tic Tuple Segmentation in N-Gram-Based Statistical
Machine Translation. In INTERSPEECH.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268–275.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187–197, Edinburgh, Scotland, United King-
dom, 7.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144–151, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Maxim Khalilov and Jos´e A. R. Fonollosa. 2009. N-
Gram-Based Statistical Machine Translation Versus
Syntax Augmented Machine Translation: Compar-
ison and System Combination. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 424–432, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 317–
321, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127–133, Edmonton,
Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007 Demonstrations, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July.
Shankar Kumar and William J. Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In HLT-NAACL, pages 169–176.
</reference>
<page confidence="0.989922">
404
</page>
<reference confidence="0.999876666666666">
Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonol-
losa, and Marta R. Costa-juss`a. 2006. N-gram-
Based Machine Translation. Computational Lin-
guistics, 32(4):527–549.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 198–206, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(1):417–449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Morristown, NJ, USA.
Christopher Quirk and Arul Menezes. 2006. Do We
Need Phrases? Challenging the Conventional Wis-
dom in Statistical Machine Translation. In HLT-
NAACL.
Holger Schwenk and Philipp Koehn. 2008. Large and
Diverse Language Models for Statistical Machine
Translation. In International Joint Conference on
Natural Language Processing, pages 661–666, Jan-
uary 2008.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado.
Christoph Tillman. 2004. A Unigram Orienta-
tion Model for Statistical Machine Translation. In
HLT-NAACL 2004: Short Papers, pages 101–104,
Boston, Massachusetts.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-
to-String Translation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 856–864, Portland, Oregon, USA, June.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond Left-to-Right: Multi-
ple Decomposition Structures for SMT. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Atlanta, Georgia,
USA, June. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.999031">
405
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.501673">
<title confidence="0.9931085">Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</title>
<author confidence="0.993831">Nadir Durrani Alexander Fraser Helmut Schmid</author>
<affiliation confidence="0.999973">University of Edinburgh Ludwig Maximilian University</affiliation>
<email confidence="0.568845">dnadir@inf.ed.ac.ukfraser,schmid@cis.uni-muenchen.de</email>
<author confidence="0.9992">Hieu Hoang Philipp Koehn</author>
<affiliation confidence="0.99995">University of Edinburgh</affiliation>
<email confidence="0.995894">hieu.hoang,pkoehn@inf.ed.ac.uk</email>
<abstract confidence="0.99509505">The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="16641" citStr="Cherry and Foster, 2012" startWordPosition="2705" endWordPosition="2708"> two halves and used 402 No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05 1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96 1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25 Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline the first half for tuning and second for test. We test our systems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marta R Costa-juss`a</author>
<author>Josep M Crego</author>
<author>David Vilar</author>
<author>Jos´e A R Fonollosa</author>
<author>Jos´e B Mari˜no</author>
<author>Hermann Ney</author>
</authors>
<title>Analysis and System Combination of Phrase- and N-Gram-Based Statistical Machine Translation Systems.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers,</booktitle>
<pages>137--140</pages>
<location>Rochester, New York,</location>
<marker>Costa-juss`a, Crego, Vilar, Fonollosa, Mari˜no, Ney, 2007</marker>
<rawString>Marta R. Costa-juss`a, Josep M. Crego, David Vilar, Jos´e A.R. Fonollosa, Jos´e B. Mari˜no, and Hermann Ney. 2007. Analysis and System Combination of Phrase- and N-Gram-Based Statistical Machine Translation Systems. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers, pages 137–140, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Improving Statistical MT by Coupling Reordering and Decoding.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>Josep M. Crego and Jos´e B. Mari˜no. 2006. Improving Statistical MT by Coupling Reordering and Decoding. Machine Translation, 20(3):199–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Gappy Translation Units under Left-to-Right SMT Decoding.</title>
<date>2009</date>
<booktitle>In Proceedings of the Meeting of the European Association for Machine Translation (EAMT),</booktitle>
<pages>66--73</pages>
<location>Barcelona,</location>
<contexts>
<context position="11330" citStr="Crego and Yvon (2009)" startWordPosition="1797" endWordPosition="1800">have ignored so far are the handling of MTUs which have discontinuous targets, and the handling of unaligned target words. Both TSM and OSM N-gram models generate MTUs linearly in left-to-right order. This assumption becomes problematic in the cases of MTUs that have target-side discontinuities (See Figure 2(a)). The MTU A -+ g ... a can not be generated because of the intervening MTUs B -+ b, C ... H -+ c and D -+ d. In the original TSM model, such cases are dealt with by merging all the intervening MTUs to form a bigger unit ti in Figure 2(c). A solution that uses split-rules is proposed by Crego and Yvon (2009) but has not been adopted in Ncode (Crego et al., 2011), the state-of-the-art TSM Ngram system. Durrani et al. (2011) dealt with this problem by applying a post-processing (PP) heuristic that modifies the alignments to remove such cases. When a source word is aligned to a discontinuous target-cept, first the link to the least frequent target word is identified, and the group of links containing this word is retained while the others are deleted. The alignment in Figure 2(a), for example, is transformed to that in Figure 2(b). This allows OSM to extract the intervening MTUs t2 ... t5 (Figure 2(</context>
</contexts>
<marker>Crego, Yvon, 2009</marker>
<rawString>Josep M. Crego and Franc¸ois Yvon. 2009. Gappy Translation Units under Left-to-Right SMT Decoding. In Proceedings of the Meeting of the European Association for Machine Translation (EAMT), pages 66–73, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Improving Reordering with Linguistically Informed Bilingual N-Grams.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>197--205</pages>
<location>Beijing, China,</location>
<contexts>
<context position="4160" citStr="Crego and Yvon (2010)" startWordPosition="611" endWordPosition="614">e 399 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399–405, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics baseline system, and shows statistically significant improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation wh</context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>Josep M. Crego and Franc¸ois Yvon. 2010. Improving Reordering with Linguistically Informed Bilingual N-Grams. In Coling 2010: Posters, pages 197– 205, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
<author>Jos´e B Mari˜no</author>
</authors>
<date>2011</date>
<booktitle>Ncode: an Open Source Bilingual N-gram SMT Toolkit. The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>96--49</pages>
<marker>Crego, Yvon, Mari˜no, 2011</marker>
<rawString>Josep M. Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no. 2011. Ncode: an Open Source Bilingual N-gram SMT Toolkit. The Prague Bulletin of Mathematical Linguistics, 96:49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A Joint Sequence Translation Model with Integrated Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1045--1054</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1966" citStr="Durrani et al., 2011" startWordPosition="268" endWordPosition="271">on outside of phrasal boundaries is ignored. The monolingual language model somewhat reduces this problem. However i) often the language model cannot overcome the dispreference of the translation model for nonlocal dependencies, ii) source-side contextual dependencies are still ignored and iii) generation of lexical translations and reordering is separated. The N-gram-based SMT framework addresses these problems by learning Markov chains over sequences of minimal translation units (MTUs) also known as tuples (Mari˜no et al., 2006) or over operations coupling lexical generation and reordering (Durrani et al., 2011). Because the models condition the MTU probabilities on the previous MTUs, they capture non-local dependencies and both source and target contextual information across phrasal boundaries. In this paper we study the effect of integrating tuple-based N-gram models (TSM) and operationbased N-gram models (OSM) into the phrasebased model in Moses, a state-of-the-art phrasebased system. Rather than using POS-based rewrite rules (Crego and Mari˜no, 2006) to form a search graph, we use the ability of the phrasebased system to memorize larger translation units to replicate the effect of source lineariz</context>
<context position="4849" citStr="Durrani et al. (2011)" startWordPosition="721" endWordPosition="724"> for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by coupling lexical generation and reordering information into a single generative process and enriching the N-gram models to learn lexical reordering triggers. Durrani et al. (2013) showed that using larger phrasal units during decoding is superior to MTU-based decoding in an N-gram-based system. However, they do not use phrase-based models in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to </context>
<context position="9590" citStr="Durrani et al. (2011)" startWordPosition="1508" endWordPosition="1511">ng with reordering operations is shown in Figure 1(d). We learn a Markov model over a sequence of operations (o1, o2, ... , oJ) that encapsulate MTUs and reordering information which is defined as follows: J posm(F, E, A) = H p(oj|oj−n+1, ..., oj−1) j=1 A 9-gram Kneser-Ney smoothed language model is trained with SRILM.3 By coupling reordering with lexical generation, each (translation or reordering) decision conditions on n − 1 previous (translation and reordering) decisions spanning across phrasal boundaries. The reordering decisions therefore influence lexical selection and 2Please refer to Durrani et al. (2011) for a list of operations and the conversion algorithm. 3We also tried a 5-gram model, the performance decreased slightly in some cases. vice versa. A heterogeneous mixture of translation and reordering operations enables the OSM model to memorize reordering patterns and lexicalized triggers unlike the TSM model where translation and reordering are modeled separately. Search: We integrated the generative story of the OSM model into the hypothesis extension process of the phrase-based decoder. Each hypothesis maintains the position of the source word covered by the last generated MTU, the right</context>
<context position="11447" citStr="Durrani et al. (2011)" startWordPosition="1818" endWordPosition="1821">ords. Both TSM and OSM N-gram models generate MTUs linearly in left-to-right order. This assumption becomes problematic in the cases of MTUs that have target-side discontinuities (See Figure 2(a)). The MTU A -+ g ... a can not be generated because of the intervening MTUs B -+ b, C ... H -+ c and D -+ d. In the original TSM model, such cases are dealt with by merging all the intervening MTUs to form a bigger unit ti in Figure 2(c). A solution that uses split-rules is proposed by Crego and Yvon (2009) but has not been adopted in Ncode (Crego et al., 2011), the state-of-the-art TSM Ngram system. Durrani et al. (2011) dealt with this problem by applying a post-processing (PP) heuristic that modifies the alignments to remove such cases. When a source word is aligned to a discontinuous target-cept, first the link to the least frequent target word is identified, and the group of links containing this word is retained while the others are deleted. The alignment in Figure 2(a), for example, is transformed to that in Figure 2(b). This allows OSM to extract the intervening MTUs t2 ... t5 (Figure 2(c)). Note that this problem does not exist when dealing with source-side discontinuities: the TSM model linearizes di</context>
<context position="12704" citStr="Durrani et al., 2011" startWordPosition="2030" endWordPosition="2033"> C ... H -+ c. The 401 Figure 2: Example (a) Original Alignments (b) Post-Processed Alignments (c) Extracted MTUs – ti ... t3 (from (a)) and t1 ... t7 (from (b)) OSM model deals with such cases through Insert Gap and Continue Cept operations. The second problem is the unaligned target-side MTUs such as ε -+ f in Figure 2(a). Inserting target-side words “spuriously” during decoding is a non-trival problem because there is no evidence of when to hypothesize such words. These cases are dealt with in N-gram-based SMT by merging such MTUs to the MTU on the left or right based on attachment counts (Durrani et al., 2011), lexical probabilities obtained from IBM Model 1 (Mari˜no et al., 2006), or POS entropy (Gispert and Mari˜no, 2006). Notice how ε -+ f (Figure 2(a)) is merged with the neighboring MTU E -+ e to form a new MTU E -+ ef (Figure 2 (c)). We initially used the post-editing heuristic (PP) as defined by Durrani et al. (2011) for both TSM and OSM N-gram models, but found that it lowers the translation quality (See Row 2 in Table 2) in some language pairs. 3.4 Solution: Insertion and Linearization To deal with these problems, we made novel modifications to the generative story of the OSM model. Rather </context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A Joint Sequence Translation Model with Integrated Reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1045–1054, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>Model With Minimal Translation Units, But Decode With Phrases.</title>
<date>2013</date>
<booktitle>In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="5055" citStr="Durrani et al. (2013)" startWordPosition="750" endWordPosition="753">on structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by coupling lexical generation and reordering information into a single generative process and enriching the N-gram models to learn lexical reordering triggers. Durrani et al. (2013) showed that using larger phrasal units during decoding is superior to MTU-based decoding in an N-gram-based system. However, they do not use phrase-based models in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other appr</context>
</contexts>
<marker>Durrani, Fraser, Schmid, 2013</marker>
<rawString>Nadir Durrani, Alexander Fraser, and Helmut Schmid. 2013. Model With Minimal Translation Units, But Decode With Phrases. In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>A Source-side Decoding Sequence Model for Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Conference of the Association for Machine Translation in the Americas 2010,</booktitle>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="4066" citStr="Feng et al. (2010)" startWordPosition="596" endWordPosition="599">al with unaligned and discontinuous target tokens consistently improves BLEU scores over the 399 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399–405, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics baseline system, and shows statistically significant improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is</context>
</contexts>
<marker>Feng, Mauser, Ney, 2010</marker>
<rawString>Minwei Feng, Arne Mauser, and Hermann Ney. 2010. A Source-side Decoding Sequence Model for Statistical Machine Translation. In Conference of the Association for Machine Translation in the Americas 2010, Denver, Colorado, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Non-Hierarchical Phrase-Based Translation. In Human Language Technologies: The</title>
<date>2010</date>
<marker>Galley, Manning, 2010</marker>
<rawString>Michel Galley and Christopher D. Manning. 2010. Accurate Non-Hierarchical Phrase-Based Translation. In Human Language Technologies: The 2010</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>966--974</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker></marker>
<rawString>Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 966–974, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a Gispert</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Linguistic Tuple Segmentation in N-Gram-Based Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Gispert, Mari˜no, 2006</marker>
<rawString>Adri`a Gispert and Jos´e B. Mari˜no. 2006. Linguistic Tuple Segmentation in N-Gram-Based Statistical Machine Translation. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Sparse Lexicalised Features and Topic Adaptation for SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>268--275</pages>
<contexts>
<context position="17023" citStr="Hasler et al., 2012" startWordPosition="2758" endWordPosition="2761">to and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline the first half for tuning and second for test. We test our systems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row </context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2012</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse Lexicalised Features and Topic Adaptation for SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 268–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom,</location>
<contexts>
<context position="16904" citStr="Heafield, 2011" startWordPosition="2744" endWordPosition="2745">24.14 33.21 30.35 35.34 16.49 24.22 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25 Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline the first half for tuning and second for test. We test our systems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our int</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17168" citStr="Huang and Chiang, 2007" startWordPosition="2777" endWordPosition="2780">ystems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row 4 (+pp+osm) shows that the OSM model consistently improves the BLEU scores over the Baseline systems (Row 1) giving significant improvements in h</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Khalilov</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>NGram-Based Statistical Machine Translation Versus Syntax Augmented Machine Translation: Comparison and System Combination.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>424--432</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="5781" citStr="Khalilov and Fonollosa, 2009" startWordPosition="866" endWordPosition="869">-gram-based system. However, they do not use phrase-based models in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other approaches that explored such models in syntax-based systems used MTUs for sentence level reranking (Khalilov and Fonollosa, 2009), in dependency translation models (Quirk and Menezes, 2006) and in target language syntax systems (Vaswani et al., 2011). 3 Integration of N-gram Models We now describe our integration of TSM and OSM N-gram models into the phrase-based sysFigure 1: Example (a) Word Alignments (b) Unfolded MTU Sequence (c) Operation Sequence (d) Step-wise Generation tem. Given a bilingual sentence pair (F, E) and its alignment (A), we first identify minimal translation units (MTUs) from it. An MTU is defined as a translation rule that cannot be broken down any further. The MTUs extracted from Figure 1(a) are A</context>
</contexts>
<marker>Khalilov, Fonollosa, 2009</marker>
<rawString>Maxim Khalilov and Jos´e A. R. Fonollosa. 2009. NGram-Based Statistical Machine Translation Versus Syntax Augmented Machine Translation: Comparison and System Combination. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 424–432, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Towards Effective Use of Training Data in Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>317--321</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="15914" citStr="Koehn and Haddow, 2012" startWordPosition="2590" endWordPosition="2593"> language models are reported in Table 1. All data is true-cased. Pair Parallel Monolingual Lang fr–en P 39 M P 91 M fr cs–en P 15.6 M P 43.4 M cs es–en P 15.2 M P 65.7 M es ru–en P 2 M P 21.7 M ru P 287.3 M en Table 1: Number of Sentences (in Millions) used for Training We follow the approach of Schwenk and Koehn (2008) and trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the heldout dev-set. We concatenated the news-test sets from four years (2008-2011) to obtain a large devsetin order to obtain more stable weights (Koehn and Haddow, 2012). For Russian-English and English-Russian language pairs, we divided the tuning-set news-test 2012 into two halves and used 402 No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05 1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96 1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25 Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline the first half for tuning and</context>
</contexts>
<marker>Koehn, Haddow, 2012</marker>
<rawString>Philipp Koehn and Barry Haddow. 2012. Towards Effective Use of Training Data in Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 317– 321, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1092" citStr="Koehn et al., 2003" startWordPosition="141" endWordPosition="144"> model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) learn local dependencies such as reorderings, idiomatic collocations, deletions and insertions by memorization. A fundamental drawback is that phrases are translated and reordered independently of each other and contextual information outside of phrasal boundaries is ignored. The monolingual language model somewhat reduces this problem. However i) often the language model cannot overcome the dispreference of the translation model for nonlocal dependencies, ii) source-side contextual dependencies are still ignored and iii) generation of lexical translations and reordering i</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT-NAACL, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In ACL 2007 Demonstrations,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="16705" citStr="Koehn et al., 2007" startWordPosition="2716" endWordPosition="2719">es en-cs en-ru Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05 1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96 1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25 Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline the first half for tuning and second for test. We test our systems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 2007 Demonstrations, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="16469" citStr="Koehn, 2004" startWordPosition="2677" endWordPosition="2678">er to obtain more stable weights (Koehn and Haddow, 2012). For Russian-English and English-Russian language pairs, we divided the tuning-set news-test 2012 into two halves and used 402 No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05 1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.96 1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.22 1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25 Table 2: Translating into and from English. Bold: Statistically Significant (Koehn, 2004) w.r.t Baseline the first half for tuning and second for test. We test our systems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William J Byrne</author>
</authors>
<title>Minimum Bayes-Risk Decoding for Statistical Machine Translation. In</title>
<date>2004</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="17129" citStr="Kumar and Byrne, 2004" startWordPosition="2772" endWordPosition="2775">ing and second for test. We test our systems on news-test 2012. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). Moses Baseline: We trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row 4 (+pp+osm) shows that the OSM model consistently improves the BLEU scores over the Baseline systems (Row </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William J. Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation. In HLT-NAACL, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>N-gramBased Machine Translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonollosa, and Marta R. Costa-juss`a. 2006. N-gramBased Machine Translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Wider Context by Using Bilingual Language Models in Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>198--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="4278" citStr="Niehues et al. (2011)" startWordPosition="629" endWordPosition="632">ulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics baseline system, and shows statistically significant improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by co</context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Language Models in Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 198–206, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1112" citStr="Och and Ney, 2004" startWordPosition="145" endWordPosition="148"> dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve. 1 Introduction Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) learn local dependencies such as reorderings, idiomatic collocations, deletions and insertions by memorization. A fundamental drawback is that phrases are translated and reordered independently of each other and contextual information outside of phrasal boundaries is ignored. The monolingual language model somewhat reduces this problem. However i) often the language model cannot overcome the dispreference of the translation model for nonlocal dependencies, ii) source-side contextual dependencies are still ignored and iii) generation of lexical translations and reordering is separated. The N-g</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(1):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="17284" citStr="Papineni et al., 2002" startWordPosition="2793" endWordPosition="2796">trained a Moses system (Koehn et al., 2007) with the following settings: maximum sentence length 80, grow-diag-finaland symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) and the no-reordering-overpunctuation heuristic. Results: Table 2 shows uncased BLEU scores (Papineni et al., 2002) on the test set. Row 2 (+pp) shows that the post-editing of alignments to remove unaligned and discontinuous target MTUs decreases the performance in the case of ru-en, csen and en-fr. Row 3 (+pp+tsm) shows that our integration of the TSM model slightly improves the BLEU scores for en-fr, and es-en. Results drop in ru-en and en-ru. Row 4 (+pp+osm) shows that the OSM model consistently improves the BLEU scores over the Baseline systems (Row 1) giving significant improvements in half the cases. The only result that is lower than the baseline system is that of the ru-en experiment, because OSM i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do We Need Phrases? Challenging the Conventional Wisdom in Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="5841" citStr="Quirk and Menezes, 2006" startWordPosition="875" endWordPosition="878">in their work, relying only on the OSM model. This paper combines insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other approaches that explored such models in syntax-based systems used MTUs for sentence level reranking (Khalilov and Fonollosa, 2009), in dependency translation models (Quirk and Menezes, 2006) and in target language syntax systems (Vaswani et al., 2011). 3 Integration of N-gram Models We now describe our integration of TSM and OSM N-gram models into the phrase-based sysFigure 1: Example (a) Word Alignments (b) Unfolded MTU Sequence (c) Operation Sequence (d) Step-wise Generation tem. Given a bilingual sentence pair (F, E) and its alignment (A), we first identify minimal translation units (MTUs) from it. An MTU is defined as a translation rule that cannot be broken down any further. The MTUs extracted from Figure 1(a) are A → a,B → b,C ... H → c1 and D → d. These units are then gene</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Christopher Quirk and Arul Menezes. 2006. Do We Need Phrases? Challenging the Conventional Wisdom in Statistical Machine Translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Philipp Koehn</author>
</authors>
<title>Large and Diverse Language Models for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In International Joint Conference on Natural Language Processing,</booktitle>
<pages>661--666</pages>
<contexts>
<context position="15613" citStr="Schwenk and Koehn (2008)" startWordPosition="2543" endWordPosition="2546">achieved in the tuple N-gram model by using POS-based split and rewrite rules. 4 Evaluation Corpus: We ran experiments with data made available for the translation task of the Eighth Workshop on Statistical Machine Translation. The sizes of bitext used for the estimation of translation and monolingual language models are reported in Table 1. All data is true-cased. Pair Parallel Monolingual Lang fr–en P 39 M P 91 M fr cs–en P 15.6 M P 43.4 M cs es–en P 15.2 M P 65.7 M es ru–en P 2 M P 21.7 M ru P 287.3 M en Table 1: Number of Sentences (in Millions) used for Training We follow the approach of Schwenk and Koehn (2008) and trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the heldout dev-set. We concatenated the news-test sets from four years (2008-2011) to obtain a large devsetin order to obtain more stable weights (Koehn and Haddow, 2012). For Russian-English and English-Russian language pairs, we divided the tuning-set news-test 2012 into two halves and used 402 No. System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.88 1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.05 1+</context>
</contexts>
<marker>Schwenk, Koehn, 2008</marker>
<rawString>Holger Schwenk and Philipp Koehn. 2008. Large and Diverse Language Models for Statistical Machine Translation. In International Joint Conference on Natural Language Processing, pages 661–666, January 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="7071" citStr="Stolcke, 2002" startWordPosition="1094" endWordPosition="1095">ght in two different ways, as we will describe next. 3.1 Tuple Sequence Model (TSM) The TSM translation model assumes that MTUs are generated monotonically. To achieve this effect, we enumerate the MTUs in the target leftto-right order. This process is also called source linearization or tuple unfolding. The resulting sequence of monotonic MTUs is shown in Figure 1(b). We then define a TSM model over this sequence (t1, t2, ... , tJ) as: J ptsm(F, E, A) = p(tj|tj−n+1, ..., tj−1) j=1 where n indicates the amount of context used. A 4-gram Kneser-Ney smoothed language model is trained with SRILM (Stolcke, 2002). Search: In previous work, the search graph in TSM N-gram SMT was not built dynamically like in the phrase-based system, but instead constructed as a preprocessing step using POS-based rewrite rules (learned when linearizing the source side). We do not adopt this framework. We use 1We use ... to denote discontinuous MTUs. 400 phrase-based search which builds up the decoding graph dynamically and searches through all possible reorderings within a fixed window. During decoding we use the phrase-internal alignments to perform source linearization. For example, if during decoding we would like to</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Intl. Conf. Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Short Papers,</booktitle>
<pages>101--104</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="4228" citStr="Tillman (2004)" startWordPosition="623" endWordPosition="624">tional Linguistics, pages 399–405, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics baseline system, and shows statistically significant improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. </context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A Unigram Orientation Model for Statistical Machine Translation. In HLT-NAACL 2004: Short Papers, pages 101–104, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Rule Markov Models for Fast Treeto-String Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>856--864</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="5902" citStr="Vaswani et al., 2011" startWordPosition="886" endWordPosition="889">s insights from these recent pieces of work and show that phrase-based search combined with N-gram-based and phrase-based models in decoding is the overall best way to go. We integrate the two N-grambased models, TSM and OSM, into phrase-based Moses and show that the translation quality is improved by taking both translation and reordering context into account. Other approaches that explored such models in syntax-based systems used MTUs for sentence level reranking (Khalilov and Fonollosa, 2009), in dependency translation models (Quirk and Menezes, 2006) and in target language syntax systems (Vaswani et al., 2011). 3 Integration of N-gram Models We now describe our integration of TSM and OSM N-gram models into the phrase-based sysFigure 1: Example (a) Word Alignments (b) Unfolded MTU Sequence (c) Operation Sequence (d) Step-wise Generation tem. Given a bilingual sentence pair (F, E) and its alignment (A), we first identify minimal translation units (MTUs) from it. An MTU is defined as a translation rule that cannot be broken down any further. The MTUs extracted from Figure 1(a) are A → a,B → b,C ... H → c1 and D → d. These units are then generated left-to-right in two different ways, as we will describ</context>
</contexts>
<marker>Vaswani, Mi, Huang, Chiang, 2011</marker>
<rawString>Ashish Vaswani, Haitao Mi, Liang Huang, and David Chiang. 2011. Rule Markov Models for Fast Treeto-String Translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 856–864, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Kristina Toutanova</author>
<author>Chris Quirk</author>
<author>Jianfeng Gao</author>
</authors>
<title>Beyond Left-to-Right: Multiple Decomposition Structures for SMT.</title>
<date>2013</date>
<booktitle>In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="4404" citStr="Zhang et al. (2013)" startWordPosition="651" endWordPosition="654">improvements in seven out of eight cases. 2 Previous Work Several researchers have tried to combine the ideas of phrase-based and N-gram-based SMT. Costajuss`a et al. (2007) proposed a method for combining the two approaches by applying sentence level reranking. Feng et al. (2010) added a linearized source-side language model in a phrase-based system. Crego and Yvon (2010) modified the phrasebased lexical reordering model of Tillman (2004) for an N-gram-based system. Niehues et al. (2011) integrated a bilingual language model based on surface word forms and POS tags into a phrasebased system. Zhang et al. (2013) explored multiple decomposition structures for generating MTUs in the task of lexical selection, and to rerank the N-best candidate translations in the output of a phrase-based. A drawback of the TSM model is the assumption that source and target information is generated monotonically. The process of reordering is disconnected from lexical generation which restricts the search to a small set of precomputed reorderings. Durrani et al. (2011) addressed this problem by coupling lexical generation and reordering information into a single generative process and enriching the N-gram models to learn</context>
</contexts>
<marker>Zhang, Toutanova, Quirk, Gao, 2013</marker>
<rawString>Hui Zhang, Kristina Toutanova, Chris Quirk, and Jianfeng Gao. 2013. Beyond Left-to-Right: Multiple Decomposition Structures for SMT. In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>