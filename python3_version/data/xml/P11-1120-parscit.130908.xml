<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.974757">
Fine-Grained Class Label Markup of Search Queries
</title>
<author confidence="0.979892">
Joseph Reisinger*
</author>
<affiliation confidence="0.998179">
Department of Computer Sciences
The University of Texas at Austin
</affiliation>
<address confidence="0.62218">
Austin, Texas 78712
</address>
<email confidence="0.996482">
joeraii@cs.utexas.edu
</email>
<author confidence="0.612214">
Marius Pas¸ca
</author>
<affiliation confidence="0.587663">
Google Inc.
</affiliation>
<address confidence="0.6854355">
1600 Amphitheatre Parkway
Mountain View, California 94043
</address>
<email confidence="0.99849">
mars@google.com
</email>
<sectionHeader confidence="0.996656" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999724">
We develop a novel approach to the seman-
tic analysis of short text segments and demon-
strate its utility on a large corpus of Web
search queries. Extracting meaning from short
text segments is difficult as there is little
semantic redundancy between terms; hence
methods based on shallow semantic analy-
sis may fail to accurately estimate meaning.
Furthermore search queries lack explicit syn-
tax often used to determine intent in ques-
tion answering. In this paper we propose a
hybrid model of semantic analysis combin-
ing explicit class-label extraction with a la-
tent class PCFG. This class-label correlation
(CLC) model admits a robust parallel approxi-
mation, allowing it to scale to large amounts of
query data. We demonstrate its performance
in terms of (1) its predicted label accuracy on
polysemous queries and (2) its ability to accu-
rately chunk queries into base constituents.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998171">
Search queries are generally short and rarely contain
much explicit syntax, making query understanding a
purely semantic endeavor. Furthermore, as in noun-
phrase understanding, shallow lexical semantics is
often irrelevant or misleading; e.g., the query [trop-
ical breeze cleaners] has little to do with island va-
cations, nor are desert birds relevant to [1970 road
runner], which refers to a car model.
This paper introduces class-label correlation
(CLC), a novel unsupervised approach to extract-
</bodyText>
<footnote confidence="0.453127">
*Contributions made during an internship at Google.
</footnote>
<page confidence="0.873677">
1200
</page>
<bodyText confidence="0.999751">
ing shallow semantic content that combines class-
based semantic markup (e.g., road runner is a car
model) with a latent variable model for capturing
weakly compositional interactions between query
constituents. Constituents are tagged with IsA class
labels from a large, automatically extracted lexicon,
using a probabilistic context free grammar (PCFG).
Correlations between the resulting label—*term dis-
tributions are captured using a set of latent produc-
tion rules specified by a hierarchical Dirichlet Pro-
cess (Teh et al., 2006) with latent data groupings.
Concretely, the IsA tags capture the inventory
of potential meanings (e.g., jaguar can be labeled
as european car or large cat) and relevant con-
stituent spans, while the latent variable model per-
forms sense and theme disambiguation (e.g., [jaguar
habitat] would lend evidence for the large cat la-
bel). In addition to broad sense disambiguation, CLC
can distinguish closely related usages, e.g., the use
of dell in [dell motherboard replacement] and [dell
stock price].1 Furthermore, by employing IsA class
labeling as a preliminary step, CLC can account for
common non-compositional phrases, such as big ap-
ple unlike systems relying purely on lexical seman-
tics. Additional examples can be found later, in Fig-
ure 5.
In addition to improving query understanding, po-
tential applications of CLC include: (1) relation ex-
traction (Baeza-Yates and Tiberi, 2007), (2) query
substitutions or broad matching (Jones et al., 2006),
and (3) classifying other short textual fragments
such as SMS messages or tweets.
We implement a parallel inference procedure for
</bodyText>
<footnote confidence="0.965331">
1Dell the computer system vs. Dell the technology company.
</footnote>
<note confidence="0.987358">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1200–1209,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999166722222222">
CLC and evaluate it on a sample of 500M search
queries along two dimensions: (1) query constituent
chunking precision (i.e., how accurate are the in-
ferred spans breaks; cf., Bergsma and Wang (2007);
Tan and Peng (2008)), and (2) class label assign-
ment precision (i.e., given the query intent, how rel-
evant are the inferred class labels), paying particu-
lar attention to cases where queries contain ambigu-
ous constituents. CLC compares favorably to sev-
eral simpler submodels, with gains in performance
stemming from coarse-graining related class labels
and increasing the number of clusters used to cap-
ture between-label correlations.
(Paper organization): Section 2 discusses relevant
background, Section 3 introduces the CLC model,
Section 4 describes the experimental setup em-
ployed, Section 5 details results, Section 6 intro-
duces areas for future work and Section 7 concludes.
</bodyText>
<sectionHeader confidence="0.987618" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999907236111112">
Query understanding has been studied extensively
in previous literature. Li (2010) defines the se-
mantic structure of noun-phrase queries as intent
heads (attributes) coupled with some number of in-
tent modifiers (attribute values), e.g., the query [al-
ice in wonderland 2010 cast] is comprised of an in-
tent head cast and two intent modifiers alice in won-
derland and 2010. In this work we focus on seman-
tic class markup of query constituents, but our ap-
proach could be easily extended to account for query
structure as well.
Popescu et al. (2010) describe a similar class-
label-based approach for query interpretation, ex-
plicitly modeling the importance of each label for
a given entity. However, details of their implemen-
tation were not publicly available, as of publication
of this paper.
For simplicity, we extract class labels using the
seed-based approach proposed by Van Durme and
Pas¸ca (2008) (in particular Pas¸ca (2010)) which gen-
eralizes Hearst (1992). Talukdar and Pereira (2010)
use graph-based semi-supervised learning to acquire
class-instance labels; Wang et al. (2009) introduce a
similar CRF-based approach but only apply it to a
small number of verticals (i.e., Computing and Elec-
tronics or Clothing and Shoes). Snow et al. (2006)
describe a learning approach for automatically ac-
quiring patterns indicative of hypernym (IsA) rela-
tions. Semantic class label lexicons derived from
any of these approaches can be used as input to CLC.
Several authors have studied query clustering in
the context of information retrieval (e.g., Beeferman
and Berger, 2000). Our approach is novel in this
regard, as we cluster queries in order to capture cor-
relations between span labels, rather than explicitly
for query understanding.
Tratz and Hovy (2010) propose a taxonomy for
classifying and interpreting noun-compounds, fo-
cusing specifically on the relationships holding be-
tween constituents. Our approach yields similar top-
ical decompositions of noun-phrases in queries and
is completely unsupervised.
Jones et al. (2006) propose an automatic method
for query substitution, i.e., replacing a given query
with another query with the similar meaning, over-
coming issues with poor paraphrase coverage in tail
queries. Correlations mined by our approach are
readily useful for downstream query substitution.
Bergsma and Wang (2007) develop a super-
vised approach to query chunking using 500 hand-
segmented queries from the AOL corpus. Tan and
Peng (2008) develop a generative model of query
segmentation that makes use of a language model
and concepts derived from Wikipedia article titles.
CLC differs fundamentally in that it learns con-
cept label markup in addition to segmentation and
uses in-domain concepts derived from queries them-
selves. This work also differs from both of these
studies significantly in scope, training on 500M
queries instead of just 500.
At the level of class-label markup, our model is
related to Bayesian PCFGs (Liang et al., 2007; John-
son et al., 2007b), and is a particular realization of an
Adaptor Grammar (Johnson et al., 2007a; Johnson,
2010).
Szpektor et al. (2008) introduce a model of con-
textual preferences, generalizing the notion of selec-
tional preference (cf. Ritter et al., 2010) to arbitrary
terms, allowing for context-sensitive inference. Our
approach differs in its use of class-instance labels for
generalizing terms, a necessary step for dealing with
the lack of syntactic information in queries.
</bodyText>
<page confidence="0.975255">
1201
</page>
<figure confidence="0.999442">
query clusters
label clusters
label pcfg
query constituents
brighton vinyl windows
class label→query span
recreational facilities→jacuzzi
rural areas→wales
destinations→wales
seaside towns→brighton
building materials→vinyl windows
consumer goods→european clothing
ΦC
ΦL ΦL
seaside towns
building materials
</figure>
<figureCaption confidence="0.985725666666667">
Figure 1: Overview of CLC markup generation for
the query [brighton vinyl windows]. Arrows denote
multinomial distributions.
</figureCaption>
<tableCaption confidence="0.653580333333333">
Table 1: Example production rules collected using
the semi-supervised approach of Van Durme and
Pas¸ca (2008).
</tableCaption>
<sectionHeader confidence="0.915406" genericHeader="method">
3 Latent Class-Label Correlation
</sectionHeader>
<bodyText confidence="0.998675913043478">
Input to CLC consists of raw search queries and a
partial grammar mapping class labels to query spans
(e.g., building materials→vinyl windows). CLC in-
fers two additional latent productions types on top
of these class labels: (1) a potentially infinite set of
label clusters OL
coarse-graining the raw input label
productions V , and (2) a finite set of query clusters
0Cspecifying distributions over label clusters; see
Ci
Figure 1 for an overview.
Operationally, CLC is implemented as a Hierar-
chical Dirichlet Process (HDP; Teh et al., 2006) with
latent groups coupled with a Probabilistic Context
Free Grammar (PCFG) likelihood function (Figure
2). We motivate our use of an HDP latent class
model instead of a full PCFG with binary produc-
tions by the fact that the space of possible binary
rule combinations is prohibitively large (561K base
labels; 314B binary rules). The next sections discuss
the three main components of CLC: §3.1 the raw IsA
class labels, §3.2 the PCFG likelihood, and §3.3 the
HDP with latent groupings.
</bodyText>
<subsectionHeader confidence="0.999591">
3.1 IsA Label Extraction
</subsectionHeader>
<bodyText confidence="0.999886133333333">
IsA class labels (hypernyms) V are extracted from
a large corpus of raw Web text using the method
proposed by Van Durme and Pas¸ca (2008) and ex-
tended by Pas¸ca (2010). Manually specified patterns
are used to extract a seed set of class labels and the
resulting label lists are reranked using cluster purity
measures. 561K labels for base noun phrases are
collected. Table 1 shows an example set of class
labels extracted for several common noun phrases.
Similar repositories of IsA labels, extracted using
other methods, are available for experimental pur-
poses (Talukdar and Pereira, 2010). In addition to
extracted rules, the CLC grammar is augmented with
a set of null rules, one per unigram, ensuring that
every query has a valid parse.
</bodyText>
<subsectionHeader confidence="0.99475">
3.2 Class-Label PCFG
</subsectionHeader>
<bodyText confidence="0.992988935483871">
In addition to the observed class-label production
rules, CLC incorporates two sets of latent produc-
tion rules coupled via an HDP (Figure 1). Class
label→query span productions extracted from raw
text are clustered into a set of latent label produc-
tion clusters L = {li, ... , l�}. Each label pro-
duction cluster lk defines a multinomial distribution
over class labels V parametrized by OL. Conceptu-
ally, OL
captures a set of class labels with similar
productions that are found in similar queries, for ex-
ample the class labels states, northeast states, u.s.
states, state areas, eastern states, and certain states
might be included in the same coarse-grained cluster
due to similarities in their productions.
Each query q E Q is assigned to a latent query
cluster cQ E C{ci, ... , cam}, which defines a dis-
tribution over label production clusters L, denoted
0C. Query clusters capture broad correlations be-
q
tween label production clusters and are necessary for
performing sense disambiguation and capturing se-
lectional preference. Query clusters and label pro-
duction clusters are linked using a single HDP, al-
lowing the number of label clusters to vary over the
course of Gibbs sampling, based on the variance of
the underlying data (Section 3.3). Viewed as a gram-
mar, CLC only contains unary rules mapping labels
to query spans; production correlations are captured
directly by the query cluster, unlike in HDP-PCFG
(Liang et al., 2007), as branching parses over the en-
</bodyText>
<page confidence="0.984186">
1202
</page>
<figure confidence="0.908319153846154">
α0
∞α
γ
label clusters
!C
β
ξ
O
L
q t
TT
z
l
Indices Cardinality
HDP base measure β ∼ GEM(γ) - |L |→ ∞
Query cluster φCi ∼ DP(αC,β) i ∈ |C ||L |→ ∞
Label cluster φLk ∼ Dirichlet(αL) k ∈ |L ||V |
Query cluster ind πq ∼ Dirichlet(ξ) q ∈ |Q ||C|
cq ∼ πq
∈|Q|
q
1
∼φC t ∈ q, q ∈ |Q |1
c�
∼ φL t ∈ q, q ∈ |Q |1
z�,t
</figure>
<figureCaption confidence="0.229497">
HDP prior; the middle section is the additional machinery necessary for modeling latent groupings an
</figureCaption>
<bodyText confidence="0.972504833333333">
d the
bottom section contains the indicators for the latent class model. PCFG likelihood is not shown.
γ–
base-measure smoother; higher val-
ues lead to more uniform mass over label
clusters.
αC – Query cluster smoothing; higher values lead
to more uniform mass over label clusters.
αL – Label cluster smoothing; higher values lead
to more label diversity within clusters.
ξ – Query cluster assignment smoothing; higher
values lead to more uniform assignment.
</bodyText>
<equation confidence="0.729778">
� P(r|φLl�)P(lr|φCcy)
P(tq|φL,φC,cq) =
rERy
</equation>
<bodyText confidence="0.860405555555556">
of CLC with an HDP prior linking
and
The
HDP is a Bayesian generative model of shared struc-
ture for grouped data (Teh et al., 2006). A set of
is drawn from a Dirich-
let Process with base measure
using the stick-
breaking construction, an
</bodyText>
<figure confidence="0.631225">
φC
φL.
β∼ GEM(γ)
γ
d clusters for each group k,
Label cluster ind zq,t
Label ind lq,t
query clusters
C
</figure>
<figureCaption confidence="0.999764">
Figure 2: Generative process and graphical model for CLC. The top section of the model is the standard
</figureCaption>
<bodyText confidence="0.907137105263158">
tire label sparse are intractably large.
Given a query q, a query cluster assignment cq and
a set of label production clusters
we define a parse
of q to be a sequence of productions tq forming a
parse tree consuming all the tokens in q. As with
Bayesian
L,
PCFGs (Johnson, 2010), the probability of
a tree tq is the product of the probabilities of the
production rules used to construct it
where Rq is the set of production rules used to de-
rive tq,
is the probability of r given its label
cluster assignment lr, and
is the probabil-
ity of label cluster lr in query cluster c.
The probability of a query q is the sum of the
probabilities of the parse trees that can
</bodyText>
<equation confidence="0.998679333333333">
P(r|OLl�)
P(lr|OCcy)
generate it,
�
P(q|φL,φC, cq) =
���y����q�
</equation>
<bodyText confidence="0.709966">
where
</bodyText>
<equation confidence="0.8633015">
{t|y(t) = q} is the set of trees with q as their
yield (i.e., generate the string of tokens in q).
</equation>
<subsectionHeader confidence="0.877927">
3.3 Hierarchical Dirichlet Process with Latent
</subsectionHeader>
<bodyText confidence="0.743699666666667">
Groups
We complete the Bayesian generative specification
base clusters
</bodyText>
<page confidence="0.43764">
1203
</page>
<figure confidence="0.471979568181818">
HDP-LG
Table 2: CLC-HDP-LG
hyperparameters.
k
are drawn from a separate Dirichlet
Process with base measure
defined over the space
of label clusters. Data in each group k are condi-
tionally independent given
Intuitively,
defines
a common
of label clusters, and each query
cluster
defines a separate distribution over the
label clusters.
In order to account for variable query-cluster as-
signment, we extend the HDP model with latent
groupings
for each query. The re-
sulting Hierarchical Dirichlet Process with Latent
Groups (HDP-LG) can be used to define a set of
query clusters over a set of (potentially infinite) base
label clusters (Figure 2). Each query cluster
(la-
tent group) assigns weight to different subsets of the
available label clusters
capturing correlations
between them at the query level. Each query q main-
tains a distribution over query clusters
capturing
its affinity for each latent group. The full
φC
∼DP(β),
β,
β.
β
“menu”
φCk
πq∼Dir(ξ)
φC
φL,
πq,
generative
</figure>
<bodyText confidence="0.949336333333333">
specification of CLC is shown in Figure 2; hyperpa-
rameters are shown in Table 2.
In addition to the full joint CLC model, we evalu-
</bodyText>
<figure confidence="0.97263125">
P(t|φL,φC,cq)
density
0.4
0.3
0.2
0.1
2 4 6 8 10 12
ate several simpler models:
</figure>
<listItem confidence="0.955118125">
1. CLC-BASE – no query clusters, one label per
label cluster.
2. CLC-DPMM – no query clusters, DPMM(αC)
distribution over labels.
Query length
3. CLC-HDP-LG – full HDP-LG model with ICI
query clusters over a potentially infinite num-
ber of query clusters.
</listItem>
<bodyText confidence="0.583881">
as well as various hyperparameter settings.
</bodyText>
<subsectionHeader confidence="0.988129">
3.4 Parallel Approximate Gibbs Sampler
</subsectionHeader>
<bodyText confidence="0.999993296296296">
We perform inference in CLC via Gibbs sampling,
leveraging Multinomial-Dirichlet conjugacy to inte-
grate out 7r, OC and OL (Teh et al., 2006; Johnson
et al., 2007b). The remaining indicator variables c, z
and l are sampled iteratively, conditional on all other
variable assignments. Although there are an expo-
nential number of parse trees for a given query, this
space can be sampled efficiently using dynamic pro-
gramming (Finkel et al., 2006; Johnson et al., 2007b)
In order to apply CLC to Web-scale data, we
implement an efficient parallel approximate Gibbs
sampler in the MapReduce framework Dean and
Ghemawat (2004). Each Gibbs iteration consists
of a single MapReduce step for sampling, followed
by an additional MapReduce step for computing
marginal counts. 2 Relevant assignments c, z and
l are stored locally with each query and are dis-
tributed across compute nodes. Each node is respon-
sible only for resampling assignments for its local
set of queries. Marginals are fetched opportunisti-
cally from a separate distributed hash server as they
are needed by the sampler. Each Map step computes
a single Gibbs step for 10% of the available data, us-
ing the marginals computed at the previous step. By
resampling only 10% of the available data each it-
eration, we minimize the potentially negative effects
of using the previous step’s marginal distribution.
</bodyText>
<sectionHeader confidence="0.998011" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.983837">
4.1 Query Corpus
</subsectionHeader>
<bodyText confidence="0.997577">
Our dataset consists of a sample of 450M En-
glish queries submitted by anonymous Web users to
</bodyText>
<footnote confidence="0.9660305">
2This approximation and architecture is similar to Smola
and Narayanamurthy (2010).
</footnote>
<figureCaption confidence="0.982628">
Figure 3: Distribution in the query corpus, bro-
ken down by query length (red/solid=all queries;
blue/dashed=queries with ambiguous spans); most
queries contain between 2-6 tokens.
</figureCaption>
<bodyText confidence="0.99976725">
Google. The queries have an average of 3.81 tokens
per query (1.7B tokens). Single token queries are re-
moved as the model is incapable of using context to
disambiguate their meaning. Figure 3 shows the dis-
tribution of remaining queries. During training, we
include 10 copies of each query (4.5B queries total),
allowing an estimate of the Bayes average posterior
from a single Gibbs sample.
</bodyText>
<subsectionHeader confidence="0.974832">
4.2 Evaluations
</subsectionHeader>
<bodyText confidence="0.99979368">
Query markup is evaluated for phrase-chunking pre-
cision (Section 5.1) and label precision (Section 5.2)
by human raters across two different samples: (1)
an unbiased sample from the original corpus, and
(2) a biased sample of queries containing ambigu-
ous spans.
Two raters scored a total of 10K labels from 800
spans across 300 queries. Span labels were marked
as incorrect (0.0), badspan (0.0), ambiguous (0.5),
or correct (1.0), with numeric scores for label pre-
cision as indicated. Chunking precision is measured
as the percentage of labels not marked as badspan.
We report two sets of precision scores depend-
ing on how null labels are handled: Strict evaluation
treats null-labeled spans as incorrect, while Normal
evaluation removes null-labeled spans from the pre-
cision calculation. Normal evaluation was included
since the simpler models (e.g., CLC-BASE) tend to
produce a significantly higher number of null assign-
ments.
Model evaluations were broken down into max-
imum a posteriori (MAP) and Bayes average esti-
mates. MAP estimates are calculated as the single
most likely label/cluster assignment across all query
copies; all assignments in the sample are averaged
</bodyText>
<page confidence="0.992862">
1204
</page>
<figure confidence="0.866033">
Gibbs iterations
</figure>
<figureCaption confidence="0.9925756">
Figure 4: Convergence rates of CLC-
BASE (red/solid), CLC-HDP-LG 100C,40L
(green/dashed), CLC-HDP-LG 1000C,40L
(blue/dotted) in terms of % of query cluster swaps,
label cluster swaps and null rule assignments.
</figureCaption>
<bodyText confidence="0.975677">
to obtain the Bayes average precision estimate.3
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999614666666667">
A total of five variants of CLC were evaluated with
different combinations of |C |and HDP prior con-
centration αC (controlling the effective number of
label clusters). Referring to models in terms of their
parametrizations is potentially confusing. There-
fore, we will make use of the fact that models with
αC = 1 yielded roughly 40 label clusters on aver-
age, and models with αC = 0.1 yielded roughly 200
label clusters, naming model variants simply by the
number of query and label clusters: (1) CLC-BASE,
(2) CLC-DPMM 1C-40L, (3) CLC-HDP-LG 100C-
40L, (4) CLC-HDP-LG 1000C-40L, and (5) CLC-
HDP-LG 1000C-200L. Figure 4 shows the model
convergence for CLC-BASE, CLC-HDP-LG 100C-
40L, and CLC-HDP-LG 1000C-40L.
</bodyText>
<footnote confidence="0.998525">
3We calculate the Bayes average precision estimates at
the top 10 (Bayes@10) and top 20 (Bayes@20) parse trees,
weighted by probability.
</footnote>
<subsectionHeader confidence="0.993183">
5.1 Chunking Precision
</subsectionHeader>
<bodyText confidence="0.99996375">
Chunking precision scores for each model are
shown in Table 3 (average % of labels not marked
badspan). CLC-HDP-LG 1000C-40L has the high-
est precision across both MAP and Bayes esti-
mates (∼93% accuracy), followed by CLC-HDP-LG
1000C-200L (∼90% accuracy) and CLC-DPMM 1C-
40L (∼85%). CLC-BASE performed the worst by
a significant margin (∼78%), indicating that label
coarse-graining is more important than query clus-
tering for chunking accuracy. No significant dif-
ferences in label chunking accuracy were found be-
tween Bayes and MAP inference.
</bodyText>
<subsectionHeader confidence="0.999795">
5.2 Predicting Span Labels
</subsectionHeader>
<bodyText confidence="0.99983246875">
The full CLC-HDP-LG model variants obtain higher
label precision than the simpler models, with CLC-
HDP-LG 1000C-40L achieving the highest precision
of the three (∼63% accuracy). Increasing the num-
ber of label clusters too high, however, significantly
reduces precision: CLC-HDP-LG 1000C-200L ob-
tains only ∼51% accuracy. However, comparing
to CLC-DPMM 1C-40L and CLC-BASE demonstrates
that the addition of label clusters and query clusters
both lead to gains in label precision. These relative
rankings are robust across strict and normal evalua-
tion regimes.
The breakdown over MAP and Bayes posterior
estimation is less clear when considering label pre-
cision: the simpler models CLC-BASE and CLC-
DPMM 1C-40L perform significantly worse than
Bayes when using MAP estimation, while in CLC-
HDP-LG the reverse holds.
There is little evidence for correlation between
precision and query length (weak, not statistically
significant negative correlation using Spearman’s p).
This result is interesting as the relative prevalence
of natural language queries increases with query
length, potentially degrading performance. How-
ever, we did find a strong positive correlation be-
tween precision and the number of labels produc-
tions applicable to a query, i.e., production rule fer-
tility is a potential indicator of semantic quality.
Finally, the histogram column in Table 3 shows
the distribution of rater responses for each model.
In general, the more precise models tend to have
a significantly lower proportion of missing spans
</bodyText>
<figure confidence="0.997885125">
0.070
0.065
0.060
0.055
0.050
0.045
0.040
50 100 150 200 250
50 100 150 200 250
50 100 150 200 250
0.8
0.6
0.4
0.2
0.0
0.50
0.45
0.40
0.35
0.30
0.25
% cluster moves
% label moves
% null rules
</figure>
<page confidence="0.798056">
1205
</page>
<table confidence="0.761978136363636">
Model Chunking Label Precision Ambiguous Label Precision Spearman’s p
Precision normal strict hist normal strict
Class-Label Correlation Base
Bayes@10 78.7±1.1 37.7±1.2 35.8±1.2 35.4±2.0 33.2±1.9
Bayes@20 78.7±1.1 37.7±1.2 35.8±1.2 35.4±2.0 33.2±1.9
MAP 76.3±2.2 33.3±2.2 31.8±2.2 36.2±4.0 33.2±3.8
Class-Label Correlation DPMM 1C 40L
Bayes@10 84.9±0.4 46.6±0.6 44.3±0.5 36.0±1.1 33.7±1.0
Bayes@20 84.8±0.4 47.4±0.5 45.2±0.5 37.8±1.0 35.5±1.0
MAP 84.1±0.8 42.6±1.0 40.5±0.9 11.2±1.3 10.6±1.3
Class-Label Correlation HDP-LG 100C 40L
Bayes@10 83.8±0.4 55.6±0.5 51.0±0.5 55.6±1.0 47.7±1.0
Bayes@20 83.6±0.4 56.9±0.5 52.3±0.5 57.4±1.0 49.8±0.9
MAP 82.7±0.5 58.5±0.5 53.6±0.5 60.4±1.1 51.5±1.0
Class-Label Correlation HDP-LG 1000C 40L
Bayes@10 93.1±0.2 61.1±0.3 60.0±0.3 43.2±0.9 40.2±0.9
Bayes@20 92.8±0.2 62.6±0.3 61.7±0.3 44.9±0.8 42.2±0.8
MAP 92.7±0.2 63.7±0.3 62.7±0.3 44.1±0.9 41.1±0.9
Class-Label Correlation HDP-LG 1000C 200L
Bayes@10 90.3±0.5 50.9±0.8 48.6±0.7 45.8±1.5 42.5±1.3
Bayes@20 89.9±0.5 50.2±0.7 48.0±0.7 44.4±1.4 41.3±1.3
MAP 90.0±0.6 51.0±0.8 48.9±0.8 49.2±1.5 46.0±1.4
</table>
<tableCaption confidence="0.970038">
Table 3: Chunking and label precision across five models. Confidence intervals are standard error; sparklines
</tableCaption>
<bodyText confidence="0.9847025">
show distribution of precision scores (left is zero, right is one). Hist shows the distribution of human rating
response (log y scale): green/first is correct, blue/second is ambiguous, cyan/third is missing and red/fourth
is incorrect. Spearman’s p columns give label precision correlations with query length (weak negative corre-
lation) and the number of applicable labels (weak to strong positive correlation); dots indicate significance.
</bodyText>
<figure confidence="0.973858875">
q. len # labels
-0.13 0.51•
-0.13 0.51•
-0.13 0.52•
-0.05 0.25
-0.02 0.23
-0.03 0.12
0.03 0.44•
0.04 0.41•
0.02 0.41•
-0.06 0.26•
-0.10 0.27•
-0.12 0.28•
-0.10 0.13
-0.08 0.11
-0.07 0.04
</figure>
<figureCaption confidence="0.491754">
(blue/second bar; due to null rule assignment) in ad-
ditional to more correct (green/first) and fewer in-
correct (red/fourth) spans.
</figureCaption>
<subsectionHeader confidence="0.994511">
5.3 High Polysemy Subset
</subsectionHeader>
<bodyText confidence="0.999917777777778">
We repeat the analysis of label precision on a subset
of queries containing one of the manually-selected
polysemous spans shown in Table 4. The CLC-
HDP-LG -based models still significantly outper-
form the simpler models, but unlike in the broader
setting, CLC-HDP-LG 100C-40L significantly out-
performs CLC-HDP-LG 1000C-40L, indicating that
lower query cluster granularity helps address poly-
semy (Table 3).
</bodyText>
<subsectionHeader confidence="0.72492">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.967228545454546">
Figure 5 gives examples of both high-precision and
low-precision queries markups inferred by CLC-
HDP-LG. In general, CLC performs well on queries
with clear intent head / intent modifier structure (Li,
acapella, alamo, apple, atlas, bad, bank, batman,
beloved, black forest, bravo, bush, canton, casino,
champion, club, comet, concord, dallas, diamond,
driver, english, ford, gamma, ion, lemon, man-
hattan, navy, pa, palm, port, put, resident evil,
ronaldo, sacred heart, saturn, seven, solution, so-
pranos, sparta, supra, texas, village, wolf, young
Table 4: Samples from a list of 90 manually se-
lected ambiguous spans used to evaluate model per-
formance under polysemy.
2010). More complex queries, such as [never know
until you try quotes] or [how old do you have to be
a bartender in new york] do not fit this model; how-
ever, expanding the set of extracted labels to also
cover instances such as never know until you try
would mitigate this problem, motivating the use of
n-gram language models with semantic markup.
A large number of mistakes made by CLC are
</bodyText>
<page confidence="0.99066">
1206
</page>
<figureCaption confidence="0.995785333333333">
Figure 5: Examples of high- and low-precision query markups inferred by CLC-HDP-LG. Black text is the
original query; lines indicate potential spans; small text shows potential labels colored and numbered by
label cluster; small bar shows percentage of assignments to that label cluster.
</figureCaption>
<bodyText confidence="0.929088705882353">
Top 10%
Bottom 20% Middle 20%
due to named-entity categories with weak seman-
tics such as rock bands or businesses (e.g., [tropi-
cal breeze cleaners], [cosmic railroad band] or [so-
pranos cigars]). When the named entity is common
enough, it is detected by the rule set, but for the long
tail of named entities this is not the case. One poten-
tial solution is to use a stronger notion of selectional
preference and slot-filling, rather than just relying on
correlation between labels.
Other examples of common errors include inter-
preting weymouth in [weymouth train time table] as
a town in Massachusetts instead of a town in the UK
(lack of domain knowledge), and using lower qual-
ity semantic labels (e.g., neighboring countries for
france, or great retailers for target).
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999865888888889">
Adding both latent label clusters (DPMM) and la-
tent query clusters (extending to HDP-LG) improve
chunking and label precision over the baseline CLC-
BASE system. The label clusters are important be-
cause they capture intra-group correlations between
class labels, while the query clusters are important
for capturing inter-group correlations. However, the
algorithm is sensitive to the relative number of clus-
ters in each case: Too many labels/label clusters rel-
</bodyText>
<page confidence="0.802118">
1207
</page>
<bodyText confidence="0.993540075">
ative to the number of query clusters make it difficult D. Beeferman and A. Berger. 2000. Agglomerative clus-
to learn correlations (O(n2) query clusters are re- tering of a search engine query log. In Proceedings of
quired to capture pairwise interactions). Too many the 6th ACM SIGKDD Conference on Knowledge Dis-
query clusters, on the other hand, make the model covery and Data Mining (KDD-00), pages 407–416.
intractable computationally. The HDP automates se- S. Bergsma and Q. Wang. 2007. Learning noun phrase
lecting the number of clusters, but still requires man- query segmentation. In Proceedings of the 2007 Con-
ual hyperparameter setting. ference on Empirical Methods in Natural Language
(Future Work) Many query slots have weak se- Processing (EMNLP-07), pages 819–826. Prague,
mantics and hence are misleading for CLC. For Czech Republic.
example [pacific breeze cleaners] or [dale hartley J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
subaru] should be parsed such that the type of the fied data processing on large clusters. In Proceedings
leading slot is determined not by its direct content, of the 6th Symposium on Operating Systems Design
but by its context; seeing subaru or cleaners after and Implementation (OSDI-04), pages 137–150. San
a noun-phrase slot is a strong indicator of its type Francisco, California.
(dealership or shop name). The current CLC model J. Finkel, C. Manning, and A. Ng. 2006. Solving the
only couples these slots through their correlations in problem of cascading errors: Approximate Bayesian
query clusters, not directly through relative position inference for linguistic annotation pipelines. In Pro-
or context. Binary productions in the PCFG or a dis- ceedings of the 2006 Conference on Empirical Meth-
criminative learning model would help address this. ods in Natural Language Processing (EMNLP-06),
Finally, we did not measure label coverage with pages 618–626. Sydney, Australia.
respect to a human evaluation set; coverage is use- M. Hearst. 1992. Automatic acquisition of hyponyms
ful as it indicates whether our inferred semantics are from large text corpora. In Proceedings of the 14th In-
biased with respect to human norms. ternational Conference on Computational Linguistics
7 Conclusions (COLING-92), pages 539–545. Nantes, France.
We introduced CLC, a set of latent variable PCFG M. Johnson. 2010. PCFGs, topic models, adaptor
models for semantic analysis of short textual seg- grammars and learning topical collocations and the
ments. CLC captures semantic information in the structure of proper names. In Proceedings of the
form of interactions between clusters of automati- 48th Annual Meeting of the Association for Compu-
cally extracted class-labels, e.g., finding that place- tational Linguistics (ACL-10), pages 1148–1157. Up-
names commonly co-occur with business-names. psala, Sweden.
We applied CLC to a corpus containing 500M search M. Johnson, T. Griffiths, and S. Goldwater. 2007a. Adap-
queries, demonstrating its scalability and straight- tor grammars: a framework for specifying composi-
forward parallel implementation using frameworks tional nonparametric bayesian models. In Advances
like MapReduce or Hadoop. CLC was able to chunk in Neural Information Processing Systems 19, pages
queries into spans more accurately and infer more 641–648. Vancouver, Canada.
precise labels than several sub-models even across a M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
highly ambiguous query subset. The key to obtain- Bayesian inference for PCFGs via Markov Chain
ing these results was coarse-graining the input class- Monte Carlo. In Proceedings of the 2007 Confer-
label set and using a latent variable model to capture ence of the North American Association for Computa-
interactions between coarse-grained labels. tional Linguistics (NAACL-HLT-07), pages 139–146.
</bodyText>
<reference confidence="0.985988864864865">
References Rochester, New York.
R. Baeza-Yates and A. Tiberi. 2007. Extracting semantic R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
relations from query logs. In Proceedings of the 13th erating query substitutions. In Proceedings of the 15h
ACM Conference on Knowledge Discovery and Data World Wide Web Conference (WWW-06), pages 387–
Mining (KDD-07), pages 76–85. San Jose, California. 396. Edinburgh, Scotland.
1208 X. Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1337–1345. Upp-
sala, Sweden.
P. Liang, S. Petrov, M. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Proceedings of the 2007 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
07), pages 688–697. Prague, Czech Republic.
M. Pas¸ca. 2010. The role of queries in ranking labeled in-
stances extracted from text. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING-10), pages 955–962. Beijing, China.
A. Popescu, P. Pantel, and G. Mishne. 2010. Seman-
tic lexicon adaptation for use in query interpretation.
In Proceedings of the 19th World Wide Web Confer-
ence (WWW-10), pages 1167–1168. Raleigh, North
Carolina.
A. Ritter, Mausam, and O. Etzioni. 2010. A latent Dirich-
let allocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
424–434. Uppsala, Sweden.
A. Smola and S. Narayanamurthy. 2010. An architec-
ture for parallel topic models. In Proceedings of the
36th Conference on Very Large Data Bases (VLDB-
10), pages 703–710. singapore.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801–808. Sydney, Australia.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-08), pages 683–691. Colum-
bus, Ohio.
P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1473–1481. Upp-
sala, Sweden.
B. Tan and F. Peng. 2008. Unsupervised query segmenta-
tion using generative language models and Wikipedia.
In Proceedings of the 17th World Wide Web Confer-
ence (WWW-08), pages 347–356. Beijing, China.
Y. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hier-
archical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566–1581.
S. Tratz and E. Hovy. 2010. A taxonomy, dataset, and
classifier for automatic noun compound interpretation.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-10), pages
678–687. Uppsala, Sweden.
B. Van Durme and M. Pas¸ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. In Proceedings of the 23rd National Confer-
ence on Artificial Intelligence (AAAI-08), pages 1243–
1248. Chicago, Illinois.
T. Wang, R. Hoffmann, X. Li, and J. Szymanski.
2009. Semi-supervised learning of semantic classes
for query understanding: from the Web and for the
Web. In Proceedings of the 18th International Con-
ference on Information and Knowledge Management
(CIKM-09), pages 37–46. Hong Kong, China.
</reference>
<page confidence="0.995264">
1209
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.252015">
<title confidence="0.83448">Fine-Grained Class Label Markup of Search Queries</title>
<affiliation confidence="0.959108">Department of Computer The University of Texas at</affiliation>
<address confidence="0.730596">Austin, Texas</address>
<email confidence="0.999592">joeraii@cs.utexas.edu</email>
<affiliation confidence="0.456263">Google</affiliation>
<address confidence="0.9353645">1600 Amphitheatre Mountain View, California</address>
<email confidence="0.999758">mars@google.com</email>
<abstract confidence="0.997548952380952">We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a laclass PCFG. This correlation model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>