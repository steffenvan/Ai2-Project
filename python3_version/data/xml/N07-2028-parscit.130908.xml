<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007662">
<title confidence="0.89197">
Efficient Computation of Entropy Gradient for
Semi-Supervised Conditional Random Fields
</title>
<author confidence="0.94077">
Gideon S. Mann and Andrew McCallum
</author>
<affiliation confidence="0.998521">
Department of Computer Science
University of Massachusetts
</affiliation>
<address confidence="0.705614">
Amherst, MA 01003
</address>
<email confidence="0.995308">
gideon.mann@gmail.com , mccallum@cs.umass.edu
</email>
<sectionHeader confidence="0.99857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884523809524">
Entropy regularization is a straightforward
and successful method of semi-supervised
learning that augments the traditional con-
ditional likelihood objective function with
an additional term that aims to minimize
the predicted label entropy on unlabeled
data. It has previously been demonstrated
to provide positive results in linear-chain
CRFs, but the published method for cal-
culating the entropy gradient requires sig-
nificantly more computation than super-
vised CRF training. This paper presents
a new derivation and dynamic program
for calculating the entropy gradient that
is significantly more efficient—having the
same asymptotic time complexity as su-
pervised CRF training. We also present
efficient generalizations of this method
for calculating the label entropy of all
sub-sequences, which is useful for active
learning, among other applications.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968465116279">
Semi-supervised learning is of growing importance
in machine learning and NLP (Zhu, 2005). Condi-
tional random fields (CRFs) (Lafferty et al., 2001)
are an appealing target for semi-supervised learning
because they achieve state-of-the-art performance
across a broad spectrum of sequence labeling tasks,
and yet, like many other machine learning methods,
training them by supervised learning typically re-
quires large annotated data sets.
Entropy regularization (ER) is a method of semi-
supervised learning first proposed for classification
tasks (Grandvalet and Bengio, 2004). In addition to
maximizing conditional likelihood of the available
labels, ER also aims to minimize the entropy of the
predicted label distribution on unlabeled data. By in-
sisting on peaked, confident predictions, ER guides
the decision boundary away from dense regions of
input space. It is simple and compelling—no pre-
clustering, no “auxiliary functions,” tuning of only
one meta-parameter and it is discriminative.
Jiao et al. (2006) apply this method to linear-
chain CRFs and demonstrate encouraging accuracy
improvements on a gene-name-tagging task. How-
ever, the method they present for calculating the
gradient of the entropy takes substantially greater
time than the traditional supervised-only gradient.
Whereas supervised training requires only classic
forward/backward, taking time O(ns2) (sequence
length times the square of the number of labels),
their training method takes O(n2s3)—a factor of
O(ns) more. This greatly reduces the practicality
of using large amounts of unlabeled data, which is
exactly the desired use-case.
This paper presents a new, more efficient entropy
gradient derivation and dynamic program that has
the same asymptotic time complexity as the gradient
for traditional CRF training, O(ns2). In order to de-
scribe this calculation, the paper introduces the con-
cept of subsequence constrained entropy—the en-
tropy of a CRF for an observed data sequence when
part of the label sequence is fixed. These meth-
ods will allow training on larger unannotated data
set sizes than previously possible and support active
</bodyText>
<page confidence="0.987265">
109
</page>
<note confidence="0.4230715">
Proceedings of NAACL HLT 2007, Companion Volume, pages 109–112,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.467738">
learning.
</bodyText>
<sectionHeader confidence="0.953201" genericHeader="introduction">
2 Semi-Supervised CRF Training
</sectionHeader>
<bodyText confidence="0.9997845">
Lafferty et al. (2001) present linear-chain CRFs, a
discriminative probabilistic model over observation
sequences x and label sequences Y = hY1..Yni,
where |x |= |Y  |= n, and each label Yi has s differ-
ent possible discrete values. For a linear-chain CRF
of Markov order one:
</bodyText>
<equation confidence="0.99694">
pθ(Y |x) = Z1x) exp XθkFk(x,Y ) ,
k
</equation>
<bodyText confidence="0.96385675">
where Fk(x, Y ) = Pi fk(x, Yi, Yi+1, i),
and the partition function Z(x) =
PY exp(Pk θkFk(x,Y )). Given training
data D = hd1..dni, the model is trained by
maximizing the log-likelihood of the data
L(θ; D) = Pd log pθ(Y (d)|x(d)) by gradient
methods (e.g. Limited Memory BFGS), where the
gradient of the likelihood is:
The second term (the expected counts of the features
given the model) can be computed in a tractable
amount of time, since according to the Markov as-
sumption, the feature expectations can be rewritten:
</bodyText>
<equation confidence="0.9978666">
X
pθ(Y |x)Fk(x,Y ) =
Y
X X pθ(Yi, Yi+1|x)fk(x, Yi, Yi+1).
i Yi,Yi�1
</equation>
<bodyText confidence="0.999896">
A dynamic program (the forward/backward algo-
rithm) then computes in time O(ns2) all the needed
probabilities pθ(Yi, Yi+1), where n is the sequence
length, and s is the number of labels.
For semi-supervised training by entropy regular-
ization, we change the objective function by adding
the negative entropy of the unannotated data U =
hu1..uni. (Here Gaussian prior is also shown.)
</bodyText>
<equation confidence="0.979205">
L(θ; D, U) = X log pθ(Y (d)|x(d)) − X
n k
+ λ X
u pθ(Y (u)|x(u)) log pθ(Y (u)|x(u)).
</equation>
<bodyText confidence="0.999523">
This negative entropy term increases as the decision
boundary is moved into sparsely-populated regions
of input space.
</bodyText>
<sectionHeader confidence="0.8568445" genericHeader="method">
3 An Efficient Form of the Entropy
Gradient
</sectionHeader>
<bodyText confidence="0.999822666666667">
In order to maximize the above objective function,
the gradient for the entropy term must be computed.
Jiao et al. (2006) perform this computation by:
</bodyText>
<equation confidence="0.957618">
∂
∂θ − H(Y|x) = covpθ(Y|x)[F(x,Y)]θ,
</equation>
<bodyText confidence="0.760754">
where
</bodyText>
<equation confidence="0.996444333333333">
covp,(Y |x)[Fj(x, Y ), Fk(x, Y )] =
Ep,(Y |x)[Fj(x, Y ), Fk(x, Y )]
− Ep,(Y |x)[Fj(x, Y )]Ep,(Y |x)[Fk(x, Y )].
</equation>
<bodyText confidence="0.999718625">
While the second term of the covariance is easy
to compute, the first term requires calculation of
quadratic feature expectations. The algorithm they
propose to compute this term is O(n2s3) as it re-
quires an extra nested loop in forward/backward.
However, the above form of the gradient is not
the only possibility. We present here an alternative
derivation of the gradient:
</bodyText>
<equation confidence="0.998438422222222">
∂ X
− H(Y |x) = pθ(Y |x) log pθ(Y |x)
∂θk
Y
„ ∂ «
X
= ∂θk pθ(Y |x) log pθ(Y |x)
Y „ ∂ «
+ pθ(Y |x) ∂θk log pθ(Y |x)
pθ(Y |x) log pθ(Y |x)
× Fk(x, Y ) − X
Y , !pθ(Y 0|x)Fk(x, Y 0)
X
pθ(Y |x) Fk(x, Y ) −
YI
Since
Y &apos;)
Y &apos;), the second summand can
PYpθ(Y|x)PY 0pθ(Y &apos;|X)Fk(x,
=PY 0pθ(Y &apos;|X)Fk(x,
-
cels, leaving:
pθ(Y |x) log pθ(Y |x)Fk(x, Y )
(
x) log
(
x)
(Y
x)Fk(
,Y
)
pθ
Y |
pθ
Y |
!
Xpθ
0|
x
0
! .
!pθ(Y 0|x)Fk(x, Y 0) .
∂ X
∂θ − H(Y |x) =
Y
</equation>
<bodyText confidence="0.999960666666667">
Like the gradient obtained by Jiao et al. (2006),
there are two terms, and the second is easily com-
putable given the feature expectations obtained by
</bodyText>
<equation confidence="0.963771882352941">
∂θk d
∂
X
L(θ; D) = Fk(x(d), Y (d))
X− X pθ(Y |x(d))Fk(x(d), Y ).
d Y
θk
2σ2
∂
∂θk
X=
Y
X
+
Y
X−Y
Y ,
</equation>
<page confidence="0.98619">
110
</page>
<bodyText confidence="0.9991852">
forward/backward and the entropy for the sequence.
However, unlike the previous method, here the first
term can be efficiently calculated as well. First,
the term must be further factored into a form more
amenable to analysis:
</bodyText>
<equation confidence="0.999662888888889">
E
pθ(Y |x) log pθ(Y |x)Fk(x,Y )
Y
pθ(Y |x) log pθ(Y |x) E fk(x, Yi, Yi+1, i)
i
E fk(x, Yi, Yi+1, i)
Yi,Yi+1
E pθ(Y |x) log pθ(Y |x).
Y−(i..i+1)
</equation>
<bodyText confidence="0.961368333333333">
Here, Y−(i..i+1) = (Y1..(i−1)Y(i+2)..n). In order
to efficiently calculate this term, it is sufficient
to calculate F-Y−(i..i+1) pθ(Y |x) log pθ(Y |x) for all
pairs yi, yi+1. The next section presents a dynamic
program which can perform these computations in
O(ns2).
</bodyText>
<sectionHeader confidence="0.993715" genericHeader="method">
4 Subsequence Constrained Entropy
</sectionHeader>
<bodyText confidence="0.994281">
We define subsequence constrained entropy as
</bodyText>
<equation confidence="0.961095">
EHσ(Y−(a..b)|ya..b, x) = pθ(Y |x) log pθ(Y |x).
Y−(a..b)
</equation>
<bodyText confidence="0.83388325">
The key to the efficient calculation for all subsets
is to note that the entropy can be factored given a
linear-chain CRF of Markov order 1, since Yi+2 is
independent of Yi given Yi+1.
</bodyText>
<figureCaption confidence="0.880643666666667">
Figure 1: Partial lattice shown for com-
puting the subsequence constrained entropy:
EY p(Y−(3..4), y3, y4) log p(Y−(3..4), y3, y4). Once the
complete Hα and Hβ lattices are constructed (in the direction
of the arrows), the entropy for each label sequence can be
computed in linear time.
</figureCaption>
<bodyText confidence="0.945023285714286">
illustrates an example in which the constrained se-
quence is of size two, but the method applies to
arbitrary-length contiguous label sequences.
Computing the Hα(·) and Hβ(·) lattices is easily
performed using the probabilities obtained by for-
ward/backward. First recall the decomposition for-
mulas for entropy:
</bodyText>
<equation confidence="0.959521666666667">
H(X,Y ) = H(X) + H(Y |X)
H(Y |X) = E P(X = x)H(Y |X = x).
x
</equation>
<bodyText confidence="0.998237666666667">
Using this decomposition, we can define a dynamic
program over the entropy lattices similar to for-
ward/backward:
</bodyText>
<equation confidence="0.987411761904762">
Hα(Y1..i|yi+1, x)
=H(Yi|yi+1, x) + H(Y1..(i−1)|Yi, yi+1, x)
α α β β
H (0|y1) H (Y1|y2) H (Y6|y5) H (0|y6)
y3
y4
=E
Y
E=
i
E pθ(Y−(a..b), ya..b|x) log pθ(Y−(a..b), ya..b|x) E= pθ(yi|yi+1, x) log pθ(yi|yi+1, x)
Y−(a..b) yi
E= pθ(ya..b|x)pθ(Y−(a..b)|ya..b, x)X E pθ(yi|yi+1, x)Hα(Y1..(i−1)|yi).
Y−(a..b) +
yi
(log pθ(ya..b|x) + log pθ(Y−(a..b)|ya..b, x))
=pθ(ya..b|x)log pθ(ya..b|x)
+ pθ(ya..b|x)Hσ(Y−(a..b)|ya..b, x)
=pθ(ya..b|x)log pθ(ya..b|x)
+ pθ(ya..b|x)Hα(Y1..(a−1)|ya, x)
+ pθ(ya..b|x)Hβ(Y(b+1)..n|yb, x).
</equation>
<bodyText confidence="0.999624083333333">
Given the Hα(·) and Hβ(·) lattices, any sequence
entropy can be computed in constant time. Figure 1
The base case for the dynamic program is
Hα(0|y1) = p(y1) log p(y1). The backward entropy
is computed in a similar fashion. The conditional
probabilities pθ(yi|yi−1, x) in each of these dynamic
programs are available by marginalizing over the
per-transition marginal probabilities obtained from
forward/backward.
The computational complexity of this calcula-
tion for one label sequence requires one run of for-
ward/backward at O(ns2), and equivalent time to
</bodyText>
<page confidence="0.996207">
111
</page>
<bodyText confidence="0.999971894736842">
calculate the lattices for Hα and H�. To calculate
the gradient requires one final iteration over all label
pairs at each position, which is again time O(ns2),
but no greater, as forward/backward and the en-
tropy calculations need only to be done once. The
complete asymptotic computational cost of calcu-
lating the entropy gradient is O(ns2), which is the
same time as supervised training, and a factor of
O(ns) faster than the method proposed by Jiao et
al. (2006).
Wall clock timing experiments show that this
method takes approximately 1.5 times as long as
traditional supervised training—less than the con-
stant factors would suggest.1 In practice, since the
three extra dynamic programs do not require re-
calculation of the dot-product between parameters
and input features (typically the most expensive part
of inference), they are significantly faster than cal-
culating the original forward/backward lattice.
</bodyText>
<sectionHeader confidence="0.996344" genericHeader="method">
5 Confidence Estimation
</sectionHeader>
<bodyText confidence="0.999974692307693">
In addition to its merits for computing the entropy
gradient, subsequence constrained entropy has other
uses, including confidence estimation. Kim et al.
(2006) propose using entropy as a confidence esti-
mator in active learning in CRFs, where examples
with the most uncertainty are selected for presenta-
tion to humans labelers. In practice, they approxi-
mate the entropy of the labels given the N-best la-
bels. Not only could our method quickly and ex-
actly compute the true entropy, but it could also be
used to find the subsequence that has the highest un-
certainty, which could further reduce the additional
human tagging effort.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.995195076923077">
Hernando et al. (2005) present a dynamic program
for calculating the entropy of a HMM, which has
some loose similarities to the forward pass of the
algorithm proposed in this paper. Notably, our algo-
rithm allows for efficient calculation of entropy for
any label subsequence.
Semi-supervised learning has been used in many
models, predominantly for classification, as opposed
to structured output models like CRFs. Zhu (2005)
1Reporting experimental results with accuracy is unneces-
sary since we duplicate the training method of Jiao et al. (2006).
provides a comprehensive survey of popular semi-
supervised learning techniques.
</bodyText>
<sectionHeader confidence="0.999052" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999695">
This paper presents two algorithmic advances. First,
it introduces an efficient method for calculating
subsequence constrained entropies in linear-chain
CRFs, (useful for active learning). Second, it
demonstrates how these subsequence constrained
entropies can be used to efficiently calculate the
gradient of the CRF entropy in time O(ns2)—
the same asymptotic time complexity as the for-
ward/backward algorithm, and a O(ns) improve-
ment over previous algorithms—enabling the prac-
tical application of CRF entropy regularization to
large unlabeled data sets.
</bodyText>
<sectionHeader confidence="0.999066" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9606981">
This work was supported in part by DoD contract #HM1582-
06-1-2013, in part by The Central Intelligence Agency, the Na-
tional Security Agency and National Science Foundation under
NSF grant #IIS-0427594, and in part by the Defense Advanced
Research Projects Agency (DARPA), through the Department
of the Interior, NBC, Acquisition Services Division, under con-
tract number NBCHD030010. Any opinions, findings and con-
clusions or recommendations expressed in this material belong
to the author(s) and do not necessarily reflect those of the spon-
sor.
</bodyText>
<sectionHeader confidence="0.998663" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997909">
Y. Grandvalet and Y. Bengio. 2004. Semi-supervised learning
by entropy minimization. In NIPS.
D. Hernando, V. Crespi, and G. Cybenko. 2005. Efficient com-
putation of the hidden markov model entropy for a given
observation sequence. IEEE Trans. on Information Theory,
51:7:2681–2685.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random fields
for improved sequence segmentation and labeling. In COL-
ING/ACL.
S. Kim, Y. Song, K. Kim, J.-W. Cha, and G. G. Lee. 2006.
Mmr-based active machine learning for bio named entity
recognition. In HLT/NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, pages 282–
289.
X. Zhu. 2005. Semi-supervised learning literature survey.
Technical Report 1530, Computer Sciences, University of
Wisconsin-Madison.
</reference>
<page confidence="0.998294">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953718">
<title confidence="0.984086">Efficient Computation of Entropy Gradient Semi-Supervised Conditional Random Fields</title>
<author confidence="0.999958">S Mann</author>
<affiliation confidence="0.999816">Department of Computer University of</affiliation>
<address confidence="0.993778">Amherst, MA</address>
<abstract confidence="0.999609227272727">Entropy regularization is a straightforward and successful method of semi-supervised learning that augments the traditional conditional likelihood objective function with an additional term that aims to minimize the predicted label entropy on unlabeled data. It has previously been demonstrated to provide positive results in linear-chain CRFs, but the published method for calculating the entropy gradient requires significantly more computation than supervised CRF training. This paper presents a new derivation and dynamic program for calculating the entropy gradient that is significantly more efficient—having the same asymptotic time complexity as supervised CRF training. We also present efficient generalizations of this method for calculating the label entropy of all sub-sequences, which is useful for active learning, among other applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Grandvalet</author>
<author>Y Bengio</author>
</authors>
<title>Semi-supervised learning by entropy minimization.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1698" citStr="Grandvalet and Bengio, 2004" startWordPosition="230" endWordPosition="233">ive learning, among other applications. 1 Introduction Semi-supervised learning is of growing importance in machine learning and NLP (Zhu, 2005). Conditional random fields (CRFs) (Lafferty et al., 2001) are an appealing target for semi-supervised learning because they achieve state-of-the-art performance across a broad spectrum of sequence labeling tasks, and yet, like many other machine learning methods, training them by supervised learning typically requires large annotated data sets. Entropy regularization (ER) is a method of semisupervised learning first proposed for classification tasks (Grandvalet and Bengio, 2004). In addition to maximizing conditional likelihood of the available labels, ER also aims to minimize the entropy of the predicted label distribution on unlabeled data. By insisting on peaked, confident predictions, ER guides the decision boundary away from dense regions of input space. It is simple and compelling—no preclustering, no “auxiliary functions,” tuning of only one meta-parameter and it is discriminative. Jiao et al. (2006) apply this method to linearchain CRFs and demonstrate encouraging accuracy improvements on a gene-name-tagging task. However, the method they present for calculat</context>
</contexts>
<marker>Grandvalet, Bengio, 2004</marker>
<rawString>Y. Grandvalet and Y. Bengio. 2004. Semi-supervised learning by entropy minimization. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hernando</author>
<author>V Crespi</author>
<author>G Cybenko</author>
</authors>
<title>Efficient computation of the hidden markov model entropy for a given observation sequence.</title>
<date>2005</date>
<journal>IEEE Trans. on Information Theory,</journal>
<pages>51--7</pages>
<contexts>
<context position="10735" citStr="Hernando et al. (2005)" startWordPosition="1748" endWordPosition="1751">entropy gradient, subsequence constrained entropy has other uses, including confidence estimation. Kim et al. (2006) propose using entropy as a confidence estimator in active learning in CRFs, where examples with the most uncertainty are selected for presentation to humans labelers. In practice, they approximate the entropy of the labels given the N-best labels. Not only could our method quickly and exactly compute the true entropy, but it could also be used to find the subsequence that has the highest uncertainty, which could further reduce the additional human tagging effort. 6 Related Work Hernando et al. (2005) present a dynamic program for calculating the entropy of a HMM, which has some loose similarities to the forward pass of the algorithm proposed in this paper. Notably, our algorithm allows for efficient calculation of entropy for any label subsequence. Semi-supervised learning has been used in many models, predominantly for classification, as opposed to structured output models like CRFs. Zhu (2005) 1Reporting experimental results with accuracy is unnecessary since we duplicate the training method of Jiao et al. (2006). provides a comprehensive survey of popular semisupervised learning techni</context>
</contexts>
<marker>Hernando, Crespi, Cybenko, 2005</marker>
<rawString>D. Hernando, V. Crespi, and G. Cybenko. 2005. Efficient computation of the hidden markov model entropy for a given observation sequence. IEEE Trans. on Information Theory, 51:7:2681–2685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jiao</author>
<author>S Wang</author>
<author>C-H Lee</author>
<author>R Greiner</author>
<author>D Schuurmans</author>
</authors>
<title>Semi-supervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In COLING/ACL.</booktitle>
<contexts>
<context position="2135" citStr="Jiao et al. (2006)" startWordPosition="297" endWordPosition="300">ypically requires large annotated data sets. Entropy regularization (ER) is a method of semisupervised learning first proposed for classification tasks (Grandvalet and Bengio, 2004). In addition to maximizing conditional likelihood of the available labels, ER also aims to minimize the entropy of the predicted label distribution on unlabeled data. By insisting on peaked, confident predictions, ER guides the decision boundary away from dense regions of input space. It is simple and compelling—no preclustering, no “auxiliary functions,” tuning of only one meta-parameter and it is discriminative. Jiao et al. (2006) apply this method to linearchain CRFs and demonstrate encouraging accuracy improvements on a gene-name-tagging task. However, the method they present for calculating the gradient of the entropy takes substantially greater time than the traditional supervised-only gradient. Whereas supervised training requires only classic forward/backward, taking time O(ns2) (sequence length times the square of the number of labels), their training method takes O(n2s3)—a factor of O(ns) more. This greatly reduces the practicality of using large amounts of unlabeled data, which is exactly the desired use-case.</context>
<context position="5075" citStr="Jiao et al. (2006)" startWordPosition="776" endWordPosition="779">s the sequence length, and s is the number of labels. For semi-supervised training by entropy regularization, we change the objective function by adding the negative entropy of the unannotated data U = hu1..uni. (Here Gaussian prior is also shown.) L(θ; D, U) = X log pθ(Y (d)|x(d)) − X n k + λ X u pθ(Y (u)|x(u)) log pθ(Y (u)|x(u)). This negative entropy term increases as the decision boundary is moved into sparsely-populated regions of input space. 3 An Efficient Form of the Entropy Gradient In order to maximize the above objective function, the gradient for the entropy term must be computed. Jiao et al. (2006) perform this computation by: ∂ ∂θ − H(Y|x) = covpθ(Y|x)[F(x,Y)]θ, where covp,(Y |x)[Fj(x, Y ), Fk(x, Y )] = Ep,(Y |x)[Fj(x, Y ), Fk(x, Y )] − Ep,(Y |x)[Fj(x, Y )]Ep,(Y |x)[Fk(x, Y )]. While the second term of the covariance is easy to compute, the first term requires calculation of quadratic feature expectations. The algorithm they propose to compute this term is O(n2s3) as it requires an extra nested loop in forward/backward. However, the above form of the gradient is not the only possibility. We present here an alternative derivation of the gradient: ∂ X − H(Y |x) = pθ(Y |x) log pθ(Y |x) ∂θ</context>
<context position="9597" citStr="Jiao et al. (2006)" startWordPosition="1571" endWordPosition="1574">kward. The computational complexity of this calculation for one label sequence requires one run of forward/backward at O(ns2), and equivalent time to 111 calculate the lattices for Hα and H�. To calculate the gradient requires one final iteration over all label pairs at each position, which is again time O(ns2), but no greater, as forward/backward and the entropy calculations need only to be done once. The complete asymptotic computational cost of calculating the entropy gradient is O(ns2), which is the same time as supervised training, and a factor of O(ns) faster than the method proposed by Jiao et al. (2006). Wall clock timing experiments show that this method takes approximately 1.5 times as long as traditional supervised training—less than the constant factors would suggest.1 In practice, since the three extra dynamic programs do not require recalculation of the dot-product between parameters and input features (typically the most expensive part of inference), they are significantly faster than calculating the original forward/backward lattice. 5 Confidence Estimation In addition to its merits for computing the entropy gradient, subsequence constrained entropy has other uses, including confiden</context>
<context position="11260" citStr="Jiao et al. (2006)" startWordPosition="1829" endWordPosition="1832">uld further reduce the additional human tagging effort. 6 Related Work Hernando et al. (2005) present a dynamic program for calculating the entropy of a HMM, which has some loose similarities to the forward pass of the algorithm proposed in this paper. Notably, our algorithm allows for efficient calculation of entropy for any label subsequence. Semi-supervised learning has been used in many models, predominantly for classification, as opposed to structured output models like CRFs. Zhu (2005) 1Reporting experimental results with accuracy is unnecessary since we duplicate the training method of Jiao et al. (2006). provides a comprehensive survey of popular semisupervised learning techniques. 7 Conclusion This paper presents two algorithmic advances. First, it introduces an efficient method for calculating subsequence constrained entropies in linear-chain CRFs, (useful for active learning). Second, it demonstrates how these subsequence constrained entropies can be used to efficiently calculate the gradient of the CRF entropy in time O(ns2)— the same asymptotic time complexity as the forward/backward algorithm, and a O(ns) improvement over previous algorithms—enabling the practical application of CRF en</context>
</contexts>
<marker>Jiao, Wang, Lee, Greiner, Schuurmans, 2006</marker>
<rawString>F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuurmans. 2006. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>Y Song</author>
<author>K Kim</author>
<author>J-W Cha</author>
<author>G G Lee</author>
</authors>
<title>Mmr-based active machine learning for bio named entity recognition.</title>
<date>2006</date>
<booktitle>In HLT/NAACL.</booktitle>
<contexts>
<context position="10229" citStr="Kim et al. (2006)" startWordPosition="1661" endWordPosition="1664">ming experiments show that this method takes approximately 1.5 times as long as traditional supervised training—less than the constant factors would suggest.1 In practice, since the three extra dynamic programs do not require recalculation of the dot-product between parameters and input features (typically the most expensive part of inference), they are significantly faster than calculating the original forward/backward lattice. 5 Confidence Estimation In addition to its merits for computing the entropy gradient, subsequence constrained entropy has other uses, including confidence estimation. Kim et al. (2006) propose using entropy as a confidence estimator in active learning in CRFs, where examples with the most uncertainty are selected for presentation to humans labelers. In practice, they approximate the entropy of the labels given the N-best labels. Not only could our method quickly and exactly compute the true entropy, but it could also be used to find the subsequence that has the highest uncertainty, which could further reduce the additional human tagging effort. 6 Related Work Hernando et al. (2005) present a dynamic program for calculating the entropy of a HMM, which has some loose similari</context>
</contexts>
<marker>Kim, Song, Kim, Cha, Lee, 2006</marker>
<rawString>S. Kim, Y. Song, K. Kim, J.-W. Cha, and G. G. Lee. 2006. Mmr-based active machine learning for bio named entity recognition. In HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1272" citStr="Lafferty et al., 2001" startWordPosition="171" endWordPosition="174">py gradient requires significantly more computation than supervised CRF training. This paper presents a new derivation and dynamic program for calculating the entropy gradient that is significantly more efficient—having the same asymptotic time complexity as supervised CRF training. We also present efficient generalizations of this method for calculating the label entropy of all sub-sequences, which is useful for active learning, among other applications. 1 Introduction Semi-supervised learning is of growing importance in machine learning and NLP (Zhu, 2005). Conditional random fields (CRFs) (Lafferty et al., 2001) are an appealing target for semi-supervised learning because they achieve state-of-the-art performance across a broad spectrum of sequence labeling tasks, and yet, like many other machine learning methods, training them by supervised learning typically requires large annotated data sets. Entropy regularization (ER) is a method of semisupervised learning first proposed for classification tasks (Grandvalet and Bengio, 2004). In addition to maximizing conditional likelihood of the available labels, ER also aims to minimize the entropy of the predicted label distribution on unlabeled data. By ins</context>
<context position="3447" citStr="Lafferty et al. (2001)" startWordPosition="491" endWordPosition="494"> that has the same asymptotic time complexity as the gradient for traditional CRF training, O(ns2). In order to describe this calculation, the paper introduces the concept of subsequence constrained entropy—the entropy of a CRF for an observed data sequence when part of the label sequence is fixed. These methods will allow training on larger unannotated data set sizes than previously possible and support active 109 Proceedings of NAACL HLT 2007, Companion Volume, pages 109–112, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics learning. 2 Semi-Supervised CRF Training Lafferty et al. (2001) present linear-chain CRFs, a discriminative probabilistic model over observation sequences x and label sequences Y = hY1..Yni, where |x |= |Y |= n, and each label Yi has s different possible discrete values. For a linear-chain CRF of Markov order one: pθ(Y |x) = Z1x) exp XθkFk(x,Y ) , k where Fk(x, Y ) = Pi fk(x, Yi, Yi+1, i), and the partition function Z(x) = PY exp(Pk θkFk(x,Y )). Given training data D = hd1..dni, the model is trained by maximizing the log-likelihood of the data L(θ; D) = Pd log pθ(Y (d)|x(d)) by gradient methods (e.g. Limited Memory BFGS), where the gradient of the likelih</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<contexts>
<context position="1214" citStr="Zhu, 2005" startWordPosition="164" endWordPosition="165">the published method for calculating the entropy gradient requires significantly more computation than supervised CRF training. This paper presents a new derivation and dynamic program for calculating the entropy gradient that is significantly more efficient—having the same asymptotic time complexity as supervised CRF training. We also present efficient generalizations of this method for calculating the label entropy of all sub-sequences, which is useful for active learning, among other applications. 1 Introduction Semi-supervised learning is of growing importance in machine learning and NLP (Zhu, 2005). Conditional random fields (CRFs) (Lafferty et al., 2001) are an appealing target for semi-supervised learning because they achieve state-of-the-art performance across a broad spectrum of sequence labeling tasks, and yet, like many other machine learning methods, training them by supervised learning typically requires large annotated data sets. Entropy regularization (ER) is a method of semisupervised learning first proposed for classification tasks (Grandvalet and Bengio, 2004). In addition to maximizing conditional likelihood of the available labels, ER also aims to minimize the entropy of </context>
<context position="11138" citStr="Zhu (2005)" startWordPosition="1812" endWordPosition="1813">ute the true entropy, but it could also be used to find the subsequence that has the highest uncertainty, which could further reduce the additional human tagging effort. 6 Related Work Hernando et al. (2005) present a dynamic program for calculating the entropy of a HMM, which has some loose similarities to the forward pass of the algorithm proposed in this paper. Notably, our algorithm allows for efficient calculation of entropy for any label subsequence. Semi-supervised learning has been used in many models, predominantly for classification, as opposed to structured output models like CRFs. Zhu (2005) 1Reporting experimental results with accuracy is unnecessary since we duplicate the training method of Jiao et al. (2006). provides a comprehensive survey of popular semisupervised learning techniques. 7 Conclusion This paper presents two algorithmic advances. First, it introduces an efficient method for calculating subsequence constrained entropies in linear-chain CRFs, (useful for active learning). Second, it demonstrates how these subsequence constrained entropies can be used to efficiently calculate the gradient of the CRF entropy in time O(ns2)— the same asymptotic time complexity as the</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>X. Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>