<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000041">
<title confidence="0.9969405">
Improving the Estimation of Word Importance for News Multi-Document
Summarization
</title>
<author confidence="0.998592">
Kai Hong
</author>
<affiliation confidence="0.999113">
University of Pennsylvania
</affiliation>
<address confidence="0.857721">
Philadelphia, PA, 19104
</address>
<email confidence="0.99741">
hongkai1@seas.upenn.edu
</email>
<sectionHeader confidence="0.997368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999648">
We introduce a supervised model
for predicting word importance that
incorporates a rich set of features. Our
model is superior to prior approaches
for identifying words used in human
summaries. Moreover we show
that an extractive summarizer using
these estimates of word importance is
comparable in automatic evaluation with
the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983185185185">
In automatic extractive summarization, sentence
importance is calculated by taking into account,
among possibly other features, the importance
of words that appear in the sentence. In this
paper, we describe experiments on identifying
words from the input that are also included in
human summaries; we call such words summary
keywords. We review several unsupervised
approaches for summary keyword identification
and further combine these, along with features
including position, part-of-speech, subjectivity,
topic categories, context and intrinsic importance,
in a superior supervised model for predicting word
importance.
One of the novel features we develop aims
to determine the intrinsic importance of words.
To this end, we analyze abstract-article pairs in
the New York Times corpus (Sandhaus, 2008)
to identify words that tend to be preserved in
the abstracts. We demonstrate that judging word
importance just based on this criterion leads to
significantly higher performance than selecting
sentences at random. Identifying intrinsically
important words allows us to generate summaries
without doing any feature computation on the
input, equivalent in quality to the standard baseline
of extracting the first 100 words from the latest
</bodyText>
<author confidence="0.338922">
Ani Nenkova
</author>
<affiliation confidence="0.686518">
University of Pennsylvania
</affiliation>
<address confidence="0.491579">
Philadelphia, PA, 19104
</address>
<email confidence="0.978141">
nenkova@seas.upenn.edu
</email>
<bodyText confidence="0.9986475">
article in the input. Finally, we integrate the
schemes for assignment of word importance into
a summarizer which greedily optimizes for the
presence of important words. We show that our
better estimation of word importance leads to
better extractive summaries.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="introduction">
2 Prior work
</sectionHeader>
<bodyText confidence="0.997833666666667">
The idea of identifying words that are descriptive
of the input can be dated back to Luhn’s earliest
work in automatic summarization (Luhn, 1958).
There keywords were identified based on the
number of times they appeared in the input,
and words that appeared most and least often
were excluded. Then the sentences in which
keywords appeared near each other, presumably
better conveying the relationship between the
keywords, were selected to form a summary.
Many successful recent systems also estimate
word importance. The simplest but competitive
way to do this task is to estimate the word
probability from the input (Nenkova and
Vanderwende, 2005). Another powerful method
is log-likelihood ratio test (Lin and Hovy, 2000),
which identifies the set of words that appear in
the input more often than in a background corpus
(Conroy et al., 2006; Harabagiu and Lacatusu,
2005).
In contrast to selecting a set of keywords,
weights are assigned to all words in the input
in the majority of summarization methods.
Approaches based on (approximately) optimizing
the coverage of these words have become widely
popular. Earliest such work relied on TF*IDF
weights (Filatova and Hatzivassiloglou, 2004),
later approaches included heuristics to identify
summary-worthy bigrams (Riedhammer et al.,
2010). Most optimization approaches, however,
use TF*IDF or word probability in the input as
word weights (McDonald, 2007; Shen and Li,
2010; Berg-Kirkpatrick et al., 2011).
</bodyText>
<page confidence="0.944072">
712
</page>
<note confidence="0.9930635">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999784869565217">
Word weights have also been estimated by
supervised approaches, with word probability and
location of occurrence as typical features (Yih et
al., 2007; Takamura and Okumura, 2009; Sipos et
al., 2012).
A handful of investigations have productively
explored the mutually reinforcing relationship
between word and sentence importance, iteratively
re-estimating each in either supervised or
unsupervised framework (Zha, 2002; Wan et
al., 2007; Wei et al., 2008; Liu et al., 2011).
Most existing work directly focuses on predicting
sentence importance, with emphasis on the
formalization of the problem (Kupiec et al., 1995;
Celikyilmaz and Hakkani-Tur, 2010; Litvak et al.,
2010). There has been little work directly focused
on predicting keywords from the input that will
appear in human summaries. Also there has been
only a few investigations of suitable features
for estimating word importance and identifying
keywords in summaries; we address this issue by
exploring a range of possible indicators of word
importance in our model.
</bodyText>
<sectionHeader confidence="0.990789" genericHeader="method">
3 Data and Planned Experiments
</sectionHeader>
<bodyText confidence="0.999946521739131">
We carry out our experiments on two datasets from
the Document Understanding Conference (DUC)
(Over et al., 2007). DUC 2003 is used for training
and development, DUC 2004 is used for testing.
These are the last two years in which generic
summarization was evaluated at DUC workshops.
There are 30 multi-document clusters in DUC
2003 and 50 in DUC 2004, each with about 10
news articles on a related topic. The task is
to produce a 100-word generic summary. Four
human abstractive summaries are available for
each cluster.
We compare different keyword extraction
methods by the F-measurer they achieve against
the gold-standard summary keywords. We do not
use stemming when calculating these scores.
In our work, keywords for an input are defined
as those words that appear in at least i of the
human abstracts, yielding four gold-standard sets
of keywords, denoted by Gi. |Gi |is thus the
cardinality of the set for the input. We only
consider the words in the summary that also
appear in the original input2, with stopwords
</bodyText>
<footnote confidence="0.765024">
12*precision*recall/(precision+recall)
2On average 26.3% (15.0% with stemming) of the words
in the four abstracts never appear in the input.
</footnote>
<bodyText confidence="0.990622666666667">
excluded3. Table 1 shows the average number of
unique content words for the respective keyword
gold-standard.
</bodyText>
<table confidence="0.732066">
i 1 2 3 4
Mean |Gi |102 32 15 6
</table>
<tableCaption confidence="0.998922">
Table 1: Average number of words in Gi
</tableCaption>
<bodyText confidence="0.998597923076923">
For the summarization task, we compare results
using ROUGE (Lin, 2004). We report ROUGE-1,
-2, -4 recall, with stemming and without removing
stopwords. We consider ROUGE-2 recall as
the main metric for this comparison due to its
effectiveness in comparing machine summaries
(Owczarzak et al., 2012). All of the summaries
were truncated to the first 100 words by ROUGE4.
We use Wilcoxon signed-rank test to examine
the statistical significance as advocated by Rankel
et al. (2011) for both tasks, and consider
differences to be significant if the p-value is less
than 0.05.
</bodyText>
<sectionHeader confidence="0.993484" genericHeader="method">
4 Unsupervised Word Weighting
</sectionHeader>
<bodyText confidence="0.999419333333333">
In this section we describe three unsupervised
approaches of assigning importance weights to
words. The first two are probability and
log-likelihood ratio, which have been extensively
used in prior work. We also apply a markov
random walk model for keyword ranking, similar
to Mihalcea and Tarau (2004). In the next
section we describe a summarizer that uses these
weights to form a summary and then describe
our regression approach to combine these and
other predictors in order to achieve more accurate
predictions for the word importance in Section 7.
The task is to assign a score to each word in the
input. The keywords extracted are thus the content
words with highest scores.
</bodyText>
<subsectionHeader confidence="0.996555">
4.1 Word Probability (Prob)
</subsectionHeader>
<bodyText confidence="0.998911">
The frequency with which a word occurs in the
input is often considered as an indicator of its
importance. The weight for a word is computed
as p(w) = c�w)
N , where c(w) is the number of
times word w appears in the input and N is the
total number of word tokens in the input.
</bodyText>
<footnote confidence="0.98866875">
3We use the stopword list from the SMART system
(Salton, 1971), augmented with punctuation and symbols.
4ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n
4 -m -a -l 100 -x
</footnote>
<page confidence="0.997132">
713
</page>
<subsectionHeader confidence="0.993148">
4.2 Log-likelihood Ratio (LLR)
</subsectionHeader>
<bodyText confidence="0.99999">
The log-likelihood ratio test (Lin and Hovy, 2000)
compares the distribution of a word in the input
with that in a large background corpus to identify
topic words. We use the Gigaword corpus (Graff et
al., 2007) for background counts. The test statistic
has a x2 distribution, so a desired confidence level
can be chosen to find a small set of topic words.
</bodyText>
<subsectionHeader confidence="0.999616">
4.3 Markov Random Walk Model (MRW)
</subsectionHeader>
<bodyText confidence="0.9998959375">
Graph methods have been successfully applied to
weighting sentences for generic (Wan and Yang,
2008; Mihalcea and Tarau, 2004; Erkan and
Radev, 2004) and query-focused summarization
(Otterbacher et al., 2009).
Here instead of constructing a graph with
sentences as nodes and edges weighted by
sentence similarity, we treat the words as vertices,
similar to Mihalcea and Tarau (2004). The
difference in our approach is that the edges
between the words are defined by syntactic
dependencies rather than depending on the
co-occurrence of words within a window of k. We
use the Stanford dependency parser (Marneffe et
al., 2006). In our approach, we consider a word
w more likely to be included in a human summary
when it is syntactically related to other (important)
words, even if w itself is not mentioned often.
The edge weight between two vertices is equal to
the number of syntactic dependencies of any type
between two words within the same sentence in
the input. The weights are then normalized by
summing up the weights of edges linked to one
node.
We apply the Pagerank algorithm (Lawrence
et al., 1998) on the resulting graph. We set the
probability of performing random jump between
nodes A=0.15. The algorithm terminates when
the change of node weight between iterations is
smaller than 10−4 for all nodes. Word importance
is equal to the final weight of its corresponding
node in the graph.
</bodyText>
<sectionHeader confidence="0.977114" genericHeader="method">
5 Summary Generation Process
</sectionHeader>
<bodyText confidence="0.999893857142857">
In this section, we outline how summaries
are generated by a greedy optimization system
which selects the sentence with highest weight
iteratively. This is the main process we use in all
our summarization systems. For comparison we
also use a summarization algorithm based on KL
divergence.
</bodyText>
<subsectionHeader confidence="0.990861">
5.1 Greedy Optimization Approach
</subsectionHeader>
<bodyText confidence="0.999985565217391">
Our algorithm extracts sentences by weighting
them based on word importance. The approach is
similar to the standard word probability baseline
(Nenkova et al., 2006) but we explore a range
of possibilities for assigning weights to individual
words. For each sentence, we calculate the
sentence weight by summing up the weights of
all words, normalized by the number of words in
the sentence. We sort the sentences in descending
order of their scores into a queue. To create a
summary, we iteratively dequeue one sentence,
check if the sentence is more than 8 words (as
in Erkan and Radev (2004)), then append it to
the current summary if it is non-redundant. A
sentence is considered non-redundant if it is not
similar to any sentences already in the summary,
measured by cosine similarity on binary vector
representations with stopwords excluded. We use
the cut-off of 0.5 for cosine similarity. This value
was tuned on the DUC 2003 dataset, by testing the
impact of the cut-off value on the ROUGE scores
for the final summary. Possible values ranged
from 0.1 to 0.9 with step of 0.1.
</bodyText>
<subsectionHeader confidence="0.996209">
5.2 KL Divergence Summarizer
</subsectionHeader>
<bodyText confidence="0.999900181818182">
The KLSUM summarizer (Haghighi and
Vanderwende, 2009) aims at minimizing the KL
divergence between the probability distribution
over words estimated from the summary and
the input respectively. This summarizer is a
component of the popular topic model approaches
(Daum´e and Marcu, 2006; Celikyilmaz and
Hakkani-T¨ur, 2011; Mason and Charniak, 2011)
and achieves competitive performance with
minimal differences compared to a full-blown
topic model system.
</bodyText>
<sectionHeader confidence="0.986416" genericHeader="method">
6 Global Indicators from NYT
</sectionHeader>
<bodyText confidence="0.99989575">
Some words evoke topics that are of intrinsic
interest to people. Here we search for global
indicators of word importance regardless of
particular input.
</bodyText>
<subsectionHeader confidence="0.995379">
6.1 Global Indicators of Word Importance
</subsectionHeader>
<bodyText confidence="0.999950166666667">
We analyze a large corpus of original documents
and corresponding summaries in order to identify
words that consistently get included in or excluded
from the summary. In the 2004-2007 NYT corpus,
many news articles have abstracts along with the
original article, which makes it an appropriate
</bodyText>
<page confidence="0.936815">
714
</page>
<equation confidence="0.543353">
Metric
Top-30 words
KL(A 11 G)(w)
KL(G 11 A)(w)
</equation>
<bodyText confidence="0.7064905">
photo(s), pres, article, column, reviews, letter, York, Sen, NY, discusses, drawing, op-ed, holds, Bush
correction, editorial, dept, city, NJ, map, corp, graph, contends, Iraq, John, dies, sec, state, comments
Mr, Ms, p.m., lot, Tuesday, CA, Wednesday, Friday, told, Monday, time, a.m., added, thing, Sunday
things, asked, good, night, Saturday, nyt, back, senator, wanted, kind, Jr., Mrs, bit, looked, wrote
</bodyText>
<equation confidence="0.750822">
Pra(w)
</equation>
<bodyText confidence="0.754836">
photo, photos, article, York, column, letter, Bush, state, reviews, million, American
pres, percent, Iraq, year, people, government, John, years, company, correction
national, federal, officials, city, drawing, billion, public, world, administration
</bodyText>
<tableCaption confidence="0.97702">
Table 2: Top 30 words by three metrics from NYT corpus
</tableCaption>
<bodyText confidence="0.999942409090909">
resource to do such analysis. We identified
160,001 abstract-original pairs in the corpus.
From these, we generate two language models,
one estimated from the text of all abstracts (LMA),
the other estimated from the corpus of original
articles (LMG). We use SRILM (Stolcke, 2002)
with Ney smoothing.
We denote the probability of word w in LMA as
PrA(w), the probability in LMG as PrG(w), and
calculate the difference PrA(w)−PrG(w) and the
ratio PrA(w)/PrG(w) to capture the change of
probability. In addition, we calculate KL-like
weighted scores for words which reflect both the
change of probabilities between the two samples
and the overall frequency of the word. Here
we calculate both KL(A ∥ G) and KL(G ∥
A). Words with high values for the former score
are favored in the summaries because they have
higher probability in the abstracts than in the
originals and have relatively high probability in
the abstracts. The later score is high for words that
are often not included in summaries.
</bodyText>
<equation confidence="0.991364">
KL(A ∥ G)(w) = PrA(w) · ln PrG(w)
PrA(w)
PrG(w)
Pr G(w) · ln PrA(w)
∥
KL(G A)(w)
</equation>
<bodyText confidence="0.999942666666667">
Table 2 shows examples of the global
information captured from the three types
of scores—KL(A ∥ G), KL(G ∥ A) and
PrA(w)—listing the 30 content words with
highest scores for each type. Words that tend to
be used in the summaries, characterized by high
KL(A ∥ G) scores, include locations (York, NJ,
Iraq), people’s names and titles (Bush, Sen, John),
some abbreviations (pres, corp, dept) and verbs of
conflict (contends, dies). On the other hand, from
KL(G ∥ A), we can see that it is unlikely for
writers to include courtesy titles (Mr, Ms, Jr.) and
relative time reference in summaries. The words
with high PrA(w) scores overlaps with those
ranked highly by KL(A ∥ G) to some extent,
but also includes a number of generally frequent
words which appeared often both in the abstracts
and original texts, such as million and percent.
</bodyText>
<subsectionHeader confidence="0.999913">
6.2 Blind Sentence Extraction
</subsectionHeader>
<bodyText confidence="0.963334866666667">
In later sections we include the measures of
global word importance as a feature of our
regression model for predicting word weights for
summarization. Before turning to that, however,
we report the results of an experiment aimed to
confirm the usefulness of these features. We
present a system, BLIND, which uses only weights
assigned to words by KL(A ∥ G) from NYT,
without doing any analysis of the original input.
We rank all non-stopword words from the input
according to this score. The top k words are given
weight 1, while the others are given weight 0.
The summaries are produced following the greedy
procedure described in Section 5.1.
Systems R-1 R-2 R-4
</bodyText>
<table confidence="0.8842266">
30.32 4.42 0.36
30.77 5.18 0.53
32.91 5.94 0.61
31.39 6.11 0.63
34.26 7.22 1.21
</table>
<tableCaption confidence="0.848789">
Table 3: Blind sentence extraction system,
compared with three baseline systems (%)
</tableCaption>
<bodyText confidence="0.990500583333333">
Table 3 shows that the BLIND system has R-2
recall of 0.0594 using the top 300 keywords,
significantly better than picking sentences from
the input randomly. It also achieves comparable
performance with the baseline in DUC 2004,
formed by selecting the first 100 words from
the latest article in the input (LASTESTLEAD).
However it is significantly worse than another
baseline of selecting the first sentences from the
input. Table 4 gives sample summaries generated
by these three approaches. These results confirm
that the information gleaned from the analysis
</bodyText>
<figure confidence="0.7389692">
RANDOM
BLIND (80 keywords)
BLIND (300 keywords)
LASTESTLEAD
FIRST-SENTENCE
</figure>
<page confidence="0.960698">
715
</page>
<subsectionHeader confidence="0.867877">
Random Summary
</subsectionHeader>
<bodyText confidence="0.784057636363636">
It was sunny and about 14 degrees C(57 degrees F) in Tashkent on Sunday. The president is a strong person, and he has been
through far more difficult political situations, Mityukov said, according to Interfax. But Yeltsin’s aides say his first term,
from 1991 to 1996, does not count because it began six months before the Soviet Union collapsed and before the current
constitution took effect. He must stay in bed like any other person, Yakushkin said. The issue was controversial earlier this
year when Yeltsin refused to spell out his intentions and his aides insisted he had the legal right to seek re-election.
NYT Summary from global keyword selection, KL(A ∥ G), k = 300
Russia’s constitutional court opened hearings Thursday on whether Boris Yeltsin can seek a third term. Yeltsin’s growing
health problems would also seem to rule out another election campaign. The Russian constitution has a two-term limit for
presidents. Russian president Boris Yeltsin cut short a trip to Central Asia on Monday due to a respiratory infection that
revived questions about his overall health and ability to lead Russia through a sustained economic crisis. The upper house of
parliament was busy voting on a motion saying he should resign. The start of the meeting was shown on Russian television.
</bodyText>
<table confidence="0.750412666666667">
First Sentence Generated Summary
President Boris Yeltsin has suffered minor burns on his right hand, his press office said Thursday. President Boris Yeltsin’s
doctors have pronounced his health more or less normal, his wife Naina said in an interview published Wednesday. President
Boris Yeltsin, on his first trip out of Russia since this spring, canceled a welcoming ceremony in Uzbekistan on Sunday
because he wasn’t feeling well, his spokesman said. Doctors ordered Russian President Boris Yeltsin to cut short his Central
Asian trip because of a respiratory infection and he agreed to return home Monday, a day earlier than planned, officials said.
</table>
<tableCaption confidence="0.998024">
Table 4: Summary comparison by Random, Blind Extraction and First Sentence systems
</tableCaption>
<bodyText confidence="0.999597333333333">
of NYT abstract-original pairs encodes highly
relevant information about important content
independent of the actual text of the input.
</bodyText>
<sectionHeader confidence="0.994225" genericHeader="method">
7 Regression-Based Keyword Extraction
</sectionHeader>
<bodyText confidence="0.999970041666666">
Here we introduce a logistic regression model
for assigning importance weights to words in the
input. Crucially, this model combines evidence
from multiple indicators of importance. We have
at our disposal abundant data for learning because
each content word in the input can be treated as
a labeled instance. There are in total 32,052
samples from the 30 inputs of DUC 2003 for
training, 54,591 samples from the 50 inputs of
DUC 2004 for testing. For a word in the input,
we assign label 1 if the word appears in at least
one of the four human summaries for this input.
Otherwise we assign label 0.
In the rest of this section, we describe the rich
variety of features included in our system. We also
analyze and discuss the predictive power of those
features by performing Wilcoxon signed-rank test
on the DUC 2003 dataset. There are in total 9, 261
features used, among them 1, 625 are significant
(p-value &lt; 0.05). We rank these features in
increasing p-values derived from Wilcoxon test.
Apart from the widely used features of word
frequency and positions, some other less explored
features are highly significant.
</bodyText>
<subsectionHeader confidence="0.992506">
7.1 Frequency Features
</subsectionHeader>
<bodyText confidence="0.9999902">
We use the Probability, LLR chi-square statistic
value and MRW scores as features. Since prior
work has demonstrated that for LLR weights in
particular, it is useful to identify a small set of
important words and ignore all other words in
summary selection (Gupta et al., 2007), we use
a number of keyword indicators as features. For
these indicators, the value of feature is 1 if the
word is ranked within top ki, 0 otherwise. Here ki
are preset cutoffs5. These cutoffs capture different
possibilities for defining the keywords in the input.
We also add the number of input documents that
contain the word as a feature. There are a total of
100 features in this group, all of which are highly
significant, ranked among the top 200.
</bodyText>
<subsectionHeader confidence="0.997562">
7.2 Standard features
</subsectionHeader>
<bodyText confidence="0.999715105263158">
We now describe some standard features which
have been applied in prior work on summarization.
Word Locations: Especially in news articles,
sentences that occur at the beginning are often the
most important ones. In line with this observation,
we calculate several features related to the position
in which a word appears. We first compute
the relative positions for word tokens, where
the tokens are numbered sequentially in order of
appearance in each document in the input. The
relative position for one word token is therefore
its corresponding number, divided by total number
of tokens minus one in the document, e.g., 0
for the first token, 1 for the last token. For
each word, we calculate its earliest first location,
latest last location, average location and average
first location for tokens of this word across all
documents in the input. In addition we have a
binary feature indicating if the word appears in the
</bodyText>
<footnote confidence="0.5686025">
510, 15, 20, 30, 40, · · · , 190, 200, 220, 240, 260, 280,
300, 350, 400, 450, 500, 600, 700 (in total 33 values)
</footnote>
<page confidence="0.998051">
716
</page>
<bodyText confidence="0.99996545">
first sentence and the number of times it appears
in a first sentence among documents in one input.
There are 6 features in this group. All of them are
very significant, ranked within the top 100.
Word type: These features include Part of
Speech (POS) tags, Name Entity (NE) labels and
capitalization information. We use the Stanford
POS-Tagger (Toutanova et al., 2003) and Name
Entity Recognizer (Finkel et al., 2005). We have
one feature corresponding to each possible POS
and NE tag. The value of this feature is the
proportion of occurrences of the word with this
tag; in most cases only one feature gets a non-zero
value. We have two features which indicate if
one word has been capitalized and the ratio of its
capitalized occurrences.
Most of the NE features (6 out of 8) are
significant: there are more Organizations and
Locations but fewer Time and Date words in the
human summaries. Of the POS tags, 11 out of 41
are significant: there are more nouns (NN, NNS,
NNPS); fewer verbs (VBG, VBP, VB) and fewer
cardinal numbers in the abstracts compared to the
input. Capitalized words also tend to be included
in human summaries.
KL: Prior work has shown that having estimates
of sentence importance can also help in estimating
word importance (Wan et al., 2007; Liu et al.,
2011; Wei et al., 2008). The summarizer based
on KL-divergence assigns importance to sentences
directly, in a complex function according to the
word distribution in the sentence. Therefore,
we use these summaries as potential indicators
of word importance. We include two features
here, the first one indicates if the word appears
in a KLSUM summary of the input, as well as
a feature corresponding to the number of times
the word appeared in that summary. Both of the
features are highly significant, ranked within the
top 200.
</bodyText>
<subsectionHeader confidence="0.971322">
7.3 NYT-weights as Features
</subsectionHeader>
<bodyText confidence="0.982451461538462">
We include features from the relative rank of
a word according to KL(A 11 G), KL(G 11
A), PrA(w)−PrG(w), PrA(w)/PrG(w) and
PrA(w), derived from the NYT as described in
Section 6. If the rank of a word is within top-k
or bottom-k by one metric, we would label it as
1, where k is selected from a set of pre-defined
values6. We have in total 70 features in this
6100, 200, 500, 1000, 2000, 5000, 10000 in this case.
category, of which 56 are significant, 47 having
a p-value less than 10−7. The predictive power of
those global indicators are only behind the features
which indicates frequency and word positions.
</bodyText>
<subsectionHeader confidence="0.9685">
7.4 Unigrams
</subsectionHeader>
<bodyText confidence="0.999996555555556">
This is a binary feature corresponding to each
of the words that appeared at least twice in the
training data. The idea is to learn which words
from the input tend to be mentioned in the human
summaries. There are in total 8,691 unigrams,
among which 1, 290 are significant. Despite the
high number of significant unigram features, most
of them are not as significant as the more general
ones we described so far. It is interesting to
compare the significant unigrams identified in the
DUC abstract/input data with those derived from
the NYT corpus. Unigrams that tend to appear in
DUC summaries include president, government,
political. We also find the same unigrams among
the top words from NYT corpus according to
KL(A 11 G) . As for words unlikely to appear in
summaries, we see Wednesday, added, thing, etc,
which again rank high according to KL(G 11 A).
</bodyText>
<subsectionHeader confidence="0.970225">
7.5 Dictionary Features: MPQA and LIWC
</subsectionHeader>
<bodyText confidence="0.999995769230769">
Unigram features are notoriously sparse. To
mitigate the sparsity problem, we resort to
more general groupings to words according to
salient semantic and functional categories. We
employ two hand-crafted dictionaries, MPQA for
subjectivity analysis and LIWC for topic analysis.
The MPQA dictionary (Wiebe and Cardie,
2005) contains words with different polarities
(positive, neutral, negative) and intensities (strong,
weak). The combinations correspond to six
features. It turns out that words with strong
polarity, either positive or negative, are seldomly
included in the summaries. Most strikingly,
the p-value from significance test for the strong
negative words is less than 10−4—these words
are rarely included in summaries. There is no
significant difference on weak polarity categories.
Another dictionary we use is LIWC (Tausczik
and Pennebaker, 2007), which contains manually
constructed dictionaries for multiple categories
of words. The value of the feature is 1 for
one word if the word appears in the particular
dictionary for the category. 34 out of 64 LIWC
features are significant. Interesting categories
which appear at higher rate in summaries include
events about death, anger, achievements, money
</bodyText>
<page confidence="0.992026">
717
</page>
<bodyText confidence="0.99997725">
and negative emotions. Those that appear at lower
rate in the summaries include auxiliary verbs, hear,
pronouns, negation, function words, social words,
swear, adverbs, words related to families, etc.
</bodyText>
<subsectionHeader confidence="0.993467">
7.6 Context Features
</subsectionHeader>
<bodyText confidence="0.999989181818182">
We use context features here, based on the
assumption that context importance around a word
affects the importance of this word. For context
we consider the words before and after the target
word. We extend our feature space by calculating
the weighted average of the feature values of the
context words. For word w, we denote Lw as the
set of words before w, Rw as the set of words
after w. We denote the feature for one word as
w.fi, the way of calculating the newly extended
word-before feature w.lf, could be written as:
</bodyText>
<equation confidence="0.9858955">
∑w.lfz = p(wl) · wl.fi, ∀wl ∈ Lw
i
</equation>
<bodyText confidence="0.998734125">
Here p(wl) is the probability word wl appears
before w among all words in Lw.
For context features, we calculate the weighted
average of the most widely used basic features,
including frequency, location and capitalization
for surrounding contexts. There are in total
220 features of this kind, among which 117 are
significant, 74 having a p-value less than 10−4.
</bodyText>
<sectionHeader confidence="0.998306" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.99979725">
The performance of our logistic regression model
is evaluated on two tasks: keyword identification
and extractive summarization. We name our
system REGSUM.
</bodyText>
<subsectionHeader confidence="0.931196">
8.1 Regression for Keyword Identification
</subsectionHeader>
<bodyText confidence="0.967426769230769">
For each input, we define the set of keywords
as the top k words according to the scores
generated from different models. We compare
our regression system with three unsupervised
systems: PROB, LLR, MRW. To show the
effectiveness of new features, we compare our
results with a regression system trained only
on word frequency and location related features
described in Section 7. Those features are the
ones standardly used for ranking the importance
of words in recent summarization works (Yih et
al., 2007; Takamura and Okumura, 2009; Sipos et
al., 2012), and we name this system REGBASIC.
</bodyText>
<figureCaption confidence="0.737012666666667">
Figure 1 shows the performance of systems
when selecting the 100 words with highest weights
Figure 1: Precision, Recall and F-score of
</figureCaption>
<bodyText confidence="0.97983075">
keyword identification, 100 words selected, G1 as
gold-standard
as keywords. Each word from the input that
appeared in any of the four human summaries is
considered as a gold-standard keyword. Among
the unsupervised approaches, word probability
identifies keywords better than LLR and MRW
by at least 4% on F-score. REGBASIC does not
give better performance at keyword identification
compared with PROB, even though it includes
location information. Our system gets 2.2%
F-score improvement over PROB, 5.2% over
REGBASIC, and more improvement over the
other approaches. All of these improvements are
statistically significant by Wilcoxon test.
Table 5 shows the performance of keyword
identification for different Gi and different
number of keywords selected. The regression
system has no advantage over PROB when
identifying keywords that appeared in all of the
four human summaries. However our system
achieves significant improvement for predicting
words that appeared in more than one or two
human summaries.
</bodyText>
<subsectionHeader confidence="0.968736">
8.2 Regression for Summarization
</subsectionHeader>
<bodyText confidence="0.999890333333333">
We now show that the performance of extractive
summarization can be improved by better
estimation of word weights. We compare our
regression system with the four models introduced
in Section 8.1. We also include PEER-65, the best
system in DUC-2004, as well as KLSUM for
comparison. Apart from these, we compare our
model with two state-of-the-art systems, including
the submodular approach (SUBMOD) (Lin and
</bodyText>
<footnote confidence="0.9979432">
7We also apply a weighted keyword evaluation approach,
similar to the pyramid method for summarization. Still
our system shows significant improvement over the others.
See https://www.seas.upenn.edu/~hongkai1/regsum.html for
details.
</footnote>
<page confidence="0.977896">
718
</page>
<table confidence="0.999775769230769">
Gi #words PROB LLR MRW REGBASIC REGSUM
G1 80 43.6 37.9 38.9 39.9 45.7
G1 100 44.3 38.7 39.2 41.0 46.5
G1 120 44.6 38.5 39.2 40.9 46.4
G2 30 47.8 44.0 42.4 47.4 50.2
G2 35 47.1 43.3 42.1 47.0 49.5
G2 40 46.5 42.4 41.8 46.4 49.2
G3 10 51.2 46.2 43.8 46.9 50.2
G3 15 51.4 47.5 43.7 49.8 52.9
G3 20 49.7 47.6 42.5 49.3 51.5
G4 5 50.0 48.8 44.9 43.6 45.1
G4 6 51.4 46.9 43.7 45.2 47.6
G4 7 50.9 48.2 43.7 45.8 47.8
</table>
<tableCaption confidence="0.99961">
Table 5: Keyword identification F-score (%) for different Gi and different number of words selected.
</tableCaption>
<bodyText confidence="0.9997685625">
Bilmes, 2012) and the determinantal point process
(DPP) summarizer (Kulesza and Taskar, 2012).
The summaries were kindly provided by the
authors of these systems (Hong et al., 2014).
As can been seen in Table 6, our system
outperforms PROB, LLR, MRW, PEER-65,
KLSUM and REGBASIC. These improvements
are significant on ROUGE-2 recall. Interestingly,
although the supervised system REGBASIC which
uses only frequency and positions achieve
low performance in keyword identification, the
summaries it generates are of high quality. The
inclusion of position features negatively affects the
performance in summary keyword identification
but boosts the weights for the words which appear
close to the beginning of the documents, which is
helpful for identifying informative sentences. By
including other features we greatly improve over
REGBASIC in keyword identification. Similarly
here the richer set of features results in better
quality summaries.
We also examined the ROUGE-1, -2, -4
recall compared with the SUBMOD and DPP
summarizers8. There is no significant difference
on R-2 and R-4 recall compared with these
two state-of-the-art systems. DPP performed
significantly better than our system on R-1 recall,
but that system is optimizing on R-1 F-score in
training. Overall, our conceptually simple system
is on par with the state of the art summarizers and
points to the need for better models for estimating
word importance.
</bodyText>
<footnote confidence="0.93808">
8The results are slightly different from the ones reported
in the original papers due to the fact that we truncated to 100
words, while they truncated to 665 bytes.
</footnote>
<table confidence="0.999866">
System R-1 R-2 R-4
PROB 35.14 8.17 1.06
LLR 34.60 7.56 0.83
MRW 35.78 8.15 0.99
REGBASIC 37.56 9.28 1.49
KL 37.97 8.53 1.26
PEER-65 37.62 8.96 1.51
SUBMOD 39.18 9.35 1.39
DPP 39.79 9.62 1.57
REGSUM 38.57 9.75 1.60
</table>
<tableCaption confidence="0.995452">
Table 6: System performance comparison (%)
</tableCaption>
<sectionHeader confidence="0.994686" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999991466666667">
We presented a series of experiments which
show that keyword identification can be improved
in a supervised framework which incorporates
a rich set of indicators of importance. We
also show that the better estimation of word
importance leads to better extractive summaries.
Our analysis of features related to global
importance, sentiment and topical categories
reveals rather unexpected results and confirms that
word importance estimation is a worthy research
direction. Success in the task is likely to improve
sophisticated summarization approaches too, as
well as sentence compression systems which use
only crude frequency related measures to decide
which words should be deleted from a sentence.9
</bodyText>
<footnote confidence="0.9959305">
9The work is partially funded by NSF CAREER award
IIS 0953445.
</footnote>
<page confidence="0.996958">
719
</page>
<sectionHeader confidence="0.996216" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999587198019802">
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481–490.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010.
A hybrid hierarchical model for multi-document
summarization. In Proceedings of ACL, pages
815–824.
Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011.
Discovery of topically coherent sentences for
extractive summarization. In Proceedings of
ACL-HLT, pages 491–499.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O’Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of COLING/ACL, pages 152–159.
Hal Daum´e, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
ACL, pages 305–312.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22(1):457–479.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in
multi-sentence text extraction. In Proceedings of
COLING.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In Proceedings of ACL, pages
363–370.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2007.
English gigaword third edition. Linguistic Data
Consortium, Philadelphia, PA.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky.
2007. Measuring importance and query relevance
in topic-focused multi-document summarization. In
Proceedings of ACL, pages 193–196.
Aria Haghighi and Lucy Vanderwende. 2009.
Exploring content models for multi-document
summarization. In Proceedings of HLT-NAACL,
pages 362–370.
Sanda Harabagiu and Finley Lacatusu. 2005. Topic
themes for multi-document summarization. In
Proceedings of SIGIR 2005, pages 202–209.
Kai Hong, John M. Conroy, Benoit Favre, Alex
Kulesza, Hui Lin, and Ani Nenkova. 2014. A
repositary of state of the art and competitive baseline
summaries for generic news summarization. In
Proceedings of LREC, May.
Alex Kulesza and Ben Taskar. 2012. Determinantal
point processes for machine learning. Foundations
and Trends in Machine Learning, 5(2–3).
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR, pages 68–73.
Page Lawrence, Brin Sergey, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation
ranking: Bringing order to the web. Technical
report, Stanford University.
Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In UAI, pages 479–490.
Chin-Yew Lin and Eduard Hovy. 2000. The
automated acquisition of topic signatures for text
summarization. In Proceedgins of COLING, pages
495–501.
Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Text
Summarization Branches Out: Proceedings of the
ACL-04 Workshop, pages 74–81.
Marina Litvak, Mark Last, and Menahem Friedman.
2010. A new approach to improving multilingual
summarization using a genetic algorithm. In
Proceedings of ACL, pages 927–936.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised
framework for keyword extraction from meeting
transcripts. Transactions on Audio Speech and
Language Processing, 19(3):538–548.
H. P. Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of Research and
Development, 2(2):159–165, April.
M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating Typed Dependency Parses from Phrase
Structure Parses. In Proceedings of LREC-06, pages
449–454.
Rebecca Mason and Eugene Charniak. 2011.
Extractive multi-document summaries should
explicitly not contain document-specific content.
In Proceedings of the Workshop on Automatic
Summarization for Different Genres, Media, and
Languages, pages 49–54.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of ECIR, pages 557–564.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into text. In Proceedings of EMNLP,
pages 404–411.
Ani Nenkova and Lucy Vanderwende. 2005. The
impact of frequency on summarization. Technical
report, Microsoft Research.
</reference>
<page confidence="0.957484">
720
</page>
<reference confidence="0.999617614457831">
Ani Nenkova, Lucy Vanderwende, and Kathleen
McKeown. 2006. A compositional context sensitive
multi-document summarizer: exploring the factors
that influence summarization. In Proceedings of
SIGIR, pages 573–580.
Jahna Otterbacher, G¨unes Erkan, and Dragomir R.
Radev. 2009. Biased lexrank: Passage
retrieval using random walks with question-based
priors. Information Processing and Management,
45(1):42–54.
Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Inf. Process. Manage., 43(6):1506–1520.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in
summarization. In NAACL-HLT 2012: Workshop
on Evaluation Metrics and System Comparison for
Automatic Summarization, pages 1–9.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O’Leary. 2011. Ranking human and machine
summarization systems. In Proceedings of EMNLP,
pages 467–473.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-T¨ur. 2010. Long story short -
global unsupervised models for keyphrase based
meeting summarization. Speech Communication,
52(10):801–815.
G. Salton. 1971. The SMART Retrieval System:
Experiments in Automatic Document Processing.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
PA.
Chao Shen and Tao Li. 2010. Multi-document
summarization via the minimum dominating set. In
Proceedings of Coling, pages 984–992.
Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin learning of
submodular summarization models. In Proceedings
of EACL, pages 224–233.
Andreas Stolcke. 2002. SRILM – an extensible
language modeling toolkit. In Proceedings of
ICSLP, volume 2, pages 901–904.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of EACL,
pages 781–789.
Yla R Tausczik and James W Pennebaker. 2007.
The Psychological Meaning of Words: LIWC and
Computerized Text Analysis Methods. Journal of
Language and Social Psychology, 29:24–54.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the NAACL-HLT, pages
173–180.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of SIGIR, pages 299–306.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007. Towards an iterative reinforcement approach
for simultaneous document summarization and
keyword extraction. In Proceedings of ACL, pages
552–559.
Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He. 2008.
Query-sensitive mutual reinforcement chain and
its application in query-oriented multi-document
summarization. In Proceedings of SIGIR, pages
283–290.
Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities), page 1(2).
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document
summarization by maximizing informative
content-words. In Proceedings of IJCAI, pages
1776–1782.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings
of SIGIR, pages 113–120.
</reference>
<page confidence="0.997776">
721
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.650427">
<title confidence="0.999457">Improving the Estimation of Word Importance for News Multi-Document Summarization</title>
<author confidence="0.998665">Kai</author>
<affiliation confidence="0.834738">University of Philadelphia, PA,</affiliation>
<email confidence="0.995587">hongkai1@seas.upenn.edu</email>
<abstract confidence="0.997968818181818">We introduce a supervised model for predicting word importance that incorporates a rich set of features. Our model is superior to prior approaches for identifying words used in human summaries. Moreover we that an extractive summarizer using these estimates of word importance is comparable in automatic evaluation with the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>481--490</pages>
<contexts>
<context position="3604" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="526" endWordPosition="529">y et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of ACL-HLT, pages 481–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>A hybrid hierarchical model for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>815--824</pages>
<contexts>
<context position="4474" citStr="Celikyilmaz and Hakkani-Tur, 2010" startWordPosition="649" endWordPosition="652">so been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 Data and Planned Experiments We carry out our experiments on two datasets from the Document Understanding Conference (DUC) (Over et al., 2007). DUC 2003 is used for training and development, DUC 2004 is used for testing</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, 2010</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hybrid hierarchical model for multi-document summarization. In Proceedings of ACL, pages 815–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Discovery of topically coherent sentences for extractive summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>491--499</pages>
<marker>Celikyilmaz, Hakkani-T¨ur, 2011</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-T¨ur. 2011. Discovery of topically coherent sentences for extractive summarization. In Proceedings of ACL-HLT, pages 491–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Dianne P O’Leary</author>
</authors>
<title>Topic-focused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>152--159</pages>
<marker>Conroy, Schlesinger, O’Leary, 2006</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, and Dianne P. O’Leary. 2006. Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of COLING/ACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>305--312</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e, III and Daniel Marcu. 2006. Bayesian query-focused summarization. In Proceedings of ACL, pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8575" citStr="Erkan and Radev, 2004" startWordPosition="1339" endWordPosition="1342">arameters: -c 95 -r 1000 -n 4 -m -a -l 100 -x 713 4.2 Log-likelihood Ratio (LLR) The log-likelihood ratio test (Lin and Hovy, 2000) compares the distribution of a word in the input with that in a large background corpus to identify topic words. We use the Gigaword corpus (Graff et al., 2007) for background counts. The test statistic has a x2 distribution, so a desired confidence level can be chosen to find a small set of topic words. 4.3 Markov Random Walk Model (MRW) Graph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with sentences as nodes and edges weighted by sentence similarity, we treat the words as vertices, similar to Mihalcea and Tarau (2004). The difference in our approach is that the edges between the words are defined by syntactic dependencies rather than depending on the co-occurrence of words within a window of k. We use the Stanford dependency parser (Marneffe et al., 2006). In our approach, we consider a word w more likely to be included in a human summary when it is syntactically related to oth</context>
<context position="10774" citStr="Erkan and Radev (2004)" startWordPosition="1702" endWordPosition="1705">reedy Optimization Approach Our algorithm extracts sentences by weighting them based on word importance. The approach is similar to the standard word probability baseline (Nenkova et al., 2006) but we explore a range of possibilities for assigning weights to individual words. For each sentence, we calculate the sentence weight by summing up the weights of all words, normalized by the number of words in the sentence. We sort the sentences in descending order of their scores into a queue. To create a summary, we iteratively dequeue one sentence, check if the sentence is more than 8 words (as in Erkan and Radev (2004)), then append it to the current summary if it is non-redundant. A sentence is considered non-redundant if it is not similar to any sentences already in the summary, measured by cosine similarity on binary vector representations with stopwords excluded. We use the cut-off of 0.5 for cosine similarity. This value was tuned on the DUC 2003 dataset, by testing the impact of the cut-off value on the ROUGE scores for the final summary. Possible values ranged from 0.1 to 0.9 with step of 0.1. 5.2 KL Divergence Summarizer The KLSUM summarizer (Haghighi and Vanderwende, 2009) aims at minimizing the KL</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R. Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multi-sentence text extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="3338" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="489" endWordPosition="492">his task is to estimate the word probability from the input (Nenkova and Vanderwende, 2005). Another powerful method is log-likelihood ratio test (Lin and Hovy, 2000), which identifies the set of words that appear in the input more often than in a background corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as t</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multi-sentence text extraction. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="22032" citStr="Finkel et al., 2005" startWordPosition="3540" endWordPosition="3543">e input. In addition we have a binary feature indicating if the word appears in the 510, 15, 20, 30, 40, · · · , 190, 200, 220, 240, 260, 280, 300, 350, 400, 450, 500, 600, 700 (in total 33 values) 716 first sentence and the number of times it appears in a first sentence among documents in one input. There are 6 features in this group. All of them are very significant, ranked within the top 100. Word type: These features include Part of Speech (POS) tags, Name Entity (NE) labels and capitalization information. We use the Stanford POS-Tagger (Toutanova et al., 2003) and Name Entity Recognizer (Finkel et al., 2005). We have one feature corresponding to each possible POS and NE tag. The value of this feature is the proportion of occurrences of the word with this tag; in most cases only one feature gets a non-zero value. We have two features which indicate if one word has been capitalized and the ratio of its capitalized occurrences. Most of the NE features (6 out of 8) are significant: there are more Organizations and Locations but fewer Time and Date words in the human summaries. Of the POS tags, 11 out of 41 are significant: there are more nouns (NN, NNS, NNPS); fewer verbs (VBG, VBP, VB) and fewer car</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>J Kong</author>
<author>K Chen</author>
<author>K Maeda</author>
</authors>
<title>English gigaword third edition. Linguistic Data Consortium,</title>
<date>2007</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8245" citStr="Graff et al., 2007" startWordPosition="1284" endWordPosition="1287">icator of its importance. The weight for a word is computed as p(w) = c�w) N , where c(w) is the number of times word w appears in the input and N is the total number of word tokens in the input. 3We use the stopword list from the SMART system (Salton, 1971), augmented with punctuation and symbols. 4ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n 4 -m -a -l 100 -x 713 4.2 Log-likelihood Ratio (LLR) The log-likelihood ratio test (Lin and Hovy, 2000) compares the distribution of a word in the input with that in a large background corpus to identify topic words. We use the Gigaword corpus (Graff et al., 2007) for background counts. The test statistic has a x2 distribution, so a desired confidence level can be chosen to find a small set of topic words. 4.3 Markov Random Walk Model (MRW) Graph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with sentences as nodes and edges weighted by sentence similarity, we treat the words as vertices, similar to Mihalcea and Tarau (2004). The difference in our approach is t</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>D. Graff, J. Kong, K. Chen, and K. Maeda. 2007. English gigaword third edition. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surabhi Gupta</author>
<author>Ani Nenkova</author>
<author>Dan Jurafsky</author>
</authors>
<title>Measuring importance and query relevance in topic-focused multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>193--196</pages>
<contexts>
<context position="20092" citStr="Gupta et al., 2007" startWordPosition="3206" endWordPosition="3209">on the DUC 2003 dataset. There are in total 9, 261 features used, among them 1, 625 are significant (p-value &lt; 0.05). We rank these features in increasing p-values derived from Wilcoxon test. Apart from the widely used features of word frequency and positions, some other less explored features are highly significant. 7.1 Frequency Features We use the Probability, LLR chi-square statistic value and MRW scores as features. Since prior work has demonstrated that for LLR weights in particular, it is useful to identify a small set of important words and ignore all other words in summary selection (Gupta et al., 2007), we use a number of keyword indicators as features. For these indicators, the value of feature is 1 if the word is ranked within top ki, 0 otherwise. Here ki are preset cutoffs5. These cutoffs capture different possibilities for defining the keywords in the input. We also add the number of input documents that contain the word as a feature. There are a total of 100 features in this group, all of which are highly significant, ranked among the top 200. 7.2 Standard features We now describe some standard features which have been applied in prior work on summarization. Word Locations: Especially </context>
</contexts>
<marker>Gupta, Nenkova, Jurafsky, 2007</marker>
<rawString>Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 2007. Measuring importance and query relevance in topic-focused multi-document summarization. In Proceedings of ACL, pages 193–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>362--370</pages>
<contexts>
<context position="11348" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1797" endWordPosition="1800">ntence is more than 8 words (as in Erkan and Radev (2004)), then append it to the current summary if it is non-redundant. A sentence is considered non-redundant if it is not similar to any sentences already in the summary, measured by cosine similarity on binary vector representations with stopwords excluded. We use the cut-off of 0.5 for cosine similarity. This value was tuned on the DUC 2003 dataset, by testing the impact of the cut-off value on the ROUGE scores for the final summary. Possible values ranged from 0.1 to 0.9 with step of 0.1. 5.2 KL Divergence Summarizer The KLSUM summarizer (Haghighi and Vanderwende, 2009) aims at minimizing the KL divergence between the probability distribution over words estimated from the summary and the input respectively. This summarizer is a component of the popular topic model approaches (Daum´e and Marcu, 2006; Celikyilmaz and Hakkani-T¨ur, 2011; Mason and Charniak, 2011) and achieves competitive performance with minimal differences compared to a full-blown topic model system. 6 Global Indicators from NYT Some words evoke topics that are of intrinsic interest to people. Here we search for global indicators of word importance regardless of particular input. 6.1 Global In</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of HLT-NAACL, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
</authors>
<title>Topic themes for multi-document summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>202--209</pages>
<contexts>
<context position="3019" citStr="Harabagiu and Lacatusu, 2005" startWordPosition="441" endWordPosition="444">t appeared most and least often were excluded. Then the sentences in which keywords appeared near each other, presumably better conveying the relationship between the keywords, were selected to form a summary. Many successful recent systems also estimate word importance. The simplest but competitive way to do this task is to estimate the word probability from the input (Nenkova and Vanderwende, 2005). Another powerful method is log-likelihood ratio test (Lin and Hovy, 2000), which identifies the set of words that appear in the input more often than in a background corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedin</context>
</contexts>
<marker>Harabagiu, Lacatusu, 2005</marker>
<rawString>Sanda Harabagiu and Finley Lacatusu. 2005. Topic themes for multi-document summarization. In Proceedings of SIGIR 2005, pages 202–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Hong</author>
<author>John M Conroy</author>
<author>Benoit Favre</author>
<author>Alex Kulesza</author>
<author>Hui Lin</author>
<author>Ani Nenkova</author>
</authors>
<title>A repositary of state of the art and competitive baseline summaries for generic news summarization.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC,</booktitle>
<contexts>
<context position="30669" citStr="Hong et al., 2014" startWordPosition="4954" endWordPosition="4957">00 44.3 38.7 39.2 41.0 46.5 G1 120 44.6 38.5 39.2 40.9 46.4 G2 30 47.8 44.0 42.4 47.4 50.2 G2 35 47.1 43.3 42.1 47.0 49.5 G2 40 46.5 42.4 41.8 46.4 49.2 G3 10 51.2 46.2 43.8 46.9 50.2 G3 15 51.4 47.5 43.7 49.8 52.9 G3 20 49.7 47.6 42.5 49.3 51.5 G4 5 50.0 48.8 44.9 43.6 45.1 G4 6 51.4 46.9 43.7 45.2 47.6 G4 7 50.9 48.2 43.7 45.8 47.8 Table 5: Keyword identification F-score (%) for different Gi and different number of words selected. Bilmes, 2012) and the determinantal point process (DPP) summarizer (Kulesza and Taskar, 2012). The summaries were kindly provided by the authors of these systems (Hong et al., 2014). As can been seen in Table 6, our system outperforms PROB, LLR, MRW, PEER-65, KLSUM and REGBASIC. These improvements are significant on ROUGE-2 recall. Interestingly, although the supervised system REGBASIC which uses only frequency and positions achieve low performance in keyword identification, the summaries it generates are of high quality. The inclusion of position features negatively affects the performance in summary keyword identification but boosts the weights for the words which appear close to the beginning of the documents, which is helpful for identifying informative sentences. By</context>
</contexts>
<marker>Hong, Conroy, Favre, Kulesza, Lin, Nenkova, 2014</marker>
<rawString>Kai Hong, John M. Conroy, Benoit Favre, Alex Kulesza, Hui Lin, and Ani Nenkova. 2014. A repositary of state of the art and competitive baseline summaries for generic news summarization. In Proceedings of LREC, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Ben Taskar</author>
</authors>
<title>Determinantal point processes for machine learning. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<pages>5--2</pages>
<contexts>
<context position="30581" citStr="Kulesza and Taskar, 2012" startWordPosition="4939" endWordPosition="4942">tml for details. 718 Gi #words PROB LLR MRW REGBASIC REGSUM G1 80 43.6 37.9 38.9 39.9 45.7 G1 100 44.3 38.7 39.2 41.0 46.5 G1 120 44.6 38.5 39.2 40.9 46.4 G2 30 47.8 44.0 42.4 47.4 50.2 G2 35 47.1 43.3 42.1 47.0 49.5 G2 40 46.5 42.4 41.8 46.4 49.2 G3 10 51.2 46.2 43.8 46.9 50.2 G3 15 51.4 47.5 43.7 49.8 52.9 G3 20 49.7 47.6 42.5 49.3 51.5 G4 5 50.0 48.8 44.9 43.6 45.1 G4 6 51.4 46.9 43.7 45.2 47.6 G4 7 50.9 48.2 43.7 45.8 47.8 Table 5: Keyword identification F-score (%) for different Gi and different number of words selected. Bilmes, 2012) and the determinantal point process (DPP) summarizer (Kulesza and Taskar, 2012). The summaries were kindly provided by the authors of these systems (Hong et al., 2014). As can been seen in Table 6, our system outperforms PROB, LLR, MRW, PEER-65, KLSUM and REGBASIC. These improvements are significant on ROUGE-2 recall. Interestingly, although the supervised system REGBASIC which uses only frequency and positions achieve low performance in keyword identification, the summaries it generates are of high quality. The inclusion of position features negatively affects the performance in summary keyword identification but boosts the weights for the words which appear close to th</context>
</contexts>
<marker>Kulesza, Taskar, 2012</marker>
<rawString>Alex Kulesza and Ben Taskar. 2012. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 5(2–3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="4439" citStr="Kupiec et al., 1995" startWordPosition="645" endWordPosition="648"> Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 Data and Planned Experiments We carry out our experiments on two datasets from the Document Understanding Conference (DUC) (Over et al., 2007). DUC 2003 is used for training and develo</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of SIGIR, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Page Lawrence</author>
<author>Brin Sergey</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="9535" citStr="Lawrence et al., 1998" startWordPosition="1500" endWordPosition="1503">s rather than depending on the co-occurrence of words within a window of k. We use the Stanford dependency parser (Marneffe et al., 2006). In our approach, we consider a word w more likely to be included in a human summary when it is syntactically related to other (important) words, even if w itself is not mentioned often. The edge weight between two vertices is equal to the number of syntactic dependencies of any type between two words within the same sentence in the input. The weights are then normalized by summing up the weights of edges linked to one node. We apply the Pagerank algorithm (Lawrence et al., 1998) on the resulting graph. We set the probability of performing random jump between nodes A=0.15. The algorithm terminates when the change of node weight between iterations is smaller than 10−4 for all nodes. Word importance is equal to the final weight of its corresponding node in the graph. 5 Summary Generation Process In this section, we outline how summaries are generated by a greedy optimization system which selects the sentence with highest weight iteratively. This is the main process we use in all our summarization systems. For comparison we also use a summarization algorithm based on KL </context>
</contexts>
<marker>Lawrence, Sergey, Motwani, Winograd, 1998</marker>
<rawString>Page Lawrence, Brin Sergey, Rajeev Motwani, and Terry Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Learning mixtures of submodular shells with application to document summarization.</title>
<date>2012</date>
<booktitle>In UAI,</booktitle>
<pages>479--490</pages>
<marker>Lin, Bilmes, 2012</marker>
<rawString>Hui Lin and Jeff Bilmes. 2012. Learning mixtures of submodular shells with application to document summarization. In UAI, pages 479–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedgins of COLING,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="2868" citStr="Lin and Hovy, 2000" startWordPosition="415" endWordPosition="418">n automatic summarization (Luhn, 1958). There keywords were identified based on the number of times they appeared in the input, and words that appeared most and least often were excluded. Then the sentences in which keywords appeared near each other, presumably better conveying the relationship between the keywords, were selected to form a summary. Many successful recent systems also estimate word importance. The simplest but competitive way to do this task is to estimate the word probability from the input (Nenkova and Vanderwende, 2005). Another powerful method is log-likelihood ratio test (Lin and Hovy, 2000), which identifies the set of words that appear in the input more often than in a background corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches,</context>
<context position="8084" citStr="Lin and Hovy, 2000" startWordPosition="1255" endWordPosition="1258">ted are thus the content words with highest scores. 4.1 Word Probability (Prob) The frequency with which a word occurs in the input is often considered as an indicator of its importance. The weight for a word is computed as p(w) = c�w) N , where c(w) is the number of times word w appears in the input and N is the total number of word tokens in the input. 3We use the stopword list from the SMART system (Salton, 1971), augmented with punctuation and symbols. 4ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n 4 -m -a -l 100 -x 713 4.2 Log-likelihood Ratio (LLR) The log-likelihood ratio test (Lin and Hovy, 2000) compares the distribution of a word in the input with that in a large background corpus to identify topic words. We use the Gigaword corpus (Graff et al., 2007) for background counts. The test statistic has a x2 distribution, so a desired confidence level can be chosen to find a small set of topic words. 4.3 Markov Random Walk Model (MRW) Graph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with senten</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedgins of COLING, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="6301" citStr="Lin, 2004" startWordPosition="953" endWordPosition="954">ts, yielding four gold-standard sets of keywords, denoted by Gi. |Gi |is thus the cardinality of the set for the input. We only consider the words in the summary that also appear in the original input2, with stopwords 12*precision*recall/(precision+recall) 2On average 26.3% (15.0% with stemming) of the words in the four abstracts never appear in the input. excluded3. Table 1 shows the average number of unique content words for the respective keyword gold-standard. i 1 2 3 4 Mean |Gi |102 32 15 6 Table 1: Average number of words in Gi For the summarization task, we compare results using ROUGE (Lin, 2004). We report ROUGE-1, -2, -4 recall, with stemming and without removing stopwords. We consider ROUGE-2 recall as the main metric for this comparison due to its effectiveness in comparing machine summaries (Owczarzak et al., 2012). All of the summaries were truncated to the first 100 words by ROUGE4. We use Wilcoxon signed-rank test to examine the statistical significance as advocated by Rankel et al. (2011) for both tasks, and consider differences to be significant if the p-value is less than 0.05. 4 Unsupervised Word Weighting In this section we describe three unsupervised approaches of assign</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Litvak</author>
<author>Mark Last</author>
<author>Menahem Friedman</author>
</authors>
<title>A new approach to improving multilingual summarization using a genetic algorithm.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>927--936</pages>
<contexts>
<context position="4496" citStr="Litvak et al., 2010" startWordPosition="653" endWordPosition="656">roaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 Data and Planned Experiments We carry out our experiments on two datasets from the Document Understanding Conference (DUC) (Over et al., 2007). DUC 2003 is used for training and development, DUC 2004 is used for testing. These are the last t</context>
</contexts>
<marker>Litvak, Last, Friedman, 2010</marker>
<rawString>Marina Litvak, Mark Last, and Menahem Friedman. 2010. A new approach to improving multilingual summarization using a genetic algorithm. In Proceedings of ACL, pages 927–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Feifan Liu</author>
<author>Yang Liu</author>
</authors>
<title>A supervised framework for keyword extraction from meeting transcripts.</title>
<date>2011</date>
<booktitle>Transactions on Audio Speech and Language Processing,</booktitle>
<contexts>
<context position="4296" citStr="Liu et al., 2011" startWordPosition="624" endWordPosition="627">ociation for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 Data and Planned Experiments We carry out</context>
<context position="22898" citStr="Liu et al., 2011" startWordPosition="3693" endWordPosition="3696">e word has been capitalized and the ratio of its capitalized occurrences. Most of the NE features (6 out of 8) are significant: there are more Organizations and Locations but fewer Time and Date words in the human summaries. Of the POS tags, 11 out of 41 are significant: there are more nouns (NN, NNS, NNPS); fewer verbs (VBG, VBP, VB) and fewer cardinal numbers in the abstracts compared to the input. Capitalized words also tend to be included in human summaries. KL: Prior work has shown that having estimates of sentence importance can also help in estimating word importance (Wan et al., 2007; Liu et al., 2011; Wei et al., 2008). The summarizer based on KL-divergence assigns importance to sentences directly, in a complex function according to the word distribution in the sentence. Therefore, we use these summaries as potential indicators of word importance. We include two features here, the first one indicates if the word appears in a KLSUM summary of the input, as well as a feature corresponding to the number of times the word appeared in that summary. Both of the features are highly significant, ranked within the top 200. 7.3 NYT-weights as Features We include features from the relative rank of a</context>
</contexts>
<marker>Liu, Liu, Liu, 2011</marker>
<rawString>Fei Liu, Feifan Liu, and Yang Liu. 2011. A supervised framework for keyword extraction from meeting transcripts. Transactions on Audio Speech and Language Processing, 19(3):538–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="2287" citStr="Luhn, 1958" startWordPosition="327" endWordPosition="328"> input, equivalent in quality to the standard baseline of extracting the first 100 words from the latest Ani Nenkova University of Pennsylvania Philadelphia, PA, 19104 nenkova@seas.upenn.edu article in the input. Finally, we integrate the schemes for assignment of word importance into a summarizer which greedily optimizes for the presence of important words. We show that our better estimation of word importance leads to better extractive summaries. 2 Prior work The idea of identifying words that are descriptive of the input can be dated back to Luhn’s earliest work in automatic summarization (Luhn, 1958). There keywords were identified based on the number of times they appeared in the input, and words that appeared most and least often were excluded. Then the sentences in which keywords appeared near each other, presumably better conveying the relationship between the keywords, were selected to form a summary. Many successful recent systems also estimate word importance. The simplest but competitive way to do this task is to estimate the word probability from the input (Nenkova and Vanderwende, 2005). Another powerful method is log-likelihood ratio test (Lin and Hovy, 2000), which identifies </context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2):159–165, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marneffe</author>
<author>B Maccartney</author>
<author>C Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06,</booktitle>
<pages>449--454</pages>
<contexts>
<context position="9050" citStr="Marneffe et al., 2006" startWordPosition="1414" endWordPosition="1417">aph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with sentences as nodes and edges weighted by sentence similarity, we treat the words as vertices, similar to Mihalcea and Tarau (2004). The difference in our approach is that the edges between the words are defined by syntactic dependencies rather than depending on the co-occurrence of words within a window of k. We use the Stanford dependency parser (Marneffe et al., 2006). In our approach, we consider a word w more likely to be included in a human summary when it is syntactically related to other (important) words, even if w itself is not mentioned often. The edge weight between two vertices is equal to the number of syntactic dependencies of any type between two words within the same sentence in the input. The weights are then normalized by summing up the weights of edges linked to one node. We apply the Pagerank algorithm (Lawrence et al., 1998) on the resulting graph. We set the probability of performing random jump between nodes A=0.15. The algorithm termi</context>
</contexts>
<marker>Marneffe, Maccartney, Manning, 2006</marker>
<rawString>M. Marneffe, B. Maccartney, and C. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of LREC-06, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
<author>Eugene Charniak</author>
</authors>
<title>Extractive multi-document summaries should explicitly not contain document-specific content.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="11644" citStr="Mason and Charniak, 2011" startWordPosition="1840" endWordPosition="1843">xcluded. We use the cut-off of 0.5 for cosine similarity. This value was tuned on the DUC 2003 dataset, by testing the impact of the cut-off value on the ROUGE scores for the final summary. Possible values ranged from 0.1 to 0.9 with step of 0.1. 5.2 KL Divergence Summarizer The KLSUM summarizer (Haghighi and Vanderwende, 2009) aims at minimizing the KL divergence between the probability distribution over words estimated from the summary and the input respectively. This summarizer is a component of the popular topic model approaches (Daum´e and Marcu, 2006; Celikyilmaz and Hakkani-T¨ur, 2011; Mason and Charniak, 2011) and achieves competitive performance with minimal differences compared to a full-blown topic model system. 6 Global Indicators from NYT Some words evoke topics that are of intrinsic interest to people. Here we search for global indicators of word importance regardless of particular input. 6.1 Global Indicators of Word Importance We analyze a large corpus of original documents and corresponding summaries in order to identify words that consistently get included in or excluded from the summary. In the 2004-2007 NYT corpus, many news articles have abstracts along with the original article, which</context>
</contexts>
<marker>Mason, Charniak, 2011</marker>
<rawString>Rebecca Mason and Eugene Charniak. 2011. Extractive multi-document summaries should explicitly not contain document-specific content. In Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 49–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of ECIR,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="3553" citStr="McDonald, 2007" startWordPosition="520" endWordPosition="521"> than in a background corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, </context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of ECIR, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into text.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="7136" citStr="Mihalcea and Tarau (2004)" startWordPosition="1083" endWordPosition="1086">rzak et al., 2012). All of the summaries were truncated to the first 100 words by ROUGE4. We use Wilcoxon signed-rank test to examine the statistical significance as advocated by Rankel et al. (2011) for both tasks, and consider differences to be significant if the p-value is less than 0.05. 4 Unsupervised Word Weighting In this section we describe three unsupervised approaches of assigning importance weights to words. The first two are probability and log-likelihood ratio, which have been extensively used in prior work. We also apply a markov random walk model for keyword ranking, similar to Mihalcea and Tarau (2004). In the next section we describe a summarizer that uses these weights to form a summary and then describe our regression approach to combine these and other predictors in order to achieve more accurate predictions for the word importance in Section 7. The task is to assign a score to each word in the input. The keywords extracted are thus the content words with highest scores. 4.1 Word Probability (Prob) The frequency with which a word occurs in the input is often considered as an indicator of its importance. The weight for a word is computed as p(w) = c�w) N , where c(w) is the number of tim</context>
<context position="8551" citStr="Mihalcea and Tarau, 2004" startWordPosition="1335" endWordPosition="1338">ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n 4 -m -a -l 100 -x 713 4.2 Log-likelihood Ratio (LLR) The log-likelihood ratio test (Lin and Hovy, 2000) compares the distribution of a word in the input with that in a large background corpus to identify topic words. We use the Gigaword corpus (Graff et al., 2007) for background counts. The test statistic has a x2 distribution, so a desired confidence level can be chosen to find a small set of topic words. 4.3 Markov Random Walk Model (MRW) Graph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with sentences as nodes and edges weighted by sentence similarity, we treat the words as vertices, similar to Mihalcea and Tarau (2004). The difference in our approach is that the edges between the words are defined by syntactic dependencies rather than depending on the co-occurrence of words within a window of k. We use the Stanford dependency parser (Marneffe et al., 2006). In our approach, we consider a word w more likely to be included in a human summary when it is synt</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of EMNLP, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The impact of frequency on summarization.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="2793" citStr="Nenkova and Vanderwende, 2005" startWordPosition="404" endWordPosition="407">ng words that are descriptive of the input can be dated back to Luhn’s earliest work in automatic summarization (Luhn, 1958). There keywords were identified based on the number of times they appeared in the input, and words that appeared most and least often were excluded. Then the sentences in which keywords appeared near each other, presumably better conveying the relationship between the keywords, were selected to form a summary. Many successful recent systems also estimate word importance. The simplest but competitive way to do this task is to estimate the word probability from the input (Nenkova and Vanderwende, 2005). Another powerful method is log-likelihood ratio test (Lin and Hovy, 2000), which identifies the set of words that appear in the input more often than in a background corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summ</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
<author>Kathleen McKeown</author>
</authors>
<title>A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>573--580</pages>
<contexts>
<context position="10345" citStr="Nenkova et al., 2006" startWordPosition="1627" endWordPosition="1630">−4 for all nodes. Word importance is equal to the final weight of its corresponding node in the graph. 5 Summary Generation Process In this section, we outline how summaries are generated by a greedy optimization system which selects the sentence with highest weight iteratively. This is the main process we use in all our summarization systems. For comparison we also use a summarization algorithm based on KL divergence. 5.1 Greedy Optimization Approach Our algorithm extracts sentences by weighting them based on word importance. The approach is similar to the standard word probability baseline (Nenkova et al., 2006) but we explore a range of possibilities for assigning weights to individual words. For each sentence, we calculate the sentence weight by summing up the weights of all words, normalized by the number of words in the sentence. We sort the sentences in descending order of their scores into a queue. To create a summary, we iteratively dequeue one sentence, check if the sentence is more than 8 words (as in Erkan and Radev (2004)), then append it to the current summary if it is non-redundant. A sentence is considered non-redundant if it is not similar to any sentences already in the summary, measu</context>
</contexts>
<marker>Nenkova, Vanderwende, McKeown, 2006</marker>
<rawString>Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. In Proceedings of SIGIR, pages 573–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Biased lexrank: Passage retrieval using random walks with question-based priors.</title>
<date>2009</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="8634" citStr="Otterbacher et al., 2009" startWordPosition="1346" endWordPosition="1349">g-likelihood Ratio (LLR) The log-likelihood ratio test (Lin and Hovy, 2000) compares the distribution of a word in the input with that in a large background corpus to identify topic words. We use the Gigaword corpus (Graff et al., 2007) for background counts. The test statistic has a x2 distribution, so a desired confidence level can be chosen to find a small set of topic words. 4.3 Markov Random Walk Model (MRW) Graph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with sentences as nodes and edges weighted by sentence similarity, we treat the words as vertices, similar to Mihalcea and Tarau (2004). The difference in our approach is that the edges between the words are defined by syntactic dependencies rather than depending on the co-occurrence of words within a window of k. We use the Stanford dependency parser (Marneffe et al., 2006). In our approach, we consider a word w more likely to be included in a human summary when it is syntactically related to other (important) words, even if w itself is not mentioned oft</context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2009</marker>
<rawString>Jahna Otterbacher, G¨unes Erkan, and Dragomir R. Radev. 2009. Biased lexrank: Passage retrieval using random walks with question-based priors. Information Processing and Management, 45(1):42–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
<author>Hoa Dang</author>
<author>Donna Harman</author>
</authors>
<date>2007</date>
<booktitle>Duc in context. Inf. Process. Manage.,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="4997" citStr="Over et al., 2007" startWordPosition="732" endWordPosition="735">sis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 Data and Planned Experiments We carry out our experiments on two datasets from the Document Understanding Conference (DUC) (Over et al., 2007). DUC 2003 is used for training and development, DUC 2004 is used for testing. These are the last two years in which generic summarization was evaluated at DUC workshops. There are 30 multi-document clusters in DUC 2003 and 50 in DUC 2004, each with about 10 news articles on a related topic. The task is to produce a 100-word generic summary. Four human abstractive summaries are available for each cluster. We compare different keyword extraction methods by the F-measurer they achieve against the gold-standard summary keywords. We do not use stemming when calculating these scores. In our work, k</context>
</contexts>
<marker>Over, Dang, Harman, 2007</marker>
<rawString>Paul Over, Hoa Dang, and Donna Harman. 2007. Duc in context. Inf. Process. Manage., 43(6):1506–1520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
<author>Ani Nenkova</author>
</authors>
<title>An assessment of the accuracy of automatic evaluation in summarization.</title>
<date>2012</date>
<booktitle>In NAACL-HLT 2012: Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="6529" citStr="Owczarzak et al., 2012" startWordPosition="986" endWordPosition="989">ds 12*precision*recall/(precision+recall) 2On average 26.3% (15.0% with stemming) of the words in the four abstracts never appear in the input. excluded3. Table 1 shows the average number of unique content words for the respective keyword gold-standard. i 1 2 3 4 Mean |Gi |102 32 15 6 Table 1: Average number of words in Gi For the summarization task, we compare results using ROUGE (Lin, 2004). We report ROUGE-1, -2, -4 recall, with stemming and without removing stopwords. We consider ROUGE-2 recall as the main metric for this comparison due to its effectiveness in comparing machine summaries (Owczarzak et al., 2012). All of the summaries were truncated to the first 100 words by ROUGE4. We use Wilcoxon signed-rank test to examine the statistical significance as advocated by Rankel et al. (2011) for both tasks, and consider differences to be significant if the p-value is less than 0.05. 4 Unsupervised Word Weighting In this section we describe three unsupervised approaches of assigning importance weights to words. The first two are probability and log-likelihood ratio, which have been extensively used in prior work. We also apply a markov random walk model for keyword ranking, similar to Mihalcea and Tarau</context>
</contexts>
<marker>Owczarzak, Conroy, Dang, Nenkova, 2012</marker>
<rawString>Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An assessment of the accuracy of automatic evaluation in summarization. In NAACL-HLT 2012: Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Rankel</author>
<author>John Conroy</author>
<author>Eric Slud</author>
<author>Dianne O’Leary</author>
</authors>
<title>Ranking human and machine summarization systems.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>467--473</pages>
<marker>Rankel, Conroy, Slud, O’Leary, 2011</marker>
<rawString>Peter Rankel, John Conroy, Eric Slud, and Dianne O’Leary. 2011. Ranking human and machine summarization systems. In Proceedings of EMNLP, pages 467–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korbinian Riedhammer</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Long story short -global unsupervised models for keyphrase based meeting summarization.</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<issue>10</issue>
<marker>Riedhammer, Favre, Hakkani-T¨ur, 2010</marker>
<rawString>Korbinian Riedhammer, Benoit Favre, and Dilek Hakkani-T¨ur. 2010. Long story short -global unsupervised models for keyphrase based meeting summarization. Speech Communication, 52(10):801–815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>The SMART Retrieval System: Experiments</title>
<date>1971</date>
<booktitle>in Automatic Document Processing.</booktitle>
<publisher>Prentice-Hall, Inc.,</publisher>
<location>Upper Saddle River, NJ, USA.</location>
<contexts>
<context position="7884" citStr="Salton, 1971" startWordPosition="1223" endWordPosition="1224">ombine these and other predictors in order to achieve more accurate predictions for the word importance in Section 7. The task is to assign a score to each word in the input. The keywords extracted are thus the content words with highest scores. 4.1 Word Probability (Prob) The frequency with which a word occurs in the input is often considered as an indicator of its importance. The weight for a word is computed as p(w) = c�w) N , where c(w) is the number of times word w appears in the input and N is the total number of word tokens in the input. 3We use the stopword list from the SMART system (Salton, 1971), augmented with punctuation and symbols. 4ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n 4 -m -a -l 100 -x 713 4.2 Log-likelihood Ratio (LLR) The log-likelihood ratio test (Lin and Hovy, 2000) compares the distribution of a word in the input with that in a large background corpus to identify topic words. We use the Gigaword corpus (Graff et al., 2007) for background counts. The test statistic has a x2 distribution, so a desired confidence level can be chosen to find a small set of topic words. 4.3 Markov Random Walk Model (MRW) Graph methods have been successfully applied to weighting </context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>G. Salton. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The new york times annotated corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1344" citStr="Sandhaus, 2008" startWordPosition="186" endWordPosition="187">er, we describe experiments on identifying words from the input that are also included in human summaries; we call such words summary keywords. We review several unsupervised approaches for summary keyword identification and further combine these, along with features including position, part-of-speech, subjectivity, topic categories, context and intrinsic importance, in a superior supervised model for predicting word importance. One of the novel features we develop aims to determine the intrinsic importance of words. To this end, we analyze abstract-article pairs in the New York Times corpus (Sandhaus, 2008) to identify words that tend to be preserved in the abstracts. We demonstrate that judging word importance just based on this criterion leads to significantly higher performance than selecting sentences at random. Identifying intrinsically important words allows us to generate summaries without doing any feature computation on the input, equivalent in quality to the standard baseline of extracting the first 100 words from the latest Ani Nenkova University of Pennsylvania Philadelphia, PA, 19104 nenkova@seas.upenn.edu article in the input. Finally, we integrate the schemes for assignment of wor</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Shen</author>
<author>Tao Li</author>
</authors>
<title>Multi-document summarization via the minimum dominating set.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>984--992</pages>
<contexts>
<context position="3572" citStr="Shen and Li, 2010" startWordPosition="522" endWordPosition="525">round corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-esti</context>
</contexts>
<marker>Shen, Li, 2010</marker>
<rawString>Chao Shen and Tao Li. 2010. Multi-document summarization via the minimum dominating set. In Proceedings of Coling, pages 984–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruben Sipos</author>
<author>Pannaga Shivaswamy</author>
<author>Thorsten Joachims</author>
</authors>
<title>Large-margin learning of submodular summarization models.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>224--233</pages>
<contexts>
<context position="4020" citStr="Sipos et al., 2012" startWordPosition="585" endWordPosition="588">hy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Als</context>
<context position="28113" citStr="Sipos et al., 2012" startWordPosition="4544" endWordPosition="4547">e our system REGSUM. 8.1 Regression for Keyword Identification For each input, we define the set of keywords as the top k words according to the scores generated from different models. We compare our regression system with three unsupervised systems: PROB, LLR, MRW. To show the effectiveness of new features, we compare our results with a regression system trained only on word frequency and location related features described in Section 7. Those features are the ones standardly used for ranking the importance of words in recent summarization works (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012), and we name this system REGBASIC. Figure 1 shows the performance of systems when selecting the 100 words with highest weights Figure 1: Precision, Recall and F-score of keyword identification, 100 words selected, G1 as gold-standard as keywords. Each word from the input that appeared in any of the four human summaries is considered as a gold-standard keyword. Among the unsupervised approaches, word probability identifies keywords better than LLR and MRW by at least 4% on F-score. REGBASIC does not give better performance at keyword identification compared with PROB, even though it includes l</context>
</contexts>
<marker>Sipos, Shivaswamy, Joachims, 2012</marker>
<rawString>Ruben Sipos, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Large-margin learning of submodular summarization models. In Proceedings of EACL, pages 224–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="13322" citStr="Stolcke, 2002" startWordPosition="2092" endWordPosition="2093">s, bit, looked, wrote Pra(w) photo, photos, article, York, column, letter, Bush, state, reviews, million, American pres, percent, Iraq, year, people, government, John, years, company, correction national, federal, officials, city, drawing, billion, public, world, administration Table 2: Top 30 words by three metrics from NYT corpus resource to do such analysis. We identified 160,001 abstract-original pairs in the corpus. From these, we generate two language models, one estimated from the text of all abstracts (LMA), the other estimated from the corpus of original articles (LMG). We use SRILM (Stolcke, 2002) with Ney smoothing. We denote the probability of word w in LMA as PrA(w), the probability in LMG as PrG(w), and calculate the difference PrA(w)−PrG(w) and the ratio PrA(w)/PrG(w) to capture the change of probability. In addition, we calculate KL-like weighted scores for words which reflect both the change of probabilities between the two samples and the overall frequency of the word. Here we calculate both KL(A ∥ G) and KL(G ∥ A). Words with high values for the former score are favored in the summaries because they have higher probability in the abstracts than in the originals and have relati</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings of ICSLP, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>781--789</pages>
<contexts>
<context position="3999" citStr="Takamura and Okumura, 2009" startWordPosition="581" endWordPosition="584">ics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in</context>
<context position="28092" citStr="Takamura and Okumura, 2009" startWordPosition="4540" endWordPosition="4543">active summarization. We name our system REGSUM. 8.1 Regression for Keyword Identification For each input, we define the set of keywords as the top k words according to the scores generated from different models. We compare our regression system with three unsupervised systems: PROB, LLR, MRW. To show the effectiveness of new features, we compare our results with a regression system trained only on word frequency and location related features described in Section 7. Those features are the ones standardly used for ranking the importance of words in recent summarization works (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012), and we name this system REGBASIC. Figure 1 shows the performance of systems when selecting the 100 words with highest weights Figure 1: Precision, Recall and F-score of keyword identification, 100 words selected, G1 as gold-standard as keywords. Each word from the input that appeared in any of the four human summaries is considered as a gold-standard keyword. Among the unsupervised approaches, word probability identifies keywords better than LLR and MRW by at least 4% on F-score. REGBASIC does not give better performance at keyword identification compared with PROB, even</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proceedings of EACL, pages 781–789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods.</title>
<date>2007</date>
<journal>Journal of Language and Social Psychology,</journal>
<pages>29--24</pages>
<contexts>
<context position="25837" citStr="Tausczik and Pennebaker, 2007" startWordPosition="4173" endWordPosition="4176">ectivity analysis and LIWC for topic analysis. The MPQA dictionary (Wiebe and Cardie, 2005) contains words with different polarities (positive, neutral, negative) and intensities (strong, weak). The combinations correspond to six features. It turns out that words with strong polarity, either positive or negative, are seldomly included in the summaries. Most strikingly, the p-value from significance test for the strong negative words is less than 10−4—these words are rarely included in summaries. There is no significant difference on weak polarity categories. Another dictionary we use is LIWC (Tausczik and Pennebaker, 2007), which contains manually constructed dictionaries for multiple categories of words. The value of the feature is 1 for one word if the word appears in the particular dictionary for the category. 34 out of 64 LIWC features are significant. Interesting categories which appear at higher rate in summaries include events about death, anger, achievements, money 717 and negative emotions. Those that appear at lower rate in the summaries include auxiliary verbs, hear, pronouns, negation, function words, social words, swear, adverbs, words related to families, etc. 7.6 Context Features We use context f</context>
</contexts>
<marker>Tausczik, Pennebaker, 2007</marker>
<rawString>Yla R Tausczik and James W Pennebaker. 2007. The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of Language and Social Psychology, 29:24–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the NAACL-HLT,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="21983" citStr="Toutanova et al., 2003" startWordPosition="3532" endWordPosition="3535">n for tokens of this word across all documents in the input. In addition we have a binary feature indicating if the word appears in the 510, 15, 20, 30, 40, · · · , 190, 200, 220, 240, 260, 280, 300, 350, 400, 450, 500, 600, 700 (in total 33 values) 716 first sentence and the number of times it appears in a first sentence among documents in one input. There are 6 features in this group. All of them are very significant, ranked within the top 100. Word type: These features include Part of Speech (POS) tags, Name Entity (NE) labels and capitalization information. We use the Stanford POS-Tagger (Toutanova et al., 2003) and Name Entity Recognizer (Finkel et al., 2005). We have one feature corresponding to each possible POS and NE tag. The value of this feature is the proportion of occurrences of the word with this tag; in most cases only one feature gets a non-zero value. We have two features which indicate if one word has been capitalized and the ratio of its capitalized occurrences. Most of the NE features (6 out of 8) are significant: there are more Organizations and Locations but fewer Time and Date words in the human summaries. Of the POS tags, 11 out of 41 are significant: there are more nouns (NN, NNS</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the NAACL-HLT, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Multi-document summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="8525" citStr="Wan and Yang, 2008" startWordPosition="1331" endWordPosition="1334">ation and symbols. 4ROUGE version 1.5.5 with parameters: -c 95 -r 1000 -n 4 -m -a -l 100 -x 713 4.2 Log-likelihood Ratio (LLR) The log-likelihood ratio test (Lin and Hovy, 2000) compares the distribution of a word in the input with that in a large background corpus to identify topic words. We use the Gigaword corpus (Graff et al., 2007) for background counts. The test statistic has a x2 distribution, so a desired confidence level can be chosen to find a small set of topic words. 4.3 Markov Random Walk Model (MRW) Graph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with sentences as nodes and edges weighted by sentence similarity, we treat the words as vertices, similar to Mihalcea and Tarau (2004). The difference in our approach is that the edges between the words are defined by syntactic dependencies rather than depending on the co-occurrence of words within a window of k. We use the Stanford dependency parser (Marneffe et al., 2006). In our approach, we consider a word w more likely to be included in a hum</context>
</contexts>
<marker>Wan, Yang, 2008</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2008. Multi-document summarization using cluster-based link analysis. In Proceedings of SIGIR, pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
<author>Jianguo Xiao</author>
</authors>
<title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>552--559</pages>
<contexts>
<context position="4259" citStr="Wan et al., 2007" startWordPosition="616" endWordPosition="619">e of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 Data</context>
<context position="22880" citStr="Wan et al., 2007" startWordPosition="3689" endWordPosition="3692">ich indicate if one word has been capitalized and the ratio of its capitalized occurrences. Most of the NE features (6 out of 8) are significant: there are more Organizations and Locations but fewer Time and Date words in the human summaries. Of the POS tags, 11 out of 41 are significant: there are more nouns (NN, NNS, NNPS); fewer verbs (VBG, VBP, VB) and fewer cardinal numbers in the abstracts compared to the input. Capitalized words also tend to be included in human summaries. KL: Prior work has shown that having estimates of sentence importance can also help in estimating word importance (Wan et al., 2007; Liu et al., 2011; Wei et al., 2008). The summarizer based on KL-divergence assigns importance to sentences directly, in a complex function according to the word distribution in the sentence. Therefore, we use these summaries as potential indicators of word importance. We include two features here, the first one indicates if the word appears in a KLSUM summary of the input, as well as a feature corresponding to the number of times the word appeared in that summary. Both of the features are highly significant, ranked within the top 200. 7.3 NYT-weights as Features We include features from the </context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of ACL, pages 552–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Furu Wei</author>
<author>Wenjie Li</author>
<author>Qin Lu</author>
<author>Yanxiang He</author>
</authors>
<title>Query-sensitive mutual reinforcement chain and its application in query-oriented multi-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>283--290</pages>
<contexts>
<context position="4277" citStr="Wei et al., 2008" startWordPosition="620" endWordPosition="623">Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 Data and Planned Exper</context>
<context position="22917" citStr="Wei et al., 2008" startWordPosition="3697" endWordPosition="3700">pitalized and the ratio of its capitalized occurrences. Most of the NE features (6 out of 8) are significant: there are more Organizations and Locations but fewer Time and Date words in the human summaries. Of the POS tags, 11 out of 41 are significant: there are more nouns (NN, NNS, NNPS); fewer verbs (VBG, VBP, VB) and fewer cardinal numbers in the abstracts compared to the input. Capitalized words also tend to be included in human summaries. KL: Prior work has shown that having estimates of sentence importance can also help in estimating word importance (Wan et al., 2007; Liu et al., 2011; Wei et al., 2008). The summarizer based on KL-divergence assigns importance to sentences directly, in a complex function according to the word distribution in the sentence. Therefore, we use these summaries as potential indicators of word importance. We include two features here, the first one indicates if the word appears in a KLSUM summary of the input, as well as a feature corresponding to the number of times the word appeared in that summary. Both of the features are highly significant, ranked within the top 200. 7.3 NYT-weights as Features We include features from the relative rank of a word according to </context>
</contexts>
<marker>Wei, Li, Lu, He, 2008</marker>
<rawString>Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He. 2008. Query-sensitive mutual reinforcement chain and its application in query-oriented multi-document summarization. In Proceedings of SIGIR, pages 283–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. language resources and evaluation.</title>
<date>2005</date>
<booktitle>In Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<pages>1--2</pages>
<contexts>
<context position="25298" citStr="Wiebe and Cardie, 2005" startWordPosition="4096" endWordPosition="4099">lude president, government, political. We also find the same unigrams among the top words from NYT corpus according to KL(A 11 G) . As for words unlikely to appear in summaries, we see Wednesday, added, thing, etc, which again rank high according to KL(G 11 A). 7.5 Dictionary Features: MPQA and LIWC Unigram features are notoriously sparse. To mitigate the sparsity problem, we resort to more general groupings to words according to salient semantic and functional categories. We employ two hand-crafted dictionaries, MPQA for subjectivity analysis and LIWC for topic analysis. The MPQA dictionary (Wiebe and Cardie, 2005) contains words with different polarities (positive, neutral, negative) and intensities (strong, weak). The combinations correspond to six features. It turns out that words with strong polarity, either positive or negative, are seldomly included in the summaries. Most strikingly, the p-value from significance test for the strong negative words is less than 10−4—these words are rarely included in summaries. There is no significant difference on weak polarity categories. Another dictionary we use is LIWC (Tausczik and Pennebaker, 2007), which contains manually constructed dictionaries for multip</context>
</contexts>
<marker>Wiebe, Cardie, 2005</marker>
<rawString>Janyce Wiebe and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. language resources and evaluation. In Language Resources and Evaluation (formerly Computers and the Humanities), page 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Joshua Goodman</author>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
</authors>
<title>Multi-document summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1776--1782</pages>
<contexts>
<context position="3971" citStr="Yih et al., 2007" startWordPosition="577" endWordPosition="580">s included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from t</context>
<context position="28064" citStr="Yih et al., 2007" startWordPosition="4536" endWordPosition="4539">ification and extractive summarization. We name our system REGSUM. 8.1 Regression for Keyword Identification For each input, we define the set of keywords as the top k words according to the scores generated from different models. We compare our regression system with three unsupervised systems: PROB, LLR, MRW. To show the effectiveness of new features, we compare our results with a regression system trained only on word frequency and location related features described in Section 7. Those features are the ones standardly used for ranking the importance of words in recent summarization works (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012), and we name this system REGBASIC. Figure 1 shows the performance of systems when selecting the 100 words with highest weights Figure 1: Precision, Recall and F-score of keyword identification, 100 words selected, G1 as gold-standard as keywords. Each word from the input that appeared in any of the four human summaries is considered as a gold-standard keyword. Among the unsupervised approaches, word probability identifies keywords better than LLR and MRW by at least 4% on F-score. REGBASIC does not give better performance at keyword identificat</context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and Hisami Suzuki. 2007. Multi-document summarization by maximizing informative content-words. In Proceedings of IJCAI, pages 1776–1782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="4241" citStr="Zha, 2002" startWordPosition="614" endWordPosition="615">h Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of SIGIR, pages 113–120.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>