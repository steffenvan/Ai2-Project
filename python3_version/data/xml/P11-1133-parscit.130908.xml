<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030118">
<title confidence="0.98894">
Rare Word Translation Extraction from Aligned Comparable Documents
</title>
<author confidence="0.994829">
Emmanuel Prochasson and Pascale Fung
</author>
<affiliation confidence="0.899373">
Human Language Technology Center
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
</affiliation>
<email confidence="0.998577">
{eemmanuel,pascale}@ust.hk
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996858">
We present a first known result of high pre-
cision rare word bilingual extraction from
comparable corpora, using aligned compara-
ble documents and supervised classification.
We incorporate two features, a context-vector
similarity and a co-occurrence model between
words in aligned documents in a machine
learning approach. We test our hypothesis
on different pairs of languages and corpora.
We obtain very high F-Measure between 80%
and 98% for recognizing and extracting cor-
rect translations for rare terms (from 1 to 5 oc-
currences). Moreover, we show that our sys-
tem can be trained on a pair of languages and
test on a different pair of languages, obtain-
ing a F-Measure of 77% for the classification
of Chinese-English translations using a train-
ing corpus of Spanish-French. Our method is
therefore even potentially applicable to low re-
sources languages without training data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974959183674">
Rare words have long been a challenge to translate
automatically using statistical methods due to their
low occurrences. However, the Zipf’s Law claims
that, for any corpus of natural language text, the fre-
quency of a word wn (n being its rank in the fre-
quency table) will be roughly twice as high as the
frequency of word wn+1. The logical consequence
is that in any corpus, there are very few frequent
words and many rare words.
We propose a novel approach to extract rare word
translations from comparable corpora, relying on
two main features.
The first feature is the context-vector similar-
ity (Fung, 2000; Chiao and Zweigenbaum, 2002;
Laroche and Langlais, 2010): each word is charac-
terized by its context in both source and target cor-
pora, words in translation should have similar con-
text in both languages.
The second feature follows the assumption that
specific terms and their translations should appear
together often in documents on the same topic, and
rarely in non-related documents. This is the gen-
eral assumption behind early work on bilingual lex-
icon extraction from parallel documents using sen-
tence boundary as the context window size for co-
occurrence computation, we suggest to extend it to
aligned comparable documents using document as
the context window. This document context is too
large for co-occurrence computation of functional
words or high frequency content words, but we show
through observations and experiments that this win-
dow size is appropriate for rare words.
Both these features are unreliable when the num-
ber of occurrences of words are low. We sug-
gest however that they are complementary and can
be used together in a machine learning approach.
Moreover, we suggest that the model trained for one
pair of languages can be successfully applied to ex-
tract translations from another pair of languages.
This paper is organized as follows. In the next
section, we discuss the challenge of rare lexicon
extraction, explaining the reasons why classic ap-
proaches on comparable corpora fail at dealing with
rare words. We then discuss in section 3 the con-
cept of aligned comparable documents and how we
exploited those documents for bilingual lexicon ex-
traction in section 4. We present our resources and
implementation in section 5 then carry out and com-
ment several experiments in section 6.
</bodyText>
<page confidence="0.948792">
1327
</page>
<note confidence="0.9848655">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1327–1335,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.794743" genericHeader="introduction">
2 The challenge of rare lexicon extraction
</sectionHeader>
<bodyText confidence="0.99997176744186">
There are few previous works focusing on the ex-
traction of rare word translations, especially from
comparable corpora. One of the earliest works is
from (Pekar et al., 2006). They emphasized the
fact that the context-vector based approach, used for
processing comparable corpora, perform quite un-
reliably on all but the most frequent words. In a
nutshell1, this approach proceeds by gathering the
context of words in source and target languages in-
side context-vectors, then compares source and tar-
get context-vectors using similarity measures. In
a monolingual context, such an approach is used
to automatically get synonymy relationship between
words to build thesaurus (Grefenstette, 1994). In the
multilingual case, it is used to extract translations,
that is, pairs of words with the same meaning in
source and target corpora. It relies on the Firthien
hypothesis that you shall know a word by the com-
pany it keeps (Firth, 1957).
To show that the frequency of a word influences
its alignment, (Pekar et al., 2006) used six pairs of
comparable corpora, ranking translations according
to their frequencies. The less frequent words are
ranked around 100-160 by their algorithm, while the
most frequent ones typically appear at rank 20-40.
We ran a similar experiment using a French-
English comparable corpus containing medical doc-
uments, all related to the topic of breast cancer,
all manually classified as scientific discourse. The
French part contains about 530,000 words while the
English part contains about 7.4 millions words. For
this experiment though, we sampled the English part
to obtain a 530,000-words large corpus, matching
the size of the French part.
Using an implementation of the context-vector
similarity, we show in figure 1 that frequent words
(above 400 occurrences in the corpus) reach a 60%
precision whereas rare words (below 15 occur-
rences) are correctly aligned in only 5% of the time.
These results can be explained by the fact that, for
the vector comparison to be efficient, the informa-
tion they store has to be relevant and discriminatory.
If there are not enough occurrences of a word, it is
</bodyText>
<footnote confidence="0.985546666666667">
1Detailed presentations can be found for example in (Fung,
2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais,
2010).
</footnote>
<figureCaption confidence="0.9927676">
Figure 1: Results for context-vector based translations
extraction with respect to word frequency. The vertical
axis is the amount of correct translations found for Top,,
and the horizontal axis is the word occurrences in the cor-
pus.
</figureCaption>
<bodyText confidence="0.999487125">
impossible to get a precise description of the typical
context of this word, and therefore its description
is likely to be very different for source and target
words in translation.
We confirmed this result with another observa-
tion on the full English part of the previous cor-
pus, randomly split in 14 samples of the same size.
The context-vectors for very frequent words, such
as cancer (between 3,000 and 4,000 occurrences in
each sample) are very similar across the subsets.
Less frequent words, such as abnormality (between
70 and 16 occurrences in each sample) have very
unstable context-vectors, hence a lower similarity
across the subsets. This observation actually indi-
cates that it will be difficult to align abnormality
with itself.
</bodyText>
<sectionHeader confidence="0.975816" genericHeader="method">
3 Aligned comparable documents
</sectionHeader>
<bodyText confidence="0.9999423">
A pair of aligned comparable documents is a par-
ticular case of comparable corpus: two compara-
ble documents share the same topic and domain;
they both relate the same information but are not
mutual translations; although they might share par-
allel chunks (Munteanu and Marcu, 2005) – para-
graphs, sentences or phrases – in the general case
they were written independently. These compara-
ble documents, when concatenated together in order,
form an aligned comparable corpus.
</bodyText>
<page confidence="0.988206">
1328
</page>
<bodyText confidence="0.999955421052632">
Examples of such aligned documents can be
found, for example in (Munteanu and Marcu, 2005):
they aligned comparable documents with close pub-
lication dates. (Tao and Zhai, 2005) used an iter-
ative, bootstrapping approach to align comparable
documents using examples of already aligned cor-
pora. (Smith et al., 2010) aligned documents from
Wikipedia following the interlingual links provided
on articles.
We take advantage of this alignment between doc-
uments: by looking at what is common between
two aligned documents and what is different in
other documents, we obtain more precise informa-
tion about terms than when using a larger compa-
rable corpus without alignment. This is especially
interesting in the case of rare lexicon as the clas-
sic context-vector similarity is not discriminatory
enough and fails at raising interesting translation for
rare words.
</bodyText>
<sectionHeader confidence="0.990109" genericHeader="method">
4 Rare word translations from aligned
comparable documents
</sectionHeader>
<subsectionHeader confidence="0.993831">
4.1 Co-occurrence model
</subsectionHeader>
<bodyText confidence="0.999903744186047">
Different approaches have been proposed for bilin-
gual lexicon extraction from parallel corpora, rely-
ing on the assumption that a word has one sense, one
translation, no missing translation, and that its trans-
lation appears in aligned parallel sentences (Fung,
2000). Therefore, translations can be extracted by
comparing the distribution of words across the sen-
tences. For example, (Gale and Church, 1991) used
a derivative of the χ2 statistics to evaluate the as-
sociation between words in aligned region of paral-
lel documents. Such association scores evaluate the
strength of the relation between events. In the case
of parallel sentences and lexicon extraction, they
measure how often two words appear in aligned sen-
tences, and how often one appears without the other.
More precisely, they will compare their number of
co-occurrences against the expected number of co-
occurrences under the null-hypothesis that words are
randomly distributed. If they appear together more
often than expected, they are considered as associ-
ated (Evert, 2008).
We focus in this work on rare words, more pre-
cisely on specialized terminology. We define them
as the set of terms that appear from 1 (hapaxes)
to 5 times. We use a strategy similar to the one
applied on parallel sentences, but rely on aligned
documents. Our hypothesis is very similar: words
in translation should appear in aligned comparable
documents. We used the Jaccard similarity (eq. 1)
to evaluate the association between words among
aligned comparable documents. In the general case,
this measure would not give relevant scores due to
frequency issue: it produces the same scores for
two words that appear always together, and never
one without the other, disregarding the fact that they
appear 500 times or one time only. Other associ-
ation scores generally rely on occurrence and co-
occurrence counts to tackle this issue (such as the
log-likelihood, eq. 2). In our case, the number of
co-occurrences will be limited by the number of oc-
currences of the words, from 1 to 5. Therefore, the
Jaccard similarity efficiently reflects what we want
to observe.
</bodyText>
<equation confidence="0.9519565">
J(wi, wj) = |Ai ∩ Aj|
|Ai ∪ Aj|&apos; Ai = {d : wi ∈ d} (1)
</equation>
<bodyText confidence="0.999972">
A score of 1 indicates a perfect association
(words always appear together, never one without
the other), the more one word appears without the
other, the lower the score.
</bodyText>
<subsectionHeader confidence="0.98164">
4.2 Context-vector similarity
</subsectionHeader>
<bodyText confidence="0.999974857142857">
We implemented the context-vector similarity in a
way similar to (Morin et al., 2007). In all experi-
ments, we used the same set of parameters, as they
yielded the best results on our corpora. We built the
context-vectors using nouns only as seed lexicon,
with a window size of 20. Source context-vectors
are translated in the target language using the re-
sources presented in the next section. We used the
log-likelihood (Dunning, 1993, eq. 2) for context-
vector normalization (O is the observed number of
co-occurrence in the corpus, E is the expected num-
ber of co-occurrences under the null hypothesis).
We used the Cosine similarity (eq. 3) for context-
vector comparisons.
</bodyText>
<equation confidence="0.923850428571429">
Oij
Oijlog (2)
Eij
A ·B
Cosine(A, B) = kAk2 + kBk2 − A · B (3)
ll(wi, wj) = 2 �
ij
</equation>
<page confidence="0.953455">
1329
</page>
<subsectionHeader confidence="0.985939">
4.3 Binary classification of rare translations
</subsectionHeader>
<bodyText confidence="0.99998715625">
We suggest to incorporate both the context-vector
similarity and the co-occurrence features in a ma-
chine learning approach. This approach consists of
training a classifier on positive examples of transla-
tion pairs, and negative examples of non-translations
pairs. The trained model (in our case, a decision
tree) is then used to tag an unknown pair of words as
either ”Translation” or ”Non-Translation”.
One potential problem for building the training
set, as pointed out for example by (Zhao and Ng,
2007) is this: we have a limited number of pos-
itive examples, but a very large amount of non-
translation examples as obviously is the case for
rare word translations in any training corpus. In-
cluding two many negative examples in the training
set would lead the classifier to label every pairs as
”Non-Translation”.
To tackle this problem, (Zhao and Ng, 2007)
tuned the imbalance of positive/negative ratio by re-
sampling the positive examples in the training set.
We chose to reduce the set of negative examples,
and found that a ratio of five negative examples to
one positive is optimal in our case. A lower ratio
improves precision but reduces recall for the ”Trans-
lation” class.
It is also desirable that the classifier focuses on
discriminating between confusing pairs of transla-
tions. As most of the negative examples have a
null co-occurrence score and a null context-vector
similarity, they are excluded from the training set.
The negative examples are randomly chosen among
those that fulfill the following constraints:
</bodyText>
<listItem confidence="0.87414">
• non-null features ;
• ratio of number of occurrences between
source/target words higher than 0.2 and lower
than 5.
</listItem>
<bodyText confidence="0.999902333333333">
We use the J48 decision tree algorithm, in the
Weka environment (Hall et al., 2009). Features are
computed using the Jaccard similarity (section 3)
for the co-occurrence model, and the implementa-
tion of the context-vector similarity presented in sec-
tion 4.2.
</bodyText>
<subsectionHeader confidence="0.987967">
4.4 Extension to another pair of languages
</subsectionHeader>
<bodyText confidence="0.99999325">
Even though the context vector similarity has been
shown to achieve different accuracy depending on
the pair of languages involved, the co-occurrence
model is totally language independent. In the case of
binary classification of translations, the two models
are complementary to each other: word pairs with
null co-occurrence are not considered by the context
model while the context vector model gives more se-
mantic information than the co-occurrence model.
For these reasons, we suggest that it is possible
to use a decision tree trained on one pair of lan-
guages to extract translations from another pair of
languages. A similar approach is proposed in (Al-
fonseca et al., 2008): they present a word decom-
position model designed for German language that
they successfully applied to other compounding lan-
guages. Our approach consists in training a decision
tree on a pair of languages and applying this model
to the classification of unknown pairs of words in
another pair of languages. Such an approach is es-
pecially useful for prospecting new translations from
less known languages, using a well known language
as training.
We used the same algorithms and same features as
in the previous sections, but used the data computed
from one pair of languages as the training set, and
the data computed from another pair of languages as
the testing set.
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="method">
5 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.849587">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999837923076923">
We built several corpora using two different strate-
gies. The first set was built using Wikipedia and the
interlingual links available on articles (that points
to another version of the same article in another
language). We started from the list of all French
articles2 and randomly selected articles that pro-
vide a link to Spanish and English versions. We
downloaded those, and clean them by removing the
wikipedia formatting tags to obtain raw UTF8 texts.
Articles were not selected based on their sizes, the
vocabulary used, nor a particular topic. We obtained
about 20,000 aligned documents for each language.
A second set was built using an in-house system
</bodyText>
<footnote confidence="0.988259">
2Available on http://download.wikimedia.org/.
</footnote>
<page confidence="0.805406">
1330
</page>
<table confidence="0.99955925">
[WP] French [WP] English [WP] Es [CLIR] En [CLIR] Zh
#documents 20,169 20,169 20,169 15,3247 15,3247
#tokens 4,008,284 5,470,661 2,741,789 1,334,071 1,228,330
#unique tokens 120,238 128,831 103,398 30,984 60,015
</table>
<tableCaption confidence="0.998276">
Table 1: Statistics for all parts of all corpora.
</tableCaption>
<bodyText confidence="0.999956347826087">
(unpublished) that seeks for comparable and paral-
lel documents from the web. Starting from a list of
Chinese documents (in this case, mostly news arti-
cles), we automatically selected English target docu-
ments using Cross Language Information Retrieval.
About 85% of the paired documents obtained are di-
rect translations (header/footer of web pages apart).
However, they will be processed just like aligned
comparable documents, that is, we will not take ad-
vantage of the structure of the parallel contents to
improve accuracy, but will use the exact same ap-
proach that we applied for the Wikipedia documents.
We gathered about 15,000 pairs of documents em-
ploying this method.
All corpora were processed using Tree-Tagger3
for segmentation and Part-of-Speech tagging. We
focused on nouns only and discarded all other to-
kens. We would record the lemmatized form of
tokens when available, otherwise we would record
the original form. Table 1 summarizes main statis-
tics for each corpus; [WP] refers to the Wikipedia
corpora, [CLIR] to the Chinese-English corpora ex-
tracted through cross language information retrieval.
</bodyText>
<subsectionHeader confidence="0.965003">
5.2 Dictionaries
</subsectionHeader>
<bodyText confidence="0.996674636363636">
We need a bilingual seed lexicon for the context-
vector similarity. We used a French-English lex-
icon obtained from the Web. It contains about
67,000 entries. The Spanish-English and Spanish-
French dictionaries were extracted from the linguis-
tic resources of the Apertium project4. We ob-
tained approximately 22,500 Spanish-English trans-
lations and 12,000 for Spanish-French. Finally, for
Chinese-English we used the LDC2002L27 resource
from the Linguistic Data Consortium5 with about
122,000 entries.
</bodyText>
<footnote confidence="0.9614188">
3http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
4http://www.apertium.org
5http://www.ldc.upenn.edu
</footnote>
<subsectionHeader confidence="0.991783">
5.3 Evaluation lists
</subsectionHeader>
<bodyText confidence="0.999984090909091">
To evaluate our approach, we needed evaluation lists
of terms for which translations are already known.
We used the Medical Subject Headlines, from the
UMLS meta-thesaurus6 which provides a lexicon of
specialized, medical terminology, notably in Span-
ish, English and French. We used the LDC lexi-
con presented in the previous section for Chinese-
English.
From these resources, we selected all the source
words that appears from 1 to 5 times in the corpora
in order to build the evaluation lists.
</bodyText>
<subsectionHeader confidence="0.98801">
5.4 Oracle translations
</subsectionHeader>
<bodyText confidence="0.999942473684211">
We looked at the corpora to evaluate how many
translation pairs from the evaluation lists can be
found across the aligned comparable documents.
Those translations are hereafter the oracle transla-
tions. For French/English, French/Spanish and En-
glish/Spanish, about 60% of the translation pairs can
be found. For Chinese/English, this ratio reaches
45%. The main reason for this lower result is the
inaccuracy of the segmentation tool used to process
Chinese. Segmentation tools usually rely on a train-
ing corpus and typically fail at handling rare words
which, by definition, were unlikely to be found in the
training examples. Therefore, some rare Chinese to-
kens found in our corpus are the results of faulty seg-
mentation, and the translation of those faulty words
can not be found in related documents. We encoun-
tered the same issue but at a much lower degree for
other languages because of spelling mistakes and/or
improper Part-of-Speech tagging.
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99924025">
We ran three different experiments. Experiment I
compares the accuracy of the context-vector sim-
ilarity and the co-occurrence model. Experiment
II uses supervised classification with both features.
</bodyText>
<footnote confidence="0.973535">
6http://www.nlm.nih.gov/research/umls/
</footnote>
<page confidence="0.992958">
1331
</page>
<figureCaption confidence="0.9937575">
Figure 2: Experiment I: comparison of accuracy obtained for the Toplo with the context-vector similarity and the
co-occurrence model, for hapaxes (left) and words that appear 2 to 5 times (right).
</figureCaption>
<bodyText confidence="0.987168666666667">
Experiment III extracts translation from a pair of
languages, using a classifier trained on another pair
of languages.
</bodyText>
<subsectionHeader confidence="0.7950725">
6.1 Experiment I: co-occurrence model vs.
context-vector similarity
</subsectionHeader>
<bodyText confidence="0.9997655">
We split the French-English part of the Wikipedia
corpus into different samples: the first sample con-
tains 500 pairs of documents. We then aggregated
more documents to this initial sample to test differ-
ent sizes of corpora. We built the sample in order to
ensure hapaxes in the whole corpus are hapaxes in
all subsets. That is, we ensured the 431 hapaxes in
the evaluation lists are represented in the 500 docu-
ments subset.
We extracted translations in two different ways:
</bodyText>
<listItem confidence="0.984719">
1. using the co-occurrence model;
2. using the context-vector based approach, with
the same evaluation lists.
</listItem>
<bodyText confidence="0.994616464285714">
The accuracy is computed on 1,000 pairs of trans-
lations from the set of oracle translations, and mea-
sures the amount of correct translations found for the
10 best ranks (Topio) after ranking the candidates
according to their score (context-vector similarity or
co-occurrence model). The results are presented in
figure 2.
We can draw two conclusions out of these results.
First, the size of the corpus influences the quality
of the bilingual lexicon extraction when using the
co-occurrence model. This is especially interesting
with hapaxes, for which frequency does not change
with the increase of the size of the corpora. The ac-
curacy is improved by adding more information to
the corpus, even if this additional information does
not cover the pairs of translations we are looking for.
The added documents will weaken the association
of incorrect translations, without changing the as-
sociation for rare terms translations. For example,
the precision for hapaxes using the co-occurrence
model ranges from less than 1% when using only
500 pairs of documents, to about 13% when using
all documents. The second conclusion is that the
co-occurrence model outperforms the context-vector
similarity.
However, both these approaches still perform
poorly. In the next experiment, we propose to com-
bine them using supervised classification.
</bodyText>
<subsectionHeader confidence="0.897045">
6.2 Experiment II: binary classification of
translation
</subsectionHeader>
<bodyText confidence="0.998186">
For each corpus or combination of corpora –
English-Spanish, English-French, Spanish-French
and Chinese-English, we ran three experiments, us-
ing the following features for supervised learning of
translations:
</bodyText>
<listItem confidence="0.999738">
• the context-vector similarity;
• the co-occurrence model;
• both features together.
</listItem>
<bodyText confidence="0.999813285714286">
The parameters are discussed in section 4.3. We
used all the oracle translations to train the positive
values. Results are presented in table 2, they are
computed using a 10-folds cross validation. Class
T refers to ”Translation”, ¬T to ”Non-Translation”.
The evaluation of precision/recall/F-Measure for the
class ”Translation” are given in equation 4 to 6.
</bodyText>
<page confidence="0.987538">
1332
</page>
<table confidence="0.999514482758621">
Precision Recall F-Measure Cl.
English-Spanish
context- 0.0% 0.0% 0.0% T
vectors 83.3% 99.9% 90.8% �T
co-occ. 66.2% 44.2% 53.0% T
model 89.5% 95.5% 92.4% �T
both 98.6% 88.6% 93.4% T
97.8% 99.8% 98.7% �T
French-English
context- 76.5% 10.3% 18.1% T
vectors 90.9% 99.6% 95.1% �T
co-occ. 85.7% 1.2% 2.4% T
model 90.1% 100% 94.8% �T
both 81.0% 80.2% 80.6% T
94.9% 98.7% 96.8% �T
French-Spanish
context- 0.0% 0.0% 0.0% T
vectors 81.0% 100% 89.5% �T
co-occ. 64.2% 46.5% 53.9% T
model 88.2% 93.9% 91.0% �T
both 98.7% 94.6% 96.7% T
98.8% 99.7% 99.2% �T
Chinese-English
context- 69.6% 13.3% 22.3% T
vectors 91.0% 93.1% 92.1% �T
co-occ. 73.8% 32.5% 45.1% T
model 85.2% 97.1% 90.8% �T
both 86.7% 74.7% 80.3% T
96.3% 98.3% 97.3% �T
</table>
<tableCaption confidence="0.963762">
Table 2: Experiment II: results ofbinary classification for
”Translation” and ”Non-Translation”.
</tableCaption>
<equation confidence="0.98094">
FMeasure = 2 x precision x recall (6)
precision + recall
</equation>
<bodyText confidence="0.999408655172414">
These results show first that one feature is gen-
erally not discriminatory enough to discern correct
translation and non-translation pairs. For example
with Spanish-English, by using context-vector sim-
ilarity only, we obtained very high recall/precision
for the classification of ”Non-Translation”, but null
precision/recall for the classification of ”Transla-
tion”. In some other cases, we obtained high pre-
cision but poor recall with one feature only, which is
not a usefully result as well since most of the correct
translations are still labeled as ”Non-Translation”.
However, when using both features, the precision
is strongly improved up to 98% (English-Spanish
or French-Spanish) with a high recall of about 90%
for class T. We also achieved about 86%/75% pre-
cision/recall in the case of Chinese-English, even
though they are very distant languages. This last re-
sult is also very promising since it has been obtained
from a fully automatically built corpus. Table 3
shows some examples of correctly labeled ”Trans-
lation”.
The decision trees obtained indicate that, in gen-
eral, word pairs with very high co-occurrence model
scores are translations, and that the context-vector
similarity disambiguate candidates with lower co-
occurrence model scores. Interestingly, the trained
decision trees are very similar between the different
pairs of languages, which inspired the next experi-
ment.
</bodyText>
<subsectionHeader confidence="0.9737725">
6.3 Experiment III: extension to another pair
of languages
</subsectionHeader>
<bodyText confidence="0.999996952380952">
In the last experiment, we focused on using the
knowledge acquired with a given pair of languages
to recognize proper translation pairs using a dif-
ferent pair of languages. For this experiment, we
used the data from one corpus to train the classifier,
and used the data from another combination of lan-
guages as the test set. Results are displayed in ta-
ble 4.
These last results are of great interest because
they show that translation pairs can be correctly
classified even with a classifier trained on another
pair of languages. This is very promising be-
cause it allows one to prospect new languages using
knowledge acquired on a known pairs of languages.
As an example, we reached a 77% F-Measure for
Chinese-English alignment using a classifier trained
on Spanish-French features. This not only confirms
the precision/recall of our approach in general, but
also shows that the model obtained by training tends
to be very stable and accurate across different pairs
of languages and different corpora.
</bodyText>
<equation confidence="0.963687142857143">
|T n oracle|
precisionT =
(4)
|T|
|T n oracle|
recallT = (5)
|oracle|
</equation>
<page confidence="0.896283">
1333
</page>
<table confidence="0.997461333333333">
Tested with
Trained with Sp-En Sp-Fr Fr-En Zh-En
Sp-En 98.6/88.8/93.5 98.7/94.9/96.8 91.5/48.3/63.2 99.3/63.0/77.1
Sp-Fr 89.5/77.9/83.9 90.4/82.9/86.5 75.4/53.5/62.6 98.7/63.3/77.1
Fr-En 89.5/77.9/83.9 90.4/82.9/86.5 85.2/80.0/82.6 81.0/87.6/84.2
Zh-En 96.6/89.2/92.7 97.7/94.9/96.3 81.1/50.9/62.5 97.4/65.1/78.1
</table>
<tableCaption confidence="0.985711">
Table 4: Experiment III: Precision/Recall/F-Measure for label ”Translation”, obtained for all training/testing set com-
binations.
</tableCaption>
<table confidence="0.99951484375">
English French
myometrium myom`etre
lysergide lysergide
hyoscyamus jusquiame
lysichiton lysichiton
brassicaceae brassicac´ees
yarrow achill´ee
spikemoss s´elaginelle
leiomyoma fibromyome
ryegrass ivraie
English Spanish
spirometry espirometria
lolium lolium
omentum epipl´on
pilocarpine pilocarpina
chickenpox varicela
bruxism bruxismo
psittaciformes psittaciformes
commodification mercantilizaci´on
talus astr´agalo
English Chinese
hooliganism fascism法西斯t义
kindergarten A氓
oyster 幼儿园
taxonomy 牡蛎
mongolian 分类学
subpoena 蒙古人
rupee 传票
archbishop 卢比
serfdom 大t教
typhoid 农奴
伤寒
</table>
<tableCaption confidence="0.8452394">
Table 3: Experiment II and III: examples of rare word
translations found by our algorithm. Note that even
though some words such as ”kindergarten” are not rare
in general, they occur with very low frequency in the test
corpus.
</tableCaption>
<sectionHeader confidence="0.997651" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999634">
We presented a new approach for extracting transla-
tions of rare words among aligned comparable doc-
uments. To the best of our knowledge, this is one
of the first high accuracy extraction of rare lexi-
con from non-parallel documents. We obtained a F-
Measure ranging from about 80% (French-English,
Chinese-English) to 97% (French-Spanish). We also
obtained good results for extracting lexicon for a
pair of languages, using a decision tree trained with
the data computed on another pair of languages.
We yielded a 77% F-Measure for the extraction of
Chinese-English lexicon, using Spanish-French for
training the model.
On top of these promising results, our approach
presents several other advantages. First, we showed
that it works well on automatically built corpora
which require minimal human intervention. Aligned
comparable documents can easily be collected and
are available in large volumes. Moreover, the pro-
posed machine learning method incorporating both
context-vector and co-occurrence model has shown
to give good results on pairs of languages that are
very different from each other, such as Chinese-
English. It is also applicable across different train-
ing and testing language pairs, making it possible
for us to find rare word translations even for lan-
guages without training data. The co-occurrence
model is completely language independent and have
been shown to give good results on various pairs of
languages, including Chinese-English.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99881575">
The authors would like to thank Emmanuel Morin
(LINA CNRS 6241) for providing us the compa-
rable corpus used for the experiment in section 2,
Simon Shi for extracting and providing the corpus
</bodyText>
<page confidence="0.979922">
1334
</page>
<bodyText confidence="0.999712">
described in section 5.1, and the anonymous re-
viewers for their valuable comments. This research
is partly supported by ITS/189/09 AND BBNX02-
20F00310/11PN.
</bodyText>
<sectionHeader confidence="0.998444" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823698630137">
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008. Decompounding query keywords from com-
pounding languages. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL’08), pages 253–256.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING’02), pages 1208–1212.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61–74.
Stefan Evert. 2008. Corpora and collocations. In
A. Ludeling and M. Kyto, editors, Corpus Linguis-
tics. An International Handbook, chapter 58. Mouton
de Gruyter, Berlin.
John Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis, Philological.
Longman.
Pascale Fung. 2000. A statistical view on bilingual lex-
icon extraction–from parallel corpora to non-parallel
corpora. In Jean V´eronis, editor, Parallel Text Pro-
cessing, page 428. Kluwer Academic Publishers.
William A. Gale and Kenneth W. Church. 1991. Iden-
tifying word correspondence in parallel texts. In
Proceedings of the workshop on Speech and Natural
Language, HLT’91, pages 152–157, Morristown, NJ,
USA. Association for Computational Linguistics.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publisher.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 617–625, Beijing, China, Aug.
Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining –
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL’07), pages 664–
671, Prague, Czech Republic.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Exploit-
ing Non-Parallel Corpora. Computational Linguistics,
31(4):477–504.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20(4):247–266.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the ACL, pages 403–
411.
Tao Tao and ChengXiang Zhai. 2005. Mining compa-
rable bilingual text corpora for cross-language infor-
mation integration. In KDD ’05: Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 691–696,
New York, NY, USA. ACM.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifi-
cation and resolution of Chinese zero pronouns: A
machine learning approach. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic.
</reference>
<page confidence="0.992632">
1335
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.333138">
<title confidence="0.999816">Rare Word Translation Extraction from Aligned Comparable Documents</title>
<author confidence="0.61931">Emmanuel Prochasson</author>
<author confidence="0.61931">Pascale</author>
<affiliation confidence="0.4298105">Human Language Technology Hong Kong University of Science and</affiliation>
<address confidence="0.472981">Clear Water Bay, Kowloon, Hong</address>
<abstract confidence="0.999636476190476">We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1 to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Slaven Bilac</author>
<author>Stefan Pharies</author>
</authors>
<title>Decompounding query keywords from compounding languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08),</booktitle>
<pages>253--256</pages>
<contexts>
<context position="14180" citStr="Alfonseca et al., 2008" startWordPosition="2283" endWordPosition="2287"> to achieve different accuracy depending on the pair of languages involved, the co-occurrence model is totally language independent. In the case of binary classification of translations, the two models are complementary to each other: word pairs with null co-occurrence are not considered by the context model while the context vector model gives more semantic information than the co-occurrence model. For these reasons, we suggest that it is possible to use a decision tree trained on one pair of languages to extract translations from another pair of languages. A similar approach is proposed in (Alfonseca et al., 2008): they present a word decomposition model designed for German language that they successfully applied to other compounding languages. Our approach consists in training a decision tree on a pair of languages and applying this model to the classification of unknown pairs of words in another pair of languages. Such an approach is especially useful for prospecting new translations from less known languages, using a well known language as training. We used the same algorithms and same features as in the previous sections, but used the data computed from one pair of languages as the training set, an</context>
</contexts>
<marker>Alfonseca, Bilac, Pharies, 2008</marker>
<rawString>Enrique Alfonseca, Slaven Bilac, and Stefan Pharies. 2008. Decompounding query keywords from compounding languages. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL’08), pages 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING’02),</booktitle>
<pages>1208--1212</pages>
<contexts>
<context position="1788" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="278" endWordPosition="281">ave long been a challenge to translate automatically using statistical methods due to their low occurrences. However, the Zipf’s Law claims that, for any corpus of natural language text, the frequency of a word wn (n being its rank in the frequency table) will be roughly twice as high as the frequency of word wn+1. The logical consequence is that in any corpus, there are very few frequent words and many rare words. We propose a novel approach to extract rare word translations from comparable corpora, relying on two main features. The first feature is the context-vector similarity (Fung, 2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010): each word is characterized by its context in both source and target corpora, words in translation should have similar context in both languages. The second feature follows the assumption that specific terms and their translations should appear together often in documents on the same topic, and rarely in non-related documents. This is the general assumption behind early work on bilingual lexicon extraction from parallel documents using sentence boundary as the context window size for cooccurrence computation, we suggest to extend it to aligned comparable documents</context>
<context position="5930" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="942" endWordPosition="945">to obtain a 530,000-words large corpus, matching the size of the French part. Using an implementation of the context-vector similarity, we show in figure 1 that frequent words (above 400 occurrences in the corpus) reach a 60% precision whereas rare words (below 15 occurrences) are correctly aligned in only 5% of the time. These results can be explained by the fact that, for the vector comparison to be efficient, the information they store has to be relevant and discriminatory. If there are not enough occurrences of a word, it is 1Detailed presentations can be found for example in (Fung, 2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Figure 1: Results for context-vector based translations extraction with respect to word frequency. The vertical axis is the amount of correct translations found for Top,, and the horizontal axis is the word occurrences in the corpus. impossible to get a precise description of the typical context of this word, and therefore its description is likely to be very different for source and target words in translation. We confirmed this result with another observation on the full English part of the previous corpus, randomly split in 14 samples of the same size. The con</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In Proceedings of the 19th International Conference on Computational Linguistics (COLING’02), pages 1208–1212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="11179" citStr="Dunning, 1993" startWordPosition="1792" endWordPosition="1793">a perfect association (words always appear together, never one without the other), the more one word appears without the other, the lower the score. 4.2 Context-vector similarity We implemented the context-vector similarity in a way similar to (Morin et al., 2007). In all experiments, we used the same set of parameters, as they yielded the best results on our corpora. We built the context-vectors using nouns only as seed lexicon, with a window size of 20. Source context-vectors are translated in the target language using the resources presented in the next section. We used the log-likelihood (Dunning, 1993, eq. 2) for contextvector normalization (O is the observed number of co-occurrence in the corpus, E is the expected number of co-occurrences under the null hypothesis). We used the Cosine similarity (eq. 3) for contextvector comparisons. Oij Oijlog (2) Eij A ·B Cosine(A, B) = kAk2 + kBk2 − A · B (3) ll(wi, wj) = 2 � ij 1329 4.3 Binary classification of rare translations We suggest to incorporate both the context-vector similarity and the co-occurrence features in a machine learning approach. This approach consists of training a classifier on positive examples of translation pairs, and negativ</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>Corpora and collocations.</title>
<date>2008</date>
<booktitle>Corpus Linguistics. An International Handbook, chapter 58. Mouton de Gruyter,</booktitle>
<editor>In A. Ludeling and M. Kyto, editors,</editor>
<location>Berlin.</location>
<contexts>
<context position="9424" citStr="Evert, 2008" startWordPosition="1491" endWordPosition="1492">e χ2 statistics to evaluate the association between words in aligned region of parallel documents. Such association scores evaluate the strength of the relation between events. In the case of parallel sentences and lexicon extraction, they measure how often two words appear in aligned sentences, and how often one appears without the other. More precisely, they will compare their number of co-occurrences against the expected number of cooccurrences under the null-hypothesis that words are randomly distributed. If they appear together more often than expected, they are considered as associated (Evert, 2008). We focus in this work on rare words, more precisely on specialized terminology. We define them as the set of terms that appear from 1 (hapaxes) to 5 times. We use a strategy similar to the one applied on parallel sentences, but rely on aligned documents. Our hypothesis is very similar: words in translation should appear in aligned comparable documents. We used the Jaccard similarity (eq. 1) to evaluate the association between words among aligned comparable documents. In the general case, this measure would not give relevant scores due to frequency issue: it produces the same scores for two w</context>
</contexts>
<marker>Evert, 2008</marker>
<rawString>Stefan Evert. 2008. Corpora and collocations. In A. Ludeling and M. Kyto, editors, Corpus Linguistics. An International Handbook, chapter 58. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in Linguistic Analysis,</title>
<date>1957</date>
<publisher>Philological. Longman.</publisher>
<contexts>
<context position="4646" citStr="Firth, 1957" startWordPosition="736" endWordPosition="737">most frequent words. In a nutshell1, this approach proceeds by gathering the context of words in source and target languages inside context-vectors, then compares source and target context-vectors using similarity measures. In a monolingual context, such an approach is used to automatically get synonymy relationship between words to build thesaurus (Grefenstette, 1994). In the multilingual case, it is used to extract translations, that is, pairs of words with the same meaning in source and target corpora. It relies on the Firthien hypothesis that you shall know a word by the company it keeps (Firth, 1957). To show that the frequency of a word influences its alignment, (Pekar et al., 2006) used six pairs of comparable corpora, ranking translations according to their frequencies. The less frequent words are ranked around 100-160 by their algorithm, while the most frequent ones typically appear at rank 20-40. We ran a similar experiment using a FrenchEnglish comparable corpus containing medical documents, all related to the topic of breast cancer, all manually classified as scientific discourse. The French part contains about 530,000 words while the English part contains about 7.4 millions words.</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John Firth. 1957. A synopsis of linguistic theory 1930-1955. Studies in Linguistic Analysis, Philological. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction–from parallel corpora to non-parallel corpora.</title>
<date>2000</date>
<booktitle>Parallel Text Processing,</booktitle>
<pages>428</pages>
<editor>In Jean V´eronis, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1759" citStr="Fung, 2000" startWordPosition="276" endWordPosition="277">Rare words have long been a challenge to translate automatically using statistical methods due to their low occurrences. However, the Zipf’s Law claims that, for any corpus of natural language text, the frequency of a word wn (n being its rank in the frequency table) will be roughly twice as high as the frequency of word wn+1. The logical consequence is that in any corpus, there are very few frequent words and many rare words. We propose a novel approach to extract rare word translations from comparable corpora, relying on two main features. The first feature is the context-vector similarity (Fung, 2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010): each word is characterized by its context in both source and target corpora, words in translation should have similar context in both languages. The second feature follows the assumption that specific terms and their translations should appear together often in documents on the same topic, and rarely in non-related documents. This is the general assumption behind early work on bilingual lexicon extraction from parallel documents using sentence boundary as the context window size for cooccurrence computation, we suggest to extend it to</context>
<context position="5901" citStr="Fung, 2000" startWordPosition="940" endWordPosition="941">nglish part to obtain a 530,000-words large corpus, matching the size of the French part. Using an implementation of the context-vector similarity, we show in figure 1 that frequent words (above 400 occurrences in the corpus) reach a 60% precision whereas rare words (below 15 occurrences) are correctly aligned in only 5% of the time. These results can be explained by the fact that, for the vector comparison to be efficient, the information they store has to be relevant and discriminatory. If there are not enough occurrences of a word, it is 1Detailed presentations can be found for example in (Fung, 2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Figure 1: Results for context-vector based translations extraction with respect to word frequency. The vertical axis is the amount of correct translations found for Top,, and the horizontal axis is the word occurrences in the corpus. impossible to get a precise description of the typical context of this word, and therefore its description is likely to be very different for source and target words in translation. We confirmed this result with another observation on the full English part of the previous corpus, randomly split in 14 samp</context>
<context position="8648" citStr="Fung, 2000" startWordPosition="1370" endWordPosition="1371">bout terms than when using a larger comparable corpus without alignment. This is especially interesting in the case of rare lexicon as the classic context-vector similarity is not discriminatory enough and fails at raising interesting translation for rare words. 4 Rare word translations from aligned comparable documents 4.1 Co-occurrence model Different approaches have been proposed for bilingual lexicon extraction from parallel corpora, relying on the assumption that a word has one sense, one translation, no missing translation, and that its translation appears in aligned parallel sentences (Fung, 2000). Therefore, translations can be extracted by comparing the distribution of words across the sentences. For example, (Gale and Church, 1991) used a derivative of the χ2 statistics to evaluate the association between words in aligned region of parallel documents. Such association scores evaluate the strength of the relation between events. In the case of parallel sentences and lexicon extraction, they measure how often two words appear in aligned sentences, and how often one appears without the other. More precisely, they will compare their number of co-occurrences against the expected number o</context>
</contexts>
<marker>Fung, 2000</marker>
<rawString>Pascale Fung. 2000. A statistical view on bilingual lexicon extraction–from parallel corpora to non-parallel corpora. In Jean V´eronis, editor, Parallel Text Processing, page 428. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>Identifying word correspondence in parallel texts.</title>
<date>1991</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT’91,</booktitle>
<pages>152--157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8788" citStr="Gale and Church, 1991" startWordPosition="1389" endWordPosition="1392">on as the classic context-vector similarity is not discriminatory enough and fails at raising interesting translation for rare words. 4 Rare word translations from aligned comparable documents 4.1 Co-occurrence model Different approaches have been proposed for bilingual lexicon extraction from parallel corpora, relying on the assumption that a word has one sense, one translation, no missing translation, and that its translation appears in aligned parallel sentences (Fung, 2000). Therefore, translations can be extracted by comparing the distribution of words across the sentences. For example, (Gale and Church, 1991) used a derivative of the χ2 statistics to evaluate the association between words in aligned region of parallel documents. Such association scores evaluate the strength of the relation between events. In the case of parallel sentences and lexicon extraction, they measure how often two words appear in aligned sentences, and how often one appears without the other. More precisely, they will compare their number of co-occurrences against the expected number of cooccurrences under the null-hypothesis that words are randomly distributed. If they appear together more often than expected, they are co</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William A. Gale and Kenneth W. Church. 1991. Identifying word correspondence in parallel texts. In Proceedings of the workshop on Speech and Natural Language, HLT’91, pages 152–157, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publisher.</publisher>
<contexts>
<context position="4405" citStr="Grefenstette, 1994" startWordPosition="692" endWordPosition="693">nslations, especially from comparable corpora. One of the earliest works is from (Pekar et al., 2006). They emphasized the fact that the context-vector based approach, used for processing comparable corpora, perform quite unreliably on all but the most frequent words. In a nutshell1, this approach proceeds by gathering the context of words in source and target languages inside context-vectors, then compares source and target context-vectors using similarity measures. In a monolingual context, such an approach is used to automatically get synonymy relationship between words to build thesaurus (Grefenstette, 1994). In the multilingual case, it is used to extract translations, that is, pairs of words with the same meaning in source and target corpora. It relies on the Firthien hypothesis that you shall know a word by the company it keeps (Firth, 1957). To show that the frequency of a word influences its alignment, (Pekar et al., 2006) used six pairs of comparable corpora, ranking translations according to their frequencies. The less frequent words are ranked around 100-160 by their algorithm, while the most frequent ones typically appear at rank 20-40. We ran a similar experiment using a FrenchEnglish c</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<contexts>
<context position="13282" citStr="Hall et al., 2009" startWordPosition="2142" endWordPosition="2145">atio improves precision but reduces recall for the ”Translation” class. It is also desirable that the classifier focuses on discriminating between confusing pairs of translations. As most of the negative examples have a null co-occurrence score and a null context-vector similarity, they are excluded from the training set. The negative examples are randomly chosen among those that fulfill the following constraints: • non-null features ; • ratio of number of occurrences between source/target words higher than 0.2 and lower than 5. We use the J48 decision tree algorithm, in the Weka environment (Hall et al., 2009). Features are computed using the Jaccard similarity (section 3) for the co-occurrence model, and the implementation of the context-vector similarity presented in section 4.2. 4.4 Extension to another pair of languages Even though the context vector similarity has been shown to achieve different accuracy depending on the pair of languages involved, the co-occurrence model is totally language independent. In the case of binary classification of translations, the two models are complementary to each other: word pairs with null co-occurrence are not considered by the context model while the conte</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Laroche</author>
<author>Philippe Langlais</author>
</authors>
<title>Revisiting context-based projection methods for term-translation spotting in comparable corpora.</title>
<date>2010</date>
<booktitle>In 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>617--625</pages>
<location>Beijing, China,</location>
<contexts>
<context position="1817" citStr="Laroche and Langlais, 2010" startWordPosition="282" endWordPosition="285">translate automatically using statistical methods due to their low occurrences. However, the Zipf’s Law claims that, for any corpus of natural language text, the frequency of a word wn (n being its rank in the frequency table) will be roughly twice as high as the frequency of word wn+1. The logical consequence is that in any corpus, there are very few frequent words and many rare words. We propose a novel approach to extract rare word translations from comparable corpora, relying on two main features. The first feature is the context-vector similarity (Fung, 2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010): each word is characterized by its context in both source and target corpora, words in translation should have similar context in both languages. The second feature follows the assumption that specific terms and their translations should appear together often in documents on the same topic, and rarely in non-related documents. This is the general assumption behind early work on bilingual lexicon extraction from parallel documents using sentence boundary as the context window size for cooccurrence computation, we suggest to extend it to aligned comparable documents using document as the contex</context>
<context position="5959" citStr="Laroche and Langlais, 2010" startWordPosition="946" endWordPosition="949">ge corpus, matching the size of the French part. Using an implementation of the context-vector similarity, we show in figure 1 that frequent words (above 400 occurrences in the corpus) reach a 60% precision whereas rare words (below 15 occurrences) are correctly aligned in only 5% of the time. These results can be explained by the fact that, for the vector comparison to be efficient, the information they store has to be relevant and discriminatory. If there are not enough occurrences of a word, it is 1Detailed presentations can be found for example in (Fung, 2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010). Figure 1: Results for context-vector based translations extraction with respect to word frequency. The vertical axis is the amount of correct translations found for Top,, and the horizontal axis is the word occurrences in the corpus. impossible to get a precise description of the typical context of this word, and therefore its description is likely to be very different for source and target words in translation. We confirmed this result with another observation on the full English part of the previous corpus, randomly split in 14 samples of the same size. The context-vectors for very frequen</context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>Audrey Laroche and Philippe Langlais. 2010. Revisiting context-based projection methods for term-translation spotting in comparable corpora. In 23rd International Conference on Computational Linguistics (Coling 2010), pages 617–625, Beijing, China, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>B´eatrice Daille</author>
<author>Koichi Takeuchi</author>
<author>Kyo Kageura</author>
</authors>
<title>Bilingual Terminology Mining – Using Brain, not brawn comparable corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07),</booktitle>
<pages>664--671</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10830" citStr="Morin et al., 2007" startWordPosition="1731" endWordPosition="1734"> and cooccurrence counts to tackle this issue (such as the log-likelihood, eq. 2). In our case, the number of co-occurrences will be limited by the number of occurrences of the words, from 1 to 5. Therefore, the Jaccard similarity efficiently reflects what we want to observe. J(wi, wj) = |Ai ∩ Aj| |Ai ∪ Aj|&apos; Ai = {d : wi ∈ d} (1) A score of 1 indicates a perfect association (words always appear together, never one without the other), the more one word appears without the other, the lower the score. 4.2 Context-vector similarity We implemented the context-vector similarity in a way similar to (Morin et al., 2007). In all experiments, we used the same set of parameters, as they yielded the best results on our corpora. We built the context-vectors using nouns only as seed lexicon, with a window size of 20. Source context-vectors are translated in the target language using the resources presented in the next section. We used the log-likelihood (Dunning, 1993, eq. 2) for contextvector normalization (O is the observed number of co-occurrence in the corpus, E is the expected number of co-occurrences under the null hypothesis). We used the Cosine similarity (eq. 3) for contextvector comparisons. Oij Oijlog (</context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi, and Kyo Kageura. 2007. Bilingual Terminology Mining – Using Brain, not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07), pages 664– 671, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving Machine Translation Performance by Exploiting Non-Parallel Corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="7248" citStr="Munteanu and Marcu, 2005" startWordPosition="1153" endWordPosition="1156">each sample) are very similar across the subsets. Less frequent words, such as abnormality (between 70 and 16 occurrences in each sample) have very unstable context-vectors, hence a lower similarity across the subsets. This observation actually indicates that it will be difficult to align abnormality with itself. 3 Aligned comparable documents A pair of aligned comparable documents is a particular case of comparable corpus: two comparable documents share the same topic and domain; they both relate the same information but are not mutual translations; although they might share parallel chunks (Munteanu and Marcu, 2005) – paragraphs, sentences or phrases – in the general case they were written independently. These comparable documents, when concatenated together in order, form an aligned comparable corpus. 1328 Examples of such aligned documents can be found, for example in (Munteanu and Marcu, 2005): they aligned comparable documents with close publication dates. (Tao and Zhai, 2005) used an iterative, bootstrapping approach to align comparable documents using examples of already aligned corpora. (Smith et al., 2010) aligned documents from Wikipedia following the interlingual links provided on articles. We </context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving Machine Translation Performance by Exploiting Non-Parallel Corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Pekar</author>
<author>Ruslan Mitkov</author>
<author>Dimitar Blagoev</author>
<author>Andrea Mulloni</author>
</authors>
<title>Finding translations for lowfrequency words in comparable corpora.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="3887" citStr="Pekar et al., 2006" startWordPosition="614" endWordPosition="617">ents and how we exploited those documents for bilingual lexicon extraction in section 4. We present our resources and implementation in section 5 then carry out and comment several experiments in section 6. 1327 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1327–1335, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 The challenge of rare lexicon extraction There are few previous works focusing on the extraction of rare word translations, especially from comparable corpora. One of the earliest works is from (Pekar et al., 2006). They emphasized the fact that the context-vector based approach, used for processing comparable corpora, perform quite unreliably on all but the most frequent words. In a nutshell1, this approach proceeds by gathering the context of words in source and target languages inside context-vectors, then compares source and target context-vectors using similarity measures. In a monolingual context, such an approach is used to automatically get synonymy relationship between words to build thesaurus (Grefenstette, 1994). In the multilingual case, it is used to extract translations, that is, pairs of </context>
</contexts>
<marker>Pekar, Mitkov, Blagoev, Mulloni, 2006</marker>
<rawString>Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and Andrea Mulloni. 2006. Finding translations for lowfrequency words in comparable corpora. Machine Translation, 20(4):247–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>403--411</pages>
<contexts>
<context position="7756" citStr="Smith et al., 2010" startWordPosition="1232" endWordPosition="1235">formation but are not mutual translations; although they might share parallel chunks (Munteanu and Marcu, 2005) – paragraphs, sentences or phrases – in the general case they were written independently. These comparable documents, when concatenated together in order, form an aligned comparable corpus. 1328 Examples of such aligned documents can be found, for example in (Munteanu and Marcu, 2005): they aligned comparable documents with close publication dates. (Tao and Zhai, 2005) used an iterative, bootstrapping approach to align comparable documents using examples of already aligned corpora. (Smith et al., 2010) aligned documents from Wikipedia following the interlingual links provided on articles. We take advantage of this alignment between documents: by looking at what is common between two aligned documents and what is different in other documents, we obtain more precise information about terms than when using a larger comparable corpus without alignment. This is especially interesting in the case of rare lexicon as the classic context-vector similarity is not discriminatory enough and fails at raising interesting translation for rare words. 4 Rare word translations from aligned comparable documen</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403– 411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Tao</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Mining comparable bilingual text corpora for cross-language information integration.</title>
<date>2005</date>
<booktitle>In KDD ’05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,</booktitle>
<pages>691--696</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7620" citStr="Tao and Zhai, 2005" startWordPosition="1211" endWordPosition="1214">uments is a particular case of comparable corpus: two comparable documents share the same topic and domain; they both relate the same information but are not mutual translations; although they might share parallel chunks (Munteanu and Marcu, 2005) – paragraphs, sentences or phrases – in the general case they were written independently. These comparable documents, when concatenated together in order, form an aligned comparable corpus. 1328 Examples of such aligned documents can be found, for example in (Munteanu and Marcu, 2005): they aligned comparable documents with close publication dates. (Tao and Zhai, 2005) used an iterative, bootstrapping approach to align comparable documents using examples of already aligned corpora. (Smith et al., 2010) aligned documents from Wikipedia following the interlingual links provided on articles. We take advantage of this alignment between documents: by looking at what is common between two aligned documents and what is different in other documents, we obtain more precise information about terms than when using a larger comparable corpus without alignment. This is especially interesting in the case of rare lexicon as the classic context-vector similarity is not dis</context>
</contexts>
<marker>Tao, Zhai, 2005</marker>
<rawString>Tao Tao and ChengXiang Zhai. 2005. Mining comparable bilingual text corpora for cross-language information integration. In KDD ’05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 691–696, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Identification and resolution of Chinese zero pronouns: A machine learning approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12059" citStr="Zhao and Ng, 2007" startWordPosition="1939" endWordPosition="1942"> Cosine(A, B) = kAk2 + kBk2 − A · B (3) ll(wi, wj) = 2 � ij 1329 4.3 Binary classification of rare translations We suggest to incorporate both the context-vector similarity and the co-occurrence features in a machine learning approach. This approach consists of training a classifier on positive examples of translation pairs, and negative examples of non-translations pairs. The trained model (in our case, a decision tree) is then used to tag an unknown pair of words as either ”Translation” or ”Non-Translation”. One potential problem for building the training set, as pointed out for example by (Zhao and Ng, 2007) is this: we have a limited number of positive examples, but a very large amount of nontranslation examples as obviously is the case for rare word translations in any training corpus. Including two many negative examples in the training set would lead the classifier to label every pairs as ”Non-Translation”. To tackle this problem, (Zhao and Ng, 2007) tuned the imbalance of positive/negative ratio by resampling the positive examples in the training set. We chose to reduce the set of negative examples, and found that a ratio of five negative examples to one positive is optimal in our case. A lo</context>
</contexts>
<marker>Zhao, Ng, 2007</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2007. Identification and resolution of Chinese zero pronouns: A machine learning approach. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>