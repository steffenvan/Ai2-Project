<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.968087">
Rumor has it: Identifying Misinformation in Microblogs
</title>
<author confidence="0.993353">
Vahed Qazvinian Emily Rosengren Dragomir R. Radev Qiaozhu Mei
</author>
<affiliation confidence="0.998226">
University of Michigan
</affiliation>
<address confidence="0.960165">
Ann Arbor, MI
</address>
<email confidence="0.997279">
ivahed,emirose,radev,qmeil@umich.edu
</email>
<sectionHeader confidence="0.997366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999788961538461">
A rumor is commonly defined as a state-
ment whose true value is unverifiable. Ru-
mors may spread misinformation (false infor-
mation) or disinformation (deliberately false
information) on a network of people. Identi-
fying rumors is crucial in online social media
where large amounts of information are easily
spread across a large network by sources with
unverified authority. In this paper, we address
the problem of rumor detection in microblogs
and explore the effectiveness of 3 categories of
features: content-based, network-based, and
microblog-specific memes for correctly iden-
tifying rumors. Moreover, we show how these
features are also effective in identifying disin-
formers, users who endorse a rumor and fur-
ther help it to spread. We perform our exper-
iments on more than 10,000 manually anno-
tated tweets collected from Twitter and show
how our retrieval model achieves more than
0.95 in Mean Average Precision (MAP). Fi-
nally, we believe that our dataset is the first
large-scale dataset on rumor detection. It can
open new dimensions in analyzing online mis-
information and other aspects of microblog
conversations.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981243243243">
A rumor is an unverified and instrumentally relevant
statement of information spread among people (Di-
Fonzo and Bordia, 2007). Social psychologists ar-
gue that rumors arise in contexts of ambiguity, when
the meaning of a situation is not readily apparent,
or potential threat, when people feel an acute need
for security. For instance rumors about ‘office ren-
ovation in a company’ is an example of an ambigu-
ous context, and the rumor that ‘underarm deodor-
ants cause breast cancer’ is an example of a context
in which one’s well-being is at risk (DiFonzo et al.,
1994).
The rapid growth of online social media has made
it possible for rumors to spread more quickly. On-
line social media enable unreliable sources to spread
large amounts of unverified information among peo-
ple (Herman and Chomsky, 2002). Therefore, it is
crucial to design systems that automatically detect
misinformation and disinformation (the former of-
ten seen as simply false and the latter as deliberately
false information).
Our definition of a rumor is established based on
social psychology, where a rumor is defined as a
statement whose truth-value is unverifiable or delib-
erately false. In-depth rumor analysis such as deter-
mining the intent and impact behind the spread of
a rumor is a very challenging task and is not possi-
ble without first retrieving the complete set of social
conversations (e.g., tweets) that are actually about
the rumor. In our work, we take this first step to
retrieve a complete set of tweets that discuss a spe-
cific rumor. In our approach, we address two basic
problems. The first problem concerns retrieving on-
line microblogs that are rumor-related. In the second
problem, we try to identify tweets in which the ru-
mor is endorsed (the posters show that they believe
the rumor).
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999525">
We review related work on 3 main areas: Analyzing
rumors, mining microblogs, and sentiment analysis
and subjectivity detection.
</bodyText>
<subsectionHeader confidence="0.976385">
2.1 Rumor Identification and Analysis
</subsectionHeader>
<bodyText confidence="0.99239525">
Though understanding rumors has been the sub-
ject of research in psychology for some time (All-
port and Lepkin, 1945), (Allport and Postman,
1947), (DiFonzo and Bordia, 2007), research has
</bodyText>
<page confidence="0.962308">
1589
</page>
<note confidence="0.9577335">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589–1599,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999961333333333">
only recently begun to investigate how rumors are
manifested and spread differently online. Mi-
croblogging services, like Twitter, allow small
pieces of information to spread quickly to large au-
diences, allowing rumors to be created and spread in
new ways (Ratkiewicz et al., 2010).
Related research has used different methods to
study the spread of memes and false information
on the web. Leskovec et al. use the evolution
of quotes reproduced online to identify memes and
track their spread overtime (Leskovec et al., 2009).
Ratkiewicz et al. (Ratkiewicz et al., 2010) created
the “Truthy” system, identifying misleading politi-
cal memes on Twitter using tweet features, includ-
ing hashtags, links, and mentions. Other projects
focus on highlighting disputed claims on the Inter-
net using pattern matching techniques (Ennals et al.,
2010). Though our project builds on previous work,
our work differs in its general focus on identifying
rumors from a corpus of relevant phrases and our at-
tempts to further discriminate between phrases that
confirm, refute, question, and simply talk about ru-
mors of interest.
Mendoza et al. explore Twitter data to analyze the
behavior of Twitter users under the emergency situ-
ation of 2010 earthquake in Chile (Mendoza et al.,
). They analyze the re-tweet network topology and
find that the patterns of propagation in rumors dif-
fer from news because rumors tend to be questioned
more than news by the Twitter community.
</bodyText>
<subsectionHeader confidence="0.99916">
2.2 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999880814814815">
The automated detection of rumors is similar to tra-
ditional NLP sentiment analysis tasks. Previous
work has used machine learning techniques to iden-
tify positive and negative movie reviews (Pang et
al., 2002). Hassan et al. use a supervised Markov
model, part of speech, and dependency patterns to
identify attitudinal polarities in threads posted to
Usenet discussion posts (Hassan et al., 2010). Oth-
ers have designated sentiment scores for news sto-
ries and blog posts based on algorithmically gener-
ated lexicons of positive and negative words (God-
bole et al., 2007). Pang and Lee provide a detailed
overview of current techniques and practices in sen-
timent analysis and opinion mining (Pang and Lee,
2008; Pang and Lee, 2004).
Though rumor classification is closely related to
opinion mining and sentiment analysis, it presents
a different class of problem because we are con-
cerned not just with the opinion of the person post-
ing a tweet, but with whether the statements they
post appear controversial. The automatic identifica-
tion of rumors from a corpus is most closely related
to the identification of memes done in (Leskovec et
al., 2009), but presents new challenges since we seek
to highlight a certain type of recurring phrases. Our
work presents one of the first attempts at automatic
rumor analysis.
</bodyText>
<subsectionHeader confidence="0.999971">
2.3 Mining Twitter Data
</subsectionHeader>
<bodyText confidence="0.99995856">
With its nearly constant update of new posts and
public API, Twitter can be a useful source for
collecting data to be used in exploring a num-
ber of problems related to natural language pro-
cessing and information diffusion (Bifet and Frank,
2010). Pak and Paroubek demonstrated experimen-
tally that despite frequent occurrences of irregular
speech patterns in tweets, Twitter can provide a use-
ful corpus for sentiment analysis (Pak and Paroubek,
2010). The diversity of Twitter users make this
corpus especially valuable. Ratkiewicz et al also
use Twitter to detect and track misleading political
memes (Ratkiewicz et al., 2010).
Along with many advantages, using Twitter as a
corpus for sentiment analysis does present unusual
challenges. Because posts are limited to 140 charac-
ters, tweets often contain information in an unusu-
ally compressed form and, as a result, grammar used
may be unconventional. Instances of sarcasm and
humor are also prevalent (Bifet and Frank, 2010).
The procedures we used for the collection and anal-
ysis of tweets are similar to those described in previ-
ous work. However, our goal of developing compu-
tational methods to identify rumors being transmit-
ted through tweets differentiates our project.
</bodyText>
<sectionHeader confidence="0.993405" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.999948571428572">
Assume we have a set of tweets that are about the
same topic that has some controversial aspects. Our
objective in this work is two-fold: (1) Extract tweets
that are about the controversial aspects of the story
and spread misinformation (Rumor retrieval). (2)
Identify users who believe that misinformation ver-
sus users who refute or question the rumor (Belief
</bodyText>
<page confidence="0.980801">
1590
</page>
<table confidence="0.999387666666667">
Name Rumor Regular Expression Query Status #tweets
obama Is Barack Obama muslim? Obama &amp; (muslim|islam) false 4975
airfrance Air France mid-air crash photos? (air.france|air france) &amp; (photo|pic|pix) false 505
cellphone Cell phone numbers going public? (cell|cellphone|cell phone) mostly false 215
michelle Michelle Obama hired too many staff? staff &amp; (michelle obama|first lady|1st lady) partly true 299
palin Sarah Palin getting divorced? palin &amp; divorce false 4423
</table>
<tableCaption confidence="0.999903">
Table 1: List of rumor examples and their corresponding queries used to collect data from Twitter
</tableCaption>
<bodyText confidence="0.995845285714286">
classification).
The following two tweets are two instances of the
tweets written about president Obama and the Mus-
lim world. The first tweet below is about president
Obama and Muslim world, where the second tweet
spread misinformation that president Obama is Mus-
lim.
</bodyText>
<listItem confidence="0.339671571428571">
(non-rumor) “As Obama bows to Muslim leaders
Americans are less safe not only at home but also
overseas. Note: The terror alert in Europe... ”
(rumor) “RT @johnnyA99 Ann Coulter Tells Larry
King Why People Think Obama Is A Muslim
http://bit.ly/9rs6pa #Hussein via @NewsBusters
#tcot ..”
</listItem>
<bodyText confidence="0.9217655">
The goal of the retrieval task is to discriminate
between such tweets. In the second task, we use
the tweets that are flagged as rumorous, and identify
users that endorse (believe) the rumor versus users
who deny or question it. The following three tweets
are about the same story. The first user is a believer
and the second and third are not.
(confirm) “RT @moronwatch: Obama’s a Muslim. Or
if he’s not, he sure looks like one #whyimvotingre-
publican.”
(deny) “Barack Obama is a Christian man who had
a Christian wedding with 2 kids baptised in Jesus
name. Tea Party clowns call that muslim #p2 #gop”
(doubtful) “President Barack Obama’s Religion:
Christian, Muslim, or Agnostic? - The News
of Today (Google): Share With Friend...
http://bit.ly/bk42ZQ”
The first task is substantially more challenging
than a standard IR task because of the requirement of
both high precision (every result should be actually
discussing the rumor) and high recall (the set should
be complete). To do this, we submit a handcrafted
regexp (extracted from about.com) to Twitter and re-
trieve a large primitive set of tweets that is supposed
to have a high recall. This set however, contains a lot
of false positives, tweets that match the regexp but
are not about the rumor (e.g., “Obama meets muslim
leaders”). Moreover, a rumor is usually stated using
various instances (e.g., “Barack HUSSEIN Obama”
versus “Obama is muslim”). Our goal is then to de-
sign a learning framework that filters all such false
positives and retrieves various instances of the same
rumor
Although our second task, belief classification,
can be viewed as an opinion mining task, it is sub-
stantially different from opinion mining in nature.
The difference from a standard opinion mining task
is that here we are looking for attitudes about a sub-
tle statement (e.g., “Palin is getting divorce”) instead
of the overall sentiment of the text or the opinion
towards an explicit object or person (e.g., “Sarah
Palin”).
</bodyText>
<sectionHeader confidence="0.997721" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999968647058823">
As September 2010, Twitter reports that its users
publish nearly 95 million tweets per day1. This
makes Twitter an excellent case to analyze misin-
formation in social media.
Our goal in this work was to collect and annotate
a large dataset that includes all the tweets that are
written about a rumor in a certain period of time. To
collect such a complete and self-contained dataset
about a rumor, we used the Twitter search API, and
retrieved all the tweets that matched a given regular
expression. This API is the only API that returns re-
sults from the entire public Twitter stream and not
a small randomly selected sample. To overcome the
rate limit enforced by Twitter, we collected match-
ing tweets once per hour, and remove any duplicates.
To use the search API, we carefully designed reg-
ular expression queries to be broad enough to match
</bodyText>
<footnote confidence="0.98485">
1http://twitter.com/about
</footnote>
<page confidence="0.991561">
1591
</page>
<bodyText confidence="0.999968">
all the tweets that are about a rumor. Each query
represents a popular rumor that is listed as “false”
or only “partly true” on About.com’s Urban Leg-
ends reference site2 between 2009 and 2010. Table 1
lists the rumor examples that we used to collect our
dataset along with their corresponding regular ex-
pression queries and the number of tweets collected.
</bodyText>
<subsectionHeader confidence="0.99819">
4.1 Annotation
</subsectionHeader>
<bodyText confidence="0.9995992">
We asked two annotators to go over all the tweets
in the dataset and mark each tweet with a “1” if it
is about any of the rumors from Table 1, and with
a “0” otherwise. This annotation scheme will be
used in our first task to detect false positives, tweets
that match the broad regular expressions and are re-
trieved, but are not about the rumor. For instance,
both of the following tweets match the regular ex-
pression for the palin example, but only the sec-
ond one is rumorous.
</bodyText>
<listItem confidence="0.8019978">
(0) “McCain Divorces Palin over her ‘untruths and out
right lies’ in the book written for her. McCain’s
team says Palin is a petty liar and phony”
(1) “Sarah and Todd Palin to divorce, according to local
Alaska paper. http://ow.ly/iNxF”
</listItem>
<bodyText confidence="0.994469">
We also asked the annotators to mark each pre-
viously annotated rumorous tweet with “11” if the
tweet poster endorses the rumor and with “12” if the
user refutes the rumor, questions its credibility, or is
neutral.
</bodyText>
<listItem confidence="0.54396725">
(12) “Sarah Palin Divorce Rumor Debunked on Face-
book http://ff.im/62Evd”
(11) “Todd and Sarah Palin to divorce
http://bit.ly/15StNc”
</listItem>
<bodyText confidence="0.9998635">
Our annotation of more than 10,400 tweets shows
that %35 of all the instances that matched the regu-
lar expressions are false positives, tweets that are not
rumor-related but match the initial queries. More-
over, among tweets that are about particular ru-
mors, nearly %43 show the poster believe the rumor,
demonstrating the importance of identifying misin-
formation and those who are misinformed. Table 2
shows the basic statistics extracted from the annota-
tions for each story.
</bodyText>
<footnote confidence="0.985357">
2http://urbanlegends.about.com
</footnote>
<table confidence="0.999111375">
Rumor non-rumor (0) believe (11) deny/ (12) total
doubtful/neutral
obama 3,036 926 1,013 4975
airfrance 306 71 128 505
cellphone 132 74 9 215
michelle 83 191 25 299
palin 86 1,709 2,628 4,423
total 3,643 2,971 3,803 10,417
</table>
<tableCaption confidence="0.993394">
Table 2: Number of instances in each class from the an-
notated data
</tableCaption>
<table confidence="0.982365333333333">
task r.
rumor retrieval 0.954
belief classification 0.853
</table>
<tableCaption confidence="0.9895075">
Table 3: Inter-judge agreement in two annotation tasks in
terms of r.-statistic
</tableCaption>
<subsectionHeader confidence="0.870776">
4.2 Inter-Judge Agreement
</subsectionHeader>
<bodyText confidence="0.99992075">
To calculate the annotation accuracy, we annotated
500 instances twice. These annotations were com-
pared with each other, and the Kappa coefficient (r.)
was calculated. The r. statistic is formulated as
</bodyText>
<equation confidence="0.996128">
Pr(a) − Pr(e)
1 − Pr(e)
</equation>
<bodyText confidence="0.999859571428572">
where Pr(a) is the relative observed agreement
among raters, and Pr(e) is the probability that anno-
tators agree by chance if each annotator is randomly
assigning categories (Krippendorff, 1980; Carletta,
1996). Table 3 shows that annotators can reach
a high agreement in both extracting rumors (r. =
0.95) and identifying believers (r. = 0.85).
</bodyText>
<sectionHeader confidence="0.995583" genericHeader="method">
5 Approach
</sectionHeader>
<bodyText confidence="0.999978333333333">
In this section, we describe a general framework,
which given a tweet, predicts (1) whether it is a
rumor-related statement, and if so (2) whether the
user believes the rumor or not. We describe 3 sets of
features, and explain why these are intuitive to use
for identification of rumors.
We process the tweets as they appear in the user
timeline, and do not perform any pre-processing.
Specially, we think that capitalization might be an
important property. So, we do not lower-case the
tweet texts either.
Our approach is based on building different Bayes
classifiers as high level features and then learning
a linear function of these classifiers for retrieval in
the first task and classification in the second. Each
</bodyText>
<equation confidence="0.915331">
r. =
</equation>
<page confidence="0.85524">
1592
</page>
<bodyText confidence="0.987390333333333">
Bayes classifier, which corresponds to a feature fi,
calculates the likelihood ratio for a given tweet t, as
shown in Equation 1.
</bodyText>
<equation confidence="0.996604666666667">
P(t|0+i )
(1)
P(t|0− i )
</equation>
<bodyText confidence="0.999900333333333">
Here 0+ iand 0−i are two probabilistic models built
based on feature fi using a set of positive (+) and
negative (−) training data. The likelihood ratio ex-
presses how many times more likely the tweet t is
under the positive model than the negative model
with respect to fi.
For computational reasons and to avoid dealing
with very small numbers we use the log of the like-
lihood ratio to build each classifier.
</bodyText>
<equation confidence="0.999406">
P(0+It)
LLi = log i
P(0−i |t)
</equation>
<bodyText confidence="0.967390142857143">
The first term PP(θ) can be easily calculated us-
θ−i
ing the maximum likelihood estimates of the prob-
abilities (i.e., the estimate of each probability is the
corresponding relative frequency). The second term
is calculated using various features that we explain
below.
</bodyText>
<subsectionHeader confidence="0.956887">
5.1 Content-based Features
</subsectionHeader>
<bodyText confidence="0.99999625">
The first set of features are extracted from the text of
the tweets. We propose 4 content based features. We
follow (Hassan et al., 2010) and present the tweet
with 2 different patterns:
</bodyText>
<listItem confidence="0.906116">
• Lexical patterns: All the words and segments
in the tweet are represented as they appear and
are tokenized using the space character.
• Part-of-speech patterns: All words are replaced
with their part-of-speech tags. To find the part-
of-speech of a hashtag we treat it as a word
(since they could have semantic roles in the
sentence), by omitting the tag sign, and then
precede the tag with the label TAG/. We also
introduce a new tag, URL, for URLs that appear
in a tweet.
</listItem>
<bodyText confidence="0.9991526">
From each tweet we extract 4 (2 x 2) features,
corresponding to unigrams and bigrams of each rep-
resentation. Each feature is the log-likelihood ra-
tio calculated using Equation 2. More formally,
we represent each tweet t, of length n, lexically
as (w1w2 • • • wn) and with part-of-speech tags as
(p1p2 • • • pn). After building the positive and nega-
tive models (0+, 0−) for each feature using the train-
ing data, we calculate the likelihood ratio as defined
in Equation 2 where
</bodyText>
<equation confidence="0.9991475">
lo P(wjwj+1|0+) (4)
g P(wjwj+1|0−)
</equation>
<bodyText confidence="0.9998116">
for bigram-based lexical features (TXT2). Simi-
larly, we define the unigram and bigram-based part-
of-speech features (POS1 and POS2) as the log-
likelihood ratio with respect to the positive and neg-
ative part-of-speech models.
</bodyText>
<subsectionHeader confidence="0.97731">
5.2 Network-based Features
</subsectionHeader>
<bodyText confidence="0.999873619047619">
The features that we have proposed so far are all
based on the content of individual tweets. In the
second set of features we focus on user behavior on
Twitter. We observe 4 types of network-based prop-
erties, and build 2 features that capture them.
Twitter enables users to re-tweet messages from
other people. This interaction is usually easy to de-
tect because the re-tweeted messages generally start
with the specific pattern: ‘RT @user’. We use this
property to infer about the re-tweeted message.
Let’s suppose a user ui re-tweets a message t from
the user uj (ui: “RT @uj t”). Intuitively, t is more
likely to be a rumor if (1) uj has a history of posting
or re-tweeting rumors, or (2) ui has posted or re-
tweeted rumors in the past.
Given a set of training instances, we build a pos-
itive (0+) and a negative (0−) user models. The
first model is a probability distribution over all users
that have posted a positive instance or have been re-
tweeted in a positive instance. Similarly, the sec-
ond model is a probability distribution over users
</bodyText>
<equation confidence="0.998893583333333">
P(0+i |t) P(0+i )
P(0−i |t) P(0−i )
P (0+ i ) P (t|0+ i )
= log P (0− i ) + log P (t|0− i ) (2)
P(t|0+) n P(wj|0+)
P(t|0−) j=1 log (3)
P(wj|0−)
for unigram-lexical features (TXT1) and
P(t|0+)
P(t|0−)
n−1�
j=1
</equation>
<page confidence="0.81201">
1593
</page>
<bodyText confidence="0.885253523809524">
that have posted (or been re-tweeted in) a negative
instance. After building the models, for a given
tweet we calculate two log-likelihood ratios as two
network-based features.
The first feature is the log-likelihood ratio that ui
is under a positive user model (USR1) and the sec-
ond feature is the log-likelihood ratio that the tweet
is re-tweeted from a user (uj) who is under a positive
user model than a negative user model (USR2).
The distinction between the posting user and the
re-tweeted user is important, since some times the
users modify the re-tweeted message in a way that
changes its meaning and intent. In the following ex-
ample, the original user is quoting president Obama.
The second user is re-tweeting the first user, but has
added more content to the tweet and made it sound
rumorous.
original message (non-rumor) “Obama says he’s do-
ing ‘Christ’s work’.”
re-tweeted (rumor) “Obama says he’s doing ‘Christ’s
work.’ Oh my God, CHRIST IS A MUSLIM.”
</bodyText>
<subsectionHeader confidence="0.995711">
5.3 Twitter Specific Memes
</subsectionHeader>
<bodyText confidence="0.9996355">
Our final set of features are extracted from memes
that are specific to Twitter: hashtags and URLs.
Previous work has shown the usefulness of these
memes (Ratkiewicz et al., 2010).
</bodyText>
<subsectionHeader confidence="0.508948">
5.3.1 Hashtags
</subsectionHeader>
<bodyText confidence="0.999565111111111">
One emergent phenomenon in the Twitter ecosys-
tem is the use of hashtags: words or phrases prefixed
with a hash symbol (#). These hashtags are created
by users, and are widely used for a few days, then
disappear when the topic is outdated (Huang et al.,
2010).
In our approach, we investigate whether hashtags
used in rumor-related tweets are different from other
tweets. Moreover, we examine whether people who
believe and spread rumors use hashtags that are dif-
ferent from those seen in tweets that deny or ques-
tion a rumor.
Given a set of training tweets of positive and neg-
ative examples, we build two statistical models (0+,
0−), each showing the usage probability distribution
of various hashtags. For a given tweet, t, with a set
of m hashtags (#h1 · · · #hm), we calculate the log-
likelihood ratio using Equation 2 where
</bodyText>
<table confidence="0.9690066">
Feature LL-ratio model
Content TXT1 content unigram content unigram
TXT2 content bigram content unigram
POS1 content pos content pos unigram
POS2 content pos content pos bigram
URL1 content unigram target URL unigram
Twitter URL2 content bigram target URL bigram
TAG hashtag hashtag
Network USR1 tweeting user all users in the data
USR2 re-tweeted user all users in the data
</table>
<tableCaption confidence="0.974360333333333">
Table 4: List of features used in our optimization frame-
work. Each feature is a log-likelihood ratio calculated
against a a positive (+) and negative (−) training models.
</tableCaption>
<subsectionHeader confidence="0.34949">
5.3.2 URLs
</subsectionHeader>
<bodyText confidence="0.999878083333333">
Previous work has discussed the role of URLs
in information diffusion on Twitter (Honeycutt and
Herring, 2009). Twitter users share URLs in their
tweets to refer to external sources or overcome the
length limit forced by Twitter. Intuitively, if a tweet
is a positive instance, then it is likely to be similar to
the content of URLs shared by other positive tweets.
Using the same reasoning, if a tweet is a negative
instance, then it should be more similar to the web
pages shared by other negative instances.
Given a set of training tweets, we fetch all the
URLs in these tweets and build 0+ and 0− once for
unigrams and once for bigrams. These models are
merely built on the content of the URLs and ignore
the tweet content. Similar to previous features, we
calculate the log-likelihood ratio of the content of
each tweet with respect to 0+ and 0− for unigrams
(URL1) and bigrams URL2).
Table 4 summarizes the set of features used in our
proposed framework, where each feature is a log-
likelihood ratio calculated against a positive (+) and
negative (−) training models. To build these lan-
guage models, we use the CMU Language Modeling
toolkit (Clarkson and Rosenfeld, 1997).
</bodyText>
<subsectionHeader confidence="0.947377">
5.4 Optimization
</subsectionHeader>
<bodyText confidence="0.999929">
We build an L1-regularized log-linear model (An-
drew and Gao, 2007) on various features discussed
before to predict each tweet. Suppose, a procedure
generates a set of candidates for an input x. Also,
</bodyText>
<equation confidence="0.999420666666667">
P(t|0+) �m P(#hj|0+)
P(t|0−) j=1 log (5)
P(#hj|0−)
</equation>
<page confidence="0.946059">
1594
</page>
<bodyText confidence="0.995217333333333">
let’s suppose Φ : X x Y -+ RD is a function that
maps each (x, y) to a vector of feature values. Here,
the feature vector is the vector of coefficients corre-
sponding to different network, content, and twitter-
based properties, and the parameter vector B E RD
(D G 9 in our experiments) assigns a real-valued
weight to each feature. This estimator chooses B to
minimize the sum of least squares and a regulariza-
tion term R.
</bodyText>
<equation confidence="0.987069">
Bˆ = arg min {1 �
θ 2 i ||(B, xi) − yi||22 + R(B)} (6)
</equation>
<bodyText confidence="0.999718">
where the regularizer term R(B) is the weighted L1
norm of the parameters.
</bodyText>
<equation confidence="0.971152">
R(B) = α � |Bj |(7)
j
</equation>
<bodyText confidence="0.99893652173913">
Here, α is a parameter that controls the amount of
regularization (set to 0.1 in our experiments).
Gao et. al (Gao et al., 2007) argue that op-
timizing L1-regularized objective function is chal-
lenging since its gradient is discontinuous whenever
some parameters equal zero. In this work, we use
the orthant-wise limited-memory quasi-Newton al-
gorithm (OWL-QN), which is a modification of L-
BFGS that allows it to effectively handle the dis-
continuity of the gradient (Andrew and Gao, 2007).
OWL-QN is based on the fact that when restricted
to a single orthant, the L1 regularizer is differen-
tiable, and is in fact a linear function of B. Thus,
as long as each coordinate of any two consecutive
search points does not pass through zero R(B) does
not contribute at all to the curvature of the function
on the segment joining them. Therefore, we can use
L-BFGS to approximate the Hessian of L(B) alone
and use it to build an approximation to the full reg-
ularized objective that is valid on a given orthant.
This algorithm works quite well in practice, and typ-
ically reaches convergence in even fewer iterations
than standard L-BFGS (Gao et al., 2007).
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999944">
We design 2 sets of experiments to evaluate our ap-
proach. In the first experiment we assess the effec-
tiveness of the proposed method when employed in
an Information Retrieval (IR) framework for rumor
retrieval and in the second experiment we employ
various features to detect users’ beliefs in rumors.
</bodyText>
<subsectionHeader confidence="0.998821">
6.1 Rumor Retrieval
</subsectionHeader>
<bodyText confidence="0.9999124">
In this experiment, we view different stories as
queries, and build a relevance set for each query.
Each relevance set is an annotation of the entire
10,417 tweets, where each tweet is marked as rel-
evant if it matches the regular expression query and
is marked as a rumor-related tweet by the annotators.
For instance, according to Table 2 the cellphone
dataset has only 83 relevant documents out of the
entire 10,417 documents.
For each query we use 5-fold cross-validation,
and predict the relevance of tweets as a function of
their features. We use these predictions and rank
all the tweets with respect to the query. To evalu-
ate the performance of our ranking model for a sin-
gle query (Q) with the set of relevant documents
</bodyText>
<equation confidence="0.9536665">
{d1, · · · , dm}, we calculate Average Precision as
Precision(Rk) (8)
</equation>
<bodyText confidence="0.998205666666667">
where Rk is the set of ranked retrieval results from
the top result to the kth relevant document, dk (Man-
ning et al., 2008).
</bodyText>
<subsectionHeader confidence="0.944099">
6.1.1 Baselines
</subsectionHeader>
<bodyText confidence="0.99979619047619">
We compare our proposed ranking model with a
number of other retrieval models. The first two sim-
ple baselines that indicate a difficulty lower-bound
for the problem are Random and Uniform meth-
ods. In the Random baseline, documents are ranked
based on a random number assignment to them. In
the Uniform model, we use a 5-fold cross validation,
and in each fold the label of the test documents is de-
termined by the majority vote from the training set.
The main baseline that we use in this work, is the
regular expression that was submitted to Twitter to
collect data (regexp). Using the same regular ex-
pression to mark the relevance of the documents will
cause a recall value of 1.00 (since it will retrieve all
the relevant documents), but will also retrieve false
positives, tweets that match the regular expression
but are not rumor-related. We would like to inves-
tigate whether using training data will help us de-
crease the rate of false positives in retrieval.
Finally, using the Lemur Toolkit software3, we
employ a KL divergence retrieval model with
</bodyText>
<equation confidence="0.885460666666667">
3http://www.lemurproject.org/
1
AP(Q) = m
m
L
k=1
</equation>
<page confidence="0.959703">
1595
</page>
<bodyText confidence="0.999825">
Dirichlet smoothing (KL). In this model, documents
are ranked according to the negation of the diver-
gence of query and document language models.
More formally, given the query language model θQ,
and the document language model θD, the docu-
ments are ranked by −D(θQ||θD), where D is the
KL-divergence between the two models.
</bodyText>
<equation confidence="0.930983333333333">
� p(w|θ ) log P(w |θQ) (9)
D(θQ||θD) = Q p(w|θD)
w
</equation>
<bodyText confidence="0.778159">
To estimate p(w|θD), we use Bayesian smoothing
with Dirichlet priors (Berger, 1985).
</bodyText>
<equation confidence="0.999797">
C(w, D) + µ.p(w|θS)
ps(w|θD) = (10)
µ + Ew C(w,D)
</equation>
<bodyText confidence="0.9998535">
where, µ is a parameter, C is the count function, and
thetaS is the collection language model. Higher val-
ues of µ put more emphasis on the collection model.
Here, we try two variants of the model, one using
the default parameter value in Lemur (µ = 2000),
and one in which µ is tuned based on the the data
(µ = 10). Using the test data to tune the parameter
value, µ, will help us find an upper-bound estimate
of the effectiveness of this method.
Table 5 shows the Mean Average Precision
(MAP) and Fa—i for each method in the rumor re-
trieval task. This table shows that a method that
employs training data to re-rank documents with
respect to rumors makes significant improvements
over the baselines and outperforms other strong re-
trieval systems.
</bodyText>
<subsectionHeader confidence="0.975211">
6.1.2 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.998411571428572">
To investigate the effectiveness of using indi-
vidual features in retrieving rumors, we perform
5-fold cross validations for each query, using
different feature sets each time. Figure 1 shows
the average precision and recall for our pro-
posed optimization system when content-based
(TXT1+TXT2+POS1+POS2), network-based
(USR1+USR2), and twitter specific memes
(TAG+URL1+URL2) are employed individually.
Figure 1 shows that features that are calculated us-
ing the content language models are very effective in
achieving high precision and recall. Twitter specific
features, especially hashtags, can result in high pre-
cisions but lead to a low recall value because many
</bodyText>
<figureCaption confidence="0.995089333333333">
Figure 1: Average precision and recall of the proposed
method employing each set of features: content-based,
network-based, and twitter specific.
</figureCaption>
<bodyText confidence="0.998181">
tweets do not share hashtags or are not written based
on the contents of external URLs.
Finally, we find that user history can be a good
indicator of rumors. However, we believe that this
feature could be more helpful with a complete user
set and a more comprehensive history of their activ-
ities.
</bodyText>
<subsectionHeader confidence="0.870186">
6.1.3 Domain Training Data
</subsectionHeader>
<bodyText confidence="0.999995210526316">
As our last experiment with rumor retrieval we in-
vestigate how much new labeled data from an emer-
gent rumor is required to effectively retrieve in-
stances of that particular rumor. This experiment
helps us understand how our proposed framework
could be generalized to other stories.
To do this experiment, we use the obama story,
which is a large dataset with a significant number of
false positive instances. We extract 400 randomly
selected tweets from this dataset and keep them for
testing. We also build an initial training dataset of
the other 4 rumors, and label them as not relevant.
We assess the performance of the retrieval model as
we gradually add the rest of the obama tweets. Fig-
ure 2 shows both Average Precision and labeling ac-
curacy versus the size of the labeled data used from
the obama dataset. This plot shows that both mea-
sures exhibit a fast growth and reach 80% when the
number of labeled data reaches 2000.
</bodyText>
<subsectionHeader confidence="0.996648">
6.2 Belief Classification
</subsectionHeader>
<bodyText confidence="0.999661">
In previous experiments we showed that maximiz-
ing a linear function of log-likelihood ratios is an
effective method in retrieving rumors. Here, we in-
</bodyText>
<page confidence="0.963058">
1596
</page>
<table confidence="0.999602">
Method MAP 95% C.I. Fβ=1 95% C.I.
Random 0.129 [-0.065, 0.323] 0.164 [-0.051, 0.379]
Uniform 0.129 [-0.066, 0.324] 0.198 [-0.080, 0.476]
regexp 0.587 [0.305, 0.869] 0.702 [0.479, 0.925]
KL (µ = 2000) 0.678 [0.458, 0.898] 0.538 [0.248, 0.828]
KL (µ = 10) 0.803 [0.641, 0.965] 0.681 [0.614, 0.748]
LL (all 9 features) 0.965 [0.936, 0.994] 0.897 [0.828, 0.966]
</table>
<tableCaption confidence="0.9248535">
Table 5: Mean Average Precision (MAP) and Fβ=1 of each method in the rumor retrieval task. (C.I.: Confidence
Interval)
</tableCaption>
<table confidence="0.999786454545454">
Method Accuracy Precision Recall Fβ=1 Win/Loss Ratio
random 0.501 0.441 0.513 0.474 1.004
uniform 0.439 0.439 1.000 0.610 0.781
TXT 0.934 0.925 0.924 0.924 14.087
POS 0.742 0.706 0.706 0.706 2.873
content (TXT+POS) 0.941 0.934 0.930 0.932 15.892
network (USR) 0.848 0.873 0.765 0.815 5.583
TAG 0.589 0.734 0.099 0.175 1.434
URL 0.664 0.630 0.570 0.598 1.978
twitter (TAG+URL) 0.683 0.658 0.579 0.616 2.155
all 0.935 0.944 0.906 0.925 14.395
</table>
<tableCaption confidence="0.99914">
Table 6: Accuracy, precision, recall, Fβ=1, and win/loss ratio of belief classification using different features.
</tableCaption>
<figureCaption confidence="0.9971895">
Figure 2: Average Precision and Accuracy learning curve
for the proposed method employing all 9 features.
</figureCaption>
<bodyText confidence="0.999979043478261">
vestigate whether this method, and in particular, the
proposed features are useful in detecting users’ be-
liefs in a rumor that they post about. Unlike re-
trieval, detecting whether a user endorses a rumor or
refutes it may be possible using similar methods re-
gardless of the rumor. Intuitively, linguistic features
such as negation (e.g., “obama is not a muslim”), or
capitalization (e.g., “barack HUSSEIN obama ...”),
user history (e.g., liberal tweeter vs. conservative
tweeter), hashtags (e.g., #tcot vs. #tdot), and URLs
(e.g., links to fake airfrance crash photos) should
help to identify endorsements.
We perform this experiment by making a pool
of all the tweets that are marked as “rumorous” in
the annotation task. Table 2 shows that there are
6,774 such tweets, from which 2,971 show belief
and 3,803 tweets show that the user is doubtful, de-
nies, or questions it.
Using various feature settings, we perform 5-fold
cross-validation on these 6,774 rumorous tweets.
Table 6 shows the results of this experiment in terms
of F-score, classification accuracy, and win/loss ra-
tio, the ratio of correct classification to an incorrect
</bodyText>
<page confidence="0.980567">
1597
</page>
<bodyText confidence="0.858847">
classification.
</bodyText>
<sectionHeader confidence="0.997821" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999837321428572">
In this paper we tackle the fairly unaddressed prob-
lem of identifying misinformation and disinform-
ers in Microblogs. Our contributions in this pa-
per are two-fold: (1) We propose a general frame-
work that employs statistical models and maximizes
a linear function of log-likelihood ratios to retrieve
rumorous tweets that match a more general query.
(2) We show the effectiveness of the proposed fea-
ture in capturing tweets that show user endorsement.
This will help us identify disinformers or users that
spread false information in online social media.
Our work has resulted in a manually annotated
dataset of 10,000 tweets from 5 different controver-
sial topics. To the knowledge of authors this is the
first large-scale publicly available rumor dataset, and
can open many new dimensions in studying the ef-
fects of misinformation or other aspects of informa-
tion diffusion in online social media.
In this paper we effectively retrieve instances of
rumors that are already identified and evaluated by
an external source such as About.com’s Urban Leg-
ends reference. Identifying new emergent rumors
directly from the Twitter data is a more challenging
task. As our future work, we aim to build a sys-
tem that employs our findings in this paper and the
emergent patterns in the re-tweet network topology
to identify whether a new trending topic is a rumor
or not.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999717111111111">
The authors would like to thank Paul Resnick,
Rahul Sami, and Brendan Nyhan for helpful discus-
sions. This work is supported by the National Sci-
ence Foundation grant “SoCS: Assessing Informa-
tion Credibility Without Authoritative Sources” as
IIS-0968489. Any opinions, findings, and conclu-
sions or recommendations expressed in this paper
are those of the authors and do not necessarily re-
flect the views of the supporters.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999676945454545">
Floyd H. Allport and Milton Lepkin. 1945. Wartime ru-
mors of waste and special privilege: why some people
believe them. Journal of Abnormal and Social Psy-
chology, 40(1):3 – 36.
Gordon Allport and Leo Postman. 1947. The psychology
of rumor. Holt, Rinehart, and Winston, New York.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In ICML ’07,
pages 33–40.
James Berger. 1985. Statistical decision theory and
Bayesian Analysis (2nd ed.). New York: Springer-
Verlag.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Bernhard
Pfahringer, Geoff Holmes, and Achim Hoffmann, edi-
tors, Discovery Science, volume 6332 of Lecture Notes
in Computer Science, pages 1–15. Springer Berlin /
Heidelberg.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249–254.
Philip Clarkson and Roni Rosenfeld. 1997. Statistical
language modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45–148.
Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip, and urban legend. Diogenes, 54:19–35, Febru-
ary.
Nicholas DiFonzo, P. Prashant Bordia, and Ralph L. Ros-
now. 1994. Reining in rumors. Organizational Dy-
namics, 23(1):47–62.
Rob Ennals, Dan Byler, John Mark Agosta, and Barbara
Rosario. 2010. What is disputed on the web? In Pro-
ceedings of the 4th workshop on Information Credibil-
ity, WICOW ’10, pages 67–74.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In ACL ’07.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM),
Boulder, CO, USA.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What’s with the attitude? identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245–1255, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Edward S Herman and Noam Chomsky. 2002. Manu-
facturing Consent: The Political Economy of the Mass
Media. Pantheon.
Courtenay Honeycutt and Susan C. Herring. 2009. Be-
yond microblogging: Conversation and collaboration
</reference>
<page confidence="0.822895">
1598
</page>
<reference confidence="0.999567555555556">
via twitter. Hawaii International Conference on Sys-
tem Sciences, 0:1–10.
Jeff Huang, Katherine M. Thornton, and Efthimis N.
Efthimiadis. 2010. Conversational tagging in twitter.
In Proceedings of the 21st ACM conference on Hyper-
text and hypermedia, HT ’10, pages 173–178.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Beverly Hills: Sage Pub-
lications.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD ’09: Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 497–506.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.
Twitter under crisis: Can we trust what we rt?
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL’04, Morristown,
NJ, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2:1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of confer-
ence on Empirical methods in natural language pro-
cessing, EMNLP’02, pages 79–86.
Jacob Ratkiewicz, Michael Conover, Mark Meiss, Bruno
Gonc¸alves, Snehal Patil, Alessandro Flammini, and
Filippo Menczer. 2010. Detecting and tracking
the spread of astroturf memes in microblog streams.
CoRR, abs/1011.3768.
</reference>
<page confidence="0.99744">
1599
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.464359">
<title confidence="0.996096">Rumor has it: Identifying Misinformation in Microblogs</title>
<author confidence="0.977859">Vahed Qazvinian Emily Rosengren Dragomir R Radev Qiaozhu</author>
<affiliation confidence="0.965158">University of</affiliation>
<author confidence="0.505176">Ann Arbor</author>
<abstract confidence="0.998249925925926">A rumor is commonly defined as a statement whose true value is unverifiable. Rumors may spread misinformation (false information) or disinformation (deliberately false information) on a network of people. Identifying rumors is crucial in online social media where large amounts of information are easily spread across a large network by sources with unverified authority. In this paper, we address the problem of rumor detection in microblogs and explore the effectiveness of 3 categories of features: content-based, network-based, and microblog-specific memes for correctly identifying rumors. Moreover, we show how these features are also effective in identifying disinformers, users who endorse a rumor and further help it to spread. We perform our experiments on more than 10,000 manually annotated tweets collected from Twitter and show how our retrieval model achieves more than Mean Average Precision (MAP). Finally, we believe that our dataset is the first large-scale dataset on rumor detection. It can open new dimensions in analyzing online misinformation and other aspects of microblog conversations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Floyd H Allport</author>
<author>Milton Lepkin</author>
</authors>
<title>Wartime rumors of waste and special privilege: why some people believe them.</title>
<date>1945</date>
<journal>Journal of Abnormal and Social Psychology,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="3410" citStr="Allport and Lepkin, 1945" startWordPosition="540" endWordPosition="544">step to retrieve a complete set of tweets that discuss a specific rumor. In our approach, we address two basic problems. The first problem concerns retrieving online microblogs that are rumor-related. In the second problem, we try to identify tweets in which the rumor is endorsed (the posters show that they believe the rumor). 2 Related Work We review related work on 3 main areas: Analyzing rumors, mining microblogs, and sentiment analysis and subjectivity detection. 2.1 Rumor Identification and Analysis Though understanding rumors has been the subject of research in psychology for some time (Allport and Lepkin, 1945), (Allport and Postman, 1947), (DiFonzo and Bordia, 2007), research has 1589 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589–1599, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics only recently begun to investigate how rumors are manifested and spread differently online. Microblogging services, like Twitter, allow small pieces of information to spread quickly to large audiences, allowing rumors to be created and spread in new ways (Ratkiewicz et al., 2010). Related research has used different methods </context>
</contexts>
<marker>Allport, Lepkin, 1945</marker>
<rawString>Floyd H. Allport and Milton Lepkin. 1945. Wartime rumors of waste and special privilege: why some people believe them. Journal of Abnormal and Social Psychology, 40(1):3 – 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon Allport</author>
<author>Leo Postman</author>
</authors>
<title>The psychology of rumor.</title>
<date>1947</date>
<location>Holt, Rinehart, and Winston, New York.</location>
<contexts>
<context position="3439" citStr="Allport and Postman, 1947" startWordPosition="545" endWordPosition="548">set of tweets that discuss a specific rumor. In our approach, we address two basic problems. The first problem concerns retrieving online microblogs that are rumor-related. In the second problem, we try to identify tweets in which the rumor is endorsed (the posters show that they believe the rumor). 2 Related Work We review related work on 3 main areas: Analyzing rumors, mining microblogs, and sentiment analysis and subjectivity detection. 2.1 Rumor Identification and Analysis Though understanding rumors has been the subject of research in psychology for some time (Allport and Lepkin, 1945), (Allport and Postman, 1947), (DiFonzo and Bordia, 2007), research has 1589 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589–1599, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics only recently begun to investigate how rumors are manifested and spread differently online. Microblogging services, like Twitter, allow small pieces of information to spread quickly to large audiences, allowing rumors to be created and spread in new ways (Ratkiewicz et al., 2010). Related research has used different methods to study the spread of memes </context>
</contexts>
<marker>Allport, Postman, 1947</marker>
<rawString>Gordon Allport and Leo Postman. 1947. The psychology of rumor. Holt, Rinehart, and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable training of l1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In ICML ’07,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="23189" citStr="Andrew and Gao, 2007" startWordPosition="3838" endWordPosition="3842"> These models are merely built on the content of the URLs and ignore the tweet content. Similar to previous features, we calculate the log-likelihood ratio of the content of each tweet with respect to 0+ and 0− for unigrams (URL1) and bigrams URL2). Table 4 summarizes the set of features used in our proposed framework, where each feature is a loglikelihood ratio calculated against a positive (+) and negative (−) training models. To build these language models, we use the CMU Language Modeling toolkit (Clarkson and Rosenfeld, 1997). 5.4 Optimization We build an L1-regularized log-linear model (Andrew and Gao, 2007) on various features discussed before to predict each tweet. Suppose, a procedure generates a set of candidates for an input x. Also, P(t|0+) �m P(#hj|0+) P(t|0−) j=1 log (5) P(#hj|0−) 1594 let’s suppose Φ : X x Y -+ RD is a function that maps each (x, y) to a vector of feature values. Here, the feature vector is the vector of coefficients corresponding to different network, content, and twitterbased properties, and the parameter vector B E RD (D G 9 in our experiments) assigns a real-valued weight to each feature. This estimator chooses B to minimize the sum of least squares and a regularizat</context>
<context position="24438" citStr="Andrew and Gao, 2007" startWordPosition="4064" endWordPosition="4067">� θ 2 i ||(B, xi) − yi||22 + R(B)} (6) where the regularizer term R(B) is the weighted L1 norm of the parameters. R(B) = α � |Bj |(7) j Here, α is a parameter that controls the amount of regularization (set to 0.1 in our experiments). Gao et. al (Gao et al., 2007) argue that optimizing L1-regularized objective function is challenging since its gradient is discontinuous whenever some parameters equal zero. In this work, we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of LBFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao, 2007). OWL-QN is based on the fact that when restricted to a single orthant, the L1 regularizer is differentiable, and is in fact a linear function of B. Thus, as long as each coordinate of any two consecutive search points does not pass through zero R(B) does not contribute at all to the curvature of the function on the segment joining them. Therefore, we can use L-BFGS to approximate the Hessian of L(B) alone and use it to build an approximation to the full regularized objective that is valid on a given orthant. This algorithm works quite well in practice, and typically reaches convergence in eve</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable training of l1-regularized log-linear models. In ICML ’07, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Berger</author>
</authors>
<title>Statistical decision theory and Bayesian Analysis (2nd ed.).</title>
<date>1985</date>
<publisher>SpringerVerlag.</publisher>
<location>New York:</location>
<contexts>
<context position="27942" citStr="Berger, 1985" startWordPosition="4671" endWordPosition="4672">trieval. Finally, using the Lemur Toolkit software3, we employ a KL divergence retrieval model with 3http://www.lemurproject.org/ 1 AP(Q) = m m L k=1 1595 Dirichlet smoothing (KL). In this model, documents are ranked according to the negation of the divergence of query and document language models. More formally, given the query language model θQ, and the document language model θD, the documents are ranked by −D(θQ||θD), where D is the KL-divergence between the two models. � p(w|θ ) log P(w |θQ) (9) D(θQ||θD) = Q p(w|θD) w To estimate p(w|θD), we use Bayesian smoothing with Dirichlet priors (Berger, 1985). C(w, D) + µ.p(w|θS) ps(w|θD) = (10) µ + Ew C(w,D) where, µ is a parameter, C is the count function, and thetaS is the collection language model. Higher values of µ put more emphasis on the collection model. Here, we try two variants of the model, one using the default parameter value in Lemur (µ = 2000), and one in which µ is tuned based on the the data (µ = 10). Using the test data to tune the parameter value, µ, will help us find an upper-bound estimate of the effectiveness of this method. Table 5 shows the Mean Average Precision (MAP) and Fa—i for each method in the rumor retrieval task. </context>
</contexts>
<marker>Berger, 1985</marker>
<rawString>James Berger. 1985. Statistical decision theory and Bayesian Analysis (2nd ed.). New York: SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data.</title>
<date>2010</date>
<journal>Discovery Science,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>6332</volume>
<pages>1--15</pages>
<editor>In Bernhard Pfahringer, Geoff Holmes, and Achim Hoffmann, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="6739" citStr="Bifet and Frank, 2010" startWordPosition="1074" endWordPosition="1077">the statements they post appear controversial. The automatic identification of rumors from a corpus is most closely related to the identification of memes done in (Leskovec et al., 2009), but presents new challenges since we seek to highlight a certain type of recurring phrases. Our work presents one of the first attempts at automatic rumor analysis. 2.3 Mining Twitter Data With its nearly constant update of new posts and public API, Twitter can be a useful source for collecting data to be used in exploring a number of problems related to natural language processing and information diffusion (Bifet and Frank, 2010). Pak and Paroubek demonstrated experimentally that despite frequent occurrences of irregular speech patterns in tweets, Twitter can provide a useful corpus for sentiment analysis (Pak and Paroubek, 2010). The diversity of Twitter users make this corpus especially valuable. Ratkiewicz et al also use Twitter to detect and track misleading political memes (Ratkiewicz et al., 2010). Along with many advantages, using Twitter as a corpus for sentiment analysis does present unusual challenges. Because posts are limited to 140 characters, tweets often contain information in an unusually compressed fo</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Bernhard Pfahringer, Geoff Holmes, and Achim Hoffmann, editors, Discovery Science, volume 6332 of Lecture Notes in Computer Science, pages 1–15. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="14907" citStr="Carletta, 1996" startWordPosition="2410" endWordPosition="2411"> annotated data task r. rumor retrieval 0.954 belief classification 0.853 Table 3: Inter-judge agreement in two annotation tasks in terms of r.-statistic 4.2 Inter-Judge Agreement To calculate the annotation accuracy, we annotated 500 instances twice. These annotations were compared with each other, and the Kappa coefficient (r.) was calculated. The r. statistic is formulated as Pr(a) − Pr(e) 1 − Pr(e) where Pr(a) is the relative observed agreement among raters, and Pr(e) is the probability that annotators agree by chance if each annotator is randomly assigning categories (Krippendorff, 1980; Carletta, 1996). Table 3 shows that annotators can reach a high agreement in both extracting rumors (r. = 0.95) and identifying believers (r. = 0.85). 5 Approach In this section, we describe a general framework, which given a tweet, predicts (1) whether it is a rumor-related statement, and if so (2) whether the user believes the rumor or not. We describe 3 sets of features, and explain why these are intuitive to use for identification of rumors. We process the tweets as they appear in the user timeline, and do not perform any pre-processing. Specially, we think that capitalization might be an important prope</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist., 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Roni Rosenfeld</author>
</authors>
<title>Statistical language modeling using the cmu-cambridge toolkit.</title>
<date>1997</date>
<booktitle>Proceedings ESCA Eurospeech,</booktitle>
<pages>47--45</pages>
<contexts>
<context position="23104" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="3826" endWordPosition="3829">etch all the URLs in these tweets and build 0+ and 0− once for unigrams and once for bigrams. These models are merely built on the content of the URLs and ignore the tweet content. Similar to previous features, we calculate the log-likelihood ratio of the content of each tweet with respect to 0+ and 0− for unigrams (URL1) and bigrams URL2). Table 4 summarizes the set of features used in our proposed framework, where each feature is a loglikelihood ratio calculated against a positive (+) and negative (−) training models. To build these language models, we use the CMU Language Modeling toolkit (Clarkson and Rosenfeld, 1997). 5.4 Optimization We build an L1-regularized log-linear model (Andrew and Gao, 2007) on various features discussed before to predict each tweet. Suppose, a procedure generates a set of candidates for an input x. Also, P(t|0+) �m P(#hj|0+) P(t|0−) j=1 log (5) P(#hj|0−) 1594 let’s suppose Φ : X x Y -+ RD is a function that maps each (x, y) to a vector of feature values. Here, the feature vector is the vector of coefficients corresponding to different network, content, and twitterbased properties, and the parameter vector B E RD (D G 9 in our experiments) assigns a real-valued weight to each fea</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Roni Rosenfeld. 1997. Statistical language modeling using the cmu-cambridge toolkit. Proceedings ESCA Eurospeech, 47:45–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas DiFonzo</author>
<author>Prashant Bordia</author>
</authors>
<title>Rumor, gossip, and urban legend.</title>
<date>2007</date>
<journal>Diogenes,</journal>
<pages>54--19</pages>
<contexts>
<context position="1460" citStr="DiFonzo and Bordia, 2007" startWordPosition="216" endWordPosition="220"> also effective in identifying disinformers, users who endorse a rumor and further help it to spread. We perform our experiments on more than 10,000 manually annotated tweets collected from Twitter and show how our retrieval model achieves more than 0.95 in Mean Average Precision (MAP). Finally, we believe that our dataset is the first large-scale dataset on rumor detection. It can open new dimensions in analyzing online misinformation and other aspects of microblog conversations. 1 Introduction A rumor is an unverified and instrumentally relevant statement of information spread among people (DiFonzo and Bordia, 2007). Social psychologists argue that rumors arise in contexts of ambiguity, when the meaning of a situation is not readily apparent, or potential threat, when people feel an acute need for security. For instance rumors about ‘office renovation in a company’ is an example of an ambiguous context, and the rumor that ‘underarm deodorants cause breast cancer’ is an example of a context in which one’s well-being is at risk (DiFonzo et al., 1994). The rapid growth of online social media has made it possible for rumors to spread more quickly. Online social media enable unreliable sources to spread large</context>
<context position="3467" citStr="DiFonzo and Bordia, 2007" startWordPosition="549" endWordPosition="552">specific rumor. In our approach, we address two basic problems. The first problem concerns retrieving online microblogs that are rumor-related. In the second problem, we try to identify tweets in which the rumor is endorsed (the posters show that they believe the rumor). 2 Related Work We review related work on 3 main areas: Analyzing rumors, mining microblogs, and sentiment analysis and subjectivity detection. 2.1 Rumor Identification and Analysis Though understanding rumors has been the subject of research in psychology for some time (Allport and Lepkin, 1945), (Allport and Postman, 1947), (DiFonzo and Bordia, 2007), research has 1589 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589–1599, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics only recently begun to investigate how rumors are manifested and spread differently online. Microblogging services, like Twitter, allow small pieces of information to spread quickly to large audiences, allowing rumors to be created and spread in new ways (Ratkiewicz et al., 2010). Related research has used different methods to study the spread of memes and false information on the</context>
</contexts>
<marker>DiFonzo, Bordia, 2007</marker>
<rawString>Nicholas DiFonzo and Prashant Bordia. 2007. Rumor, gossip, and urban legend. Diogenes, 54:19–35, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas DiFonzo</author>
<author>P Prashant Bordia</author>
<author>Ralph L Rosnow</author>
</authors>
<title>Reining in rumors.</title>
<date>1994</date>
<journal>Organizational Dynamics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1901" citStr="DiFonzo et al., 1994" startWordPosition="294" endWordPosition="297">ther aspects of microblog conversations. 1 Introduction A rumor is an unverified and instrumentally relevant statement of information spread among people (DiFonzo and Bordia, 2007). Social psychologists argue that rumors arise in contexts of ambiguity, when the meaning of a situation is not readily apparent, or potential threat, when people feel an acute need for security. For instance rumors about ‘office renovation in a company’ is an example of an ambiguous context, and the rumor that ‘underarm deodorants cause breast cancer’ is an example of a context in which one’s well-being is at risk (DiFonzo et al., 1994). The rapid growth of online social media has made it possible for rumors to spread more quickly. Online social media enable unreliable sources to spread large amounts of unverified information among people (Herman and Chomsky, 2002). Therefore, it is crucial to design systems that automatically detect misinformation and disinformation (the former often seen as simply false and the latter as deliberately false information). Our definition of a rumor is established based on social psychology, where a rumor is defined as a statement whose truth-value is unverifiable or deliberately false. In-dep</context>
</contexts>
<marker>DiFonzo, Bordia, Rosnow, 1994</marker>
<rawString>Nicholas DiFonzo, P. Prashant Bordia, and Ralph L. Rosnow. 1994. Reining in rumors. Organizational Dynamics, 23(1):47–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Ennals</author>
<author>Dan Byler</author>
<author>John Mark Agosta</author>
<author>Barbara Rosario</author>
</authors>
<title>What is disputed on the web?</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th workshop on Information Credibility, WICOW ’10,</booktitle>
<pages>67--74</pages>
<contexts>
<context position="4520" citStr="Ennals et al., 2010" startWordPosition="707" endWordPosition="710">s to be created and spread in new ways (Ratkiewicz et al., 2010). Related research has used different methods to study the spread of memes and false information on the web. Leskovec et al. use the evolution of quotes reproduced online to identify memes and track their spread overtime (Leskovec et al., 2009). Ratkiewicz et al. (Ratkiewicz et al., 2010) created the “Truthy” system, identifying misleading political memes on Twitter using tweet features, including hashtags, links, and mentions. Other projects focus on highlighting disputed claims on the Internet using pattern matching techniques (Ennals et al., 2010). Though our project builds on previous work, our work differs in its general focus on identifying rumors from a corpus of relevant phrases and our attempts to further discriminate between phrases that confirm, refute, question, and simply talk about rumors of interest. Mendoza et al. explore Twitter data to analyze the behavior of Twitter users under the emergency situation of 2010 earthquake in Chile (Mendoza et al., ). They analyze the re-tweet network topology and find that the patterns of propagation in rumors differ from news because rumors tend to be questioned more than news by the Twi</context>
</contexts>
<marker>Ennals, Byler, Agosta, Rosario, 2010</marker>
<rawString>Rob Ennals, Dan Byler, John Mark Agosta, and Barbara Rosario. 2010. What is disputed on the web? In Proceedings of the 4th workshop on Information Credibility, WICOW ’10, pages 67–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Galen Andrew</author>
<author>Mark Johnson</author>
<author>Kristina Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In ACL ’07.</booktitle>
<contexts>
<context position="24081" citStr="Gao et al., 2007" startWordPosition="4009" endWordPosition="4012">ure values. Here, the feature vector is the vector of coefficients corresponding to different network, content, and twitterbased properties, and the parameter vector B E RD (D G 9 in our experiments) assigns a real-valued weight to each feature. This estimator chooses B to minimize the sum of least squares and a regularization term R. Bˆ = arg min {1 � θ 2 i ||(B, xi) − yi||22 + R(B)} (6) where the regularizer term R(B) is the weighted L1 norm of the parameters. R(B) = α � |Bj |(7) j Here, α is a parameter that controls the amount of regularization (set to 0.1 in our experiments). Gao et. al (Gao et al., 2007) argue that optimizing L1-regularized objective function is challenging since its gradient is discontinuous whenever some parameters equal zero. In this work, we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of LBFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao, 2007). OWL-QN is based on the fact that when restricted to a single orthant, the L1 regularizer is differentiable, and is in fact a linear function of B. Thus, as long as each coordinate of any two consecutive search points does not pass through ze</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Jianfeng Gao, Galen Andrew, Mark Johnson, and Kristina Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Namrata Godbole</author>
<author>Manjunath Srinivasaiah</author>
<author>Steven Skiena</author>
</authors>
<title>Large-scale sentiment analysis for news and blogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM),</booktitle>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="5726" citStr="Godbole et al., 2007" startWordPosition="903" endWordPosition="907">news by the Twitter community. 2.2 Sentiment Analysis The automated detection of rumors is similar to traditional NLP sentiment analysis tasks. Previous work has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to opinion mining and sentiment analysis, it presents a different class of problem because we are concerned not just with the opinion of the person posting a tweet, but with whether the statements they post appear controversial. The automatic identification of rumors from a corpus is most closely related to the identification of memes done in (Leskovec et al., 2009), but presents new chal</context>
</contexts>
<marker>Godbole, Srinivasaiah, Skiena, 2007</marker>
<rawString>Namrata Godbole, Manjunath Srinivasaiah, and Steven Skiena. 2007. Large-scale sentiment analysis for news and blogs. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM), Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
</authors>
<title>What’s with the attitude? identifying sentences with attitude in online discussions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1245--1255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="5555" citStr="Hassan et al., 2010" startWordPosition="875" endWordPosition="878">et al., ). They analyze the re-tweet network topology and find that the patterns of propagation in rumors differ from news because rumors tend to be questioned more than news by the Twitter community. 2.2 Sentiment Analysis The automated detection of rumors is similar to traditional NLP sentiment analysis tasks. Previous work has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to opinion mining and sentiment analysis, it presents a different class of problem because we are concerned not just with the opinion of the person posting a tweet, but with whether the statements they post appear contro</context>
<context position="16811" citStr="Hassan et al., 2010" startWordPosition="2735" endWordPosition="2738">l with respect to fi. For computational reasons and to avoid dealing with very small numbers we use the log of the likelihood ratio to build each classifier. P(0+It) LLi = log i P(0−i |t) The first term PP(θ) can be easily calculated usθ−i ing the maximum likelihood estimates of the probabilities (i.e., the estimate of each probability is the corresponding relative frequency). The second term is calculated using various features that we explain below. 5.1 Content-based Features The first set of features are extracted from the text of the tweets. We propose 4 content based features. We follow (Hassan et al., 2010) and present the tweet with 2 different patterns: • Lexical patterns: All the words and segments in the tweet are represented as they appear and are tokenized using the space character. • Part-of-speech patterns: All words are replaced with their part-of-speech tags. To find the partof-speech of a hashtag we treat it as a word (since they could have semantic roles in the sentence), by omitting the tag sign, and then precede the tag with the label TAG/. We also introduce a new tag, URL, for URLs that appear in a tweet. From each tweet we extract 4 (2 x 2) features, corresponding to unigrams and</context>
</contexts>
<marker>Hassan, Qazvinian, Radev, 2010</marker>
<rawString>Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev. 2010. What’s with the attitude? identifying sentences with attitude in online discussions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1245–1255, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward S Herman</author>
<author>Noam Chomsky</author>
</authors>
<title>Manufacturing Consent: The Political Economy of the Mass Media.</title>
<date>2002</date>
<publisher>Pantheon.</publisher>
<contexts>
<context position="2134" citStr="Herman and Chomsky, 2002" startWordPosition="332" endWordPosition="335"> contexts of ambiguity, when the meaning of a situation is not readily apparent, or potential threat, when people feel an acute need for security. For instance rumors about ‘office renovation in a company’ is an example of an ambiguous context, and the rumor that ‘underarm deodorants cause breast cancer’ is an example of a context in which one’s well-being is at risk (DiFonzo et al., 1994). The rapid growth of online social media has made it possible for rumors to spread more quickly. Online social media enable unreliable sources to spread large amounts of unverified information among people (Herman and Chomsky, 2002). Therefore, it is crucial to design systems that automatically detect misinformation and disinformation (the former often seen as simply false and the latter as deliberately false information). Our definition of a rumor is established based on social psychology, where a rumor is defined as a statement whose truth-value is unverifiable or deliberately false. In-depth rumor analysis such as determining the intent and impact behind the spread of a rumor is a very challenging task and is not possible without first retrieving the complete set of social conversations (e.g., tweets) that are actuall</context>
</contexts>
<marker>Herman, Chomsky, 2002</marker>
<rawString>Edward S Herman and Noam Chomsky. 2002. Manufacturing Consent: The Political Economy of the Mass Media. Pantheon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtenay Honeycutt</author>
<author>Susan C Herring</author>
</authors>
<title>Beyond microblogging: Conversation and collaboration via twitter.</title>
<date>2009</date>
<booktitle>Hawaii International Conference on System Sciences,</booktitle>
<pages>0--1</pages>
<contexts>
<context position="22038" citStr="Honeycutt and Herring, 2009" startWordPosition="3639" endWordPosition="3642">tent unigram content unigram TXT2 content bigram content unigram POS1 content pos content pos unigram POS2 content pos content pos bigram URL1 content unigram target URL unigram Twitter URL2 content bigram target URL bigram TAG hashtag hashtag Network USR1 tweeting user all users in the data USR2 re-tweeted user all users in the data Table 4: List of features used in our optimization framework. Each feature is a log-likelihood ratio calculated against a a positive (+) and negative (−) training models. 5.3.2 URLs Previous work has discussed the role of URLs in information diffusion on Twitter (Honeycutt and Herring, 2009). Twitter users share URLs in their tweets to refer to external sources or overcome the length limit forced by Twitter. Intuitively, if a tweet is a positive instance, then it is likely to be similar to the content of URLs shared by other positive tweets. Using the same reasoning, if a tweet is a negative instance, then it should be more similar to the web pages shared by other negative instances. Given a set of training tweets, we fetch all the URLs in these tweets and build 0+ and 0− once for unigrams and once for bigrams. These models are merely built on the content of the URLs and ignore t</context>
</contexts>
<marker>Honeycutt, Herring, 2009</marker>
<rawString>Courtenay Honeycutt and Susan C. Herring. 2009. Beyond microblogging: Conversation and collaboration via twitter. Hawaii International Conference on System Sciences, 0:1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Huang</author>
<author>Katherine M Thornton</author>
<author>Efthimis N Efthimiadis</author>
</authors>
<title>Conversational tagging in twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the 21st ACM conference on Hypertext and hypermedia, HT ’10,</booktitle>
<pages>173--178</pages>
<contexts>
<context position="20802" citStr="Huang et al., 2010" startWordPosition="3435" endWordPosition="3438">umor) “Obama says he’s doing ‘Christ’s work’.” re-tweeted (rumor) “Obama says he’s doing ‘Christ’s work.’ Oh my God, CHRIST IS A MUSLIM.” 5.3 Twitter Specific Memes Our final set of features are extracted from memes that are specific to Twitter: hashtags and URLs. Previous work has shown the usefulness of these memes (Ratkiewicz et al., 2010). 5.3.1 Hashtags One emergent phenomenon in the Twitter ecosystem is the use of hashtags: words or phrases prefixed with a hash symbol (#). These hashtags are created by users, and are widely used for a few days, then disappear when the topic is outdated (Huang et al., 2010). In our approach, we investigate whether hashtags used in rumor-related tweets are different from other tweets. Moreover, we examine whether people who believe and spread rumors use hashtags that are different from those seen in tweets that deny or question a rumor. Given a set of training tweets of positive and negative examples, we build two statistical models (0+, 0−), each showing the usage probability distribution of various hashtags. For a given tweet, t, with a set of m hashtags (#h1 · · · #hm), we calculate the loglikelihood ratio using Equation 2 where Feature LL-ratio model Content </context>
</contexts>
<marker>Huang, Thornton, Efthimiadis, 2010</marker>
<rawString>Jeff Huang, Katherine M. Thornton, and Efthimis N. Efthimiadis. 2010. Conversational tagging in twitter. In Proceedings of the 21st ACM conference on Hypertext and hypermedia, HT ’10, pages 173–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Beverly Hills:</title>
<date>1980</date>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="14890" citStr="Krippendorff, 1980" startWordPosition="2408" endWordPosition="2409"> each class from the annotated data task r. rumor retrieval 0.954 belief classification 0.853 Table 3: Inter-judge agreement in two annotation tasks in terms of r.-statistic 4.2 Inter-Judge Agreement To calculate the annotation accuracy, we annotated 500 instances twice. These annotations were compared with each other, and the Kappa coefficient (r.) was calculated. The r. statistic is formulated as Pr(a) − Pr(e) 1 − Pr(e) where Pr(a) is the relative observed agreement among raters, and Pr(e) is the probability that annotators agree by chance if each annotator is randomly assigning categories (Krippendorff, 1980; Carletta, 1996). Table 3 shows that annotators can reach a high agreement in both extracting rumors (r. = 0.95) and identifying believers (r. = 0.85). 5 Approach In this section, we describe a general framework, which given a tweet, predicts (1) whether it is a rumor-related statement, and if so (2) whether the user believes the rumor or not. We describe 3 sets of features, and explain why these are intuitive to use for identification of rumors. We process the tweets as they appear in the user timeline, and do not perform any pre-processing. Specially, we think that capitalization might be a</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Beverly Hills: Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jure Leskovec</author>
<author>Lars Backstrom</author>
<author>Jon Kleinberg</author>
</authors>
<title>Meme-tracking and the dynamics of the news cycle.</title>
<date>2009</date>
<booktitle>In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>497--506</pages>
<contexts>
<context position="4208" citStr="Leskovec et al., 2009" startWordPosition="661" endWordPosition="664">1599, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics only recently begun to investigate how rumors are manifested and spread differently online. Microblogging services, like Twitter, allow small pieces of information to spread quickly to large audiences, allowing rumors to be created and spread in new ways (Ratkiewicz et al., 2010). Related research has used different methods to study the spread of memes and false information on the web. Leskovec et al. use the evolution of quotes reproduced online to identify memes and track their spread overtime (Leskovec et al., 2009). Ratkiewicz et al. (Ratkiewicz et al., 2010) created the “Truthy” system, identifying misleading political memes on Twitter using tweet features, including hashtags, links, and mentions. Other projects focus on highlighting disputed claims on the Internet using pattern matching techniques (Ennals et al., 2010). Though our project builds on previous work, our work differs in its general focus on identifying rumors from a corpus of relevant phrases and our attempts to further discriminate between phrases that confirm, refute, question, and simply talk about rumors of interest. Mendoza et al. ex</context>
<context position="6303" citStr="Leskovec et al., 2009" startWordPosition="1000" endWordPosition="1003">ive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to opinion mining and sentiment analysis, it presents a different class of problem because we are concerned not just with the opinion of the person posting a tweet, but with whether the statements they post appear controversial. The automatic identification of rumors from a corpus is most closely related to the identification of memes done in (Leskovec et al., 2009), but presents new challenges since we seek to highlight a certain type of recurring phrases. Our work presents one of the first attempts at automatic rumor analysis. 2.3 Mining Twitter Data With its nearly constant update of new posts and public API, Twitter can be a useful source for collecting data to be used in exploring a number of problems related to natural language processing and information diffusion (Bifet and Frank, 2010). Pak and Paroubek demonstrated experimentally that despite frequent occurrences of irregular speech patterns in tweets, Twitter can provide a useful corpus for sen</context>
</contexts>
<marker>Leskovec, Backstrom, Kleinberg, 2009</marker>
<rawString>Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In KDD ’09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 497–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marcelo Mendoza</author>
<author>Barbara Poblete</author>
<author>Carlos Castillo</author>
</authors>
<title>Twitter under crisis: Can we trust what we rt?</title>
<marker>Mendoza, Poblete, Castillo, </marker>
<rawString>Marcelo Mendoza, Barbara Poblete, and Carlos Castillo. Twitter under crisis: Can we trust what we rt?</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="6943" citStr="Pak and Paroubek, 2010" startWordPosition="1104" endWordPosition="1107">w challenges since we seek to highlight a certain type of recurring phrases. Our work presents one of the first attempts at automatic rumor analysis. 2.3 Mining Twitter Data With its nearly constant update of new posts and public API, Twitter can be a useful source for collecting data to be used in exploring a number of problems related to natural language processing and information diffusion (Bifet and Frank, 2010). Pak and Paroubek demonstrated experimentally that despite frequent occurrences of irregular speech patterns in tweets, Twitter can provide a useful corpus for sentiment analysis (Pak and Paroubek, 2010). The diversity of Twitter users make this corpus especially valuable. Ratkiewicz et al also use Twitter to detect and track misleading political memes (Ratkiewicz et al., 2010). Along with many advantages, using Twitter as a corpus for sentiment analysis does present unusual challenges. Because posts are limited to 140 characters, tweets often contain information in an unusually compressed form and, as a result, grammar used may be unconventional. Instances of sarcasm and humor are also prevalent (Bifet and Frank, 2010). The procedures we used for the collection and analysis of tweets are sim</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACL’04,</booktitle>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5886" citStr="Pang and Lee, 2004" startWordPosition="931" endWordPosition="934">used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to opinion mining and sentiment analysis, it presents a different class of problem because we are concerned not just with the opinion of the person posting a tweet, but with whether the statements they post appear controversial. The automatic identification of rumors from a corpus is most closely related to the identification of memes done in (Leskovec et al., 2009), but presents new challenges since we seek to highlight a certain type of recurring phrases. Our work presents one of the first attempts at automatic rumor analysis. 2.3 Mining Twitt</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL’04, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="5865" citStr="Pang and Lee, 2008" startWordPosition="927" endWordPosition="930">. Previous work has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to opinion mining and sentiment analysis, it presents a different class of problem because we are concerned not just with the opinion of the person posting a tweet, but with whether the statements they post appear controversial. The automatic identification of rumors from a corpus is most closely related to the identification of memes done in (Leskovec et al., 2009), but presents new challenges since we seek to highlight a certain type of recurring phrases. Our work presents one of the first attempts at automatic rumor analy</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2:1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of conference on Empirical methods in natural language processing, EMNLP’02,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="5367" citStr="Pang et al., 2002" startWordPosition="846" endWordPosition="849">and simply talk about rumors of interest. Mendoza et al. explore Twitter data to analyze the behavior of Twitter users under the emergency situation of 2010 earthquake in Chile (Mendoza et al., ). They analyze the re-tweet network topology and find that the patterns of propagation in rumors differ from news because rumors tend to be questioned more than news by the Twitter community. 2.2 Sentiment Analysis The automated detection of rumors is similar to traditional NLP sentiment analysis tasks. Previous work has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to opinion mining and sentiment </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of conference on Empirical methods in natural language processing, EMNLP’02, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Ratkiewicz</author>
<author>Michael Conover</author>
<author>Mark Meiss</author>
<author>Bruno Gonc¸alves</author>
<author>Snehal Patil</author>
<author>Alessandro Flammini</author>
<author>Filippo Menczer</author>
</authors>
<title>Detecting and tracking the spread of astroturf memes in microblog streams.</title>
<date>2010</date>
<location>CoRR, abs/1011.3768.</location>
<marker>Ratkiewicz, Conover, Meiss, Gonc¸alves, Patil, Flammini, Menczer, 2010</marker>
<rawString>Jacob Ratkiewicz, Michael Conover, Mark Meiss, Bruno Gonc¸alves, Snehal Patil, Alessandro Flammini, and Filippo Menczer. 2010. Detecting and tracking the spread of astroturf memes in microblog streams. CoRR, abs/1011.3768.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>