<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024879">
<note confidence="0.816127">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 148-150, Lisbon, Portugal, 2000.
</note>
<title confidence="0.998178">
Improving Chunking by Means of Lexical-Contextual Information
in Statistical Language Models
</title>
<author confidence="0.870462">
Ferran Pla and Antonio Molina and Natividad Prieto
</author>
<affiliation confidence="0.782169">
Universitat Politecnica de Valencia
</affiliation>
<address confidence="0.738374">
Canal de Vera s/n
46020 Valencia (Spain)
</address>
<email confidence="0.780633">
{fpla, amolina, nprieto}@dsic upv.es
</email>
<sectionHeader confidence="0.999275" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999665">
In this work, we present a stochastic approach
to shallow parsing. Most of the current ap-
proaches to shallow parsing have a common
characteristic: they take the sequence of lex-
ical tags proposed by a POS tagger as input
for the chunking process. Our system produces
tagging and chunking in a single process using
an Integrated Language Model (ILM) formal-
ized as Markov Models. This model integrates
several knowledge sources: lexical probabilities,
a contextual Language Model (LM) for every
chunk, and a contextual LM for the sentences.
We have extended the ILM by adding lexical in-
formation to the contextual LMs. We have ap-
plied this approach to the CoNLL-2000 shared
task improving the performance of the chunker.
</bodyText>
<sectionHeader confidence="0.60769" genericHeader="method">
2 Overview of the system
</sectionHeader>
<bodyText confidence="0.999518">
The baseline system described in (Pla et al.,
2000a) uses bigrams, formalized as finite-state
automata. It is a transducer composed of two
levels (see Figure 1). The upper one (Figure la)
represents the contextual LM for the sentences.
The symbols associated to the states are POS
tags (Ci) and chunk descriptors (Si). The lower
one modelizes the different chunks considered
(Figure lb). In this case, the symbols are the
POS tags (Ci) that belong to the correspond-
ing chunk (Si). Next, a regular substitution of
the lower models into the upper level is made
(Figure 1c). In this way, we get a single Inte-
grated LM which shows the possible concate-
nations of lexical tags and chunks. Also, each
state is relabeled with a tuple (C1, S) where
E C and Si E S. C is the POS tag set used
and S = {[Si, Si],Si,So} is the chunk set de-
fined. [Si and Si] stand for the initial and the
final state of chunk whose descriptor is Si. The
label Si is assigned to those states which are in-
side Si chunk, and So is assigned to those states
which are outside of any chunk. All the LMs
involved have been smoothed by using a back-
off technique (Katz, 1987). We have not spec-
ified lexical probabilities in every state of the
different contextual models. We assumed that
P(W31(Ci, Si)) = P(W31Ci) for every Si E S.
Once the integrated transducer has been
made, the tagging and shallow parsing process
consists of finding the sequence of states of max-
imum probability on it for an input sentence.
Therefore, this sequence must be compatible
with the contextual, syntactical and lexical con-
straints. This process can be carried out by
dynamic programming using the Viterbi algo-
rithm (Viterbi, 1967), which has been appropri-
ately modified to use our models. From the dy-
namic programming trellis, we can obtain the
maximum probability path for the input sen-
tence through the model, and thus the best se-
quence of lexical tags and the best segmentation
in chunks, in a single process.
</bodyText>
<sectionHeader confidence="0.9432195" genericHeader="method">
3 Specialized Contextual Language
Models
</sectionHeader>
<bodyText confidence="0.999984461538462">
The contextual model for the sentences and the
models for chunks (and, therefore, the ILM) can
be modified taking into account certain words
in the context where they appear. This spe-
cialization us allows to set certain contextual
constraints which modify the contextual LMs
and improve the performance of the chunker (as
shown below). This set of words can be defined
using some heuristics such as: the most frequent
words in the training corpus, the words with a
higher tagging error rate, the words that belong
to closed classes (prepositions, pronouns, etc.),
or whatever word chosen following some linguis-
</bodyText>
<page confidence="0.966752">
148
</page>
<figure confidence="0.999849333333333">
(a) Contextual LM
(e) Integrated LM
(b) LM for Chunks (Si )
</figure>
<figureCaption confidence="0.999993">
Figure 1: Integrated Language Model for Tagging and Chunking.
</figureCaption>
<bodyText confidence="0.998355333333333">
tic criterion.
To do this, we added to the POS tag set the
set of structural tags (Wi, C3) for each special-
ized word Wi in all of their possible categories
C3. Then, we relabelled the training corpus: if
a word W, was labelled with the POS tag
we changed C3 for the pair (Wi, Ci). The learn-
ing process of the bigram LMs was carried out
from this new training data set.
The Contextual LMs obtained has some spe-
cific states which are related to the specialized
words. In the basic Language Model (ILM), a
state was labelled by (Ci, Si). In the specialized
ILM, a state was specified for a certain word Wk
(only if the Wk word belongs to the category
Ci). In this way, the state is relabelled with the
tuple S3) and only the word Wk can be
emitted with a probability equal to 1.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experimental Work
</sectionHeader>
<bodyText confidence="0.999919659090909">
We applied both approaches (ILM and spe-
cialized ILM) using the training and test data
of the CoNLL-2000 shared task (http://lcg-
www.uia. ac. be/ conll20 0 0) . We also evaluated
how the performance of the chunker varies when
we modify the specialized word set. Neverthe-
less, the use of our approach on other corpora
(including different languages), other lexical tag
sets or other kinds of chunks can be done in a
direct way.
Although our system is able to carry out tag-
ging and chunking in a single process, we will
not present tagging results for this task, as the
POS tags of the data set used are not supervised
and, therefore, a comparison is not possible.
We would like to point out that we have simu-
lated a morphological analyzer for English. We
have constructed a tag dictionary with the lex-
icon of the training set and the test set used.
This dictionary gave us the possible lexical tags
for each word from the corpus. In no case, was
the test used to estimate the lexical probabili-
ties.
As stated above, several criterion can be cho-
sen to define the set of specialized words. We
have selected the most frequent words in the
training data set. We have not taken into ac-
count certain words such as punctuation sym-
bols, proper nouns, numbers, etc. This fact did
not decrease the performance of the chunker and
also reduced the number of states of the contex-
tual LMs. Figure 2 shows how the performance
of the chunker (F0=1) improves as a function of
the size of the specialized word set. The best re-
sults were obtained with the set of words whose
frequency in the training corpus was larger than
80 (about 470 words). We obtained similar re-
sults when only considering the words of the
training set belonging to closed classes (that,
about, as, if, out, while, whether, for, to, ...).
In Table 1 we present the results of chunk-
ing with the specialized ILM. When comparing
these results with the results obtained using the
basic ILM, we observed that, in general, the F-
</bodyText>
<page confidence="0.997929">
149
</page>
<bodyText confidence="0.999016">
score was improved for each chunk. The best
improvement was observed for SBAFL (from 0.37
to 79.46), PP (from 88.94 to 95.51) and PRT
(38.82 to 66.67).
</bodyText>
<sectionHeader confidence="0.992351" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999918347826087">
In this paper, we have presented a system for
Tagging and Chunking based on an Integrated
Language Model that uses a homogeneous for-
malism (finite-state machine) to combine differ-
ent knowledge sources. It is feasible both in
terms of performance and also in terms of com-
putational efficiency.
All the models involved are learnt automat-
ically from data, so the system is very flexible
with changes in the reference language, changes
in POS tags or changes in the definition of
chunks.
Our approach allows us to use any regular
model which has been previously defined or
learnt. In previous works, we have used bi-
grams (Pla et al., 2000a), and we have com-
bined them with other more complex models
which had been learnt using grammatical in-
ference techniques (Pla et al., 2000b). In this
work, we used only bigram models improved
with lexical-contextual information.
The Fo score obtained increased from 86.64 to
90.14 when we used the specialized ILM. Never-
</bodyText>
<figure confidence="0.604762">
50 103 150 200 250 300 350 500
OSPECIALIZED WORDS
</figure>
<figureCaption confidence="0.9897775">
Figure 2: F-score as a function of the number
of specialized words in the ILM
</figureCaption>
<table confidence="0.999922583333333">
test data precision recall F0,1
ADJP 72.89 % 66.89 % 69.76
ADVP 79.65% 74.13% 76.79
CONJP 40.00% 66.67% 50.00
INTJ 100.00% 100.00% 100.00
LST 0.00% 0.00% 0.00
NP 90.28% 89.41% 89.84
PP 95.89% 95.14% 95.51
PRT 60.31% 74.53% 66.67
SBAR 82.07% 77.01% 79.46
VP 91.53% 91.58% 91.55
all 90.63% 89.65% 90.14
</table>
<tableCaption confidence="0.9230665">
Table 1: Chunking results using specialized ILM
(Accuracy= 93.79%)
</tableCaption>
<bodyText confidence="0.99927275">
theless, we believe that the models could be im-
proved with a more detailed study of the words
whose contextual information is really relevant
to tagging and chunking.
</bodyText>
<sectionHeader confidence="0.998134" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.931425666666667">
This work has been partially supported by the
Spanish Research Project CICYT (TIC97-0671-
CO2-01/02).
</bodyText>
<sectionHeader confidence="0.980614" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995084375">
S. M. Katz. 1987. Estimation of Probabilities from
Sparse Data for the Language Model Component
of a Speech Recognizer. IEEE Transactions on
Acoustics, Speech and Signal Processing, 35.
F. Pla, A. Molina, and N. Prieto. 2000a. Tagging
and Chunking with Bigrams. In Proceedings of the
COLING-2000, Saarbriicken, Germany, August.
F. Pla, A. Molina, and N. Prieto. 2000b. An Inte-
grated Statistical Model for Tagging and Chunk-
ing Unrestricted Text. In Proceedings of the Text,
Speech and Dialogue 2000, Brno, Czech Republic,
September.
A. J. Viterbi. 1967. Error Bounds for Convolutional
Codes and an Asymptotically Optimal Decoding
Algorithm. IEEE Transactions on Information
Theory, pages 260-269, April.
</reference>
<figure confidence="0.5081595">
F-SCORS
88
</figure>
<page confidence="0.967317">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370933">
<note confidence="0.977413">of CoNLL-2000 and LLL-2000, 148-150, Lisbon, Portugal, 2000.</note>
<title confidence="0.9974225">Improving Chunking by Means of Lexical-Contextual Information in Statistical Language Models</title>
<author confidence="0.96504">Pla Molina</author>
<affiliation confidence="0.7876205">Universitat Politecnica de Canal de Vera</affiliation>
<address confidence="0.999453">46020 Valencia</address>
<intro confidence="0.681141">nprieto}@dsic upv.es</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>35</volume>
<contexts>
<context position="2235" citStr="Katz, 1987" startWordPosition="379" endWordPosition="380">dels into the upper level is made (Figure 1c). In this way, we get a single Integrated LM which shows the possible concatenations of lexical tags and chunks. Also, each state is relabeled with a tuple (C1, S) where E C and Si E S. C is the POS tag set used and S = {[Si, Si],Si,So} is the chunk set defined. [Si and Si] stand for the initial and the final state of chunk whose descriptor is Si. The label Si is assigned to those states which are inside Si chunk, and So is assigned to those states which are outside of any chunk. All the LMs involved have been smoothed by using a backoff technique (Katz, 1987). We have not specified lexical probabilities in every state of the different contextual models. We assumed that P(W31(Ci, Si)) = P(W31Ci) for every Si E S. Once the integrated transducer has been made, the tagging and shallow parsing process consists of finding the sequence of states of maximum probability on it for an input sentence. Therefore, this sequence must be compatible with the contextual, syntactical and lexical constraints. This process can be carried out by dynamic programming using the Viterbi algorithm (Viterbi, 1967), which has been appropriately modified to use our models. Fro</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pla</author>
<author>A Molina</author>
<author>N Prieto</author>
</authors>
<title>Tagging and Chunking with Bigrams.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING-2000,</booktitle>
<location>Saarbriicken, Germany,</location>
<contexts>
<context position="1149" citStr="Pla et al., 2000" startWordPosition="175" endWordPosition="178">cal tags proposed by a POS tagger as input for the chunking process. Our system produces tagging and chunking in a single process using an Integrated Language Model (ILM) formalized as Markov Models. This model integrates several knowledge sources: lexical probabilities, a contextual Language Model (LM) for every chunk, and a contextual LM for the sentences. We have extended the ILM by adding lexical information to the contextual LMs. We have applied this approach to the CoNLL-2000 shared task improving the performance of the chunker. 2 Overview of the system The baseline system described in (Pla et al., 2000a) uses bigrams, formalized as finite-state automata. It is a transducer composed of two levels (see Figure 1). The upper one (Figure la) represents the contextual LM for the sentences. The symbols associated to the states are POS tags (Ci) and chunk descriptors (Si). The lower one modelizes the different chunks considered (Figure lb). In this case, the symbols are the POS tags (Ci) that belong to the corresponding chunk (Si). Next, a regular substitution of the lower models into the upper level is made (Figure 1c). In this way, we get a single Integrated LM which shows the possible concatenat</context>
<context position="7406" citStr="Pla et al., 2000" startWordPosition="1287" endWordPosition="1290">have presented a system for Tagging and Chunking based on an Integrated Language Model that uses a homogeneous formalism (finite-state machine) to combine different knowledge sources. It is feasible both in terms of performance and also in terms of computational efficiency. All the models involved are learnt automatically from data, so the system is very flexible with changes in the reference language, changes in POS tags or changes in the definition of chunks. Our approach allows us to use any regular model which has been previously defined or learnt. In previous works, we have used bigrams (Pla et al., 2000a), and we have combined them with other more complex models which had been learnt using grammatical inference techniques (Pla et al., 2000b). In this work, we used only bigram models improved with lexical-contextual information. The Fo score obtained increased from 86.64 to 90.14 when we used the specialized ILM. Never50 103 150 200 250 300 350 500 OSPECIALIZED WORDS Figure 2: F-score as a function of the number of specialized words in the ILM test data precision recall F0,1 ADJP 72.89 % 66.89 % 69.76 ADVP 79.65% 74.13% 76.79 CONJP 40.00% 66.67% 50.00 INTJ 100.00% 100.00% 100.00 LST 0.00% 0.0</context>
</contexts>
<marker>Pla, Molina, Prieto, 2000</marker>
<rawString>F. Pla, A. Molina, and N. Prieto. 2000a. Tagging and Chunking with Bigrams. In Proceedings of the COLING-2000, Saarbriicken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pla</author>
<author>A Molina</author>
<author>N Prieto</author>
</authors>
<title>An Integrated Statistical Model for Tagging and Chunking Unrestricted Text.</title>
<date>2000</date>
<booktitle>In Proceedings of the Text, Speech and Dialogue</booktitle>
<location>Brno, Czech Republic,</location>
<contexts>
<context position="1149" citStr="Pla et al., 2000" startWordPosition="175" endWordPosition="178">cal tags proposed by a POS tagger as input for the chunking process. Our system produces tagging and chunking in a single process using an Integrated Language Model (ILM) formalized as Markov Models. This model integrates several knowledge sources: lexical probabilities, a contextual Language Model (LM) for every chunk, and a contextual LM for the sentences. We have extended the ILM by adding lexical information to the contextual LMs. We have applied this approach to the CoNLL-2000 shared task improving the performance of the chunker. 2 Overview of the system The baseline system described in (Pla et al., 2000a) uses bigrams, formalized as finite-state automata. It is a transducer composed of two levels (see Figure 1). The upper one (Figure la) represents the contextual LM for the sentences. The symbols associated to the states are POS tags (Ci) and chunk descriptors (Si). The lower one modelizes the different chunks considered (Figure lb). In this case, the symbols are the POS tags (Ci) that belong to the corresponding chunk (Si). Next, a regular substitution of the lower models into the upper level is made (Figure 1c). In this way, we get a single Integrated LM which shows the possible concatenat</context>
<context position="7406" citStr="Pla et al., 2000" startWordPosition="1287" endWordPosition="1290">have presented a system for Tagging and Chunking based on an Integrated Language Model that uses a homogeneous formalism (finite-state machine) to combine different knowledge sources. It is feasible both in terms of performance and also in terms of computational efficiency. All the models involved are learnt automatically from data, so the system is very flexible with changes in the reference language, changes in POS tags or changes in the definition of chunks. Our approach allows us to use any regular model which has been previously defined or learnt. In previous works, we have used bigrams (Pla et al., 2000a), and we have combined them with other more complex models which had been learnt using grammatical inference techniques (Pla et al., 2000b). In this work, we used only bigram models improved with lexical-contextual information. The Fo score obtained increased from 86.64 to 90.14 when we used the specialized ILM. Never50 103 150 200 250 300 350 500 OSPECIALIZED WORDS Figure 2: F-score as a function of the number of specialized words in the ILM test data precision recall F0,1 ADJP 72.89 % 66.89 % 69.76 ADVP 79.65% 74.13% 76.79 CONJP 40.00% 66.67% 50.00 INTJ 100.00% 100.00% 100.00 LST 0.00% 0.0</context>
</contexts>
<marker>Pla, Molina, Prieto, 2000</marker>
<rawString>F. Pla, A. Molina, and N. Prieto. 2000b. An Integrated Statistical Model for Tagging and Chunking Unrestricted Text. In Proceedings of the Text, Speech and Dialogue 2000, Brno, Czech Republic, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asymptotically Optimal Decoding Algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>260--269</pages>
<contexts>
<context position="2773" citStr="Viterbi, 1967" startWordPosition="466" endWordPosition="467">he LMs involved have been smoothed by using a backoff technique (Katz, 1987). We have not specified lexical probabilities in every state of the different contextual models. We assumed that P(W31(Ci, Si)) = P(W31Ci) for every Si E S. Once the integrated transducer has been made, the tagging and shallow parsing process consists of finding the sequence of states of maximum probability on it for an input sentence. Therefore, this sequence must be compatible with the contextual, syntactical and lexical constraints. This process can be carried out by dynamic programming using the Viterbi algorithm (Viterbi, 1967), which has been appropriately modified to use our models. From the dynamic programming trellis, we can obtain the maximum probability path for the input sentence through the model, and thus the best sequence of lexical tags and the best segmentation in chunks, in a single process. 3 Specialized Contextual Language Models The contextual model for the sentences and the models for chunks (and, therefore, the ILM) can be modified taking into account certain words in the context where they appear. This specialization us allows to set certain contextual constraints which modify the contextual LMs a</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error Bounds for Convolutional Codes and an Asymptotically Optimal Decoding Algorithm. IEEE Transactions on Information Theory, pages 260-269, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>