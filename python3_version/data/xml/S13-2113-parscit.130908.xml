<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000991">
<title confidence="0.987504">
UoS: A Graph-Based System for Graded Word Sense Induction
</title>
<author confidence="0.999454">
David Hope, Bill Keller
</author>
<affiliation confidence="0.8148365">
University of Sussex
Cognitive and Language Processing Systems Group
</affiliation>
<address confidence="0.908625">
Brighton, Sussex, UK
</address>
<email confidence="0.995913">
davehope@gmail.com, billk@sussex.ac.uk
</email>
<sectionHeader confidence="0.998578" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807529411765">
This paper presents UoS, a graph-based Word
Sense Induction system which attempts to
find all applicable senses of a target word
given its context, grading each sense accord-
ing to its suitability to the context. Senses
of a target word are induced through use of
a non-parameterised, linear-time clustering al-
gorithm that returns maximal quasi-strongly
connected components of a target word graph
in which vertex pairs are assigned to the same
cluster if either vertex has the highest edge
weight to the other. UoS participated in
SemEval-2013 Task 13: Word Sense Induc-
tion for Graded and Non-Graded Senses. Two
system were submitted; both systems returned
results comparable with those of the best per-
forming systems.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9973446">
Word Sense Induction (WSI) is the task of automat-
ically discovering word senses from text. In princi-
ple, WSI avoids reliance on a pre-defined sense in-
ventory.1 Whereas the related task of Word Sense
Disambiguation (WSD) can only assign pre-defined
senses to words on the basis of context, WSI fol-
lows the dictum that “The meaning of a word is its
use in the language.” (Wittgenstein, 1953) to dis-
cover senses through examination of context of use
in large text corpora. WSI, therefore, may be applied
</bodyText>
<footnote confidence="0.945570333333333">
1In practice, evaluation of a WSI system requires the use of
a gold standard sense inventory such as WordNet (Miller et al.,
1990) or OntoNotes (Hovy et al., 2006).
</footnote>
<bodyText confidence="0.999587521739131">
to discover new, rare, or domain specific senses;
senses undefined in existing sense inventories.2
Previous WSI evaluations (Agirre and Soroa,
2007; Manandhar et al., 2010) have approached
sense induction in terms of finding the single most
salient sense of a target word given its context.
However, as shown in Erk and McCarthy (2009), a
graded notion of sense may be more applicable, as
multiple senses of the target word may be perceived
by readers. The SemEval-2013 WSI evaluation de-
scribed in this paper is designed to explore the possi-
bility of finding all perceived senses of a target word
in a single contextual instance. The aim for partici-
pants in the task is therefore to design a system that
will induce a set of graded (weighted) senses of a
target word in a particular context.
The paper is organised as follows: Section 2 in-
troduces SemEval-2013 Task 13: Word Sense In-
duction for Graded and Non-Graded Senses; Sec-
tion 3 presents UoS, the system that participated in
the task; Section 4 reports evaluation results, show-
ing that UoS returns scores comparable with those
of the best performing systems.
</bodyText>
<sectionHeader confidence="0.956586" genericHeader="method">
2 SemEval-2013 Task 13
</sectionHeader>
<subsectionHeader confidence="0.978808">
2.1 Aim
</subsectionHeader>
<bodyText confidence="0.999838">
The aim for participants in SemEval-2013 Task 13:
Word Sense Induction for Graded and Non-Graded
Senses is to construct a system that will: (1) induce
the senses of a given set of target words and (2), label
each test set context (instance) of a target word with
</bodyText>
<footnote confidence="0.6269785">
2Surveys of WSI and WSD approaches are found in Navigli
(2009) and Navigli (2012).
</footnote>
<page confidence="0.972462">
689
</page>
<bodyText confidence="0.982231">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 689–694, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
all applicable target word senses. Candidate senses
are drawn from the WordNet 3.1 sense inventory.
Systems must therefore return a set of graded senses
for each target word in a particular context, where a
numeric weight signifies (grades) each sense’s appli-
cability to the context. A non-graded sense is simply
the highest graded (weighted) sense out of all graded
senses.
</bodyText>
<subsectionHeader confidence="0.999957">
2.2 Test Set
</subsectionHeader>
<bodyText confidence="0.9998914">
The test set consists of 4806 instances of 50 target
words: 20 verbs (1901 instances), 20 nouns (1908),
and 10 adjectives (997).3 Instances are extracted
from the Open American National Corpus, being a
mix of both written and spoken contexts of target
words.4 Only 542 instances are assigned more than
one sense by annotators, thus have graded senses.
This figure somewhat detracts from the task’s aim as
just 11.62% of the test set can be assigned graded
senses.
</bodyText>
<subsectionHeader confidence="0.997103">
2.3 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.990561615384615">
Systems are evaluated in two ways: (1) in a WSD
task and (2), a clustering task. In the first evalu-
ation, systems are assessed by their ability to cor-
rectly identify which WordNet 3.1 senses of the tar-
get word are applicable in a given instance, and to
quantify, and so, rank, senses according to their level
of applicability. The supervised evaluation method
of previous SemEval WSI tasks (Agirre and Soroa,
2007; Manandhar et al., 2010) is applied to map in-
duced senses to WordNet 3.1 senses, with the map-
ping function of Jurgens (2012) used to account for
the applicability weights. Three evaluation metrics
are used -
</bodyText>
<listItem confidence="0.998893333333333">
• Jaccard Index: measures the overlap between
gold standard senses and those returned by a
WSI system.
• Positionally-Weighted Kendall’s Tau: measures
the ability of a system to rank senses by their
applicability.
</listItem>
<footnote confidence="0.872991">
3Stated as 4664 instances on the task website. Note that the
figure of 4806 is for the revised test set.
4http://www.americannationalcorpus.org/
OANC/index.html.
</footnote>
<listItem confidence="0.708877">
• Weighted Normalized Discounted Cumulative
Gain (NDCG): measures the agreement in ap-
plicability ratings, accounting for both the
ranking and difference in weights assigned to
senses.
</listItem>
<bodyText confidence="0.959952333333333">
In the second evaluation, similarity between a partic-
ipant’s clustering solution and that of the gold stan-
dard set of senses is measured using two metrics -
</bodyText>
<listItem confidence="0.9967062">
• Fuzzy Normalised Mutual Information (NMI):
extends the method of Lancichinetti et al.
(2009) to compute NMI between overlapping
(fuzzy) clusters. Fuzzy NMI measures the
alignment of system and gold standard senses
independently of the cluster sizes, so returns a
measure of how well a WSI system would per-
form regardless of the sense distribution in a
corpus.
• Fuzzy B-Cubed: adapts the overlapping B-
</listItem>
<bodyText confidence="0.796593857142857">
Cubed measure defined in Amig´o et al. (2009)
to the fuzzy clustering setting. As an item-
based, rather than cluster-based, measure,
Fuzzy B-Cubed is sensitive to cluster size skew,
thus captures the expected performance of a
WSI system on a new corpus where the sense
distribution is the same.
</bodyText>
<sectionHeader confidence="0.97938" genericHeader="method">
3 The UoS System
</sectionHeader>
<bodyText confidence="0.996068333333333">
The UoS system uses a graph-based model of word
co-occurrence to induce target word senses as fol-
lows:
</bodyText>
<subsectionHeader confidence="0.999966">
3.1 Constructing a Target Word Graph
</subsectionHeader>
<bodyText confidence="0.976695454545454">
A graph G = (V, E) is constructed for each tar-
get word. V is a set of vertices and E C_ V x V
a set of edges. Each vertex v E V represents a
word found in a dependency relation with the tar-
get word. Words are extracted from the dependency-
parsed version of ukWaC (Ferraresi et al., 2008). In
this evaluation V consists of the 300 highest ranked
dependency relation words.5 Words are ranked us-
ing the Normalised Pointwise Mutual Information
5|V  |= 300 was found to return the best results on the trial
set over the range IVI = [100, 200, 300, ..., 1000].
</bodyText>
<page confidence="0.816211">
690
</page>
<equation confidence="0.9225834">
(NPMI) measure (Bouma, 2009)6, defined for two
words w1, w2 as:
p(w1 ,w2 )
NPMI w w 1
( i� 2) = (log p(w1) p(w2) −log p(w1, w2) ( )
</equation>
<bodyText confidence="0.999158714285714">
An edge (vi, vj) E E is a pair of vertices. An edge
represents a symmetrical relationship between ver-
tices vi and vj; here, that words wi and wj co-occur
in ukWaC contexts. Each edge (vi, vj) is assigned
a weight w(vi, vj) to quantify the significance of
wi, wj co-occurrence, the weight being the value re-
turned by NPMI(wi, wj).
</bodyText>
<subsectionHeader confidence="0.999886">
3.2 Clustering the Target Word Graph
</subsectionHeader>
<bodyText confidence="0.999990380952381">
A clustering algorithm is applied to the target word
graph, partitioning it to a set of clusters. Each
set of words in a cluster is taken to represent a
sense of the target word. The clustering algorithm
applied is MaxMax, a non-parameterised, linear-
time algorithm shown to return good results in pre-
vious WSI evaluations (Hope and Keller, 2013).
MaxMax transforms the weighted, undirected target
word graph G into an unweighted, directed graph
G&apos;, where edge direction in G&apos; indicates a maximal
affinity relationship between two vertices. A ver-
tex vi is said to have maximal affinity to a vertex
vj if the edge weight w(vi, vj) is maximal amongst
the weights of all edges incident on vi. Clusters are
identified by finding root vertices of quasi-strongly
connected (QSC) subgraphs in G&apos; (Thulasiraman
and Swamy, 1992). A directed subgraph is said to
be QSC if, for any vertices vi and vj, there is a root
vertex vk (not necessarily distinct from vi and vj)
with a directed path from vk to vi and a directed path
from vk to vj.7
</bodyText>
<subsectionHeader confidence="0.999303">
3.3 Merging Clusters
</subsectionHeader>
<bodyText confidence="0.997877666666667">
MaxMax tends to generate many fine-grained sense
clusters. Clusters are therefore merged using two
measures: cohesion and separation (Tan et al.,
</bodyText>
<footnote confidence="0.993207285714286">
6Application of the Log Likelihood Ratio measure (Dun-
ning, 1993) returned the same set of words. Though not re-
quired here, NPMI has the useful properties that: if w1 and w2
always co-occur NPMI = 1; if w1 and w2 are distributed as ex-
pected under independence NPMI = 0, and if w1 and w2 never
occur together, NPMI = −1.
7MaxMax is described in detail in Hope and Keller (2013).
</footnote>
<bodyText confidence="0.552858">
2006). The cohesion of a cluster Ci is defined as:
</bodyText>
<equation confidence="0.751716666666667">
�xECi,
yECi
|Ci |�(2)
</equation>
<bodyText confidence="0.7740425">
Separation between two clusters Ci, Cj is defined
as:
</bodyText>
<figure confidence="0.71159875">
�
�
separation(Ci, Cj) = 1 − �
(3)
</figure>
<bodyText confidence="0.988265444444445">
Cluster pairs with high cohesion and low separation
are merged, the intuition being that words in such
pairs will retain a relatively high degree of semantic
similarity. High cohesion is defined as greater than
average cohesion. Low separation is defined as a re-
ciprocal relationship between two clusters: if a clus-
ter Ci has the lowest separation to a cluster Cj (out
of all clusters) and Cj the lowest separation to Ci,
then the two (high cohesion) clusters are merged.8
</bodyText>
<subsectionHeader confidence="0.9058525">
3.4 Assigning Graded Word Senses to Target
Words
</subsectionHeader>
<bodyText confidence="0.999982636363636">
Each test instance is labelled with graded senses of
the target word. A score is computed for the test in-
stance and each target word cluster as the reciprocal
of the separation measure, where Ci is the set of con-
tent words in the instance (nouns, verbs, adjectives,
and adverbs, minus the target word itself) and Cj,
the words in the cluster. The cluster with the lowest
separation score is taken to be the most salient sense
of the target word, with all other positive separation
scores taken to be perceived, graded senses of the
target word in that particular instance.
</bodyText>
<sectionHeader confidence="0.99588" genericHeader="evaluation">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.992804111111111">
Two sets of results were submitted. The first, UoS
(top 3), returns the three highest scoring senses for
each instance; the second, UoS (# WN senses), re-
turns the n = number of target word senses in Word-
Net 3.1 most cohesive clusters, as defined by Equa-
tion (2).
Results for the seven participating WSI systems
are reported in Tables 1 and 2. The ten baselines,
provided by the organisers of the task, are -
</bodyText>
<footnote confidence="0.809272">
8The average number of WordNet 3.1 senses for target
words is 8.58. MaxMax returns an average of 59.54 clusters for
target words; merging results in an average of 21.86 clusters.
</footnote>
<equation confidence="0.987082777777778">
w(x, y)
cohesion(Ci) =
|Ci |X |Cj|
�xECi,
yECj
w(x, y)
�
� �
�
</equation>
<page confidence="0.992646">
691
</page>
<table confidence="0.999909578947368">
System/Baseline Jaccard Index Positionally Weighted Tau Weighted NDCG
F-Score F-Score F-Score
UoS (top 3) 0.232 0.625 0.374
AI-KU (r5-a1000) 0.244 0.642 0.332
AI-KU 0.197 0.620 0.387
Unimelb (50k) 0.213 0.620 0.371
Unimelb (5p) 0.218 0.614 0.365
UoS (# WN senses) 0.192 0.596 0.315
AI-KU (a1000) 0.197 0.606 0.215
Most Frequent Sense 0.552 0.560 0.718
Senses Eq. Weighted 0.149 0.787 0.436
Senses, Avg. Weight 0.187 0.613 0.499
One sense 0.192 0.609 0.288
1 of 2 random senses 0.220 0.627 0.287
1 of 3 random senses 0.244 0.633 0.287
1 of n random senses 0.290 0.638 0.286
1 sense per instance 0.000 0.945 0.000
SemCor, MFS 0.455 0.465 0.339
SemCor, All Senses 0.149 0.559 0.489
</table>
<tableCaption confidence="0.998345">
Table 1: Results for the WSD evaluation: all instances.
</tableCaption>
<listItem confidence="0.999583857142857">
• SemCor, Most Frequent Sense (MFS): labels
each instance with the MFS in SemCor.9
• SemCor, All Senses: labels each instance with
all SemCor senses, weighting each according
to its frequency in SemCor.
• 1 sense per instance: labels each instance with
a unique induced sense, equivalent to the 1
cluster per instance baseline of the SemEval-
2010 WSI task (Manandhar et al., 2010).
• One sense: labels each instance with the same
induced sense, equivalent to the MFS baseline
of the SemEval-2010 WSI task.
• Most Frequent Sense: labels each instance with
the sense that is most frequently selected by an-
notators for all target word instances.
• Senses Avg.Weighted: labels each instance with
all senses. Each sense is scored according to its
average applicability rating from the gold stan-
dard labelling.
• Senses Eq. Weighted: labels each instance with
all senses, equally weighted.
</listItem>
<footnote confidence="0.7052635">
9http://www.cse.unt.edu/˜rada/downloads.
html#semcor.
</footnote>
<listItem confidence="0.960536">
• 1 of 2 random senses: labels each instance with
one of two randomly selected induced senses.
• 1 of 3 random senses: labels each instance with
one of three randomly selected induced senses.
• 1 of n random senses: labels each instance with
one of n randomly selected induced senses,
</listItem>
<bodyText confidence="0.997251375">
where n is the number of senses for the target
word in WordNet 3.1.10
As noted by the task’s organisers11, the SemCor
scores are the fairest baselines for participating sys-
tems to compare against as they have no knowledge
of the test set sense distribution; the other baselines
are more challenging as they have knowledge of the
test set sense distribution and annotator grading.
</bodyText>
<subsectionHeader confidence="0.999507">
4.1 Summary Analysis of Evaluation Results
</subsectionHeader>
<bodyText confidence="0.9999388">
Given the number of evaluation metrics (16 in total
on the task website), individual analysis of system
results per metric is beyond the scope of this paper.
However, a ranking of systems may be obtained by
taking a summed ranked score; that is, by adding
</bodyText>
<footnote confidence="0.980447833333333">
10For the random senses baselines, induced senses are
mapped to WordNet 3.1 senses using the mapping procedure
described in Agirre and Soroa (2007). The mapping is provided
by the task organisers.
11http://www.cs.york.ac.uk/semeval-2013/
task13/index.php?id=results
</footnote>
<page confidence="0.988195">
692
</page>
<table confidence="0.999902642857143">
System/Baseline Fuzzy NMI Fuzzy B-Cubed Fuzzy B-Cubed Fuzzy B-Cubed
Precision Recall F-Score
Unimelb (50k) 0.060 0.524 0.447 0.483
Unimelb (5p) 0.056 0.470 0.449 0.459
AI-KU 0.065 0.838 0.254 0.390
AI-KU (r5-a1000) 0.039 0.502 0.409 0.451
UoS (top 3) 0.045 0.479 0.420 0.448
UoS (# WN senses) 0.047 0.988 0.112 0.201
AI-KU (a1000) 0.035 0.905 0.194 0.320
One sense 0.000 0.989 0.455 0.623
1 of 2 random senses 0.028 0.495 0.456 0.474
1 of 3 random senses 0.018 0.329 0.455 0.382
1 of n random senses 0.016 0.168 0.451 0.245
1 sense per instance 0.071 0.000 0.000 0.000
</table>
<tableCaption confidence="0.996671">
Table 2: Results for the cluster-based evaluation: all instances.
</tableCaption>
<bodyText confidence="0.999846333333333">
up each system’s rankings over all evaluation met-
rics. The summed ranking finds that UoS (top 3)
is placed first. If the WSD and cluster-based eval-
uations are considered separately, then UoS (top 3)
is ranked, respectively, first and fourth. However,
this result is countered by the relatively poor per-
formance of UoS (# WN senses), being ranked fifth
overall. Considering baselines, UoS (top 3) equals
or surpasses the SemCor baseline scores 67% of the
time, and 54% for the more challenging baselines;
UoS (# WN senses) scores, respectively, 50% and
44%.
All instances results were supplemented with
single-sense (non-graded) and multi-sense (graded)
splits at a later date.12 These results show (again,
using a ranked score) that for single-sense instances,
AI-KU is the best performing system, with UoS (top
3) placed fifth, and UoS (# WN senses) last. Both
UoS (top 3) and UoS (# WN senses) surpass the
SemCor MFS baseline, with UoS (top 3) surpassing
or equalling the harder baselines 79% of the time,
and UoS (# WN senses) 68% of the time. For multi-
sense instances, AI-KU is, again, the best perform-
ing system, with UoS (# WN senses) placed sec-
ond and UoS (top 3) sixth. UoS (top 3) surpasses
or equals the SemCor baseline scores 67% of the
time; UoS (# WN senses) 83% of the time. UoS
(top3) passes/equals, the harder baselines 63% of the
time, with UoS (# WN senses) doing so 67% of the
time. These results are somewhat confounding as
</bodyText>
<footnote confidence="0.9529285">
12http://www.cs.york.ac.uk/semeval-2013/
task13/index.php?id=results (4/4/2013)
</footnote>
<bodyText confidence="0.999973857142857">
one would expect a system that performs well in the
main set of results (all instances), as UoS (top 3)
does, to do so in at least one of the single-sense /
multi-sense splits: this is clearly not the case. In-
deed, the results suggest that UoS (# WN senses),
found to perform poorly over all instances, is better
suited to the task’s aim of finding graded senses.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999977909090909">
This paper presented UoS, a graph-based WSI sys-
tem that participated in SemEval-2013 Task 13:
Word Sense Induction for Graded and Non-Graded
Senses. UoS applied the MaxMax clustering algo-
rithm to find a set of sense clusters in a target word
graph. The number of clusters was found automati-
cally through identification of root vertices of max-
imal quasi-strongly connected subgraphs. Evalua-
tion results showed the UoS (top 3) system to be
the best performing system (all instances), if a sim-
ple ranking over all evaluation measures is applied.
The second system, UoS (# WN senses), performed
poorly, being ranked fifth out of the seven participat-
ing WSI systems. Note, however, that the number of
evaluation metrics applied, and the wide variability
in each system’s performances over different met-
rics and different splits of instance types, make it
difficult to judge exactly which system is the best
performing. Future research therefore aims to carry
out a detailed analysis of the results and to assess
whether the measures applied in the evaluation ade-
quately reflect the performance of WSI systems.
</bodyText>
<page confidence="0.998963">
693
</page>
<sectionHeader confidence="0.995644" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999644521126761">
Eneko Agirre and Aitor Soroa. 2007. SemEval-
2007 Task 02: Evaluating Word Sense Induction
and Discrimination Systems. In Proceedings of the
4th International Workshop on Semantic Evaluations,
pages 7–12. Association for Computational Linguis-
tics. Prague, Czech Republic.
Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A Comparison of Extrinsic Cluster-
ing Evaluation Metrics Based on Formal Constraints.
Information Retrieval, 12(4):461–486.
Gerlof Bouma. 2009. Normalized (Pointwise) Mutual
Information in Collocation Extraction. Proceedings of
GSCL, pages 31–40.
T. Dunning. 1993. Accurate Methods for the Statistics of
Surprise and Coincidence. Computational Linguistics,
19(1):61–74.
Katrin Erk and Diana McCarthy. 2009. Graded Word
Sense Assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1, pages 440–449, Singapore. As-
sociation for Computational Linguistics.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of english.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47–54. Mar-
rakech, Morocco.
David Hope and Bill Keller. 2013. MaxMax: A Graph-
Based Soft Clustering Algorithm Applied to Word
Sense Induction. In A. Gelbukh, editor, CICLing
2013, Part I, LNCS 7816, pages 368–381. Springer-
Verlag Berlin Heidelberg. to appear.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% Solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, pages
57–60. Association for Computational Linguistics.
David Jurgens. 2012. An Evaluation of Graded Sense
Disambiguation Using Word Sense Induction. Pro-
ceedings of *SEM First Joint Conference on Lexi-
cal and Computational Semantics, 2012. Association
for Computational Linguistics, pages 189–198. Mon-
treal,Canada.
Andrea Lancichinetti, Santo Fortunato, and J´anos
Kert´esz. 2009. Detecting the Overlapping and Hier-
archical Community Structure in Complex Networks.
New Journal of Physics, 11(3):033015.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
Task 14: Word Sense Induction and Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63–68. Association for
Computational Linguistics. Uppsala, Sweden.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys (CSUR), 41(2):10.
Roberto Navigli. 2012. A Quick Tour of Word Sense
Disambiguation, Induction and Related Approaches.
In SOFSEM 2012: Theory and Practice of Computer
Science, volume 7147 of Lecture Notes in Computer
Science, pages 115–129. Springer Berlin / Heidelberg.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2006. Introduction to Data Mining. Pearson Addison
Wesley.
K. Thulasiraman and N.S. Swamy. 1992. Graphs: The-
ory and Algorithms. Wiley.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwell.
</reference>
<page confidence="0.998605">
694
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.677032">
<title confidence="0.99998">UoS: A Graph-Based System for Graded Word Sense Induction</title>
<author confidence="0.999821">David Hope</author>
<author confidence="0.999821">Bill</author>
<affiliation confidence="0.8567925">University of Cognitive and Language Processing Systems</affiliation>
<address confidence="0.941701">Brighton, Sussex,</address>
<email confidence="0.999142">davehope@gmail.com,billk@sussex.ac.uk</email>
<abstract confidence="0.9986405">This paper presents UoS, a graph-based Word Sense Induction system which attempts to find all applicable senses of a target word given its context, grading each sense according to its suitability to the context. Senses of a target word are induced through use of a non-parameterised, linear-time clustering algorithm that returns maximal quasi-strongly connected components of a target word graph in which vertex pairs are assigned to the same cluster if either vertex has the highest edge weight to the other. UoS participated in SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses. Two system were submitted; both systems returned results comparable with those of the best performing systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>SemEval2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1767" citStr="Agirre and Soroa, 2007" startWordPosition="277" endWordPosition="280">d Sense Disambiguation (WSD) can only assign pre-defined senses to words on the basis of context, WSI follows the dictum that “The meaning of a word is its use in the language.” (Wittgenstein, 1953) to discover senses through examination of context of use in large text corpora. WSI, therefore, may be applied 1In practice, evaluation of a WSI system requires the use of a gold standard sense inventory such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). to discover new, rare, or domain specific senses; senses undefined in existing sense inventories.2 Previous WSI evaluations (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McCarthy (2009), a graded notion of sense may be more applicable, as multiple senses of the target word may be perceived by readers. The SemEval-2013 WSI evaluation described in this paper is designed to explore the possibility of finding all perceived senses of a target word in a single contextual instance. The aim for participants in the task is therefore to design a system that will induce a set of graded (weighted) sens</context>
<context position="4656" citStr="Agirre and Soroa, 2007" startWordPosition="759" endWordPosition="762">are assigned more than one sense by annotators, thus have graded senses. This figure somewhat detracts from the task’s aim as just 11.62% of the test set can be assigned graded senses. 2.3 Evaluation Measures Systems are evaluated in two ways: (1) in a WSD task and (2), a clustering task. In the first evaluation, systems are assessed by their ability to correctly identify which WordNet 3.1 senses of the target word are applicable in a given instance, and to quantify, and so, rank, senses according to their level of applicability. The supervised evaluation method of previous SemEval WSI tasks (Agirre and Soroa, 2007; Manandhar et al., 2010) is applied to map induced senses to WordNet 3.1 senses, with the mapping function of Jurgens (2012) used to account for the applicability weights. Three evaluation metrics are used - • Jaccard Index: measures the overlap between gold standard senses and those returned by a WSI system. • Positionally-Weighted Kendall’s Tau: measures the ability of a system to rank senses by their applicability. 3Stated as 4664 instances on the task website. Note that the figure of 4806 is for the revised test set. 4http://www.americannationalcorpus.org/ OANC/index.html. • Weighted Norm</context>
<context position="13780" citStr="Agirre and Soroa (2007)" startWordPosition="2329" endWordPosition="2332"> have no knowledge of the test set sense distribution; the other baselines are more challenging as they have knowledge of the test set sense distribution and annotator grading. 4.1 Summary Analysis of Evaluation Results Given the number of evaluation metrics (16 in total on the task website), individual analysis of system results per metric is beyond the scope of this paper. However, a ranking of systems may be obtained by taking a summed ranked score; that is, by adding 10For the random senses baselines, induced senses are mapped to WordNet 3.1 senses using the mapping procedure described in Agirre and Soroa (2007). The mapping is provided by the task organisers. 11http://www.cs.york.ac.uk/semeval-2013/ task13/index.php?id=results 692 System/Baseline Fuzzy NMI Fuzzy B-Cubed Fuzzy B-Cubed Fuzzy B-Cubed Precision Recall F-Score Unimelb (50k) 0.060 0.524 0.447 0.483 Unimelb (5p) 0.056 0.470 0.449 0.459 AI-KU 0.065 0.838 0.254 0.390 AI-KU (r5-a1000) 0.039 0.502 0.409 0.451 UoS (top 3) 0.045 0.479 0.420 0.448 UoS (# WN senses) 0.047 0.988 0.112 0.201 AI-KU (a1000) 0.035 0.905 0.194 0.320 One sense 0.000 0.989 0.455 0.623 1 of 2 random senses 0.028 0.495 0.456 0.474 1 of 3 random senses 0.018 0.329 0.455 0.38</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. SemEval2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7–12. Association for Computational Linguistics. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A Comparison of Extrinsic Clustering Evaluation Metrics Based on Formal Constraints.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>4</issue>
<marker>Amig´o, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A Comparison of Extrinsic Clustering Evaluation Metrics Based on Formal Constraints. Information Retrieval, 12(4):461–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalized (Pointwise) Mutual Information in Collocation Extraction.</title>
<date>2009</date>
<booktitle>Proceedings of GSCL,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="7021" citStr="Bouma, 2009" startWordPosition="1160" endWordPosition="1161">ucting a Target Word Graph A graph G = (V, E) is constructed for each target word. V is a set of vertices and E C_ V x V a set of edges. Each vertex v E V represents a word found in a dependency relation with the target word. Words are extracted from the dependencyparsed version of ukWaC (Ferraresi et al., 2008). In this evaluation V consists of the 300 highest ranked dependency relation words.5 Words are ranked using the Normalised Pointwise Mutual Information 5|V |= 300 was found to return the best results on the trial set over the range IVI = [100, 200, 300, ..., 1000]. 690 (NPMI) measure (Bouma, 2009)6, defined for two words w1, w2 as: p(w1 ,w2 ) NPMI w w 1 ( i� 2) = (log p(w1) p(w2) −log p(w1, w2) ( ) An edge (vi, vj) E E is a pair of vertices. An edge represents a symmetrical relationship between vertices vi and vj; here, that words wi and wj co-occur in ukWaC contexts. Each edge (vi, vj) is assigned a weight w(vi, vj) to quantify the significance of wi, wj co-occurrence, the weight being the value returned by NPMI(wi, wj). 3.2 Clustering the Target Word Graph A clustering algorithm is applied to the target word graph, partitioning it to a set of clusters. Each set of words in a cluster </context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalized (Pointwise) Mutual Information in Collocation Extraction. Proceedings of GSCL, pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="8752" citStr="Dunning, 1993" startWordPosition="1466" endWordPosition="1468">t the weights of all edges incident on vi. Clusters are identified by finding root vertices of quasi-strongly connected (QSC) subgraphs in G&apos; (Thulasiraman and Swamy, 1992). A directed subgraph is said to be QSC if, for any vertices vi and vj, there is a root vertex vk (not necessarily distinct from vi and vj) with a directed path from vk to vi and a directed path from vk to vj.7 3.3 Merging Clusters MaxMax tends to generate many fine-grained sense clusters. Clusters are therefore merged using two measures: cohesion and separation (Tan et al., 6Application of the Log Likelihood Ratio measure (Dunning, 1993) returned the same set of words. Though not required here, NPMI has the useful properties that: if w1 and w2 always co-occur NPMI = 1; if w1 and w2 are distributed as expected under independence NPMI = 0, and if w1 and w2 never occur together, NPMI = −1. 7MaxMax is described in detail in Hope and Keller (2013). 2006). The cohesion of a cluster Ci is defined as: �xECi, yECi |Ci |�(2) Separation between two clusters Ci, Cj is defined as: � � separation(Ci, Cj) = 1 − � (3) Cluster pairs with high cohesion and low separation are merged, the intuition being that words in such pairs will retain a re</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
</authors>
<title>Graded Word Sense Assignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing:</booktitle>
<volume>1</volume>
<pages>440--449</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="1955" citStr="Erk and McCarthy (2009)" startWordPosition="309" endWordPosition="312">tein, 1953) to discover senses through examination of context of use in large text corpora. WSI, therefore, may be applied 1In practice, evaluation of a WSI system requires the use of a gold standard sense inventory such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). to discover new, rare, or domain specific senses; senses undefined in existing sense inventories.2 Previous WSI evaluations (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McCarthy (2009), a graded notion of sense may be more applicable, as multiple senses of the target word may be perceived by readers. The SemEval-2013 WSI evaluation described in this paper is designed to explore the possibility of finding all perceived senses of a target word in a single contextual instance. The aim for participants in the task is therefore to design a system that will induce a set of graded (weighted) senses of a target word in a particular context. The paper is organised as follows: Section 2 introduces SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses; Section 3 </context>
</contexts>
<marker>Erk, McCarthy, 2009</marker>
<rawString>Katrin Erk and Diana McCarthy. 2009. Graded Word Sense Assignment. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1, pages 440–449, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of english.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,</booktitle>
<pages>47--54</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="6722" citStr="Ferraresi et al., 2008" startWordPosition="1106" endWordPosition="1109">r-based, measure, Fuzzy B-Cubed is sensitive to cluster size skew, thus captures the expected performance of a WSI system on a new corpus where the sense distribution is the same. 3 The UoS System The UoS system uses a graph-based model of word co-occurrence to induce target word senses as follows: 3.1 Constructing a Target Word Graph A graph G = (V, E) is constructed for each target word. V is a set of vertices and E C_ V x V a set of edges. Each vertex v E V represents a word found in a dependency relation with the target word. Words are extracted from the dependencyparsed version of ukWaC (Ferraresi et al., 2008). In this evaluation V consists of the 300 highest ranked dependency relation words.5 Words are ranked using the Normalised Pointwise Mutual Information 5|V |= 300 was found to return the best results on the trial set over the range IVI = [100, 200, 300, ..., 1000]. 690 (NPMI) measure (Bouma, 2009)6, defined for two words w1, w2 as: p(w1 ,w2 ) NPMI w w 1 ( i� 2) = (log p(w1) p(w2) −log p(w1, w2) ( ) An edge (vi, vj) E E is a pair of vertices. An edge represents a symmetrical relationship between vertices vi and vj; here, that words wi and wj co-occur in ukWaC contexts. Each edge (vi, vj) is as</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of english. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54. Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hope</author>
<author>Bill Keller</author>
</authors>
<title>MaxMax: A GraphBased Soft Clustering Algorithm Applied to Word Sense Induction. In</title>
<date>2013</date>
<booktitle>CICLing 2013, Part I, LNCS 7816,</booktitle>
<pages>368--381</pages>
<editor>A. Gelbukh, editor,</editor>
<publisher>SpringerVerlag</publisher>
<location>Berlin Heidelberg.</location>
<note>to appear.</note>
<contexts>
<context position="7837" citStr="Hope and Keller, 2013" startWordPosition="1310" endWordPosition="1313">between vertices vi and vj; here, that words wi and wj co-occur in ukWaC contexts. Each edge (vi, vj) is assigned a weight w(vi, vj) to quantify the significance of wi, wj co-occurrence, the weight being the value returned by NPMI(wi, wj). 3.2 Clustering the Target Word Graph A clustering algorithm is applied to the target word graph, partitioning it to a set of clusters. Each set of words in a cluster is taken to represent a sense of the target word. The clustering algorithm applied is MaxMax, a non-parameterised, lineartime algorithm shown to return good results in previous WSI evaluations (Hope and Keller, 2013). MaxMax transforms the weighted, undirected target word graph G into an unweighted, directed graph G&apos;, where edge direction in G&apos; indicates a maximal affinity relationship between two vertices. A vertex vi is said to have maximal affinity to a vertex vj if the edge weight w(vi, vj) is maximal amongst the weights of all edges incident on vi. Clusters are identified by finding root vertices of quasi-strongly connected (QSC) subgraphs in G&apos; (Thulasiraman and Swamy, 1992). A directed subgraph is said to be QSC if, for any vertices vi and vj, there is a root vertex vk (not necessarily distinct fro</context>
<context position="9063" citStr="Hope and Keller (2013)" startWordPosition="1526" endWordPosition="1529"> and vj) with a directed path from vk to vi and a directed path from vk to vj.7 3.3 Merging Clusters MaxMax tends to generate many fine-grained sense clusters. Clusters are therefore merged using two measures: cohesion and separation (Tan et al., 6Application of the Log Likelihood Ratio measure (Dunning, 1993) returned the same set of words. Though not required here, NPMI has the useful properties that: if w1 and w2 always co-occur NPMI = 1; if w1 and w2 are distributed as expected under independence NPMI = 0, and if w1 and w2 never occur together, NPMI = −1. 7MaxMax is described in detail in Hope and Keller (2013). 2006). The cohesion of a cluster Ci is defined as: �xECi, yECi |Ci |�(2) Separation between two clusters Ci, Cj is defined as: � � separation(Ci, Cj) = 1 − � (3) Cluster pairs with high cohesion and low separation are merged, the intuition being that words in such pairs will retain a relatively high degree of semantic similarity. High cohesion is defined as greater than average cohesion. Low separation is defined as a reciprocal relationship between two clusters: if a cluster Ci has the lowest separation to a cluster Cj (out of all clusters) and Cj the lowest separation to Ci, then the two (</context>
</contexts>
<marker>Hope, Keller, 2013</marker>
<rawString>David Hope and Bill Keller. 2013. MaxMax: A GraphBased Soft Clustering Algorithm Applied to Word Sense Induction. In A. Gelbukh, editor, CICLing 2013, Part I, LNCS 7816, pages 368–381. SpringerVerlag Berlin Heidelberg. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% Solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1618" citStr="Hovy et al., 2006" startWordPosition="256" endWordPosition="259">matically discovering word senses from text. In principle, WSI avoids reliance on a pre-defined sense inventory.1 Whereas the related task of Word Sense Disambiguation (WSD) can only assign pre-defined senses to words on the basis of context, WSI follows the dictum that “The meaning of a word is its use in the language.” (Wittgenstein, 1953) to discover senses through examination of context of use in large text corpora. WSI, therefore, may be applied 1In practice, evaluation of a WSI system requires the use of a gold standard sense inventory such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). to discover new, rare, or domain specific senses; senses undefined in existing sense inventories.2 Previous WSI evaluations (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McCarthy (2009), a graded notion of sense may be more applicable, as multiple senses of the target word may be perceived by readers. The SemEval-2013 WSI evaluation described in this paper is designed to explore the possibility of finding all perceived senses of a target word i</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% Solution. In Proceedings of the Human Language Technology Conference of the NAACL, pages 57–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
</authors>
<title>An Evaluation of Graded Sense Disambiguation Using Word Sense Induction.</title>
<date>2012</date>
<booktitle>Proceedings of *SEM First Joint Conference on Lexical and Computational Semantics, 2012. Association for Computational Linguistics,</booktitle>
<pages>189--198</pages>
<publisher>Montreal,Canada.</publisher>
<contexts>
<context position="4781" citStr="Jurgens (2012)" startWordPosition="784" endWordPosition="785">.62% of the test set can be assigned graded senses. 2.3 Evaluation Measures Systems are evaluated in two ways: (1) in a WSD task and (2), a clustering task. In the first evaluation, systems are assessed by their ability to correctly identify which WordNet 3.1 senses of the target word are applicable in a given instance, and to quantify, and so, rank, senses according to their level of applicability. The supervised evaluation method of previous SemEval WSI tasks (Agirre and Soroa, 2007; Manandhar et al., 2010) is applied to map induced senses to WordNet 3.1 senses, with the mapping function of Jurgens (2012) used to account for the applicability weights. Three evaluation metrics are used - • Jaccard Index: measures the overlap between gold standard senses and those returned by a WSI system. • Positionally-Weighted Kendall’s Tau: measures the ability of a system to rank senses by their applicability. 3Stated as 4664 instances on the task website. Note that the figure of 4806 is for the revised test set. 4http://www.americannationalcorpus.org/ OANC/index.html. • Weighted Normalized Discounted Cumulative Gain (NDCG): measures the agreement in applicability ratings, accounting for both the ranking an</context>
</contexts>
<marker>Jurgens, 2012</marker>
<rawString>David Jurgens. 2012. An Evaluation of Graded Sense Disambiguation Using Word Sense Induction. Proceedings of *SEM First Joint Conference on Lexical and Computational Semantics, 2012. Association for Computational Linguistics, pages 189–198. Montreal,Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Lancichinetti</author>
<author>Santo Fortunato</author>
<author>J´anos Kert´esz</author>
</authors>
<title>Detecting the Overlapping and Hierarchical Community Structure in Complex Networks.</title>
<date>2009</date>
<journal>New Journal of Physics,</journal>
<volume>11</volume>
<issue>3</issue>
<marker>Lancichinetti, Fortunato, Kert´esz, 2009</marker>
<rawString>Andrea Lancichinetti, Santo Fortunato, and J´anos Kert´esz. 2009. Detecting the Overlapping and Hierarchical Community Structure in Complex Networks. New Journal of Physics, 11(3):033015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer S Pradhan</author>
</authors>
<title>SemEval-2010 Task 14: Word Sense Induction and Disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<location>Uppsala,</location>
<contexts>
<context position="1792" citStr="Manandhar et al., 2010" startWordPosition="281" endWordPosition="284">WSD) can only assign pre-defined senses to words on the basis of context, WSI follows the dictum that “The meaning of a word is its use in the language.” (Wittgenstein, 1953) to discover senses through examination of context of use in large text corpora. WSI, therefore, may be applied 1In practice, evaluation of a WSI system requires the use of a gold standard sense inventory such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). to discover new, rare, or domain specific senses; senses undefined in existing sense inventories.2 Previous WSI evaluations (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McCarthy (2009), a graded notion of sense may be more applicable, as multiple senses of the target word may be perceived by readers. The SemEval-2013 WSI evaluation described in this paper is designed to explore the possibility of finding all perceived senses of a target word in a single contextual instance. The aim for participants in the task is therefore to design a system that will induce a set of graded (weighted) senses of a target word in a </context>
<context position="4681" citStr="Manandhar et al., 2010" startWordPosition="763" endWordPosition="766">ne sense by annotators, thus have graded senses. This figure somewhat detracts from the task’s aim as just 11.62% of the test set can be assigned graded senses. 2.3 Evaluation Measures Systems are evaluated in two ways: (1) in a WSD task and (2), a clustering task. In the first evaluation, systems are assessed by their ability to correctly identify which WordNet 3.1 senses of the target word are applicable in a given instance, and to quantify, and so, rank, senses according to their level of applicability. The supervised evaluation method of previous SemEval WSI tasks (Agirre and Soroa, 2007; Manandhar et al., 2010) is applied to map induced senses to WordNet 3.1 senses, with the mapping function of Jurgens (2012) used to account for the applicability weights. Three evaluation metrics are used - • Jaccard Index: measures the overlap between gold standard senses and those returned by a WSI system. • Positionally-Weighted Kendall’s Tau: measures the ability of a system to rank senses by their applicability. 3Stated as 4664 instances on the task website. Note that the figure of 4806 is for the revised test set. 4http://www.americannationalcorpus.org/ OANC/index.html. • Weighted Normalized Discounted Cumulat</context>
<context position="12111" citStr="Manandhar et al., 2010" startWordPosition="2057" endWordPosition="2060">287 1 of 3 random senses 0.244 0.633 0.287 1 of n random senses 0.290 0.638 0.286 1 sense per instance 0.000 0.945 0.000 SemCor, MFS 0.455 0.465 0.339 SemCor, All Senses 0.149 0.559 0.489 Table 1: Results for the WSD evaluation: all instances. • SemCor, Most Frequent Sense (MFS): labels each instance with the MFS in SemCor.9 • SemCor, All Senses: labels each instance with all SemCor senses, weighting each according to its frequency in SemCor. • 1 sense per instance: labels each instance with a unique induced sense, equivalent to the 1 cluster per instance baseline of the SemEval2010 WSI task (Manandhar et al., 2010). • One sense: labels each instance with the same induced sense, equivalent to the MFS baseline of the SemEval-2010 WSI task. • Most Frequent Sense: labels each instance with the sense that is most frequently selected by annotators for all target word instances. • Senses Avg.Weighted: labels each instance with all senses. Each sense is scored according to its average applicability rating from the gold standard labelling. • Senses Eq. Weighted: labels each instance with all senses, equally weighted. 9http://www.cse.unt.edu/˜rada/downloads. html#semcor. • 1 of 2 random senses: labels each instan</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach, and Sameer S. Pradhan. 2010. SemEval-2010 Task 14: Word Sense Induction and Disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68. Association for Computational Linguistics. Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1585" citStr="Miller et al., 1990" startWordPosition="250" endWordPosition="253">Induction (WSI) is the task of automatically discovering word senses from text. In principle, WSI avoids reliance on a pre-defined sense inventory.1 Whereas the related task of Word Sense Disambiguation (WSD) can only assign pre-defined senses to words on the basis of context, WSI follows the dictum that “The meaning of a word is its use in the language.” (Wittgenstein, 1953) to discover senses through examination of context of use in large text corpora. WSI, therefore, may be applied 1In practice, evaluation of a WSI system requires the use of a gold standard sense inventory such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). to discover new, rare, or domain specific senses; senses undefined in existing sense inventories.2 Previous WSI evaluations (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McCarthy (2009), a graded notion of sense may be more applicable, as multiple senses of the target word may be perceived by readers. The SemEval-2013 WSI evaluation described in this paper is designed to explore the possibility of finding all pe</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An On-line Lexical Database. International Journal of Lexicography, 3(4):235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A Survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="3091" citStr="Navigli (2009)" startWordPosition="510" endWordPosition="511">013 Task 13: Word Sense Induction for Graded and Non-Graded Senses; Section 3 presents UoS, the system that participated in the task; Section 4 reports evaluation results, showing that UoS returns scores comparable with those of the best performing systems. 2 SemEval-2013 Task 13 2.1 Aim The aim for participants in SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses is to construct a system that will: (1) induce the senses of a given set of target words and (2), label each test set context (instance) of a target word with 2Surveys of WSI and WSD approaches are found in Navigli (2009) and Navigli (2012). 689 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 689–694, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics all applicable target word senses. Candidate senses are drawn from the WordNet 3.1 sense inventory. Systems must therefore return a set of graded senses for each target word in a particular context, where a numeric weight signifies (grades) each sense’s applicability to the context. A non-graded sense is simply the highest</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word Sense Disambiguation: A Survey. ACM Computing Surveys (CSUR), 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A Quick Tour of Word Sense Disambiguation, Induction and Related Approaches.</title>
<date>2012</date>
<booktitle>In SOFSEM 2012: Theory and Practice of Computer Science,</booktitle>
<volume>7147</volume>
<pages>115--129</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="3110" citStr="Navigli (2012)" startWordPosition="513" endWordPosition="514">ense Induction for Graded and Non-Graded Senses; Section 3 presents UoS, the system that participated in the task; Section 4 reports evaluation results, showing that UoS returns scores comparable with those of the best performing systems. 2 SemEval-2013 Task 13 2.1 Aim The aim for participants in SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses is to construct a system that will: (1) induce the senses of a given set of target words and (2), label each test set context (instance) of a target word with 2Surveys of WSI and WSD approaches are found in Navigli (2009) and Navigli (2012). 689 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 689–694, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics all applicable target word senses. Candidate senses are drawn from the WordNet 3.1 sense inventory. Systems must therefore return a set of graded senses for each target word in a particular context, where a numeric weight signifies (grades) each sense’s applicability to the context. A non-graded sense is simply the highest graded (weighted) </context>
</contexts>
<marker>Navigli, 2012</marker>
<rawString>Roberto Navigli. 2012. A Quick Tour of Word Sense Disambiguation, Induction and Related Approaches. In SOFSEM 2012: Theory and Practice of Computer Science, volume 7147 of Lecture Notes in Computer Science, pages 115–129. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pang-Ning Tan</author>
<author>Michael Steinbach</author>
<author>Vipin Kumar</author>
</authors>
<title>Introduction to Data Mining.</title>
<date>2006</date>
<publisher>Pearson Addison Wesley.</publisher>
<marker>Tan, Steinbach, Kumar, 2006</marker>
<rawString>Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. 2006. Introduction to Data Mining. Pearson Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Thulasiraman</author>
<author>N S Swamy</author>
</authors>
<title>Graphs: Theory and Algorithms.</title>
<date>1992</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="8310" citStr="Thulasiraman and Swamy, 1992" startWordPosition="1387" endWordPosition="1390">ring algorithm applied is MaxMax, a non-parameterised, lineartime algorithm shown to return good results in previous WSI evaluations (Hope and Keller, 2013). MaxMax transforms the weighted, undirected target word graph G into an unweighted, directed graph G&apos;, where edge direction in G&apos; indicates a maximal affinity relationship between two vertices. A vertex vi is said to have maximal affinity to a vertex vj if the edge weight w(vi, vj) is maximal amongst the weights of all edges incident on vi. Clusters are identified by finding root vertices of quasi-strongly connected (QSC) subgraphs in G&apos; (Thulasiraman and Swamy, 1992). A directed subgraph is said to be QSC if, for any vertices vi and vj, there is a root vertex vk (not necessarily distinct from vi and vj) with a directed path from vk to vi and a directed path from vk to vj.7 3.3 Merging Clusters MaxMax tends to generate many fine-grained sense clusters. Clusters are therefore merged using two measures: cohesion and separation (Tan et al., 6Application of the Log Likelihood Ratio measure (Dunning, 1993) returned the same set of words. Though not required here, NPMI has the useful properties that: if w1 and w2 always co-occur NPMI = 1; if w1 and w2 are distri</context>
</contexts>
<marker>Thulasiraman, Swamy, 1992</marker>
<rawString>K. Thulasiraman and N.S. Swamy. 1992. Graphs: Theory and Algorithms. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludwig Wittgenstein</author>
</authors>
<title>Philosophical Investigations.</title>
<date>1953</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="1343" citStr="Wittgenstein, 1953" startWordPosition="210" endWordPosition="211">other. UoS participated in SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses. Two system were submitted; both systems returned results comparable with those of the best performing systems. 1 Introduction Word Sense Induction (WSI) is the task of automatically discovering word senses from text. In principle, WSI avoids reliance on a pre-defined sense inventory.1 Whereas the related task of Word Sense Disambiguation (WSD) can only assign pre-defined senses to words on the basis of context, WSI follows the dictum that “The meaning of a word is its use in the language.” (Wittgenstein, 1953) to discover senses through examination of context of use in large text corpora. WSI, therefore, may be applied 1In practice, evaluation of a WSI system requires the use of a gold standard sense inventory such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). to discover new, rare, or domain specific senses; senses undefined in existing sense inventories.2 Previous WSI evaluations (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McC</context>
</contexts>
<marker>Wittgenstein, 1953</marker>
<rawString>Ludwig Wittgenstein. 1953. Philosophical Investigations. Blackwell.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>