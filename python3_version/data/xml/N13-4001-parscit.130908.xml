<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.164719">
<title confidence="0.965147">
Deep Learning for NLP (without Magic)
</title>
<author confidence="0.984083">
Richard Socher, Chris Manning
</author>
<affiliation confidence="0.522787">
Stanford University
</affiliation>
<email confidence="0.991979">
richard@socher.org
manning@stanford.edu
</email>
<sectionHeader confidence="0.999255" genericHeader="abstract">
1 Overview
</sectionHeader>
<bodyText confidence="0.999946961538462">
Machine learning is everywhere in today’s NLP, but by and large machine learning
amounts to numerical optimization of weights for human designed representations
and features. The goal of deep learning is to explore how computers can take ad-
vantage of data to develop features and representations appropriate for complex
interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models
and learning algorithms in deep learning for natural language processing. Recently,
these methods have been shown to perform very well on various NLP tasks such
as language modeling, POS tagging, named entity recognition, sentiment analysis
and paraphrase detection, among others. The most attractive quality of these tech-
niques is that they can perform well without any external hand-designed resources
or time-intensive feature engineering. Despite these advantages, many researchers
in NLP are not familiar with these methods. Our focus is on insight and understand-
ing, using graphical illustrations and simple, intuitive derivations. The goal of the
tutorial is to make the inner workings of these techniques transparent, intuitive and
their results interpretable, rather than black boxes labeled ”magic here”. The first
part of the tutorial presents the basics of neural networks, neural word vectors, sev-
eral simple models based on local windows and the math and algorithms of training
via backpropagation. In this section applications include language modeling and
POS tagging. In the second section we present recursive neural networks which
can learn structured tree outputs as well as vector representations for phrases and
sentences. We cover both equations as well as applications. We show how training
can be achieved by a modified version of the backpropagation algorithm intro-
duced before. These modifications allow the algorithm to work on tree structures.
Applications include sentiment analysis and paraphrase detection. We also draw
connections to recent work in semantic compositionality in vector spaces. The
principle goal, again, is to make these methods appear intuitive and interpretable
</bodyText>
<page confidence="0.66381">
1
</page>
<bodyText confidence="0.978366875">
Tutorials, NAACL-HLT 2013, pages 1–3,
Atlanta, Georgia, June 9 2013. c�2013 Association for Computational Linguistics
rather than mathematically confusing. By this point in the tutorial, the audience
members should have a clear understanding of how to build a deep learning system
for word-, sentence- and document-level tasks. The last part of the tutorial gives
a general overview of the different applications of deep learning in NLP, includ-
ing bag of words models. We will provide a discussion of NLP-oriented issues in
modeling, interpretation, representational power, and optimization.
</bodyText>
<sectionHeader confidence="0.993675" genericHeader="keywords">
2 Outline
</sectionHeader>
<subsectionHeader confidence="0.640428">
Part I: The Basics
</subsectionHeader>
<listItem confidence="0.992163466666667">
• Motivation
• From logistic regression to neural networks
• Theory: Backpropagation training
• Applications: Word vector learning, POS, NER
• Unsupervised pre-training, multi-task learning, and learning relations
PART II: Recursive Neural Networks
• Motivation
• Definition of RNNs
• Theory: Backpropagation through structure
• Applications: Sentiment Analysis, Paraphrase detection, Relation Classifi-
cation
PART III: Applications and Discussion
• Overview of various NLP applications
• Efficient reconstruction or prediction of high-dimensional sparse vectors
• Discussion of future directions, advantages and limitations
</listItem>
<page confidence="0.995567">
2
</page>
<sectionHeader confidence="0.98854" genericHeader="introduction">
3 Speaker Bios
</sectionHeader>
<bodyText confidence="0.9987905">
Richard Socher1 is a PhD student at Stanford working with Chris Manning and
Andrew Ng. His research interests are machine learning for NLP and vision. He
is interested in developing new models that learn useful features, capture composi-
tional and hierarchical structure in multiple modalities and perform well across dif-
ferent tasks. He was awarded the 2011 Yahoo! Key Scientific Challenges Award,
the Distinguished Application Paper Award at ICML 2011 and a Microsoft Re-
search PhD Fellowship in 2012.
Christopher Manning2 is an Associate Professor of Computer Science and
Linguistics at Stanford University (PhD, Stanford, 1995). Manning has coauthored
leading textbooks on statistical approaches to NLP (Manning and Schuetze 1999)
and information retrieval (Manning et al. 2008). His recent work concentrates on
machine learning and natural language processing, including applications such as
statistical parsing and text understanding, joint probabilistic inference, clustering,
and deep learning over text and images.
</bodyText>
<footnote confidence="0.9988985">
1http://www.socher.org/
2http://nlp.stanford.edu/˜manning/
</footnote>
<page confidence="0.994273">
3
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.034140">
<title confidence="0.991182">Deep Learning for NLP (without Magic)</title>
<author confidence="0.7669895">Richard Socher</author>
<author confidence="0.7669895">Chris Stanford</author>
<email confidence="0.984676">manning@stanford.edu</email>
<abstract confidence="0.986110055555556">1 Overview Machine learning is everywhere in today’s NLP, but by and large machine learning amounts to numerical optimization of weights for human designed representations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models and learning algorithms in deep learning for natural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tagging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature engineering. Despite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustrations and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their results interpretable, rather than black boxes labeled ”magic here”. The first part of the tutorial presents the basics of neural networks, neural word vectors, several simple models based on local windows and the math and algorithms of training via backpropagation. In this section applications include language modeling and POS tagging. In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sentences. We cover both equations as well as applications. We show how training can be achieved by a modified version of the backpropagation algorithm introduced before. These modifications allow the algorithm to work on tree structures. Applications include sentiment analysis and paraphrase detection. We also draw connections to recent work in semantic compositionality in vector spaces. The principle goal, again, is to make these methods appear intuitive and interpretable 1 NAACL-HLT pages 1–3, Georgia, June 9 2013. Association for Computational Linguistics rather than mathematically confusing. By this point in the tutorial, the audience members should have a clear understanding of how to build a deep learning system for word-, sentenceand document-level tasks. The last part of the tutorial gives a general overview of the different applications of deep learning in NLP, including bag of words models. We will provide a discussion of NLP-oriented issues in modeling, interpretation, representational power, and optimization.</abstract>
<note confidence="0.527987">2 Outline Part I: The Basics</note>
<title confidence="0.968358769230769">Motivation • From logistic regression to neural networks • Theory: Backpropagation training • Applications: Word vector learning, POS, NER • Unsupervised pre-training, multi-task learning, and learning relations PART II: Recursive Neural Networks • Motivation • Definition of RNNs • Theory: Backpropagation through structure Applications: Sentiment Analysis, Paraphrase detection, Relation Classification PART III: Applications and Discussion • Overview of various NLP applications</title>
<abstract confidence="0.930149111111111">Efficient reconstruction or prediction of high-dimensional sparse vectors • Discussion of future directions, advantages and limitations 2 3 Speaker Bios is a PhD student at Stanford working with Chris Manning and Andrew Ng. His research interests are machine learning for NLP and vision. He is interested in developing new models that learn useful features, capture compositional and hierarchical structure in multiple modalities and perform well across different tasks. He was awarded the 2011 Yahoo! Key Scientific Challenges Award, the Distinguished Application Paper Award at ICML 2011 and a Microsoft Research PhD Fellowship in 2012. is an Associate Professor of Computer Science and Linguistics at Stanford University (PhD, Stanford, 1995). Manning has coauthored leading textbooks on statistical approaches to NLP (Manning and Schuetze 1999) and information retrieval (Manning et al. 2008). His recent work concentrates on machine learning and natural language processing, including applications such as statistical parsing and text understanding, joint probabilistic inference, clustering, and deep learning over text and images.</abstract>
<intro confidence="0.934138">3</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>