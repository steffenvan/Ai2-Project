<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9692345">
Inheritance and Constraint-Based
Grammar Formalisms
</title>
<author confidence="0.726186">
Remi Zajac*t
</author>
<affiliation confidence="0.230577">
Project P OLYGLOSS
</affiliation>
<bodyText confidence="0.998766666666667">
We describe an approach to unification grammars that integrates two paradigms: the object-
oriented approach, which offers multiple inheritance, complex objects with role-value restrictions
and role-values equality, querying as subsumption; the relational programming approach, which
offers declarativity, logical variables, nondeterminism with backtracking, and existential queries.
This approach is embodied in a constraint-based object-oriented formalism. The interpreter of the
formalism is described as a term rewriting system based on unification of typed feature structures.
The grammar writer organizes unification grammars as inheritance networks of typed feature
structures. Complex linguistic structures are described by means of recursive type constraints. We
illustrate the use of inheritance networks with two examples: an HPSG example where implication
(as used for &amp;quot;principles&amp;quot;) is modeled using inheritance and an example of bilingual transfer where
the minimal amount of information needed for the translation is specified at different levels of
generalization.
</bodyText>
<sectionHeader confidence="0.980968" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999848176470588">
Ideally, a linguistic formalism combining the best of the object-oriented approach and
the unification-based approach would be realized in a constraint-based architecture for
an object-oriented language based on inheritance, feature structures, and unification.
The Typed Feature Structure language (TFS) is an attempt to provide a synthesis
of several key concepts stemming from unification-based grammar formalisms (fea-
ture structure: Kay 1984) knowledge representation languages (inheritance), and logic
programming (narrowing). The formalism supports an object-oriented style based on
abstraction and generalization through inheritance; it is a fully declarative formalism
based on unification of typed feature structures. It is flexible and has enough expres-
sive power to support various kinds of linguistic theories, not necessarily based on
constituency&apos;.
The use of an object-oriented methodology for natural language processing is
very attractive, and the use of inheritance offers a number of advantages such as ab-
straction and generalization, information sharing and default reasoning, and modular-
ity and reusability (Daelemans 1990). Inheritance-based descriptions are already used
in computational linguistics: linguistic theories such as Systemic Functional Gram-
mar (Halliday 1985), Word Grammar (Fraser and Hudson 1990), or HPSG (Pollard
</bodyText>
<footnote confidence="0.96185275">
* IMS-CL/Ifl-AIS, University of Stuttgart, Azenbergstrage 12, D-W-7000 Stuttgart 1. E-mail:
zajac@informatik.uni-stuttgart.de.
t Research reported in this paper is partly supported by the German Ministry of Research and
Technology (BMFT, Bundesminister fur Forschung und Technologie), under grant No. 08 B3116 3. The
views and conclusions contained herein are those of the author and should not be interpreted as
representing official policies.
1 In particular, it allows a direct implementation of HPSG-style grammars (Emele 1988): HPSG is so far
the only linguistic theory based on both inheritance and feature structures.
</footnote>
<note confidence="0.46384">
0 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.999054234042553">
and Sag 1987) make use of inheritance to describe linguistic structures at the lexical,
morphological, syntactic, or semantic (conceptual) levels. These theories are usually
directly implemented in object-oriented programming languages (e.g., Loom in the
case of the PENMAN system [Mann and Matthiessen 1985]), but there is a growing
number of linguistic formalisms used for specific purposes, e.g., DATR (Evans and
Gazdar 1989) for the lexicon.
On the other hand, current linguistic theories such as LFG, UCG, HPSG, and some
formalisms for linguistic description such as FUG or PATR-II are based on the notion
of partial information: linguistic structures are described using feature structures that
give partial information about the object being modeled, a linguistic structure being
described by a set of feature structures that mutually constrain the description. Feature
structures are partially ordered according to a subsumption ordering interpreted as an
ordering on the amount of conveyed information; the combination of information is
defined as the unification of feature structures. Formalisms based on feature structure
and unification are declarative, and they can be given a sound formal semantics.
Combining object-oriented approaches to linguistic description with unification-
based grammar formalisms, as in HPSG, is very attractive. On one hand, we gain the
advantages of the object-oriented approach: abstraction and generalization through
the use of inheritance. On the other hand, we gain a fully declarative framework,
with all the advantages of logical formalisms: expressive power, simplicity, and sound
formal semantics. To arrive at such a result, we have to enrich the formalism of feature
structures with the notion of inheritance and abandon some of the procedural features
of object-oriented languages in order to gain referential transparency.
Referential transparency is one of the characteristic properties of declarative lan-
guages (Stoy 1977), where the meaning of each language construct is given by a few
simple and general rules. For example, the value of a variable should be independent
from its position within the scope of its declaration. This is true for PROLOG variables
inside a clause, but not for PASCAL or LISP variables that make use of assignment. A
higher level example is the meaning of a procedure: it is not transparent if the pro-
cedure makes use of global variables that are set by some other procedure. Similarly,
the meaning of a PROLOG predicate should be transparent because there is no global
variable, but a predicate definition might be modified during execution by imperative
predicates such as assert and retract, thus destroying the referential transparency
of pure PROLOG.
Clearly, most of the object-oriented languages lack referential transparency in sev-
eral ways, using for example procedural attachments for object methods. Another
example is the use of nonmonotonic inheritance, which is advocated in computa-
tional linguistics by, for example, Evans and Gazdar 1989; Bouma 1990; De Smedt and
de Graaf 1990; and Fraser and Hudson 1990. Nonmonotonic inheritance is seen as
a practical device designed to deal with exceptions, but such a feature goes against
generality and referential transparency. Furthermore, as expressed by Etherington et
al. 1989, a still unresolved issue in nonmonotonic reasoning is the issue of
... scaling formal non-monotonic theories up to real problems (merely
a formality?). Most existant theories are intractable—some don&apos;t have
even a proof theory—and it is often difficult to tell how large bodies
of information will (or even should) interact.
Given the complexity of the state of the art in nonmonotonic reasoning and the lack of
</bodyText>
<page confidence="0.995564">
160
</page>
<note confidence="0.980976">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
</note>
<bodyText confidence="0.998342774193548">
a basic commonly agreed formalization,&apos; the issue of nonmonotonicity is not addressed
in the work described in this article.
Knowledge representation languages are evolving toward more declarativity, as
exemplified by the evolution from KL-ONE (Brachman and Schmolze 1985) to lan-
guages such as CLASSIC (Borgida et al. 1989) or Loom (MacGregor 1988, 1990). The
terminological component describing the objects (the data model of object-oriented
database systems) has always been more declarative than the assertional component
(procedural attachment or methods), and the current trend is to integrate those two
components more closely, where the assertional component is some kind of rule-based
system, as in Loom (Yen, Neches, and MacGregor 1988).
Typed feature structures are very similar to structured objects of object-oriented
languages and to conceptual structures of knowledge representation languages. Thus,
typed feature structures have the potential to act as a lingua franca for both compu-
tational linguistics and artificial intelligence, and this should ease the communication
between those two worlds. Since conceptual structures are used for example in text
generation (Bourbeau et al. 1990) or knowledge-based machine translation (Nirenburg
et al. 1992), typed feature structures provide an attractive alternative to current proce-
dural implementations.
In Section 2, we present a language that combines the notions of partial information
and inheritance in a fully declarative framework. It is based on feature structures
augmented with the notion of types, which are organized into an inheritance network.
Using types, it is possible to define structured domains of feature structures and to
classify feature structures. Logical conditions are attached to types, akin to method
attachment, but in a fully declarative framework. Recursivity is an integral part of
the language, giving the necessary expressive power for describing complex recursive
linguistic structures. We end the section by an overview of the TFS abstract rewrite
machine used for computing descriptions of the meaning of typed feature structures.3
Section 3 describes the use of inheritance in two examples of unification grammars
using the TFS formalism: an HPSG grammar for a fragment of English and an LFG-
style transfer grammar for a small machine translation problem between English and
French.
</bodyText>
<sectionHeader confidence="0.9726" genericHeader="method">
2. Inheritance Networks of Typed Feature Structures
</sectionHeader>
<bodyText confidence="0.9998609">
Assume the existence of an (abstract) informational domain 1., for example, the set
of linguistic objects. Feature structures describe objects of this universe by specifying
values for attributes of objects and equality constraints between some values. More
precisely, as feature structures can provide only partial information about the objects
they describe, a feature structure denotes a set of objects in this universe. This set
could be a singleton set, for example, in the case of atomic feature structures. Feature
structures are ordered by a subsumption relation: a feature structure h subsumes
another feature structure f2 iff h provides the same or less information than f2: fi &gt; f2. In
our universe, this means that the set described by fi is a superset of the set described
by f2. Note that there can be feature structures that cannot consistently describe the
</bodyText>
<footnote confidence="0.9915635">
2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature
structures are derived, plays this role in formal accounts of feature structures.
3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been
implemented at the University of Stuttgart by Martin Emele and the author and has been used to test
several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al.
1990; Zajac 1990a; Bateman and Momma 19911.
</footnote>
<page confidence="0.988907">
161
</page>
<note confidence="0.827839">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.999959653846154">
same objects. For example, a feature structure describing verb phrases and a feature
structure describing noun phrases are not consistent: the intersection of the sets they
denote is usually empty.
As different sets of attribute-value pairs make sense for different kinds of objects,
we also divide our feature structures into different types. These types are ordered by
a subtype relation: a type t2 is a subtype of another type t1 if t2 provides at least as much
information as t1. For example, assuming that a verb phrase is a phrase, then the set
of verb phrases is included in the set of phrases. Using types to model this taxonomic
hierarchy, the type symbol VP denotes the set of verb phrases, the symbol PH denotes
the set of phrases, and we define VP as a subtype of PH.
This description implies that, if we know that a linguistic object is a verb phrase,
we can deduce that it is a phrase. This deduction mechanism is expressed in our
type system as type inheritance. Furthermore, with each type we associate constraints
expressed as feature structures, thereby defining an inheritance network of typed fea-
ture structures: if a feature structure is of type t and there exist supertypes of t, then t
inherits all the attribute-value pairs and equality constraints of the feature structures
associated with all the supertypes of t.
Computation is performed by a typed feature structure machine capable of check-
ing a set of type constraints defined as an inheritance network of typed feature struc-
tures. Given a typed feature structure inheritance network, we query the machine by
asking if some feature structure satisfies the constraints defined by the network. To
produce the answer, the system proceeds by gradually adding the constraints that
should be satisfied by the query: an answer will be a set of feature structures where
each feature structure is subsumed by the query and where all the type constraints of
the network hold on all substructures of the elements of the answer. The answer is the
empty set when the query does not satisfy the constraints defined by the network.
</bodyText>
<subsectionHeader confidence="0.712329">
Related work.
</subsectionHeader>
<bodyText confidence="0.999983307692308">
The basic approach described in this section is based on original work by Ait-Kaci
(1984, 1986) on the KBL language and has also been influenced by the work on HPSG
by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing lit-
erature on the semantics of feature structures, many relevant results and techniques
have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and Alt-
Kaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard
and Moshier (1990), a computational formalism, very close to the TFS formalism, is
currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990).
The early work presented in Emele and Zajac (1989a) was an attempt to directly
implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was
already a departure from Ait-Kaci&apos;s KBL language, implementing a different evalua-
tion strategy that allowed the use of cyclic feature structures. Compared with the KBL
language, the current TFS language
</bodyText>
<listItem confidence="0.999060571428571">
• allows the use of cyclic feature structures,
• has a simple operational semantics implementing an inheritance rule and
a specialization rule,
• uses a lazy evaluation scheme for the evaluation of constraints,
• implements static coherence checks,
• has a simple syntax distinguishing the definition of the partial order on
type symbols, and the definition of constraints associated to types.
</listItem>
<page confidence="0.997341">
162
</page>
<note confidence="0.87282">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
</note>
<subsectionHeader confidence="0.732075">
2.1 Types
</subsectionHeader>
<bodyText confidence="0.999256357142857">
In the following presentation, we adopt an algebraic approach based on lattice theory
(Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for
example, a proof-theoretical approach using an adaptation of a feature logic (Rounds
and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of
these different models, as this is done for the LIFE language in Ait-Kaci and Podelski
(1991). The presentation is nevertheless rather informal, and a more technical account
can be found in Emele and Zajac (1990a) and Zajac (1990b).
The universe of feature structures is structured in an inheritance network that
defines a partial ordering on kinds of available information. The backbone of the
network is defined by a finite set of type symbols T together with a partial ordering
&lt; on T: the partially ordered set (poset) (T ,&lt;). The ordering &lt; defines the subtype
relation: for A, B E T we read A &lt; B as &amp;quot;A is a subtype of B.&amp;quot; We call the smallest
types of T the minimal types.
To have a well-behaved type hierarchy, we require that (T,&lt;) be such that:
</bodyText>
<listItem confidence="0.8740558">
• T contains the symbols T and I, where T is the greatest element and I
is the least element of T.4
• any two type symbols A and B of T have a greatest common lower
bound written glbfA, B}. A poset where greatest common lower bounds
exist is a meet semi-lattice: we introduce a new operation
</listItem>
<bodyText confidence="0.9860234">
A A B = glb{A, B}, where A A B is called the meet of A and B.
Since we allow the user to specify any finite poset, a technicality arises when
two types do not have a unique greatest common lower bound: in that case, the set
of greatest common lower bounds is interpreted disjunctively using the following
powerlattice construction, which preserves the ordering and the existing meets.
The poset (T, &lt;) is embedded in (crowns(T), EH). The set crowns(T) is the set
of all nonempty subsets of incomparable elements of T (the &amp;quot;crowns&amp;quot; of T). These
subsets are partially ordered by the Hoare ordering EH: VX, Y E crowns(T), X EH Y
iff Vx E X, 3y E Y such that x &lt;y.
The canonical injection of T in crowns(T), which takes any element x of T into
the singleton {x} trivially preserves the ordering: {X} CH {y} iff x &lt; y.
The meet between two elements X and Y of crowns(T), X Fl Y, is defined as the
union of the intersection of each of the principal ideals generated by the elements of X
with each of the principal ideals generated by the elements of Y and then extracting the
maximal elements. It is easy to see that existing meets are preserved: {z} = {x} n {y}
iffz=xAy.
Some meets are added, as in the following example. Let ({a, b, c, d} , &lt;) be a poset
where d &lt;b, d &lt; a, c &lt;b, and c &lt;a (this poset is represented using a Hasse diagram
in Figure 1). The meet a A b does not exist, but {a} n {b} {c, d}, and this meet is
interpreted disjunctively.
The join between two elements X and Y of crowns(T), X U Y, is defined as the set
of maximal elements of the union of X and Y. It can be shown that, equipped with
H and LI, crowns (T), EH) is a distributive lattice. This construction is carried over to
typed feature structures, with the property that this definition of the join does not lose
information, contrary to the strict generalization of feature structures: this definition of
</bodyText>
<page confidence="0.7882365">
4 T represents underspecified information, and I represents inconsistent information.
163
</page>
<figure confidence="0.857097">
Computational Linguistics Volume 18, Number 2
</figure>
<figureCaption confidence="0.99401">
Figure 1
</figureCaption>
<bodyText confidence="0.993413">
A poset and the set of the principal ideals generated by the elements of the poset ordered by
set inclusion.
the join is appropriate to represent disjunctive information as, for example, generated
by a nondeterministic computation (see Section 2.4).
This powerlattice construction is completely transparent to the user, and to sim-
plify the presentation, we will assume in the following that (T ,&lt;) is a meet semi-
lattice.
</bodyText>
<subsectionHeader confidence="0.997825">
2.2 Feature Structures
</subsectionHeader>
<bodyText confidence="0.9692276">
We use the attribute-value matrix (AVM) notation for feature structures, and we write
the type symbol for each feature structure in front of the opening square bracket of the
AVM. In the remainder of this section, we shall implicitly refer to some given signature
(T, &lt;,F) where (T ,&lt;,) is a type hierarchy and F is a set of feature symbols, and we
shall also assume a set of variables V.
A typed feature structure t is then an expression of the form
where la is a variable in a set of variables V. A is a type symbol in T, (with
n &gt; 0) are features in .F, and ti,..., tn are typed feature structures.
We have to add some restrictions that capture properties commonly associated
with feature structures:
</bodyText>
<listItem confidence="0.978607125">
1. A feature is a selector that gives access to a substructure: it has to be
unique for a given feature structure.
2. 1 represents inconsistent information: it is not allowed in a feature
structure.
3. A variable is used to capture equality constraints (&amp;quot;reentrancy&amp;quot;) in the
feature structure, and the shared value is represented only once: there is
at most one occurrence of a variable Ei that is the root of a structure
different from T.
</listItem>
<bodyText confidence="0.9676235">
Given a signature (T, &lt;, F), feature structures are partially ordered by a subsump-
tion relation. This captures the intuitive notion that a feature structure t&apos; containing
</bodyText>
<page confidence="0.998521">
164
</page>
<note confidence="0.826739">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
</note>
<bodyText confidence="0.9123975">
more information than a feature structure t is more specific than t. A feature structure
t subsumes a structure t&apos;, t &gt; t&apos; iff:
</bodyText>
<listItem confidence="0.9970255">
1. all paths in t are in t&apos;;
2. all equality constraints in t hold in t&apos;.
3. for a given path in t, its type is greater or equal than the corresponding
type in t&apos;.
</listItem>
<bodyText confidence="0.9999645">
Since we have a partial order on feature structures, the meet operation between
two feature structures t and t&apos; is defined in the usual way as the greatest common
lower bound of t and t&apos;. It is computed using a typed unification algorithm. A feature
structure is represented as a graph where each node has a type, an equivalence class
used to represent equational constraints (&amp;quot;co-references&amp;quot;), and a set of outgoing arcs.
The unification algorithm uses the union/find procedure on an inverted set represen-
tation of the equivalence classes adapted by Aft-Kaci (1984) after Huet (1976). The
actual algorithm used in the system is optimized using several different techniques to
minimize copying and to behave as efficiently as a pattern-matcher in cases when one
of the feature structures subsumes the other (Emele 1991).
</bodyText>
<subsectionHeader confidence="0.980168">
2.3 Inheritance Network of Feature Structures
</subsectionHeader>
<bodyText confidence="0.999400526315789">
The template mechanism (as, for example, in PATR-II [Shieber 1986]) already provides
a simple inheritance mechanism used to organize lexical descriptions. In comparison,
networks of typed feature structures are more expressive and provide a more general
and more powerful inheritance mechanism, which allows the use of recursive type
definitions, whereas recursivity is forbidden in templates since they are expanded
statically using a macro-expansion mechanism. Furthermore, typing provides a no-
tion of well-formedness that is used to implement a type-discipline and consistency
checks, giving the user the means of checking statically the coherence of a set of type
definitions.
2.3.1 Type Discipline. As different combinations of attribute value pairs make sense
for different kinds of objects, we divide our feature structures into different classes
by associating with a type a certain class of feature structures. Each type defines a
specific collection of features that are appropriate for it, restrictions on their possible
values, and equality constraints between values. The definition of a type A is def (A),
expressed as a feature structure (of type A). A type symbol that does not have any
definition is called an atomic type. A type that has a definition is called a complex
type.
The association of types and feature structures allows the definition of well-
formedness conditions for feature structures using the following two typing rules:
</bodyText>
<subsectionHeader confidence="0.919178">
Typing Rule 1:
</subsectionHeader>
<bodyText confidence="0.916911571428571">
A feature appearing in a feature structure has always to be declared as
appropriate for some type. The user cannot introduce arbitrary features:
one must declare all the features that one will use. All features that are
not explicitly declared as being appropriate for some type are by default
defined as being appropriate for I.
Typing rule 2:
A feature structure cannot have a feature that is not appropriate for its
</bodyText>
<page confidence="0.975605">
165
</page>
<note confidence="0.314799">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.997585136363636">
type or for one of the supertypes. Thus, any feature structure with a fea-
ture f has to belong to some type for which f is appropriate.
These well-formedness conditions are enforced at compile-time using a type in-
ference procedure which infers for each feature structure its possible minimal types. If
the inferred type is 1, an error is reported indicating that the respective feature struc-
ture does not obey the typing rules. The internal representation built by the compiler
uses these inferred minimal types to ensure that it is not possible to add an arbitrary
feature to a feature structure during computation, but only those declared for the type
of the structure, thus preserving well-formedness.
2.3.2 Inheritance and Generalization. Since types are organized in an inheritance
network, a type inherits all the features, value restrictions, and equality constraints
from all its super-types monotonically: the constraints expressed as feature structures
are conjoined using typed unification. The compiler makes sure that the user has
specified an inheritance network, building an internal representation where for every
two types such that A &lt; B we have def (A) &lt; def(B).5 If there is a type A such that
A 0 1 and def (A) = _1_, the network is inconsistent and an error is reported. The
compiler also has a generalization step where all constraints common to all subtypes
of a given type are also defined for that type.
2.3.3 Interpreting an Inheritance Network. The constraints expressed as an inher-
itance network are interpreted as follows. For a given typed feature structure t =
ZA[...1, the feature structure t belongs to the domain of A (i.e., it satisfies the con-
straints associated with A) if and only if:
</bodyText>
<subsectionHeader confidence="0.935833">
Inheritance Rule:
</subsectionHeader>
<bodyText confidence="0.9971585">
t satisfies the constraints specified by the de finition of A and by the defi-
nitions of all the supertypes of A;
</bodyText>
<subsectionHeader confidence="0.986929">
Specialization Rule:
</subsectionHeader>
<bodyText confidence="0.999906">
t satisfies the constraints specified by the definitions of at least one of
the subtypes of A.
The inheritance rule states the necessary conditions for a feature structure of type A
to satisfy the constraints associated with A. The specialization rule states the sufficient
conditions and implements a kind of closed-world assumption: a type is exhaustively
covered by its subtypes. For example, a feature structure of type LIST can be an
empty list (type NIL) or a nonempty list (CONS), but nothing else. These two rules are
implemented by the TFS interpreter described in Section 2.4.
Example. A simple example of an inheritance network of feature structures is dis-
played in Figure 2 using Hasse diagrams. The subnetwork on the right defines a
domain of lists expressed as feature structures: the set of all possible lists is defined
by the type LIST, which has no associated constraints. This type has two subtypes:
</bodyText>
<listItem confidence="0.998023">
• NIL is an atomic type and represents the empty list;
• CONS is a complex type and represents the set of all possible nonempty
lists and defines the following constraints. A feature structure of type
</listItem>
<footnote confidence="0.557181">
5 Inheritance is pre-computed statically: A &lt; B =t def (A) = def (A) A def (B).
</footnote>
<page confidence="0.97769">
166
</page>
<figure confidence="0.969466105263158">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
1: LIST
APPEND 2: LIST]
3: LIST
2: [&apos;LIST
1: NIL
fl
3: El APPEND1 2: QLIST
1: CONS[fir81
restai
:- APPEND 2:
1: DI
3: CONS[first:p
restal 3: Ei
APPENDO
LIST
\ [first: T
NIL CONS
rest: LIST
</figure>
<figureCaption confidence="0.993305">
Figure 2
</figureCaption>
<bodyText confidence="0.991889066666667">
Type hierarchy for LIST and APPEND (T and I omitted).
CONS has only two features (first typing rule): first, whose value is the
first element of the list and can be anything, and rest, whose value is
constrained to be a list.&apos; Note that this latter constraint is recursive.
The subnetwork on the left defines the domain of the APPEND relation, encoded
using feature structures. The constraints associated with the supertype APPEND say
that it has three arguments, identified by the features 1, 2, and 3, and that the values
of all arguments should be in the LIST domain. The subtypes APPENDO and APPEND1
encode the two cases where the first argument is the empty list (APPENDO), and the
nonempty list (APPEND1), in a way similar to the classical PROLOG encoding. As
shown for the type APPEND1, it is possible to have additional constraints that are
not represented in the feature structure proper: they are introduced by the &apos; : sign.&apos;
These conditions can be inherited and are conjoined using the logical and operation.
For APPEND1, the condition states the recursive constraint on the concatenation of the
lists, which is expressed as a feature structure of type APPEND.
</bodyText>
<subsectionHeader confidence="0.999675">
2.4 The TFS Abstract Rewrite Machine
</subsectionHeader>
<bodyText confidence="0.999912">
The meaning (denotation) of a typed feature structure t in a universe U defined by
an inheritance network is represented by the largest set of feature structures St --=
{t1,..., tn} such that, for all t,
</bodyText>
<listItem confidence="0.5703297">
1. t, &lt; t, and
2. for all substructures u EA[...] of t„ type A is a minimal type and
u &lt; def (A).
The first condition says that all the elements of St satisfy the constraints expressed
by t. The second condition says that all the elements of St satisfy the constraints defined
6 Conversely, using the second typing rule, we can deduce that CONS is a possible type for
first : Mary, rest: T], since the combination of first and rest is defined as appropriate for CONS.
7 This construction provides room for future evolution of the formalism by adding new kinds of
constraints that cannot be directly expressed in the AVM format e.g., negation. A definition
&amp;quot;X : — Y, Z.&amp;quot; is read &amp;quot;X such that Y and Z.&amp;quot;
</listItem>
<page confidence="0.940324">
167
</page>
<figure confidence="0.866324">
Computational Linguistics Volume 18, Number 2
</figure>
<figureCaption confidence="0.872018">
Figure 3
</figureCaption>
<bodyText confidence="0.976995454545455">
Rewrite rules for LIST and APPEND.
by the network. If St is empty, the feature structure t is inconsistent (modulo the
constraints of the inheritance network). St can be finite, e.g. in the case of a dictionary,
but it can also be infinite in the case of recursive types: for example, the set of feature
structures subsumed by LIST is the (infinite) set of all possible lists represented as
feature structures.8
In this section, we describe an abstract rewrite machine for computing the repre-
sentation of the denotation of typed feature structures given an inheritance network.
The rewrite mechanism is based on a variant of narrowing&apos; adapted to feature struc-
tures.
An inheritance network of feature structures is compiled into a rewriting system
as follows: each direct link between a type A and a subtype B generates a rewrite rule
of the form A[a] B[19] where A[a] and B[b] are the definitions of A and B, respectively.
Figure 3 shows the rewrite rules corresponding to the network of Figure 2.
The interpreter is given a &amp;quot;query&amp;quot; (formulated as a typed feature structure) to
evaluate. The first step is to check that the feature structure respects the two typing
rules (Section 2.3.1). The idea is then to try to satisfy all the constraints defined by
the inheritance network by incrementally adding more constraints to the query using
the rewrite rules (nondeterministically) to get closer to the solution step by step. The
rewriting process stops when conditions 1 and 2 described above hold.
A rewrite step for a structure t is defined as follows: if u is a substructure of t at
path p and u is of type A, and there exists a rewrite rule A[a] 13[19] such that
</bodyText>
<listItem confidence="0.9999765">
• A[a] A U _L, and
• A[a] A U &lt; A[a]
</listItem>
<bodyText confidence="0.917727">
then the right-hand side B[b] is unified with the substructure u at path p, giving a new
structure t&apos; that is more specific than t (Figure 4).
</bodyText>
<footnote confidence="0.599950666666667">
8 See Nit-Kaci (1984), Pollard and Moshier (1990), and Emele and Zajac (1990a) for fixed-point
characterizations of the denotation of typed feature structures.
9 Narrowing uses unification instead of pattern-matching for checking the applicability of the 1.h.s. of a
rule. Narrowing is used in the logic programming paradigm (e.g., as an alternative to resolution for
implementing PROLOG interpreters). Pattern-matching is used in the functional programming
paradigm.
</footnote>
<figure confidence="0.95783248">
NIL
CONS[first: T
rest: LIST
1: NIL
APPENDO 2: 0 LIST
3: 0
APPEND1
1: CONS[first:E -
rest:El
2: g LIST
3: CONS[first:E
rest:0
LIST
LIST
[1: LIST]
APPEND 2: LIST
3: LIST
[1: LIST
APPEND 2: LIST
3: LIST
[1: 01
APPEND 2: E
3: El
168
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
</figure>
<figureCaption confidence="0.974335">
Figure 4
</figureCaption>
<bodyText confidence="0.9953908">
A rewrite step.
The first condition checks that the rule is applicable: the 1.h.s. has to be consistent
with the substructure. The implementation factorizes common 1.h.s., avoiding the DNF
expansion: if a 1.h.s. 1 of a rule 1 -- r is not consistent with the substructure, this
computation branch is a failure branch, and all rules u —p v where u &lt; 1 are discarded
in one step without further computation.
The second condition implements a lazy rewriting strategy: if A[a] A u is equal to
AP], all rules A[a] —&gt; B[b] could be applied with success, and failure could come only
from the rewriting of some other substructures after the exploration of all choices for
u. To avoid the exploration of failure branches as much as possible, the evaluation of
the substructure u is suspended until the evaluation of some other substructure having
some part in common with u makes a more specific, narrowing the set of potential
choices for the subtypes of A for u. Thus, the search space is explored &amp;quot;intelligently,&amp;quot;
postponing the evaluation of branches of computation that would correspond for ex-
ample to uninstantiated PROLOG goals (see for example van Hentenryck and Dincbas
[1987], van Hentenryck [1989] on evaluation techniques in constraint logic program-
ming).
Rewrite steps are applied nondeterministically everywhere in the structure until
no further rule is applicable.&apos;°
The choice of which substructure to rewrite is only partly determined by the
availability of information (using the lazy rewriting rule). When there are several
substructures that could be rewritten, the computation rule is to choose one of the
outermost ones, i.e., one closest to the root of the feature structure (innermost strategies
are usually nonterminating). This outermost rewriting strategy is similar to hyper-
resolution in logic programming. In comparison, PROLOG uses a leftmost computation
rule.
For a given substructure, the choice of which rule to apply is done nondetermin-
istically, and the search space is explored depth-first using a backtracking scheme. Al-
though this strategy is not complete (a complete breadth-first search strategy could be
used for debugging purposes), the use of the outermost rule has favorable termination
</bodyText>
<tableCaption confidence="0.722171">
10 Conditions do not change this general scheme (they are evaluated using the same rewriting
mechanism) and are omitted from the presentation here for the sake of simplicity. See for example
Dershowitz and Plaited (1988) and flop (1990) for a survey on rewriting.
</tableCaption>
<page confidence="0.975901">
169
</page>
<note confidence="0.510921">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.9808555">
properties when compared to PROLOG&apos;S leftmost rule: there are problems where a TFS
computation will terminate when the corresponding logic program implemented in
PROLOG will not; for example, for left-recursive rules in naive PROLOG implementa-
tions of DCGs.
</bodyText>
<sectionHeader confidence="0.96386" genericHeader="method">
3. Inheritance and Constraint-Based Grammars
</sectionHeader>
<subsectionHeader confidence="0.977024">
3.1 Related Approaches
</subsectionHeader>
<bodyText confidence="0.9998845">
In the constraint-based framework, a grammar is rega rded as a set of constraints to be
satisfied by a given linguistic object: parsing and generation differ only in the nature
of the &amp;quot;input,&amp;quot; and use the same constraint evaluation mechanism. The properties of
a computational framework for implementing constraint-based grammars are:
</bodyText>
<listItem confidence="0.999421727272727">
• A unique general constraint solving mechanism is used: grammars
define constraints on the set of acceptable linguistic structures.
• As a consequence, there is no formal distinction between &amp;quot;input&amp;quot; and
&amp;quot;output.&amp;quot; For example, the same kind of data structure could be used to
encode both the string and the structural description, and, as for the
HPSG sign (Pollard and Sag 1987), they could be embedded into a single
data structure that represents the relation between the string and the
associated linguistic structure.
• Specific mapping properties, based on constituency, linear precedence, or
functional composition, are not part of the formalism itself, but can be
encoded explicitly in the formalism.
</listItem>
<bodyText confidence="0.999937625">
An approach that uses a unique deductive mechanism for parsing and generation
is described in Dymetman and Isabelle (1988). Within this approach, a lazy evaluation
mechanism based on the specification of input/output arguments is implemented (in
PROLOG), and the evaluation is completely data-driven: the same program parses or
generates, depending only on the form of the input structure.
A constraint-based grammar does not need a context-free mechanism to build
up constituent structures for parsing or generation: Dymetman, Isabelle, and Perrault
(1990) describe a class of reversible grammars (&amp;quot;Lexical Grammars&amp;quot;) based on a few
composition rules that are very reminiscent of categorial grammars. Other kinds of
approaches have been proposed, e.g., using a dependency structure and linear prece-
dence relations (Reape 1990; see also Pollard and Sag [19871). In Saint-Dizier (1991),
linear precedence rules are defined as constraints in a language based on typed feature
structures and SLD-resolution, which is used to experiment with GB theory.
In the following sections, we describe two examples of constraint-based grammars:
an HPSG grammar for a fragment of English, and an LFG-style transfer grammar for
a small machine translation problem between English and French.
</bodyText>
<subsectionHeader confidence="0.999785">
3.2 Head-Driven Phrase Structure Grammar
</subsectionHeader>
<bodyText confidence="0.999799833333333">
In general, a grammar describes the relation between strings of words and linguistic
structures. To implement a constraint-based grammar in TFS, we have to encode both
kinds of structures using the same data structure provided by the TFS language: typed
feature structures. A linguistic structure will be encoded using features and values.
Conditions that constrain the set of valid linguistic structures have to be declared ex-
plicitly. A string of words will be encoded as a list of word forms, using the same kind
</bodyText>
<page confidence="0.972594">
170
</page>
<table confidence="0.950432888888889">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
SYN I LOC I SUBCAT
EDTRS headed-structure{ I I =[DTRS{HEAD-DTR I SYN I LOC I SUBCAT append(cij, 0)
COMP-DTRS
PHRASE &lt; syn: SUBCAT :- APPEND 1: ID
SUBCAT-FP dtrs: -FP. 2: Ili
[ snbcat: [D] 3: IJ ]
head-dtr: [syn: [subcat: 111]J}
comp-dtrs: In
</table>
<figureCaption confidence="0.820708">
Figure 5
</figureCaption>
<bodyText confidence="0.9951528125">
The HSPG subcategorization principle and its TFS encoding.
of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based
on the notion of typed feature structures (Pollard 1990), and is thus a good candidate
to illustrate the possibilities of the TFS formalism. The following presentation is based
on Emele (1988) and Emele and Zajac (1990b).
The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic
structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure,
syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot;
the relation between a string and a linguistic structure is encoded as a single feature
structure representing the &amp;quot;sign.&amp;quot;
HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type
PHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, and
SEM-FP. For example, the HPSG subcategorization principle is encoded in TFS us-
ing inheritance to model implication, and the TFS APPEND relation to encode the
functional constraint on concatenation (Figure 5). In a similar way, HEAD-FP encodes
the HPSG Head Feature Principle and SEM-FP encodes the Semantics Principle.
The type SIGN is divided into several subtypes corresponding to different map-
pings between a string and a linguistic structure. We first have the basic distinction
between phrases and words. The definition of a phrase recursively relates subphrases
and substrings, and defines the phrase as a composition of subphrases and the string
as the concatenation of substrings. Since the formalism itself does not impose any con-
straints on how the relations between phrases and strings are defined, the grammar
writer has to define them explicitly. In HPSG (Pollard and Sag 1987), the ordering
of phrases is defined using linear precedence relations: the order in which the sub-
strings associated with subphrases are concatenated to give the string associated with
a phrase are guided by these linear precedence relations (Reape 1990).
In the example given below (Figure 6), we make simplifying assumptions: the
LOCAL feature is not used and there are two possible orderings for complements. The
type IDP1 encodes Grammar Rule 1 (Pollard and Sag 1987, pp. 149-155), which says
that a &amp;quot;saturated phrasal sign,&amp;quot; i.e., a feature structure with [syn : [subcat : ()]], is the
combination of an unsaturated phrasal head with one phrasal complement on the left.
For example, for structures like S —4 NP VP, S is the &amp;quot;saturated phrasal sign,&amp;quot; NP is
</bodyText>
<footnote confidence="0.929424">
11 We will use a more condensed notation for lists with angle brackets provided by the TFS language: a
list
</footnote>
<note confidence="0.863351">
CONS [first: Mary, rest: CONS [first : sings, rest: NIL]] is written as &lt;Mary sings&gt;.
</note>
<page confidence="0.971478">
171
</page>
<figure confidence="0.787627">
Computational Linguistics Volume 18, Number 2
</figure>
<figureCaption confidence="0.982968">
Figure 6
</figureCaption>
<bodyText confidence="0.997327761904762">
Part of the HPSG PHRASE hierarchy: PHRASE inherits from &amp;quot;principles&amp;quot; (given here without
their definitions) and is subdivided into two subtypes corresponding to different complement
orderings.
the left phrasal complement and VP is the unsaturated phrasal head. Furthermore, the
string (the value of the phon feature) of the IDP1 phrase is the concatenation of the
string of the complement with the string of the head.
The type IDP2 encodes Grammar Rule 2 and states that an &amp;quot;unsaturated phrasal
sign,&amp;quot; i.e., a feature structure with [syn: [subcat : (SIGN)]1, is the combination of a
lexical head with any number of complements on the right (e.g., for VP --+ V XP*): the
string associated with IDP2 is the concatenation of the string of the head with the
concatenation of the strings of the complements, where the relation ORDER-COMP
defines in which order the complements strings are concatenated.
The difference between the parsing and the generation problem is then only in the
form of the structure given to the interpreter for evaluation. A query for the parsing
problem is an underspecified structure where only the string is given; conversely,
a query for the generation problem is an underspecified structure where only the
semantic form is given (Figure 7).
In both cases, the interpreter uses the same set of rewrite rules to fill in &amp;quot;missing
information&amp;quot; according to the type definitions. The result in both cases is exactly the
same: a fully specified structure containing the string, the full semantic form, and also
all other syntactic information such as the constituent structure (Figure 8).
</bodyText>
<subsectionHeader confidence="0.999865">
3.3 Bi-directional Transfer in Machine Translation
</subsectionHeader>
<bodyText confidence="0.9998836">
We have sketched above a very general framework for specifying mappings between
a linguistic structure, encoded as a feature structure, and a string, also encoded as a
feature structure. We apply a similar technique for specifying transfer rules for machine
translation, which we prefer to call &amp;quot;contrastive rules&amp;quot; since there is no directionality
involved (Zajac 1989; 1990a).
</bodyText>
<figure confidence="0.997151428571428">
SlJECAT-FP HEAD-FP SEM-FP
PHRASE[dtrs: TREE]
phon: a)
[syn: rcat: ())
IDP1
head-dtr: PHRASE[phon: Eiji
dtrs:
comp-dtrs: (SIGN[phon: al])
1: ijl
[
:- APPEND 2: al
3: fa
IDP2[ al
syn: snbcat: (SIGN)]
dtrs: [head-dtr: WORD[phon:
comp-dtrs: III
APPEND[2: ai ,
3: III
1: El
ORDER-COMPEcomps: 01
phon:
</figure>
<page confidence="0.965344">
172
</page>
<table confidence="0.915815666666667">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
Query for parsing
PHRASE[phon: (Kim ate every cookie)]
Query for generation
PHRASE sem: {rein: EAT
[ argl: ind: [restr: [name: KIM] 1] 1]
arg2: spec: EVERY
. r r
Ind: trestr: Vein: COOKIE]]]
</table>
<figureCaption confidence="0.923738333333333">
Figure 7
Queries for parsing and generation.
Figure 8
</figureCaption>
<bodyText confidence="0.999435866666667">
The common solution to the parsing and generation problems.
The idea is rather simple: assume we are working with linguistic structures simi-
lar to LFG&apos;s functional structures for English and French as proposed in Kaplan et al.
(1989). We define a translation relation as a type TAU-LEX with two features, eng for
the English structure and fr for the French structure. This &amp;quot;bilingual sign&amp;quot; is defined
on the lexical structure: each subtype of TAU-LEX defines a lexical correspondence
between a partial English structure and a partial French structure for a given lexical
equivalence. Such a lexical contrastive definition for a verb also has to pair the argu-
ments recursively. This is expressed in the condition part of the definition (Figure 9)
by a recursive condition TAU-LEX on the arguments. The translation of syntactic and
semantic features, like tense or determination, is also specified in the condition part,
and these contrastive definitions can and should be defined separately from the lexical
definitions as different subnetworks.
Inheritance is used here to capture generalizations over relations: all the informa-
tion in the example of Figure 9 can be unpacked and redefined as follows. We have
</bodyText>
<figure confidence="0.977285181818182">
&apos;DPI
phon: (Kim ate every cookie)
head: VHEADrense: pal
[
subcat: 0
dtrs: TREE[comp-dtrs: (...)
head dtr: IDP2 ...
-
-rein: EAT
var: 111
argl: N-OBJ ind: MX rest&apos;: IND named: El
sem: 2ARG-REL ] 1 I
name: KIM
rein: NAMING
syn: SYN
[spec: EVERY
var: 10
arg2:
hid: IDX[restr: ENTITYrn: COOKIE]]
I. IE
173
Computational Linguistics Volume 18, Number 2
</figure>
<figureCaption confidence="0.911306">
Figure 9
A transfer rule.
Figure 10
</figureCaption>
<bodyText confidence="0.953420769230769">
Part of an inheritance network of transfer relations for verbs.
three levels of generalization for the translation of verbs: TAU-LEX-V is subdivided into
several relations for translating, for example, transitive verbs (TAU-LEX-TV), intransi-
tive verbs (TAU-LEX-ITV), etc. The condition that specifies the translation of tenses is
defined for the whole class of verbs TAU-LEX-V. The condition that specifies the trans-
lation relation between subjects is defined for intransitive verbs (TAU-LEX-ITV), but
cannot be specified for all verbs since it does not apply to, e.g., impersonal verbs. This
leaves only the translation of predicates to be defined at the leaves of the hierarchy
(Figure 10). Thus, at each level of generalization, we specify the minimal amount of
information needed for translation. The same kind of organization can be used for
nouns, where syntactic features such as determination are associated with the higher
types, and where the minimal types define the equivalences between lexical forms of
nouns themselves.
</bodyText>
<figure confidence="0.998051125">
TALI-LEX-FALL &lt; TAU-LEX-ITV.
pied: FALL
eng: subj:
tense:
I
tense:
TAU-LEX-FALL
x[eng: :
fr
:- TAU-LE
Ed=
=TM
[pied: TOMBER subj:
GE=
R4ensGI
, TALY-TENSEleng:
fr:
IM113113
121=t1
TAU
/\
TAU-LEX... TAU-TENSE...
TAU-L, EX-VI
tense: Rivrn
engitense: =on
/1 TAU-TENS[fr:
eng:
TAU-LEX-FALL
[eng: [pied: FALL]
fr: pied: TOMBEIL]
MILITC21
If-tensel
TAU-LEX-ITV [eng: frubj:
fr: subj:
/I .- TA1J-LEX{eng:
fr:
=L11
MIR
=III
V-subfl
</figure>
<page confidence="0.991349">
174
</page>
<table confidence="0.994867722222222">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
Query
TAU eng: pied: FALL STUDENT I
red: pred: A
subj: num: E-SING
spec:
tense: E-PRES
Answer
TAU-LEX-FALL _ pied: FALL STUDENT - -
eng: subj: pied: pred: A _ -
fr: tense: spec: num: E-SINGI
pied: [ ETUDIANT
subj: E-PRES [pred: UN
tense: TOMBER num: F-SING
pied:
spec:
{
F-PRES
</table>
<figureCaption confidence="0.927303">
Figure 11
</figureCaption>
<bodyText confidence="0.96372575">
Query and answer for the translation of &amp;quot;A student falls.&amp;quot;
The transfer problem for each direction is then stated in the same way as for
parsing or generation: the input structure is an underspecified &amp;quot;bilingual sign&amp;quot; where
only the structure for the source language is given. Using the contrastive grammar,
the interpreter fills in missing information and builds a bilingual sign12 (Figure 11).
It is not necessary to specify in the contrastive definitions all monolingual con-
straints that have to be satisfied by the English structure and by the French structure.
We can assume that we have monolingual grammars that define the appropriate map-
pings between the set of English sentences and the set of associated English structures,
and similarily for French. Using these monolingual constraints in addition to the con-
trastive grammar, the TFS interpreter would build the fully specified monolingual
structures, implementing a constraint-based translation system.
</bodyText>
<subsectionHeader confidence="0.884077">
3.4 Termination Problems
</subsectionHeader>
<bodyText confidence="0.999860818181818">
For parsing and generation, since no constraints are imposed on the kind of mapping
between the string and the semantic form, termination has to be proved for each class
of grammar and for the particular mechanism used for either parsing or generation
with this grammar. If we restrict ourselves to classes of grammars for which termi-
nating evaluation algorithms are known, we can implement those directly in TFS.
However, the TFS evaluation strategy allows more naive implementations of gram-
mars, and the outermost rewriting of &amp;quot;sub-goals&amp;quot; terminates on a strictly larger class
of programs than for corresponding logic programs implemented in PROLOG. Further-
more, the grammar writer does not need, and actually should not, be aware of the
control that follows the shape of the input rather than a fixed strategy, thanks to the
lazy evaluation mechanism.
</bodyText>
<footnote confidence="0.5160175">
12 See also Reape (1990) for another approach to MT using feature structures and based on Whitelock
(1990).
</footnote>
<page confidence="0.992683">
175
</page>
<note confidence="0.634942">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.999937515151515">
For HPSG-style grammars, completeness and coherence as defined for LFG, and
extended to the general case by Wedekind (1988), are implemented in HPSG using the
&amp;quot;subcategorization feature principle&amp;quot; (Johnson 1987): for the TFS implementation of
HPSG, termination is guaranteed, at least for the simplified version containing only
head-complement structures, described in Section 3.2. Termination conditions for pars-
ing are well understood in the framework of context-free grammars. For generation
using feature structures, one of the problems is that the input could be &amp;quot;extended&amp;quot;
during processing, i.e., arbitrary feature structures could be introduced in the semantic
part of the input by unification with the semantic part of a rule. However, if the seman-
tic part of the input is fully specified according to a set of type definitions describing
the set of well-formed semantic structures (and this condition is easy to check), this
cannot arise in a type-based system since it is not possible to add arbitrary features to
a typed feature structure.
A more general approach is described in Dymetman, Isabelle, and Perrault (1990),
who define sufficient termination properties for parsing and generation for the class
of &amp;quot;Lexical Grammars.&amp;quot; These termination properties are conditions on the existence
of &amp;quot;conservative guides&amp;quot; for parsing and generation and seem generalizable to other
classes of grammars as well, and are also applicable to TFS implementations. Since
Lexical Grammars are implemented in PROLOG, left-recursion must be eliminated for
parsing and for generation, but this does not apply to TFS implementations. The idea
of conservative guides is relatively simple and says that for parsing, each rule must
consume a nonempty part of the string, and for generation, each rule must consume
a nonempty part of the semantic form. These conditions seem to be equivalent as
to require the existence of a well-founded relation on strings (for parsing) and of
a well-founded relation on semantic forms (for generation). The existence of such
well-founded relations is actually a necessary condition for proving the termination
of parsing and generation (see Deville [1990] for a more general discussion on well-
founded relations in the context of logic programming).
Termination for reversible transfer grammars is discussed in van Noord (1990).
One of the problems mentioned there is the extension of the &amp;quot;input,&amp;quot; as in generation,
and the answer is similar (see above). However, properties similar to the &amp;quot;conservative
guides&amp;quot; of Dymetman, Isabelle, and Perrault (1990) have to hold in order to ensure
termination.
</bodyText>
<sectionHeader confidence="0.957601" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.960313230769231">
The TFS system has been developed to provide a computational environment for the
design and the implementation of formal models of natural language. The TFS formal-
ism is designed as a specification language that can be used to design and implement
formal linguistic models. It is not a programming language: it does not offer means
of defining control information that would make execution more efficient (but less
general), as it would be needed if it would be envisaged to use the system in an
application-oriented environment (e.g., as a parser in a natural language interface to a
database system). From formal linguistic models developed in TFS, it could be envis-
aged to develop programs, i.e., parsers or generators, that would efficiently implement
the declarative knowledge contained in the formal specifications.&amp;quot;
13 See for example in Biggerstaff and Perlis (1989) the papers on the development of programs from
specifications, a very important issue in software engineering. See also Ait-Kaci and Meyer (1990) for a
programming language based on typed feature structures.
</bodyText>
<page confidence="0.997232">
176
</page>
<note confidence="0.9488">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
</note>
<bodyText confidence="0.999061">
The TFS system is implemented using rewriting techniques in a constraint-based
architecture adapted to feature structures:
</bodyText>
<listItem confidence="0.965080272727273">
• The language is a logical language directly based on typed feature
structures, and supports an object-oriented style based on multiple
inheritance.
• Grammars are expressed as inheritance networks of typed feature
structures. They define constraints on the set of acceptable linguistic
structures. As a consequence, there is no formal distinction between
&amp;quot;input&amp;quot; and &amp;quot;output.&amp;quot;
• A unique general constraint solving mechanism is used. Specific
mapping properties, based on constituency, linear precedence or
functional composition, are not part of the formalism itself, but can be
encoded explicitly using the formalism.
</listItem>
<bodyText confidence="0.999966954545454">
Although the current implementation is very much at the level of an experimental
prototype, and is still evolving, it has validated the basic concepts of the language
and of the implementation, and has been used to test different linguistic models and
formalisms such as LFG, DCG, HPSG, and SFG on small examples. From these var-
ious experimentations, we have defined extensions and improvements, both on the
language and on the implementation, that are needed for scaling up the system.
On the language side, more expressivity is needed. For example, disjunctions over
feature structures, various kinds of negation (Alt-Kaci 1986), and sets of feature struc-
tures (Pollard and Moshier 1990) are necessary to formalize, e.g., nontrivial semantic
structures. Some types together with a specific syntax and associated operations could
be conveniently added to the system as libraries of built-in types, e.g., characters,
strings, and trees.
On the implementation side, the use of implementation techniques adapted from
PROLOG implementations, constraint satisfaction languages, and object-oriented lan-
guages can be beneficial to the implementation of typed feature structure—based sys-
tems and have to be more thoroughly explored.140ne of the major efficiency issues in
the current implementation is the lack of an efficient indexing scheme for typed fea-
ture structures. For example, since the dictionaries are accessed using unification only,
each entry is tried one after the other, leading to an extremely inefficient behavior with
large dictionaries. Thus, the use of a general indexing scheme based on a combination
of methods used in PROLOG implementations and in object-oriented database systems
is necessary and will be implemented in a future version of the system.
</bodyText>
<sectionHeader confidence="0.844982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8575308">
The design and the implementation of the
TFS system have been carried out in
cooperation with Martin Emele. I would like
to thank Stefan Momma and Ulrich Heid;
their numerous comments and advices
helped to make this article more readable. I
would also like to thank anonymous
referees for their many detailed and helpful
comments. All remaining errors are of
course the sole responsibility of the author.
</bodyText>
<footnote confidence="0.645355">
14 For example, the use in the current implementation of several techniques adapted from PROLOG
implementations such as structure sharing, chronological dereferencing (Emele 1991) and last call
optimization, have improved efficiency by several orders of magnitude over previous &amp;quot;naive&amp;quot;
implementations.
</footnote>
<page confidence="0.982832">
177
</page>
<table confidence="0.878836194444444">
Computational Linguistics Volume 18, Number 2
References
Ait-Kaci, Hassan (1984). &amp;quot;A lattice theoretic
approach to computation based on a
calculus of partially ordered type
structures.&amp;quot; Doctoral dissertation,
University of Pennsylvania, Philadelphia,
PA.
Ait-Kaci, Hassan (1986). &amp;quot;An algebraic
semantics approach to the effective
resolution of type equations.&amp;quot; Theoretical
Computer Science, 45, 293-351.
Ait-Kaci, Hassan, and Meyer, Richard
(1990). Wild_LIFE, a user manual.
DEC-PRL Technical Note PRL-TN-1,
Rueil-Malmaison, France.
Ait-Kaci, Hassan, and Podelski, Andreas
(1991). Towards a meaning of LIFE.
DEC-PRL Research Report PRL-RR-11,
Rueil-Malmaison, France.
Bateman, John A., and Momma, Stefan
(1991). The nondirectional representation
of Systemic Functional Grammars and
Semantics as Typed Feature Structures.
IMS Technical Report, University of
Stuttgart, Germany.
Biggerstaff, Ted J., and Perlis, Alan J. (eds).
(1989). Software Reusability. ACM
Press-Addison-Wesley.
Birkoff, Garrett (1940). Lattice Theory.
American Mathematical Society, third
edition, 1973.
Borgida, Alexander; Brachman, Ronald J.;
McGuinness, Deborah L.; and Resnick,
Lori Alperin (1989). &amp;quot;CLASSIC: a
structural data model for objects.&amp;quot; In
</table>
<reference confidence="0.987283563218391">
Proceedings, 1989 ACM SIGMOD
International Conference on Management of
Data, Portland, Oregon.
Bouma, Gosse (1990). &amp;quot;Non-monotonic
inheritance and unification.&amp;quot; In
Proceedings, Workshop on Inheritance in
Natural Language Processing, Institute for
Language Technology and Al, Tilburg
University, The Netherlands.
Bourbeau, L.; Carcagno, D.; Goldberg, E.;
Kittredge, R.; and Polguere, A. (1990).
&amp;quot;Bilingual generation of weather forecasts
in an operation environment.&amp;quot; In
Proceedings, 13th International Conference on
Computational Linguistics (COLING&apos;90).
Helsinki.
Brachman, Ronald J., and Schmolze, James
G. (1985). &amp;quot;An overview of the KL-ONE
knowledge representation language.&amp;quot;
Cognitive Science 9, 171-216.
Carpenter, Bob (1990). &amp;quot;Typed feature
structures: inheritance, (in)equality and
extensionality.&amp;quot; In Proceedings, Workshop on
Inheritance in Natural Language Processing,
Institute for Language Technology and
Al, Tilburg University, The Netherlands.
Daelema:ns, Walter (1990). &amp;quot;Inheritance and
object-oriented natural language
processing.&amp;quot; In Proceedings, Workshop on
Inheritance in Natural Language Processing,
Institute for Language Technology and
Al, Tilburg University, The Netherlands.
Dershowitz, N., and Plaisted, D. A. (1988).
&amp;quot;Equational programming.&amp;quot; In Machine
Intelligence 11, edited by Hayes, Michie,
and Richards. Oxford: Clarendon Press.
De Smedt, Koenraad, and de Graaf, Josje
(1990). &amp;quot;Structured inheritance in
frame-based representation of linguistic
categories.&amp;quot; In Proceedings, Workshop on
Inheritance in Natural Language Processing.
Institute for Language Technology and
Al, Tilburg University, The Netherlands.
Deville, Yves (1990). Logic programming.
Systematic Program Development. Reading,
MA: Addison-Wesley.
Dymetman, Marc, and Isabelle, Pierre
(1988). &amp;quot;Reversible logic grammars for
machine translation.&amp;quot; In Proceedings, 2nd
International Conference on Theoretical and
Methodological Issues in Machine Translation
of Natural Language. Pittsburgh, PA.
Dymetman, Marc; Isabelle, Pierre; and
Perrault, Francois (1990). &amp;quot;A symmetrical
approach to parsing and generation.&amp;quot; In
Proceedings, 13th International Conference on
Computational Linguistics (COLING&apos;90).
Helsinki.
Emele, Martin (1988). &amp;quot;A typed feature
structure unification-based approach to
generation.&amp;quot; In Proceedings, WGNLC of the
IECE. Oiso University, Japan.
Emele, Martin (1991). &amp;quot;Unification with lazy
non-redundant copying.&amp;quot; In Proceedings,
29th Annual Meeting of the ACL. Berkeley,
CA.
Emele, Martin, and Zajac, Remi (1989).
&amp;quot;RETIF: A rewriting system for typed
feature structures.&amp;quot; ATR Technical report
TR-I-0071, ATR, Kyoto.
Emele, Martin, and Zajac, Remi (1989).
&amp;quot;Multiple inheritance in RETIF.&amp;quot; ATR
Technical report TR-I-0114, ATR, Kyoto.
Emele, Martin, and Zajac, Remi (1990a). &amp;quot;A
fixed-point semantics for feature type
systems.&amp;quot; In Proceedings, 2nd Workshop on
Conditional and Typed Rewriting Systems
(CTRS&apos;90). Montréal, Québec.
Emele, Martin, and Zajac, Remi (1990b).
&amp;quot;Typed unification grammars.&amp;quot; In
Proceedings, 13th International Conference on
Computational Linguistics (COLING&apos;90).
Helsinki.
Emele, Martin; Heid, Ulrich; Momma,
Stefan; and Zajac, Read (1990).
&amp;quot;Organizing linguistic knowledge for
multilingual generation.&amp;quot; In Proceedings,
</reference>
<page confidence="0.997998">
178
</page>
<note confidence="0.7890585">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
13th International Conference on
</note>
<reference confidence="0.999577917355372">
Computational Linguistics (COLING&apos;90).
Helsinki.
Etherington, David W.; Forbus, Kenneth D.;
Ginsberg, Matthew L.; Israel, David; and
Lifschitz, Vladimir (1989). &amp;quot;Critical issues
in nonmonotonic reasoning.&amp;quot; In
Proceedings, 1st International Conference on
Principles of Knowledge Representation and
Reasoning. Toronto, Ontario.
Evans, Roger, and Gazdar, Gerald (1989).
&amp;quot;Inference in DATR.&amp;quot; In Proceedings, 4th
European ACL Conference. Manchester, U.K.
Franz, Alex (1990). &amp;quot;A parser for HPSG.&amp;quot;
CMU report CMU-LCL-90-3, Laboratory
for Computational Linguistics, Carnegie
Mellon University.
Fraser, Norman M., and Hudson, Richard
A. (1990). &amp;quot;Word grammar: An
inheritance-based theory of language.&amp;quot; In
Proceedings, Workshop on Inheritance in
Natural Language Processing. Institute for
Language Technology and Al, Tilburg
University, The Netherlands.
Halliday, M. A. K. (1985). Introduction to
Functional Grammar. London: Edward
Arnold.
Huet, Gerard (1976). Resolution d&apos;equations
dans les langages d&apos;ordre 1, 2, . , w. Doctoral
dissertation, Universite de Paris VII.
Johnson, Mark (1987). &amp;quot;Grammatical
relations in attribute-value grammars.&amp;quot; In
Proceedings, West Coast Conference on Formal
Linguistics, Vol. 6. Stanford, CA.
Kaplan, Ronald M.; Netter, Klaus;
Wedekind, Jürgen; and Zaenen, Annie
(1989). &amp;quot;Translation by structural
correspondences.&amp;quot; In Proceedings, 4th
European ACL Conference. Manchester, U.K.
Kay, Martin (1984). &amp;quot;Functional unification
grammar: A formalism for machine
translation.&amp;quot; In Proceedings, 10th
International Conference on Computational
Linguistics (COLING-84). Stanford, CA.
Klop, Jan Willem (1990). &amp;quot;Term rewriting
systems.&amp;quot; To appear in Handbook of Logic
in Computer Science, Volume 1, edited by
S. Abramsky, D. Gabbay and T. Maibaum.
Oxford University Press.
MacGregor, Robert M. (1988). &amp;quot;A deductive
pattern matcher.&amp;quot; In Proceedings, 7th
National Conference on Artificial Intelligence
(AAAT 88). St. Paul, MN, 403-408.
MacGregor, Robert M. (1990). &amp;quot;LOOM user
manual.&amp;quot; USC/ISI Technical Report,
Marina del Rey, CA.
Mann, William C., and Matthiessen,
Christian I. M. I. (1985). &amp;quot;Demonstration
of the Nigel text generation computer
program.&amp;quot; In Systemic Perspectives on
Discourse, Volume 1, edited by James
D. Benson and William S. Greaves.
Norwood, NJ: Ablex.
Nirenburg, Sergei, Carbonele, Jaime, Tomita,
Masaru, and Goodman, Kenneth (1992).
Machine Translation. A Knowledge-Based
Approach. San Mateo, CA: Morgan
Kaufmann.
Pollard, Carl J. (In press). &amp;quot;Sorts in
unification-based grammar and what they
mean.&amp;quot; In Unification in Natural Language
Analysis, edited by M. Pinkal and
B. Gregor. Cambridge, MA: The MIT
Press.
Pollard, Carl J., and Moshier, Drew (1989).
&amp;quot;Unifying partial descriptions of sets.&amp;quot; In
Information, Language and Cognition, edited
by P. Hanson, Vancouver Studies in
Cognitive Science 1. Vancouver:
University of British Columbia Press.
Pollard, Carl J., and Sag, Ivan A. (1987).
Information-Based Syntax and Semantics.
Volume 1: Syntax. CSLI Lecture Notes 13,
Chicago University Press.
Pollard, Carl J., and Sag, Ivan A.
(Unpublished research). Information-Based
Syntax and Semantics. Volume 2: Agreement,
Binding and Control.
Reape, Mike (1990). &amp;quot;Parsing semi-free
word order and bounded discontinuous
constituency and &amp;quot;shake &apos;n&apos; bake&amp;quot;
machine translation (or &apos;generation as
parsing&apos;).&amp;quot; International Workshop on
Constraint Based Formalisms for Natural
Language Generation, Bad Teinach,
Germany.
Rounds, Williams C., and Kasper, Robert T.
(1986). &amp;quot;A Complete Logical Calculus for
Record Structures Representing Linguistic
Information.&amp;quot; IEEE Symposium on Logic in
Computer Science.
Saint-Dizier, Patrick (1991). &amp;quot;Processing
language with logical types and actives
constraints.&amp;quot; In Proceedings, 5th Conference
of the European Chapter of the ACL. Berlin,
Germany.
Shieber, Stuart (1986). An Introduction to
Unification-based Grammar Formalisms. CSLI
Lectures Notes 4, Chicago University
Press.
Smolka, Gert (1988). &amp;quot;A feature logic with
subsorts.&amp;quot; LILOG Report 33, IBM
Deutschland GmbH, Stuttgart.
Smolka, Gert (1989). &amp;quot;Feature constraint
logics for unification grammars.&amp;quot; IWBS
Report 93, IBM Deutschland GmbH,
Stuttgart.
Smolka, Gert, and Alt-Kaci, Hassan (1988).
&amp;quot;Inheritance hierarchies: Semantics and
unification.&amp;quot; I. Symbolic Computation, 7,
343-370.
Stoy, Joseph E. (1977). Denotational Semantics:
</reference>
<page confidence="0.973995">
179
</page>
<reference confidence="0.977724533333333">
Computational Linguistics Volume 18, Number 2
The Scott—Strachey Approach to Programming
Language Theory. Cambridge, MA: The
MIT Press.
van Hentenryck, Pascal (1989). Constraint
Satisfaction in Logic Programming,
Cambridge, MA: The MIT Press.
van Hentenryck, Pascal, and Dincbas,
Mehmet (1987). &amp;quot;Forward checking in
logic programming.&amp;quot; In Proceedings, 4th
International Conference on Logic
Programming. Melbourne, Australia.
van Noord, Gertjan (1990). &amp;quot;Reversible
unification-based machine translation.&amp;quot; In
Proceedings, 13th International Conference on
Computational Linguistics (COLING&apos;90).
Helsinki.
Wedekind, Jurgen (1988). &amp;quot;Generation as
structure driven generation.&amp;quot; In
Proceedings, 12th International Conference on
Computational Linguistics (COLING&apos;88).
Budapest.
Whitelock, Pete (1990). &amp;quot;Shake-and-bake
translation.&amp;quot; Ms. Sharp Laboratories of
Europe, Oxford.
Yen, John; Neches, Robert; and MacGregor,
Robert (1988). &amp;quot;Classification-based
programming: A deep integration of
frames and rules.&amp;quot; Technical Report
ISI/RR-88-213, USC/Information Science
Institute.
Zajac, Remi (1989). &amp;quot;A transfer model using
a typed feature structure rewriting system
with inheritance.&amp;quot; In Proceedings, 27th
Annual Meeting of the ACL. Vancouver, B.C.
Zajac, Remi (1990a). &amp;quot;A relational approach
to translation.&amp;quot; In Proceedings, 3rd
International Conference on Theoretical and
Methodological Issues in Machine Translation
of Natural Language. Austin, Texas.
Zajac, Rend (1990). &amp;quot;Semantics of typed
feature structures.&amp;quot; International Workshop
on Constraint Based Formalisms for Natural
Language Generation. Bad Teinach,
Germany.
</reference>
<page confidence="0.998296">
180
</page>
<note confidence="0.6122775">
Remi Zajac Inheritance and Constraint-Based Grammar Formalisms
Appendix: Syntax of the TFS Language
</note>
<bodyText confidence="0.999938333333333">
In the TFS language, there are two kinds of comments. In-line comments begin with
a semicolon and end with the end of line. These comments can appear anywhere
where a white space character is allowed in the syntax. They are skipped during
reading. Syntactic comments begin and end with the % character. These comments can
appear only where specified in the BNF syntax specification. They are attached to the
structure produced by the reader and can be displayed if the appropriate printer option
is set. Identifiers are case-sensitive. Macros are expanded statically by the compiler.
The operator &amp; is interpreted as the meet (unification).
The following extensions of the BNF notation are used:
</bodyText>
<listItem confidence="0.9995176">
• [X] denotes the optional element X (zero or one occurrence).
• X* denotes the free iteration of element X (zero, one or more
occurrences).
• X± denotes the iteration of element X (one or more occurrences).
• Symbols in typewriter font denote symbols of the TFS syntax.
</listItem>
<equation confidence="0.945062611111111">
&lt;entity&gt; ::= &lt;query&gt; I &lt;definition&gt;
&lt;query&gt; ::=? &lt;expression&gt; .
&lt;definition&gt; ::= &lt;po-definition&gt; I &lt;macro-definition&gt; I &lt;type-definition&gt;
&lt;po-definition&gt; ::= &lt;type-symbol&gt; &lt; &lt;type-symbol&gt; .
&lt;macro-definition&gt; ::= &lt;identifier&gt; : = &lt;expression&gt; .
&lt;type-definition&gt; ::= &lt;type-symbol&gt; &lt;expression&gt; : — &lt;conditions&gt; .
&lt;conditions&gt; ::= &lt;expression&gt; [ , &lt;conditions&gt; [
&lt;expression&gt; ::= &lt;feature-structure&gt; [ &amp; &lt;expression&gt; [
&lt;feature-structure&gt; ::= &lt;comments&gt; &lt;tagged-feature-structure&gt;
&lt;tagged-feature-structure&gt; ::= &lt;tag&gt; [ = &lt;typed-feature-structure&gt; [ I
&lt;typed-feature-structure&gt;
&lt;typed-feature-structure&gt; ::= &lt;type-symbol&gt; [ &lt;attribute-value-matrix&gt; [ I
&lt;attribute-value-matrix&gt; I
&lt;list&gt;
&lt;attribute-value-matrix&gt; ::= [ [ &lt;attribute-value-pairs&gt; I ]
&lt;attribute-value-pairs&gt; ::= &lt;attribute-value-pair&gt; [ , &lt;attribute-value-pairs&gt; [
&lt;attribute-value-pair&gt; ::= &lt;attribute&gt; : &lt;expression&gt;
&lt;list&gt; ::= &lt; &lt;expression&gt;* [ . &lt;expression&gt; l &gt;
</equation>
<reference confidence="0.83834525">
&lt;tag&gt; ::= #&lt;identifier&gt;
&lt;type-symbol&gt; ::= &lt;identifier&gt;
&lt;attribute&gt; ::= &lt;identifier&gt;
Example 1: the textual definitions for Figure 2.
NIL &lt; LIST.
CONS &lt; LIST.
CONS [first : T, rest: LIST] .
APPEND[1: LIST, 2: LIST, 3: LIST] .
</reference>
<page confidence="0.967343">
181
</page>
<reference confidence="0.844157421052632">
Computational Linguistics Volume 18, Number 2
APPENDO &lt; APPEND.
APPENDO[1: NIL, 2: #1=LIST, 3: #1].
APPEND1 &lt; APPEND.
APPEND1[1: &lt;#x . #11&gt;, 2: #12, 3: &lt;#x . #13&gt;]
APPEND[1: #11, 2: #12, 3: #13].
Example 2: the textual definitions for Figure 10.
TAU-LEX &lt; TAU.
TAU-LEX-V &lt; TAU-LEX.
TAU-LEX-V[eng: [tense: #e-tense],
fr: [tense: #f-tense]]
TAU-TENSE[eng: #e-tense, fr: #f-tense].
TAU-LEX-ITV &lt; TAU-LEX-V.
TAU-LEX-V[eng: [subj: #e-subj],
fr: [subj: #f-subj]]
TAU-LEX[eng: #e-subj, fr: #f-subj].
TAU-LEX-FALL &lt; TAU-LEX-ITV.
TAU-LEX-FALL[eng: [pred: FALL],
fr: [pred: TOMBER]]
</reference>
<page confidence="0.99799">
182
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.987914">Inheritance and Constraint-Based Grammar Formalisms</title>
<abstract confidence="0.972121363468636">OLYGLOSS We describe an approach to unification grammars that integrates two paradigms: the objectoriented approach, which offers multiple inheritance, complex objects with role-value restrictions and role-values equality, querying as subsumption; the relational programming approach, which offers declarativity, logical variables, nondeterminism with backtracking, and existential queries. This approach is embodied in a constraint-based object-oriented formalism. The interpreter of the formalism is described as a term rewriting system based on unification of typed feature structures. The grammar writer organizes unification grammars as inheritance networks of typed feature structures. Complex linguistic structures are described by means of recursive type constraints. We illustrate the use of inheritance networks with two examples: an HPSG example where implication (as used for &amp;quot;principles&amp;quot;) is modeled using inheritance and an example of bilingual transfer where the minimal amount of information needed for the translation is specified at different levels of generalization. Ideally, a linguistic formalism combining the best of the object-oriented approach and the unification-based approach would be realized in a constraint-based architecture for an object-oriented language based on inheritance, feature structures, and unification. The Typed Feature Structure language (TFS) is an attempt to provide a synthesis of several key concepts stemming from unification-based grammar formalisms (feature structure: Kay 1984) knowledge representation languages (inheritance), and logic programming (narrowing). The formalism supports an object-oriented style based on abstraction and generalization through inheritance; it is a fully declarative formalism based on unification of typed feature structures. It is flexible and has enough expressive power to support various kinds of linguistic theories, not necessarily based on constituency&apos;. The use of an object-oriented methodology for natural language processing is very attractive, and the use of inheritance offers a number of advantages such as abstraction and generalization, information sharing and default reasoning, and modularity and reusability (Daelemans 1990). Inheritance-based descriptions are already used in computational linguistics: linguistic theories such as Systemic Functional Grammar (Halliday 1985), Word Grammar (Fraser and Hudson 1990), or HPSG (Pollard * IMS-CL/Ifl-AIS, University of Stuttgart, Azenbergstrage 12, D-W-7000 Stuttgart 1. E-mail: zajac@informatik.uni-stuttgart.de. t Research reported in this paper is partly supported by the German Ministry of Research and Technology (BMFT, Bundesminister fur Forschung und Technologie), under grant No. 08 B3116 3. The views and conclusions contained herein are those of the author and should not be interpreted as representing official policies. 1 In particular, it allows a direct implementation of HPSG-style grammars (Emele 1988): HPSG is so far only linguistic theory based on both inheritance structures. 0 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 and Sag 1987) make use of inheritance to describe linguistic structures at the lexical, morphological, syntactic, or semantic (conceptual) levels. These theories are usually directly implemented in object-oriented programming languages (e.g., Loom in the case of the PENMAN system [Mann and Matthiessen 1985]), but there is a growing number of linguistic formalisms used for specific purposes, e.g., DATR (Evans and Gazdar 1989) for the lexicon. On the other hand, current linguistic theories such as LFG, UCG, HPSG, and some formalisms for linguistic description such as FUG or PATR-II are based on the notion of partial information: linguistic structures are described using feature structures that give partial information about the object being modeled, a linguistic structure being described by a set of feature structures that mutually constrain the description. Feature structures are partially ordered according to a subsumption ordering interpreted as an ordering on the amount of conveyed information; the combination of information is defined as the unification of feature structures. Formalisms based on feature structure and unification are declarative, and they can be given a sound formal semantics. Combining object-oriented approaches to linguistic description with unificationbased grammar formalisms, as in HPSG, is very attractive. On one hand, we gain the advantages of the object-oriented approach: abstraction and generalization through the use of inheritance. On the other hand, we gain a fully declarative framework, with all the advantages of logical formalisms: expressive power, simplicity, and sound formal semantics. To arrive at such a result, we have to enrich the formalism of feature structures with the notion of inheritance and abandon some of the procedural features of object-oriented languages in order to gain referential transparency. Referential transparency is one of the characteristic properties of declarative languages (Stoy 1977), where the meaning of each language construct is given by a few simple and general rules. For example, the value of a variable should be independent from its position within the scope of its declaration. This is true for PROLOG variables inside a clause, but not for PASCAL or LISP variables that make use of assignment. A higher level example is the meaning of a procedure: it is not transparent if the procedure makes use of global variables that are set by some other procedure. Similarly, the meaning of a PROLOG predicate should be transparent because there is no global variable, but a predicate definition might be modified during execution by imperative predicates such as assert and retract, thus destroying the referential transparency of pure PROLOG. Clearly, most of the object-oriented languages lack referential transparency in several ways, using for example procedural attachments for object methods. Another example is the use of nonmonotonic inheritance, which is advocated in computational linguistics by, for example, Evans and Gazdar 1989; Bouma 1990; De Smedt and de Graaf 1990; and Fraser and Hudson 1990. Nonmonotonic inheritance is seen as a practical device designed to deal with exceptions, but such a feature goes against generality and referential transparency. Furthermore, as expressed by Etherington et al. 1989, a still unresolved issue in nonmonotonic reasoning is the issue of ... scaling formal non-monotonic theories up to real problems (merely a formality?). Most existant theories are intractable—some don&apos;t have even a proof theory—and it is often difficult to tell how large bodies information will (or even Given the complexity of the state of the art in nonmonotonic reasoning and the lack of 160 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms a basic commonly agreed formalization,&apos; the issue of nonmonotonicity is not addressed in the work described in this article. Knowledge representation languages are evolving toward more declarativity, as exemplified by the evolution from KL-ONE (Brachman and Schmolze 1985) to languages such as CLASSIC (Borgida et al. 1989) or Loom (MacGregor 1988, 1990). The describing the objects (the data model of object-oriented systems) has always been more declarative than the (procedural attachment or methods), and the current trend is to integrate those two components more closely, where the assertional component is some kind of rule-based system, as in Loom (Yen, Neches, and MacGregor 1988). Typed feature structures are very similar to structured objects of object-oriented languages and to conceptual structures of knowledge representation languages. Thus, typed feature structures have the potential to act as a lingua franca for both computational linguistics and artificial intelligence, and this should ease the communication between those two worlds. Since conceptual structures are used for example in text generation (Bourbeau et al. 1990) or knowledge-based machine translation (Nirenburg et al. 1992), typed feature structures provide an attractive alternative to current procedural implementations. In Section 2, we present a language that combines the notions of partial information and inheritance in a fully declarative framework. It is based on feature structures augmented with the notion of types, which are organized into an inheritance network. Using types, it is possible to define structured domains of feature structures and to classify feature structures. Logical conditions are attached to types, akin to method attachment, but in a fully declarative framework. Recursivity is an integral part of the language, giving the necessary expressive power for describing complex recursive linguistic structures. We end the section by an overview of the TFS abstract rewrite used for computing descriptions of the meaning of typed feature Section 3 describes the use of inheritance in two examples of unification grammars using the TFS formalism: an HPSG grammar for a fragment of English and an LFGstyle transfer grammar for a small machine translation problem between English and French. 2. Inheritance Networks of Typed Feature Structures the existence of an (abstract) informational domain example, the set linguistic objects. structures objects of this universe by specifying values for attributes of objects and equality constraints between some values. More precisely, as feature structures can provide only partial information about the objects describe, a feature structure denotes a objects in this universe. This set could be a singleton set, for example, in the case of atomic feature structures. Feature are ordered by a subsumption relation: a feature structure feature structure f2 iff same or less information f2: fi &gt; f2. In universe, this means that the set described by fi is a the set described by f2. Note that there can be feature structures that cannot consistently describe the 2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature structures are derived, plays this role in formal accounts of feature structures. 3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been implemented at the University of Stuttgart by Martin Emele and the author and has been used to test several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al. 1990; Zajac 1990a; Bateman and Momma 19911. 161 Computational Linguistics Volume 18, Number 2 same objects. For example, a feature structure describing verb phrases and a feature structure describing noun phrases are not consistent: the intersection of the sets they denote is usually empty. As different sets of attribute-value pairs make sense for different kinds of objects, we also divide our feature structures into different types. These types are ordered by subtype relation: a type a another type if t2 provides least as much t1. For example, assuming that a verb phrase is a phrase, then the set of verb phrases is included in the set of phrases. Using types to model this taxonomic the type symbol the set of verb phrases, the symbol set of phrases, and we define a subtype of This description implies that, if we know that a linguistic object is a verb phrase, we can deduce that it is a phrase. This deduction mechanism is expressed in our system as inheritance. with each type we associate expressed as feature structures, thereby defining an inheritance network of typed feastructures: if a feature structure is of type there exist supertypes of inherits all the attribute-value pairs and equality constraints of the feature structures with all the supertypes of Computation is performed by a typed feature structure machine capable of checking a set of type constraints defined as an inheritance network of typed feature structures. Given a typed feature structure inheritance network, we query the machine by asking if some feature structure satisfies the constraints defined by the network. To produce the answer, the system proceeds by gradually adding the constraints that should be satisfied by the query: an answer will be a set of feature structures where each feature structure is subsumed by the query and where all the type constraints of the network hold on all substructures of the elements of the answer. The answer is the empty set when the query does not satisfy the constraints defined by the network. Related work. The basic approach described in this section is based on original work by Ait-Kaci (1984, 1986) on the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and Alt- Kaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was already a departure from Ait-Kaci&apos;s KBL language, implementing a different evaluation strategy that allowed the use of cyclic feature structures. Compared with the KBL language, the current TFS language • allows the use of cyclic feature structures, • has a simple operational semantics implementing an inheritance rule and a specialization rule, • uses a lazy evaluation scheme for the evaluation of constraints, • implements static coherence checks, • has a simple syntax distinguishing the definition of the partial order on type symbols, and the definition of constraints associated to types. 162 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms 2.1 Types In the following presentation, we adopt an algebraic approach based on lattice theory (Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for example, a proof-theoretical approach using an adaptation of a feature logic (Rounds and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of these different models, as this is done for the LIFE language in Ait-Kaci and Podelski (1991). The presentation is nevertheless rather informal, and a more technical account can be found in Emele and Zajac (1990a) and Zajac (1990b). The universe of feature structures is structured in an inheritance network that defines a partial ordering on kinds of available information. The backbone of the is defined by a finite set of type symbols with a partial ordering on partially ordered set (poset) ,&lt;). ordering &lt; defines the subtype for A, E read A &lt; &amp;quot;A is a subtype of call the smallest of minimal types. have a well-behaved type hierarchy, we require that such that: T the symbols the greatest element and the least element of any two type symbols A and a greatest common lower written A poset where greatest common lower bounds exist is a meet semi-lattice: we introduce a new operation A B = where A B called the Since we allow the user to specify any finite poset, a technicality arises when two types do not have a unique greatest common lower bound: in that case, the set of greatest common lower bounds is interpreted disjunctively using the following powerlattice construction, which preserves the ordering and the existing meets. poset &lt;) embedded in set the set all nonempty subsets of incomparable elements of T (the &amp;quot;crowns&amp;quot; of are partially ordered by the Hoare ordering X Y Vx 3y that &lt;y. canonical injection of takes any element singleton preserves the ordering: CH &lt; meet between two elements X Y, defined as the union of the intersection of each of the principal ideals generated by the elements of X each of the principal ideals generated by the elements of then extracting the elements. It is easy to see that existing meets are preserved: {z} = iffz=xAy. meets are added, as in the following example. Let b, c, d} , &lt;) a poset &lt;b, d &lt; a, c &lt;b, &lt;a poset is represented using a Hasse diagram Figure 1). The meet not exist, but {a} {c, d}, this meet interpreted disjunctively. join between two elements X and X U Y, defined as the set maximal elements of the union of X and can be shown that, equipped with (T), a distributive lattice. This construction is carried over to typed feature structures, with the property that this definition of the join does not lose information, contrary to the strict generalization of feature structures: this definition of 4 T represents underspecified information, and I represents inconsistent information. 163 Computational Linguistics Volume 18, Number 2 Figure 1 A poset and the set of the principal ideals generated by the elements of the poset ordered by set inclusion. the join is appropriate to represent disjunctive information as, for example, generated by a nondeterministic computation (see Section 2.4). This powerlattice construction is completely transparent to the user, and to simthe presentation, we will assume in the following that ,&lt;) a meet semilattice. 2.2 Feature Structures We use the attribute-value matrix (AVM) notation for feature structures, and we write the type symbol for each feature structure in front of the opening square bracket of the AVM. In the remainder of this section, we shall implicitly refer to some given signature &lt;,F) ,&lt;,) a type hierarchy and F is a set of feature symbols, and we also assume a set of variables A typed feature structure t is then an expression of the form la is a variable in a set of variables a type symbol in &gt; are features in .F, and ti,...,are typed feature structures. We have to add some restrictions that capture properties commonly associated structures: 1. A feature is a selector that gives access to a substructure: it has to be unique for a given feature structure. 2. 1 represents inconsistent information: it is not allowed in a feature structure. 3. A variable is used to capture equality constraints (&amp;quot;reentrancy&amp;quot;) in the feature structure, and the shared value is represented only once: there is at most one occurrence of a variable Ei that is the root of a structure from Given a signature (T, &lt;, F), feature structures are partially ordered by a subsumprelation. This captures the intuitive notion that a feature structure 164 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms information than a feature structure more specific than feature structure a structure t all paths in in all equality constraints in in for a given path in type is greater or equal than the corresponding in Since we have a partial order on feature structures, the meet operation between feature structures defined in the usual way as the greatest common bound of is computed using a typed unification algorithm. A feature structure is represented as a graph where each node has a type, an equivalence class used to represent equational constraints (&amp;quot;co-references&amp;quot;), and a set of outgoing arcs. The unification algorithm uses the union/find procedure on an inverted set representation of the equivalence classes adapted by Aft-Kaci (1984) after Huet (1976). The actual algorithm used in the system is optimized using several different techniques to minimize copying and to behave as efficiently as a pattern-matcher in cases when one of the feature structures subsumes the other (Emele 1991). 2.3 Inheritance Network of Feature Structures The template mechanism (as, for example, in PATR-II [Shieber 1986]) already provides a simple inheritance mechanism used to organize lexical descriptions. In comparison, networks of typed feature structures are more expressive and provide a more general and more powerful inheritance mechanism, which allows the use of recursive type definitions, whereas recursivity is forbidden in templates since they are expanded statically using a macro-expansion mechanism. Furthermore, typing provides a notion of well-formedness that is used to implement a type-discipline and consistency checks, giving the user the means of checking statically the coherence of a set of type definitions. Type Discipline. different combinations of attribute value pairs make sense for different kinds of objects, we divide our feature structures into different classes by associating with a type a certain class of feature structures. Each type defines a specific collection of features that are appropriate for it, restrictions on their possible and equality constraints between values. The definition of a type expressed as a feature structure (of type A). A type symbol that does not have any definition is called an atomic type. A type that has a definition is called a complex type. The association of types and feature structures allows the definition of wellformedness conditions for feature structures using the following two typing rules: Typing Rule 1: A feature appearing in a feature structure has always to be declared as appropriate for some type. The user cannot introduce arbitrary features: one must declare all the features that one will use. All features that are not explicitly declared as being appropriate for some type are by default defined as being appropriate for I. Typing rule 2: A feature structure cannot have a feature that is not appropriate for its 165 Computational Linguistics Volume 18, Number 2 type or for one of the supertypes. Thus, any feature structure with a feature f has to belong to some type for which f is appropriate. These well-formedness conditions are enforced at compile-time using a type inference procedure which infers for each feature structure its possible minimal types. If the inferred type is 1, an error is reported indicating that the respective feature structure does not obey the typing rules. The internal representation built by the compiler uses these inferred minimal types to ensure that it is not possible to add an arbitrary feature to a feature structure during computation, but only those declared for the type of the structure, thus preserving well-formedness. Inheritance and Generalization. types are organized in an inheritance network, a type inherits all the features, value restrictions, and equality constraints from all its super-types monotonically: the constraints expressed as feature structures are conjoined using typed unification. The compiler makes sure that the user has specified an inheritance network, building an internal representation where for every types such that &lt; B have (A) &lt; If there is a type that 0 1 (A) = _1_, network is inconsistent and an error is reported. The compiler also has a generalization step where all constraints common to all subtypes of a given type are also defined for that type. Interpreting an Inheritance Network. constraints expressed as an inhernetwork are interpreted as follows. For a given typed feature structure the feature structure to the domain of A (i.e., it satisfies the constraints associated with A) if and only if: Inheritance Rule: the constraints specified by the de finition of by the defiof all the supertypes of Specialization Rule: the constraints specified by the definitions of at least one of subtypes of The inheritance rule states the necessary conditions for a feature structure of type A to satisfy the constraints associated with A. The specialization rule states the sufficient conditions and implements a kind of closed-world assumption: a type is exhaustively by its subtypes. For example, a feature structure of type be an list (type a nonempty list nothing else. These two rules are implemented by the TFS interpreter described in Section 2.4. simple example of an inheritance network of feature structures is displayed in Figure 2 using Hasse diagrams. The subnetwork on the right defines a domain of lists expressed as feature structures: the set of all possible lists is defined the type has no associated constraints. This type has two subtypes: NIL an atomic type and represents the empty list; CONS a complex type and represents the set of all possible nonempty lists and defines the following constraints. A feature structure of type Inheritance is pre-computed statically: &lt; B =t = 166 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms 1: LIST APPEND 2: LIST] 3: LIST 2: [&apos;LIST 1: NIL fl El APPEND1 restai APPEND 1: DI 3: CONS[first:p 3: APPENDO LIST T NIL CONS Figure 2 hierarchy for LIST and APPEND I omitted). only two features (first typing rule): value is the element of the list and can be anything, and value is constrained to be a list.&apos; Note that this latter constraint is recursive. subnetwork on the left defines the domain of the encoded feature structures. The constraints associated with the supertype that it has three arguments, identified by the features 1, 2, and 3, and that the values all arguments should be in the The subtypes the two cases where the first argument is the empty list the list a way similar to the classical As for the type is possible to have additional constraints that are not represented in the feature structure proper: they are introduced by the &apos; : sign.&apos; conditions can be inherited and are conjoined using the logical condition states the recursive constraint on the concatenation of the which is expressed as a feature structure of type 2.4 The TFS Abstract Rewrite Machine meaning (denotation) of a typed feature structure a universe by inheritance network is represented by the largest set of feature structures --= that, for all t, for all substructures a minimal type and &lt; first condition says that all the elements of the constraints expressed second condition says that all the elements of satisfy the constraints defined Conversely, using the second typing rule, we can deduce that a possible type for : the combination of defined as appropriate for construction provides room for future evolution of the formalism by adding new kinds of constraints that cannot be directly expressed in the AVM format e.g., negation. A definition : — Y, read &amp;quot;X such that Y and 167 Computational Linguistics Volume 18, Number 2 Figure 3 Rewrite rules for LIST and APPEND. the network. If is empty, the feature structure inconsistent (modulo the of the inheritance network). can be finite, e.g. in the case of a dictionary, but it can also be infinite in the case of recursive types: for example, the set of feature subsumed by the (infinite) set of all possible lists represented as In this section, we describe an abstract rewrite machine for computing the representation of the denotation of typed feature structures given an inheritance network. The rewrite mechanism is based on a variant of narrowing&apos; adapted to feature structures. An inheritance network of feature structures is compiled into a rewriting system follows: each direct link between a type a subtype a rewrite rule the form A[a] B[19] where A[a] and B[b] are the definitions of Figure 3 shows the rewrite rules corresponding to the network of Figure 2. The interpreter is given a &amp;quot;query&amp;quot; (formulated as a typed feature structure) to evaluate. The first step is to check that the feature structure respects the two typing rules (Section 2.3.1). The idea is then to try to satisfy all the constraints defined by the inheritance network by incrementally adding more constraints to the query using the rewrite rules (nondeterministically) to get closer to the solution step by step. The rewriting process stops when conditions 1 and 2 described above hold. rewrite step for a structure defined as follows: if a substructure of of type there exists a rewrite rule A[a] 13[19] such that A[a] U _L, A[a] the right-hand side unified with the substructure path a new is more specific than 4). 8 See Nit-Kaci (1984), Pollard and Moshier (1990), and Emele and Zajac (1990a) for fixed-point characterizations of the denotation of typed feature structures. 9 Narrowing uses unification instead of pattern-matching for checking the applicability of the 1.h.s. of a rule. Narrowing is used in the logic programming paradigm (e.g., as an alternative to resolution for implementing PROLOG interpreters). Pattern-matching is used in the functional programming paradigm. NIL T rest: LIST 1: NIL APPENDO 2: 0 LIST 3: 0 APPEND1 1: rest:El g 3: CONS[first:E rest:0 LIST LIST [1: LIST] APPEND 2: LIST 3: LIST [1: LIST APPEND 2: LIST 3: LIST [1: 01 E 3: El 168 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms Figure 4 A rewrite step. The first condition checks that the rule is applicable: the 1.h.s. has to be consistent with the substructure. The implementation factorizes common 1.h.s., avoiding the DNF if a 1.h.s. a rule -r not consistent with the substructure, this branch is a failure branch, and all rules v &lt; 1 discarded in one step without further computation. second condition implements a rewriting if A[a] equal to all rules B[b] be applied with success, and failure could come only from the rewriting of some other substructures after the exploration of all choices for avoid the exploration of failure branches as much as possible, the evaluation of substructure suspended until the evaluation of some other substructure having part in common with specific, narrowing the set of potential for the subtypes of A for the search space is explored &amp;quot;intelligently,&amp;quot; postponing the evaluation of branches of computation that would correspond for example to uninstantiated PROLOG goals (see for example van Hentenryck and Dincbas [1987], van Hentenryck [1989] on evaluation techniques in constraint logic programming). Rewrite steps are applied nondeterministically everywhere in the structure until no further rule is applicable.&apos;° The choice of which substructure to rewrite is only partly determined by the availability of information (using the lazy rewriting rule). When there are several substructures that could be rewritten, the computation rule is to choose one of the outermost ones, i.e., one closest to the root of the feature structure (innermost strategies are usually nonterminating). This outermost rewriting strategy is similar to hyperresolution in logic programming. In comparison, PROLOG uses a leftmost computation rule. For a given substructure, the choice of which rule to apply is done nondeterministically, and the search space is explored depth-first using a backtracking scheme. Although this strategy is not complete (a complete breadth-first search strategy could be used for debugging purposes), the use of the outermost rule has favorable termination 10 Conditions do not change this general scheme (they are evaluated using the same rewriting mechanism) and are omitted from the presentation here for the sake of simplicity. See for example Dershowitz and Plaited (1988) and flop (1990) for a survey on rewriting. 169 Computational Linguistics Volume 18, Number 2 properties when compared to PROLOG&apos;S leftmost rule: there are problems where a TFS computation will terminate when the corresponding logic program implemented in for example, for left-recursive rules in naive PROLOG implementations of DCGs. 3. Inheritance and Constraint-Based Grammars 3.1 Related Approaches In the constraint-based framework, a grammar is rega rded as a set of constraints to be satisfied by a given linguistic object: parsing and generation differ only in the nature of the &amp;quot;input,&amp;quot; and use the same constraint evaluation mechanism. The properties of a computational framework for implementing constraint-based grammars are: • A unique general constraint solving mechanism is used: grammars define constraints on the set of acceptable linguistic structures. • As a consequence, there is no formal distinction between &amp;quot;input&amp;quot; and &amp;quot;output.&amp;quot; For example, the same kind of data structure could be used to encode both the string and the structural description, and, as for the HPSG sign (Pollard and Sag 1987), they could be embedded into a single data structure that represents the relation between the string and the associated linguistic structure. • Specific mapping properties, based on constituency, linear precedence, or functional composition, are not part of the formalism itself, but can be encoded explicitly in the formalism. An approach that uses a unique deductive mechanism for parsing and generation is described in Dymetman and Isabelle (1988). Within this approach, a lazy evaluation mechanism based on the specification of input/output arguments is implemented (in PROLOG), and the evaluation is completely data-driven: the same program parses or generates, depending only on the form of the input structure. A constraint-based grammar does not need a context-free mechanism to build up constituent structures for parsing or generation: Dymetman, Isabelle, and Perrault (1990) describe a class of reversible grammars (&amp;quot;Lexical Grammars&amp;quot;) based on a few composition rules that are very reminiscent of categorial grammars. Other kinds of approaches have been proposed, e.g., using a dependency structure and linear precedence relations (Reape 1990; see also Pollard and Sag [19871). In Saint-Dizier (1991), linear precedence rules are defined as constraints in a language based on typed feature structures and SLD-resolution, which is used to experiment with GB theory. In the following sections, we describe two examples of constraint-based grammars: an HPSG grammar for a fragment of English, and an LFG-style transfer grammar for a small machine translation problem between English and French. Phrase Structure Grammar In general, a grammar describes the relation between strings of words and linguistic structures. To implement a constraint-based grammar in TFS, we have to encode both kinds of structures using the same data structure provided by the TFS language: typed feature structures. A linguistic structure will be encoded using features and values. Conditions that constrain the set of valid linguistic structures have to be declared explicitly. A string of words will be encoded as a list of word forms, using the same kind</abstract>
<note confidence="0.798339833333333">170 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms I append(cij, 0) COMP-DTRS PHRASE syn: SUBCAT 1: ID SUBCAT-FP dtrs: -FP. 2: Ili</note>
<abstract confidence="0.908810257763976">snbcat: [D] IJ head-dtr: [syn: [subcat: comp-dtrs: In Figure 5 The HSPG subcategorization principle and its TFS encoding. of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based on the notion of typed feature structures (Pollard 1990), and is thus a good candidate to illustrate the possibilities of the TFS formalism. The following presentation is based on Emele (1988) and Emele and Zajac (1990b). The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure, syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot; the relation between a string and a linguistic structure is encoded as a single feature structure representing the &amp;quot;sign.&amp;quot; HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type the constraints associated with types HEAD-FP, For the HPSG subcategorization principle is encoded in TFS usinheritance to model implication, and the TFS relation encode the constraint on concatenation (Figure 5). In a similar way, Head Feature Principle and the Semantics Principle. type divided into several subtypes corresponding to different mappings between a string and a linguistic structure. We first have the basic distinction between phrases and words. The definition of a phrase recursively relates subphrases and substrings, and defines the phrase as a composition of subphrases and the string as the concatenation of substrings. Since the formalism itself does not impose any constraints on how the relations between phrases and strings are defined, the grammar writer has to define them explicitly. In HPSG (Pollard and Sag 1987), the ordering of phrases is defined using linear precedence relations: the order in which the substrings associated with subphrases are concatenated to give the string associated with a phrase are guided by these linear precedence relations (Reape 1990). In the example given below (Figure 6), we make simplifying assumptions: the is not used and there are two possible orderings for complements. The type IDP1 encodes Grammar Rule 1 (Pollard and Sag 1987, pp. 149-155), which says a &amp;quot;saturated phrasal sign,&amp;quot; i.e., a feature structure with : [subcat : ()]], the combination of an unsaturated phrasal head with one phrasal complement on the left. example, for structures like —4 VP, S the &amp;quot;saturated phrasal sign,&amp;quot; 11 We will use a more condensed notation for lists with angle brackets provided by the TFS language: a list CONS [first: Mary, rest: CONS [first : sings, rest: NIL]] is written as &lt;Mary sings&gt;. 171 Computational Linguistics Volume 18, Number 2 Figure 6 of the HPSG from &amp;quot;principles&amp;quot; (given here without their definitions) and is subdivided into two subtypes corresponding to different complement orderings. phrasal complement and the unsaturated phrasal head. Furthermore, the (the value of the of the IDP1 phrase is the concatenation of the string of the complement with the string of the head. type Grammar Rule 2 and states that an &amp;quot;unsaturated phrasal i.e., a feature structure with [subcat : (SIGN)]1, the combination of a head with any number of complements on the right (e.g., for --+ V associated with the concatenation of the string of the head with the of the strings of the complements, where the relation defines in which order the complements strings are concatenated. The difference between the parsing and the generation problem is then only in the form of the structure given to the interpreter for evaluation. A query for the parsing problem is an underspecified structure where only the string is given; conversely, a query for the generation problem is an underspecified structure where only the semantic form is given (Figure 7). In both cases, the interpreter uses the same set of rewrite rules to fill in &amp;quot;missing information&amp;quot; according to the type definitions. The result in both cases is exactly the same: a fully specified structure containing the string, the full semantic form, and also all other syntactic information such as the constituent structure (Figure 8). Transfer in Machine Translation We have sketched above a very general framework for specifying mappings between a linguistic structure, encoded as a feature structure, and a string, also encoded as a feature structure. We apply a similar technique for specifying transfer rules for machine translation, which we prefer to call &amp;quot;contrastive rules&amp;quot; since there is no directionality involved (Zajac 1989; 1990a). SlJECAT-FP HEAD-FP SEM-FP PHRASE[dtrs: TREE] phon: a) [syn: rcat: ()) IDP1 head-dtr: PHRASE[phon: Eiji dtrs: comp-dtrs: (SIGN[phon: al]) 1: ijl [ :- APPEND 2: al 3: fa IDP2[ al syn: snbcat: (SIGN)] dtrs: [head-dtr: WORD[phon: , 3: III 1: El ORDER-COMPEcomps: 01 phon: 172 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms for PHRASE[phon: (Kim ate every cookie)] Query for generation PHRASE sem: [ argl: EAT arg2: ind: [restr: [name: KIM] 1] 1] spec: EVERY . r r Ind: trestr: Vein: COOKIE]]] Queries for parsing and generation. Figure 8 common solution the and generation problems. The idea is rather simple: assume we are working with linguistic structures similar to LFG&apos;s functional structures for English and French as proposed in Kaplan et al. We define a translation relation as a type two features, English structure and the French structure. This &amp;quot;bilingual sign&amp;quot; is defined the lexical structure: each subtype of a lexical correspondence between a partial English structure and a partial French structure for a given lexical equivalence. Such a lexical contrastive definition for a verb also has to pair the arguments recursively. This is expressed in the condition part of the definition (Figure 9) a recursive condition the arguments. The translation of syntactic and semantic features, like tense or determination, is also specified in the condition part, and these contrastive definitions can and should be defined separately from the lexical definitions as different subnetworks. is used here to capture generalizations over the information in the example of Figure 9 can be unpacked and redefined as follows. We have &apos;DPI (Kim ate [ subcat: 0 (...) head dtr: IDP2 ... - EAT var: 111 N-OBJ ind: MX IND named: El sem: 2ARG-REL ] 1 I name: KIM rein: NAMING [spec: EVERY var: 10 arg2: I. IE 173 Computational Linguistics Volume 18, Number 2 Figure 9 A transfer rule. Figure 10 Part of an inheritance network of transfer relations for verbs. levels of generalization for the translation of verbs: subdivided into relations for translating, for example, transitive verbs intransiverbs The condition that specifies the translation of tenses is for the whole class of verbs condition that specifies the transrelation between subjects is defined for intransitive verbs cannot be specified for all verbs since it does not apply to, e.g., impersonal verbs. This leaves only the translation of predicates to be defined at the leaves of the hierarchy (Figure 10). Thus, at each level of generalization, we specify the minimal amount of information needed for translation. The same kind of organization can be used for nouns, where syntactic features such as determination are associated with the higher types, and where the minimal types define the equivalences between lexical forms of nouns themselves. TALI-LEX-FALL &lt; TAU-LEX-ITV. eng: subj: tense: I tense: TAU-LEX-FALL : fr :- TAU-LE Ed= =TM TOMBER subj: GE= R4ensGI fr: IM113113 121=t1 TAU /\ TAU-LEX... TAU-TENSE... engitense: =on eng: TAU-LEX-FALL fr: pied: TOMBEIL] MILITC21 If-tensel fr: subj: .fr: =L11 MIR =III V-subfl 174 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms Query TAU eng: pied: FALL STUDENT I red: subj: spec: Answer TAU-LEX-FALL _ pied: subj: FALL pied: spec: [ E-PRES TOMBER pied: spec: { F-PRES STUDENT - eng: tense: pied: ETUDIANT _ fr: subj: tense: Figure 11 Query and answer for the translation of &amp;quot;A student falls.&amp;quot; The transfer problem for each direction is then stated in the same way as for parsing or generation: the input structure is an underspecified &amp;quot;bilingual sign&amp;quot; where only the structure for the source language is given. Using the contrastive grammar, interpreter fills in missing information and builds a bilingual (Figure 11). It is not necessary to specify in the contrastive definitions all monolingual constraints that have to be satisfied by the English structure and by the French structure. We can assume that we have monolingual grammars that define the appropriate mappings between the set of English sentences and the set of associated English structures, and similarily for French. Using these monolingual constraints in addition to the contrastive grammar, the TFS interpreter would build the fully specified monolingual structures, implementing a constraint-based translation system. 3.4 Termination Problems For parsing and generation, since no constraints are imposed on the kind of mapping between the string and the semantic form, termination has to be proved for each class of grammar and for the particular mechanism used for either parsing or generation with this grammar. If we restrict ourselves to classes of grammars for which terminating evaluation algorithms are known, we can implement those directly in TFS. However, the TFS evaluation strategy allows more naive implementations of grammars, and the outermost rewriting of &amp;quot;sub-goals&amp;quot; terminates on a strictly larger class of programs than for corresponding logic programs implemented in PROLOG. Furthermore, the grammar writer does not need, and actually should not, be aware of the control that follows the shape of the input rather than a fixed strategy, thanks to the lazy evaluation mechanism. 12 See also Reape (1990) for another approach to MT using feature structures and based on Whitelock (1990). 175 Computational Linguistics Volume 18, Number 2 For HPSG-style grammars, completeness and coherence as defined for LFG, and extended to the general case by Wedekind (1988), are implemented in HPSG using the &amp;quot;subcategorization feature principle&amp;quot; (Johnson 1987): for the TFS implementation of HPSG, termination is guaranteed, at least for the simplified version containing only head-complement structures, described in Section 3.2. Termination conditions for parsing are well understood in the framework of context-free grammars. For generation using feature structures, one of the problems is that the input could be &amp;quot;extended&amp;quot; during processing, i.e., arbitrary feature structures could be introduced in the semantic part of the input by unification with the semantic part of a rule. However, if the semantic part of the input is fully specified according to a set of type definitions describing the set of well-formed semantic structures (and this condition is easy to check), this cannot arise in a type-based system since it is not possible to add arbitrary features to a typed feature structure. A more general approach is described in Dymetman, Isabelle, and Perrault (1990), who define sufficient termination properties for parsing and generation for the class of &amp;quot;Lexical Grammars.&amp;quot; These termination properties are conditions on the existence of &amp;quot;conservative guides&amp;quot; for parsing and generation and seem generalizable to other classes of grammars as well, and are also applicable to TFS implementations. Since Lexical Grammars are implemented in PROLOG, left-recursion must be eliminated for parsing and for generation, but this does not apply to TFS implementations. The idea of conservative guides is relatively simple and says that for parsing, each rule must consume a nonempty part of the string, and for generation, each rule must consume a nonempty part of the semantic form. These conditions seem to be equivalent as to require the existence of a well-founded relation on strings (for parsing) and of a well-founded relation on semantic forms (for generation). The existence of such well-founded relations is actually a necessary condition for proving the termination parsing and (see Deville [1990] a more general discussion on wellfounded relations in the context of logic programming). Termination for reversible transfer grammars is discussed in van Noord (1990). One of the problems mentioned there is the extension of the &amp;quot;input,&amp;quot; as in generation, and the answer is similar (see above). However, properties similar to the &amp;quot;conservative guides&amp;quot; of Dymetman, Isabelle, and Perrault (1990) have to hold in order to ensure termination. The TFS system has been developed to provide a computational environment for the design and the implementation of formal models of natural language. The TFS formalis designed as a language can be used to design and implement formal linguistic models. It is not a programming language: it does not offer means of defining control information that would make execution more efficient (but less general), as it would be needed if it would be envisaged to use the system in an application-oriented environment (e.g., as a parser in a natural language interface to a database system). From formal linguistic models developed in TFS, it could be envisaged to develop programs, i.e., parsers or generators, that would efficiently implement the declarative knowledge contained in the formal specifications.&amp;quot; for example in Biggerstaff and Perlis (1989) the papers on the development of programs from specifications, a very important issue in software engineering. See also Ait-Kaci and Meyer (1990) for a programming language based on typed feature structures. 176 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms The TFS system is implemented using rewriting techniques in a constraint-based architecture adapted to feature structures: • The language is a logical language directly based on typed feature structures, and supports an object-oriented style based on multiple inheritance. • Grammars are expressed as inheritance networks of typed feature structures. They define constraints on the set of acceptable linguistic structures. As a consequence, there is no formal distinction between &amp;quot;input&amp;quot; and &amp;quot;output.&amp;quot; • A unique general constraint solving mechanism is used. Specific mapping properties, based on constituency, linear precedence or functional composition, are not part of the formalism itself, but can be encoded explicitly using the formalism. Although the current implementation is very much at the level of an experimental prototype, and is still evolving, it has validated the basic concepts of the language and of the implementation, and has been used to test different linguistic models and formalisms such as LFG, DCG, HPSG, and SFG on small examples. From these various experimentations, we have defined extensions and improvements, both on the language and on the implementation, that are needed for scaling up the system. On the language side, more expressivity is needed. For example, disjunctions over feature structures, various kinds of negation (Alt-Kaci 1986), and sets of feature structures (Pollard and Moshier 1990) are necessary to formalize, e.g., nontrivial semantic structures. Some types together with a specific syntax and associated operations could be conveniently added to the system as libraries of built-in types, e.g., characters, strings, and trees. On the implementation side, the use of implementation techniques adapted from PROLOG implementations, constraint satisfaction languages, and object-oriented languages can be beneficial to the implementation of typed feature structure—based sysand have to be more thoroughly of the major efficiency issues in the current implementation is the lack of an efficient indexing scheme for typed feature structures. For example, since the dictionaries are accessed using unification only, each entry is tried one after the other, leading to an extremely inefficient behavior with large dictionaries. Thus, the use of a general indexing scheme based on a combination of methods used in PROLOG implementations and in object-oriented database systems is necessary and will be implemented in a future version of the system. Acknowledgments The design and the implementation of the TFS system have been carried out in cooperation with Martin Emele. I would like to thank Stefan Momma and Ulrich Heid; their numerous comments and advices helped to make this article more readable. I would also like to thank anonymous referees for their many detailed and helpful comments. All remaining errors are of course the sole responsibility of the author. 14 For example, the use in the current implementation of several techniques adapted from PROLOG implementations such as structure sharing, chronological dereferencing (Emele 1991) and last call optimization, have improved efficiency by several orders of magnitude over previous &amp;quot;naive&amp;quot; implementations.</abstract>
<note confidence="0.608402666666667">177 Computational Linguistics Volume 18, Number 2 References</note>
<abstract confidence="0.913681333333333">Ait-Kaci, Hassan (1984). &amp;quot;A lattice theoretic approach to computation based on a calculus of partially ordered type</abstract>
<affiliation confidence="0.6525915">structures.&amp;quot; Doctoral dissertation, University of Pennsylvania, Philadelphia,</affiliation>
<abstract confidence="0.589044">PA. Ait-Kaci, Hassan (1986). &amp;quot;An algebraic semantics approach to the effective of type equations.&amp;quot;</abstract>
<note confidence="0.880556322981367">Science, 293-351. Ait-Kaci, Hassan, and Meyer, Richard (1990). Wild_LIFE, a user manual. DEC-PRL Technical Note PRL-TN-1, Rueil-Malmaison, France. Ait-Kaci, Hassan, and Podelski, Andreas (1991). Towards a meaning of LIFE. DEC-PRL Research Report PRL-RR-11, Rueil-Malmaison, France. Bateman, John A., and Momma, Stefan (1991). The nondirectional representation of Systemic Functional Grammars and Semantics as Typed Feature Structures. IMS Technical Report, University of Stuttgart, Germany. Biggerstaff, Ted J., and Perlis, Alan J. (eds). Reusability. Press-Addison-Wesley. Garrett (1940). Theory. American Mathematical Society, third edition, 1973. Borgida, Alexander; Brachman, Ronald J.; McGuinness, Deborah L.; and Resnick, Lori Alperin (1989). &amp;quot;CLASSIC: a structural data model for objects.&amp;quot; In Proceedings, 1989 ACM SIGMOD International Conference on Management of Oregon. Bouma, Gosse (1990). &amp;quot;Non-monotonic inheritance and unification.&amp;quot; In Proceedings, Workshop on Inheritance in Language Processing, for Language Technology and Al, Tilburg University, The Netherlands. Bourbeau, L.; Carcagno, D.; Goldberg, E.; Kittredge, R.; and Polguere, A. (1990). &amp;quot;Bilingual generation of weather forecasts in an operation environment.&amp;quot; In Proceedings, 13th International Conference on Linguistics Helsinki. Brachman, Ronald J., and Schmolze, James G. (1985). &amp;quot;An overview of the KL-ONE knowledge representation language.&amp;quot; Science 171-216. Carpenter, Bob (1990). &amp;quot;Typed feature structures: inheritance, (in)equality and In Workshop on Inheritance in Natural Language Processing, Institute for Language Technology and Al, Tilburg University, The Netherlands. Daelema:ns, Walter (1990). &amp;quot;Inheritance and object-oriented natural language In Workshop on Inheritance in Natural Language Processing, Institute for Language Technology and Al, Tilburg University, The Netherlands. Dershowitz, N., and Plaisted, D. A. (1988). programming.&amp;quot; In 11, by Hayes, Michie, and Richards. Oxford: Clarendon Press. De Smedt, Koenraad, and de Graaf, Josje (1990). &amp;quot;Structured inheritance in frame-based representation of linguistic In Workshop on Inheritance in Natural Language Processing. Institute for Language Technology and Al, Tilburg University, The Netherlands. Yves (1990). programming. Program Development. MA: Addison-Wesley. Dymetman, Marc, and Isabelle, Pierre (1988). &amp;quot;Reversible logic grammars for translation.&amp;quot; In 2nd International Conference on Theoretical and Methodological Issues in Machine Translation Natural Language. PA. Dymetman, Marc; Isabelle, Pierre; and Perrault, Francois (1990). &amp;quot;A symmetrical approach to parsing and generation.&amp;quot; In Proceedings, 13th International Conference on Linguistics Helsinki. Emele, Martin (1988). &amp;quot;A typed feature structure unification-based approach to In WGNLC of the University, Japan. Emele, Martin (1991). &amp;quot;Unification with lazy copying.&amp;quot; In Annual Meeting of the ACL. CA. Emele, Martin, and Zajac, Remi (1989). &amp;quot;RETIF: A rewriting system for typed feature structures.&amp;quot; ATR Technical report TR-I-0071, ATR, Kyoto. Emele, Martin, and Zajac, Remi (1989). &amp;quot;Multiple inheritance in RETIF.&amp;quot; ATR Technical report TR-I-0114, ATR, Kyoto. Emele, Martin, and Zajac, Remi (1990a). &amp;quot;A fixed-point semantics for feature type In 2nd Workshop on Conditional and Typed Rewriting Systems Québec. Emele, Martin, and Zajac, Remi (1990b). &amp;quot;Typed unification grammars.&amp;quot; In Proceedings, 13th International Conference on Linguistics Helsinki. Emele, Martin; Heid, Ulrich; Momma, Stefan; and Zajac, Read (1990). &amp;quot;Organizing linguistic knowledge for generation.&amp;quot; In 178 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms 13th International Conference on Linguistics Helsinki. Etherington, David W.; Forbus, Kenneth D.; Ginsberg, Matthew L.; Israel, David; and Lifschitz, Vladimir (1989). &amp;quot;Critical issues in nonmonotonic reasoning.&amp;quot; In Proceedings, 1st International Conference on Principles of Knowledge Representation and Ontario. Evans, Roger, and Gazdar, Gerald (1989). in DATR.&amp;quot; In 4th ACL Conference. U.K. Franz, Alex (1990). &amp;quot;A parser for HPSG.&amp;quot; CMU report CMU-LCL-90-3, Laboratory for Computational Linguistics, Carnegie Mellon University. Fraser, Norman M., and Hudson, Richard A. (1990). &amp;quot;Word grammar: An inheritance-based theory of language.&amp;quot; In Proceedings, Workshop on Inheritance in Language Processing. for Language Technology and Al, Tilburg University, The Netherlands. M. A. K. (1985). to Grammar. Edward Arnold. Gerard (1976). d&apos;equations les langages d&apos;ordre 1, 2, . , w. dissertation, Universite de Paris VII. Johnson, Mark (1987). &amp;quot;Grammatical relations in attribute-value grammars.&amp;quot; In Proceedings, West Coast Conference on Formal 6. Stanford, CA. Kaplan, Ronald M.; Netter, Klaus; Wedekind, Jürgen; and Zaenen, Annie (1989). &amp;quot;Translation by structural In 4th ACL Conference. U.K. Kay, Martin (1984). &amp;quot;Functional unification grammar: A formalism for machine In 10th International Conference on Computational (COLING-84). CA. Klop, Jan Willem (1990). &amp;quot;Term rewriting To appear in of Logic Computer Science, 1, edited by</note>
<author confidence="0.77384">S Abramsky</author>
<author confidence="0.77384">D Gabbay</author>
<author confidence="0.77384">T Maibaum</author>
<affiliation confidence="0.975256">Oxford University Press.</affiliation>
<address confidence="0.933564">MacGregor, Robert M. (1988). &amp;quot;A deductive</address>
<note confidence="0.764473533333333">matcher.&amp;quot; In 7th National Conference on Artificial Intelligence 88). Paul, MN, 403-408. MacGregor, Robert M. (1990). &amp;quot;LOOM user manual.&amp;quot; USC/ISI Technical Report, Marina del Rey, CA. Mann, William C., and Matthiessen, Christian I. M. I. (1985). &amp;quot;Demonstration of the Nigel text generation computer In Perspectives on Volume 1, by James D. Benson and William S. Greaves. Norwood, NJ: Ablex. Nirenburg, Sergei, Carbonele, Jaime, Tomita, Masaru, and Goodman, Kenneth (1992).</note>
<degree confidence="0.6094965">Machine Translation. A Knowledge-Based Mateo, CA: Morgan Kaufmann. Pollard, Carl J. (In press). &amp;quot;Sorts in unification-based grammar and what they In in Natural Language</degree>
<author confidence="0.735776">by M Pinkal</author>
<note confidence="0.823216">B. Gregor. Cambridge, MA: The MIT Press. Pollard, Carl J., and Moshier, Drew (1989).</note>
<title confidence="0.860474">amp;quot;Unifying partial descriptions of sets.&amp;quot; In Language and Cognition,</title>
<author confidence="0.992194">by P Hanson</author>
<author confidence="0.992194">Vancouver Studies in</author>
<affiliation confidence="0.982176">Cognitive Science 1. Vancouver: University of British Columbia Press.</affiliation>
<note confidence="0.786765333333333">Pollard, Carl J., and Sag, Ivan A. (1987). Information-Based Syntax and Semantics. 1: Syntax. Lecture Notes 13,</note>
<affiliation confidence="0.922734">Chicago University Press.</affiliation>
<address confidence="0.820791">Pollard, Carl J., and Sag, Ivan A.</address>
<note confidence="0.636838">research). Syntax and Semantics. Volume 2: Agreement, Binding and Control. Reape, Mike (1990). &amp;quot;Parsing semi-free</note>
<abstract confidence="0.947765666666667">word order and bounded discontinuous constituency and &amp;quot;shake &apos;n&apos; bake&amp;quot; machine translation (or &apos;generation as</abstract>
<title confidence="0.604498">Workshop on Constraint Based Formalisms for Natural</title>
<keyword confidence="0.339617">Generation, Teinach,</keyword>
<address confidence="0.7854125">Germany. Rounds, Williams C., and Kasper, Robert T.</address>
<title confidence="0.7265075">(1986). &amp;quot;A Complete Logical Calculus for Record Structures Representing Linguistic</title>
<author confidence="0.608309">Symposium on Logic in</author>
<affiliation confidence="0.978734">Computer Science.</affiliation>
<address confidence="0.860797">Saint-Dizier, Patrick (1991). &amp;quot;Processing</address>
<note confidence="0.67334325">language with logical types and actives In 5th Conference the European Chapter of the ACL. Germany.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Proceedings</author>
</authors>
<date>1989</date>
<booktitle>ACM SIGMOD International Conference on Management of Data,</booktitle>
<location>Portland, Oregon.</location>
<marker>Proceedings, 1989</marker>
<rawString>Proceedings, 1989 ACM SIGMOD International Conference on Management of Data, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
</authors>
<title>Non-monotonic inheritance and unification.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Workshop on Inheritance in Natural Language Processing,</booktitle>
<institution>Institute for Language Technology and Al, Tilburg University, The Netherlands.</institution>
<contexts>
<context position="6300" citStr="Bouma 1990" startWordPosition="897" endWordPosition="898">ome other procedure. Similarly, the meaning of a PROLOG predicate should be transparent because there is no global variable, but a predicate definition might be modified during execution by imperative predicates such as assert and retract, thus destroying the referential transparency of pure PROLOG. Clearly, most of the object-oriented languages lack referential transparency in several ways, using for example procedural attachments for object methods. Another example is the use of nonmonotonic inheritance, which is advocated in computational linguistics by, for example, Evans and Gazdar 1989; Bouma 1990; De Smedt and de Graaf 1990; and Fraser and Hudson 1990. Nonmonotonic inheritance is seen as a practical device designed to deal with exceptions, but such a feature goes against generality and referential transparency. Furthermore, as expressed by Etherington et al. 1989, a still unresolved issue in nonmonotonic reasoning is the issue of ... scaling formal non-monotonic theories up to real problems (merely a formality?). Most existant theories are intractable—some don&apos;t have even a proof theory—and it is often difficult to tell how large bodies of information will (or even should) interact. G</context>
</contexts>
<marker>Bouma, 1990</marker>
<rawString>Bouma, Gosse (1990). &amp;quot;Non-monotonic inheritance and unification.&amp;quot; In Proceedings, Workshop on Inheritance in Natural Language Processing, Institute for Language Technology and Al, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bourbeau</author>
<author>D Carcagno</author>
<author>E Goldberg</author>
<author>R Kittredge</author>
<author>A Polguere</author>
</authors>
<title>Bilingual generation of weather forecasts in an operation environment.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90).</booktitle>
<location>Helsinki.</location>
<contexts>
<context position="8256" citStr="Bourbeau et al. 1990" startWordPosition="1185" endWordPosition="1188"> is to integrate those two components more closely, where the assertional component is some kind of rule-based system, as in Loom (Yen, Neches, and MacGregor 1988). Typed feature structures are very similar to structured objects of object-oriented languages and to conceptual structures of knowledge representation languages. Thus, typed feature structures have the potential to act as a lingua franca for both computational linguistics and artificial intelligence, and this should ease the communication between those two worlds. Since conceptual structures are used for example in text generation (Bourbeau et al. 1990) or knowledge-based machine translation (Nirenburg et al. 1992), typed feature structures provide an attractive alternative to current procedural implementations. In Section 2, we present a language that combines the notions of partial information and inheritance in a fully declarative framework. It is based on feature structures augmented with the notion of types, which are organized into an inheritance network. Using types, it is possible to define structured domains of feature structures and to classify feature structures. Logical conditions are attached to types, akin to method attachment,</context>
</contexts>
<marker>Bourbeau, Carcagno, Goldberg, Kittredge, Polguere, 1990</marker>
<rawString>Bourbeau, L.; Carcagno, D.; Goldberg, E.; Kittredge, R.; and Polguere, A. (1990). &amp;quot;Bilingual generation of weather forecasts in an operation environment.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90). Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald J Brachman</author>
<author>James G Schmolze</author>
</authors>
<title>An overview of the KL-ONE knowledge representation language.&amp;quot;</title>
<date>1985</date>
<journal>Cognitive Science</journal>
<volume>9</volume>
<pages>171--216</pages>
<contexts>
<context position="7325" citStr="Brachman and Schmolze 1985" startWordPosition="1049" endWordPosition="1052">lems (merely a formality?). Most existant theories are intractable—some don&apos;t have even a proof theory—and it is often difficult to tell how large bodies of information will (or even should) interact. Given the complexity of the state of the art in nonmonotonic reasoning and the lack of 160 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms a basic commonly agreed formalization,&apos; the issue of nonmonotonicity is not addressed in the work described in this article. Knowledge representation languages are evolving toward more declarativity, as exemplified by the evolution from KL-ONE (Brachman and Schmolze 1985) to languages such as CLASSIC (Borgida et al. 1989) or Loom (MacGregor 1988, 1990). The terminological component describing the objects (the data model of object-oriented database systems) has always been more declarative than the assertional component (procedural attachment or methods), and the current trend is to integrate those two components more closely, where the assertional component is some kind of rule-based system, as in Loom (Yen, Neches, and MacGregor 1988). Typed feature structures are very similar to structured objects of object-oriented languages and to conceptual structures of </context>
</contexts>
<marker>Brachman, Schmolze, 1985</marker>
<rawString>Brachman, Ronald J., and Schmolze, James G. (1985). &amp;quot;An overview of the KL-ONE knowledge representation language.&amp;quot; Cognitive Science 9, 171-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Typed feature structures: inheritance, (in)equality and extensionality.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Workshop on Inheritance in Natural Language Processing, Institute for Language Technology</booktitle>
<institution>and Al, Tilburg University, The Netherlands.</institution>
<contexts>
<context position="13676" citStr="Carpenter 1990" startWordPosition="2066" endWordPosition="2067"> this section is based on original work by Ait-Kaci (1984, 1986) on the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and AltKaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was already a departure from Ait-Kaci&apos;s KBL language, implementing a different evaluation strategy that allowed the use of cyclic feature structures. Compared with the KBL language, the current TFS language • allows the use of cyclic feature structures, • has a simple operational semantics implementing an inheritance rule and a specialization rule, • uses a lazy evaluation scheme for the evaluation of constraints, • implem</context>
</contexts>
<marker>Carpenter, 1990</marker>
<rawString>Carpenter, Bob (1990). &amp;quot;Typed feature structures: inheritance, (in)equality and extensionality.&amp;quot; In Proceedings, Workshop on Inheritance in Natural Language Processing, Institute for Language Technology and Al, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daelema ns</author>
<author>Walter</author>
</authors>
<title>Inheritance and object-oriented natural language processing.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Workshop on Inheritance in Natural Language Processing,</booktitle>
<institution>Institute for Language Technology and Al, Tilburg University, The Netherlands.</institution>
<marker>ns, Walter, 1990</marker>
<rawString>Daelema:ns, Walter (1990). &amp;quot;Inheritance and object-oriented natural language processing.&amp;quot; In Proceedings, Workshop on Inheritance in Natural Language Processing, Institute for Language Technology and Al, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Dershowitz</author>
<author>D A Plaisted</author>
</authors>
<title>Equational programming.&amp;quot;</title>
<date>1988</date>
<booktitle>In Machine Intelligence 11,</booktitle>
<publisher>Press.</publisher>
<location>Oxford: Clarendon</location>
<note>edited by</note>
<marker>Dershowitz, Plaisted, 1988</marker>
<rawString>Dershowitz, N., and Plaisted, D. A. (1988). &amp;quot;Equational programming.&amp;quot; In Machine Intelligence 11, edited by Hayes, Michie, and Richards. Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koenraad De Smedt</author>
<author>de Graaf</author>
</authors>
<title>Structured inheritance in frame-based representation of linguistic categories.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Workshop on Inheritance in Natural Language Processing. Institute for Language Technology</booktitle>
<institution>Tilburg University, The Netherlands.</institution>
<marker>De Smedt, de Graaf, 1990</marker>
<rawString>De Smedt, Koenraad, and de Graaf, Josje (1990). &amp;quot;Structured inheritance in frame-based representation of linguistic categories.&amp;quot; In Proceedings, Workshop on Inheritance in Natural Language Processing. Institute for Language Technology and Al, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Deville</author>
</authors>
<title>Logic programming. Systematic Program Development.</title>
<date>1990</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<marker>Deville, 1990</marker>
<rawString>Deville, Yves (1990). Logic programming. Systematic Program Development. Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
<author>Pierre Isabelle</author>
</authors>
<title>Reversible logic grammars for machine translation.&amp;quot; In</title>
<date>1988</date>
<booktitle>Proceedings, 2nd International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="35026" citStr="Dymetman and Isabelle (1988)" startWordPosition="5659" endWordPosition="5662">d &amp;quot;output.&amp;quot; For example, the same kind of data structure could be used to encode both the string and the structural description, and, as for the HPSG sign (Pollard and Sag 1987), they could be embedded into a single data structure that represents the relation between the string and the associated linguistic structure. • Specific mapping properties, based on constituency, linear precedence, or functional composition, are not part of the formalism itself, but can be encoded explicitly in the formalism. An approach that uses a unique deductive mechanism for parsing and generation is described in Dymetman and Isabelle (1988). Within this approach, a lazy evaluation mechanism based on the specification of input/output arguments is implemented (in PROLOG), and the evaluation is completely data-driven: the same program parses or generates, depending only on the form of the input structure. A constraint-based grammar does not need a context-free mechanism to build up constituent structures for parsing or generation: Dymetman, Isabelle, and Perrault (1990) describe a class of reversible grammars (&amp;quot;Lexical Grammars&amp;quot;) based on a few composition rules that are very reminiscent of categorial grammars. Other kinds of appro</context>
</contexts>
<marker>Dymetman, Isabelle, 1988</marker>
<rawString>Dymetman, Marc, and Isabelle, Pierre (1988). &amp;quot;Reversible logic grammars for machine translation.&amp;quot; In Proceedings, 2nd International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
<author>Pierre Isabelle</author>
<author>Francois Perrault</author>
</authors>
<title>A symmetrical approach to parsing and generation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90).</booktitle>
<location>Helsinki.</location>
<marker>Dymetman, Isabelle, Perrault, 1990</marker>
<rawString>Dymetman, Marc; Isabelle, Pierre; and Perrault, Francois (1990). &amp;quot;A symmetrical approach to parsing and generation.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90). Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
</authors>
<title>A typed feature structure unification-based approach to generation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, WGNLC of the IECE.</booktitle>
<institution>Oiso University,</institution>
<contexts>
<context position="3062" citStr="Emele 1988" startWordPosition="410" endWordPosition="411">rammar (Halliday 1985), Word Grammar (Fraser and Hudson 1990), or HPSG (Pollard * IMS-CL/Ifl-AIS, University of Stuttgart, Azenbergstrage 12, D-W-7000 Stuttgart 1. E-mail: zajac@informatik.uni-stuttgart.de. t Research reported in this paper is partly supported by the German Ministry of Research and Technology (BMFT, Bundesminister fur Forschung und Technologie), under grant No. 08 B3116 3. The views and conclusions contained herein are those of the author and should not be interpreted as representing official policies. 1 In particular, it allows a direct implementation of HPSG-style grammars (Emele 1988): HPSG is so far the only linguistic theory based on both inheritance and feature structures. 0 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 and Sag 1987) make use of inheritance to describe linguistic structures at the lexical, morphological, syntactic, or semantic (conceptual) levels. These theories are usually directly implemented in object-oriented programming languages (e.g., Loom in the case of the PENMAN system [Mann and Matthiessen 1985]), but there is a growing number of linguistic formalisms used for specific purposes, e.g., DATR (Evans</context>
<context position="37393" citStr="Emele (1988)" startWordPosition="6029" endWordPosition="6030">sed Grammar Formalisms SYN I LOC I SUBCAT EDTRS headed-structure{ I I =[DTRS{HEAD-DTR I SYN I LOC I SUBCAT append(cij, 0) COMP-DTRS PHRASE &lt; syn: SUBCAT :- APPEND 1: ID SUBCAT-FP dtrs: -FP. 2: Ili [ snbcat: [D] 3: IJ ] head-dtr: [syn: [subcat: 111]J} comp-dtrs: In Figure 5 The HSPG subcategorization principle and its TFS encoding. of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based on the notion of typed feature structures (Pollard 1990), and is thus a good candidate to illustrate the possibilities of the TFS formalism. The following presentation is based on Emele (1988) and Emele and Zajac (1990b). The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure, syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot; the relation between a string and a linguistic structure is encoded as a single feature structure representing the &amp;quot;sign.&amp;quot; HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type PHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, and SEM-FP. For example, the HPSG sub</context>
</contexts>
<marker>Emele, 1988</marker>
<rawString>Emele, Martin (1988). &amp;quot;A typed feature structure unification-based approach to generation.&amp;quot; In Proceedings, WGNLC of the IECE. Oiso University, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
</authors>
<title>Unification with lazy non-redundant copying.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 29th Annual Meeting of the ACL.</booktitle>
<location>Berkeley, CA.</location>
<contexts>
<context position="20882" citStr="Emele 1991" startWordPosition="3326" endWordPosition="3327"> a typed unification algorithm. A feature structure is represented as a graph where each node has a type, an equivalence class used to represent equational constraints (&amp;quot;co-references&amp;quot;), and a set of outgoing arcs. The unification algorithm uses the union/find procedure on an inverted set representation of the equivalence classes adapted by Aft-Kaci (1984) after Huet (1976). The actual algorithm used in the system is optimized using several different techniques to minimize copying and to behave as efficiently as a pattern-matcher in cases when one of the feature structures subsumes the other (Emele 1991). 2.3 Inheritance Network of Feature Structures The template mechanism (as, for example, in PATR-II [Shieber 1986]) already provides a simple inheritance mechanism used to organize lexical descriptions. In comparison, networks of typed feature structures are more expressive and provide a more general and more powerful inheritance mechanism, which allows the use of recursive type definitions, whereas recursivity is forbidden in templates since they are expanded statically using a macro-expansion mechanism. Furthermore, typing provides a notion of well-formedness that is used to implement a type</context>
<context position="54823" citStr="Emele 1991" startWordPosition="8742" endWordPosition="8743">he system. Acknowledgments The design and the implementation of the TFS system have been carried out in cooperation with Martin Emele. I would like to thank Stefan Momma and Ulrich Heid; their numerous comments and advices helped to make this article more readable. I would also like to thank anonymous referees for their many detailed and helpful comments. All remaining errors are of course the sole responsibility of the author. 14 For example, the use in the current implementation of several techniques adapted from PROLOG implementations such as structure sharing, chronological dereferencing (Emele 1991) and last call optimization, have improved efficiency by several orders of magnitude over previous &amp;quot;naive&amp;quot; implementations. 177 Computational Linguistics Volume 18, Number 2 References Ait-Kaci, Hassan (1984). &amp;quot;A lattice theoretic approach to computation based on a calculus of partially ordered type structures.&amp;quot; Doctoral dissertation, University of Pennsylvania, Philadelphia, PA. Ait-Kaci, Hassan (1986). &amp;quot;An algebraic semantics approach to the effective resolution of type equations.&amp;quot; Theoretical Computer Science, 45, 293-351. Ait-Kaci, Hassan, and Meyer, Richard (1990). Wild_LIFE, a user manua</context>
</contexts>
<marker>Emele, 1991</marker>
<rawString>Emele, Martin (1991). &amp;quot;Unification with lazy non-redundant copying.&amp;quot; In Proceedings, 29th Annual Meeting of the ACL. Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
<author>Remi Zajac</author>
</authors>
<title>RETIF: A rewriting system for typed feature structures.&amp;quot;</title>
<date>1989</date>
<tech>ATR Technical report TR-I-0071, ATR, Kyoto.</tech>
<contexts>
<context position="13740" citStr="Emele and Zajac (1989" startWordPosition="2075" endWordPosition="2078">, 1986) on the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and AltKaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was already a departure from Ait-Kaci&apos;s KBL language, implementing a different evaluation strategy that allowed the use of cyclic feature structures. Compared with the KBL language, the current TFS language • allows the use of cyclic feature structures, • has a simple operational semantics implementing an inheritance rule and a specialization rule, • uses a lazy evaluation scheme for the evaluation of constraints, • implements static coherence checks, • has a simple syntax distinguishi</context>
</contexts>
<marker>Emele, Zajac, 1989</marker>
<rawString>Emele, Martin, and Zajac, Remi (1989). &amp;quot;RETIF: A rewriting system for typed feature structures.&amp;quot; ATR Technical report TR-I-0071, ATR, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
<author>Remi Zajac</author>
</authors>
<title>Multiple inheritance in RETIF.&amp;quot;</title>
<date>1989</date>
<tech>ATR Technical report TR-I-0114, ATR, Kyoto.</tech>
<contexts>
<context position="13740" citStr="Emele and Zajac (1989" startWordPosition="2075" endWordPosition="2078">, 1986) on the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and AltKaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was already a departure from Ait-Kaci&apos;s KBL language, implementing a different evaluation strategy that allowed the use of cyclic feature structures. Compared with the KBL language, the current TFS language • allows the use of cyclic feature structures, • has a simple operational semantics implementing an inheritance rule and a specialization rule, • uses a lazy evaluation scheme for the evaluation of constraints, • implements static coherence checks, • has a simple syntax distinguishi</context>
</contexts>
<marker>Emele, Zajac, 1989</marker>
<rawString>Emele, Martin, and Zajac, Remi (1989). &amp;quot;Multiple inheritance in RETIF.&amp;quot; ATR Technical report TR-I-0114, ATR, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
<author>Remi Zajac</author>
</authors>
<title>A fixed-point semantics for feature type systems.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 2nd Workshop on Conditional and Typed Rewriting Systems (CTRS&apos;90).</booktitle>
<location>Montréal, Québec.</location>
<contexts>
<context position="10811" citStr="Emele and Zajac 1990" startWordPosition="1585" endWordPosition="1588">iverse, this means that the set described by fi is a superset of the set described by f2. Note that there can be feature structures that cannot consistently describe the 2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature structures are derived, plays this role in formal accounts of feature structures. 3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been implemented at the University of Stuttgart by Martin Emele and the author and has been used to test several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al. 1990; Zajac 1990a; Bateman and Momma 19911. 161 Computational Linguistics Volume 18, Number 2 same objects. For example, a feature structure describing verb phrases and a feature structure describing noun phrases are not consistent: the intersection of the sets they denote is usually empty. As different sets of attribute-value pairs make sense for different kinds of objects, we also divide our feature structures into different types. These types are ordered by a subtype relation: a type t2 is a subtype of another type t1 if t2 provides at least as much information as t1. For ex</context>
<context position="15082" citStr="Emele and Zajac (1990" startWordPosition="2280" endWordPosition="2283">ajac Inheritance and Constraint-Based Grammar Formalisms 2.1 Types In the following presentation, we adopt an algebraic approach based on lattice theory (Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for example, a proof-theoretical approach using an adaptation of a feature logic (Rounds and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of these different models, as this is done for the LIFE language in Ait-Kaci and Podelski (1991). The presentation is nevertheless rather informal, and a more technical account can be found in Emele and Zajac (1990a) and Zajac (1990b). The universe of feature structures is structured in an inheritance network that defines a partial ordering on kinds of available information. The backbone of the network is defined by a finite set of type symbols T together with a partial ordering &lt; on T: the partially ordered set (poset) (T ,&lt;). The ordering &lt; defines the subtype relation: for A, B E T we read A &lt; B as &amp;quot;A is a subtype of B.&amp;quot; We call the smallest types of T the minimal types. To have a well-behaved type hierarchy, we require that (T,&lt;) be such that: • T contains the symbols T and I, where T is the greates</context>
<context position="30306" citStr="Emele and Zajac (1990" startWordPosition="4919" endWordPosition="4922">g more constraints to the query using the rewrite rules (nondeterministically) to get closer to the solution step by step. The rewriting process stops when conditions 1 and 2 described above hold. A rewrite step for a structure t is defined as follows: if u is a substructure of t at path p and u is of type A, and there exists a rewrite rule A[a] 13[19] such that • A[a] A U _L, and • A[a] A U &lt; A[a] then the right-hand side B[b] is unified with the substructure u at path p, giving a new structure t&apos; that is more specific than t (Figure 4). 8 See Nit-Kaci (1984), Pollard and Moshier (1990), and Emele and Zajac (1990a) for fixed-point characterizations of the denotation of typed feature structures. 9 Narrowing uses unification instead of pattern-matching for checking the applicability of the 1.h.s. of a rule. Narrowing is used in the logic programming paradigm (e.g., as an alternative to resolution for implementing PROLOG interpreters). Pattern-matching is used in the functional programming paradigm. NIL CONS[first: T rest: LIST 1: NIL APPENDO 2: 0 LIST 3: 0 APPEND1 1: CONS[first:E - rest:El 2: g LIST 3: CONS[first:E rest:0 LIST LIST [1: LIST] APPEND 2: LIST 3: LIST [1: LIST APPEND 2: LIST 3: LIST [1: 01 </context>
<context position="37419" citStr="Emele and Zajac (1990" startWordPosition="6032" endWordPosition="6035">lisms SYN I LOC I SUBCAT EDTRS headed-structure{ I I =[DTRS{HEAD-DTR I SYN I LOC I SUBCAT append(cij, 0) COMP-DTRS PHRASE &lt; syn: SUBCAT :- APPEND 1: ID SUBCAT-FP dtrs: -FP. 2: Ili [ snbcat: [D] 3: IJ ] head-dtr: [syn: [subcat: 111]J} comp-dtrs: In Figure 5 The HSPG subcategorization principle and its TFS encoding. of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based on the notion of typed feature structures (Pollard 1990), and is thus a good candidate to illustrate the possibilities of the TFS formalism. The following presentation is based on Emele (1988) and Emele and Zajac (1990b). The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure, syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot; the relation between a string and a linguistic structure is encoded as a single feature structure representing the &amp;quot;sign.&amp;quot; HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type PHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, and SEM-FP. For example, the HPSG subcategorization principle i</context>
</contexts>
<marker>Emele, Zajac, 1990</marker>
<rawString>Emele, Martin, and Zajac, Remi (1990a). &amp;quot;A fixed-point semantics for feature type systems.&amp;quot; In Proceedings, 2nd Workshop on Conditional and Typed Rewriting Systems (CTRS&apos;90). Montréal, Québec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
<author>Remi Zajac</author>
</authors>
<title>Typed unification grammars.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90).</booktitle>
<location>Helsinki.</location>
<contexts>
<context position="10811" citStr="Emele and Zajac 1990" startWordPosition="1585" endWordPosition="1588">iverse, this means that the set described by fi is a superset of the set described by f2. Note that there can be feature structures that cannot consistently describe the 2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature structures are derived, plays this role in formal accounts of feature structures. 3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been implemented at the University of Stuttgart by Martin Emele and the author and has been used to test several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al. 1990; Zajac 1990a; Bateman and Momma 19911. 161 Computational Linguistics Volume 18, Number 2 same objects. For example, a feature structure describing verb phrases and a feature structure describing noun phrases are not consistent: the intersection of the sets they denote is usually empty. As different sets of attribute-value pairs make sense for different kinds of objects, we also divide our feature structures into different types. These types are ordered by a subtype relation: a type t2 is a subtype of another type t1 if t2 provides at least as much information as t1. For ex</context>
<context position="15082" citStr="Emele and Zajac (1990" startWordPosition="2280" endWordPosition="2283">ajac Inheritance and Constraint-Based Grammar Formalisms 2.1 Types In the following presentation, we adopt an algebraic approach based on lattice theory (Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for example, a proof-theoretical approach using an adaptation of a feature logic (Rounds and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of these different models, as this is done for the LIFE language in Ait-Kaci and Podelski (1991). The presentation is nevertheless rather informal, and a more technical account can be found in Emele and Zajac (1990a) and Zajac (1990b). The universe of feature structures is structured in an inheritance network that defines a partial ordering on kinds of available information. The backbone of the network is defined by a finite set of type symbols T together with a partial ordering &lt; on T: the partially ordered set (poset) (T ,&lt;). The ordering &lt; defines the subtype relation: for A, B E T we read A &lt; B as &amp;quot;A is a subtype of B.&amp;quot; We call the smallest types of T the minimal types. To have a well-behaved type hierarchy, we require that (T,&lt;) be such that: • T contains the symbols T and I, where T is the greates</context>
<context position="30306" citStr="Emele and Zajac (1990" startWordPosition="4919" endWordPosition="4922">g more constraints to the query using the rewrite rules (nondeterministically) to get closer to the solution step by step. The rewriting process stops when conditions 1 and 2 described above hold. A rewrite step for a structure t is defined as follows: if u is a substructure of t at path p and u is of type A, and there exists a rewrite rule A[a] 13[19] such that • A[a] A U _L, and • A[a] A U &lt; A[a] then the right-hand side B[b] is unified with the substructure u at path p, giving a new structure t&apos; that is more specific than t (Figure 4). 8 See Nit-Kaci (1984), Pollard and Moshier (1990), and Emele and Zajac (1990a) for fixed-point characterizations of the denotation of typed feature structures. 9 Narrowing uses unification instead of pattern-matching for checking the applicability of the 1.h.s. of a rule. Narrowing is used in the logic programming paradigm (e.g., as an alternative to resolution for implementing PROLOG interpreters). Pattern-matching is used in the functional programming paradigm. NIL CONS[first: T rest: LIST 1: NIL APPENDO 2: 0 LIST 3: 0 APPEND1 1: CONS[first:E - rest:El 2: g LIST 3: CONS[first:E rest:0 LIST LIST [1: LIST] APPEND 2: LIST 3: LIST [1: LIST APPEND 2: LIST 3: LIST [1: 01 </context>
<context position="37419" citStr="Emele and Zajac (1990" startWordPosition="6032" endWordPosition="6035">lisms SYN I LOC I SUBCAT EDTRS headed-structure{ I I =[DTRS{HEAD-DTR I SYN I LOC I SUBCAT append(cij, 0) COMP-DTRS PHRASE &lt; syn: SUBCAT :- APPEND 1: ID SUBCAT-FP dtrs: -FP. 2: Ili [ snbcat: [D] 3: IJ ] head-dtr: [syn: [subcat: 111]J} comp-dtrs: In Figure 5 The HSPG subcategorization principle and its TFS encoding. of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based on the notion of typed feature structures (Pollard 1990), and is thus a good candidate to illustrate the possibilities of the TFS formalism. The following presentation is based on Emele (1988) and Emele and Zajac (1990b). The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure, syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot; the relation between a string and a linguistic structure is encoded as a single feature structure representing the &amp;quot;sign.&amp;quot; HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type PHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, and SEM-FP. For example, the HPSG subcategorization principle i</context>
</contexts>
<marker>Emele, Zajac, 1990</marker>
<rawString>Emele, Martin, and Zajac, Remi (1990b). &amp;quot;Typed unification grammars.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90). Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emele</author>
<author>Ulrich Heid</author>
<author>Stefan Momma</author>
<author>Read Zajac</author>
</authors>
<title>Organizing linguistic knowledge for multilingual generation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Computational Linguistics (COLING&apos;90).</booktitle>
<location>Helsinki.</location>
<contexts>
<context position="10831" citStr="Emele et al. 1990" startWordPosition="1589" endWordPosition="1592"> the set described by fi is a superset of the set described by f2. Note that there can be feature structures that cannot consistently describe the 2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature structures are derived, plays this role in formal accounts of feature structures. 3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been implemented at the University of Stuttgart by Martin Emele and the author and has been used to test several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al. 1990; Zajac 1990a; Bateman and Momma 19911. 161 Computational Linguistics Volume 18, Number 2 same objects. For example, a feature structure describing verb phrases and a feature structure describing noun phrases are not consistent: the intersection of the sets they denote is usually empty. As different sets of attribute-value pairs make sense for different kinds of objects, we also divide our feature structures into different types. These types are ordered by a subtype relation: a type t2 is a subtype of another type t1 if t2 provides at least as much information as t1. For example, assuming that</context>
</contexts>
<marker>Emele, Heid, Momma, Zajac, 1990</marker>
<rawString>Emele, Martin; Heid, Ulrich; Momma, Stefan; and Zajac, Read (1990). &amp;quot;Organizing linguistic knowledge for multilingual generation.&amp;quot; In Proceedings, Computational Linguistics (COLING&apos;90). Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David W Etherington</author>
<author>Kenneth D Forbus</author>
<author>Matthew L Ginsberg</author>
<author>David Israel</author>
<author>Vladimir Lifschitz</author>
</authors>
<title>Critical issues in nonmonotonic reasoning.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 1st International Conference on Principles of Knowledge Representation and Reasoning.</booktitle>
<location>Toronto, Ontario.</location>
<contexts>
<context position="6572" citStr="Etherington et al. 1989" startWordPosition="937" endWordPosition="940"> referential transparency of pure PROLOG. Clearly, most of the object-oriented languages lack referential transparency in several ways, using for example procedural attachments for object methods. Another example is the use of nonmonotonic inheritance, which is advocated in computational linguistics by, for example, Evans and Gazdar 1989; Bouma 1990; De Smedt and de Graaf 1990; and Fraser and Hudson 1990. Nonmonotonic inheritance is seen as a practical device designed to deal with exceptions, but such a feature goes against generality and referential transparency. Furthermore, as expressed by Etherington et al. 1989, a still unresolved issue in nonmonotonic reasoning is the issue of ... scaling formal non-monotonic theories up to real problems (merely a formality?). Most existant theories are intractable—some don&apos;t have even a proof theory—and it is often difficult to tell how large bodies of information will (or even should) interact. Given the complexity of the state of the art in nonmonotonic reasoning and the lack of 160 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms a basic commonly agreed formalization,&apos; the issue of nonmonotonicity is not addressed in the work described in this art</context>
</contexts>
<marker>Etherington, Forbus, Ginsberg, Israel, Lifschitz, 1989</marker>
<rawString>Etherington, David W.; Forbus, Kenneth D.; Ginsberg, Matthew L.; Israel, David; and Lifschitz, Vladimir (1989). &amp;quot;Critical issues in nonmonotonic reasoning.&amp;quot; In Proceedings, 1st International Conference on Principles of Knowledge Representation and Reasoning. Toronto, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>Inference in DATR.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 4th European ACL Conference.</booktitle>
<location>Manchester, U.K.</location>
<contexts>
<context position="3679" citStr="Evans and Gazdar 1989" startWordPosition="497" endWordPosition="500">1988): HPSG is so far the only linguistic theory based on both inheritance and feature structures. 0 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 and Sag 1987) make use of inheritance to describe linguistic structures at the lexical, morphological, syntactic, or semantic (conceptual) levels. These theories are usually directly implemented in object-oriented programming languages (e.g., Loom in the case of the PENMAN system [Mann and Matthiessen 1985]), but there is a growing number of linguistic formalisms used for specific purposes, e.g., DATR (Evans and Gazdar 1989) for the lexicon. On the other hand, current linguistic theories such as LFG, UCG, HPSG, and some formalisms for linguistic description such as FUG or PATR-II are based on the notion of partial information: linguistic structures are described using feature structures that give partial information about the object being modeled, a linguistic structure being described by a set of feature structures that mutually constrain the description. Feature structures are partially ordered according to a subsumption ordering interpreted as an ordering on the amount of conveyed information; the combination </context>
<context position="6288" citStr="Evans and Gazdar 1989" startWordPosition="893" endWordPosition="896">ables that are set by some other procedure. Similarly, the meaning of a PROLOG predicate should be transparent because there is no global variable, but a predicate definition might be modified during execution by imperative predicates such as assert and retract, thus destroying the referential transparency of pure PROLOG. Clearly, most of the object-oriented languages lack referential transparency in several ways, using for example procedural attachments for object methods. Another example is the use of nonmonotonic inheritance, which is advocated in computational linguistics by, for example, Evans and Gazdar 1989; Bouma 1990; De Smedt and de Graaf 1990; and Fraser and Hudson 1990. Nonmonotonic inheritance is seen as a practical device designed to deal with exceptions, but such a feature goes against generality and referential transparency. Furthermore, as expressed by Etherington et al. 1989, a still unresolved issue in nonmonotonic reasoning is the issue of ... scaling formal non-monotonic theories up to real problems (merely a formality?). Most existant theories are intractable—some don&apos;t have even a proof theory—and it is often difficult to tell how large bodies of information will (or even should)</context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Evans, Roger, and Gazdar, Gerald (1989). &amp;quot;Inference in DATR.&amp;quot; In Proceedings, 4th European ACL Conference. Manchester, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Franz</author>
</authors>
<title>A parser for HPSG.&amp;quot;</title>
<date>1990</date>
<tech>CMU report CMU-LCL-90-3,</tech>
<institution>Laboratory for Computational Linguistics, Carnegie Mellon University.</institution>
<contexts>
<context position="13689" citStr="Franz 1990" startWordPosition="2068" endWordPosition="2069"> based on original work by Ait-Kaci (1984, 1986) on the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and AltKaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was already a departure from Ait-Kaci&apos;s KBL language, implementing a different evaluation strategy that allowed the use of cyclic feature structures. Compared with the KBL language, the current TFS language • allows the use of cyclic feature structures, • has a simple operational semantics implementing an inheritance rule and a specialization rule, • uses a lazy evaluation scheme for the evaluation of constraints, • implements static c</context>
</contexts>
<marker>Franz, 1990</marker>
<rawString>Franz, Alex (1990). &amp;quot;A parser for HPSG.&amp;quot; CMU report CMU-LCL-90-3, Laboratory for Computational Linguistics, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman M Fraser</author>
<author>Richard A Hudson</author>
</authors>
<title>Word grammar: An inheritance-based theory of language.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Workshop on Inheritance in Natural Language Processing. Institute for Language Technology</booktitle>
<institution>and Al, Tilburg University, The Netherlands.</institution>
<contexts>
<context position="2512" citStr="Fraser and Hudson 1990" startWordPosition="331" endWordPosition="334">ure structures. It is flexible and has enough expressive power to support various kinds of linguistic theories, not necessarily based on constituency&apos;. The use of an object-oriented methodology for natural language processing is very attractive, and the use of inheritance offers a number of advantages such as abstraction and generalization, information sharing and default reasoning, and modularity and reusability (Daelemans 1990). Inheritance-based descriptions are already used in computational linguistics: linguistic theories such as Systemic Functional Grammar (Halliday 1985), Word Grammar (Fraser and Hudson 1990), or HPSG (Pollard * IMS-CL/Ifl-AIS, University of Stuttgart, Azenbergstrage 12, D-W-7000 Stuttgart 1. E-mail: zajac@informatik.uni-stuttgart.de. t Research reported in this paper is partly supported by the German Ministry of Research and Technology (BMFT, Bundesminister fur Forschung und Technologie), under grant No. 08 B3116 3. The views and conclusions contained herein are those of the author and should not be interpreted as representing official policies. 1 In particular, it allows a direct implementation of HPSG-style grammars (Emele 1988): HPSG is so far the only linguistic theory based </context>
<context position="6356" citStr="Fraser and Hudson 1990" startWordPosition="906" endWordPosition="909">of a PROLOG predicate should be transparent because there is no global variable, but a predicate definition might be modified during execution by imperative predicates such as assert and retract, thus destroying the referential transparency of pure PROLOG. Clearly, most of the object-oriented languages lack referential transparency in several ways, using for example procedural attachments for object methods. Another example is the use of nonmonotonic inheritance, which is advocated in computational linguistics by, for example, Evans and Gazdar 1989; Bouma 1990; De Smedt and de Graaf 1990; and Fraser and Hudson 1990. Nonmonotonic inheritance is seen as a practical device designed to deal with exceptions, but such a feature goes against generality and referential transparency. Furthermore, as expressed by Etherington et al. 1989, a still unresolved issue in nonmonotonic reasoning is the issue of ... scaling formal non-monotonic theories up to real problems (merely a formality?). Most existant theories are intractable—some don&apos;t have even a proof theory—and it is often difficult to tell how large bodies of information will (or even should) interact. Given the complexity of the state of the art in nonmonoto</context>
</contexts>
<marker>Fraser, Hudson, 1990</marker>
<rawString>Fraser, Norman M., and Hudson, Richard A. (1990). &amp;quot;Word grammar: An inheritance-based theory of language.&amp;quot; In Proceedings, Workshop on Inheritance in Natural Language Processing. Institute for Language Technology and Al, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>Introduction to Functional Grammar.</title>
<date>1985</date>
<location>London: Edward Arnold.</location>
<contexts>
<context position="2473" citStr="Halliday 1985" startWordPosition="327" endWordPosition="328">d on unification of typed feature structures. It is flexible and has enough expressive power to support various kinds of linguistic theories, not necessarily based on constituency&apos;. The use of an object-oriented methodology for natural language processing is very attractive, and the use of inheritance offers a number of advantages such as abstraction and generalization, information sharing and default reasoning, and modularity and reusability (Daelemans 1990). Inheritance-based descriptions are already used in computational linguistics: linguistic theories such as Systemic Functional Grammar (Halliday 1985), Word Grammar (Fraser and Hudson 1990), or HPSG (Pollard * IMS-CL/Ifl-AIS, University of Stuttgart, Azenbergstrage 12, D-W-7000 Stuttgart 1. E-mail: zajac@informatik.uni-stuttgart.de. t Research reported in this paper is partly supported by the German Ministry of Research and Technology (BMFT, Bundesminister fur Forschung und Technologie), under grant No. 08 B3116 3. The views and conclusions contained herein are those of the author and should not be interpreted as representing official policies. 1 In particular, it allows a direct implementation of HPSG-style grammars (Emele 1988): HPSG is s</context>
</contexts>
<marker>Halliday, 1985</marker>
<rawString>Halliday, M. A. K. (1985). Introduction to Functional Grammar. London: Edward Arnold.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Huet</author>
</authors>
<title>Resolution d&apos;equations dans les langages d&apos;ordre 1, 2,</title>
<date>1976</date>
<booktitle>w. Doctoral dissertation, Universite de Paris VII.</booktitle>
<contexts>
<context position="20647" citStr="Huet (1976)" startWordPosition="3289" endWordPosition="3290"> corresponding type in t&apos;. Since we have a partial order on feature structures, the meet operation between two feature structures t and t&apos; is defined in the usual way as the greatest common lower bound of t and t&apos;. It is computed using a typed unification algorithm. A feature structure is represented as a graph where each node has a type, an equivalence class used to represent equational constraints (&amp;quot;co-references&amp;quot;), and a set of outgoing arcs. The unification algorithm uses the union/find procedure on an inverted set representation of the equivalence classes adapted by Aft-Kaci (1984) after Huet (1976). The actual algorithm used in the system is optimized using several different techniques to minimize copying and to behave as efficiently as a pattern-matcher in cases when one of the feature structures subsumes the other (Emele 1991). 2.3 Inheritance Network of Feature Structures The template mechanism (as, for example, in PATR-II [Shieber 1986]) already provides a simple inheritance mechanism used to organize lexical descriptions. In comparison, networks of typed feature structures are more expressive and provide a more general and more powerful inheritance mechanism, which allows the use o</context>
</contexts>
<marker>Huet, 1976</marker>
<rawString>Huet, Gerard (1976). Resolution d&apos;equations dans les langages d&apos;ordre 1, 2, . , w. Doctoral dissertation, Universite de Paris VII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Grammatical relations in attribute-value grammars.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, West Coast Conference on Formal Linguistics,</booktitle>
<volume>6</volume>
<location>Stanford, CA.</location>
<contexts>
<context position="48130" citStr="Johnson 1987" startWordPosition="7724" endWordPosition="7725">ng logic programs implemented in PROLOG. Furthermore, the grammar writer does not need, and actually should not, be aware of the control that follows the shape of the input rather than a fixed strategy, thanks to the lazy evaluation mechanism. 12 See also Reape (1990) for another approach to MT using feature structures and based on Whitelock (1990). 175 Computational Linguistics Volume 18, Number 2 For HPSG-style grammars, completeness and coherence as defined for LFG, and extended to the general case by Wedekind (1988), are implemented in HPSG using the &amp;quot;subcategorization feature principle&amp;quot; (Johnson 1987): for the TFS implementation of HPSG, termination is guaranteed, at least for the simplified version containing only head-complement structures, described in Section 3.2. Termination conditions for parsing are well understood in the framework of context-free grammars. For generation using feature structures, one of the problems is that the input could be &amp;quot;extended&amp;quot; during processing, i.e., arbitrary feature structures could be introduced in the semantic part of the input by unification with the semantic part of a rule. However, if the semantic part of the input is fully specified according to </context>
</contexts>
<marker>Johnson, 1987</marker>
<rawString>Johnson, Mark (1987). &amp;quot;Grammatical relations in attribute-value grammars.&amp;quot; In Proceedings, West Coast Conference on Formal Linguistics, Vol. 6. Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Klaus Netter</author>
<author>Jürgen Wedekind</author>
<author>Annie Zaenen</author>
</authors>
<title>Translation by structural correspondences.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 4th European ACL Conference.</booktitle>
<location>Manchester, U.K.</location>
<contexts>
<context position="42742" citStr="Kaplan et al. (1989)" startWordPosition="6877" endWordPosition="6880">hon: comp-dtrs: III APPEND[2: ai , 3: III 1: El ORDER-COMPEcomps: 01 phon: 172 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms Query for parsing PHRASE[phon: (Kim ate every cookie)] Query for generation PHRASE sem: {rein: EAT [ argl: ind: [restr: [name: KIM] 1] 1] arg2: spec: EVERY . r r Ind: trestr: Vein: COOKIE]]] Figure 7 Queries for parsing and generation. Figure 8 The common solution to the parsing and generation problems. The idea is rather simple: assume we are working with linguistic structures similar to LFG&apos;s functional structures for English and French as proposed in Kaplan et al. (1989). We define a translation relation as a type TAU-LEX with two features, eng for the English structure and fr for the French structure. This &amp;quot;bilingual sign&amp;quot; is defined on the lexical structure: each subtype of TAU-LEX defines a lexical correspondence between a partial English structure and a partial French structure for a given lexical equivalence. Such a lexical contrastive definition for a verb also has to pair the arguments recursively. This is expressed in the condition part of the definition (Figure 9) by a recursive condition TAU-LEX on the arguments. The translation of syntactic and sem</context>
</contexts>
<marker>Kaplan, Netter, Wedekind, Zaenen, 1989</marker>
<rawString>Kaplan, Ronald M.; Netter, Klaus; Wedekind, Jürgen; and Zaenen, Annie (1989). &amp;quot;Translation by structural correspondences.&amp;quot; In Proceedings, 4th European ACL Conference. Manchester, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Functional unification grammar: A formalism for machine translation.&amp;quot; In</title>
<date>1984</date>
<booktitle>Proceedings, 10th International Conference on Computational Linguistics (COLING-84).</booktitle>
<location>Stanford, CA.</location>
<contexts>
<context position="1624" citStr="Kay 1984" startWordPosition="213" endWordPosition="214">ample of bilingual transfer where the minimal amount of information needed for the translation is specified at different levels of generalization. 1. Introduction Ideally, a linguistic formalism combining the best of the object-oriented approach and the unification-based approach would be realized in a constraint-based architecture for an object-oriented language based on inheritance, feature structures, and unification. The Typed Feature Structure language (TFS) is an attempt to provide a synthesis of several key concepts stemming from unification-based grammar formalisms (feature structure: Kay 1984) knowledge representation languages (inheritance), and logic programming (narrowing). The formalism supports an object-oriented style based on abstraction and generalization through inheritance; it is a fully declarative formalism based on unification of typed feature structures. It is flexible and has enough expressive power to support various kinds of linguistic theories, not necessarily based on constituency&apos;. The use of an object-oriented methodology for natural language processing is very attractive, and the use of inheritance offers a number of advantages such as abstraction and generali</context>
</contexts>
<marker>Kay, 1984</marker>
<rawString>Kay, Martin (1984). &amp;quot;Functional unification grammar: A formalism for machine translation.&amp;quot; In Proceedings, 10th International Conference on Computational Linguistics (COLING-84). Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Willem Klop</author>
</authors>
<title>Term rewriting systems.&amp;quot; To appear</title>
<date>1990</date>
<booktitle>in Handbook of Logic in Computer Science,</booktitle>
<volume>1</volume>
<publisher>Oxford University Press.</publisher>
<note>edited by</note>
<marker>Klop, 1990</marker>
<rawString>Klop, Jan Willem (1990). &amp;quot;Term rewriting systems.&amp;quot; To appear in Handbook of Logic in Computer Science, Volume 1, edited by S. Abramsky, D. Gabbay and T. Maibaum. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M MacGregor</author>
</authors>
<title>A deductive pattern matcher.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 7th National Conference on Artificial Intelligence (AAAT 88). St. Paul, MN,</booktitle>
<pages>403--408</pages>
<contexts>
<context position="7400" citStr="MacGregor 1988" startWordPosition="1065" endWordPosition="1066">a proof theory—and it is often difficult to tell how large bodies of information will (or even should) interact. Given the complexity of the state of the art in nonmonotonic reasoning and the lack of 160 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms a basic commonly agreed formalization,&apos; the issue of nonmonotonicity is not addressed in the work described in this article. Knowledge representation languages are evolving toward more declarativity, as exemplified by the evolution from KL-ONE (Brachman and Schmolze 1985) to languages such as CLASSIC (Borgida et al. 1989) or Loom (MacGregor 1988, 1990). The terminological component describing the objects (the data model of object-oriented database systems) has always been more declarative than the assertional component (procedural attachment or methods), and the current trend is to integrate those two components more closely, where the assertional component is some kind of rule-based system, as in Loom (Yen, Neches, and MacGregor 1988). Typed feature structures are very similar to structured objects of object-oriented languages and to conceptual structures of knowledge representation languages. Thus, typed feature structures have the</context>
</contexts>
<marker>MacGregor, 1988</marker>
<rawString>MacGregor, Robert M. (1988). &amp;quot;A deductive pattern matcher.&amp;quot; In Proceedings, 7th National Conference on Artificial Intelligence (AAAT 88). St. Paul, MN, 403-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M MacGregor</author>
</authors>
<title>LOOM user manual.&amp;quot;</title>
<date>1990</date>
<tech>USC/ISI Technical Report,</tech>
<location>Marina del Rey, CA.</location>
<marker>MacGregor, 1990</marker>
<rawString>MacGregor, Robert M. (1990). &amp;quot;LOOM user manual.&amp;quot; USC/ISI Technical Report, Marina del Rey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Christian I M I Matthiessen</author>
</authors>
<title>Demonstration of the Nigel text generation computer program.&amp;quot;</title>
<date>1985</date>
<booktitle>In Systemic Perspectives on Discourse,</booktitle>
<volume>1</volume>
<publisher>Ablex.</publisher>
<location>Norwood, NJ:</location>
<note>edited by</note>
<contexts>
<context position="3558" citStr="Mann and Matthiessen 1985" startWordPosition="478" endWordPosition="481">rpreted as representing official policies. 1 In particular, it allows a direct implementation of HPSG-style grammars (Emele 1988): HPSG is so far the only linguistic theory based on both inheritance and feature structures. 0 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 2 and Sag 1987) make use of inheritance to describe linguistic structures at the lexical, morphological, syntactic, or semantic (conceptual) levels. These theories are usually directly implemented in object-oriented programming languages (e.g., Loom in the case of the PENMAN system [Mann and Matthiessen 1985]), but there is a growing number of linguistic formalisms used for specific purposes, e.g., DATR (Evans and Gazdar 1989) for the lexicon. On the other hand, current linguistic theories such as LFG, UCG, HPSG, and some formalisms for linguistic description such as FUG or PATR-II are based on the notion of partial information: linguistic structures are described using feature structures that give partial information about the object being modeled, a linguistic structure being described by a set of feature structures that mutually constrain the description. Feature structures are partially order</context>
</contexts>
<marker>Mann, Matthiessen, 1985</marker>
<rawString>Mann, William C., and Matthiessen, Christian I. M. I. (1985). &amp;quot;Demonstration of the Nigel text generation computer program.&amp;quot; In Systemic Perspectives on Discourse, Volume 1, edited by James D. Benson and William S. Greaves. Norwood, NJ: Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Jaime Carbonele</author>
<author>Masaru Tomita</author>
<author>Kenneth Goodman</author>
</authors>
<title>Machine Translation. A Knowledge-Based Approach.</title>
<date>1992</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="8319" citStr="Nirenburg et al. 1992" startWordPosition="1193" endWordPosition="1196">assertional component is some kind of rule-based system, as in Loom (Yen, Neches, and MacGregor 1988). Typed feature structures are very similar to structured objects of object-oriented languages and to conceptual structures of knowledge representation languages. Thus, typed feature structures have the potential to act as a lingua franca for both computational linguistics and artificial intelligence, and this should ease the communication between those two worlds. Since conceptual structures are used for example in text generation (Bourbeau et al. 1990) or knowledge-based machine translation (Nirenburg et al. 1992), typed feature structures provide an attractive alternative to current procedural implementations. In Section 2, we present a language that combines the notions of partial information and inheritance in a fully declarative framework. It is based on feature structures augmented with the notion of types, which are organized into an inheritance network. Using types, it is possible to define structured domains of feature structures and to classify feature structures. Logical conditions are attached to types, akin to method attachment, but in a fully declarative framework. Recursivity is an integr</context>
</contexts>
<marker>Nirenburg, Carbonele, Tomita, Goodman, 1992</marker>
<rawString>Nirenburg, Sergei, Carbonele, Jaime, Tomita, Masaru, and Goodman, Kenneth (1992). Machine Translation. A Knowledge-Based Approach. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Carl J Pollard</author>
</authors>
<title>Sorts in unification-based grammar and what they mean.&amp;quot; In Unification in Natural Language Analysis,</title>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<note>edited by</note>
<marker>Pollard, </marker>
<rawString>Pollard, Carl J. (In press). &amp;quot;Sorts in unification-based grammar and what they mean.&amp;quot; In Unification in Natural Language Analysis, edited by M. Pinkal and B. Gregor. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
<author>Drew Moshier</author>
</authors>
<title>Unifying partial descriptions of sets.&amp;quot;</title>
<date>1989</date>
<booktitle>In Information, Language and Cognition, edited by P. Hanson, Vancouver Studies in Cognitive Science 1.</booktitle>
<publisher>Press.</publisher>
<institution>University of British Columbia</institution>
<location>Vancouver:</location>
<marker>Pollard, Moshier, 1989</marker>
<rawString>Pollard, Carl J., and Moshier, Drew (1989). &amp;quot;Unifying partial descriptions of sets.&amp;quot; In Information, Language and Cognition, edited by P. Hanson, Vancouver Studies in Cognitive Science 1. Vancouver: University of British Columbia Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Information-Based Syntax and Semantics.</title>
<date>1987</date>
<booktitle>Volume 1: Syntax. CSLI Lecture Notes 13,</booktitle>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="13221" citStr="Pollard and Sag (1987)" startWordPosition="1993" endWordPosition="1996">uce the answer, the system proceeds by gradually adding the constraints that should be satisfied by the query: an answer will be a set of feature structures where each feature structure is subsumed by the query and where all the type constraints of the network hold on all substructures of the elements of the answer. The answer is the empty set when the query does not satisfy the constraints defined by the network. Related work. The basic approach described in this section is based on original work by Ait-Kaci (1984, 1986) on the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and AltKaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 19</context>
<context position="34575" citStr="Pollard and Sag 1987" startWordPosition="5592" endWordPosition="5595">fied by a given linguistic object: parsing and generation differ only in the nature of the &amp;quot;input,&amp;quot; and use the same constraint evaluation mechanism. The properties of a computational framework for implementing constraint-based grammars are: • A unique general constraint solving mechanism is used: grammars define constraints on the set of acceptable linguistic structures. • As a consequence, there is no formal distinction between &amp;quot;input&amp;quot; and &amp;quot;output.&amp;quot; For example, the same kind of data structure could be used to encode both the string and the structural description, and, as for the HPSG sign (Pollard and Sag 1987), they could be embedded into a single data structure that represents the relation between the string and the associated linguistic structure. • Specific mapping properties, based on constituency, linear precedence, or functional composition, are not part of the formalism itself, but can be encoded explicitly in the formalism. An approach that uses a unique deductive mechanism for parsing and generation is described in Dymetman and Isabelle (1988). Within this approach, a lazy evaluation mechanism based on the specification of input/output arguments is implemented (in PROLOG), and the evaluati</context>
<context position="37481" citStr="Pollard and Sag 1987" startWordPosition="6042" endWordPosition="6045">AD-DTR I SYN I LOC I SUBCAT append(cij, 0) COMP-DTRS PHRASE &lt; syn: SUBCAT :- APPEND 1: ID SUBCAT-FP dtrs: -FP. 2: Ili [ snbcat: [D] 3: IJ ] head-dtr: [syn: [subcat: 111]J} comp-dtrs: In Figure 5 The HSPG subcategorization principle and its TFS encoding. of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based on the notion of typed feature structures (Pollard 1990), and is thus a good candidate to illustrate the possibilities of the TFS formalism. The following presentation is based on Emele (1988) and Emele and Zajac (1990b). The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure, syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot; the relation between a string and a linguistic structure is encoded as a single feature structure representing the &amp;quot;sign.&amp;quot; HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type PHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, and SEM-FP. For example, the HPSG subcategorization principle is encoded in TFS using inheritance to model implication, and t</context>
<context position="38850" citStr="Pollard and Sag 1987" startWordPosition="6249" endWordPosition="6252">ciple and SEM-FP encodes the Semantics Principle. The type SIGN is divided into several subtypes corresponding to different mappings between a string and a linguistic structure. We first have the basic distinction between phrases and words. The definition of a phrase recursively relates subphrases and substrings, and defines the phrase as a composition of subphrases and the string as the concatenation of substrings. Since the formalism itself does not impose any constraints on how the relations between phrases and strings are defined, the grammar writer has to define them explicitly. In HPSG (Pollard and Sag 1987), the ordering of phrases is defined using linear precedence relations: the order in which the substrings associated with subphrases are concatenated to give the string associated with a phrase are guided by these linear precedence relations (Reape 1990). In the example given below (Figure 6), we make simplifying assumptions: the LOCAL feature is not used and there are two possible orderings for complements. The type IDP1 encodes Grammar Rule 1 (Pollard and Sag 1987, pp. 149-155), which says that a &amp;quot;saturated phrasal sign,&amp;quot; i.e., a feature structure with [syn : [subcat : ()]], is the combinati</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl J., and Sag, Ivan A. (1987). Information-Based Syntax and Semantics. Volume 1: Syntax. CSLI Lecture Notes 13, Chicago University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Carl J Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>(Unpublished research). Information-Based Syntax and Semantics. Volume 2: Agreement, Binding and Control.</title>
<marker>Pollard, Sag, </marker>
<rawString>Pollard, Carl J., and Sag, Ivan A. (Unpublished research). Information-Based Syntax and Semantics. Volume 2: Agreement, Binding and Control.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>Parsing semi-free word order and bounded discontinuous constituency and &amp;quot;shake &apos;n&apos; bake&amp;quot; machine translation (or &apos;generation as parsing&apos;).&amp;quot; International Workshop on Constraint Based Formalisms for Natural Language Generation,</title>
<date>1990</date>
<location>Bad Teinach, Germany.</location>
<contexts>
<context position="35730" citStr="Reape 1990" startWordPosition="5763" endWordPosition="5764"> arguments is implemented (in PROLOG), and the evaluation is completely data-driven: the same program parses or generates, depending only on the form of the input structure. A constraint-based grammar does not need a context-free mechanism to build up constituent structures for parsing or generation: Dymetman, Isabelle, and Perrault (1990) describe a class of reversible grammars (&amp;quot;Lexical Grammars&amp;quot;) based on a few composition rules that are very reminiscent of categorial grammars. Other kinds of approaches have been proposed, e.g., using a dependency structure and linear precedence relations (Reape 1990; see also Pollard and Sag [19871). In Saint-Dizier (1991), linear precedence rules are defined as constraints in a language based on typed feature structures and SLD-resolution, which is used to experiment with GB theory. In the following sections, we describe two examples of constraint-based grammars: an HPSG grammar for a fragment of English, and an LFG-style transfer grammar for a small machine translation problem between English and French. 3.2 Head-Driven Phrase Structure Grammar In general, a grammar describes the relation between strings of words and linguistic structures. To implement</context>
<context position="39104" citStr="Reape 1990" startWordPosition="6290" endWordPosition="6291">ase recursively relates subphrases and substrings, and defines the phrase as a composition of subphrases and the string as the concatenation of substrings. Since the formalism itself does not impose any constraints on how the relations between phrases and strings are defined, the grammar writer has to define them explicitly. In HPSG (Pollard and Sag 1987), the ordering of phrases is defined using linear precedence relations: the order in which the substrings associated with subphrases are concatenated to give the string associated with a phrase are guided by these linear precedence relations (Reape 1990). In the example given below (Figure 6), we make simplifying assumptions: the LOCAL feature is not used and there are two possible orderings for complements. The type IDP1 encodes Grammar Rule 1 (Pollard and Sag 1987, pp. 149-155), which says that a &amp;quot;saturated phrasal sign,&amp;quot; i.e., a feature structure with [syn : [subcat : ()]], is the combination of an unsaturated phrasal head with one phrasal complement on the left. For example, for structures like S —4 NP VP, S is the &amp;quot;saturated phrasal sign,&amp;quot; NP is 11 We will use a more condensed notation for lists with angle brackets provided by the TFS la</context>
<context position="47785" citStr="Reape (1990)" startWordPosition="7674" endWordPosition="7675">ar. If we restrict ourselves to classes of grammars for which terminating evaluation algorithms are known, we can implement those directly in TFS. However, the TFS evaluation strategy allows more naive implementations of grammars, and the outermost rewriting of &amp;quot;sub-goals&amp;quot; terminates on a strictly larger class of programs than for corresponding logic programs implemented in PROLOG. Furthermore, the grammar writer does not need, and actually should not, be aware of the control that follows the shape of the input rather than a fixed strategy, thanks to the lazy evaluation mechanism. 12 See also Reape (1990) for another approach to MT using feature structures and based on Whitelock (1990). 175 Computational Linguistics Volume 18, Number 2 For HPSG-style grammars, completeness and coherence as defined for LFG, and extended to the general case by Wedekind (1988), are implemented in HPSG using the &amp;quot;subcategorization feature principle&amp;quot; (Johnson 1987): for the TFS implementation of HPSG, termination is guaranteed, at least for the simplified version containing only head-complement structures, described in Section 3.2. Termination conditions for parsing are well understood in the framework of context-f</context>
</contexts>
<marker>Reape, 1990</marker>
<rawString>Reape, Mike (1990). &amp;quot;Parsing semi-free word order and bounded discontinuous constituency and &amp;quot;shake &apos;n&apos; bake&amp;quot; machine translation (or &apos;generation as parsing&apos;).&amp;quot; International Workshop on Constraint Based Formalisms for Natural Language Generation, Bad Teinach, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Williams C Rounds</author>
<author>Robert T Kasper</author>
</authors>
<title>A Complete Logical Calculus for Record Structures Representing Linguistic Information.&amp;quot;</title>
<date>1986</date>
<booktitle>IEEE Symposium on Logic in Computer Science.</booktitle>
<contexts>
<context position="10400" citStr="Rounds and Kasper 1986" startWordPosition="1517" endWordPosition="1520">s can provide only partial information about the objects they describe, a feature structure denotes a set of objects in this universe. This set could be a singleton set, for example, in the case of atomic feature structures. Feature structures are ordered by a subsumption relation: a feature structure h subsumes another feature structure f2 iff h provides the same or less information than f2: fi &gt; f2. In our universe, this means that the set described by fi is a superset of the set described by f2. Note that there can be feature structures that cannot consistently describe the 2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature structures are derived, plays this role in formal accounts of feature structures. 3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been implemented at the University of Stuttgart by Martin Emele and the author and has been used to test several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al. 1990; Zajac 1990a; Bateman and Momma 19911. 161 Computational Linguistics Volume 18, Number 2 same objects. For example, a feature structure describing verb phrases and a fe</context>
<context position="14803" citStr="Rounds and Kasper 1986" startWordPosition="2234" endWordPosition="2237">pecialization rule, • uses a lazy evaluation scheme for the evaluation of constraints, • implements static coherence checks, • has a simple syntax distinguishing the definition of the partial order on type symbols, and the definition of constraints associated to types. 162 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms 2.1 Types In the following presentation, we adopt an algebraic approach based on lattice theory (Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for example, a proof-theoretical approach using an adaptation of a feature logic (Rounds and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of these different models, as this is done for the LIFE language in Ait-Kaci and Podelski (1991). The presentation is nevertheless rather informal, and a more technical account can be found in Emele and Zajac (1990a) and Zajac (1990b). The universe of feature structures is structured in an inheritance network that defines a partial ordering on kinds of available information. The backbone of the network is defined by a finite set of type symbols T together with a partial ordering &lt; on T: the partially ordered set (poset) (T ,&lt;). T</context>
</contexts>
<marker>Rounds, Kasper, 1986</marker>
<rawString>Rounds, Williams C., and Kasper, Robert T. (1986). &amp;quot;A Complete Logical Calculus for Record Structures Representing Linguistic Information.&amp;quot; IEEE Symposium on Logic in Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Saint-Dizier</author>
</authors>
<title>Processing language with logical types and actives constraints.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, 5th Conference of the European Chapter of the ACL.</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="35788" citStr="Saint-Dizier (1991)" startWordPosition="5772" endWordPosition="5773">aluation is completely data-driven: the same program parses or generates, depending only on the form of the input structure. A constraint-based grammar does not need a context-free mechanism to build up constituent structures for parsing or generation: Dymetman, Isabelle, and Perrault (1990) describe a class of reversible grammars (&amp;quot;Lexical Grammars&amp;quot;) based on a few composition rules that are very reminiscent of categorial grammars. Other kinds of approaches have been proposed, e.g., using a dependency structure and linear precedence relations (Reape 1990; see also Pollard and Sag [19871). In Saint-Dizier (1991), linear precedence rules are defined as constraints in a language based on typed feature structures and SLD-resolution, which is used to experiment with GB theory. In the following sections, we describe two examples of constraint-based grammars: an HPSG grammar for a fragment of English, and an LFG-style transfer grammar for a small machine translation problem between English and French. 3.2 Head-Driven Phrase Structure Grammar In general, a grammar describes the relation between strings of words and linguistic structures. To implement a constraint-based grammar in TFS, we have to encode both</context>
</contexts>
<marker>Saint-Dizier, 1991</marker>
<rawString>Saint-Dizier, Patrick (1991). &amp;quot;Processing language with logical types and actives constraints.&amp;quot; In Proceedings, 5th Conference of the European Chapter of the ACL. Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>An Introduction to Unification-based Grammar Formalisms.</title>
<date>1986</date>
<journal>CSLI Lectures Notes</journal>
<volume>4</volume>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="20995" citStr="Shieber 1986" startWordPosition="3342" endWordPosition="3343">ivalence class used to represent equational constraints (&amp;quot;co-references&amp;quot;), and a set of outgoing arcs. The unification algorithm uses the union/find procedure on an inverted set representation of the equivalence classes adapted by Aft-Kaci (1984) after Huet (1976). The actual algorithm used in the system is optimized using several different techniques to minimize copying and to behave as efficiently as a pattern-matcher in cases when one of the feature structures subsumes the other (Emele 1991). 2.3 Inheritance Network of Feature Structures The template mechanism (as, for example, in PATR-II [Shieber 1986]) already provides a simple inheritance mechanism used to organize lexical descriptions. In comparison, networks of typed feature structures are more expressive and provide a more general and more powerful inheritance mechanism, which allows the use of recursive type definitions, whereas recursivity is forbidden in templates since they are expanded statically using a macro-expansion mechanism. Furthermore, typing provides a notion of well-formedness that is used to implement a type-discipline and consistency checks, giving the user the means of checking statically the coherence of a set of ty</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, Stuart (1986). An Introduction to Unification-based Grammar Formalisms. CSLI Lectures Notes 4, Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gert Smolka</author>
</authors>
<title>A feature logic with subsorts.&amp;quot;</title>
<date>1988</date>
<tech>LILOG Report 33,</tech>
<institution>IBM Deutschland GmbH,</institution>
<location>Stuttgart.</location>
<contexts>
<context position="13395" citStr="Smolka (1988" startWordPosition="2022" endWordPosition="2023">e is subsumed by the query and where all the type constraints of the network hold on all substructures of the elements of the answer. The answer is the empty set when the query does not satisfy the constraints defined by the network. Related work. The basic approach described in this section is based on original work by Ait-Kaci (1984, 1986) on the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and AltKaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was already a departure from Ait-Kaci&apos;s KBL language, implementing a different evaluation strategy that allowed the use of cyclic feature structu</context>
<context position="14817" citStr="Smolka 1988" startWordPosition="2238" endWordPosition="2239">es a lazy evaluation scheme for the evaluation of constraints, • implements static coherence checks, • has a simple syntax distinguishing the definition of the partial order on type symbols, and the definition of constraints associated to types. 162 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms 2.1 Types In the following presentation, we adopt an algebraic approach based on lattice theory (Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for example, a proof-theoretical approach using an adaptation of a feature logic (Rounds and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of these different models, as this is done for the LIFE language in Ait-Kaci and Podelski (1991). The presentation is nevertheless rather informal, and a more technical account can be found in Emele and Zajac (1990a) and Zajac (1990b). The universe of feature structures is structured in an inheritance network that defines a partial ordering on kinds of available information. The backbone of the network is defined by a finite set of type symbols T together with a partial ordering &lt; on T: the partially ordered set (poset) (T ,&lt;). The ordering &lt; </context>
</contexts>
<marker>Smolka, 1988</marker>
<rawString>Smolka, Gert (1988). &amp;quot;A feature logic with subsorts.&amp;quot; LILOG Report 33, IBM Deutschland GmbH, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gert Smolka</author>
</authors>
<title>Feature constraint logics for unification grammars.&amp;quot;</title>
<date>1989</date>
<tech>IWBS Report 93,</tech>
<institution>IBM Deutschland GmbH,</institution>
<location>Stuttgart.</location>
<marker>Smolka, 1989</marker>
<rawString>Smolka, Gert (1989). &amp;quot;Feature constraint logics for unification grammars.&amp;quot; IWBS Report 93, IBM Deutschland GmbH, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gert Smolka</author>
<author>Hassan Alt-Kaci</author>
</authors>
<title>Inheritance hierarchies: Semantics and unification.&amp;quot;</title>
<date>1988</date>
<journal>I. Symbolic Computation,</journal>
<volume>7</volume>
<pages>343--370</pages>
<marker>Smolka, Alt-Kaci, 1988</marker>
<rawString>Smolka, Gert, and Alt-Kaci, Hassan (1988). &amp;quot;Inheritance hierarchies: Semantics and unification.&amp;quot; I. Symbolic Computation, 7, 343-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph E Stoy</author>
</authors>
<title>Denotational Semantics: Computational Linguistics Volume 18, Number 2 The Scott—Strachey Approach to Programming Language Theory.</title>
<date>1977</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="5228" citStr="Stoy 1977" startWordPosition="725" endWordPosition="726"> gain the advantages of the object-oriented approach: abstraction and generalization through the use of inheritance. On the other hand, we gain a fully declarative framework, with all the advantages of logical formalisms: expressive power, simplicity, and sound formal semantics. To arrive at such a result, we have to enrich the formalism of feature structures with the notion of inheritance and abandon some of the procedural features of object-oriented languages in order to gain referential transparency. Referential transparency is one of the characteristic properties of declarative languages (Stoy 1977), where the meaning of each language construct is given by a few simple and general rules. For example, the value of a variable should be independent from its position within the scope of its declaration. This is true for PROLOG variables inside a clause, but not for PASCAL or LISP variables that make use of assignment. A higher level example is the meaning of a procedure: it is not transparent if the procedure makes use of global variables that are set by some other procedure. Similarly, the meaning of a PROLOG predicate should be transparent because there is no global variable, but a predica</context>
</contexts>
<marker>Stoy, 1977</marker>
<rawString>Stoy, Joseph E. (1977). Denotational Semantics: Computational Linguistics Volume 18, Number 2 The Scott—Strachey Approach to Programming Language Theory. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal van Hentenryck</author>
</authors>
<title>Constraint Satisfaction in Logic Programming,</title>
<date>1989</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>van Hentenryck, 1989</marker>
<rawString>van Hentenryck, Pascal (1989). Constraint Satisfaction in Logic Programming, Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal van Hentenryck</author>
<author>Mehmet Dincbas</author>
</authors>
<title>Forward checking in logic programming.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, 4th International Conference on Logic Programming.</booktitle>
<location>Melbourne, Australia.</location>
<marker>van Hentenryck, Dincbas, 1987</marker>
<rawString>van Hentenryck, Pascal, and Dincbas, Mehmet (1987). &amp;quot;Forward checking in logic programming.&amp;quot; In Proceedings, 4th International Conference on Logic Programming. Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Reversible unification-based machine translation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90).</booktitle>
<location>Helsinki.</location>
<marker>van Noord, 1990</marker>
<rawString>van Noord, Gertjan (1990). &amp;quot;Reversible unification-based machine translation.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics (COLING&apos;90). Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Wedekind</author>
</authors>
<title>Generation as structure driven generation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 12th International Conference on Computational Linguistics (COLING&apos;88).</booktitle>
<location>Budapest.</location>
<contexts>
<context position="48042" citStr="Wedekind (1988)" startWordPosition="7713" endWordPosition="7714">ting of &amp;quot;sub-goals&amp;quot; terminates on a strictly larger class of programs than for corresponding logic programs implemented in PROLOG. Furthermore, the grammar writer does not need, and actually should not, be aware of the control that follows the shape of the input rather than a fixed strategy, thanks to the lazy evaluation mechanism. 12 See also Reape (1990) for another approach to MT using feature structures and based on Whitelock (1990). 175 Computational Linguistics Volume 18, Number 2 For HPSG-style grammars, completeness and coherence as defined for LFG, and extended to the general case by Wedekind (1988), are implemented in HPSG using the &amp;quot;subcategorization feature principle&amp;quot; (Johnson 1987): for the TFS implementation of HPSG, termination is guaranteed, at least for the simplified version containing only head-complement structures, described in Section 3.2. Termination conditions for parsing are well understood in the framework of context-free grammars. For generation using feature structures, one of the problems is that the input could be &amp;quot;extended&amp;quot; during processing, i.e., arbitrary feature structures could be introduced in the semantic part of the input by unification with the semantic par</context>
</contexts>
<marker>Wedekind, 1988</marker>
<rawString>Wedekind, Jurgen (1988). &amp;quot;Generation as structure driven generation.&amp;quot; In Proceedings, 12th International Conference on Computational Linguistics (COLING&apos;88). Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pete Whitelock</author>
</authors>
<title>Shake-and-bake translation.&amp;quot; Ms. Sharp Laboratories of Europe,</title>
<date>1990</date>
<location>Oxford.</location>
<contexts>
<context position="47867" citStr="Whitelock (1990)" startWordPosition="7687" endWordPosition="7688">uation algorithms are known, we can implement those directly in TFS. However, the TFS evaluation strategy allows more naive implementations of grammars, and the outermost rewriting of &amp;quot;sub-goals&amp;quot; terminates on a strictly larger class of programs than for corresponding logic programs implemented in PROLOG. Furthermore, the grammar writer does not need, and actually should not, be aware of the control that follows the shape of the input rather than a fixed strategy, thanks to the lazy evaluation mechanism. 12 See also Reape (1990) for another approach to MT using feature structures and based on Whitelock (1990). 175 Computational Linguistics Volume 18, Number 2 For HPSG-style grammars, completeness and coherence as defined for LFG, and extended to the general case by Wedekind (1988), are implemented in HPSG using the &amp;quot;subcategorization feature principle&amp;quot; (Johnson 1987): for the TFS implementation of HPSG, termination is guaranteed, at least for the simplified version containing only head-complement structures, described in Section 3.2. Termination conditions for parsing are well understood in the framework of context-free grammars. For generation using feature structures, one of the problems is that</context>
</contexts>
<marker>Whitelock, 1990</marker>
<rawString>Whitelock, Pete (1990). &amp;quot;Shake-and-bake translation.&amp;quot; Ms. Sharp Laboratories of Europe, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Yen</author>
<author>Robert Neches</author>
<author>Robert MacGregor</author>
</authors>
<title>Classification-based programming: A deep integration of frames and rules.&amp;quot;</title>
<date>1988</date>
<tech>Technical Report ISI/RR-88-213,</tech>
<institution>USC/Information Science Institute.</institution>
<marker>Yen, Neches, MacGregor, 1988</marker>
<rawString>Yen, John; Neches, Robert; and MacGregor, Robert (1988). &amp;quot;Classification-based programming: A deep integration of frames and rules.&amp;quot; Technical Report ISI/RR-88-213, USC/Information Science Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remi Zajac</author>
</authors>
<title>A transfer model using a typed feature structure rewriting system with inheritance.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 27th Annual Meeting of the ACL.</booktitle>
<location>Vancouver, B.C.</location>
<contexts>
<context position="13740" citStr="Zajac (1989" startWordPosition="2077" endWordPosition="2078"> the KBL language and has also been influenced by the work on HPSG by Pollard and Sag (1987) and Pollard and Moshier (1990). Among the growing literature on the semantics of feature structures, many relevant results and techniques have been published by Smolka (1988, 1989), Smolka and Ait-Kaci (1988), and AltKaci and Podelski (1991). Based on Pollard and Sag (1987), Pollard (1990), and Pollard and Moshier (1990), a computational formalism, very close to the TFS formalism, is currently under design at CMU for implementing HPSG (Carpenter 1990, Franz 1990). The early work presented in Emele and Zajac (1989a) was an attempt to directly implement the ideas presented in Nit-Kaci (1984, 1986); Emele and Zajac (1989b) was already a departure from Ait-Kaci&apos;s KBL language, implementing a different evaluation strategy that allowed the use of cyclic feature structures. Compared with the KBL language, the current TFS language • allows the use of cyclic feature structures, • has a simple operational semantics implementing an inheritance rule and a specialization rule, • uses a lazy evaluation scheme for the evaluation of constraints, • implements static coherence checks, • has a simple syntax distinguishi</context>
<context position="41891" citStr="Zajac 1989" startWordPosition="6743" endWordPosition="6744">h cases is exactly the same: a fully specified structure containing the string, the full semantic form, and also all other syntactic information such as the constituent structure (Figure 8). 3.3 Bi-directional Transfer in Machine Translation We have sketched above a very general framework for specifying mappings between a linguistic structure, encoded as a feature structure, and a string, also encoded as a feature structure. We apply a similar technique for specifying transfer rules for machine translation, which we prefer to call &amp;quot;contrastive rules&amp;quot; since there is no directionality involved (Zajac 1989; 1990a). SlJECAT-FP HEAD-FP SEM-FP PHRASE[dtrs: TREE] phon: a) [syn: rcat: ()) IDP1 head-dtr: PHRASE[phon: Eiji dtrs: comp-dtrs: (SIGN[phon: al]) 1: ijl [ :- APPEND 2: al 3: fa IDP2[ al syn: snbcat: (SIGN)] dtrs: [head-dtr: WORD[phon: comp-dtrs: III APPEND[2: ai , 3: III 1: El ORDER-COMPEcomps: 01 phon: 172 Remi Zajac Inheritance and Constraint-Based Grammar Formalisms Query for parsing PHRASE[phon: (Kim ate every cookie)] Query for generation PHRASE sem: {rein: EAT [ argl: ind: [restr: [name: KIM] 1] 1] arg2: spec: EVERY . r r Ind: trestr: Vein: COOKIE]]] Figure 7 Queries for parsing and gen</context>
</contexts>
<marker>Zajac, 1989</marker>
<rawString>Zajac, Remi (1989). &amp;quot;A transfer model using a typed feature structure rewriting system with inheritance.&amp;quot; In Proceedings, 27th Annual Meeting of the ACL. Vancouver, B.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remi Zajac</author>
</authors>
<title>A relational approach to translation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 3rd International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language.</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="10811" citStr="Zajac 1990" startWordPosition="1587" endWordPosition="1588">is means that the set described by fi is a superset of the set described by f2. Note that there can be feature structures that cannot consistently describe the 2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature structures are derived, plays this role in formal accounts of feature structures. 3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been implemented at the University of Stuttgart by Martin Emele and the author and has been used to test several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al. 1990; Zajac 1990a; Bateman and Momma 19911. 161 Computational Linguistics Volume 18, Number 2 same objects. For example, a feature structure describing verb phrases and a feature structure describing noun phrases are not consistent: the intersection of the sets they denote is usually empty. As different sets of attribute-value pairs make sense for different kinds of objects, we also divide our feature structures into different types. These types are ordered by a subtype relation: a type t2 is a subtype of another type t1 if t2 provides at least as much information as t1. For ex</context>
<context position="15082" citStr="Zajac (1990" startWordPosition="2282" endWordPosition="2283">itance and Constraint-Based Grammar Formalisms 2.1 Types In the following presentation, we adopt an algebraic approach based on lattice theory (Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for example, a proof-theoretical approach using an adaptation of a feature logic (Rounds and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of these different models, as this is done for the LIFE language in Ait-Kaci and Podelski (1991). The presentation is nevertheless rather informal, and a more technical account can be found in Emele and Zajac (1990a) and Zajac (1990b). The universe of feature structures is structured in an inheritance network that defines a partial ordering on kinds of available information. The backbone of the network is defined by a finite set of type symbols T together with a partial ordering &lt; on T: the partially ordered set (poset) (T ,&lt;). The ordering &lt; defines the subtype relation: for A, B E T we read A &lt; B as &amp;quot;A is a subtype of B.&amp;quot; We call the smallest types of T the minimal types. To have a well-behaved type hierarchy, we require that (T,&lt;) be such that: • T contains the symbols T and I, where T is the greates</context>
<context position="30306" citStr="Zajac (1990" startWordPosition="4921" endWordPosition="4922">straints to the query using the rewrite rules (nondeterministically) to get closer to the solution step by step. The rewriting process stops when conditions 1 and 2 described above hold. A rewrite step for a structure t is defined as follows: if u is a substructure of t at path p and u is of type A, and there exists a rewrite rule A[a] 13[19] such that • A[a] A U _L, and • A[a] A U &lt; A[a] then the right-hand side B[b] is unified with the substructure u at path p, giving a new structure t&apos; that is more specific than t (Figure 4). 8 See Nit-Kaci (1984), Pollard and Moshier (1990), and Emele and Zajac (1990a) for fixed-point characterizations of the denotation of typed feature structures. 9 Narrowing uses unification instead of pattern-matching for checking the applicability of the 1.h.s. of a rule. Narrowing is used in the logic programming paradigm (e.g., as an alternative to resolution for implementing PROLOG interpreters). Pattern-matching is used in the functional programming paradigm. NIL CONS[first: T rest: LIST 1: NIL APPENDO 2: 0 LIST 3: 0 APPEND1 1: CONS[first:E - rest:El 2: g LIST 3: CONS[first:E rest:0 LIST LIST [1: LIST] APPEND 2: LIST 3: LIST [1: LIST APPEND 2: LIST 3: LIST [1: 01 </context>
<context position="37419" citStr="Zajac (1990" startWordPosition="6034" endWordPosition="6035">I LOC I SUBCAT EDTRS headed-structure{ I I =[DTRS{HEAD-DTR I SYN I LOC I SUBCAT append(cij, 0) COMP-DTRS PHRASE &lt; syn: SUBCAT :- APPEND 1: ID SUBCAT-FP dtrs: -FP. 2: Ili [ snbcat: [D] 3: IJ ] head-dtr: [syn: [subcat: 111]J} comp-dtrs: In Figure 5 The HSPG subcategorization principle and its TFS encoding. of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based on the notion of typed feature structures (Pollard 1990), and is thus a good candidate to illustrate the possibilities of the TFS formalism. The following presentation is based on Emele (1988) and Emele and Zajac (1990b). The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure, syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot; the relation between a string and a linguistic structure is encoded as a single feature structure representing the &amp;quot;sign.&amp;quot; HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type PHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, and SEM-FP. For example, the HPSG subcategorization principle i</context>
</contexts>
<marker>Zajac, 1990</marker>
<rawString>Zajac, Remi (1990a). &amp;quot;A relational approach to translation.&amp;quot; In Proceedings, 3rd International Conference on Theoretical and Methodological Issues in Machine Translation of Natural Language. Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rend Zajac</author>
</authors>
<title>Semantics of typed feature structures.&amp;quot; International Workshop on Constraint Based Formalisms for Natural Language Generation.</title>
<date>1990</date>
<location>Bad Teinach, Germany.</location>
<contexts>
<context position="10811" citStr="Zajac 1990" startWordPosition="1587" endWordPosition="1588">is means that the set described by fi is a superset of the set described by f2. Note that there can be feature structures that cannot consistently describe the 2 The KR logic (Rounds and Kasper 1986), from which most of other logic formalizations of feature structures are derived, plays this role in formal accounts of feature structures. 3 This formalism is fully implemented. An interpreter for the TFS rewrite machine has been implemented at the University of Stuttgart by Martin Emele and the author and has been used to test several linguistic models such as DCG, LFG, HPSG, and SFG [Emele and Zajac 1990b; Emele et al. 1990; Zajac 1990a; Bateman and Momma 19911. 161 Computational Linguistics Volume 18, Number 2 same objects. For example, a feature structure describing verb phrases and a feature structure describing noun phrases are not consistent: the intersection of the sets they denote is usually empty. As different sets of attribute-value pairs make sense for different kinds of objects, we also divide our feature structures into different types. These types are ordered by a subtype relation: a type t2 is a subtype of another type t1 if t2 provides at least as much information as t1. For ex</context>
<context position="15082" citStr="Zajac (1990" startWordPosition="2282" endWordPosition="2283">itance and Constraint-Based Grammar Formalisms 2.1 Types In the following presentation, we adopt an algebraic approach based on lattice theory (Birkoff 1984; AIt-Kaci 1984). Alternative presentations could be developed as well; for example, a proof-theoretical approach using an adaptation of a feature logic (Rounds and Kasper 1986; Smolka 1988). It is possible to prove formally the equivalence of these different models, as this is done for the LIFE language in Ait-Kaci and Podelski (1991). The presentation is nevertheless rather informal, and a more technical account can be found in Emele and Zajac (1990a) and Zajac (1990b). The universe of feature structures is structured in an inheritance network that defines a partial ordering on kinds of available information. The backbone of the network is defined by a finite set of type symbols T together with a partial ordering &lt; on T: the partially ordered set (poset) (T ,&lt;). The ordering &lt; defines the subtype relation: for A, B E T we read A &lt; B as &amp;quot;A is a subtype of B.&amp;quot; We call the smallest types of T the minimal types. To have a well-behaved type hierarchy, we require that (T,&lt;) be such that: • T contains the symbols T and I, where T is the greates</context>
<context position="30306" citStr="Zajac (1990" startWordPosition="4921" endWordPosition="4922">straints to the query using the rewrite rules (nondeterministically) to get closer to the solution step by step. The rewriting process stops when conditions 1 and 2 described above hold. A rewrite step for a structure t is defined as follows: if u is a substructure of t at path p and u is of type A, and there exists a rewrite rule A[a] 13[19] such that • A[a] A U _L, and • A[a] A U &lt; A[a] then the right-hand side B[b] is unified with the substructure u at path p, giving a new structure t&apos; that is more specific than t (Figure 4). 8 See Nit-Kaci (1984), Pollard and Moshier (1990), and Emele and Zajac (1990a) for fixed-point characterizations of the denotation of typed feature structures. 9 Narrowing uses unification instead of pattern-matching for checking the applicability of the 1.h.s. of a rule. Narrowing is used in the logic programming paradigm (e.g., as an alternative to resolution for implementing PROLOG interpreters). Pattern-matching is used in the functional programming paradigm. NIL CONS[first: T rest: LIST 1: NIL APPENDO 2: 0 LIST 3: 0 APPEND1 1: CONS[first:E - rest:El 2: g LIST 3: CONS[first:E rest:0 LIST LIST [1: LIST] APPEND 2: LIST 3: LIST [1: LIST APPEND 2: LIST 3: LIST [1: 01 </context>
<context position="37419" citStr="Zajac (1990" startWordPosition="6034" endWordPosition="6035">I LOC I SUBCAT EDTRS headed-structure{ I I =[DTRS{HEAD-DTR I SYN I LOC I SUBCAT append(cij, 0) COMP-DTRS PHRASE &lt; syn: SUBCAT :- APPEND 1: ID SUBCAT-FP dtrs: -FP. 2: Ili [ snbcat: [D] 3: IJ ] head-dtr: [syn: [subcat: 111]J} comp-dtrs: In Figure 5 The HSPG subcategorization principle and its TFS encoding. of definitions as in Figure 2.&amp;quot; HPSG is so far the only formal linguistic theory based on the notion of typed feature structures (Pollard 1990), and is thus a good candidate to illustrate the possibilities of the TFS formalism. The following presentation is based on Emele (1988) and Emele and Zajac (1990b). The basic linguistic object in HPSG (Pollard and Sag 1987) is a complex linguistic structure, the &amp;quot;sign,&amp;quot; with four levels of description: phonology, constituent structure, syntax, and semantics. In HPSG, there is no distinction between &amp;quot;input&amp;quot; and &amp;quot;output:&amp;quot; the relation between a string and a linguistic structure is encoded as a single feature structure representing the &amp;quot;sign.&amp;quot; HPSG &amp;quot;principles&amp;quot; are encoded using inheritance: a feature structure of type PHRASE inherits the constraints associated with types SUBCAT-FP, HEAD-FP, and SEM-FP. For example, the HPSG subcategorization principle i</context>
</contexts>
<marker>Zajac, 1990</marker>
<rawString>Zajac, Rend (1990). &amp;quot;Semantics of typed feature structures.&amp;quot; International Workshop on Constraint Based Formalisms for Natural Language Generation. Bad Teinach, Germany.</rawString>
</citation>
<citation valid="false">
<authors>
<author>identifier</author>
</authors>
<title>Example 1: the textual definitions for Figure 2.</title>
<journal>NIL &lt; LIST.</journal>
<marker>identifier, </marker>
<rawString>&lt;tag&gt; ::= #&lt;identifier&gt; &lt;type-symbol&gt; ::= &lt;identifier&gt; &lt;attribute&gt; ::= &lt;identifier&gt; Example 1: the textual definitions for Figure 2. NIL &lt; LIST.</rawString>
</citation>
<citation valid="false">
<authors>
<author>CONS LIST</author>
</authors>
<booktitle>CONS [first : T, rest: LIST] . APPEND[1: LIST, 2: LIST, 3: LIST] .</booktitle>
<marker>LIST, </marker>
<rawString>CONS &lt; LIST. CONS [first : T, rest: LIST] . APPEND[1: LIST, 2: LIST, 3: LIST] .</rawString>
</citation>
<citation valid="false">
<booktitle>Computational Linguistics Volume 18, Number 2 APPENDO &lt; APPEND. APPENDO[1: NIL,</booktitle>
<volume>2</volume>
<pages>1</pages>
<marker></marker>
<rawString>Computational Linguistics Volume 18, Number 2 APPENDO &lt; APPEND. APPENDO[1: NIL, 2: #1=LIST, 3: #1].</rawString>
</citation>
<citation valid="false">
<title>Example 2: the textual definitions for Figure 10.</title>
<booktitle>APPEND1 &lt; APPEND. APPEND1[1: &lt;#x .</booktitle>
<volume>11</volume>
<pages>13</pages>
<marker></marker>
<rawString>APPEND1 &lt; APPEND. APPEND1[1: &lt;#x . #11&gt;, 2: #12, 3: &lt;#x . #13&gt;] APPEND[1: #11, 2: #12, 3: #13]. Example 2: the textual definitions for Figure 10.</rawString>
</citation>
<citation valid="false">
<authors>
<author>TAU-LEX-V TAU-LEX</author>
</authors>
<title>TAU-LEX-V[eng: [tense: #e-tense], fr: [tense: #f-tense]] TAU-TENSE[eng: #e-tense, fr: #f-tense]. TAU-LEX-ITV &lt; TAU-LEX-V. TAU-LEX-V[eng: [subj: #e-subj], fr: [subj: #f-subj]] TAU-LEX[eng: #e-subj, fr: #f-subj]. TAU-LEX-FALL &lt; TAU-LEX-ITV. TAU-LEX-FALL[eng: [pred: FALL], fr: [pred: TOMBER</title>
<marker>TAU-LEX, </marker>
<rawString>TAU-LEX &lt; TAU. TAU-LEX-V &lt; TAU-LEX. TAU-LEX-V[eng: [tense: #e-tense], fr: [tense: #f-tense]] TAU-TENSE[eng: #e-tense, fr: #f-tense]. TAU-LEX-ITV &lt; TAU-LEX-V. TAU-LEX-V[eng: [subj: #e-subj], fr: [subj: #f-subj]] TAU-LEX[eng: #e-subj, fr: #f-subj]. TAU-LEX-FALL &lt; TAU-LEX-ITV. TAU-LEX-FALL[eng: [pred: FALL], fr: [pred: TOMBER]]</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>