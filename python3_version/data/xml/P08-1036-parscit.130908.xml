<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995752">
A Joint Model of Text and Aspect Ratings for Sentiment Summarization
</title>
<author confidence="0.98566">
Ivan Titov
</author>
<affiliation confidence="0.995296">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.803475">
Urbana, IL 61801
</address>
<email confidence="0.998811">
titov@uiuc.edu
</email>
<note confidence="0.905243333333333">
Ryan McDonald
Google Inc.
76 Ninth Avenue
</note>
<address confidence="0.892608">
New York, NY 10011
</address>
<email confidence="0.998048">
ryanmcd@google.com
</email>
<sectionHeader confidence="0.996728" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9966786">
Online reviews are often accompanied with
numerical ratings provided by users for a set
of service or product aspects. We propose
a statistical model which is able to discover
corresponding topics in text and extract tex-
tual evidence from reviews supporting each of
these aspect ratings – a fundamental problem
in aspect-based sentiment summarization (Hu
and Liu, 2004a). Our model achieves high ac-
curacy, without any explicitly labeled data ex-
cept the user provided opinion ratings. The
proposed approach is general and can be used
for segmentation in other applications where
sequential data is accompanied with corre-
lated signals.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995350666666667">
User generated content represents a unique source of
information in which user interface tools have facil-
itated the creation of an abundance of labeled con-
tent, e.g., topics in blogs, numerical product and ser-
vice ratings in user reviews, and helpfulness rank-
ings in online discussion forums. Many previous
studies on user generated content have attempted to
predict these labels automatically from the associ-
ated text. However, these labels are often present
in the data already, which opens another interesting
line of research: designing models leveraging these
labelings to improve a wide variety of applications.
In this study, we look at the problem of aspect-
based sentiment summarization (Hu and Liu, 2004a;
Popescu and Etzioni, 2005; Gamon et al., 2005;
</bodyText>
<note confidence="0.7294462">
Nikos’ Fine Dining
Food 4/5 “Best fish in the city”, “Excellent appetizers”
Decor 3/5 “Cozy with an old world feel”, “Too dark”
Service 1/5 “Our waitress was rude”, “Awful service”
Value 5/5 “Good Greek food for the $”, “Great price!”
</note>
<figureCaption confidence="0.998116">
Figure 1: An example aspect-based summary.
</figureCaption>
<bodyText confidence="0.997437791666667">
Carenini et al., 2006; Zhuang et al., 2006).1 An
aspect-based summarization system takes as input
a set of user reviews for a specific product or ser-
vice and produces a set of relevant aspects, the ag-
gregated sentiment for each aspect, and supporting
textual evidence. For example, figure 1 summarizes
a restaurant using aspects food, decor, service, and
value plus a numeric rating out of 5.
Standard aspect-based summarization consists of
two problems. The first is aspect identification and
mention extraction. Here the goal is to find the set
of relevant aspects for a rated entity and extract all
textual mentions that are associated with each. As-
pects can be fine-grained, e.g., fish, lamb, calamari,
or coarse-grained, e.g., food, decor, service. Sim-
ilarly, extracted text can range from a single word
to phrases and sentences. The second problem is
sentiment classification. Once all the relevant as-
pects and associated pieces of texts are extracted,
the system should aggregate sentiment over each as-
pect to provide the user with an average numeric or
symbolic rating. Sentiment classification is a well
studied problem (Wiebe, 2000; Pang et al., 2002;
Turney, 2002) and in many domains users explicitly
</bodyText>
<footnote confidence="0.841969">
1We use the term aspect to denote properties of an object
that can be rated by a user as in Snyder and Barzilay (2007).
Other studies use the term feature (Hu and Liu, 2004b).
</footnote>
<page confidence="0.935451">
308
</page>
<note confidence="0.94593285">
Proceedings of ACL-08: HLT, pages 308–316,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
�
Food: 5; Decor: 5; Service: 5; Value: 5
The chicken was great. On top of that our service was
excellent and the price was right. Can’t wait to go back!
Food: 2; Decor: 1; Service: 3; Value: 2
We went there for our anniversary. My soup was cold and
expensive plus it felt like they hadn’t painted since 1980.
Food: 3; Decor: 5; Service: 4; Value: 5
The food is only mediocre, but well worth the cost.
Wait staff was friendly. Lot’s of fun decorations.
Food “The chicken was great”, “My soup was
cold”, “The food is only mediocre”
Decor “it felt like they hadn’t painted since
1980”, “Lots of fun decorations”
Service “service was excellent”,
“Wait staff was friendly”
Value “the price was right”, “My soup was cold
and expensive”, “well worth the cost”
</note>
<figureCaption confidence="0.999551">
Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews.
</figureCaption>
<bodyText confidence="0.994822701754386">
provide ratings for each aspect making automated
means unnecessary.2 Aspect identification has also
been thoroughly studied (Hu and Liu, 2004b; Ga-
mon et al., 2005; Titov and McDonald, 2008), but
again, ontologies and users often provide this infor-
mation negating the need for automation.
Though it may be reasonable to expect a user to
provide a rating for each aspect, it is unlikely that
a user will annotate every sentence and phrase in a
review as being relevant to some aspect. Thus, it
can be argued that the most pressing challenge in
an aspect-based summarization system is to extract
all relevant mentions for each aspect, as illustrated
in figure 2. When labeled data exists, this prob-
lem can be solved effectively using a wide variety
of methods available for text classification and in-
formation extraction (Manning and Schutze, 1999).
However, labeled data is often hard to come by, es-
pecially when one considers all possible domains of
products and services. Instead, we propose an un-
supervised model that leverages aspect ratings that
frequently accompany an online review.
In order to construct such model, we make two
assumptions. First, ratable aspects normally repre-
sent coherent topics which can be potentially dis-
covered from co-occurrence information in the text.
Second, we hypothesize that the most predictive fea-
tures of an aspect rating are features derived from
the text segments discussing the corresponding as-
pect. Motivated by these observations, we construct
a joint statistical model of text and sentiment ratings.
The model is at heart a topic model in that it as-
signs words to a set of induced topics, each of which
may represent one particular aspect. The model is
extended through a set of maximum entropy classi-
fiers, one per each rated aspect, that are used to pre-
2E.g., http://zagat.com and http://tripadvisor.com.
dict the sentiment rating towards each of the aspects.
However, only the words assigned to an aspects cor-
responding topic are used in predicting the rating
for that aspect. As a result, the model enforces that
words assigned to an aspects’ topic are predictive of
the associated rating. Our approach is more general
than the particular statistical model we consider in
this paper. For example, other topic models can be
used as a part of our model and the proposed class of
models can be employed in other tasks beyond senti-
ment summarization, e.g., segmentation of blogs on
the basis of topic labels provided by users, or topic
discovery on the basis of tags given by users on so-
cial bookmarking sites.3
The rest of the paper is structured as follows. Sec-
tion 2 begins with a discussion of the joint text-
sentiment model approach. In Section 3 we provide
both a qualitative and quantitative evaluation of the
proposed method. We conclude in Section 4 with an
examination of related work.
</bodyText>
<sectionHeader confidence="0.997858" genericHeader="introduction">
2 The Model
</sectionHeader>
<bodyText confidence="0.9993764">
In this section we describe a new statistical model
called the Multi-Aspect Sentiment model (MAS),
which consists of two parts. The first part is based on
Multi-Grain Latent Dirichlet Allocation (Titov and
McDonald, 2008), which has been previously shown
to build topics that are representative of ratable as-
pects. The second part is a set of sentiment pre-
dictors per aspect that are designed to force specific
topics in the model to be directly correlated with a
particular aspect.
</bodyText>
<subsectionHeader confidence="0.928156">
2.1 Multi-Grain LDA
</subsectionHeader>
<bodyText confidence="0.995928333333333">
The Multi-Grain Latent Dirichlet Allocation model
(MG-LDA) is an extension of Latent Dirichlet Allo-
cation (LDA) (Blei et al., 2003). As was demon-
</bodyText>
<footnote confidence="0.958089">
3See e.g. del.ico.us (http://del.ico.us).
</footnote>
<page confidence="0.999205">
309
</page>
<bodyText confidence="0.9772795625">
strated in Titov and McDonald (2008), the topics
produced by LDA do not correspond to ratable as-
pects of entities. In particular, these models tend to
build topics that globally classify terms into product
instances (e.g., Creative Labs Mp3 players versus
iPods, or New York versus Paris Hotels). To com-
bat this, MG-LDA models two distinct types of top-
ics: global topics and local topics. As in LDA, the
distribution of global topics is fixed for a document
(a user review). However, the distribution of local
topics is allowed to vary across the document.
A word in the document is sampled either from
the mixture of global topics or from the mixture of
local topics specific to the local context of the word.
It was demonstrated in Titov and McDonald (2008)
that ratable aspects will be captured by local topics
and global topics will capture properties of reviewed
items. For example, consider an extract from a re-
view of a London hotel: “... public transport in Lon-
don is straightforward, the tube station is about an 8
minute walk ... or you can get a bus for £1.50”. It
can be viewed as a mixture of topic London shared
by the entire review (words: “London”, “tube”, “£”),
and the ratable aspect location, specific for the local
context of the sentence (words: “transport”, “walk”,
“bus”). Local topics are reused between very differ-
ent types of items, whereas global topics correspond
only to particular types of items.
In MG-LDA a document is represented as a set
of sliding windows, each covering T adjacent sen-
tences within a document.4 Each window v in docu-
ment d has an associated distribution over local top-
ics �loc
d,v and a distribution defining preference for lo-
cal topics versus global topics 7rd,v. A word can be
sampled using any window covering its sentence s,
where the window is chosen according to a categor-
ical distribution Od,s. Importantly, the fact that win-
dows overlap permits the model to exploit a larger
co-occurrence domain. These simple techniques are
capable of modeling local topics without more ex-
pensive modeling of topic transitions used in (Grif-
fiths et al., 2004; Wang and McCallum, 2005; Wal-
lach, 2006; Gruber et al., 2007). Introduction of a
symmetrical Dirichlet prior Dir(γ) for the distribu-
tion Od,s can control the smoothness of transitions.
4Our particular implementation is over sentences, but sliding
windows in theory can be over any sized fragment of text.
</bodyText>
<figure confidence="0.995714">
(a) (b)
</figure>
<figureCaption confidence="0.9927825">
Figure 3: (a) MG-LDA model. (b) An extension of MG-
LDA to obtain MAS.
</figureCaption>
<bodyText confidence="0.943428375">
The formal definition of the model with Kgl
global and Kloc local topics is as follows: First,
draw Kgl word distributions for global topics cogl
z
from a Dirichlet prior Dir(0gl) and Kloc word dis-
tributions for local topics coloc
z′ - from Dir(0loc).
Then, for each document d:
</bodyText>
<listItem confidence="0.951416727272727">
• Choose a distribution of global topics �gl
d ∼ Dir(agl).
• For each sentence s choose a distribution over sliding
windows &apos;Od,s(v) ∼ Dir(-y).
• For each sliding window v
– choose �loc
d,v ∼ Dir(aloc),
– choose ird,v ∼ Beta(amix).
• For each word i in sentence s of document d
– choose window vd,i ∼ ?Pd,s,
– choose rd,i ∼ ird,vd,i,
</listItem>
<bodyText confidence="0.858609333333333">
– if rd,i = gl choose global topic zd,i ∼ �gl
d ,
– if rd,i=loc choose local topic zd,i∼�loc
d,vd,i,
– choose word wd,i from the word distribution �rd,i
zd,i.
Beta(αmix) is a prior Beta distribution for choos-
ing between local and global topics. In Figure 3a the
corresponding graphical model is presented.
</bodyText>
<subsectionHeader confidence="0.998824">
2.2 Multi-Aspect Sentiment Model
</subsectionHeader>
<bodyText confidence="0.999965307692308">
MG-LDA constructs a set of topics that ideally cor-
respond to ratable aspects of an entity (often in a
many-to-one relationship of topics to aspects). A
major shortcoming of this model – and all other un-
supervised models – is that this correspondence is
not explicit, i.e., how does one say that topic X is re-
ally about aspect Y? However, we can observe that
numeric aspect ratings are often included in our data
by users who left the reviews. We then make the
assumption that the text of the review discussing an
aspect is predictive of its rating. Thus, if we model
the prediction of aspect ratings jointly with the con-
struction of explicitly associated topics, then such a
</bodyText>
<page confidence="0.987685">
310
</page>
<bodyText confidence="0.995405595505618">
model should benefit from both higher quality topics
and a direct assignment from topics to aspects. This
is the basic idea behind the Multi-Aspect Sentiment
model (MAS).
In its simplest form, MAS introduces a classifier
for each aspect, which is used to predict its rating.
Each classifier is explicitly associated to a single
topic in the model and only words assigned to that
topic can participate in the prediction of the senti-
ment rating for the aspect. However, it has been ob-
served that ratings for different aspects can be cor-
related (Snyder and Barzilay, 2007), e.g., very neg-
ative opinion about room cleanliness is likely to re-
sult not only in a low rating for the aspect rooms,
but also is very predictive of low ratings for the as-
pects service and dining. This complicates discovery
of the corresponding topics, as in many reviews the
most predictive features for an aspect rating might
correspond to another aspect. Another problem with
this overly simplistic model is the presence of opin-
ions about an item in general without referring to
any particular aspect. For example, “this product is
the worst I have ever purchased” is a good predic-
tor of low ratings for every aspect. In such cases,
non-aspect ‘background’ words will appear to be the
most predictive. Therefore, the use of the aspect sen-
timent classifiers based only on the words assigned
to the corresponding topics is problematic. Such a
model will not be able to discover coherent topics
associated with each aspect, because in many cases
the most predictive fragments for each aspect rating
will not be the ones where this aspect is discussed.
Our proposal is to estimate the distribution of pos-
sible values of an aspect rating on the basis of the
overall sentiment rating and to use the words as-
signed to the corresponding topic to compute cor-
rections for this aspect. An aspect rating is typically
correlated to the overall sentiment rating5 and the
fragments discussing this particular aspect will help
to correct the overall sentiment in the appropriate di-
rection. For example, if a review of a hotel is gen-
erally positive, but it includes a sentence “the neigh-
borhood is somewhat seedy” then this sentence is
predictive of rating for an aspect location being be-
low other ratings. This rectifies the aforementioned
5In the dataset used in our experiments all three aspect rat-
ings are equivalent for 5,250 reviews out of 10,000.
problems. First, aspect sentiment ratings can often
be regarded as conditionally independent given the
overall rating, therefore the model will not be forced
to include in an aspect topic any words from other
aspect topics. Secondly, the fragments discussing
overall opinion will influence the aspect rating only
through the overall sentiment rating. The overall
sentiment is almost always present in the real data
along with the aspect ratings, but it can be coarsely
discretized and we preferred to use a latent overall
sentiment.
The MAS model is presented in Figure 3b. Note
that for simplicity we decided to omit in the figure
the components of the MG-LDA model other than
variables r, z and w, though they are present in the
statistical model. MAS also allows for extra unasso-
ciated local topics in order to capture aspects not ex-
plicitly rated by the user. As in MG-LDA, MAS has
global topics which are expected to capture topics
corresponding to particular types of items, such Lon-
don hotels or seaside resorts for the hotel domain. In
figure 3b we shaded the aspect ratings ya, assuming
that every aspect rating is present in the data (though
in practice they might be available only for some re-
views). In this model the distribution of the overall
sentiment rating yo„ is based on all the n-gram fea-
tures of a review text. Then the distribution of ya, for
every rated aspect a, can be computed from the dis-
tribution of yo„ and from any n-gram feature where
at least one word in the n-gram is assigned to the
associated aspect topic (r = loc, z = a).
Instead of having a latent variable yo„,6 we use a
similar model which does not have an explicit no-
tion of yo„. The distribution of a sentiment rating ya
for each rated aspect a is computed from two scores.
The first score is computed on the basis of all the n-
grams, but using a common set of weights indepen-
dent of the aspect a. Another score is computed only
using n-grams associated with the related topic, but
an aspect-specific set of weights is used in this com-
putation. More formally, we consider the log-linear
distribution:
</bodyText>
<equation confidence="0.770063">
I:
P(ya = y|w, r, z)�exp(ba y+ Jf,y+pa f,r,zJa f,y), (1)
fEw
</equation>
<bodyText confidence="0.989521">
where w, r, z are vectors of all the words in a docu-
</bodyText>
<footnote confidence="0.8170865">
6Preliminary experiments suggested that this is also a feasi-
ble approach, but somewhat more computationally expensive.
</footnote>
<page confidence="0.997241">
311
</page>
<bodyText confidence="0.9995745">
ment, assignments of context (global or local) and
topics for all the words in the document, respec-
tively. bay is the bias term which regulates the prior
distribution P(ya = y), f iterates through all the
n-grams, Jy,f and Jay,f are common weights and
aspect-specific weights for n-gram feature f. pa f,r,z
is equal to a fraction of words in n-gram feature f
assigned to the aspect topic (r = loc, z = a).
</bodyText>
<subsectionHeader confidence="0.963932">
2.3 Inference in MAS
</subsectionHeader>
<bodyText confidence="0.989095684210526">
Exact inference in the MAS model is intractable.
Following Titov and McDonald (2008) we use a col-
lapsed Gibbs sampling algorithm that was derived
for the MG-LDA model based on the Gibbs sam-
pling method proposed for LDA in (Griffiths and
Steyvers, 2004). Gibbs sampling is an example of a
Markov Chain Monte Carlo algorithm (Geman and
Geman, 1984). It is used to produce a sample from
a joint distribution when only conditional distribu-
tions of each variable can be efficiently computed.
In Gibbs sampling, variables are sequentially sam-
pled from their distributions conditioned on all other
variables in the model. Such a chain of model states
converges to a sample from the joint distribution. A
naive application of this technique to LDA would
imply that both assignments of topics to words z
and distributions θ and ϕ should be sampled. How-
ever, (Griffiths and Steyvers, 2004) demonstrated
that an efficient collapsed Gibbs sampler can be con-
structed, where only assignments z need to be sam-
pled, whereas the dependency on distributions θ and
ϕ can be integrated out analytically.
In the case of MAS we also use maximum a-
posteriori estimates of the sentiment predictor pa-
rameters bay, Jy,f and Jay,f. The MAP estimates for
parameters bay, Jy,f and Jay,f are obtained by us-
ing stochastic gradient ascent. The direction of the
gradient is computed simultaneously with running a
chain by generating several assignments at each step
and averaging over the corresponding gradient esti-
mates. For details on computing gradients for log-
linear graphical models with Gibbs sampling we re-
fer the reader to (Neal, 1992).
Space constraints do not allow us to present either
the derivation or a detailed description of the sam-
pling algorithm. However, note that the conditional
distribution used in sampling decomposes into two
parts:
</bodyText>
<equation confidence="0.95277875">
P(vd,i = v, rd,i = r, zd,i = z|v’, r’, z’, w, y) a
ηd,i
v,r,z X ρd,i
r,z, (2)
</equation>
<bodyText confidence="0.9996434">
where v’, r’ and z’ are vectors of assignments of
sliding windows, context (global or local) and top-
ics for all the words in the collection except for the
considered word at position i in document d; y is the
vector of sentiment ratings. The first factor ηd,i
v,r,z is
responsible for modeling co-occurrences on the win-
dow and document level and coherence of the topics.
This factor is proportional to the conditional distri-
bution used in the Gibbs sampler of the MG-LDA
model (Titov and McDonald, 2008). The last fac-
tor quantifies the influence of the assignment of the
word (d, i) on the probability of the sentiment rat-
ings. It appears only if ratings are known (observ-
able) and equals:
</bodyText>
<equation confidence="0.992292666666667">
P(d
ya |w,r’, rd,i = r,z’, zd,i = z)
P (yd a|w, r’, z’, rd,i = gl) ,
</equation>
<bodyText confidence="0.999608666666667">
where the probability distribution is computed as de-
fined in expression (1), yda is the rating for the ath
aspect of review d.
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99995625">
In this section we present qualitative and quantita-
tive experiments. For the qualitative analysis we
show that topics inferred by the MAS model cor-
respond directly to the associated aspects. For the
quantitative analysis we show that the MAS model
induces a distribution over the rated aspects which
can be used to accurately predict whether a text frag-
ment is relevant to an aspect or not.
</bodyText>
<subsectionHeader confidence="0.998314">
3.1 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.9998735">
To perform qualitative experiments we used a set
of reviews of hotels taken from TripAdvisor.com7
that contained 10,000 reviews (109,024 sentences,
2,145,313 words in total). Every review was
rated with at least three aspects: service, location
and rooms. Each rating is an integer from 1 to 5.
The dataset was tokenized and sentence split auto-
matically.
</bodyText>
<note confidence="0.299024">
7(c) 2005-06, TripAdvisor, LLC All rights reserved
</note>
<figure confidence="0.636557666666667">
d,i = 11
ρr,z
a
</figure>
<page confidence="0.993588">
312
</page>
<table confidence="0.846512">
rated aspect top words
local service staff friendly helpful service desk concierge excellent extremely hotel great reception english pleasant help
topics location hotel walk location station metro walking away right minutes close bus city located just easy restaurants
rooms room bathroom shower bed tv small water clean comfortable towels bath nice large pillows space beds tub
- breakfast free coffee internet morning access buffet day wine nice lobby complimentary included good fruit
- $ night parking rate price paid day euros got cost pay hotel worth euro expensive car extra deal booked
- room noise night street air did door floor rooms open noisy window windows hear outside problem quiet sleep
global - moscow st russian petersburg nevsky russia palace hermitage kremlin prospect river prospekt kempinski
topics - paris tower french eiffel dame notre rue st louvre rer champs opera elysee george parisian du pantheon cafes
</table>
<tableCaption confidence="0.997671">
Table 1: Top words from MAS for hotel reviews.
</tableCaption>
<table confidence="0.9993598">
Krooms top words
2 rooms clean hotel room small nice comfortable modern good quite large lobby old decor spacious decorated bathroom size
room noise night street did air rooms door open noisy window floor hear windows problem outside quiet sleep bit light
3 room clean bed comfortable rooms bathroom small beds nice large size tv spacious good double big space huge king
room floor view rooms suite got views given quiet building small balcony upgraded nice high booked asked overlooking
room bathroom shower air water did like hot small towels door old window toilet conditioning open bath dirty wall tub
4 room clean rooms comfortable bed small beds nice bathroom size large modern spacious good double big quiet decorated
check arrived time day airport early room luggage took late morning got long flight ready minutes did taxi bags went
room noise night street did air rooms noisy open door hear windows window outside quiet sleep problem floor conditioning
bathroom room shower tv bed small water towels bath tub large nice toilet clean space toiletries flat wall sink screen
</table>
<tableCaption confidence="0.999663">
Table 2: Top words for aspect rooms with different number of topics Kroo,...
</tableCaption>
<bodyText confidence="0.999931888888889">
We ran the sampling chain for 700 iterations to
produce a sample. Distributions of words in each
topic were estimated as the proportion of words as-
signed to each topic, taking into account topic model
priors 0gl and 0loc. The sliding windows were cho-
sen to cover 3 sentences for all the experiments. All
the priors were chosen to be equal to 0.1. We used
15 local topics and 30 global topics. In the model,
the first three local topics were associated to the
rating classifiers for each aspects. As a result, we
would expect these topics to correspond to the ser-
vice, location, and rooms aspects respectively. Un-
igram and bigram features were used in the senti-
ment predictors in the MAS model. Before apply-
ing the topic models we removed punctuation and
also removed stop words using the standard list of
stop words,8 however, all the words and punctuation
were used in the sentiment predictors.
It does not take many chain iterations to discover
initial topics. This happens considerably faster than
the appropriate weights of the sentiment predictor
being learned. This poses a problem, because, in the
beginning, the sentiment predictors are not accurate
enough to force the model to discover appropriate
topics associated with each of the rated aspects. And
as soon as topic are formed, aspect sentiment predic-
tors cannot affect them anymore because they do not
</bodyText>
<footnote confidence="0.9388805">
8http://www.dcs.gla.ac.uk/idom/ir resources/linguistic utils/
stop words
</footnote>
<bodyText confidence="0.960370764705882">
have access to the true words associated with their
aspects. To combat this problem we first train the
sentiment classifiers by assuming that paf,r,z is equal
for all the local topics, which effectively ignores the
topic model. Then we use the estimated parame-
ters within the topic model.9 Secondly, we mod-
ify the sampling algorithm. The conditional prob-
ability used in sampling, expression (2), is propor-
tional to the product of two factors. The first factor,
d,i
�v,r,z, expresses a preference for topics likely from
the co-occurrence information, whereas the second
d,i
one, pr,z, favors the choice of topics which are pre-
dictive of the observable sentiment ratings. We used
(pd,i
r,z)1+0.sstq in the sampling distribution instead of
</bodyText>
<subsectionHeader confidence="0.610077">
d,i
</subsectionHeader>
<bodyText confidence="0.8774916">
pr,z, where t is the iteration number. q was chosen
to be 4, though the quality of the topics seemed to
be indistinguishable with any q between 3 and 10.
This can be thought of as having 1 + 0.95tq ratings
instead of a single vector assigned to each review,
i.e., focusing the model on prediction of the ratings
rather than finding the topic labels which are good at
explaining co-occurrences of words. These heuris-
tics influence sampling only during the first itera-
tions of the chain.
Top words for some of discovered local topics, in-
9Initial experiments suggested that instead of doing this
‘pre-training’ we could start with very large priors αloc and
αm , and then reduce them through the course of training.
However, this is significantly more computationally expensive.
</bodyText>
<page confidence="0.998416">
313
</page>
<figure confidence="0.999234285714286">
Precision
100
90
70
60
50
40
20
80
30
10
0
topic model
maxent classifier
Precision
100
90
70
60
50
40
1 topic
30 2 topics
3 topics
20 4 topics
maxent classifier
80
10
0
0 10 20 30 40 50 60 70 80 90 100
Recall
0 10 20 30 40 50 60 70 80 90 100
Recall
0 10 20 30 40 50 60 70 80 90 100
Recall
topic model
maxent classifier
Precision 100
90
80
70
60
50
40
30
20
10
0
(a) (b) (c)
</figure>
<figureCaption confidence="0.998517">
Figure 4: (a) Aspect service. (b) Aspect location. (c) Aspect rooms.
</figureCaption>
<bodyText confidence="0.999988102564102">
cluding the first 3 topics associated with the rated as-
pects, and also top words for some of global topics
are presented in Table 1. We can see that the model
discovered as its first three topics the correct associ-
ated aspects: service, location, and rooms. Other lo-
cal topics, as for the MG-LDA model, correspond to
other aspects discussed in reviews (breakfast, prices,
noise), and as it was previously shown in Titov and
McDonald (2008), aspects for global topics corre-
spond to the types of reviewed items (hotels in Rus-
sia, Paris hotels) or background words.
Notice though, that the 3rd local topic induced for
the rating rooms is slightly narrow. This can be ex-
plained by the fact that the aspect rooms is a central
aspect of hotel reviews. A very significant fraction
of text in every review can be thought of as a part of
the aspect rooms. These portions of reviews discuss
different coherent sub-aspects related to the aspect
rooms, e.g., the previously discovered topic noise.
Therefore, it is natural to associate several topics to
such central aspects. To test this we varied the num-
ber of topics associated with the sentiment predictor
for the aspect rooms. Top words for resulting top-
ics are presented in Table 2. It can be observed that
the topic model discovered appropriate topics while
the number of topics was below 4. With 4 topics
a semantically unrelated topic (check-in/arrival) is
induced. Manual selection of the number of topics
is undesirable, but this problem can be potentially
tackled with Dirichlet Process priors or a topic split
criterion based on the accuracy of the sentiment pre-
dictor in the MAS model. We found that both ser-
vice and location did not benefit by the assignment
of additional topics to their sentiment rating models.
The experimental results suggest that the MAS
model is reliable in the discovery of topics corre-
sponding to the rated aspects. In the next section
we will show that the induced topics can be used to
accurately extract fragments for each aspect.
</bodyText>
<subsectionHeader confidence="0.999829">
3.2 Sentence Labeling
</subsectionHeader>
<bodyText confidence="0.999872222222222">
A primary advantage of MAS over unsupervised
models, such as MG-LDA or clustering, is that top-
ics are linked to a rated aspect, i.e., we know ex-
actly which topics model which aspects. As a re-
sult, these topics can be directly used to extract tex-
tual mentions that are relevant for an aspect. To test
this, we hand labeled 779 random sentences from
the dataset considered in the previous set of experi-
ments. The sentences were labeled with one or more
aspects. Among them, 164, 176 and 263 sentences
were labeled as related to aspects service, location
and rooms, respectively. The remaining sentences
were not relevant to any of the rated aspects.
We compared two models. The first model uses
the first three topics of MAS to extract relevant men-
tions based on the probability of that topic/aspect be-
ing present in the sentence. To obtain these probabil-
ities we used estimators based on the proportion of
words in the sentence assigned to an aspects’ topic
and normalized within local topics. To improve the
reliability of the estimator we produced 100 sam-
ples for each document while keeping assignments
of the topics to all other words in the collection fixed.
The probability estimates were then obtained by av-
eraging over these samples. We did not perform
any model selection on the basis of the hand-labeled
data, and tested only a single model of each type.
</bodyText>
<page confidence="0.996943">
314
</page>
<bodyText confidence="0.999986103448276">
For the second model we trained a maximum en-
tropy classifier, one per each aspect, using 10-fold
cross validation and unigram/bigram features. Note
that this is a supervised system and as such repre-
sents an upper-bound in performance one might ex-
pect when comparing an unsupervised model such
as MAS. We chose this comparison to demonstrate
that our model can find relevant text mentions with
high accuracy relative to a supervised model. It is
difficult to compare our model to other unsupervised
systems such as MG-LDA or LDA. Again, this is
because those systems have no mechanism for di-
rectly correlating topics or clusters to corresponding
aspects, highlighting the benefit of MAS.
The resulting precision-recall curves for the as-
pects service, location and rooms are presented
in Figure 4. In Figure 4c, we varied the number
of topics associated with the aspect rooms.10 The
average precision we obtained (the standard mea-
sure proportional to the area under the curve) is
75.8%, 85.5% for aspects service and location, re-
spectively. For the aspect rooms these scores are
equal to 75.0%, 74.5%, 87.6%, 79.8% with 1–4 top-
ics per aspect, respectively. The logistic regression
models achieve 80.8%, 94.0% and 88.3% for the as-
pects service, location and rooms. We can observe
that the topic model, which does not use any explic-
itly aspect-labeled text, achieves accuracies lower
than, but comparable to a supervised model.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999950857142857">
There is a growing body of work on summariz-
ing sentiment by extracting and aggregating senti-
ment over ratable aspects and providing correspond-
ing textual evidence. Text excerpts are usually ex-
tracted through string matching (Hu and Liu, 2004a;
Popescu and Etzioni, 2005), sentence clustering
(Gamon et al., 2005), or through topic models (Mei
et al., 2007; Titov and McDonald, 2008). String ex-
traction methods are limited to fine-grained aspects
whereas clustering and topic model approaches must
resort to ad-hoc means of labeling clusters or topics.
However, this is the first work we are aware of that
uses a pre-defined set of aspects plus an associated
signal to learn a mapping from text to an aspect for
</bodyText>
<footnote confidence="0.519387">
10To improve readability we smoothed the curve for the as-
pect rooms.
</footnote>
<bodyText confidence="0.999011222222222">
the purpose of extraction.
A closely related model to ours is that of Mei et
al. (2007) which performs joint topic and sentiment
modeling of collections. Our model differs from
theirs in many respects: Mei et al. only model senti-
ment predictions for the entire document and not on
the aspect level; They treat sentiment predictions as
unobserved variables, whereas we treat them as ob-
served signals that help to guide the creation of top-
ics; They model co-occurrences solely on the docu-
ment level, whereas our model is based on MG-LDA
and models both local and global contexts.
Recently, Blei and McAuliffe (2008) proposed an
approach for joint sentiment and topic modeling that
can be viewed as a supervised LDA (sLDA) model
that tries to infer topics appropriate for use in a
given classification or regression problem. MAS and
sLDA are similar in that both use sentiment predic-
tions as an observed signal that is predicted by the
model. However, Blei et al. do not consider multi-
aspect ranking or look at co-occurrences beyond the
document level, both of which are central to our
model. Parallel to this study Branavan et al. (2008)
also showed that joint models of text and user anno-
tations benefit extractive summarization. In partic-
ular, they used signals from pros-cons lists whereas
our models use aspect rating signals.
</bodyText>
<sectionHeader confidence="0.999811" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.997495166666667">
In this paper we presented a joint model of text and
aspect ratings for extracting text to be displayed in
sentiment summaries. The model uses aspect ratings
to discover the corresponding topics and can thus ex-
tract fragments of text discussing these aspects with-
out the need of annotated data. We demonstrated
that the model indeed discovers corresponding co-
herent topics and achieves accuracy in sentence la-
beling comparable to a standard supervised model.
The primary area of future work is to incorporate the
model into an end-to-end sentiment summarization
system in order to evaluate it at that level.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.789139">
This work benefited from discussions with Sasha
Blair-Goldensohn and Fernando Pereira.
</bodyText>
<page confidence="0.999134">
315
</page>
<sectionHeader confidence="0.998312" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998330634146341">
David M. Blei and Jon D. McAuliffe. 2008. Supervised
topic models. In Advances in Neural Information Pro-
cessing Systems (NIPS).
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine Learning Re-
search, 3(5):993–1022.
S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzi-
lay. 2008. Learning document-level semantic proper-
ties from free-text annotations. In Proceedings of the
Annual Conference of the Association for Computa-
tional Linguistics.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-Document
Summarization of Evaluative Text. In Proceedings of
the Conference of the European Chapter of the Asso-
ciation for Computational Linguistics.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proc. of the 6th International Symposium on Intelli-
gent Data Analysis, pages 121–132.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721–741.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the Natural Academy of
Sciences, 101 Suppl 1:5228–5235.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B. Tenen-
baum. 2004. Integrating topics and syntax. In Ad-
vances in Neural Information Processing Systems.
A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden
Topic Markov Models. In Proceedings of the Confer-
ence on Artificial Intelligence and Statistics.
M. Hu and B. Liu. 2004a. Mining and summarizing
customer reviews. In Proceedings of the 2004 ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168–177. ACM Press
New York, NY, USA.
M. Hu and B. Liu. 2004b. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteenth
National Conference on Artificial Intellgience.
C. Manning and M. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th Interna-
tional Conference on World Wide Web, pages 171–180.
Radford Neal. 1992. Connectionist learning of belief
networks. Artificial Intelligence, 56:71–113.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
A.M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
B. Snyder and R. Barzilay. 2007. Multiple Aspect Rank-
ing using the Good Grief Algorithm. In Proceedings
of the Joint Conference of the North American Chapter
of the Association for Computational Linguistics and
Human Language Technologies, pages 300–307.
I. Titov and R. McDonald. 2008. Modeling online re-
views with multi-grain topic models. In Proceedings
of the 17h International Conference on World Wide
Web.
P. Turney. 2002. Thumbs up or thumbs down? Senti-
ment orientation applied to unsupervised classification
of reviews. In Proceedings of the Annual Conference
of the Association for Computational Linguistics.
Hanna M. Wallach. 2006. Topic modeling; beyond bag
of words. In International Conference on Machine
Learning.
Xuerui Wang and Andrew McCallum. 2005. A note on
topical n-grams. Technical Report UM-CS-2005-071,
University of Massachusetts.
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In Proceedings of the National Conference on
Artificial Intelligence.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management (CIKM), pages 43–50.
</reference>
<page confidence="0.999229">
316
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946342">
<title confidence="0.999983">A Joint Model of Text and Aspect Ratings for Sentiment Summarization</title>
<author confidence="0.999863">Ivan Titov</author>
<affiliation confidence="0.9999395">Department of Computer Science University of Illinois at Urbana-Champaign</affiliation>
<address confidence="0.988546">Urbana, IL 61801</address>
<email confidence="0.999599">titov@uiuc.edu</email>
<author confidence="0.999957">Ryan McDonald</author>
<affiliation confidence="0.999894">Google Inc.</affiliation>
<address confidence="0.9992925">76 Ninth Avenue New York, NY 10011</address>
<email confidence="0.999673">ryanmcd@google.com</email>
<abstract confidence="0.9974631875">Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Jon D McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="32580" citStr="Blei and McAuliffe (2008)" startWordPosition="5509" endWordPosition="5512">pect rooms. the purpose of extraction. A closely related model to ours is that of Mei et al. (2007) which performs joint topic and sentiment modeling of collections. Our model differs from theirs in many respects: Mei et al. only model sentiment predictions for the entire document and not on the aspect level; They treat sentiment predictions as unobserved variables, whereas we treat them as observed signals that help to guide the creation of topics; They model co-occurrences solely on the document level, whereas our model is based on MG-LDA and models both local and global contexts. Recently, Blei and McAuliffe (2008) proposed an approach for joint sentiment and topic modeling that can be viewed as a supervised LDA (sLDA) model that tries to infer topics appropriate for use in a given classification or regression problem. MAS and sLDA are similar in that both use sentiment predictions as an observed signal that is predicted by the model. However, Blei et al. do not consider multiaspect ranking or look at co-occurrences beyond the document level, both of which are central to our model. Parallel to this study Branavan et al. (2008) also showed that joint models of text and user annotations benefit extractive</context>
</contexts>
<marker>Blei, McAuliffe, 2008</marker>
<rawString>David M. Blei and Jon D. McAuliffe. 2008. Supervised topic models. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>5</issue>
<contexts>
<context position="7815" citStr="Blei et al., 2003" startWordPosition="1275" endWordPosition="1278">we describe a new statistical model called the Multi-Aspect Sentiment model (MAS), which consists of two parts. The first part is based on Multi-Grain Latent Dirichlet Allocation (Titov and McDonald, 2008), which has been previously shown to build topics that are representative of ratable aspects. The second part is a set of sentiment predictors per aspect that are designed to force specific topics in the model to be directly correlated with a particular aspect. 2.1 Multi-Grain LDA The Multi-Grain Latent Dirichlet Allocation model (MG-LDA) is an extension of Latent Dirichlet Allocation (LDA) (Blei et al., 2003). As was demon3See e.g. del.ico.us (http://del.ico.us). 309 strated in Titov and McDonald (2008), the topics produced by LDA do not correspond to ratable aspects of entities. In particular, these models tend to build topics that globally classify terms into product instances (e.g., Creative Labs Mp3 players versus iPods, or New York versus Paris Hotels). To combat this, MG-LDA models two distinct types of topics: global topics and local topics. As in LDA, the distribution of global topics is fixed for a document (a user review). However, the distribution of local topics is allowed to vary acro</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3(5):993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>H Chen</author>
<author>J Eisenstein</author>
<author>R Barzilay</author>
</authors>
<title>Learning document-level semantic properties from free-text annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33102" citStr="Branavan et al. (2008)" startWordPosition="5599" endWordPosition="5602">el is based on MG-LDA and models both local and global contexts. Recently, Blei and McAuliffe (2008) proposed an approach for joint sentiment and topic modeling that can be viewed as a supervised LDA (sLDA) model that tries to infer topics appropriate for use in a given classification or regression problem. MAS and sLDA are similar in that both use sentiment predictions as an observed signal that is predicted by the model. However, Blei et al. do not consider multiaspect ranking or look at co-occurrences beyond the document level, both of which are central to our model. Parallel to this study Branavan et al. (2008) also showed that joint models of text and user annotations benefit extractive summarization. In particular, they used signals from pros-cons lists whereas our models use aspect rating signals. 5 Conclusions In this paper we presented a joint model of text and aspect ratings for extracting text to be displayed in sentiment summaries. The model uses aspect ratings to discover the corresponding topics and can thus extract fragments of text discussing these aspects without the need of annotated data. We demonstrated that the model indeed discovers corresponding coherent topics and achieves accura</context>
</contexts>
<marker>Branavan, Chen, Eisenstein, Barzilay, 2008</marker>
<rawString>S.R.K. Branavan, H. Chen, J. Eisenstein, and R. Barzilay. 2008. Learning document-level semantic properties from free-text annotations. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>R Ng</author>
<author>A Pauls</author>
</authors>
<title>Multi-Document Summarization of Evaluative Text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1985" citStr="Carenini et al., 2006" startWordPosition="306" endWordPosition="309">s are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Nikos’ Fine Dining Food 4/5 “Best fish in the city”, “Excellent appetizers” Decor 3/5 “Cozy with an old world feel”, “Too dark” Service 1/5 “Our waitress was rude”, “Awful service” Value 5/5 “Good Greek food for the $”, “Great price!” Figure 1: An example aspect-based summary. Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5. Standard aspect-based summarization consists of two problems. The first is aspect identification and mention extraction. Here the goal is to find the set of relevant aspects for a rated entity and extract all textual mentions th</context>
</contexts>
<marker>Carenini, Ng, Pauls, 2006</marker>
<rawString>G. Carenini, R. Ng, and A. Pauls. 2006. Multi-Document Summarization of Evaluative Text. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>A Aue</author>
<author>S Corston-Oliver</author>
<author>E Ringger</author>
</authors>
<title>Pulse: Mining customer opinions from free text.</title>
<date>2005</date>
<booktitle>In Proc. of the 6th International Symposium on Intelligent Data Analysis,</booktitle>
<pages>121--132</pages>
<contexts>
<context position="1684" citStr="Gamon et al., 2005" startWordPosition="256" endWordPosition="259">f labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on user generated content have attempted to predict these labels automatically from the associated text. However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Nikos’ Fine Dining Food 4/5 “Best fish in the city”, “Excellent appetizers” Decor 3/5 “Cozy with an old world feel”, “Too dark” Service 1/5 “Our waitress was rude”, “Awful service” Value 5/5 “Good Greek food for the $”, “Great price!” Figure 1: An example aspect-based summary. Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarizes a restaurant using</context>
<context position="4490" citStr="Gamon et al., 2005" startWordPosition="722" endWordPosition="726"> the cost. Wait staff was friendly. Lot’s of fun decorations. Food “The chicken was great”, “My soup was cold”, “The food is only mediocre” Decor “it felt like they hadn’t painted since 1980”, “Lots of fun decorations” Service “service was excellent”, “Wait staff was friendly” Value “the price was right”, “My soup was cold and expensive”, “well worth the cost” Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews. provide ratings for each aspect making automated means unnecessary.2 Aspect identification has also been thoroughly studied (Hu and Liu, 2004b; Gamon et al., 2005; Titov and McDonald, 2008), but again, ontologies and users often provide this information negating the need for automation. Though it may be reasonable to expect a user to provide a rating for each aspect, it is unlikely that a user will annotate every sentence and phrase in a review as being relevant to some aspect. Thus, it can be argued that the most pressing challenge in an aspect-based summarization system is to extract all relevant mentions for each aspect, as illustrated in figure 2. When labeled data exists, this problem can be solved effectively using a wide variety of methods avail</context>
<context position="31499" citStr="Gamon et al., 2005" startWordPosition="5325" endWordPosition="5328"> aspect, respectively. The logistic regression models achieve 80.8%, 94.0% and 88.3% for the aspects service, location and rooms. We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model. 4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence. Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008). String extraction methods are limited to fine-grained aspects whereas clustering and topic model approaches must resort to ad-hoc means of labeling clusters or topics. However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms. the purpose of extraction. A closely related model to ours is that of Mei et al. (2007) which performs joint topic and sentiment mod</context>
</contexts>
<marker>Gamon, Aue, Corston-Oliver, Ringger, 2005</marker>
<rawString>M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger. 2005. Pulse: Mining customer opinions from free text. In Proc. of the 6th International Symposium on Intelligent Data Analysis, pages 121–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="17504" citStr="Geman and Geman, 1984" startWordPosition="2956" endWordPosition="2959">or distribution P(ya = y), f iterates through all the n-grams, Jy,f and Jay,f are common weights and aspect-specific weights for n-gram feature f. pa f,r,z is equal to a fraction of words in n-gram feature f assigned to the aspect topic (r = loc, z = a). 2.3 Inference in MAS Exact inference in the MAS model is intractable. Following Titov and McDonald (2008) we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in (Griffiths and Steyvers, 2004). Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm (Geman and Geman, 1984). It is used to produce a sample from a joint distribution when only conditional distributions of each variable can be efficiently computed. In Gibbs sampling, variables are sequentially sampled from their distributions conditioned on all other variables in the model. Such a chain of model states converges to a sample from the joint distribution. A naive application of this technique to LDA would imply that both assignments of topics to words z and distributions θ and ϕ should be sampled. However, (Griffiths and Steyvers, 2004) demonstrated that an efficient collapsed Gibbs sampler can be cons</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the Natural Academy of Sciences, 101 Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="17410" citStr="Griffiths and Steyvers, 2004" startWordPosition="2940" endWordPosition="2943"> topics for all the words in the document, respectively. bay is the bias term which regulates the prior distribution P(ya = y), f iterates through all the n-grams, Jy,f and Jay,f are common weights and aspect-specific weights for n-gram feature f. pa f,r,z is equal to a fraction of words in n-gram feature f assigned to the aspect topic (r = loc, z = a). 2.3 Inference in MAS Exact inference in the MAS model is intractable. Following Titov and McDonald (2008) we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in (Griffiths and Steyvers, 2004). Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm (Geman and Geman, 1984). It is used to produce a sample from a joint distribution when only conditional distributions of each variable can be efficiently computed. In Gibbs sampling, variables are sequentially sampled from their distributions conditioned on all other variables in the model. Such a chain of model states converges to a sample from the joint distribution. A naive application of this technique to LDA would imply that both assignments of topics to words z and distributions θ and ϕ should be sampled. However, (Gr</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the Natural Academy of Sciences, 101 Suppl 1:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="9977" citStr="Griffiths et al., 2004" startWordPosition="1642" endWordPosition="1646">ng windows, each covering T adjacent sentences within a document.4 Each window v in document d has an associated distribution over local topics �loc d,v and a distribution defining preference for local topics versus global topics 7rd,v. A word can be sampled using any window covering its sentence s, where the window is chosen according to a categorical distribution Od,s. Importantly, the fact that windows overlap permits the model to exploit a larger co-occurrence domain. These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in (Griffiths et al., 2004; Wang and McCallum, 2005; Wallach, 2006; Gruber et al., 2007). Introduction of a symmetrical Dirichlet prior Dir(γ) for the distribution Od,s can control the smoothness of transitions. 4Our particular implementation is over sentences, but sliding windows in theory can be over any sized fragment of text. (a) (b) Figure 3: (a) MG-LDA model. (b) An extension of MGLDA to obtain MAS. The formal definition of the model with Kgl global and Kloc local topics is as follows: First, draw Kgl word distributions for global topics cogl z from a Dirichlet prior Dir(0gl) and Kloc word distributions for local</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2004</marker>
<rawString>T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B. Tenenbaum. 2004. Integrating topics and syntax. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruber</author>
<author>Y Weiss</author>
<author>M Rosen-Zvi</author>
</authors>
<title>Hidden Topic Markov Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="10039" citStr="Gruber et al., 2007" startWordPosition="1654" endWordPosition="1657">.4 Each window v in document d has an associated distribution over local topics �loc d,v and a distribution defining preference for local topics versus global topics 7rd,v. A word can be sampled using any window covering its sentence s, where the window is chosen according to a categorical distribution Od,s. Importantly, the fact that windows overlap permits the model to exploit a larger co-occurrence domain. These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in (Griffiths et al., 2004; Wang and McCallum, 2005; Wallach, 2006; Gruber et al., 2007). Introduction of a symmetrical Dirichlet prior Dir(γ) for the distribution Od,s can control the smoothness of transitions. 4Our particular implementation is over sentences, but sliding windows in theory can be over any sized fragment of text. (a) (b) Figure 3: (a) MG-LDA model. (b) An extension of MGLDA to obtain MAS. The formal definition of the model with Kgl global and Kloc local topics is as follows: First, draw Kgl word distributions for global topics cogl z from a Dirichlet prior Dir(0gl) and Kloc word distributions for local topics coloc z′ - from Dir(0loc). Then, for each document d: </context>
</contexts>
<marker>Gruber, Weiss, Rosen-Zvi, 2007</marker>
<rawString>A. Gruber, Y. Weiss, and M. Rosen-Zvi. 2007. Hidden Topic Markov Models. In Proceedings of the Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM Press</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="643" citStr="Hu and Liu, 2004" startWordPosition="93" endWordPosition="96">spect Ratings for Sentiment Summarization Ivan Titov Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 titov@uiuc.edu Ryan McDonald Google Inc. 76 Ninth Avenue New York, NY 10011 ryanmcd@google.com Abstract Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals. 1 Introduction User generated content represents a unique source of information in which user interface tools have facilitated the creation of an abundance of labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on u</context>
<context position="3349" citStr="Hu and Liu, 2004" startWordPosition="531" endWordPosition="534">acted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics � Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensive plus it felt like they hadn’t painted since 1980. Food: 3; Decor: 5; Service: 4; Value: 5 The food is only mediocre, but well worth the cost. Wait staff was friendly. Lot’s of fun decorations. Food “The chick</context>
<context position="31428" citStr="Hu and Liu, 2004" startWordPosition="5315" endWordPosition="5318">e scores are equal to 75.0%, 74.5%, 87.6%, 79.8% with 1–4 topics per aspect, respectively. The logistic regression models achieve 80.8%, 94.0% and 88.3% for the aspects service, location and rooms. We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model. 4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence. Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008). String extraction methods are limited to fine-grained aspects whereas clustering and topic model approaches must resort to ad-hoc means of labeling clusters or topics. However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms. the purpose of extraction. A closely related model to ours is</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004a. Mining and summarizing customer reviews. In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM Press New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining Opinion Features in Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of Nineteenth National Conference on Artificial Intellgience.</booktitle>
<contexts>
<context position="643" citStr="Hu and Liu, 2004" startWordPosition="93" endWordPosition="96">spect Ratings for Sentiment Summarization Ivan Titov Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 titov@uiuc.edu Ryan McDonald Google Inc. 76 Ninth Avenue New York, NY 10011 ryanmcd@google.com Abstract Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings – a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals. 1 Introduction User generated content represents a unique source of information in which user interface tools have facilitated the creation of an abundance of labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on u</context>
<context position="3349" citStr="Hu and Liu, 2004" startWordPosition="531" endWordPosition="534">acted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics � Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensive plus it felt like they hadn’t painted since 1980. Food: 3; Decor: 5; Service: 4; Value: 5 The food is only mediocre, but well worth the cost. Wait staff was friendly. Lot’s of fun decorations. Food “The chick</context>
<context position="31428" citStr="Hu and Liu, 2004" startWordPosition="5315" endWordPosition="5318">e scores are equal to 75.0%, 74.5%, 87.6%, 79.8% with 1–4 topics per aspect, respectively. The logistic regression models achieve 80.8%, 94.0% and 88.3% for the aspects service, location and rooms. We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model. 4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence. Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008). String extraction methods are limited to fine-grained aspects whereas clustering and topic model approaches must resort to ad-hoc means of labeling clusters or topics. However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms. the purpose of extraction. A closely related model to ours is</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004b. Mining Opinion Features in Customer Reviews. In Proceedings of Nineteenth National Conference on Artificial Intellgience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>M Schutze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5173" citStr="Manning and Schutze, 1999" startWordPosition="837" endWordPosition="840">ers often provide this information negating the need for automation. Though it may be reasonable to expect a user to provide a rating for each aspect, it is unlikely that a user will annotate every sentence and phrase in a review as being relevant to some aspect. Thus, it can be argued that the most pressing challenge in an aspect-based summarization system is to extract all relevant mentions for each aspect, as illustrated in figure 2. When labeled data exists, this problem can be solved effectively using a wide variety of methods available for text classification and information extraction (Manning and Schutze, 1999). However, labeled data is often hard to come by, especially when one considers all possible domains of products and services. Instead, we propose an unsupervised model that leverages aspect ratings that frequently accompany an online review. In order to construct such model, we make two assumptions. First, ratable aspects normally represent coherent topics which can be potentially discovered from co-occurrence information in the text. Second, we hypothesize that the most predictive features of an aspect rating are features derived from the text segments discussing the corresponding aspect. Mo</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and M. Schutze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Mei</author>
<author>X Ling</author>
<author>M Wondra</author>
<author>H Su</author>
<author>C X Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="31542" citStr="Mei et al., 2007" startWordPosition="5333" endWordPosition="5336"> models achieve 80.8%, 94.0% and 88.3% for the aspects service, location and rooms. We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model. 4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence. Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008). String extraction methods are limited to fine-grained aspects whereas clustering and topic model approaches must resort to ad-hoc means of labeling clusters or topics. However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms. the purpose of extraction. A closely related model to ours is that of Mei et al. (2007) which performs joint topic and sentiment modeling of collections. Our model differs fro</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of the 16th International Conference on World Wide Web, pages 171–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Connectionist learning of belief networks.</title>
<date>1992</date>
<journal>Artificial Intelligence,</journal>
<pages>56--71</pages>
<contexts>
<context position="18768" citStr="Neal, 1992" startWordPosition="3166" endWordPosition="3167"> whereas the dependency on distributions θ and ϕ can be integrated out analytically. In the case of MAS we also use maximum aposteriori estimates of the sentiment predictor parameters bay, Jy,f and Jay,f. The MAP estimates for parameters bay, Jy,f and Jay,f are obtained by using stochastic gradient ascent. The direction of the gradient is computed simultaneously with running a chain by generating several assignments at each step and averaging over the corresponding gradient estimates. For details on computing gradients for loglinear graphical models with Gibbs sampling we refer the reader to (Neal, 1992). Space constraints do not allow us to present either the derivation or a detailed description of the sampling algorithm. However, note that the conditional distribution used in sampling decomposes into two parts: P(vd,i = v, rd,i = r, zd,i = z|v’, r’, z’, w, y) a ηd,i v,r,z X ρd,i r,z, (2) where v’, r’ and z’ are vectors of assignments of sliding windows, context (global or local) and topics for all the words in the collection except for the considered word at position i in document d; y is the vector of sentiment ratings. The first factor ηd,i v,r,z is responsible for modeling co-occurrences</context>
</contexts>
<marker>Neal, 1992</marker>
<rawString>Radford Neal. 1992. Connectionist learning of belief networks. Artificial Intelligence, 56:71–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3124" citStr="Pang et al., 2002" startWordPosition="489" endWordPosition="492">set of relevant aspects for a rated entity and extract all textual mentions that are associated with each. Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics � Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was co</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1664" citStr="Popescu and Etzioni, 2005" startWordPosition="252" endWordPosition="255"> creation of an abundance of labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on user generated content have attempted to predict these labels automatically from the associated text. However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Nikos’ Fine Dining Food 4/5 “Best fish in the city”, “Excellent appetizers” Decor 3/5 “Cozy with an old world feel”, “Too dark” Service 1/5 “Our waitress was rude”, “Awful service” Value 5/5 “Good Greek food for the $”, “Great price!” Figure 1: An example aspect-based summary. Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarize</context>
<context position="31457" citStr="Popescu and Etzioni, 2005" startWordPosition="5319" endWordPosition="5322">to 75.0%, 74.5%, 87.6%, 79.8% with 1–4 topics per aspect, respectively. The logistic regression models achieve 80.8%, 94.0% and 88.3% for the aspects service, location and rooms. We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model. 4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence. Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008). String extraction methods are limited to fine-grained aspects whereas clustering and topic model approaches must resort to ad-hoc means of labeling clusters or topics. However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms. the purpose of extraction. A closely related model to ours is that of Mei et al. (2007) wh</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A.M. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Multiple Aspect Ranking using the Good Grief Algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies,</booktitle>
<pages>300--307</pages>
<contexts>
<context position="3295" citStr="Snyder and Barzilay (2007)" startWordPosition="521" endWordPosition="524">, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics � Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensive plus it felt like they hadn’t painted since 1980. Food: 3; Decor: 5; Service: 4; Value: 5 The food is only mediocre, but well worth the cost. Wait staff w</context>
<context position="12560" citStr="Snyder and Barzilay, 2007" startWordPosition="2095" endWordPosition="2098">ruction of explicitly associated topics, then such a 310 model should benefit from both higher quality topics and a direct assignment from topics to aspects. This is the basic idea behind the Multi-Aspect Sentiment model (MAS). In its simplest form, MAS introduces a classifier for each aspect, which is used to predict its rating. Each classifier is explicitly associated to a single topic in the model and only words assigned to that topic can participate in the prediction of the sentiment rating for the aspect. However, it has been observed that ratings for different aspects can be correlated (Snyder and Barzilay, 2007), e.g., very negative opinion about room cleanliness is likely to result not only in a low rating for the aspect rooms, but also is very predictive of low ratings for the aspects service and dining. This complicates discovery of the corresponding topics, as in many reviews the most predictive features for an aspect rating might correspond to another aspect. Another problem with this overly simplistic model is the presence of opinions about an item in general without referring to any particular aspect. For example, “this product is the worst I have ever purchased” is a good predictor of low rat</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>B. Snyder and R. Barzilay. 2007. Multiple Aspect Ranking using the Good Grief Algorithm. In Proceedings of the Joint Conference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies, pages 300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>R McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17h International Conference on World Wide Web.</booktitle>
<contexts>
<context position="4517" citStr="Titov and McDonald, 2008" startWordPosition="727" endWordPosition="730">f was friendly. Lot’s of fun decorations. Food “The chicken was great”, “My soup was cold”, “The food is only mediocre” Decor “it felt like they hadn’t painted since 1980”, “Lots of fun decorations” Service “service was excellent”, “Wait staff was friendly” Value “the price was right”, “My soup was cold and expensive”, “well worth the cost” Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews. provide ratings for each aspect making automated means unnecessary.2 Aspect identification has also been thoroughly studied (Hu and Liu, 2004b; Gamon et al., 2005; Titov and McDonald, 2008), but again, ontologies and users often provide this information negating the need for automation. Though it may be reasonable to expect a user to provide a rating for each aspect, it is unlikely that a user will annotate every sentence and phrase in a review as being relevant to some aspect. Thus, it can be argued that the most pressing challenge in an aspect-based summarization system is to extract all relevant mentions for each aspect, as illustrated in figure 2. When labeled data exists, this problem can be solved effectively using a wide variety of methods available for text classificatio</context>
<context position="7402" citStr="Titov and McDonald, 2008" startWordPosition="1207" endWordPosition="1210">bels provided by users, or topic discovery on the basis of tags given by users on social bookmarking sites.3 The rest of the paper is structured as follows. Section 2 begins with a discussion of the joint textsentiment model approach. In Section 3 we provide both a qualitative and quantitative evaluation of the proposed method. We conclude in Section 4 with an examination of related work. 2 The Model In this section we describe a new statistical model called the Multi-Aspect Sentiment model (MAS), which consists of two parts. The first part is based on Multi-Grain Latent Dirichlet Allocation (Titov and McDonald, 2008), which has been previously shown to build topics that are representative of ratable aspects. The second part is a set of sentiment predictors per aspect that are designed to force specific topics in the model to be directly correlated with a particular aspect. 2.1 Multi-Grain LDA The Multi-Grain Latent Dirichlet Allocation model (MG-LDA) is an extension of Latent Dirichlet Allocation (LDA) (Blei et al., 2003). As was demon3See e.g. del.ico.us (http://del.ico.us). 309 strated in Titov and McDonald (2008), the topics produced by LDA do not correspond to ratable aspects of entities. In particula</context>
<context position="8634" citStr="Titov and McDonald (2008)" startWordPosition="1414" endWordPosition="1417">e models tend to build topics that globally classify terms into product instances (e.g., Creative Labs Mp3 players versus iPods, or New York versus Paris Hotels). To combat this, MG-LDA models two distinct types of topics: global topics and local topics. As in LDA, the distribution of global topics is fixed for a document (a user review). However, the distribution of local topics is allowed to vary across the document. A word in the document is sampled either from the mixture of global topics or from the mixture of local topics specific to the local context of the word. It was demonstrated in Titov and McDonald (2008) that ratable aspects will be captured by local topics and global topics will capture properties of reviewed items. For example, consider an extract from a review of a London hotel: “... public transport in London is straightforward, the tube station is about an 8 minute walk ... or you can get a bus for £1.50”. It can be viewed as a mixture of topic London shared by the entire review (words: “London”, “tube”, “£”), and the ratable aspect location, specific for the local context of the sentence (words: “transport”, “walk”, “bus”). Local topics are reused between very different types of items, </context>
<context position="17242" citStr="Titov and McDonald (2008)" startWordPosition="2910" endWordPosition="2913">ary experiments suggested that this is also a feasible approach, but somewhat more computationally expensive. 311 ment, assignments of context (global or local) and topics for all the words in the document, respectively. bay is the bias term which regulates the prior distribution P(ya = y), f iterates through all the n-grams, Jy,f and Jay,f are common weights and aspect-specific weights for n-gram feature f. pa f,r,z is equal to a fraction of words in n-gram feature f assigned to the aspect topic (r = loc, z = a). 2.3 Inference in MAS Exact inference in the MAS model is intractable. Following Titov and McDonald (2008) we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in (Griffiths and Steyvers, 2004). Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm (Geman and Geman, 1984). It is used to produce a sample from a joint distribution when only conditional distributions of each variable can be efficiently computed. In Gibbs sampling, variables are sequentially sampled from their distributions conditioned on all other variables in the model. Such a chain of model states converges to a sample from the joint dis</context>
<context position="19563" citStr="Titov and McDonald, 2008" startWordPosition="3305" endWordPosition="3308">used in sampling decomposes into two parts: P(vd,i = v, rd,i = r, zd,i = z|v’, r’, z’, w, y) a ηd,i v,r,z X ρd,i r,z, (2) where v’, r’ and z’ are vectors of assignments of sliding windows, context (global or local) and topics for all the words in the collection except for the considered word at position i in document d; y is the vector of sentiment ratings. The first factor ηd,i v,r,z is responsible for modeling co-occurrences on the window and document level and coherence of the topics. This factor is proportional to the conditional distribution used in the Gibbs sampler of the MG-LDA model (Titov and McDonald, 2008). The last factor quantifies the influence of the assignment of the word (d, i) on the probability of the sentiment ratings. It appears only if ratings are known (observable) and equals: P(d ya |w,r’, rd,i = r,z’, zd,i = z) P (yd a|w, r’, z’, rd,i = gl) , where the probability distribution is computed as defined in expression (1), yda is the rating for the ath aspect of review d. 3 Experiments In this section we present qualitative and quantitative experiments. For the qualitative analysis we show that topics inferred by the MAS model correspond directly to the associated aspects. For the quan</context>
<context position="26785" citStr="Titov and McDonald (2008)" startWordPosition="4534" endWordPosition="4537">30 40 50 60 70 80 90 100 Recall topic model maxent classifier Precision 100 90 80 70 60 50 40 30 20 10 0 (a) (b) (c) Figure 4: (a) Aspect service. (b) Aspect location. (c) Aspect rooms. cluding the first 3 topics associated with the rated aspects, and also top words for some of global topics are presented in Table 1. We can see that the model discovered as its first three topics the correct associated aspects: service, location, and rooms. Other local topics, as for the MG-LDA model, correspond to other aspects discussed in reviews (breakfast, prices, noise), and as it was previously shown in Titov and McDonald (2008), aspects for global topics correspond to the types of reviewed items (hotels in Russia, Paris hotels) or background words. Notice though, that the 3rd local topic induced for the rating rooms is slightly narrow. This can be explained by the fact that the aspect rooms is a central aspect of hotel reviews. A very significant fraction of text in every review can be thought of as a part of the aspect rooms. These portions of reviews discuss different coherent sub-aspects related to the aspect rooms, e.g., the previously discovered topic noise. Therefore, it is natural to associate several topics </context>
<context position="31569" citStr="Titov and McDonald, 2008" startWordPosition="5337" endWordPosition="5340">.8%, 94.0% and 88.3% for the aspects service, location and rooms. We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model. 4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence. Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008). String extraction methods are limited to fine-grained aspects whereas clustering and topic model approaches must resort to ad-hoc means of labeling clusters or topics. However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms. the purpose of extraction. A closely related model to ours is that of Mei et al. (2007) which performs joint topic and sentiment modeling of collections. Our model differs from theirs in many respects: </context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>I. Titov and R. McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of the 17h International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Sentiment orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Conference of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3139" citStr="Turney, 2002" startWordPosition="493" endWordPosition="494">ects for a rated entity and extract all textual mentions that are associated with each. Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics � Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensiv</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? Sentiment orientation applied to unsupervised classification of reviews. In Proceedings of the Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling; beyond bag of words.</title>
<date>2006</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="10017" citStr="Wallach, 2006" startWordPosition="1651" endWordPosition="1653">thin a document.4 Each window v in document d has an associated distribution over local topics �loc d,v and a distribution defining preference for local topics versus global topics 7rd,v. A word can be sampled using any window covering its sentence s, where the window is chosen according to a categorical distribution Od,s. Importantly, the fact that windows overlap permits the model to exploit a larger co-occurrence domain. These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in (Griffiths et al., 2004; Wang and McCallum, 2005; Wallach, 2006; Gruber et al., 2007). Introduction of a symmetrical Dirichlet prior Dir(γ) for the distribution Od,s can control the smoothness of transitions. 4Our particular implementation is over sentences, but sliding windows in theory can be over any sized fragment of text. (a) (b) Figure 3: (a) MG-LDA model. (b) An extension of MGLDA to obtain MAS. The formal definition of the model with Kgl global and Kloc local topics is as follows: First, draw Kgl word distributions for global topics cogl z from a Dirichlet prior Dir(0gl) and Kloc word distributions for local topics coloc z′ - from Dir(0loc). Then,</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling; beyond bag of words. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuerui Wang</author>
<author>Andrew McCallum</author>
</authors>
<title>A note on topical n-grams.</title>
<date>2005</date>
<tech>Technical Report UM-CS-2005-071,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="10002" citStr="Wang and McCallum, 2005" startWordPosition="1647" endWordPosition="1650">g T adjacent sentences within a document.4 Each window v in document d has an associated distribution over local topics �loc d,v and a distribution defining preference for local topics versus global topics 7rd,v. A word can be sampled using any window covering its sentence s, where the window is chosen according to a categorical distribution Od,s. Importantly, the fact that windows overlap permits the model to exploit a larger co-occurrence domain. These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in (Griffiths et al., 2004; Wang and McCallum, 2005; Wallach, 2006; Gruber et al., 2007). Introduction of a symmetrical Dirichlet prior Dir(γ) for the distribution Od,s can control the smoothness of transitions. 4Our particular implementation is over sentences, but sliding windows in theory can be over any sized fragment of text. (a) (b) Figure 3: (a) MG-LDA model. (b) An extension of MGLDA to obtain MAS. The formal definition of the model with Kgl global and Kloc local topics is as follows: First, draw Kgl word distributions for global topics cogl z from a Dirichlet prior Dir(0gl) and Kloc word distributions for local topics coloc z′ - from D</context>
</contexts>
<marker>Wang, McCallum, 2005</marker>
<rawString>Xuerui Wang and Andrew McCallum. 2005. A note on topical n-grams. Technical Report UM-CS-2005-071, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="3105" citStr="Wiebe, 2000" startWordPosition="487" endWordPosition="488"> to find the set of relevant aspects for a rated entity and extract all textual mentions that are associated with each. Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics � Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our annivers</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>J. Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhuang</author>
<author>F Jing</author>
<author>X Y Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM international conference on Information and knowledge management (CIKM),</booktitle>
<pages>43--50</pages>
<contexts>
<context position="2007" citStr="Zhuang et al., 2006" startWordPosition="310" endWordPosition="313">the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Nikos’ Fine Dining Food 4/5 “Best fish in the city”, “Excellent appetizers” Decor 3/5 “Cozy with an old world feel”, “Too dark” Service 1/5 “Our waitress was rude”, “Awful service” Value 5/5 “Good Greek food for the $”, “Great price!” Figure 1: An example aspect-based summary. Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5. Standard aspect-based summarization consists of two problems. The first is aspect identification and mention extraction. Here the goal is to find the set of relevant aspects for a rated entity and extract all textual mentions that are associated with</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie review mining and summarization. In Proceedings of the 15th ACM international conference on Information and knowledge management (CIKM), pages 43–50.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>