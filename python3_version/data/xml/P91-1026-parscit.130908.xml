<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9058475">
AUTOMATIC NOUN CLASSIFICATION BY USING
JAPANESE-ENGLISH WORD PAIRS*
</title>
<author confidence="0.7755045">
Naomi Inoue
KDD R &amp; D Laboratories
</author>
<address confidence="0.623411">
2-1-5 Ohara, Kamifukuoka-shi Saitama 356, Japan
</address>
<email confidence="0.995044">
inoue@kddlab.kddlabs.epjp
</email>
<sectionHeader confidence="0.990339" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999799625">
This paper describes a method of
classifying semantically similar nouns. The
approach is based on the &amp;quot;distributional
hypothesis&amp;quot;. Our approach is characterized
by distinguishing among senses of the same
word in order to resolve the &amp;quot;polysemy&amp;quot; issue.
The classification result demonstrates that
our approach is successful.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999796048780488">
Sets of semantically similar words are
very useful in natural language processing.
The general approach toward classifying
words is to use semantic categories, for
example the thesaurus. The &amp;quot;is-a&amp;quot; relation is
connected between words and categories.
However, it is not easy to acquire the &amp;quot;is-a&amp;quot;
connection by hand, and it becomes
expensive.
Approaches toward automatically
classifying words using existing dictionaries
were therefore attempted[Chodorow]
[Tsurumaru] [Nakamura]. These approaches
are partially successful. However, there is a
fatal problem in these approaches, namely,
existing dictionaries, particularly Japanese
dictionaries, are not assembled on the basis
of semantic hierarchy.
On the other hand, approaches toward
automatically classifying words by using a
large-scale corpus have also been
attempted[Shirai][Hindle]. They seem to be
based on the idea that semantically similar
words appear in similar environments. This
idea is derived from Harris&apos;s &amp;quot;distributional
hypothesis&amp;quot;[Harris] in linguistics. Focusing
on nouns, the idea claims that each noun is
characterized by verbs with which it occurs,
and also that nouns are similar to the extent
that they share verbs. These automatic
classification approaches are also partially
successful. However, Hindle says that there
is a number of issues to be confronted. The
most important issue is that of &amp;quot;polysemy&amp;quot;.
In Hindle&apos;s experiment, two senses of &amp;quot;table&amp;quot;,
that is to say &amp;quot;table under which one can
hide&amp;quot; and &amp;quot;table which can be commuted or
memorized&amp;quot;, are conflated in the set of words
similar to &amp;quot;table&amp;quot;. His result shows that
senses of the word must be distinguished
before classification.
</bodyText>
<listItem confidence="0.988799">
(1)I sit on the table.
(2)I sit on the chair.
(3)I fill in the table.
(4)I fill in the list.
</listItem>
<bodyText confidence="0.999933764705882">
For example, the above sentences may
appear in the corpus. In sentences (1) and (2),
&amp;quot;table&amp;quot; and &amp;quot;chair&amp;quot; share the same verb &amp;quot;sit
on&amp;quot;. In sentences (3) and (4), &amp;quot;table&amp;quot; and
&amp;quot;list&amp;quot; share the same verb &amp;quot;fill in&amp;quot;. However,
&amp;quot;table&amp;quot; is used in two different senses. Unless
they are distinguished before classification,
&amp;quot;table&amp;quot;, &amp;quot;chair&amp;quot; and &amp;quot;list&amp;quot; may be put into the
same category because &amp;quot;chair&amp;quot; and &amp;quot;list&amp;quot;
share the same verbs which are associated
with &amp;quot;table&amp;quot;. It is thus necessary to
distinguish the senses of &amp;quot;table&amp;quot; before
automatic classification. Moreover, when the
corpus is not sufficiently large, this must be
performed for verbs as well as nouns. In the
following Japanese sentences, the Japanese
verb &amp;quot;RI &lt; &amp;quot;is used in different senses. One is
</bodyText>
<footnote confidence="0.7333735">
* This study was done during the author&apos;s stay
at ATR Interpreting Telephony Research Laboratories.
</footnote>
<page confidence="0.996679">
201
</page>
<figure confidence="0.9748252">
J1: 7 * — tJLC&lt;
A t A A tA
space at object
El:Please fill A--out in the reply form and submit the summary to you.
--
</figure>
<figureCaption confidence="0.997047">
Figure 1 An example of deep semantic relations and the correspondence
</figureCaption>
<bodyText confidence="0.953851956521739">
&amp;quot;to request information from someone&amp;quot;. The
other is &amp;quot;to give attention in hearing&amp;quot;.
Japanese words&amp;quot; tiff (name)&amp;quot; and&amp;quot;
(music)&amp;quot; share the same verb&amp;quot; &lt; &amp;quot;. Using
the small corpus,&amp;quot; fa (name)&amp;quot; and&amp;quot; iR
(music)&amp;quot; may be classified into the same
category because they share the same verb,
though not the same sense, on relatively
frequent.
(5)t &lt;
(6)1114 &lt;
This paper describes an approach to
automatically classify the Japanese nouns.
Our approach is characterized by
distinguishing among senses of the same
word by using Japanese-English word pairs
extracted from a bilingual database. We
suppose here that some senses of Japanese
words are distinguished when Japanese
sentences are translated into another
language. For example, The following
Japanese sentences (7),(8) are translated into
English sentences (9),(10), respectively.
</bodyText>
<equation confidence="0.9120505">
(7)1A 41* A 1±1
(8)ia
</equation>
<footnote confidence="0.526299">
(9)He sends a letter.
(10)He publishes a book.
</footnote>
<bodyText confidence="0.999977888888889">
The Japanese word&amp;quot; 1i t&amp;quot; has at least
two senses. One is &amp;quot;to cause to go or be taken
to a place&amp;quot; and the other is &amp;quot;to have printed
and put on sale&amp;quot;. In the above example, the
Japanese word&amp;quot; &amp; t&amp;quot; corresponds to &amp;quot;send&amp;quot;
from sentences (7) and (9). The Japanese
word &amp;quot; d1 t&amp;quot; also corresponds to &amp;quot;publish&amp;quot;
from sentences (8) and (10). That is to say,
the Japanese word&amp;quot; 9i t&amp;quot; is translated into
different English words according to the
sense. This example shows that it may be
possible to distinguish among senses of the
same word by using words from another
language. We used Japanese-English word
pairs, for example,&amp;quot; t -send&amp;quot; and&amp;quot; FI t -
publish&amp;quot;, as senses of Japanese words.
In this paper, these word pairs are
acquired from ATR&apos;s large scale database.
</bodyText>
<sectionHeader confidence="0.98891" genericHeader="method">
2. CONTENT OF THE DATABASE
</sectionHeader>
<bodyText confidence="0.993668423076923">
ATR has constructed a large-scale
database which is collected from simulated
telephone and keyboard conversations
[Ehara]. The sentences collected in Japanese
are manually translated into English. We
obtain a bilingual database. The database is
called the ATR Dialogue Database(ADD).
ATR aims to build ADD to one million words
covering two tasks. One task is dialogues
between secretaries and participants of
international conferences. The other is
dialogues between travel agents and
customers. Collected Japanese and English
sentences are morphologically analyzed.
Japanese sentences are also dependency
analyzed and given deep semantic relations.
We use 63 deep semantic cases[Inoue].
Correspondences of Japanese and English
are made by several linguistic units, for
example words, sentences and so on.
Figure 1 shows an example of deep
semantic relations and correspondences of
Japanese and English words. The sentence is
already morphologically analyzed. The solid
line shows deep semantic relations. The
Japanese nouns&amp;quot; &apos;I 7° 7 — 4&amp;quot;and
</bodyText>
<page confidence="0.989233">
202
</page>
<bodyText confidence="0.986437142857143">
g9&amp;quot; modify the Japanese verbs &amp;quot;* 1,1&amp;quot; and &amp;quot;WI
L. &amp;quot;, respectively. The semantic relations are
&amp;quot;space at&amp;quot; and &amp;quot;object&amp;quot;, which are almost
equal to &amp;quot;locative&amp;quot; and &amp;quot;objective&amp;quot; of
Fillmore&apos;s deep casefFillmore]. The dotted
line shows the word correspondence between
Japanese and English. The Japanese words
</bodyText>
<equation confidence="0.638228">
4 7
</equation>
<bodyText confidence="0.990920875">
L &amp;quot; correspond to the English words &amp;quot;reply
form&amp;quot;, &amp;quot;fill out&amp;quot;, &amp;quot;summary&amp;quot; and &amp;quot;submit&amp;quot;,
respectively. Here,&amp;quot; * &amp;quot; and&amp;quot; tFS L &amp;quot; are
conjugations of&amp;quot; # &lt; &amp;quot; and &amp;quot; ±l t&amp;quot;,
respectively. However, it is possible to
extract semantic relations and word
correspondence in dictionary form, because
ADD includes the dictionary forms.
</bodyText>
<sectionHeader confidence="0.993208" genericHeader="method">
3. CLASSIFICATION OF NOUNS
</sectionHeader>
<subsectionHeader confidence="0.998934">
3.1 Using Data
</subsectionHeader>
<bodyText confidence="0.997692222222222">
We automatically extracted from ADD
not only deep semantic relations between
Japanese nouns and verbs but also the
English word which corresponds to the
Japanese word. We used telephone dialogues
between secretaries and participants because
the scale of analyzed words was largest.
Table 1 shows the current number of
analyzed words.
</bodyText>
<tableCaption confidence="0.988936">
Table 1 Analyzed words counts of ADD
</tableCaption>
<table confidence="0.8591466">
Media Task Words
Telephone Conference 139,774
Travel 11,709
Keyboard Conference 64,059
Travel 0
</table>
<bodyText confidence="0.99911119047619">
Figure 2 shows an example of the data
extracted from ADD. Each field is delimited
by the delimiter &amp;quot;1&amp;quot;. The first field is the
dialogue identification number in which the
semantic relation appears. The second and
the third fields are Japanese nouns and their
corresponding English words. The next 2
fields are Japanese verbs and their
corresponding English words. The last is the
semantic relations between nouns and verbs.
Moreover, we automatically acquired
word pairs from the data shown in Figure 2.
Different senses of nouns appear far less
frequently than those of verbs because the
database is restricted to a specific task. In
this experiment, only word pairs of verbs are
used. Figure 3 shows deep semantic relations
between nouns and word pairs of verbs. The
last field is raw frequency of co-occurrence.
We used the data shown in Figure 3 for noun
classification.
</bodyText>
<figure confidence="0.542694">
ima &apos;registration feel 1±11- Ipaylobject
151Wr9Isummarylkh t Isendlobject
15717 El — 4 :/ Ylproceedinglffii-
lissuelobject
4rlconference14 Ibe heldlobject
8&apos; N. 1Iquestion14 Ihavelobject
31&apos; IbusAZ Itakelobject
1801* NJ Inewspaped* Z Iseelspace at
</figure>
<figureCaption confidence="0.9813125">
Figure 2 An example of data extracted
from ADD
</figureCaption>
<bodyText confidence="0.9998745">
The experiment is done for a sample of
138 nouns which are included in the 500
most frequent words. The 500 most frequent
words cover 90% of words accumulated in the
telephone dialogue. Those nouns appear
more frequently than 9 in ADD.
</bodyText>
<equation confidence="0.843578333333333">
ROVII:ht-paylobjectIl
Wt -sendlobject12
7&apos;I2 I= 4 :/ Yl1t-issuelobjectI2
-be held&apos;objectI6
N. PI l Z -havelobject17
1* -takelobjectil
</equation>
<bodyText confidence="0.633076">
ffr 1111 1* Z., -seelspace atI1
Figure 3 An example of semantic rela-
tions of nouns and word pairs
</bodyText>
<subsectionHeader confidence="0.972572">
3.2 Semantic Distance of Nouns
</subsectionHeader>
<bodyText confidence="0.999975">
Our classification approach is based on
the &amp;quot;distributional hypothesis&amp;quot;. Based on
this semantic theory, nouns are similar to
the extent that they share verb senses. The
aim of this paper is to show the efficiency of
using the word pair as the word sense. We
therefore used the following expression(1),
which was already defined by Shirai[Shirai]
as the distance between two words. The
</bodyText>
<page confidence="0.962111">
203
</page>
<equation confidence="0.776451">
d(a,b) = EE4)(M(a,v,r),M(b,v,r))
v(V,r(R
1 (1)
EE(M(a,v,r) + M(b,v,r))
vEVJER
</equation>
<bodyText confidence="0.805328">
Here, a,b : noun (a,b E N)
r: semantic relation
v: verb senses
N : the set of nouns
V: the set of verb senses
R: the set of semantic relations
M(a,v,r) : the frequency of the semantic relation r
between a and v
</bodyText>
<equation confidence="0.90117">
41(x,y) = fx + y (x &gt;0, y &gt;0)
1. (x =0 or y=0)
</equation>
<bodyText confidence="0.998796545454546">
second term of the expression can show the
semantic similarity between two nouns,
because it is the ratio of the verb senses with
which both nouns (a and b) occur and all the
verb senses with which each noun (a or b)
occurs. The distance is normalized from 0.0 to
1.0. If one noun (a) shares all verb senses
with the other noun (b) and the frequency is
also same, the distance is 0.0. If one noun (a)
shares no verb senses with the other noun
(b), the distance is 1.0.
</bodyText>
<subsectionHeader confidence="0.998437">
3.3 Classification Method
</subsectionHeader>
<bodyText confidence="0.999986777777778">
For the classification, we adopted cluster
analysis which is one of the approaches in
multivariant analysis. Cluster analysis is
generally used in various fields, for example
biology, psychology, etc.. Some hierarchical
clustering methods, for example the nearest
neighbor method, the centroid method, etc.,
have been studied. It has been proved that
the centroid method can avoid the chain
effect. The chain effect is an undesirable
phenomenon in which the nearest unit is not
always classified into a cluster and more
distant units are chained into a cluster. The
centroid method is a method in which the
cluster is characterized by the centroid of
categorized units. In the following section,
the result obtained by the centroid method is
shown.
</bodyText>
<sectionHeader confidence="0.827212" genericHeader="method">
4.EXPERIMENT
</sectionHeader>
<subsectionHeader confidence="0.999142">
4.1 Clustering Result
</subsectionHeader>
<bodyText confidence="0.970309928571429">
All 138 nouns are hierarchically
classified. However, only some subsets of the
whole hierarchy are shown, as space is
limited. In Figure 4, we can see that
semantically similar nouns, which may be
defined as &amp;quot;things made from paper&amp;quot;, are
grouped together. The X-axis is the semantic
distance defined before. Figure 5 shows
another subset. All nouns in Figure 5, &amp;quot;
(decision)&amp;quot;, &amp;quot;M (presentation)&amp;quot;, &amp;quot;X —
(speech)&amp;quot; and &amp;quot; T&apos;(talk)&amp;quot;, have an active
concept like verbs. Subsets of nouns shown in
Figures 4 and 5 are fairly coherent. However,
all subsets of nouns are not coherent. In
</bodyText>
<figureCaption confidence="0.957352">
Figure 6,&amp;quot; A 4 F (slide)&amp;quot;, &amp;quot; (draft)&amp;quot;,
</figureCaption>
<bodyText confidence="0.996823714285714">
&amp;quot; (conference site)&amp;quot;, &amp;quot;8 H (8th)&amp;quot; and&amp;quot;
(station)&amp;quot; are grouped together. The
semantic distances are 0.67, 0.6, 0.7 and 0.8.
The distance is upset when &amp;quot; J( conference
site)&amp;quot; is attached to the cluster containing
&amp;quot; X 4 F (slide)&amp;quot; and &amp;quot;W. (draft)&amp;quot;. This is
one characteristic of the centroid method.
However, this seems to result in a
semantically less similar cluster. The word
pairs of verbs, the deep semantic relations
and the frequency are shown in Table 2.
After &amp;quot;A 4 F (slide)&amp;quot; and &amp;quot; tilt (draft)&amp;quot; are
grouped into a cluster, the cluster and &amp;quot;
(conference site)&amp;quot; share two word pairs,&amp;quot;
-use&amp;quot; and &amp;quot;fil Z) -be&amp;quot;. &amp;quot;M -be&amp;quot; contributes
more largely to attach &amp;quot; i( conference
site)&amp;quot; to the cluster than &amp;quot; -use&amp;quot; because
the frequency of co-occurrence is greater. In
this sample, &amp;quot; a 7., -be&amp;quot; occurs with more
nouns than &amp;quot; -use&amp;quot;. It shows that &amp;quot;)l -
be&amp;quot; is less important in characterizing nouns
</bodyText>
<page confidence="0.996606">
204
</page>
<bodyText confidence="0.999946833333333">
though the raw frequency of co-occurrence is
greater. It is therefore necessary to develop a
means of not relying on the raw frequency of
co-occurrence, in order to make the
clustering result more accurate. This is left
to further study.
</bodyText>
<subsectionHeader confidence="0.996962">
4.2 Estimation of the Result
</subsectionHeader>
<bodyText confidence="0.9992643">
All nouns are hierarchically classified,
but some semantically separated clusters are
acquired if the threshold is used.
It is possible to compare clusters derived
from this experiment with semantic
categories which are used in our automatic
interpreting telephony system. We used
expression (2), which was defined by
Goodman and Kruskal[Goodman], in order to
objectively compare them.
</bodyText>
<figure confidence="0.950089">
0.0 0.2 0.4 0.6 0.8 1.0
7 1- (list)
TR (form)
**(material)
A-2 (hope)
*fa (document)
7 17 I- I 1- (abstract)
7° (program)
</figure>
<figureCaption confidence="0.770632">
Figure 4 An example of the classification of nouns
</figureCaption>
<figure confidence="0.889624666666667">
0.0 0.2 0.4 0.6 0.8 1.0
ik(decision)
*(presentation)
A e 4- (speech)
.r*S (talk)
Figure 5 Another example of the classification of nouns
0.0 0.2 0.4 0.6 0.8 1 0
-I 1-` (slide)
Efik&amp;quot; (draft)
IA (conference site)
812 (8th)
E (station)
</figure>
<figureCaption confidence="0.993995">
Figure 6 Another example of the classification of nouns
</figureCaption>
<page confidence="0.996306">
205
</page>
<tableCaption confidence="0.941263">
Table 2 A subset of semantically similar nouns
</tableCaption>
<figure confidence="0.872640074074074">
noun word pairs of verb deep case frequency
A --, 4 V (slide) t Z -make goal 1
ft Z -make object 1
1 &amp;quot;&amp;quot;) -use object 1
JP; a (draft) ft Z -make object 1
a Z -be object 1
4 o -look forward to object 1
0•14 (conference site) Oh Z -take condition 1
fil -) -get space to 1
It i -use object 1
ITI* 6 -can space at 1
B i -say space at 1
A Z -be object 2
88 (8th) t4.&apos; Z -end time 2
A Z -be object 1
04 &lt; -guess content 1
IR (station) til. Z -take condition 1
OM to i t -there be space from 1
p = (2)
Here, Pi = 1 - Lin
P2 = 4.(1 - fim/fi.)
1=1
Lin .7&amp;quot;-- L2. &amp;quot;&apos;,
fan; = max{fal, flap fag}
fij =
f. = n.i/n
A: a set of clusters which are automatically obtained.
</figure>
<bodyText confidence="0.818097555555556">
B: a set of clusters which are used in our interpreting
telephony system.
p: the number of clusters of a set A
q: the number of clusters of a set B
nij : the number of nouns which are included in both the ith
cluster of A and the jth cluster of B
n.j: the number of nouns which are included in the jth cluster
of B
n: all nouns which are included in A or B
</bodyText>
<page confidence="0.996502">
206
</page>
<bodyText confidence="0.999953722222222">
They proposed that one set of clusters, called
&apos;A&apos;, can be estimated to the extent that &apos;A&apos;
associates with the other set of clusters,
called &apos;B&apos;. In figure 7, two results are shown.
One (solid line) is the result of using the word
pair to distinguish among senses of the same
verb. The other (dotted line) is the result of
using the verb form itself. The X-axis is the
number of classified nouns and the Y-axis is
the value derived from the above
expression.Figure 7 shows that it is better to
use word pairs of verbs than not use them,
when fewer than about 30 nouns are
classified. However, both are almost the
same, when more than about 30 nouns are
classified. The result proves that the
distinction of senses of verbs is successful
when only a few nouns are classified.
</bodyText>
<figure confidence="0.936634928571429">
0.6
Word Pairs of Verbs
- Form
---- Verb
0.5
0.4
0.3
0.2
f
1
0.1 r4
0.0
0 50 100
Number of Nouns
</figure>
<figureCaption confidence="0.999906">
Figure 7 Estimation result
</figureCaption>
<sectionHeader confidence="0.932266" genericHeader="conclusions">
5. CONCLUSION
</sectionHeader>
<bodyText confidence="0.999896518518518">
Using word pairs of Japanese and
English to distinguish among senses of the
same verb, we have shown that using word
pairs to classify nouns is better than not
using word pairs, when only a few nouns are
classified. However, this experiment did not
succeed for a sufficient number of nouns for
two reasons. One is that the raw co-occurrent
frequency is used to calculate the semantic
distance. The other is that the sample size is
too small. It is thus necessary to resolve the
following issues to make the classification
result more accurate.
(1)to develop a means of using the
frequency normalized by expected word
pairs.
(2)to estimate an adequate sample size.
In this experiment, we acquired word
pairs and semantic relations from our
database. However, they are made by hand.
It is also preferable to develop a method of
automatically acquiring them from the
bilingual text database.
Moreover, we want to apply the
hierarchically classified result to the
translated word selection problem in
Machine translation.
</bodyText>
<sectionHeader confidence="0.986407" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.97091675">
The author is deeply grateful to
Dr. Akira Kurematsu, President of ATR
Interpreting Telephony Research
Laboratories, Dr. Toshiyuki Takezawa and
other members of the Knowledge &amp; Data
Base Department for their encouragement,
during the author&apos;s stay at ATR Interpreting
Telephony Research Laboratories.
</bodyText>
<sectionHeader confidence="0.984302" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9998545625">
[Chodorow] Chodorow, M. S., et al.
&amp;quot;Extracting Semantic Hierarchies from a
Large On-line Dictionary.&amp;quot;, Proceedings of
the 23rd Annual Meeting of the ACL, 1985.
[Ehara] Ehara, T., et al. &amp;quot;ATR Dialogue
Database&amp;quot;, Proceedings of ICSLP, 1990.
[Fillmore] Fillmore, C. J. &amp;quot;The case for case&amp;quot;,
in E. Bach &amp; Harms (Eds.) Universals in
linguistic theory, 1968.
[Goodman] Goodman, L. A., and Kruskal
W.H. &amp;quot;Measures of Association for Cross
Classifications&amp;quot;, J. Amer. Statist. Assoc. 49,
1954.
[Harris] Harris, Z. S. &amp;quot;Mathematical
Structures of Language&amp;quot;, a Wiley-
Interscience Publication.
</reference>
<page confidence="0.968318">
207
</page>
<reference confidence="0.999764846153846">
[Hindle] Hindle, D. &amp;quot;Noun Classification
from Predicate-Argument Structures&amp;quot;,
Proceedings of 28th Annual Meeting of the
ACL, 1990.
[Inoue] Inoue, N., et al. &amp;quot;Semantic Relations
in ATR Linguistic Database&amp;quot; (in Japanese),
ATR Technical Report TR-I-0029, 1988.
[Nakamura] Nakamura, J., et al. &amp;quot;Automatic
Analysis of Semantic Relation between
English Nouns by an Ordinal English
Dictionary&amp;quot; (in Japanese), the Institute of
Electronics, Information and
Communication Engineers, Technical
Report, NLC-86, 1986.
[Shirai] Shirai K., et al. &amp;quot;Database
Formulation and Learning Procedure for
Kakariuke Dependency Analysis&amp;quot; (in
Japanese), Transactions of Information
Processing Society of Japan, Vol.26, No.4,
1985.
[Tsurumaru] Tsurumaru H., et al.
&amp;quot;Automatic Extraction of Hierarchical
Structure of Words from Definition
Sentences&amp;quot; (in Japanese), the Information
Processing Society of Japan, Sig. Notes, 87-
NL-64, 1987.
</reference>
<page confidence="0.99778">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819030">
<title confidence="0.9370895">AUTOMATIC NOUN CLASSIFICATION BY USING JAPANESE-ENGLISH WORD PAIRS*</title>
<author confidence="0.998268">Naomi Inoue</author>
<affiliation confidence="0.995427">KDD R &amp; D Laboratories</affiliation>
<address confidence="0.995966">2-1-5 Ohara, Kamifukuoka-shi Saitama 356, Japan</address>
<email confidence="0.947891">inoue@kddlab.kddlabs.epjp</email>
<abstract confidence="0.999359">This paper describes a method of classifying semantically similar nouns. The approach is based on the &amp;quot;distributional hypothesis&amp;quot;. Our approach is characterized by distinguishing among senses of the same word in order to resolve the &amp;quot;polysemy&amp;quot; issue. The classification result demonstrates that our approach is successful.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M S Chodorow</author>
</authors>
<title>Extracting Semantic Hierarchies from a Large On-line Dictionary.&amp;quot;,</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="979" citStr="[Chodorow]" startWordPosition="133" endWordPosition="133">g senses of the same word in order to resolve the &amp;quot;polysemy&amp;quot; issue. The classification result demonstrates that our approach is successful. 1. INTRODUCTION Sets of semantically similar words are very useful in natural language processing. The general approach toward classifying words is to use semantic categories, for example the thesaurus. The &amp;quot;is-a&amp;quot; relation is connected between words and categories. However, it is not easy to acquire the &amp;quot;is-a&amp;quot; connection by hand, and it becomes expensive. Approaches toward automatically classifying words using existing dictionaries were therefore attempted[Chodorow] [Tsurumaru] [Nakamura]. These approaches are partially successful. However, there is a fatal problem in these approaches, namely, existing dictionaries, particularly Japanese dictionaries, are not assembled on the basis of semantic hierarchy. On the other hand, approaches toward automatically classifying words by using a large-scale corpus have also been attempted[Shirai][Hindle]. They seem to be based on the idea that semantically similar words appear in similar environments. This idea is derived from Harris&apos;s &amp;quot;distributional hypothesis&amp;quot;[Harris] in linguistics. Focusing on nouns, the idea cl</context>
</contexts>
<marker>[Chodorow]</marker>
<rawString>Chodorow, M. S., et al. &amp;quot;Extracting Semantic Hierarchies from a Large On-line Dictionary.&amp;quot;, Proceedings of the 23rd Annual Meeting of the ACL, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ehara</author>
</authors>
<title>ATR Dialogue Database&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of ICSLP,</booktitle>
<contexts>
<context position="5125" citStr="[Ehara]" startWordPosition="810" endWordPosition="810">blish&amp;quot; from sentences (8) and (10). That is to say, the Japanese word&amp;quot; 9i t&amp;quot; is translated into different English words according to the sense. This example shows that it may be possible to distinguish among senses of the same word by using words from another language. We used Japanese-English word pairs, for example,&amp;quot; t -send&amp;quot; and&amp;quot; FI t - publish&amp;quot;, as senses of Japanese words. In this paper, these word pairs are acquired from ATR&apos;s large scale database. 2. CONTENT OF THE DATABASE ATR has constructed a large-scale database which is collected from simulated telephone and keyboard conversations [Ehara]. The sentences collected in Japanese are manually translated into English. We obtain a bilingual database. The database is called the ATR Dialogue Database(ADD). ATR aims to build ADD to one million words covering two tasks. One task is dialogues between secretaries and participants of international conferences. The other is dialogues between travel agents and customers. Collected Japanese and English sentences are morphologically analyzed. Japanese sentences are also dependency analyzed and given deep semantic relations. We use 63 deep semantic cases[Inoue]. Correspondences of Japanese and E</context>
</contexts>
<marker>[Ehara]</marker>
<rawString>Ehara, T., et al. &amp;quot;ATR Dialogue Database&amp;quot;, Proceedings of ICSLP, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>The case for case&amp;quot;, in E. Bach &amp; Harms (Eds.) Universals in linguistic theory,</title>
<date>1968</date>
<marker>[Fillmore]</marker>
<rawString>Fillmore, C. J. &amp;quot;The case for case&amp;quot;, in E. Bach &amp; Harms (Eds.) Universals in linguistic theory, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Goodman</author>
<author>W H Kruskal</author>
</authors>
<title>Measures of Association for Cross Classifications&amp;quot;,</title>
<date>1954</date>
<journal>J. Amer. Statist. Assoc.</journal>
<volume>49</volume>
<contexts>
<context position="12979" citStr="[Goodman]" startWordPosition="2100" endWordPosition="2100">gh the raw frequency of co-occurrence is greater. It is therefore necessary to develop a means of not relying on the raw frequency of co-occurrence, in order to make the clustering result more accurate. This is left to further study. 4.2 Estimation of the Result All nouns are hierarchically classified, but some semantically separated clusters are acquired if the threshold is used. It is possible to compare clusters derived from this experiment with semantic categories which are used in our automatic interpreting telephony system. We used expression (2), which was defined by Goodman and Kruskal[Goodman], in order to objectively compare them. 0.0 0.2 0.4 0.6 0.8 1.0 7 1- (list) TR (form) **(material) A-2 (hope) *fa (document) 7 17 I- I 1- (abstract) 7° (program) Figure 4 An example of the classification of nouns 0.0 0.2 0.4 0.6 0.8 1.0 ik(decision) *(presentation) A e 4- (speech) .r*S (talk) Figure 5 Another example of the classification of nouns 0.0 0.2 0.4 0.6 0.8 1 0 -I 1-` (slide) Efik&amp;quot; (draft) IA (conference site) 812 (8th) E (station) Figure 6 Another example of the classification of nouns 205 Table 2 A subset of semantically similar nouns noun word pairs of verb deep case frequency A -</context>
</contexts>
<marker>[Goodman]</marker>
<rawString>Goodman, L. A., and Kruskal W.H. &amp;quot;Measures of Association for Cross Classifications&amp;quot;, J. Amer. Statist. Assoc. 49, 1954.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Z S Harris</author>
</authors>
<title>Mathematical Structures of Language&amp;quot;, a WileyInterscience Publication.</title>
<contexts>
<context position="1532" citStr="[Harris]" startWordPosition="206" endWordPosition="206">xisting dictionaries were therefore attempted[Chodorow] [Tsurumaru] [Nakamura]. These approaches are partially successful. However, there is a fatal problem in these approaches, namely, existing dictionaries, particularly Japanese dictionaries, are not assembled on the basis of semantic hierarchy. On the other hand, approaches toward automatically classifying words by using a large-scale corpus have also been attempted[Shirai][Hindle]. They seem to be based on the idea that semantically similar words appear in similar environments. This idea is derived from Harris&apos;s &amp;quot;distributional hypothesis&amp;quot;[Harris] in linguistics. Focusing on nouns, the idea claims that each noun is characterized by verbs with which it occurs, and also that nouns are similar to the extent that they share verbs. These automatic classification approaches are also partially successful. However, Hindle says that there is a number of issues to be confronted. The most important issue is that of &amp;quot;polysemy&amp;quot;. In Hindle&apos;s experiment, two senses of &amp;quot;table&amp;quot;, that is to say &amp;quot;table under which one can hide&amp;quot; and &amp;quot;table which can be commuted or memorized&amp;quot;, are conflated in the set of words similar to &amp;quot;table&amp;quot;. His result shows that sens</context>
</contexts>
<marker>[Harris]</marker>
<rawString>Harris, Z. S. &amp;quot;Mathematical Structures of Language&amp;quot;, a WileyInterscience Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun Classification from Predicate-Argument Structures&amp;quot;,</title>
<date>1990</date>
<booktitle>Proceedings of 28th Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="1362" citStr="[Hindle]" startWordPosition="182" endWordPosition="182">ords and categories. However, it is not easy to acquire the &amp;quot;is-a&amp;quot; connection by hand, and it becomes expensive. Approaches toward automatically classifying words using existing dictionaries were therefore attempted[Chodorow] [Tsurumaru] [Nakamura]. These approaches are partially successful. However, there is a fatal problem in these approaches, namely, existing dictionaries, particularly Japanese dictionaries, are not assembled on the basis of semantic hierarchy. On the other hand, approaches toward automatically classifying words by using a large-scale corpus have also been attempted[Shirai][Hindle]. They seem to be based on the idea that semantically similar words appear in similar environments. This idea is derived from Harris&apos;s &amp;quot;distributional hypothesis&amp;quot;[Harris] in linguistics. Focusing on nouns, the idea claims that each noun is characterized by verbs with which it occurs, and also that nouns are similar to the extent that they share verbs. These automatic classification approaches are also partially successful. However, Hindle says that there is a number of issues to be confronted. The most important issue is that of &amp;quot;polysemy&amp;quot;. In Hindle&apos;s experiment, two senses of &amp;quot;table&amp;quot;, that i</context>
</contexts>
<marker>[Hindle]</marker>
<rawString>Hindle, D. &amp;quot;Noun Classification from Predicate-Argument Structures&amp;quot;, Proceedings of 28th Annual Meeting of the ACL, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Inoue</author>
</authors>
<title>Semantic Relations in ATR Linguistic Database&amp;quot;</title>
<date>1988</date>
<tech>(in Japanese), ATR Technical Report TR-I-0029,</tech>
<contexts>
<context position="5690" citStr="[Inoue]" startWordPosition="890" endWordPosition="890">lephone and keyboard conversations [Ehara]. The sentences collected in Japanese are manually translated into English. We obtain a bilingual database. The database is called the ATR Dialogue Database(ADD). ATR aims to build ADD to one million words covering two tasks. One task is dialogues between secretaries and participants of international conferences. The other is dialogues between travel agents and customers. Collected Japanese and English sentences are morphologically analyzed. Japanese sentences are also dependency analyzed and given deep semantic relations. We use 63 deep semantic cases[Inoue]. Correspondences of Japanese and English are made by several linguistic units, for example words, sentences and so on. Figure 1 shows an example of deep semantic relations and correspondences of Japanese and English words. The sentence is already morphologically analyzed. The solid line shows deep semantic relations. The Japanese nouns&amp;quot; &apos;I 7° 7 — 4&amp;quot;and 202 g9&amp;quot; modify the Japanese verbs &amp;quot;* 1,1&amp;quot; and &amp;quot;WI L. &amp;quot;, respectively. The semantic relations are &amp;quot;space at&amp;quot; and &amp;quot;object&amp;quot;, which are almost equal to &amp;quot;locative&amp;quot; and &amp;quot;objective&amp;quot; of Fillmore&apos;s deep casefFillmore]. The dotted line shows the word cor</context>
</contexts>
<marker>[Inoue]</marker>
<rawString>Inoue, N., et al. &amp;quot;Semantic Relations in ATR Linguistic Database&amp;quot; (in Japanese), ATR Technical Report TR-I-0029, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nakamura</author>
</authors>
<title>Automatic Analysis of Semantic Relation between English Nouns by an Ordinal English Dictionary&amp;quot;</title>
<date>1986</date>
<booktitle>(in Japanese), the Institute of Electronics, Information and Communication Engineers, Technical Report, NLC-86,</booktitle>
<contexts>
<context position="1002" citStr="[Nakamura]" startWordPosition="135" endWordPosition="135">rd in order to resolve the &amp;quot;polysemy&amp;quot; issue. The classification result demonstrates that our approach is successful. 1. INTRODUCTION Sets of semantically similar words are very useful in natural language processing. The general approach toward classifying words is to use semantic categories, for example the thesaurus. The &amp;quot;is-a&amp;quot; relation is connected between words and categories. However, it is not easy to acquire the &amp;quot;is-a&amp;quot; connection by hand, and it becomes expensive. Approaches toward automatically classifying words using existing dictionaries were therefore attempted[Chodorow] [Tsurumaru] [Nakamura]. These approaches are partially successful. However, there is a fatal problem in these approaches, namely, existing dictionaries, particularly Japanese dictionaries, are not assembled on the basis of semantic hierarchy. On the other hand, approaches toward automatically classifying words by using a large-scale corpus have also been attempted[Shirai][Hindle]. They seem to be based on the idea that semantically similar words appear in similar environments. This idea is derived from Harris&apos;s &amp;quot;distributional hypothesis&amp;quot;[Harris] in linguistics. Focusing on nouns, the idea claims that each noun is </context>
</contexts>
<marker>[Nakamura]</marker>
<rawString>Nakamura, J., et al. &amp;quot;Automatic Analysis of Semantic Relation between English Nouns by an Ordinal English Dictionary&amp;quot; (in Japanese), the Institute of Electronics, Information and Communication Engineers, Technical Report, NLC-86, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shirai</author>
</authors>
<title>Database Formulation and Learning Procedure for Kakariuke Dependency Analysis&amp;quot;</title>
<date>1985</date>
<journal>(in Japanese), Transactions of Information Processing Society of Japan,</journal>
<location>Vol.26, No.4,</location>
<contexts>
<context position="1354" citStr="[Shirai]" startWordPosition="182" endWordPosition="182">etween words and categories. However, it is not easy to acquire the &amp;quot;is-a&amp;quot; connection by hand, and it becomes expensive. Approaches toward automatically classifying words using existing dictionaries were therefore attempted[Chodorow] [Tsurumaru] [Nakamura]. These approaches are partially successful. However, there is a fatal problem in these approaches, namely, existing dictionaries, particularly Japanese dictionaries, are not assembled on the basis of semantic hierarchy. On the other hand, approaches toward automatically classifying words by using a large-scale corpus have also been attempted[Shirai][Hindle]. They seem to be based on the idea that semantically similar words appear in similar environments. This idea is derived from Harris&apos;s &amp;quot;distributional hypothesis&amp;quot;[Harris] in linguistics. Focusing on nouns, the idea claims that each noun is characterized by verbs with which it occurs, and also that nouns are similar to the extent that they share verbs. These automatic classification approaches are also partially successful. However, Hindle says that there is a number of issues to be confronted. The most important issue is that of &amp;quot;polysemy&amp;quot;. In Hindle&apos;s experiment, two senses of &amp;quot;table&amp;quot;</context>
<context position="9195" citStr="[Shirai]" startWordPosition="1449" endWordPosition="1449">ROVII:ht-paylobjectIl Wt -sendlobject12 7&apos;I2 I= 4 :/ Yl1t-issuelobjectI2 -be held&apos;objectI6 N. PI l Z -havelobject17 1* -takelobjectil ffr 1111 1* Z., -seelspace atI1 Figure 3 An example of semantic relations of nouns and word pairs 3.2 Semantic Distance of Nouns Our classification approach is based on the &amp;quot;distributional hypothesis&amp;quot;. Based on this semantic theory, nouns are similar to the extent that they share verb senses. The aim of this paper is to show the efficiency of using the word pair as the word sense. We therefore used the following expression(1), which was already defined by Shirai[Shirai] as the distance between two words. The 203 d(a,b) = EE4)(M(a,v,r),M(b,v,r)) v(V,r(R 1 (1) EE(M(a,v,r) + M(b,v,r)) vEVJER Here, a,b : noun (a,b E N) r: semantic relation v: verb senses N : the set of nouns V: the set of verb senses R: the set of semantic relations M(a,v,r) : the frequency of the semantic relation r between a and v 41(x,y) = fx + y (x &gt;0, y &gt;0) 1. (x =0 or y=0) second term of the expression can show the semantic similarity between two nouns, because it is the ratio of the verb senses with which both nouns (a and b) occur and all the verb senses with which each noun (a or b) occ</context>
</contexts>
<marker>[Shirai]</marker>
<rawString>Shirai K., et al. &amp;quot;Database Formulation and Learning Procedure for Kakariuke Dependency Analysis&amp;quot; (in Japanese), Transactions of Information Processing Society of Japan, Vol.26, No.4, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tsurumaru</author>
</authors>
<title>Automatic Extraction of Hierarchical Structure of Words from Definition Sentences&amp;quot;</title>
<date>1987</date>
<booktitle>(in Japanese), the Information Processing Society of Japan, Sig. Notes, 87-NL-64,</booktitle>
<contexts>
<context position="991" citStr="[Tsurumaru]" startWordPosition="134" endWordPosition="134"> the same word in order to resolve the &amp;quot;polysemy&amp;quot; issue. The classification result demonstrates that our approach is successful. 1. INTRODUCTION Sets of semantically similar words are very useful in natural language processing. The general approach toward classifying words is to use semantic categories, for example the thesaurus. The &amp;quot;is-a&amp;quot; relation is connected between words and categories. However, it is not easy to acquire the &amp;quot;is-a&amp;quot; connection by hand, and it becomes expensive. Approaches toward automatically classifying words using existing dictionaries were therefore attempted[Chodorow] [Tsurumaru] [Nakamura]. These approaches are partially successful. However, there is a fatal problem in these approaches, namely, existing dictionaries, particularly Japanese dictionaries, are not assembled on the basis of semantic hierarchy. On the other hand, approaches toward automatically classifying words by using a large-scale corpus have also been attempted[Shirai][Hindle]. They seem to be based on the idea that semantically similar words appear in similar environments. This idea is derived from Harris&apos;s &amp;quot;distributional hypothesis&amp;quot;[Harris] in linguistics. Focusing on nouns, the idea claims that ea</context>
</contexts>
<marker>[Tsurumaru]</marker>
<rawString>Tsurumaru H., et al. &amp;quot;Automatic Extraction of Hierarchical Structure of Words from Definition Sentences&amp;quot; (in Japanese), the Information Processing Society of Japan, Sig. Notes, 87-NL-64, 1987.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>