<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9953205">
CODRA: A Novel Discriminative Framework
for Rhetorical Analysis
</title>
<author confidence="0.99496">
Shafiq Joty*
</author>
<affiliation confidence="0.974153">
Qatar Computing Research Institute
</affiliation>
<author confidence="0.990084">
Giuseppe Carenini**
</author>
<affiliation confidence="0.997091">
University of British Columbia
</affiliation>
<author confidence="0.995009">
Raymond T. Ng†
</author>
<affiliation confidence="0.996666">
University of British Columbia
</affiliation>
<bodyText confidence="0.966450066666667">
Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship
between them carries important information that allows the discourse to express a meaning as a
whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence
structure. In this article, we present CODRA— a COmplete probabilistic Discriminative
framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory,
which posits a tree representation of a discourse.
CODRA comprises a discourse segmenter and a discourse parser. First, the discourse
segmenter, which is based on a binary classifier, identifies the elementary discourse units in a
given text. Then the discourse parser builds a discourse tree by applying an optimal parsing
algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential
parsing and the other for multi-sentential parsing. We present two approaches to combine these
two stages of parsing effectively. By conducting a series of empirical evaluations over two
different data sets, we demonstrate that CODRA significantly outperforms the state-of-the-art,
often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated
by CODRA can potentially improve the accuracy even further.
</bodyText>
<sectionHeader confidence="0.996861" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.7137598">
A well-written text is not merely a sequence of independent and isolated sentences,
but instead a sequence of structured and related sentences, where the meaning of a
sentence relates to the previous and the following ones. In other words, a well-written
* Arabic Language Technologies, Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar.
E-mail: sjoty®qf.org.qa.
</bodyText>
<note confidence="0.550799571428571">
** Computer Science Department, University of British Columbia, Vancouver, BC, Canada, V6T 1Z4.
E-mail: carenini®cs.ubc.ca.
† Computer Science Department, University of British Columbia, Vancouver, BC, Canada, V6T 1Z4.
E-mail: rng®cs.ubc.ca.
Submission received: 11 May 2014; revised version received: 29 January 2015; accepted for publication:
18 March 2015.
doi:10.1162/COLI a 00226
</note>
<footnote confidence="0.799379333333333">
No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. Law.
</footnote>
<note confidence="0.778809">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.9929278125">
text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically
binds its clauses and sentences together to express a meaning as a whole. Rhetorical
analysis seeks to uncover this coherence structure underneath the text; this has been
shown to be beneficial for many Natural Language Processing (NLP) applications,
including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002;
Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad
et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al.
2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013),
information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and
question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be
useful for other discourse analysis tasks, including co-reference resolution using Veins
theory (Cristea, Ide, and Romary 1998).
Different formal theories of discourse have been proposed from different view-
points to describe the coherence structure of a text. For example, Martin (1992) and
Knott and Dale (1994) propose discourse relations based on the usage of discourse
connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Seg-
mented Discourse Representation Theory, which is driven by sentence semantics.
Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse struc-
ture. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is
perhaps the most influential theory of discourse in computational linguistics. Although
it was initially intended to be used in text generation, later it became popular as a frame-
work for parsing the structure of a text (Taboada and Mann 2006). RST represents texts
by labeled hierarchical structures, called Discourse Trees (DTs). For example, consider
the DT shown in Figure 1 for the following text:
But he added: “Some people use the purchasers’ index as a leading indicator, some use it as a
coincident indicator. But the thing it’s supposed to measure—manufacturing strength—it
missed altogether last month.”
The leaves of a DT correspond to contiguous atomic text spans, called elementary
discourse units (EDUs; six in the example). EDUs are clause-like units that serve as
building blocks. Adjacent EDUs are connected by coherence relations (e.g., Elaboration,
Contrast), forming larger discourse units (represented by internal nodes), which in turn
are also subject to this relation linking. Discourse units linked by a rhetorical relation are
</bodyText>
<figureCaption confidence="0.722869">
Figure 1
</figureCaption>
<bodyText confidence="0.550818333333333">
Discourse tree for two sentences in RST–DT. Each sentence contains three EDUs. Horizontal lines
indicate text segments; satellites are connected to their nuclei by curved arrows and two nuclei
are connected with straight lines.
</bodyText>
<figure confidence="0.959192857142857">
Attribution
But he
added:
(1)
Contrast
Contrast
Same-Unit
&amp;quot;Some people use the purchasers’
index as a leading indicator,
some use it as a
coincident indicator.
it missed altogether
last month.&amp;quot;
Elaboration
But the thing it’s
supposed to measure
-- manufacturing
strength --
(2) (3)
(6)
(4) (s)
</figure>
<page confidence="0.998123">
386
</page>
<note confidence="0.930851">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.9998307">
further distinguished based on their relative importance in the text: nuclei are the core
parts of the relation and satellites are peripheral or supportive ones. For example, in
Figure 1, Elaboration is a relation between a nucleus (EDU 4) and a satellite (EDU 5),
and Contrast is a relation between two nuclei (EDUs 2 and 3). Carlson, Marcu, and
Okurowski (2002) constructed the first large RST-annotated corpus (RST–DT) on Wall
Street Journal articles from the Penn Treebank. Whereas Mann and Thompson (1988) had
suggested about 25 relations, the RST–DT uses 53 mono-nuclear and 25 multi-nuclear
relations. The relations are grouped into 16 coarse-grained categories; see Carlson and
Marcu (2001) for a detailed description of the relations. Conventionally, rhetorical anal-
ysis in RST involves two subtasks: discourse segmentation is the task of breaking the
text into a sequence of EDUs, and discourse parsing is the task of linking the discourse
units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse
parsing and rhetorical parsing interchangeably.
While recent advances in automatic discourse segmentation have attained high
accuracies (an F-score of 90.5% reported by Fisher and Roark [2007]), discourse parsing
still poses significant challenges (Feng and Hirst 2012) and the performance of the exist-
ing discourse parsers (Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault
et al. 2010) is still considerably inferior compared with the human gold standard. Thus,
the impact of rhetorical structure in downstream NLP applications is still very limited.
The work we present in this article aims to reduce this performance gap and take
discourse parsing one step further. To this end, we address three key limitations of
existing discourse parsers.
First, existing discourse parsers typically model the structure and the labels of a
DT separately, and also do not take into account the sequential dependencies between
the DT constituents. However, for several NLP tasks, it has recently been shown that
joint models typically outperform independent or pipeline models (Murphy 2012, page
687). This is also supported in a recent study by Feng and Hirst (2012), in which the
performance of a greedy bottom–up discourse parser improved when sequential de-
pendencies were considered by using gold annotations for the neighboring (i.e., previous
and next) discourse units as contextual features in the parsing model. To address this
limitation of existing parsers, as the first contribution, we propose a novel discourse
parser based on probabilistic discriminative parsing models, expressed as Conditional
Random Fields (CRFs) (Sutton, McCallum, and Rohanimanesh 2007), to infer the proba-
bility of all possible DT constituents. The CRF models effectively represent the structure
and the label of a DT constituent jointly, and, whenever possible, capture the sequential
dependencies.
Second, existing discourse parsers typically apply greedy and sub-optimal parsing
algorithms to build a DT. To cope with this limitation, we use the inferred (posterior)
probabilities from our CRF parsing models in a probabilistic CKY-like bottom–up
parsing algorithm (Jurafsky and Martin 2008), which is non-greedy and optimal.
Furthermore, a simple modification of this parsing algorithm allows us to generate
k-best (i.e., the k highest probability) parse hypotheses for each input text that could
then be used in a reranker to improve over the initial ranking using additional (global)
features of the discourse tree as evidence, a strategy that has been successfully explored
in syntactic parsing (Charniak and Johnson 2005; Collins and Koo 2005).
Third, most of the existing discourse parsers do not discriminate between intra-
sentential parsing (i.e., building the DTs for the individual sentences) and multi-
sentential parsing (i.e., building the DT for the whole document). However, we argue
that distinguishing between these two parsing conditions can result in more effective
parsing. Two separate parsing models could exploit the fact that rhetorical relations
</bodyText>
<page confidence="0.99093">
387
</page>
<note confidence="0.840046">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.999973692307692">
are distributed differently intra-sententially versus multi-sententially. Also, they could
independently choose their own informative feature sets. As another key contribution of
our work, we devise two different parsing components: one for intra-sentential parsing,
the other for multi-sentential parsing. This provides for scalable, modular, and flexible
solutions that can exploit the strong correlation observed between the text structure (i.e.,
sentence boundaries) and the structure of the discourse tree.
In order to develop a complete and robust discourse parser, we combine our intra-
sentential and multi-sentential parsing components in two different ways. Because most
sentences have a well-formed discourse sub-tree in the full DT (e.g., the second sentence
in Figure 1), our first approach constructs a DT for every sentence using our intra-
sentential parser, and then runs the multi-sentential parser on the resulting sentence-
level DTs to build a complete DT for the whole document. However, this approach
would fail in those cases where discourse structures violate sentence boundaries, also
called “leaky” boundaries (Vliet and Redeker 2011). For example, consider the first
sentence in Figure 1. It does not have a well-formed discourse sub-tree because the
unit containing EDUs 2 and 3 merges with the next sentence and only then is the
resulting unit merged with EDU 1. Our second approach, in order to deal with these
leaky cases, builds sentence-level sub-trees by applying the intra-sentential parser on a
sliding window covering two adjacent sentences and by then consolidating the results
produced by overlapping windows. After that, the multi-sentential parser takes all these
sentence-level sub-trees and builds a full DT for the whole document.
Our discourse parser assumes that the input text has already been segmented
into elementary discourse units. As an additional contribution, we propose a novel
discriminative approach to discourse segmentation that not only achieves state-of-the-
art performance, but also reduces time and space complexities by using fewer features.
Notice that the combination of our segmenter with our parser forms a COmplete
probabilistic Discriminative framework for Rhetorical Analysis (CODRA).
Whereas previous systems have been tested on only one corpus, we evaluate our
framework on texts from two very different genres: news articles and instructional how-
to manuals. The results demonstrate that our approach to discourse parsing provides
consistent and statistically significant improvements over previous methods both at the
sentence level and at the document level. The performance of our final system compares
very favorably to the performance of state-of-the-art discourse parsers. Finally, the
oracle accuracy computed based on the k-best parse hypotheses generated by our parser
demonstrates that a reranker could potentially improve the accuracy further.
After discussing related work in Section 2, we present our rhetorical analysis frame-
work in Section 3. In Section 4, we describe our discourse parser. Then, in Section 5 we
present our discourse segmenter. The experiments and analysis of results are presented
in Section 6. Finally, we summarize our contributions with future directions in Section 7.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.999862428571428">
Rhetorical analysis has a long history—dating back to Mann and Thompson (1988),
when RST was initially proposed as a useful linguistic method for describing natural
texts, to more recent attempts to automatically extract the rhetorical structure of a
given text (Hernault et al. 2010). In this section, we provide a brief overview of the
computational approaches that follow RST as the theory of discourse, and that are
related to our work; see the survey by Stede (2011) for a broader overview that also
includes other theories of discourse.
</bodyText>
<page confidence="0.995973">
388
</page>
<note confidence="0.961783">
Joty, Carenini, and Ng CODRA
</note>
<subsectionHeader confidence="0.672392">
2.1 Unsupervised and Rule-Based Approaches
</subsectionHeader>
<bodyText confidence="0.999872911111111">
Although the most effective approaches to rhetorical analysis to date rely on supervised
machine learning methods trained on human-annotated data, unsupervised methods
have also been proposed, as they do not require human-annotated data and can be more
easily applied to new domains.
Often, discourse connectives like but, because, and although convey clear information
on the kind of relation linking the two text segments. In his early work, Marcu (2000a)
presented a shallow rule-based approach relying on discourse connectives (or cues) and
surface patterns. He used hand-coded rules, derived from an extensive corpus study, to
break the text into EDUs and to build DTs for sentences first, then for paragraphs,
and so on. Despite the fact that this work pioneered the field of rhetorical analysis,
it has many limitations. First, identifying discourse connectives is a difficult task on
its own, because (depending on the usage), the same phrase may or may not signal
a discourse relation (Pitler and Nenkova 2009). For example, but can either signal a
Contrast discourse relation or can simply perform non-discourse acts. Second, discourse
segmentation using only discourse connectives fails to attain high accuracy (Soricut and
Marcu 2003). Third, DT structures do not always correspond to paragraph structures;
for example, Sporleder and Lapata (2004) report that more than 20% of the paragraphs
in the RST–DT corpus (Carlson, Marcu, and Okurowski 2002) do not correspond to a
discourse unit in the DT. Fourth, discourse cues are sometimes ambiguous; for example,
but can signal Contrast, Antithesis and Concession, and so on.
Finally, a more serious problem with the rule-based approach is that often rhetorical
relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu
and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of
307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British
National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack
a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba
and Di-Eugenio 2009) report even higher figures: About 60% of discourse relations are
not explicitly signaled. Therefore, rather than relying on hand-coded rules based on
discourse cues and surface patterns, recent approaches use machine learning techniques
with a large set of informative features.
While some rhetorical relations need to be explicitly signaled by discourse cues
(e.g., Concession) and some do not (e.g., Background), there is a large middle ground
of relations that may be signaled or not. For these “middle ground” relations, can we
exploit features present in the signaled cases to automatically identify relations when
they are not explicitly signaled? The idea is to use unambiguous discourse cues (e.g.,
although for Contrast, for example for Elaboration) to automatically label a large corpus
with rhetorical relations that could then be used to train a supervised model.&apos;
A series of previous studies have explored this idea. Marcu and Echihabi (2002)
first attempted to identify four broad classes of relations: Contrast, Elaboration, Condition,
and Cause–Explanation–Evidence. They used a naive Bayes classifier based on word pairs
(w1, w2), where w1 occurs in the left segment, and w2 occurs in the right segment.
Sporleder and Lascarides (2005) included other features (e.g., words and their stems,
Part-of-Speech [POS] tags, positions, segment lengths) in a boosting-based classifier
(i.e., BoosTexter [Schapire and Singer 2000]) to further improve relation classification
accuracy. However, these studies evaluated classification performance on the instances
</bodyText>
<footnote confidence="0.910203">
1 We categorize this approach as unsupervised because it does not rely on human-annotated data.
</footnote>
<page confidence="0.991981">
389
</page>
<note confidence="0.840642">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.999934727272727">
where rhetorical relations were originally signaled (i.e., the discourse cues were artifi-
cially removed), and did not verify how well this approach performs on the instances
that are not originally signaled. Subsequent studies (Blair-Goldensohn, McKeown, and
Rambow 2007; Sporleder 2007; Sporleder and Lascarides 2008) confirm that classifiers
trained on instances stripped of their original discourse cues do not generalize well to
implicit cases because they are linguistically quite different.
Note that this approach to identifying discourse relations in the absence of manually
labeled data does not fully solve the parsing problem (i.e., building DTs); rather, it only
attempts to identify a small subset of coarser relations between two (flat) text segments
(i.e., a tagging problem). Arguably, to perform a complete rhetorical analysis, one needs
to use supervised machine learning techniques based on human-annotated data.
</bodyText>
<subsectionHeader confidence="0.988388">
2.2 Supervised Approaches
</subsectionHeader>
<bodyText confidence="0.999959545454545">
Marcu (1999) applies supervised machine learning techniques to build a discourse
segmenter and a shift–reduce discourse parser. Both the segmenter and the parser rely
on C4.5 decision tree classifiers (Poole and Mackworth 2010) to learn the rules auto-
matically from the data. The discourse segmenter mainly uses discourse cues, shallow-
syntactic (i.e., POS tags) and contextual features (i.e., neighboring words and their
POS tags). To learn the shift–reduce actions, the discourse parser encodes five types of
features: lexical (e.g., discourse cues), shallow-syntactic, textual similarity, operational
(previous n shift–reduce operations), and rhetorical sub-structural features. Despite the
fact that this work has pioneered many of today’s machine learning approaches to
discourse parsing, it has all the limitations mentioned in Section 1.
The work of Marcu (1999) is considerably improved by Soricut and Marcu (2003).
They present the publicly available SPADE system,Z which comes with probabilistic
models for discourse segmentation and sentence-level discourse parsing. Their segmen-
tation and parsing models are based on lexico-syntactic patterns (or features) extracted
from the lexicalized syntactic tree of a sentence. The discourse parser uses an optimal
parsing algorithm to find the most probable DT structure for a sentence. SPADE was
trained and tested on the RST–DT corpus. This work, by showing empirically the
connection between syntax and discourse structure at the sentence level, has greatly in-
fluenced all major contributions in this area ever since. However, it is limited in several
ways. First, SPADE does not produce a full-text (i.e., document-level) parse. Second, it
applies a generative parsing model based on only lexico-syntactic features, whereas dis-
criminative models are generally considered to be more accurate, and can incorporate
arbitrary features more effectively (Murphy 2012). Third, the parsing model makes an
independence assumption between the label and the structure of a DT constituent, and
it ignores the sequential and the hierarchical dependencies between the DT constituents.
Subsequent research addresses the question of how much syntax one really needs in
rhetorical analysis. Sporleder and Lapata (2005) focus on the discourse chunking prob-
lem, comprising two subtasks: discourse segmentation and (flat) nuclearity assignment.
They formulate discourse chunking in two alternative ways. First, one-step classifica-
tion, where the discourse chunker, a multi-class classifier, assigns to each token one of
the four labels: (1) B–NUC (beginning of a nucleus), (2) I–NUC (inside a nucleus), (3) B–
SAT (beginning of a satellite), and (4) I–SAT (inside a satellite). Therefore, this approach
performs discourse segmentation and nuclearity assignment simultaneously. Second,
</bodyText>
<footnote confidence="0.823484">
2 http://www.isi.edu/licensed-sw/spade/.
</footnote>
<page confidence="0.99002">
390
</page>
<note confidence="0.931286">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.999795063829787">
two-step classification, where in the first step, the discourse segmenter (a binary clas-
sifier) labels each token as either B (beginning of an EDU) or I (inside an EDU). Then,
in the second step, a nuclearity labeler (another binary classifier) assigns a nuclearity
status to each segment. The two-step approach avoids illegal chunk sequences like a
B–NUC followed by an I–SAT or a B–SAT followed by an I–NUC, and in this approach,
it is easier to incorporate sentence-level properties like the constraint that a sentence
must contain at least one nucleus. They examine whether shallow-syntactic features
(e.g., POS and phrase tags) would be sufficient for these purposes. The evaluation on
the RST–DT shows that the two-step approach outperforms the one-step approach, and
its performance is comparable to that of SPADE, which requires relatively expensive full
syntactic parses.
In follow–up work, Fisher and Roark (2007) demonstrate over 4% absolute perfor-
mance gain in discourse segmentation, by combining the features extracted from the
syntactic tree with the ones derived via POS tagging and shallow syntactic parsing
(i.e., chunking). Using quite a large number of features in a binary log-linear model,
they achieve state-of-the-art performance in discourse segmentation on the RST–DT
test set.
In a different approach, Regneri, Egg, and Koller (2008) propose to use Underspec-
ified Discourse Representation (UDR) as an intermediate representation for discourse
parsing. Underspecified representations offer a single compact representation to express
possible ambiguities in a linguistic structure, and have been primarily used to deal with
scope ambiguity in semantic structures (Reyle 1993; Egg, Koller, and Niehren 2001;
Althaus et al. 2003; Koller, Regneri, and Thater 2008). Assuming that a UDR of a DT
is already given in the form of a dominance graph (Althaus et al. 2003), Regneri, Egg,
and Koller (2008) convert it into a more expressive and complete UDR representation
called regular tree grammar (Koller, Regneri, and Thater 2008), for which efficient
algorithms (Knight and Graehl 2005) already exist to derive the best configuration
(i.e., the best discourse tree).
Hernault et al. (2010) present the publicly available HILDA system,3 which comes
with a discourse segmenter and a parser based on Support Vector Machines (SVMs).
The discourse segmenter is a binary SVM classifier that uses the same lexico-syntactic
features used in SPADE, but with more context (i.e., the lexico-syntactic features for
the previous two words and the following two words). The discourse parser itera-
tively uses two SVM classifiers in a pipeline to build a DT. In each iteration, a binary
classifier first decides which of the adjacent units to merge, then a multi-class classi-
fier connects the selected units with an appropriate relation label. Using this simple
method, they report promising results in document-level discourse parsing on the
RST–DT.
For a different genre, instructional texts, Subba and Di-Eugenio (2009) propose a
shift–reduce discourse parser that relies on a classifier for relation labeling. Their clas-
sifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a
large set of features including the linguistically rich compositional semantics coming from
a semantic parser. They demonstrate that including compositional semantics with other
features improves the performance of the classifier, thus, also improves the performance
of the parser.
Both HILDA and the ILP-based approach of Subba and Di-Eugenio (2009) are lim-
ited in several ways. First, they do not differentiate between intra- and multi-sentential
</bodyText>
<footnote confidence="0.771048">
3 http://nlp.prendingerlab.net/hilda/.
</footnote>
<page confidence="0.991682">
391
</page>
<note confidence="0.355166">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.999733906976744">
parsing, and both scenarios use a single uniform parsing model. Second, they take a
greedy (i.e., sub-optimal) approach to construct a DT. Third, they disregard sequential
dependencies between DT constituents. Furthermore, HILDA considers the structure
and the labels of a DT separately. Our discourse parser CODRA, as described in the
next section, addresses all these limitations.
More recent work than ours also attempts to address some of the above-mentioned
limitations of the existing discourse parsers. Similar to us, Feng and Hirst (2014) gen-
erate a document-level DT in two stages, where a multi-sentential parsing follows an
intra-sentential one. At each stage, they iteratively use two separate linear-chain CRFs
(Lafferty, McCallum, and Pereira 2001) in a cascade: one for predicting the presence of
rhetorical relations between adjacent discourse units in a sequence, and the other to
predict the relation label between the two most probable adjacent units to be merged
as selected by the previous CRF. While they use CRFs to take into account the sequen-
tial dependencies between DT constituents, they use them greedily during parsing to
achieve efficiency. They also propose a greedy post-editing step based on an additional
feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them
a significant gain in performance. In a different approach, Li et al. (2014) propose a
discourse-level dependency structure to capture direct relationships between EDUs
rather than deep hierarchical relationships. They first create a discourse dependency
treebank by converting the deep annotations in RST–DT to shallow head-dependent anno-
tations between EDUs. To find the dependency parse (i.e., an optimal spanning tree)
for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald
et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm
online learning framework (McDonald, Crammer, and Pereira 2005).
With the successful application of deep learning to numerous NLP problems in-
cluding syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b),
and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse
parsing also use deep neural networks (DNNs) and related feature representation meth-
ods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose
a recursive DNN for discourse parsing. However, as in Socher et al. (2013a, 2013b), word
vectors (i.e., embeddings) are not learned explicitly for the task, rather they are taken
from Collobert et al. (2011). Given the vectors of the words in an EDU, their model
first composes them hierarchically based on a syntactic parse tree to get the vector
representation for the EDU. Adjacent discourse units are then merged hierarchically
to get the vector representations for the higher order discourse units. In every step, the
merging is done using one binary (structure) and one multi-class (relation) classifier,
each having a three-layer neural network architecture. The cost function for training
the model is given by these two cascaded classifiers applied at different levels of the
DT. Similar to our method, they use the classifier probabilities in a CKY-like parsing
algorithm to find the global optimal DT. Finally, Ji and Eisenstein (2014) present a feature
representation learning method in a shift–reduce discourse parser (Marcu 1999). Unlike
DNNs, which learn non-linear feature transformations in a maximum likelihood model,
they learn linear transformations of features in a max margin classification model.
</bodyText>
<sectionHeader confidence="0.59663" genericHeader="method">
3. Overview of Our Rhetorical Analysis Framework
</sectionHeader>
<bodyText confidence="0.943476">
CODRA takes as input a raw text and produces a discourse tree that describes the
text in terms of coherence relations that hold between adjacent discourse units (i.e.,
clauses, sentences) in the text. An example DT generated by an online demo of CODRA
</bodyText>
<page confidence="0.995729">
392
</page>
<figure confidence="0.948468222222222">
Joty, Carenini, and Ng CODRA
Segmenter Parser
Segmentation
model
Sentences
segmented
into EDUs
Algorithm
model
Intra-sentential
parser
Algorithm
model
Multi-sentential
parser
Document-level
discourse tree
Document
</figure>
<figureCaption confidence="0.972422">
Figure 2
</figureCaption>
<bodyText confidence="0.98353534375">
CODRA architecture.
is shown in Appendix A.4 The color of a node represents its nuclearity status: blue
denoting nucleus and yellow denoting satellite. The demo also allows some useful user
interactions—for example, collapsing or expanding a node, highlighting an EDU, and
so on.5
CODRA follows a pipeline architecture, shown in Figure 2. Given a raw text, the
first task in the rhetorical analysis pipeline is to break the text into a sequence of EDUs
(i.e., discourse segmentation). Because it is taken for granted that sentence bound-
aries are also EDU boundaries (i.e., EDUs do not span across multiple sentences), the
discourse segmentation task boils down to finding EDU boundaries inside sentences.
CODRA uses a maximum entropy model for discourse segmentation (see Section 5).
Once the EDUs are identified, the discourse parsing problem is determining which
discourse units (EDUs or larger units) to relate (i.e., the structure), and what relations
(i.e., the labels) to use in the process of building the DT. Specifically, discourse parsing
requires: (1) a parsing model to explore the search space of possible structures and
labels for their nodes, and (2) a parsing algorithm for selecting the best parse tree(s)
among the candidates. A probabilistic parsing model like ours assigns a probability to
every possible DT. The parsing algorithm then picks the most probable DTs.
The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and
Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that
disregard the structural interdependencies between the DT constituents. However, we
hypothesize that, like syntactic parsing, discourse parsing is also a structured predic-
tion problem, which involves predicting multiple variables (i.e., the structure and the
relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012)
also found these interdependencies to be critical for parsing performance. To capture
the structural dependencies between the DT constituents, CODRA uses undirected
conditional graphical models (i.e., CRFs) as its parsing models.
To find the most probable DT, unlike most previous studies (Marcu 1999; Subba and
Di-Eugenio 2009; Hernault et al. 2010), which adopt a greedy solution, CODRA applies
an optimal CKY parsing algorithm to the inferred posterior probabilities (obtained from
the CRFs) of all possible DT constituents. Furthermore, the parsing algorithm allows
CODRA to generate a list of k-best parse hypotheses for a given text.
</bodyText>
<footnote confidence="0.997316333333333">
4 The demo of CODRA is available at http://109.228.0.153/Discourse Parser Demo/.
The source code of CODRA is available from http://alt.qcri.org/tools/.
5 The input text in the demo in Appendix A is taken from www.bbc.co.uk/news/world-asia-26106490.
</footnote>
<page confidence="0.997752">
393
</page>
<figure confidence="0.827755">
Computational Linguistics Volume 41, Number 3
</figure>
<figureCaption confidence="0.915895">
Figure 3
</figureCaption>
<bodyText confidence="0.993984533333334">
Distributions of the eight most frequent relations in intra-sentential and multi-sentential parsing
scenarios on the RST–DT training set.
Note that the way CRFs and CKY are used in CODRA is quite different from the
way they are used in syntactic parsing. For example, in the CRF-based constituency
parsing proposed by Finkel, Kleeman, and Manning (2008), the conditional probability
distribution of a parse tree given a sentence decomposes across factors defined over
productions, and the standard inside–outside algorithm is used for inference on possible
trees. In contrast, CODRA first uses the standard forward–backward algorithm in a
“fat” chain structured6 CRF (to be discussed in Section 4.1.1) to compute the posterior
probabilities of all possible DT constituents for a given text (i.e., EDUs); then it uses a
CKY parsing algorithm to combine those probabilities and find the most probable DT.
Another crucial question related to parsing models is whether to use a single model
or two different models for parsing at the sentence-level (i.e., intra-sentential) and at the
document-level (i.e., multi-sentential). A simple and straightforward strategy would
be to use a single unified parsing model for both intra- and multi-sentential parsing
without distinguishing the two cases, as was previously done (Marcu 1999; Subba and
Di-Eugenio 2009; Hernault et al. 2010). That approach has the advantages of making
the parsing process easier, and the model gets more data to learn from. However, for a
solution like ours, which tries to capture the interdependencies between constituents,
this would be problematic with respect to scalability and inappropriate because of two
modeling issues.
More specifically, for scalability note that the number of valid trees grows exponen-
tially with the number of EDUs in a document.&apos; Therefore, an exhaustive search over
all the valid DTs is often infeasible, even for relatively small documents.
For modeling, a single unified approach is inappropriate for two reasons. On the
one hand, it appears that discourse relations are distributed differently intra- versus
multi-sententially. For example, Figure 3 shows a comparison between the two dis-
tributions of the eight most frequent relations in the RST–DT training set. Notice that
Same–Unit is more frequent than Joint in the intra-sentential case, whereas Joint is more
frequent than Same–Unit in the multi-sentential case. Similarly, the relative distributions
</bodyText>
<footnote confidence="0.700115666666667">
6 By the term “fat” we refer to CRFs with multiple (interconnected) chains of output variables.
7 For n + 1 EDUs, the number of valid discourse tree structures (i.e., not counting possible variations in the
nuclearity and relation labels) is the Catalan number Cn.
</footnote>
<page confidence="0.996987">
394
</page>
<note confidence="0.667574">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.999541235294118">
of Background, Contrast, Cause, and Explanation are different in the two parsing scenarios.
On the other hand, different kinds of features are applicable and informative for intra-
versus multi-sentential parsing. For example, syntactic features like dominance sets
(Soricut and Marcu 2003) are extremely useful for parsing at the sentence-level, but
are not even applicable in the multi-sentential case. Likewise, lexical chain features
(Sporleder and Lapata 2004), which are useful for multi-sentential parsing, are not
applicable at the sentence level.
Based on these above observations, CODRA comprises two separate modules: an
intra-sentential parser and a multi-sentential parser, as shown in Figure 2. First, the
intra-sentential parser produces one or more discourse sub-trees for each sentence.
Then, the multi-sentential parser generates a full DT for the document from these
sub-trees. Both of our parsers have the same two components: a parsing model and a
parsing algorithm. Whereas the two parsing models are rather different, the same parsing
algorithm is shared by the two modules. Staging multi-sentential parsing on top of
intra-sentential parsing in this way allows CODRA to explicitly exploit the strong
correlation observed between the text structure and the DT structure, as explained in
detail in Section 4.3.
</bodyText>
<sectionHeader confidence="0.957381" genericHeader="method">
4. The Discourse Parser
</sectionHeader>
<bodyText confidence="0.9996426875">
Before describing the parsing models and the parsing algorithm of CODRA in detail,
we introduce some terminology that we will use throughout this article.
A DT can be formally represented as a set of constituents of the form R[i, m, j], where
i &lt; m &lt; j. This refers to a rhetorical relation R between the discourse unit containing
EDUs i through m and the discourse unit containing EDUs m+1 through j. For example,
the DT for the second sentence in Figure 1 can be represented as {Elaboration–NS[4,4,5],
Same–Unit–NN[4,5,6]}. Notice that in this representation, a relation R also specifies the
nuclearity status of the discourse units involved, which can be one of Nucleus–Satellite
(NS), Satellite–Nucleus (SN), or Nucleus–Nucleus (NN). Attaching nuclearity status to the
relations allows us to perform the two subtasks of discourse parsing, relation identifica-
tion and nuclearity assignment, simultaneously.
A common assumption made for generating DTs effectively is that they are binary
trees (Soricut and Marcu 2003; Hernault et al. 2010). That is, multi-nuclear relations (e.g.,
Joint, Same–Unit) involving more than two discourse units are mapped to a hierarchical
right-branching binary tree. For example, a flat Joint(e1,e2,e3,e4) (Figure 4a) is mapped
to a right-branching binary tree Joint(e1, Joint(e2, Joint(e3, e4))) (Figure 4b).
</bodyText>
<figureCaption confidence="0.60148">
Figure 4
</figureCaption>
<bodyText confidence="0.677045">
Multi-nuclear relation and its corresponding binary tree representation.
</bodyText>
<page confidence="0.994083">
395
</page>
<note confidence="0.375958">
Computational Linguistics Volume 41, Number 3
</note>
<subsectionHeader confidence="0.99691">
4.1 Parsing Models
</subsectionHeader>
<bodyText confidence="0.970682033333334">
As mentioned before, the job of the intra- and multi-sentential parsing models of
CODRA is to assign a probability to each of the constituents of all possible DTs at
the sentence level and at the document level, respectively. Formally, given the model
parameters  at a particular parsing scenario (i.e., sentence-level or document-level), for
each possible constituent R[i, m,j] in a candidate DT at that parsing scenario, the parsing
model estimates P(R[i, m, j]|), which specifies a joint distribution over the label R and
the structure [i, m,j] of the constituent. For example, when applied to the sentences in
Figure 1 separately, the intra-sentential parsing model (with learned parameters s)
estimates P(R[1,1, 2]|s), P(R[2, 2, 3]|s), P(R[1, 2,3]|s), and P(R[1,1, 3]|s) for the first
sentence, and P(R[4, 4, 5]|s), P(R[5, 5, 6]|s), P(R[4, 5, 6]|s), and P(R[4, 4, 6]|s) for the
second sentence, respectively, for all R ranging over the set of relations.
4.1.1 Intra-Sentential Parsing Model. Figure 5 shows the parsing model of CODRA for
intra-sentential parsing. The observed nodes Uj (at the bottom) in a sequence represent
the discourse units (EDUs or larger units). The first layer of hidden nodes are the struc-
ture nodes, where Sj  {0, 1} denotes whether two adjacent discourse units Uj−1 and Uj
should be connected or not. The second layer of hidden nodes are the relation nodes,
with Rj  {1... M} denoting the relation between two adjacent units Uj−1 and Uj, where
M is the total number of relations in the relation set. The connections between adjacent
nodes in a hidden layer encode sequential dependencies between the respective hidden
nodes, and can enforce constraints such as the fact that a node must have a unique
mother, namely, a Sj= 1 must not follow a Sj−1= 1. The connections between the two
hidden layers model the structure and the relation of DT constituents jointly.
Notice that the probabilistic graphical model shown in Figure 5 is a chain-structured
undirected graphical model (also known as Markov Random Field or MRF [Murphy
2012]) with two hidden layers, i.e., structure chain and relation chain. It becomes a
Dynamic Conditional Random Field (DCRF) (Sutton, McCallum, and Rohanimanesh
2007) when we directly model the hidden (output) variables by conditioning the clique
potentials (i.e., factors) on the observed (input) variables:
Figure 5
The intra-sentential parsing model of CODRA.
</bodyText>
<equation confidence="0.999831875">
P(R2:t, S2:t|x, s) = 1 t��-11 ϕ(Ri,Ri+1|x,s,r)iP(Si,Si+1|x,s,s)ω(Ri,Si|x,s,c) (1)
Z(x, s) 11
i=2
R R R R R
2 3 j t-1 t
S S S S S
2 3 j t-1
t
</equation>
<page confidence="0.270268">
U1
</page>
<figure confidence="0.641228555555556">
U U U U U
2 3 j t-1 t
Relation
sequence
Structure
sequence
Unit
sequence
at level i
</figure>
<page confidence="0.992174">
396
</page>
<note confidence="0.662981">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.9999334">
where {4)} and {*} are the factors over the edges of the relation and structure chains,
respectively, and {w} are the factors over the edges connecting the relation and struc-
ture nodes (i.e., between-chain edges). Here, x represents input features extracted from
the observed variables, s = [s,r, s,s, s,c] are model parameters, and Z(x, s) is the
partition function. We use the standard log-linear representation of the factors:
</bodyText>
<equation confidence="0.999864666666667">
4)(Ri, Ri+1|x, s,r) = exp(Ts,r f (Ri, Ri+1, x)) (2)
*(Si,Si+1|x,s,s) = exp(Ts,s f(Si,Si+1,x)) (3)
w(Ri,Si|x,s,c) = exp(Ts,c f(Ri, Si, x)) (4)
</equation>
<bodyText confidence="0.99994656">
where f (Y, Z, x) is a feature vector derived from the input features x and the local labels Y
and Z, and s,y is the corresponding weight vector—that is, s,r and s,s are the weight
vectors for the factors over the relation edges and the structure edges, respectively, and
s,c is the weight vector for the factors over the between-chain edges.
A DCRF is a generalization of linear-chain CRFs (Lafferty, McCallum, and Pereira
2001) to represent complex interactions between output variables (i.e., labels), such as
when performing multiple labeling tasks on the same sequence. Recently, there has been
an explosion of interest in CRFs for solving structured output classification problems,
with many successful applications in NLP including syntactic parsing (Finkel, Kleeman,
and Manning 2008), syntactic chunking (Sha and Pereira 2003), and discourse chunk-
ing (Ghosh et al. 2011) in accordance with the Penn Discourse Treebank (Prasad et al.
2008).
DCRFs, being a discriminative approach to sequence modeling, have several advan-
tages over their generative counterparts such as Hidden Markov Models (HMMs) and
MRFs, which first model the joint distribution p(y,x|), and then infer the conditional
distribution p(y|x, ). It has been advocated that discriminative models are generally
more accurate than generative ones because they do not “waste resources” modeling
complex distributions that are observed (i.e., p(x)); instead, they focus directly on mod-
eling what we care about, namely, the distribution of labels given the data (Murphy
2012).
Other key advantages include the ability to incorporate arbitrary overlapping local
and global features, and the ability to relax strong independence assumptions. Further-
more, CRFs surmount the label bias problem (Lafferty, McCallum, and Pereira 2001) of
the Maximum Entropy Markov Model (McCallum, Freitag, and Pereira 2000), which is
considered to be a discriminative version of the HMM.
</bodyText>
<subsubsectionHeader confidence="0.80957">
4.1.2 Training and Applying the Intra-Sentential Parsing Model. In order to obtain the
</subsubsectionHeader>
<bodyText confidence="0.991792888888889">
probability of the constituents of all candidate DTs for a sentence, CODRA applies the
intra-sentential parsing model (with learned parameters s) recursively to sequences
at different levels of the DT, and computes the posterior marginals over the relation–
structure pairs. It uses the standard forward–backward algorithm to compute the poste-
rior marginals. To illustrate the process, let us assume that the sentence contains four
EDUs, e1, · · · , e4 (see Figure 6). At the first (i.e., bottom) level of the DT, when all the
discourse units are EDUs, there is only one unit sequence (e1, e2, e3, e4) to which CODRA
applies the DCRF model. Figure 6a at the top left shows the corresponding DCRF
model. For this sequence it computes the posterior marginals P(R2, S2=1|e1, e2, e3, e4, s),
</bodyText>
<page confidence="0.986729">
397
</page>
<figure confidence="0.9849132">
Computational Linguistics Volume 41, Number 3
R2:4
S 2:4
(i) (ii) (iii)
(c)
(a)
e1 e e
2
R2 R3
S S3
2
3
e4
R4
S4
R3:4
S3:4
R4
S4
(iii)
(b)
e e
1
R2
S2
e3:4
2
S3:4
R3:4
(i) (ii)
e e e4 e
1:2 3 1
R 3
S3
R4
S4
S2:3
e
R2:3
2:3
R4
e4
S4
e1 e
2:4 e1:2 e3:4 e1:3 e4
</figure>
<figureCaption confidence="0.950164">
Figure 6
</figureCaption>
<bodyText confidence="0.967570266666667">
The intra-sentential parsing model is applied to (a) the only possible sequence at the first level,
(b) the three possible sequences at the second level, and (c) the three possible sequences at the
third level.
P(R3, S3=1|e1, e2, e3, e4, ©s), and P(R4, S4=1 |e1, e2, e3, e4, ©s) to obtain the probability of the
DT constituents R[1,1, 2], R[2, 2, 3], and R[3, 3, 4], respectively.
At the second level, there are three unit sequences: (e1:2, e3, e4), (e1,e2:3, e4), and
(e1,e2,e3:4). Figure 6b shows their corresponding DCRF models. Notice that each of these
sequences has a discourse unit that connects two EDUs, and the probability of this con-
nection has already been computed at the previous level. CODRA computes the poste-
rior marginals P(R3, S3=1|e1:2,e3,e4,©s), P(R2:3S2:3=1|e1,e2:3,e4,©s), P(R4, S4=1|e1,e2:3,e4,©s),
and P(R3:4,S3:4=1|e1,e2,e3:4,©s) from these three sequences, which correspond to the
probability of the constituents R[1, 2, 3], R[1,1,3], R[2, 3, 4], and R[2, 2, 4], respectively.
Similarly, it attains the probability of the constituents R[1,1,4], R[1,2,4], and R[1,3,4]
by computing their respective posterior marginals from the three sequences at the third
(i.e., top) level of the candidate DTs (see Figure 6c).
Algorithm 1 describes how CODRA generates the unit sequences at different
levels of the candidate DTs for a given number of EDUs in a sentence. Specifically,
to compute the probability of a DT constituent R[i,k,j], CODRA generates
sequences like (e1,· · · , ei−1, ei:k, ek+1:j, ej+1, · · · , e„) for 1  i  k &lt; j  n. However,
in doing so, it may generate some duplicate sequences. Clearly, the sequence
(e1, · · · , ei−1, ei:i, ei+1:j, ej+1, · · · , e„) for 1  i  k &lt; j &lt; n is already considered for
computing the probability of the constituent R[i + 1, j, j + 1]. Therefore, it is a duplicate
sequence that CODRA excludes from the list of sequences. The algorithm has a
complexity of O(n3), where n is the number of EDUs in the sentence.
Once CODRA acquires the probability of all possible intra-sentential DT con-
stituents, the discourse sub-trees for the sentences are built by applying an optimal
parsing algorithm (Section 4.2) using one of the methods described in Section 4.3.
Algorithm 1 is also used to generate sequences for training the model (i.e., learning
©s). For example, Figure 7 demonstrates how we generate the training instances (right)
from a gold DT with four EDUs (left). To find the relevant labels for the sequences
</bodyText>
<page confidence="0.984986">
398
</page>
<note confidence="0.66103">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.791241">
Algorithm 1 Generating unit sequences for a sentence with n EDUs.
Input: Sequence of EDUs: (e1, e2, · · · , en)
Output: List of sequences: L
</bodyText>
<equation confidence="0.801830076923077">
for i = 1 -+ n − 1 do // all possible starting positions for the subsequence
forj = i + 1 -+ n do // all possible ending positions for the subsequence
if j == n then // sequences at top and bottom levels
for k = i -+ j − 1 do // all possible cut points within the subsequence
L.append ((e1,··· , ei−1, ei:k, ek+1:j, ej+1,··· , en))
end
else // sequences at intermediate levels
for k = i + 1 -+ j − 1 do // cut points excluding duplicate sequences
L.append ((e1,··· , ei−1, ei:k, ek+1:j, ej+1,··· , en))
end
end
end
end
</equation>
<bodyText confidence="0.986765166666667">
generated by the algorithm, we consult the gold DT and see if two discourse units are
connected by a relation r (i.e., the corresponding labels are S = 1, R = r) or not (i.e.,
the corresponding labels are S = 0,R =NR). We train the model by maximizing the
conditional likelihood of the labels in each of these training examples (see Equation (1)).
4.1.3 Multi-Sentential Parsing Model. Given the discourse units (sub-trees) for all the
individual sentences in a document, a simple approach to build the DT of the document
would be to apply a new DCRF model, similar to the one in Figure 5 (with different
parameters), to all the possible sequences generated from these units by Algorithm 1 to
infer the probability of all possible higher-order (multi-sentential) constituents. How-
ever, the number of possible sequences and their length increase with the number of
sentences in a document. For example, assuming that each sentence has a well-formed
DT, for a document with n sentences, Algorithm 1 generates O(n3) sequences, where
</bodyText>
<figureCaption confidence="0.99062">
Figure 7
</figureCaption>
<bodyText confidence="0.929573">
A gold discourse tree (left), and the 7 training instances it generates (right). NR = No Relation.
</bodyText>
<page confidence="0.978977">
399
</page>
<note confidence="0.337575">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.9998346875">
the sequence at the bottom level has n units, each of the sequences at the second level
has n-1 units, and so on. Because the DCRF model in Figure 5 has a “fat” chain struc-
ture, one could use the forward–backward algorithm for exact inference in this model
(Murphy 2012). Forward–backward on a sequence containing T units costs O(TM2)
time, where M is the number of relations in our relation set. This makes the chain-
structured DCRF model impractical for multi-sentential parsing of long documents,
since learning requires running inference on every training sequence with an overall
time complexity of O(TM2n3) = O(M2n4) per document (Sutton and McCallum 2012).
To address this problem, we have developed a simplified parsing model for multi-
sentential parsing. Our model is shown in Figure 8. The two observed nodes Ut−1
and Ut are two adjacent (multi-sentential) discourse units. The (hidden) structure node
S  {0, 1} denotes whether the two discourse units should be linked or not. The other
hidden node R  {1... M} represents the relation between the two units. Notice that
similar to the model in Figure 5, this is also an undirected graphical model and becomes
a CRF model if we directly model the labels by conditioning the clique potential ϕ on
the input features x, derived from the observed variables:
</bodyText>
<equation confidence="0.999662666666667">
P(Rt, St|x, ©d) = 1
Z(x, ©d)ϕ(Rt, St|x, ©d) (5)
ϕ(Rt,St|x,©d) = exp(©Td f(Rt, St, x)) (6)
</equation>
<bodyText confidence="0.999973307692308">
where f (Rt, St, x) is a feature vector derived from the input features x and the labels Rt
and St, and ©d is the corresponding weight vector. Although this model is similar in
spirit to the parsing model in Figure 5, it now breaks the chain structure, which makes
the inference much faster (i.e., a complexity of O(M2)). Breaking the chain structure also
allows CODRA to balance the data for training (an equal number of instances with S=1
and S=0), which dramatically reduces the learning time of the model.
CODRA applies this parsing model to all possible adjacent units at all levels
in the multi-sentential case, and computes the posterior marginals of the relation–
structure pairs P(Rt, St=1|Ut−1, Ut, ©d) using the forward–backward algorithm to obtain
the probability of all possible DT constituents. Given the sentence-level discourse units,
Algorithm 2, which is a simplified variation of Algorithm 1, extracts all possible adjacent
discourse units for multi-sentential parsing. Similar to Algorithm 1, Algorithm 2 also
has a complexity of O(n3), where n is the number of sentence-level discourse units.
</bodyText>
<figureCaption confidence="0.794078">
Figure 8
</figureCaption>
<bodyText confidence="0.570107">
The multi-sentential parsing model of CODRA.
</bodyText>
<figure confidence="0.994854375">
Relation
Structure
Adjacent Units
at level i
U U
t-1 t
Rt
St
</figure>
<page confidence="0.966759">
400
</page>
<note confidence="0.608646">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.9485042">
Algorithm 2 Generating all possible adjacent discourse units at all levels of a document-
level discourse tree.
Input: Sequence of units: (U1, U2, · · · Un), where Ux[0]:= start EDU ID of unit x, and
Ux[1]:= end EDU ID of unit x.
Output: List of adjacent units: L
</bodyText>
<equation confidence="0.89423">
for i = 1 -+ n − 1 do all possible starting positions for the subsequence
forj = i + 1 n do all possible ending positions for the subsequence
for k = i j − 1 do all possible cut points within the subsequence
Left = Ui[0] : Uk[1]
Right = Uk+1[0] : Uj[1]
L.append ((Left, Right))
end
end
end
</equation>
<bodyText confidence="0.99648865625">
Both our intra- and multi-sentential parsing models are designed using MALLET’s
graphical model toolkit GRMM (McCallum 2002). In order to avoid overfitting, we
regularize the CRF models with l2 regularization and learn the model parameters using
the limited-memory BFGS (L-BFGS) fitting algorithm.
4.1.4 Features Used in the Parsing Models. Crucial to parsing performance is the set
of features used in the parsing models, as summarized in Table 1. We categorize
the features into seven groups and specify which groups are used in what parsing
model. Notice that some of the features are used in both models. Most of the features
have been explored in previous studies (e.g., Soricut and Marcu 2003; Sporleder and
Lapata 2005; Hernault et al. 2010). However, we improve some of these as explained
subsequently.
The features are extracted from two adjacent discourse units Ut−1 and Ut. Organiza-
tional features encode useful information about text organization as shown by duVerle
and Prendinger (2009). We measure the length of the discourse units as the number of
EDUs and tokens in it. However, in order to better adjust to the length variations, rather
than computing their absolute numbers in a unit, we choose to measure their relative
numbers with respect to their total numbers in the two units. For example, if the two
discourse units under consideration contain three EDUs in total, a unit containing two
of the EDUs will have a relative EDU number of 0.67. We also measure the distances of
the units in terms of the number of EDUs from the beginning and end of the sentence
(or text in the multi-sentential case). Text structural features capture the correlation
between text structure and rhetorical structure by counting the number of sentence and
paragraph boundaries in the discourse units.
Discourse cues (e.g., because, but), when present, signal rhetorical relations between
two text segments, and have been used as a primary source of information in earlier
studies (Knott and Dale 1994; Marcu 2000a). However, recent studies (Hernault et al.
2010; Biran and Rambow 2011) suggest that an empirically acquired lexical N-gram
dictionary is more effective than a fixed list of cue phrases, since this approach is domain
independent and capable of capturing non-lexical cues such as punctuation.
In order to build a lexical N-gram dictionary empirically from the training corpus,
we extract the first and last N tokens (NE11, 2,3}) of each discourse unit and rank them
according to their mutual information with the two labels, Structure (S) and Relation (R).
</bodyText>
<page confidence="0.9945">
401
</page>
<figure confidence="0.742955">
Computational Linguistics Volume 41, Number 3
Table 1
Features used in our intra- and multi-sentential parsing models.
8 Organizational features Intra &amp; Multi-Sentential
Number of EDUs in unit 1 (or unit 2).
Number of tokens in unit 1 (or unit 2).
Distance of unit 1 in EDUs to the beginning (or to the end).
Distance of unit 2 in EDUs to the beginning (or to the end).
4 Text structural features Multi-Sentential
Number of sentences in unit 1 (or unit 2).
Number of paragraphs in unit 1 (or unit 2).
8 N-gram features NE{1, 2, 3} Intra &amp; Multi-Sentential
Beginning (or end) lexical N-grams in unit 1.
Beginning (or end) lexical N-grams in unit 2.
Beginning (or end) POS N-grams in unit 1.
Beginning (or end) POS N-grams in unit 2.
5 Dominance set features Intra-Sentential
</figure>
<bodyText confidence="0.921015">
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two units.
</bodyText>
<sectionHeader confidence="0.446099" genericHeader="method">
9 Lexical chain features Multi-Sentential
</sectionHeader>
<bodyText confidence="0.970115">
Number of chains spanning unit 1 and unit 2.
Number of chains start in unit 1 and end in unit 2.
Number of chains start (or end) in unit 1 (or in unit 2).
Number of chains skipping both unit 1 and unit 2.
Number of chains skipping unit 1 (or unit 2).
</bodyText>
<sectionHeader confidence="0.810616" genericHeader="method">
2 Contextual features Intra &amp; Multi-Sentential
</sectionHeader>
<bodyText confidence="0.738396">
Previous and next feature vectors.
</bodyText>
<sectionHeader confidence="0.610562" genericHeader="method">
2 Sub-structural features Intra &amp; Multi-Sentential
</sectionHeader>
<bodyText confidence="0.897756333333333">
Root nodes of the left and right rhetorical sub-trees.
More specifically, given an N-gram x, we compute its conditional entropy H with respect
to S and R as follows:8
</bodyText>
<equation confidence="0.898296333333333">
� c(x, s, r)
H(S,R|x) _ − log (7)
scS rER c(x)
</equation>
<bodyText confidence="0.999622333333333">
where c(x) is the empirical count of N-gram x, and c(x, s, r) is the joint empirical count
of N-gram x with the labels s and r. This is in contrast to HILDA (Hernault et al. 2010),
which ranks the N-grams by their frequencies in the training corpus. However, Blitzer
</bodyText>
<page confidence="0.8529225">
8 The higher the conditional entropy, the lower the mutual information, and vice versa.
402
</page>
<note confidence="0.621522">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.9945742">
(2008) found mutual information to be more effective than frequency as a method for
feature selection. Intuitively, the most informative discourse cues are not only the most
frequent, but also the ones that are indicative of the labels in the training data. In
addition to the lexical N-grams we also encode the POS tags of the first and last N
tokens (NE{1, 2,3}) in a discourse unit as shallow-syntactic features in our models.
</bodyText>
<figureCaption confidence="0.811939">
Figure 9
</figureCaption>
<bodyText confidence="0.416451">
Dominance set features for intra-sentential discourse parsing.
</bodyText>
<page confidence="0.987133">
403
</page>
<note confidence="0.338195">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.999653210526316">
Lexico-syntactic features dominance sets extracted from the Discourse Segmented
Lexicalized Syntactic Tree (DS-LST) of a sentence have been shown to be extremely
effective for intra-sentential discourse parsing in SPADE (Soricut and Marcu 2003).
Figure 9a shows the DS-LST (i.e., lexicalized syntactic tree with EDUs identified) for
a sentence with three EDUs from the RST–DT corpus, and Figure 9b shows the corre-
sponding discourse tree. In a DS-LST, each EDU except the one with the root node must
have a head node NH that is attached to an attachment node NA residing in a separate
EDU. A dominance set D (shown at the bottom of Figure 9a) contains these attachment
points (shown in boxes) of the EDUs in a DS-LST. In addition to the syntactic and
lexical information of the head and attachment nodes, each element in the dominance
set also includes a dominance relationship between the EDUs involved; the EDU with
the attachment node dominates (represented by “&gt;”) the EDU with the head node.
Soricut and Marcu (2003) hypothesize that the dominance set (i.e., lexical heads,
syntactic labels, and dominance relationships) carries the most informative clues for
intra-sentential parsing. For instance, the dominance relationship between the EDUs in
our example sentence is 3 &gt; 1 &gt; 2, which favors the DT structure [1, 1, 2] over [2, 2, 3].
In order to extract dominance set features for two adjacent discourse units Ut−1 and Ut,
containing EDUs ei:j and ej+1:k, respectively, we first compute the dominance set from
the DS-LST of the sentence. We then extract the element from the set that holds across
the EDUs j and j + 1. In our example, for the two units, containing EDUs e1 and e2,
respectively, the relevant dominance set element is (1, efforts/NP)&gt;(2, to/S). We encode
the syntactic labels and lexical heads of NH and NA, and the dominance relationship as
features in our intra-sentential parsing model.
Lexical chains (Morris and Hirst 1991) are sequences of semantically related words
that can indicate topical boundaries in a text (Galley et al. 2003; Joty, Carenini, and Ng
2013). Features extracted from lexical chains are also shown to be useful for finding
paragraph-level discourse structure (Sporleder and Lapata 2004). For example, consider
the text with four paragraphs (P1 to P4) in Figure 10a. Now, let us assume that there is a
lexical chain that spans the whole text, skipping paragraphs P2 and P3, while a second
chain only spans P2 and P3. This situation makes it more likely that P2 and P3 should be
linked in the DT before either of them is linked with another paragraph. Therefore, the
DT structure in Figure 10b should be more likely than the structure in Figure 10c.
One challenge in computing lexical chains is that words can have multiple senses,
and semantic relationships depend on the sense rather than the word itself. Several
methods have been proposed to compute lexical chains (Barzilay and Elhadad 1997;
Hirst and St. Onge 1997; Silber and McCoy 2002; Galley and McKeown 2003). We follow
the state-of-the-art approach proposed by Galley and McKeown (2003), which extracts
lexical chains after performing Word Sense Disambiguation (WSD).
</bodyText>
<figure confidence="0.711753166666667">
P P P P P P P P
1 2 3 4 1 2 3 4
(b) (c)
P P P P
1 2 3 4
(a)
</figure>
<figureCaption confidence="0.912204">
Figure 10
</figureCaption>
<bodyText confidence="0.7827615">
Correlation between lexical chains and discourse structure. (a) Lexical chains spanning
paragraphs. (b) and (c) Two possible DT structures.
</bodyText>
<page confidence="0.996671">
404
</page>
<note confidence="0.849">
Joty, Carenini, and Ng CODRA
</note>
<figureCaption confidence="0.896769">
Figure 11
</figureCaption>
<figure confidence="0.63253">
Extracting lexical chains. (a) A Lexical Semantic Relatedness Graph (LSRG) for five noun-tokens.
(b) Resultant graph after performing WSD. The box at the bottom shows the lexical chains.
</figure>
<bodyText confidence="0.9999010625">
In the preprocessing step, we extract the nouns from the document and lemmatize
them using WordNet’s built-in morphy function (Fellbaum 1998). Then, by looking up
in WordNet we expand each noun to all of its senses, and build a Lexical Semantic
Relatedness Graph (LSRG) (Galley and McKeown 2003; Chali and Joty 2007). In an
LSRG, the nodes represent noun-tokens with their candidate senses, and the weighted
edges between senses of two different tokens represent one of the three semantic rela-
tions: repetition, synonym, and hypernym. For example, Figure 11a shows a partial LSRG,
where the token bank has two possible senses, namely, money bank and river bank. Using
the money bank sense, bank is connected with institution and company by hypernymy
relations (edges marked with H), and with another bank by a repetition relation (edges
marked with R). Similarly, using the river bank sense, it is connected with riverside by a
hypernymy relation and with bank by a repetition relation. Nouns that are not found in
WordNet are considered as proper nouns having only one sense, and are connected by
only repetition relations.
We use this LSRG first to perform WSD, then to construct lexical chains. For WSD,
the weights of all edges leaving the nodes under their different senses are summed
up and the one with the highest score is considered to be the right sense for the word-
token. For example, if repetition and synonymy are weighted equally, and hypernymy is
given half as much weight as either of them, the score of bank’s two senses are: 1 + 0.5 +
0.5 = 2 for the sense money bank and 1 + 0.5 = 1.5 for the sense river bank. Therefore, the
selected sense for bank in this context is river bank. In case of a tie, we select the sense
that is most frequent (i.e., the first sense in WordNet). Note that this approach to WSD
is different from that of Sporleder and Lapata (2004), which takes a greedy approach.
Finally, we prune the graph by only keeping the links that connect words with the
selected senses. At the end of the process, we are left with the edges that form the
actual lexical chains. For example, Figure 11b shows the result of pruning the graph
in Figure 11a. The lexical chains extracted from the pruned graph are shown in the box
at the bottom. Following Sporleder and Lapata (2004), for each chain element, we keep
track of the location (i.e., sentence ID) in the text where that element was found, and
exclude chains containing only one element. Given two discourse units, we count the
number of chains that: hit the two units, exclusively hit the two units, skip both units,
skip one of the units, start in a unit, and end in a unit.
</bodyText>
<figure confidence="0.99958752173913">
bank
company
bank
company
(a)
riverside
institution
bank
(bank, company, institution, bank)
(riverside)
institution
R
R
S
H
H
H
bank
H
H
H
riverside
(b)
</figure>
<page confidence="0.762472">
405
</page>
<note confidence="0.262901">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.999907634146342">
We also consider more contextual information by including the above features
computed for the neighboring adjacent discourse unit pairs in the current feature vector.
For example, the contextual features for units Ut−1 and Ut include the feature vector
computed from Ut−2 and Ut−1 and the feature vector computed from Ut and Ut+1.
We incorporate hierarchical dependencies between the constituents in a DT by rhetor-
ical sub-structural features. For two adjacent units Ut−1 and Ut, we extract the roots of
the two rhetorical sub-trees. For example, the root of the rhetorical sub-tree spanning
over EDUs e1:2 in Figure 9b is Elaboration–NS. However, extraction of these features
assumes the presence of labels for the sub-trees, which is not the case when we apply
the parser to a new text (sentence or document) in order to build its DT in a non-
greedy fashion. One way to deal with this is to loop twice through the parsing process
using two different parsing models—one trained with the complete feature set, and the
other trained without the sub-structural features. We first build an initial, sub-optimal
DT using the parsing model that is trained without the sub-structural features. This
intermediate DT will now provide labels for the sub-structures. Next we can build
a final, more accurate DT by using the complete parsing model. This idea of two-
pass discourse parsing, where the second pass performs post-editing using additional
features, has recently been adopted by Feng and Hirst (2014) in their greedy parser.
One could even continue doing post-editing multiple times until the DT converges.
However, this could be very time consuming as each post-editing pass requires: (1) ap-
plying the parsing model to every possible unit sequence and computing the posterior
marginals for all possible DT constituents, and (2) using the parsing algorithm to find
the most probable DT. Recall from our earlier discussion in Section 4.1.3 that for n
discourse units and M rhetorical relations, the first step requires O(M2n4) and O(M2n3)
for intra- and multi-sentential parsing, respectively; we will see in the next section that
the second step requires O(Mn3). In spite of the computational cost, the gain we attained
in the subsequent passes was not significant for our development set. Therefore, we
restrict our parser to only one-pass post-editing.
Note that in parsing models where the score (i.e., likelihood) of a parse tree de-
composes across local factors (e.g., the CRF-based syntactic parser of Finkel, Kleeman,
and Manning [2008]), it is possible to define a semiring using the factors and the local
scores (e.g., given by the inside algorithm). The CKY algorithm could then give the
optimal parse tree in a single post-editing pass (Smith 2011). However, because our
intra-sentential parsing model is designed to capture sequential dependencies between
DT constituents, the score of a DT does not directly decompose across factors over
discourse productions. Therefore, designing such a semiring was not possible in our
case.
In addition to these features, we also experimented with other features including
WordNet-based lexical semantics, subjectivity, and TF.IDF-based cosine similarity. However,
because such features did not improve parsing performance on our development set,
they were excluded from our final set of features.
</bodyText>
<subsectionHeader confidence="0.993962">
4.2 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.9997194">
The intra- and multi-sentential parsing models of CODRA assign a probability to every
possible DT constituent in their respective parsing scenarios. The job of the parsing
algorithm is then to find the k most probable DTs for a given text. We implement a
probabilistic CKY-like bottom–up parsing algorithm that uses dynamic programming
to compute the most likely parses (Jurafsky and Martin 2008). For simplicity, we first
</bodyText>
<page confidence="0.994068">
406
</page>
<note confidence="0.831722">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.999850333333333">
describe the specific case of generating the single most probable DT, then we describe
how to generalize this algorithm to produce the k most probable DTs for a given text.
Formally, the search problem for finding the most probable DT can be written as
</bodyText>
<equation confidence="0.971029">
DT* = argmax P(DT|©) (8)
DT
</equation>
<bodyText confidence="0.999981">
where © specifies the parameters of the parsing model (intra- or multi-sentential). Given
n discourse units, our parsing algorithm uses the upper-triangular portion of the nxn
dynamic programming table D, where cell D[i, j] (for i &lt; j) stores:
</bodyText>
<equation confidence="0.997794">
D[i,j] = P(r*[Ui(0), Um*(1), Uj(1)]) (9)
</equation>
<bodyText confidence="0.850907">
where Ux(0) and Ux(1) are the start and end EDU Ids of discourse unit Ux, and
</bodyText>
<equation confidence="0.9888815">
(m*, r*) = argmax P(R[Ui(0), Um(1), Uj(1)]) x D[i,m] x D[m + 1,j] (10)
i&lt;m&lt;j ; RE{1···M}
</equation>
<bodyText confidence="0.999808125">
Recall that the notation R[Ui(0), Um(1), Uj(1)] in this expression refers to a rhetorical
relation R that holds between the discourse unit containing EDUs Ui(0) through Um(1)
and the unit containing EDUs Um(1) + 1 through Uj(1).
In addition to D, which stores the probability of the most probable constituents of a
DT, the algorithm also simultaneously maintains two other nxn dynamic programming
tables S and R for storing the structure (i.e., Um.(1)) and the relations (i.e., r*) of the
corresponding DT constituents, respectively. For example, given four EDUs e1 · · · e4, the
S and R dynamic programming tables at the left side in Figure 12 together represent
the DT shown at the right. More specifically, to find the DT, we first look at the top-right
entries in the two tables, and find S[1, 4] = 2 and R[1, 4] = r2, which specify that the two
discourse units e1.2 and e3.4 should be connected by the relation r2 (the root in the DT).
Then, we see how EDUs e1 and e2 should be connected by looking at the entries S[1,2]
and R[1,2], and find S[1,2] = 1 and R[1,2] = r1, which indicates that these two units
should be connected by the relation r1 (the left pre-terminal in the DT). Finally, to see
how EDUs e3 and e4 should be linked, we look at the entries S[3,4] and R[3,4], which tell
us that they should be linked by the relation r4 (the right pre-terminal). The algorithm
</bodyText>
<figureCaption confidence="0.54802">
Figure 12
</figureCaption>
<bodyText confidence="0.392672">
The S and R dynamic programming tables (left), and the corresponding discourse tree (right).
</bodyText>
<page confidence="0.984496">
407
</page>
<note confidence="0.552472">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.9990809375">
works in polynomial time. Specifically, for n discourse units and M number of relations,
the time and space complexities are O(n3M) and O(n2), respectively.
A key advantage of using a probabilistic parsing algorithm like the one we use is
that it allows us to generate a list of k most probable parse trees. It is straightforward
to generalize the above algorithm to produce k most probable DTs. Specifically, when
filling up the dynamic programming tables, rather than storing a single best parse for
each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates
simultaneously. One can show that the time and space complexities of the k-best version
of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005).
Note that, in contrast to other document-level discourse parsers (Marcu 2000b;
Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which
use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This
approach of CODRA is also different from the sentence-level discourse parser SPADE
(Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal,
then it assigns the most probable relations to the internal nodes. More specifically, the
cell D[i, j] in SPADE’s dynamic programming table stores
</bodyText>
<equation confidence="0.993275">
D[i,j] = P([Ui(0), Um.(1), Uj(1)]) (11)
</equation>
<bodyText confidence="0.921843">
where m* = argmax P([Ui(0), Um(1), Uj(1)]). Disregarding the relation label R while
i&lt;m&lt;j
populating D, this approach may find a discourse tree that is not globally optimal.
</bodyText>
<subsectionHeader confidence="0.996865">
4.3 Document-Level Parsing Approaches
</subsectionHeader>
<bodyText confidence="0.999383375">
Now that we have presented our intra-sentential and multi-sentential parsing com-
ponents, we are ready to describe how they can be effectively combined in a uni-
fied framework (Figure 2) to perform document-level rhetorical analysis. Recall that
a key motivation for a two-stage10 parsing is that it allows us to capture the strong
correlation between text structure and discourse structure in a scalable, modular, and
flexible way. In the following, we describe two different approaches to model this
correlation.
4.3.1 1S–1S (1 Sentence–1 Sub-tree). A key finding from previous studies on sentence-
level discourse analysis is that most sentences have a well-formed discourse sub-tree
in the full document-level DT (Soricut and Marcu 2003; Fisher and Roark 2007). For
example, Figure 13a shows 10 EDUs in three sentences (see boxes), where the DTs for
the sentences obey their respective sentence boundaries.
Our first approach, called 1S–1S (1 Sentence–1 Sub-tree), aims to maximally exploit
this finding. It first constructs a DT for every sentence using our intra-sentential parser,
and then it provides our multi-sentential parser with the sentence-level DTs to build the
rhetorical parse for the whole document.
</bodyText>
<footnote confidence="0.6901164">
4.3.2 Sliding Window. Although the assumption made by 1S–1S clearly simplifies the
parsing process, it completely ignores the cases where rhetorical structures violate
9 We agree that with potentially sub-optimal, sub-structural features in the parsing model, CKY may end
up finding a sub-optimal DT. But that is a separate issue.
10 Do not confuse the term two-stage with the term two-pass.
</footnote>
<page confidence="0.981182">
408
</page>
<figure confidence="0.985239047619047">
Joty, Carenini, and Ng CODRA
1 2 3
S1
4 5 6 7
S
2
8 9 10
S
3
1 2 3
S1
?
4 5 6 7
? ?
S
2
8 9 10
S
3
(b)
(a)
</figure>
<figureCaption confidence="0.987684">
Figure 13
</figureCaption>
<bodyText confidence="0.9966154">
Two possible DTs for three sentences.
sentence boundaries. For example, in the DT shown in Figure 13b, sentence S2 does
not have a well-formed sub-tree because some of its units attach to the left (i.e., 4–5
and 6) and some to the right (i.e., 7). Vliet and Redeker (2011) call these cases “leaky”
boundaries.
Although we find fewer than 5% of the sentences in the RST–DT have leaky
boundaries, in other corpora this can be true for a larger portion of the sentences.
For example, we observe that over 12% of the sentences in the instructional corpus of
Subba and Di-Eugenio (2009) have leaky boundaries. However, we notice that in most
cases where DT structures violate sentence boundaries, its units are merged with the
units of its adjacent sentences, as in Figure 13b. For example, this is true for 75% of
the leaky cases in our development set containing 20 news articles from the RST–DT
and for 79% of the leaky cases in our development set containing 20 how-to manuals
from the instructional corpus. Based on this observation, we propose a sliding window
approach.
In this approach, our intra-sentential parser works with a window of two consec-
utive sentences, and builds a DT for the two sentences. For example, given the three
sentences in Figure 13, our intra-sentential parser constructs a DT for S1–S2 and a DT
for S2–S3. In this process, each sentence in a document except the boundary sentences
(i.e., the first and the last) will be associated with two DTs: one with the previous
sentence (say, DTp) and one with the next (say, DTn). In other words, for each non-
boundary sentence, we will have two decisions: one from DTp and one from DTn.
Our parser consolidates the two decisions and generates one or more sub-trees for
each sentence by checking the following three mutually exclusive conditions one after
another:
</bodyText>
<listItem confidence="0.959753111111111">
• Same in both: If the sentence under consideration has the same (in both
structure and labels) well-formed sub-tree in both DTp and DTn, we take
this sub-tree. For example, in Figure 14a, S2 has the same sub-tree in the
two DTs (one for S1–S2 and one for S2–S3). The two decisions agree on the
DT for the sentence.
• Different but no cross: If the sentence under consideration has a well-
formed sub-tree in both DTp and DTn, but the two sub-trees vary either
in structure or in labels, we pick the most probable one. For example,
consider the DT for S1–S2 (at the left) in Figure 14a and the DT for S2–S3
</listItem>
<page confidence="0.991173">
409
</page>
<figure confidence="0.767714">
Computational Linguistics Volume 41, Number 3
</figure>
<figureCaption confidence="0.961535">
Figure 14
</figureCaption>
<bodyText confidence="0.92187575">
Extracting sub-trees for S2.
in Figure 14b. In both cases S2 has a well-formed sub-tree, but they differ
in structure. We pick the sub-tree which has the higher probability in the
two dynamic programming tables.
</bodyText>
<listItem confidence="0.973734">
• Cross: If either or both of DTP and DT„ segment the sentence into multiple
</listItem>
<bodyText confidence="0.866117428571428">
sub-trees, we pick the one having more sub-trees. For example, consider
the two DTs in Figure 14c. In the DT for S1–S2 on the left, S2 has three
sub-trees (4–5, 6, 7), whereas in the DT for S2–S3 on the right, it has two
(4–6, 7). So, we extract the three sub-trees for S2 from the first DT. If the
sentence has the same number of sub-trees in both DTP and DT,,, we pick
the one with higher probability in the dynamic programming tables. Note
that our choice of picking the DT with more sub-trees is intended to allow
the parser to find more leaky cases. However, other heuristics are also
possible. For example, another simple heuristic that one could try is: When
both DTs segment the sentence into multiple sub-trees, pick the one with
fewer sub-trees, and when only one of the DTs segment the sentence into
multiple sub-trees, pick that one.
At the end, the multi-sentential parser takes all these sentence-level sub-trees for a
document, and builds a full rhetorical parse for the whole document.
</bodyText>
<sectionHeader confidence="0.986419" genericHeader="method">
5. The Discourse Segmenter
</sectionHeader>
<bodyText confidence="0.998574333333333">
Our discourse parser assumes that the input text has been already segmented into a
sequence of EDUs. However, discourse segmentation is also a challenging problem,
and previous studies (Soricut and Marcu 2003; Fisher and Roark 2007) have identified
</bodyText>
<figure confidence="0.999809444444445">
(a)
(b)
S
(i)
S1
S2
S3
(c)
2
(ii)
1 2 3
4 5 6 7
4 5 6 7
8 9 10
4 5 6 7
8 9 10
S1
S2
S2
S
3
S2
S3
1 2 3
4 5 6 7
4 5 6 7
8 9 10
</figure>
<page confidence="0.986024">
410
</page>
<note confidence="0.82646">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.999919142857143">
it as a primary source of inaccuracy for discourse parsing. Regardless of its importance
in discourse parsing, discourse segmentation itself can be useful in several NLP ap-
plications, including sentence compression (Sporleder and Lapata 2005) and textual
alignment in statistical machine translation (Stede 2011). Therefore, in CODRA, we
have developed our own discourse segmenter, which not only achieves state-of-the-
art performance as shown later, but also reduces the time complexity by using fewer
features.
</bodyText>
<subsectionHeader confidence="0.935181">
5.1 Segmentation Model
</subsectionHeader>
<bodyText confidence="0.999986">
The discourse segmenter in CODRA implements a binary classifier to decide for each
word–token (except the last) in a sentence, whether to place an EDU boundary after
that token. We use a maximum entropy model to build a discriminative classifier. More
specifically, we use a Logistic Regression classifier with parameter ©:
</bodyText>
<equation confidence="0.992962">
P(yjw, ©) = Ber(yj Sigm(©Tx)) (12)
</equation>
<bodyText confidence="0.999862">
where the output y E 10, 1} denotes whether to put an EDU boundary (i.e., y = 1) or
not (i.e., y = 0) after the word–token w, which is represented by a feature vector x. In
the equation, Ber(rj) and Sigm(rj) refer to the Bernoulli distribution and the Sigmoid (also
known as logistic) function, respectively. The negative log-likelihood (NLL) of the model
with l2 regularization for N data points (i.e., word–tokens) is given by
</bodyText>
<equation confidence="0.997110666666667">
N
NLL(θ) = − yi log Sigm(©Txi) + (1 − yi) log (1 − Sigm(©Txi)) + A©T© (13)
i=1
</equation>
<bodyText confidence="0.999681571428572">
where yi is the gold label for word–token wi (represented by feature vector xi). We learn
the model parameters © using the L-BFGS fitting algorithm, which is time- and space-
efficient. To avoid overfitting, we use 5-fold cross validation to learn the regularization
strength parameter A from the training data. We also use a simple bagging technique
(Breiman 1996) to deal with the sparsity of boundary (i.e., y = 1) tags.
Note that our first attempt at the discourse segmentation task implemented a linear-
chain CRF model (Lafferty, McCallum, and Pereira 2001) to capture the sequential
dependencies between the tags in a discriminative way. However, the binary Logistic
Regression classifier, using the same set of features, not only outperforms the CRF
model, but also reduces time and space complexity. One possible explanation for the
low performance of the CRF model is that Markov dependencies between tags cannot
be effectively captured due to the sparsity of boundary tags. Also, because we could
not balance the data by using techniques like bagging in the CRF model, this further
degrades the performance.
</bodyText>
<subsectionHeader confidence="0.996324">
5.2 Features Used in the Segmentation Model
</subsectionHeader>
<bodyText confidence="0.99937025">
Our set of features for discourse segmentation are mostly inspired from previous studies
but used in a novel way, as we describe here.
Our first subset of features, which we call SPADE features, includes the lexico-
syntactic patterns extracted from the lexicalized syntactic tree of the given sentence.
</bodyText>
<page confidence="0.989267">
411
</page>
<note confidence="0.554859">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.999479523809524">
These features replicate the features used in SPADE’s segmenter, but used in a discrim-
inative way. In order to decide on an EDU boundary after a word–token wk, we search
for the lowest constituent in the lexicalized syntactic tree that spans over tokens wi ... wj
such that i &lt; k &lt; j. The production that expands this constituent in the tree, with the
potential EDU boundary marked, forms the primary feature. For instance, to determine
the existence of an EDU boundary after the word efforts in our sample sentence shown in
Figure 9, the production NP(efforts) -+ PRP$(its) NNS(efforts) f S(to) extracted from the
lexicalized syntactic tree in Figure 9a constitutes the primary feature, where f denotes
the potential EDU boundary.
SPADE predicts an EDU boundary if the relative frequency (i.e., maximum like-
lihood estimate) of a potential boundary given the production in the training data
is greater than 0.5. If the production has not been observed frequently enough, the
unlexicalized version of the production (e.g., NP -+ PRP$ NNS f S) is used for pre-
diction. If the unlexicalized version is also found to be rare, other variations of the
production, depending on whether they include the lexical heads and how many
non-terminals (one or two) they consider before and after the potential boundary, are
examined one after another (see Fisher and Roark [2007] for details). In contrast, we
compute the maximum likelihood estimates for a primary production (feature) and
its other variations, and use those directly as features with/without binarizing the
values.
Shallow syntactic features like Chunk and POS tags have been shown to possess
valuable clues for discourse segmentation (Fisher and Roark 2007; Sporleder and Lapata
2005). For example, it is less likely that an EDU boundary occurs within a chunk. We
annotate the tokens of a sentence with chunk and POS tags using the state-of-the-art
Illinois taggerll and encode these as features in our model. Note that the chunker
assigns each token a tag using the BIO notation, where B stands for beginning of a par-
ticular phrase (e.g., noun or verb phrase), I stands for inside of a particular phrase, and
O stands for outside of a particular phrase. The rationale for using the Illinois chunker
is that it uses a larger set of tags (23 in total); thus it is more informative than most of
the other existing taggers, which typically use only five tags (B–NP, I–NP, B–VP, I–VP,
and O).
EDUs are normally multi-word strings. Thus, a token near the beginning or end of
a sentence is unlikely to be the end of a segment. Therefore, for each token we include
its relative position (i.e., absolute position/total number of tokens) in the sentence and
distances to the beginning and end of the sentence as features.
It is unlikely that two consecutive tokens are tagged with EDU boundaries. There-
fore, we incorporate contextual information for a token into our model by including the
above features computed for its neighboring tokens.
We also experimented with different N-gram (N E 11,2,3}) features extracted
from the token sequence, POS sequence, and chunk sequence. However, because such
features did not improve segmentation accuracy on the development set, they were
excluded from our final set of features.
</bodyText>
<sectionHeader confidence="0.999228" genericHeader="evaluation">
6. Experiments
</sectionHeader>
<bodyText confidence="0.9956845">
In this section we present our experimental results. First, we describe the corpora on
which the experiments were performed and the evaluation metrics used to measure the
</bodyText>
<footnote confidence="0.731783">
11 Available athttp://cogcomp.cs.illinois.edu/page/software.
</footnote>
<page confidence="0.987856">
412
</page>
<note confidence="0.833017">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.9989235">
performance of the discourse segmenter and the parser. Then we show the performance
of our discourse segmenter, followed by the performance of our discourse parser.
</bodyText>
<subsectionHeader confidence="0.982695">
6.1 Corpora
</subsectionHeader>
<bodyText confidence="0.99993324">
Whereas previous studies on discourse analysis only report their results on a particular
corpus, to demonstrate the generality of our method, we experiment with texts from
two very different genres: news articles and instructional how-to manuals.
Our first corpus is the standard RST–DT (Carlson, Marcu, and Okurowski 2002),
which contains discourse annotations for 385 Wall Street Journal news articles taken from
the Penn Treebank corpus (Marcus, Marcinkiewicz, and Santorini 1994). The corpus
is partitioned into a training set of 347 documents and a test set of 38 documents. A
total of 53 documents selected from both training and test sets were annotated by two
human annotators. We measure human agreements based on this doubly annotated
data set. We used 25 documents from the training set as our development set. In RST–DT,
the original 25 rhetorical relations defined by Mann and Thompson (1988) are further
divided into a set of 18 coarser relation classes with 78 finer-grained relations (see
Carlson and Marcu [2001] for details). Our second corpus is the instructional corpus
prepared by Subba and Di-Eugenio (2009), which contains discourse annotations for
176 how-to manuals on home repair. The corpus was annotated with 26 informational
relations (e.g., Preparation–Act, Act–Goal).
For our experiments with the intra-sentential discourse parser, we extracted a
sentence-level DT from a document-level DT by finding the sub-tree that exactly spans
over the sentence. In RST–DT, by our count, 7,321 out of 7,673 sentences in the training
set, 951 out of 991 sentences in the test set, and 1,114 out of 1, 208 sentences in the
doubly-annotated set have a well-formed DT. On the other hand, 3,032 out of 3,430
sentences in the instructional corpus have a well-formed DT. This forms the corpora
for our experiments with intra-sentential discourse parsing. However, the existence of
a well-formed DT is not a necessity for discourse segmentation; therefore, we do not
exclude any sentence in our discourse segmentation experiments.
</bodyText>
<subsectionHeader confidence="0.997558">
6.2 Evaluation (and Agreement) Metrics
</subsectionHeader>
<bodyText confidence="0.998058692307692">
In this subsection we describe the metrics used to measure both how much the annota-
tors agree with each other, and how well the systems perform when their outputs are
compared with human annotations for the discourse analysis tasks.
6.2.1 Metrics for Discourse Segmentation. Because sentence boundaries are considered to
also be the EDU boundaries, we measure segmentation accuracy with respect to the
intra-sentential segment boundaries, which is a standard method (Soricut and Marcu
2003; Fisher and Roark 2007). Specifically, if a sentence contains n EDUs, which cor-
responds to n − 1 intra-sentential segment boundaries, we measure the segmenter’s
ability to correctly identify these n − 1 boundaries. Let h be the total number of intra-
sentential segment boundaries in the human annotation, m be the total number of intra-
sentential segment boundaries in the model output, and c be the total number of correct
segment boundaries in the model output. Then, we measure Precision (P), Recall (R),
and F-score for segmentation performance as follows:
</bodyText>
<equation confidence="0.998289">
P= cm, R = ch, and F−score = 2PR
P + R =2c (14)
h + m
</equation>
<page confidence="0.999124">
413
</page>
<figure confidence="0.750155666666667">
Computational Linguistics Volume 41, Number 3
Figure 15
A hypothetical system-generated DT for the two sentences in Figure 1.
</figure>
<figureCaption confidence="0.952947">
Figure 16
</figureCaption>
<bodyText confidence="0.967031045454545">
Measuring the accuracy of a discourse parser. (a) The human-annotated discourse tree. (b) The
system-generated discourse tree.
6.2.2 Metrics for Discourse Parsing. To evaluate parsing performance, we use the standard
unlabeled and labeled precision, recall, and F-score as proposed by Marcu (2000b). The
unlabeled metric measures how accurate the discourse parser is in finding the right
structure (i.e., the skeleton) of the DT, while the labeled metrics measure the parser’s
ability to find the right labels (i.e., nuclearity statuses or relation labels) in addition to
the right structure. Assume, for example, that given the two sentences of Figure 1, our
system generates the DT shown in Figure 15. In Figure 16, we show the same gold DT
shown in Figure 1 (on the left), and the same system-generated DT shown in Figure 15
(on the right), when the two trees are aligned. For the sake of illustration, instead of
showing the real EDUs, we only show their IDs. Notice that the automatic segmenter
made two mistakes: (1) it broke the EDU marked 2–3 (Some people use the purchasers’ index
as a leading indicator) in the human annotation into two separate EDUs, and (2) it could
not identify EDU 5 (But the thing it’s supposed to measure) and EDU 6 (— manufacturing
strength —) as two separate EDUs. Therefore, when we align the two annotations, we
obtain seven EDUs in total.
In Table 2, we list all constituents of the two DTs and their associated labels at the
span, nuclei, and relation levels. The recall (R) and precision (P) figures are shown at the
bottom of the table. Notice that, following (Marcu 2000b), the relation labels are assigned
to the children nodes rather than to the parent nodes in the evaluation process to deal
with non-binary trees in human annotations. To our knowledge, no implementation of
</bodyText>
<figure confidence="0.991997744186047">
some use it as a
coincident indicator.
1�)
as a leading
indicator,
Contrast
Attribution
Joint
But he
added:
1�)
Elaboration
&amp;quot;Some people use
the purchasers’ index
But the thing it’s supposed to
measure -- manuFacturing strength --
1�)
it missed altogether
lastmonth.&amp;quot;
1�)
Contrast
12) 13
Attribution
(1)
Contrast
Same-Unit
Contrast
(2-3) (4)
(7)
Elaboration
(5) (6)
(a)
(1)
Contrast
(5-6)
(7)
Contrast
Attribution
Joint
(2) (3)
Elaboration
(4)
(b)
</figure>
<page confidence="0.9925">
414
</page>
<note confidence="0.929533">
Joty, Carenini, and Ng CODRA
</note>
<tableCaption confidence="0.977054">
Table 2
</tableCaption>
<figure confidence="0.8702631875">
Measuring parsing accuracy (P = Precision, R = Recall).
Spans Nuclearity Relations
Constituents Human System Human System Human System
1–1 * * S S Attribution Attribution
2–2 * S Elaboration
3–3 * N Span
4–4 * * N N Contrast Contrast
5–5 * N Span
6–6 * S Elaboration
7–7 * * N N Same–Unit Joint
2–3 * * N N Contrast Contrast
5–6 * * N N Same–Unit Joint
2–4 * * S N Contrast Span
5–7 * * N N Contrast Contrast
1–4 * N Contrast
2–7 * N Span
</figure>
<equation confidence="0.548161">
R = 7/10, P = 7/10 R = 6/10, P = 6/10 R = 4/10, P = 4/10
</equation>
<bodyText confidence="0.9966135">
the evaluation metrics was made publicly available. Therefore, to help other researchers,
we have made our source code of the evaluation metrics publicly available.12
Given this evaluation setup, it is easy to understand that if the number of EDUs is
the same in the human and system annotations (e.g., when the discourse parser uses
gold discourse segmentation), and the discourse trees are binary, then we get the same
figures for precision, recall, and F-score.
</bodyText>
<subsectionHeader confidence="0.993044">
6.3 Discourse Segmentation Evaluation
</subsectionHeader>
<bodyText confidence="0.994001">
In this section we present our experiments on discourse segmentation.
</bodyText>
<subsubsectionHeader confidence="0.473915">
6.3.1 Experimental Set-up for Discourse Segmentation. We compare the performance of
</subsubsectionHeader>
<bodyText confidence="0.999888666666667">
our discourse segmenter with the performance of the two publicly available discourse
segmenters, namely, the discourse segmenters of the HILDA (Hernault et al. 2010) and
SPADE (Soricut and Marcu 2003) systems. We also compare our results with the state-
of-the-art results reported by Fisher and Roark (2007) on the RST–DT test set. In all
our experiments when comparing two systems, we use paired t-test on the F-scores to
measure statistical significance and report the p-value.
We ran HILDA with its default settings. For SPADE, we applied the same modifi-
cations to its default settings as described in Fisher and Roark (2007), which delivers
significantly improved performance over its original version. Specifically, in our exper-
iments on the RST–DT corpus, we trained SPADE using the human-annotated syntactic
trees extracted from the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1994),
and, during testing, we replaced the Charniak parser (Charniak 2000) with a more
</bodyText>
<page confidence="0.7712955">
12 Available from alt.qcri.org/tools/.
415
</page>
<note confidence="0.467266">
Computational Linguistics Volume 41, Number 3
</note>
<tableCaption confidence="0.990715">
Table 3
</tableCaption>
<bodyText confidence="0.991045416666667">
Discourse segmentation results of different models on the two corpora. Performances
significantly superior to SPADE are denoted by *.
accurate reranking parser (Charniak and Johnson 2005). However, because of the lack of
gold syntactic trees in the instructional corpus, we trained SPADE in this corpus using
the syntactic trees produced by the reranking parser. To avoid using the gold syntactic
trees, we used the reranking parser in our system for both training and testing purposes.
This syntactic parser was trained on the sections of the Penn Treebank not included in
our test set. We applied the same canonical lexical head projection rules (Magerman
1995; Collins 2003) to lexicalize the syntactic trees as done in HILDA and SPADE.
Note that previous studies (Fisher and Roark 2007; Soricut and Marcu 2003;
Hernault et al. 2010) on discourse segmentation only report their performance on the
RST–DT test set. To compare our results with them, we evaluated our model on the
RST–DT test set. In addition, we showed a more general performance of SPADE and our
system on the two corpora based on 10-fold cross validation.13 However, SPADE does
not come with a training module for its segmenter. We reimplemented this module and
verified its correctness by reproducing the results on the RST–DT test set.
6.3.2 Results for Discourse Segmentation. Table 3 shows the discourse segmentation results
of different systems in Precision, Recall, and F-score on the two corpora. On the RST–
DT corpus, HILDA’s segmenter delivers the weakest performance, having an F-score of
only 74.1. Note that the high segmentation accuracy reported by Hernault et al. (2010)
is due to a less stringent evaluation metric. SPADE performs much better than HILDA
with an absolute F-score improvement of 11.1%. Our segmenter DS outperforms SPADE
with an absolute F-score improvement of 4.9% (p-value &lt; 2.4e-06), and also achieves
comparable results to the ones of Fisher and Roark (2007) (F&amp;R), even though we use
fewer features.14 Notice that human agreement for this task is quite high—namely,
an F-score of 98.3 computed on the doubly-annotated portion of the RST–DT corpus
mentioned in Section 6.1.
Because Fisher and Roark (2007) only report their results on the RST–DT test
set and we did not have access to their system, we compare our approach only
with SPADE when evaluating on a whole corpus based on 10-fold cross validation.
On the RST–DT corpus, our segmenter delivers an absolute F-score improvement of
3.8 percentage points, which represents a more than 25% relative error rate reduction.
13 Because the two tasks—discourse segmentation and intra-sentential parsing—operate at the sentence
level, the cross validation was performed over sentences for their evaluation.
14 Because we did not have access to the system or to the complete output/results of Fisher and Roark
(2007), we were not able to perform a statistical significance test.
</bodyText>
<figure confidence="0.99876916">
Instructional
RST–DT
10-fold 10-fold
HILDA SPADE F&amp;R DS
Human
SPADE DS
SPADE DS
Precision
Recall
F-score
74.1 85.2 90.5* 90.1*
65.1 73.9*
82.8 89.7*
72.8 80.9*
98.5
83.7 87.5*
98.2
86.2 89.9*
98.3
84.9 88.7*
Standard Test Set
Doubly
10-fold
77.9 83.8 91.3* 88.0*
70.6 86.8 89.7* 92.3*
</figure>
<page confidence="0.997845">
416
</page>
<note confidence="0.805949">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.9999203">
The improvement is higher on the instructional corpus with an absolute F-score im-
provement of 8.1 percentage points, which corresponds to a relative error reduction
of 30%. The improvements for both corpora are statistically significant (p-value &lt;
3.0e-06). When we compare our results on the two corpora, we observe a substantial
decrease in performance on the instructional corpus. This could be because of a smaller
amount of data in this corpus and/or to the inaccuracies in the syntactic parser and
taggers, which are trained on news articles. A promising future direction would be to
apply effective domain adaptation methods (e.g., easyadapt [Daum´e 2007]) to improve
discourse segmentation performance in the instructional domain by leveraging the rich
data in the news domain (i.e., RST–DT).
</bodyText>
<subsectionHeader confidence="0.966198">
6.4 Discourse Parsing Evaluation
</subsectionHeader>
<bodyText confidence="0.995808366666667">
In this section we present our experiments on discourse parsing. First, we describe the
experimental set-up. Then, we present the results of the parsers. While presenting the
performance of our discourse parser, we show a breakdown of intra-sentential versus
inter-sentential results, in addition to the aggregated results at the document level.
6.4.1 Experimental Set-up for Discourse Parsing. In our experiments on sentence-level (i.e.,
intra-sentential) discourse parsing, we compare our approach with SPADE (Soricut and
Marcu 2003) on the RST–DT corpus, and with the ILP-based approach of Subba and
Di-Eugenio (2009) on the instructional corpus, because they are the state of the art in
their respective genres. For SPADE, we applied the same modifications to its default
settings as described in Section 6.3.1, which leads to improved performance. Similarly,
in our experiments on document-level (i.e., multi-sentential) parsing, we compare our
approach with HILDA (Hernault et al. 2010) on the RST–DT corpus, and with the ILP-
based approach (Subba and Di-Eugenio 2009) on the instructional corpus. The results
for HILDA were obtained by running the system with default settings on the same
inputs we provided to our system. Because we could not run the ILP-based system (not
publicly available), we report the performance presented in their paper.
Our experiments on the RST–DT corpus use the same 18 coarser coherence relations
(see Figure 18 later in this article), defined by Carlson and Marcu (2001) and also used
in SPADE and HILDA systems. More specifically, the relation set consists of 16 relation
categories and two pseudo-relations, namely, Textual–Organization and Same–Unit. After
attaching the nuclearity statuses (NS, SN, NN) to these relations, we obtain 41 distinct
relations.15 Our experiments on the instructional corpus consider the same 26 primary
relations (e.g., Goal:Act, Cause:Effect) used by Subba and Di-Eugenio (2009) and also treat
the reversals of non-commutative relations as separate relations. That is, Goal–Act and
Act–Goal are considered to be two different coherence relations. Attaching the nuclearity
statuses to these relations provides 76 distinct relations.
Based on our experiments on the development set, the size of the automatically built
bi-gram and tri-gram dictionaries was set to 95% of their total number of items, and
the size of the unigram dictionary was set to 100%. Note that the unigram dictionary
contains only special tags denoting EDU, sentence, and paragraph boundaries.
</bodyText>
<footnote confidence="0.7914235">
15 Not all relations take all the possible nuclearity statuses. For example, Elaboration and Attribution are
mono-nuclear relations, and Same–Unit and Joint are multi-nuclear relations.
</footnote>
<page confidence="0.987062">
417
</page>
<note confidence="0.524679">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.941392714285714">
6.4.2 Evaluation of the Intra-Sentential Discourse Parser. This section presents our ex-
perimental evaluation on intra-sentential discourse parsing. First, we show the
performance of the sentence-level parsers when they are provided with manual (or
gold) discourse segmentations. This allows us to judge the parsing performance
independently of the segmentation task. Then, we show the end-to-end performance of
our intra-sentential framework, that is, the intra-sentential parsing performance based
on automatic discourse segmentation.
Intra-sentential parsing results based on manual segmentation
Table 4 presents the intra-sentential discourse parsing results when manual discourse
segmentation is used. Recall from our discussion on evaluation metrics in Section 6.2.2
that precision, recall, and F-score are the same when manual segmentation is used.
Therefore, we report only one of them. Notice that our sentence-level discourse parser
PAR-S consistently outperforms SPADE on the RST–DT test set in all three metrics, and
the improvements are statistically significant (p-value &lt; 0.01). Especially, on the relation
labeling task, which is the hardest among the three tasks, we achieve an absolute F-score
improvement of 12.2 percentage points, which represents a relative error rate reduction
of 37.7%.
To verify our claim that capturing the sequential dependencies between DT con-
stituents using a DCRF model actually contributes to the performance gain, we also
compare our results with an intra-sentential parser (see CRF-NC in Table 4) that uses
a simplified CRF model similar to the one shown in Figure 8. Although the simpli-
fied model has two hidden variables to model the structure and the relation of a DT
constituent jointly, it does not have a chain structure, thus it ignores the sequential
dependencies between DT constituents. The comparison in all three measures demon-
strates that the improvements are indeed partly due to the DCRF model (p–value &lt;
0.01).16 A comparison between CRF-NC and SPADE shows that CRF-NC significantly
outperforms SPADE in all three measures (p-value &lt; 0.01). This could be due to the
fact that CRF-NC is trained discriminatively with a large number of features, whereas
SPADE is trained generatively with only lexico-syntactic features.
Notice that the scores of our parser (PAR-S) are close to the human agreement on
the doubly-annotated data, and these results on the RST–DT test set are also consistent
with the mean scores over 10-folds on the whole RST–DT corpus.17
The improvements are higher on the instructional corpus, where we compare our
mean results over 10-folds with the reported results of the ILP-based system of Subba
and Di-Eugenio (2009), giving absolute F-score improvements of 5.4 percentage points,
17.6 percentage points, and 12.8 percentage points in span, nuclearity, and relations,
respectively.18 Our parser PAR-S reduces the errors by 76.1%, 62.4%, and 34.6% in
span, nuclearity and relations, respectively.
If we compare the performance of our intra-sentential discourse parser on the two
corpora, we notice that our parser PAR-S is more accurate in finding the right tree
16 The parsing performance reported in Table 4 for CRF-NC is when the CRF parsing model is trained on a
balanced data set (an equal number of instances with S=1 and S=0); Training on full but imbalanced data
set gives slightly lower results.
17 Our EMNLP and ACL publications (Joty, Carenini, and Ng 2012; Joty et al. 2013) reported slightly lower
parsing accuracies. Fixing a bug in the parsing algorithm accounts for the difference.
18 Subba and Di-Eugenio (2009) report their results based on an arbitrary split between training and test
sets. Because we did not have access to their particular split, we compare our model’s performance based
on 10-fold cross validation with their reported results. Also, because we did not have access to their
system/output, we could not perform a significance test on the instructional corpus.
</bodyText>
<page confidence="0.995014">
418
</page>
<note confidence="0.941234">
Joty, Carenini, and Ng CODRA
</note>
<tableCaption confidence="0.933309666666667">
Table 4
Intra-sentential parsing results based on manual discourse segmentation. Performances
significantly superior to SPADE are denoted by *.
</tableCaption>
<table confidence="0.997297333333333">
RST–DT Instructional
Standard Test Set 10-fold Doubly Reported 10-fold
Scores SPADE CRF-NC PAR-S PAR-S Human ILP PAR-S
Span 93.5 95.1* 96.5* 95.4 95.7 92.9 98.3
Nuclearity 85.8 87.7* 89.4* 88.6 90.4 71.8 89.4
Relation 67.6 76.6* 79.8* 78.9 83.0 63.0 75.8
</table>
<bodyText confidence="0.973965086956522">
structure (see Span row in the table) on the instructional corpus. This may be due to the
fact that sentences in the instructional domain are relatively short and contain fewer
EDUs than sentences in the news domain, thus making it easier to find the right tree
structure. However, when we compare the performance on the relation labeling task,
we observe a decrease on the instructional corpus. This may be due to the small amount
of data available for training and the imbalanced distribution of a large number of
discourse relations (i.e., 76 with nuclearity attached) in this corpus.
Intra-sentential parsing results based on automatic segmentation
In order to evaluate the performance of the fully automatic sentence-level discourse
analysis systems, we feed the intra-sentential discourse parsers the output of their
respective discourse segmenters. Table 5 shows the (P)recision, (R)ecall, and (F)–score
results for different evaluation metrics. We compare our intra-sentential parser PAR-S
with SPADE on the RST–DT test set. We achieve absolute F-score improvements of
5.7 percentage points, 6.4 percentage points, and 9.5 percentage points in span, nu-
clearity, and relation, respectively. These improvements are statistically significant (p-
value&lt;0.001). Our system, therefore, reduces the errors by 24.5%, 21.4%, and 22.6% in
span, nuclearity, and relations, respectively. These results are also consistent with the
mean results over 10-folds on the whole RST–DT corpus.
The rightmost column in the table shows our mean results over 10-folds on the
instructional corpus. We could not compare our system with the ILP-based approach
of Subba and Di-Eugenio (2009) because no results were reported using an automatic
segmenter. It is interesting to observe how much our parser is affected by an automatic
segmenter on the two corpora (see Tables 4 and 5). Nevertheless, taking into account the
</bodyText>
<tableCaption confidence="0.768043666666667">
Table 5
Intra-sentential parsing results using automatic discourse segmentation. Performances
significantly superior to SPADE are denoted by *.
</tableCaption>
<table confidence="0.984535428571429">
RST–DT Instructional
Test set 10-fold 10-fold
Scores SPADE PAR-S PAR-S PAR-S
P R F P R F P R F P R F
Span 75.9 77.4 76.7 80.8* 84.0* 82.4* 79.6 80.7 80.1 73.5 80.7 76.9
Nuclearity 69.8 70.5 70.2 75.2* 78.1* 76.6* 73.9 76.5 75.2 64.6 71.0 67.6
Relation 57.4 58.5 58.0 66.1* 68.8* 67.5* 65.2 67.4 66.8 54.8 60.4 57.5
</table>
<page confidence="0.719527">
419
</page>
<note confidence="0.339136">
Computational Linguistics Volume 41, Number 3
</note>
<tableCaption confidence="0.853137">
Table 6
Parsing results of document-level parsers using manual segmentation. Performances
significantly superior to HILDA (p-value &lt;0.0001) are denoted by *. Significant differences
between TSP 1-1 and TSP SW (p-value &lt;0.01) are denoted by †.
</tableCaption>
<table confidence="0.917460833333333">
RST–DT Instructional
Metrics HILDA CRF-O CRF-T TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW
Span 74.68 77.02* 81.34* 82.56* 83.84*† 88.70 70.35 80.67 82.88†
Nuc. 58.99 63.84* 66.52* 68.32* 68.90* 77.72 49.47 63.03 64.13
Rel. 44.32 48.46* 53.01* 55.83* 55.87* 65.75 35.44 43.52 44.20
segmentation results in Table 3, this is not surprising because previous studies (Soricut
</table>
<bodyText confidence="0.998677166666667">
and Marcu 2003) have already shown that automatic segmentation is the primary
impediment to high accuracy discourse parsing. This demonstrates the need for a more
accurate discourse segmentation model in the instructional genre.
6.4.3 Evaluation of the Complete Parser. We experiment with our full document-level
discourse parser on the two corpora using the two parsing approaches described in
Section 4.3, namely, 1S-1S and the sliding window. On RST–DT, the standard split was
used for training and testing. On the instructional corpus, Subba and Di-Eugenio (2009)
used 151 documents for training and 25 documents for testing. Because we did not
have access to their particular split, we took five random samples of 151 documents for
training and 25 documents for testing, and report the average performance over the five
test sets.
Table 6 presents results for our two-stage discourse parser (TSP) using approaches
1S-1S (TSP 1-1) and the sliding window (TSP SW) on manually segmented texts. Recall
that precision, recall, and F-score are the same when manual segmentation is used.
We compare our parser with the state-of-the-art on the two corpora: HILDA (Hernault
et al. 2010) on RST–DT, and the ILP-based approach (Subba and Di-Eugenio 2009) on
the instructional domain. On both corpora, our systems outperform existing systems
by a wide margin (p-value &lt;7.1e-05 on RST–DT).19 On RST–DT, our parser TSP 1-1
achieves absolute improvements of 7.9 percentage points, 9.3 percentage points, and
11.5 percentage points in span, nuclearity, and relation, respectively, over HILDA. This
represents relative error reductions of 31.2%, 22.7%, and 20.7% in span, nuclearity,
and relation, respectively.
Beside HILDA, we also compare our results with two baseline parsers on RST–DT:
(1) CRF-O, which uses a single unified CRF-based parsing model shown in Figure 8 (the
one used for multi-sentential parsing) without distinguishing between intra- and multi-
sentential parsing, and (2) CRF-T, which uses two different CRF-based parsing models
for intra- and multi-sentential parsing in the two-stage approach 1S-1S, both models
having the same structure as in Figure 8. Thus, CRF-T is a variation of TSP 1-1, where
the DCRF-based (chain-structured) intra-sentential parsing model is replaced with a
simpler CRF-based parsing model.20 Note that although CRF-O does not explicitly
</bodyText>
<footnote confidence="0.425341">
19 Because we did not have access to the output or to the system of Subba and Di-Eugenio (2009), we were
not able to perform a significance test on the instructional corpus.
20 The performance of this model for intra-sentential parsing is reported in Table 4 under the name CRF-NC.
</footnote>
<page confidence="0.993445">
420
</page>
<note confidence="0.870413">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.999912720930232">
discriminate between intra- and multi-sentential parsing, it uses N-gram features that
include sentence and EDU boundaries to encode this information into the model.
Table 6 shows that both CRF-O and CRF-T outperform HILDA by a good margin
(p-value &lt;0.0001). This improvement can be attributed to the optimal parsing algo-
rithm and better feature selection strategy. When we compare CRF-T with CRF-O, we
notice significant performance gains for CRF-T (p-value &lt;0.001). The absolute gains
are 4.32 percentage points, 2.68 percentage points, and 4.55 percentage points in span,
nuclearity, and relation, respectively. This comparison clearly demonstrates the benefit
of using a two-stage approach with two different parsing models over a framework
with one single unified parsing model. Finally, when we compare our best results with
the human agreements, we still observe room for further improvement in all three
measures.
On the instructional genre, our parser TSP 1-1 delivers absolute F-score improve-
ments of 10.3 percentage points, 13.6 percentage points, and 8.1 percentage points in
span, nuclearity, and relations, respectively, over the ILP-based approach of Subba and
Di-Eugenio (2009). Our parser, therefore, reduces errors by 34.7%, 26.9%, and 12.5% in
span, nuclearity, and relations, respectively.
If we compare the performance of our discourse parsers on the two corpora, we
observe lower results on the instructional corpus. There could be two reasons for this.
First, the instructional corpus has a smaller amount of data with a larger set of relations
(76 with nuclearity attached). Second, some of the frequent relations are semantically
very similar (e.g., Preparation-Act, Step1-Step2), which makes it difficult even for the
human annotators to distinguish them (Subba and Di-Eugenio 2009).
Comparison between our two document-level parsing approaches reveals that TSP
SW significantly outperforms TSP 1-1 only in finding the right structure on both cor-
pora (p-value &lt;0.01). Not surprisingly, the improvement is higher on the instructional
corpus. A likely explanation is that the instructional corpus contains more leaky bound-
aries (12%), allowing the sliding window approach to be more effective in finding those,
without inducing much noise for the labels. This demonstrates the potential of TSP
SW for data sets with even more leaky boundaries, e.g., the Dutch (Vliet and Redeker
2011) and the German Potsdam (Stede 2004) corpora. However, it would be interesting
to see how other heuristics to do consolidation in the cross condition (Section 4.3.2)
perform.
To analyze errors made by TSP SW, we looked at some poorly parsed examples and
found that although TSP SW finds more correct structures, a corresponding improve-
ment in labeling relations is not present because in some cases, it tends to induce noise
from the neighboring sentences for the labels. For example, when parsing is performed
on the first sentence in Figure 1 in isolation using 1S-1S, our parser rightly identifies the
Contrast relation between EDUs 2 and 3. But, when it is considered with its neighboring
sentences by the sliding window, the parser labels it as Elaboration. A promising strategy
to deal with this and similar problems would be to apply both approaches to each
sentence and combine them by consolidating three probabilistic decisions, namely, the
one from 1S-1S and the two from the sliding window.
</bodyText>
<subsubsectionHeader confidence="0.811137">
6.4.4 k-best Parsing Results Based on Manual Segmentation. As described in Section 4.2, a
</subsubsectionHeader>
<bodyText confidence="0.9897075">
straight-forward modification of our probabilistic parsing algorithm allows us to gener-
ate a list of k-best parse hypotheses for a given text. We adapt our parsing algorithm
accordingly to produce k most probable DTs for each text, and measure the oracle
accuracy based on the F-scores of the Relation metric which gives aggregated evaluation
</bodyText>
<page confidence="0.996948">
421
</page>
<note confidence="0.544281">
Computational Linguistics Volume 41, Number 3
</note>
<tableCaption confidence="0.990614">
Table 7
</tableCaption>
<table confidence="0.491455333333333">
Oracle scores as a function of k of k-best sentence-level parses on RST–DT.
k 1 2 3 4 5 10 15 20 25 30
PAR-s 79.77 84.42 86.55 87.68 88.09 90.37 91.74 92.57 92.95 93.22
</table>
<bodyText confidence="0.998445">
on structure and relation labels (see Table 2). Specifically, the oracle accuracy O−score
for k-best discourse parsing is measured as follows:
</bodyText>
<equation confidence="0.990391333333333">
�N i=1 maxk j=1 F−scorer(gi,hj i)
O−score = (15)
N
</equation>
<bodyText confidence="0.99945672">
where N is the total number of texts (sentences or documents) evaluated, gi is the gold
DT annotation for text i, ht is the j�h parse hypothesis generated by the parser for text i,
and F-scorer(gi,ht) is the F-score accuracy of hypothesis ht on the Relation metric, which
essentially measures how similar ht is to gi in terms of its structure and labels.
Table 7 presents the oracle scores of our intra-sentential parser PAR-S on the RST–
DT test set as a function of k of k-best parsing. The 1-best result tells that the parser has
the base accuracy of 79.8%. The 2-best shows dramatic oracle-rate improvements (i.e.,
4.65% absolute), meaning that often our parser generates the best tree as its top two
outputs. 3-best and 4-best also show moderate improvements (about 2%). Things start
to slow down afterwards, and we achieve oracle rates of 90.37% and 92.57% at 10−best
and 20-best, respectively. The 30-best parsing gives an oracle score of 93.2%.
The results of our k-best intra-sentential discourse parser demonstrate that a k-best
reranking approach like that of Collins and Koo (2005) and Charniak and Johnson (2005)
used for syntactic parsing can potentially improve the parsing accuracy even further by
exploiting additional global features of the candidate discourse trees as evidence.
The scenario is quite different at the document-level; Table 8 shows the k-best
parsing results of TSP 1S-1S on the RST–DT test set. The improvements in oracle-rate
are small at the document-level when compared with the sentence-level parsing. For
example, the 2-best and the 5-best improve over the base accuracy by only 0.7 percent-
age points and 1.0 percentage points, respectively. The improvements get even slower
after that. However, this is not surprising because generally document-level DTs are big
with many constituents, and only a very few of these constituents change from k-best to
k + 1-best parsing. These small changes among the candidate DTs do not contribute
much to the overall F-score accuracy (for further clarification see how F-score is calcu-
lated in Section 6.2.2).
</bodyText>
<tableCaption confidence="0.7567645">
Table 8
Oracle scores as a function of k of k-best document-level parses on RST–DT.
</tableCaption>
<table confidence="0.945613">
k 1 2 3 4 5 10 15 20 25 30
TSP 1S-1S 55.83 56.52 56.67 56.80 56.91 57.23 57.54 57.65 57.67 57.74
</table>
<page confidence="0.982196">
422
</page>
<note confidence="0.945908">
Joty, Carenini, and Ng CODRA
</note>
<tableCaption confidence="0.998243">
Table 9
</tableCaption>
<table confidence="0.98485">
Parsing results using different subsets of features on RST–DT test set.
Sentence-level Document-level (TSP 1-1)
Scores Dom +Org +N-gr +Con +Sub Org +N-gr +L-ch +Con +Sub
Span 91.3 92.1 93.3 94.6 96.5 74.2 75.8 78.5 80.9 82.6
Nuclearity 78.2 80.3 83.8 86.8 89.4 60.6 63.7 65.6 66.9 68.3
Relation 66.2 68.1 74.1 76.3 79.8 46.3 50.1 52.4 53.6 55.8
</table>
<bodyText confidence="0.9993714">
The results of our k-best document-level parsing suggest that often the best tree
is missing in the top k parses. Thus, a reranking of k-best document-level parses may
not be a suitable option for further improvement at the document-level. An alternative
to k-best reranking is to use a sampling-based parsing strategy (Wick et al. 2011) to
explore the space of possible trees, as recently used for dependency parsing (Zhang
et al. 2014). However, note that the potential gain we may obtain by using a reranker
at the sentence level will also improve the (combined) accuracy of the document-level
parser.
6.4.5 Analysis of Features. To analyze the relative importance of different features used in
our parsing models, Table 9 presents the sentence- and document-level parsing results
on a manually segmented RST–DT test set using different subsets of features. The
feature subsets were defined in Section 4.1.4. In each parsing condition, the subsets of
features are added incrementally, based on their availability and historical importance.
The columns in Table 9 represent the inclusion order of the feature subsets.
Because SPADE (Soricut and Marcu 2003) achieved the previous best results on
intra-sentential parsing using Dominance set features, these are included as the initial
set of features in our intra-sentential parsing model. In HILDA, Hernault et al. (2010)
demonstrate the importance of Organizational and N-gram features for full text parsing.
We add these two feature subsets one after another in our intra- and multi-sentential
parsing models.21 Contextual features require other features to be computed; thus they
were added after those features. Because computation of Sub-structural features re-
quires an initial parse tree (i.e., when the parser is applied), they are added at the very
end.
Notice that inclusion of every new subset of features appears to improve the per-
formance over the previous set. Specifically, for sentence-level parsing, when we add
the Organizational features with the Dominance set features, we achieve about 2 per-
centage points absolute improvements in nuclearity and relations. With N-gram fea-
tures, the gain is even higher: 6 percentage points in relations and 3.5 percentage points
in nuclearity for sentence-level parsing, and 3.8 percentage points in relations and
3.1 percentage points in nuclearity for document-level parsing. This demonstrates the
utility of the N-gram features, which is also consistent with the previous findings of
duVerle and Prendinger (2009) and Schilder (2002).
The features extracted from Lexical chains (L-ch) have also proved to be useful for
document-level parsing. They deliver absolute improvements of 2.7 percentage points,
2.9 percentage points, and 2.3 percentage points in span, nuclearity, and relations,
</bodyText>
<page confidence="0.859409">
21 Text structural features are included in the Organizational features for multi-sentential parsing.
423
</page>
<figure confidence="0.643842">
Computational Linguistics Volume 41, Number 3
</figure>
<figureCaption confidence="0.963281">
Figure 17
</figureCaption>
<bodyText confidence="0.953523875">
Discourse trees generated by human annotator and our system for the text [what’s more,]e1
[he believes]e2 [seasonal swings in the auto industry this year aren’t occurring at the same time in the
past,]e3 [because of production and pricing differences]e4 [that are curbing the accuracy of
seasonal adjustments]e5] [built into the employment data.]e6
respectively. Including the Contextual features further gives improvements of 3 per-
centage points in nuclearity and 2.2 percentage points in relation for sentence-level
parsing, and 1.3 percentage points in nuclearity, and 1.2 percentage points in relation
for document-level parsing. Notice that Sub-structural features are more beneficial for
document-level parsing than they are for sentence-level parsing, that is, an improve-
ment of 2.2 percentage points versus an improvement of 0.9 percentage points. This
is not surprising because document-level DTs are generally much larger than sentence-
level DTs, making the sub-structural features more effective for document-level parsing.
6.4.6 Error Analysis. We further analyze the errors made by our discourse parser. As
described in previous sections, the parser could be wrong in finding the right structure
as well as the right nuclearity and relation labels. Figure 17 presents an example where
our parser makes mistakes in finding the right structure (notice the units connected
by Attribution and Cause in the two example DTs) and the right relation label (Topic-
Comment vs. Background). The comparison between intra- and multi-sentential parsing
results presented in Sections 6.4.2 and 6.4.3 tells us that the errors in structure occur
more frequently when the DT is large (e.g., at the document level) and the parsing model
fails to capture the long-range structural dependencies between the DT constituents.
To further analyze the errors made by our parser on the hardest task of relation
labeling, in Figure 18 we present the confusion matrix for our document-level parser
TSP 1-1 on the RST–DT test set. In order to judge independently the ability of our parser
to assign the correct relation labels, the confusion matrix is computed based on the
constituents (see Table 2), where our parser found the right span (i.e., structure).22 The
relations in the matrix are ordered according to their frequency in the training set.
In general, the errors can be explained by two different causes acting together:
(1) imbalanced distribution of the relations in the corpus, and (2) semantic similarity
between the relations. The most frequent relation Elaboration tends to overshadow
others, especially the ones that are semantically similar (e.g., Explanation, Background)
22 Therefore, the counts of the relations shown in the table may not match the ones in the test set.
</bodyText>
<page confidence="0.997349">
424
</page>
<note confidence="0.7695615">
Joty, Carenini, and Ng CODRA
T-C T-O T-CM M-M CMP EV SU CND EN CA TE EX BA CO JO S-U AT EL
</note>
<equation confidence="0.8151535">
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 7
0 0 0 10 0 0 0 0 0 0 0 1 1 0 0 0 1 3
0 0 0 1 4 0 0 1 0 1 0 3 3 0 1 1 0 2
0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 2 11
0 0 0 0 0 0 8 0 0 0 0 0 0 0 1 0 0 12
0 0 0 0 0 0 0 22 0 0 0 0 1 3 0 0 3 2
0 0 0 0 0 0 0 1 24 1 0 0 0 0 0 0 1 7
0 0 0 0 0 0 0 0 2 3 0 4 2 2 7 0 3 11
0 0 0 1 0 0 0 1 2 0 7 1 9 1 9 0 3 4
0 0 0 1 0 0 0 0 1 5 0 12 0 1 3 0 3 12
0 0 0 1 0 0 0 1 0 1 4 1 19 2 6 1 5 12
0 0 0 1 2 0 0 2 0 1 3 2 2 33 7 0 0 9
0 0 0 0 0 0 1 2 0 1 1 1 1 2 57 1 0 13
0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 85 1 0
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 3 272 9
0 1 0 0 0 0 0 0 14 6 1 8 1 0 8 2 2 359
</equation>
<figureCaption confidence="0.652075">
Figure 18
</figureCaption>
<bodyText confidence="0.643881666666667">
Confusion matrix for relation labels on the RST–DT test set. The y-axis represents true and x-axis
represents predicted relations. The relations are Topic-Change (T-C), Topic-Comment (T-CM),
TextualOrganization (T-O), Manner-Means (M-M), Comparison (CMP), Evaluation (EV),
Summary (SU), Condition (CND), Enablement (EN), Cause (CA), Temporal (TE), Explanation
(EX), Background (BA), Contrast (CO), Joint (JO), Same–Unit (S-U), Attribution (AT), and
Elaboration (EL).
</bodyText>
<figureCaption confidence="0.706956">
Figure 19
</figureCaption>
<bodyText confidence="0.980127285714286">
Our system mistakenly labels a Summary as Elaboration.
and less frequent (e.g., Summary, Evaluation). Furthermore, our models sometimes fail
to distinguish relations that are semantically similar (e.g., Temporal vs. Background, Cause
vs. Explanation).
Now, let us look more closely at a few of these errors. Figure 19 presents an example
where our parser mistakenly labels a Summary as Elaboration. Clearly, in this example
the text in parentheses (i.e., (CFD)) is an acronym or summary of the text to the left.
However, parenthesized texts are also used to provide additional information (i.e., to
elaborate), as exemplified in Figure 20 by two text snippets from the RST-DT. Notice
that although the structure of the text (widow of the ..) in the first example is quite
distinguishable from the structure of (CFD), the text (D., Maine) in the second example
is similar to (CFD) in structure, thus it confuses our model.23
Figure 21 presents two examples where our parser mistakenly labels Background and
Cause as Elaboration. However, notice that the two discourse relations (i.e., Background
</bodyText>
<page confidence="0.987458">
23 D., Maine in this example refers to Democrat from state Maine.
</page>
<figure confidence="0.999517555555556">
T-C
T-O
T-CM
M-M
CMP
EV
SU
CND
EN
CA
TE
EX
BA
CO
JO
S-U
AT
EL
</figure>
<page confidence="0.676277">
425
</page>
<figure confidence="0.701702666666667">
Computational Linguistics Volume 41, Number 3
Figure 20
Two examples of Elaboration by texts in parentheses.
</figure>
<figureCaption confidence="0.819922">
Figure 21
</figureCaption>
<subsectionHeader confidence="0.669432">
Confusion between Background/Cause and Elaboration.
</subsectionHeader>
<bodyText confidence="0.999946818181818">
vs. Elaboration and Cause vs. Elaboration) in these examples are semantically very close,
and arguably both can be applicable.
Given these observations, we see two possible ways to improve our system. First,
we would like to use a more robust method (e.g., ensemble methods with bagging) to
deal with the imbalanced distribution of relations, along with taking advantage of richer
semantic knowledge (e.g., compositional semantics) to cope with the errors caused by
semantic similarity between the relations. Second, to capture long-range dependencies
between DT constituents, we would like to explore the idea of k-best discriminative
reranking using tree kernels (Dinarelli, Moschitti, and Riccardi 2011). Because our parser
already produces k most probable DTs, developing a reranker based on discourse tree
kernels is very much within our reach.
</bodyText>
<sectionHeader confidence="0.711428" genericHeader="conclusions">
7. Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999913266666667">
In this article we have presented CODRA, a complete probabilistic discriminative
framework for performing rhetorical analysis in the RST framework. CODRA com-
prises components for performing both discourse segmentation and discourse parsing.
The discourse segmenter is a binary classifier based on a maximum entropy model, and
the discourse parser applies an optimal parsing algorithm to probabilities inferred from
two CRF models: one for intra-sentential parsing and the other for multi-sentential pars-
ing. The CRF models effectively represent the structure and the label of discourse tree
constituents jointly. Furthermore, the DCRF model for intra-sentential parsing captures
the sequential dependencies between the constituents. The two separate parsing models
use their own informative feature sets and the distributional variations of the relation
labels in their respective parsing conditions.
We have also presented two approaches to effectively combine the intra-sentential
and the multi-sentential parsing modules, which can exploit the strong correlation
observed between the text structure and the structure of the discourse tree. The first ap-
proach (1S–1S) builds a DT for every sentence using the intra-sentential parser, and then
</bodyText>
<page confidence="0.996892">
426
</page>
<note confidence="0.871101">
Joty, Carenini, and Ng CODRA
</note>
<bodyText confidence="0.99993952">
runs the multi-sentential parser on the resulting sentence-level DTs. To deal with leaky
boundaries, our second approach (sliding window) builds sentence-level discourse
sub-trees by applying the intra-sentential parser on a sliding window, covering two ad-
jacent sentences and then consolidating the results produced by overlapping windows.
After that, the multi-sentential parser takes all these sentence-level sub-trees and builds
a full rhetorical parse for the whole document.
Finally, we have extended the parsing algorithm to generate k most probable parse
hypotheses for each input text, which could be used in a reranker to improve over the
initial ranking using global features like long-range structural dependencies.
Empirical evaluations on two different genres demonstrate that our approach to
discourse segmentation achieves state-of-the-art performance more efficiently using
fewer features. A series of experiments on the discourse parsing task shows that both
our intra- and multi-sentential parsers significantly outperform the state of the art,
often by a wide margin. A comparison between our combination strategies reveals
that the sliding window approach is more robust across domains. Furthermore, the
oracle accuracy computed based on the k-best parse hypotheses generated by our
parser demonstrates that a reranker could potentially improve the accuracy even
further.
Our error analysis reveals that although the sliding window approach finds more
correct tree structures, in some cases it induces noise for the relation labels from the
neighboring sentences. With respect to the performance of our discourse parser on
the relation labeling task we also found that the most frequent relations tend to mislead
the identification of the less frequent ones, and the models sometimes fail to distinguish
relations that are semantically similar.
The work presented in this article leads us to several interesting future directions.
Our short-term goal is to develop a k-best discriminative reranking discourse parser
using tree kernels applied to discourse trees. We also plan to investigate to what extent
discourse segmentation and discourse parsing can be performed jointly.
We would also like to explore how our system performs on other genres like
conversational (e.g., blogs, e-mails) and evaluative (e.g., customer reviews) texts. To
address the problem of limited annotated data in various genres, we are planning to
develop an interactive version of our system that will allow users to fix the output of
the system with minimal effort and let the system learn from that feedback.
Another interesting future direction is to perform extrinsic evaluations of our sys-
tem in downstream applications. One important application of rhetorical structure is
text summarization, where a significant challenge is producing not only informative
but also coherent summaries. A number of researchers have already investigated the
utility of rhetorical structure for measuring text importance (i.e., informativeness) in
summarization (Marcu 2000b; Daum´e and Marcu 2002; Louis, Joshi, and Nenkova
2010). Recently, Christensen et al. (2013, 2014) propose to perform sentence selection
and ordering at the same time, and use constraints on discourse structure to make the
summaries coherent. However, they represent the discourse as an unweighted directed
graph, which is shallow and not sufficiently informative in most cases. Furthermore,
their approach does not allow compression at the sentence level, which is often ben-
eficial in summarization. In the future, we would like to investigate the utility of our
rhetorical structure for performing sentence compression, selection, and ordering in a
joint summarization process.
Discourse structure can also play important roles in sentiment analysis. A key re-
search problem in sentiment analysis is extracting fine-grained opinions about different
aspects of a product. Several recent papers (Somasundaran 2010; Lazaridou, Titov, and
</bodyText>
<page confidence="0.990449">
427
</page>
<note confidence="0.62925">
Computational Linguistics Volume 41, Number 3
</note>
<bodyText confidence="0.999928638888889">
Sporleder 2013) exploited the rhetorical structure for this task. Another challenging
problem is assessing the overall opinion expressed in a review because not all sentences
in a review contribute equally to the overall sentiment. For example, some sentences
are subjective, whereas others are objective (Pang and Lee 2004); some express the main
claims, whereas others support them (Taboada et al. 2011); some express opinions about
the main entity, whereas others are about the peripherals. Discourse structure could
be useful to capture the relative weights of the discourse units towards the overall
sentiment. For example, the nucleus and satellite distinction along with the rhetori-
cal relations could be useful to infer the relative weights of the connecting discourse
units.
Among other applications of discourse structure, Machine Translation (MT) and
its evaluation have received a resurgence of interest recently. A workshop dedicated
to discourse in machine translation was arranged recently at the ACL 2013 conference
(Webber et al. 2013). Researchers believe that MT systems should consider discourse
phenomena that go beyond the current sentence to ensure consistency in the choice
of lexical items or referring expressions, and the fact that source-language coherence
relations are also realized in the target language (i.e., translating at the document-
level [Hardmeier, Nivre, and Tiedemann 2012]). Guzm´an et al. (2014a, 2014b) and
Joty et al. (2014) propose new discourse-aware automatic evaluation metrics for MT
systems using our discourse analysis tool. They demonstrate that sentence-level dis-
course information is complementary to the state-of-the-art evaluation metrics, and by
combining the discourse-based metrics with the metrics from the ASIYA MT evaluation
toolkit (Gim´enez and M`arquez 2010), they won the WMT 2014 metrics shared task
challenge (Mach´aˇcek and Bojar 2014) both at the segment- and at the system-level. These
results suggest that discourse structure helps to distinguish better translations from
worse ones. Thus, it would be interesting to explore whether discourse information
can be used to rerank alternative MT hypotheses as a post-processing step for the MT
output.
A longer-term goal is to extend our framework to also work with graph structures of
discourse, as recommended by several recent discourse theories (Wolf and Gibson 2005).
Once we achieve similar performance on graph structures, we will perform extrinsic
evaluations to determine their relative utility for various NLP tasks.
Finally, we hope that the online demo, the source code of CODRA, and the
evaluation metrics that we made publicly available in this work will facilitate other
researchers in extending our work and in applying discourse parsing to their NLP
tasks.
</bodyText>
<subsectionHeader confidence="0.623675">
Bibliographic Note
</subsectionHeader>
<bodyText confidence="0.999771">
Portions of this work were previously published in two conference proceedings (Joty,
Carenini, and Ng 2012; Joty et al. 2013). This article significantly extends our previous
work in several ways, most notably: (1) we extend the parsing algorithm to generate
k-most probable parse hypotheses for each input text (Section 4.2); (2) we show the
oracle accuracies for k-best discourse parsing both at the sentence level and at the
document level (Section 6.4.4); (3) to support our claim, we compare our best results
with several variations of our approach (see CRF-NC in Section 6.4.2, and CRF-O and
CRF-T in Section 6.4.3); (4) we analyze the relative importance of different features
for intra- and multi-sentential discourse parsing (Section 6.4.5); and (5) we perform in-
depth error analysis of our complete rhetorical analysis framework (Section 6.4.6).
</bodyText>
<page confidence="0.997335">
428
</page>
<note confidence="0.760176666666667">
Joty, Carenini, and Ng CODRA
Appendix Sample Output Generated by an Online Demo of CODRA
A.
</note>
<page confidence="0.992731">
429
</page>
<note confidence="0.715615">
Computational Linguistics Volume 41, Number 3
</note>
<sectionHeader confidence="0.995362" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995088">
The authors acknowledge the funding
support of NSERC Canada Graduate
Scholarship (CGS-D). Many thanks to Bonnie
Webber, Amanda Stent, Carolyn Rose, Lluis
Marquez, Samantha Wray, and the
anonymous reviewers for their insightful
comments on an earlier version of this
article.
</bodyText>
<sectionHeader confidence="0.998663" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999441869158879">
Althaus, Ernst, Denys Duchier, Alexander
Koller, Kurt Mehlhorn, Joachim Niehren,
and Sven Thiel. 2003. An efficient graph
algorithm for dominance constraints.
Journal of Algorithms, 48(1):194–219.
Asher, Nicholas and Alex Lascarides, 2003.
Logics of Conversation. Cambridge
University Press.
Barzilay, Regina and Michael Elhadad.1997.
Using lexical chains for text
summarization. In Proceedings of the 35th
Annual Meeting of the Association for
Computational Linguistics and the 8th
European Chapter Meeting of the Association
for Computational Linguistics, Workshop on
Intelligent Scalable Test Summarization,
pages 10–17, Madrid.
Biran, Or and Owen Rambow. 2011.
Identifying justifications in written dialogs
by classifying text as argumentative.
International Journal of Semantic Computing,
5(4):363–381.
Blair-Goldensohn, Sasha, Kathleen
McKeown, and Owen Rambow. 2007.
Building and refining rhetorical-semantic
relation models. In Proceedings of the
Human Language Technologies: The Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
HLT-NAACL’07, pages 428–435.
Rochester, NY.
Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems. Ph.D.
thesis, University of Pennsylvania.
Breiman, Leo. 1996. Bagging predictors.
Machine Learning, 24(2):123–140.
Carlson, Lynn and Daniel Marcu. 2001.
Discourse tagging reference manual.
Technical Report ISI-TR-545, University of
Southern California Information Sciences
Institute.
Carlson, Lynn, Daniel Marcu, and Mary Ellen
Okurowski. 2002. RST Discourse Treebank
(RST–DT) LDC2002T07. Linguistic Data
Consortium, Philadelphia.
Chali, Yllias and Shafiq Joty. 2007. Word
sense disambiguation using lexical
cohesion. In Proceedings of SemEval-2007,
pages 476–479, Prague.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of the 1st North American
Chapter of the Association for Computational
Linguistics Conference, NAACL’00,
pages 132–139, Seattle, WA.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics, ACL’05,
pages 173–180, Ann Arbor, MI.
Christensen, Janara, Mausam, Stephen
Soderland, and Oren Etzioni. 2013.
Towards coherent multi-document
summarization. In Proceedings of the 2013
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies,
NAACL-HLT’13, pages 1163–1173,
Atlanta, GA.
Christensen, Janara, Stephen Soderland,
Gagan Bansal, and Mausam. 2014.
Hierarchical summarization: Scaling up
multi-document summarization. In
Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics,
ACL’13, pages 902–912, Baltimore, MD.
Collins, Michael. 2003. Head-driven
statistical models for natural language
Parsing. Computational Linguistics,
29(4):589–637.
Collins, Michael and Terry Koo. 2005.
Discriminative reranking for natural
language parsing. Computational
Linguistics, 31(1):25–70.
Collobert, Ronan, Jason Weston, L´eon Bottou,
Michael Karlen, Koray Kavukcuoglu, and
Pavel Kuksa. 2011. Natural language
processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.
Cristea, Dan, Nancy Ide, and Laurent
Romary. 1998. Veins theory: A model of
global discourse cohesion and coherence.
In Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and of the 17th International
Conference on Computational Linguistics
(COLING/ACL’98), pages 281–285.
Montreal.
Danlos, Laurence. 2009. D-STAG: A
discourse analysis formalism based on
synchronous TAGs. TAL, 50(1):111–143.
Daum´e, III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics, ACL’07,
pages 256–263, Prague.
</reference>
<page confidence="0.997553">
430
</page>
<note confidence="0.9472">
Joty, Carenini, and Ng CODRA
</note>
<reference confidence="0.9918995">
Daum´e, III, Hal and Daniel Marcu. 2002. A
noisy-channel model for document
compression. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, ACL ’02,
pages 449–456, Philadelphia, PA.
Dinarelli, Marco, Alessandro Moschitti, and
Giuseppe Riccardi. 2011. Discriminative
reranking for spoken language
understanding. IEEE Transactions on Audio,
Speech and Language Processing (TASLP),
20:526–539.
duVerle, David and Helmut Prendinger.
2009. A novel discourse parser based on
support vector machine classification. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural
Language Processing of the AFNLP,
pages 665–673, Suntec.
Egg, Markus, Alexander Koller, and Joachim
Niehren. 2001. The constraint language for
lambda structures. Journal of Logic,
Language and Information, 10(4):457–485.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
Conference on Computational Linguistics -
Volume 1, COLING ’96, pages 340–345,
Copenhagen.
Fellbaum, Christiane. 1998. WordNet—An
Electronic Lexical Database. MIT Press,
Cambridge, MA.
Feng, Vanessa and Graeme Hirst. 2012.
Text-level discourse parsing with rich
linguistic features. In Proceedings of the
50th Annual Meeting of the Association for
Computational Linguistics, ACL ’12,
pages 60–68, Jeju Island.
Feng, Vanessa and Graeme Hirst. 2014. A
linear-time bottom-up discourse parser
with constraints and post-editing. In
Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics,
ACL ’14, pages 511–521, Baltimore,
MD.
Finkel, Jenny Rose, Alex Kleeman, and
Christopher Manning. 2008. Efficient,
feature-based, conditional random field
parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational
Linguistics, ACL’08, pages 959–967,
Columbus, OH.
Fisher, Seeger and Brian Roark. 2007. The
utility of parse-derived features for
automatic discourse segmentation. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics,
ACL’07, pages 488–495, Prague.
Galley, Michel and Kathleen McKeown. 2003.
Improving word sense disambiguation in
lexical chaining. In Proceedings of the 18th
International Joint Conference on Artificial
Intelligence, IJCAI’03, pages 1486–1488,
Acapulco.
Galley, Michel, Kathleen McKeown, Eric
Fosler-Lussier, and Hongyan Jing. 2003.
Discourse segmentation of multi-party
conversation. In Proceedings of the 41st
Annual Meeting of the Association for
Computational Linguistics - Volume 1,
ACL ’03, pages 562–569, Sapporo.
Ghosh, Sucheta, Richard Johansson,
Giuseppe Riccardi, and Sara Tonelli. 2011.
Shallow discourse parsing with
conditional random fields. In Proceedings of
the 5th International Joint Conference on
Natural Language Processing, IJCNLP’11,
pages 1071–1079, Chiang Mai.
Gim´enez, Jes´us and Lluis M`arquez. 2010.
Linguistic measures for automatic
machine translation evaluation. Machine
Translation, 24(3–4):77–86.
Guzm´an, Francisco, Shafiq Joty, Lluis
M`arquez, Alessandro Moschitti, Preslav
Nakov, and Massimo Nicosia. 2014a.
Learning to differentiate better from worse
translations. In Proceedings of the 2014
Conference on Empirical Methods in Natural
Language Processing (EMNLP),
pages 214–220, Doha.
Guzm´an, Francisco, Shafiq Joty, Lluis
M`arquez, and Preslav Nakov. 2014b. Using
discourse structure improves machine
translation evaluation. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long
Papers), pages 687–698, Baltimore, MD.
Halliday, Michael and Ruqaiya Hasan. 1976.
Cohesion in English. Longman, London.
Hardmeier, Christian, Joakim Nivre, and
J¨org Tiedemann. 2012. Document-wide
decoding for phrase-based statistical
machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 1179–1190,
Jeju Island.
Hernault, Hugo, Helmut Prendinger, David
duVerle, and Mitsuru Ishizuka. 2010.
HILDA: A discourse parser using support
vector machine classification. Dialogue and
Discourse, 1(3):1–33.
Hirst, Graeme and David St-Onge. 1997.
Lexical Chains as Representation of
Context for the Detection and Correction
of Malapropisms. In Christiane Fellbaum,
</reference>
<page confidence="0.995266">
431
</page>
<note confidence="0.663723">
Computational Linguistics Volume 41, Number 3
</note>
<reference confidence="0.993835686440678">
editor, WordNet: An Electronic Lexical
Database and Some of its Applications.
MIT press, pages 305–332.
Hobbs, Jerry. 1979. Coherence and
coreference. Cognitive Science, 3:67–90.
Huang, Liang and David Chiang. 2005.
Better k-best parsing. In Proceedings of the
Ninth International Workshop on Parsing
Technology, Parsing ’05, pages 53–64,
Stroudsburg, PA.
Ji, Yangfeng and Jacob Eisenstein. 2014.
Representation learning for text-level
discourse parsing. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long
Papers), pages 13–24, Baltimore, MD.
Joty, Shafiq, Giuseppe Carenini, and
Raymond T. Ng. 2012. A novel
discriminative framework for
sentence-level discourse analysis. In
Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning, EMNLP-CoNLL ’12,
pages 904–915, Jeju Island.
Joty, Shafiq, Giuseppe Carenini, and
Raymond T. Ng. 2013. Topic segmentation
and labeling in asynchronous
Conversations. Journal of Artificial
Intelligence Research (JAIR), 47:521–573.
Joty, Shafiq, Giuseppe Carenini, Raymond T.
Ng, and Yashar Mehdad. 2013. Combining
intra- and multi-sentential rhetorical
parsing for document-level discourse
analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics, ACL ’13, pages 486–496,
Sofia.
Joty, Shafiq, Francisco Guzm´an, Lluis
M`arquez, and Preslav Nakov. 2014.
DiscoTK: Using discourse structure for
machine translation evaluation. In
Proceedings of the Ninth Workshop on
Statistical Machine Translation, WMT ’14,
pages 402–408, Baltimore, MD.
Jurafsky, Daniel and James Martin. 2008.
Statistical parsing. In Speech and Language
Processing, chapter 14. Prentice Hall.
Knight, Kevin and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers
for natural language processing. In
Computational Linguistics and Intelligent Text
Processing, volume 3406 of Lecture Notes in
Computer Science. Springer, Berlin
Heidelberg, pages 1–24.
Knott, Alistair and Robert Dale. 1994. Using
linguistic phenomena to motivate a set of
coherence relations. Discourse Processes,
18:35–62.
Koller, Alexander, Michaela Regneri, and
Stefan Thater. 2008. Regular tree
grammars as a formalism for scope
underspecification. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics on Human
Language Technologies, pages 218–226,
Columbus, OH.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data. In
Proceedings of the Eighteenth International
Conference on Machine Learning,
pages 282–289, San Francisco, CA.
Lazaridou, Angeliki, Ivan Titov, and Caroline
Sporleder. 2013. A Bayesian model for joint
unsupervised induction of sentiment,
aspect and discourse representations. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics,
ACL ’13, Sofia.
Li, Jiwei, Rumeng Li, and Eduard Hovy.
2014. Recursive deep models for discourse
parsing. In Proceedings of the 2014
Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 2061–2069, Doha.
Li, Sujian, Liang Wang, Ziqiang Cao, and
Wenjie Li. 2014. Text-level discourse
dependency parsing. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long
Papers), pages 25–35, Baltimore, MD.
Louis, Annie, Aravind Joshi, and Ani
Nenkova. 2010. Discourse indicators for
content selection in summarization. In
Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and
Dialogue, SIGDIAL ’10, pages 147–156,
Tokyo.
Mach´aˇcek, Matouˇs and Ondˇrej Bojar. 2014.
Results of the WMT14 metrics shared
task. In Proceedings of the Ninth
Workshop on Statistical Machine
Translation, Baltimore, MD.
Magerman, David. 1995. Statistical
decision-tree models for parsing. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
ACL’95, pages 276–283, Cambridge, MA.
Mann, William and Sandra Thompson. 1988.
Rhetorical structure theory: Toward a
functional theory of text organization. Text,
8(3):243–281.
Marcu, Daniel. 1999. A decision-based
approach to rhetorical parsing. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics on
</reference>
<page confidence="0.976368">
432
</page>
<note confidence="0.693546">
Joty, Carenini, and Ng CODRA
</note>
<reference confidence="0.999876813559322">
Computational Linguistics, ACL’99,
pages 365–372, Morristown, NJ.
Marcu, Daniel. 2000a. The rhetorical parsing
of unrestricted texts: A surface-based
approach. Computational Linguistics,
26:395–448.
Marcu, Daniel. 2000b. The Theory and Practice
of Discourse Parsing and Summarization.
MIT Press, Cambridge, MA.
Marcu, Daniel and Abdessamad Echihabi.
2002. An unsupervised approach to
recognizing discourse relations. In
Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, ACL’02, pages 368–375.
Philadelphia, PA.
Marcus, Mitchell, Mary Marcinkiewicz, and
Beatrice Santorini. 1994. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19(2):313–330.
Martin, James, 1992. English Text: System and
Structure. John Benjamins Publishing
Company, Philadelphia/Amsterdam.
Maslennikov, Mstislav and Tat-Seng Chua.
2007. A multi-resolution framework for
information extraction from free text. In
Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics,
pages 592–599, Prague.
McCallum, Andrew. 2002. MALLET: A
machine learning for language toolkit.
http://mallet.cs.umass.edu.
McCallum, Andrew, Dayne Freitag, and
Fernando C. N. Pereira. 2000. Maximum
entropy Markov models for information
extraction and segmentation. In
Proceedings of the Seventeenth International
Conference on Machine Learning, ICML ’00,
pages 591–598, San Francisco, CA.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, ACL ’05, pages 91–98,
Ann Arbor, MI.
McDonald, Ryan, Fernando Pereira, Kiril
Ribarov, and Jan Hajiˇc. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings
of the Conference on Human Language
Technology and Empirical Methods in
Natural Language Processing, HLT ’05,
pages 523–530, Stroudsburg, PA.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of structure of text.
Computational Linguistics, 17(1):21–48.
Murphy, Kevin. 2012. Machine Learning: A
Probabilistic Perspective. The MIT Press.
Cambridge, MA.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, ACL ’04,
pages 271–278. Barcelona.
Pitler, Emily and Ani Nenkova. 2009. Using
syntax to disambiguate explicit discourse
connectives in text. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ’09, pages 13–16, Suntec.
Poole, David and Alan Mackworth, 2010.
Artificial Intelligence: Foundations of
Computational Agents. Cambridge
University Press.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings of
the Sixth International Conference on
Language Resources and Evaluation (LREC),
pages 2961–2968, Marrakech.
Prasad, Rashmi, Aravind Joshi, Nikhil
Dinesh, Alan Lee, Eleni Miltsakaki, and
Bonnie Webber. 2005. The Penn Discourse
TreeBank as a resource for natural
language generation. In Proceedings of the
Corpus Linguistics Workshop on Using
Corpora for Natural Language Generation,
pages 25–32, Birmingham.
Regneri, Michaela, Markus Egg, and
Alexander Koller. 2008. Efficient
processing of underspecified discourse
representations. In Proceedings of the 46th
Annual Meeting of the Association for
Computational Linguistics on Human
Language Technologies: Short Papers,
HLT-Short ’08, pages 245–248,
Columbus, OH.
Reyle, Uwe. 1993. Dealing with ambiguities
by underspecification: Construction,
representation and deduction. Journal of
Semantics, 10(2):123–179.
Schapire, Robert E. and Yoram Singer. 2000.
Boostexter: A boosting-based system for
text categorization. Machine Learning,
39(2–3):135–168.
Schauer, Holger and Udo Hahn. 2001.
Anaphoric cues for coherence relations. In
Proceedings of the Conference on Recent
Advances in Natural Language Processing,
RANLP ’01, pages 228–234,
Tzigov Chark.
Schilder, Frank. 2002. Robust discourse
parsing via discourse markers, topicality
</reference>
<page confidence="0.982995">
433
</page>
<reference confidence="0.992858773109244">
Computational Linguistics Volume 41, Number 3
and position. Natural Language Engineering,
8(3):235–255.
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields. In
Proceedings of the 2003 Conference of the
North American Chapter of the Association for
Computational Linguistics on Human
Language Technology - Volume 1,
NAACL-HLT’03, pages 134–141,
Edmonton.
Silber, Gregory and Kathleen McCoy. 2002.
Efficiently computed lexical chains as an
intermediate representation for automatic
text summarization. Computational
Linguistics, 28(4):487–496.
Smith, Noah A. 2011. Linguistic Structure
Prediction. Synthesis Lectures on Human
Language Technologies. Morgan and
Claypool.
Socher, Richard, John Bauer, Christopher D.
Manning, and Ng Andrew Y. 2013a.
Parsing with compositional vector
grammars. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers),
pages 455–465, Sofia.
Socher, Richard, Alex Perelygin, Jean Wu,
Jason Chuang, Christopher D. Manning,
Andrew Ng, and Christopher Potts. 2013b.
Recursive deep models for semantic
compositionality over a sentiment
treebank. In Proceedings of the 2013
Conference on Empirical Methods in Natural
Language Processing, pages 1631–1642,
Seattle, WA.
Somasundaran, S. 2010. Discourse-Level
Relations for Opinion Analysis. Ph.D. thesis,
University of Pittsburgh, PA.
Soricut, Radu and Daniel Marcu. 2003.
Sentence level discourse parsing using
syntactic and lexical information. In
Proceedings of the 2003 Conference of the
North American Chapter of the Association for
Computational Linguistics on Human
Language Technology - Volume 1,
NAACL’03, pages 149–156,
Edmonton.
Sporleder, Caroline. 2007. Manually vs.
automatically labelled data in discourse
relation classification. Effects of example
and feature selection. LDV Forum,
22(1):1–20.
Sporleder, Caroline and Mirella Lapata. 2004.
Automatic paragraph identification: A
study across languages and domains. In
Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing, EMNLP ’04, pages 72–79,
Barcelona.
Sporleder, Caroline and Mirella Lapata. 2005.
Discourse chunking and its application to
sentence compression. In Proceedings of the
Conference on Human Language Technology
and Empirical Methods in Natural
Language Processing, HLT-EMNLP’05,
pages 257–264, Vancouver.
Sporleder, Caroline and Alex Lascarides.
2005. Exploiting linguistic cues to classify
rhetorical relations. In Proceedings of Recent
Advances in Natural Language Processing
(RANLP), pages 157–166, Bulgaria.
Sporleder, Caroline and Alex Lascarides.
2008. Using automatically labelled
examples to classify rhetorical relations:
An assessment. Natural Language
Engineering, 14(3):369–416.
Stede, Manfred. 2004. The Potsdam
Commentary Corpus. In Proceedings of the
ACL-04 Workshop on Discourse Annotation,
pages 96–102, Barcelona.
Stede, Manfred. 2011. Discourse Processing.
Synthesis Lectures on Human Language
Technologies. Morgan and Claypool
Publishers.
Subba, Rajen and Barbara Di-Eugenio. 2009.
An effective discourse parser that
uses rich linguistic information. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, HLT-NAACL’09,
pages 566–574, Boulder, CO.
Sutton, Charles and Andrew McCallum.
2012. An introduction to conditional
random fields. Foundations and Trends in
Machine Learning, 4(4):267–373.
Sutton, Charles, Andrew McCallum, and
Khashayar Rohanimanesh. 2007. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. Journal of
Machine Learning Research (JMLR),
8:693–723.
Taboada, Maite. 2006. Discourse markers as
signals (or not) of rhetorical relations.
Journal of Pragmatics, 38(4):567–592.
Taboada, Maite, Julian Brooke, Milan
Tofiloski, Kimberly Voll, and Manfred
Stede. 2011. Lexicon-based methods for
sentiment analysis. Computational
Linguistics, 37(2):267–307.
Taboada, Maite and William C. Mann. 2006.
Rhetorical structure theory: Looking back
and moving ahead. Discourse Studies,
8(3):423–459.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and rhetorical
</reference>
<page confidence="0.999089">
434
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.689175">
<title confidence="0.961496">CODRA: A Novel Discriminative Framework for Rhetorical Analysis</title>
<affiliation confidence="0.952983">Qatar Computing Research Institute University of British Columbia T. University of British Columbia</affiliation>
<abstract confidence="0.993072">Clauses and sentences rarely stand on their own in an actual discourse; rather, the relationship between them carries important information that allows the discourse to express a meaning as a whole beyond the sum of its individual parts. Rhetorical analysis seeks to uncover this coherence In this article, we present a COmplete probabilistic Discriminative framework for performing Rhetorical Analysis in accordance with Rhetorical Structure Theory, which posits a tree representation of a discourse. a discourse segmenter and a discourse parser. First, the discourse segmenter, which is based on a binary classifier, identifies the elementary discourse units in a given text. Then the discourse parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intra-sentential parsing and the other for multi-sentential parsing. We present two approaches to combine these two stages of parsing effectively. By conducting a series of empirical evaluations over two data sets, we demonstrate that outperforms the state-of-the-art, often by a wide margin. We also show that a reranking of the k-best parse hypotheses generated potentially improve the accuracy even further.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ernst Althaus</author>
<author>Denys Duchier</author>
<author>Alexander Koller</author>
<author>Kurt Mehlhorn</author>
<author>Joachim Niehren</author>
<author>Sven Thiel</author>
</authors>
<title>An efficient graph algorithm for dominance constraints.</title>
<date>2003</date>
<journal>Journal of Algorithms,</journal>
<volume>48</volume>
<issue>1</issue>
<contexts>
<context position="23371" citStr="Althaus et al. 2003" startWordPosition="3465" endWordPosition="3468">chunking). Using quite a large number of features in a binary log-linear model, they achieve state-of-the-art performance in discourse segmentation on the RST–DT test set. In a different approach, Regneri, Egg, and Koller (2008) propose to use Underspecified Discourse Representation (UDR) as an intermediate representation for discourse parsing. Underspecified representations offer a single compact representation to express possible ambiguities in a linguistic structure, and have been primarily used to deal with scope ambiguity in semantic structures (Reyle 1993; Egg, Koller, and Niehren 2001; Althaus et al. 2003; Koller, Regneri, and Thater 2008). Assuming that a UDR of a DT is already given in the form of a dominance graph (Althaus et al. 2003), Regneri, Egg, and Koller (2008) convert it into a more expressive and complete UDR representation called regular tree grammar (Koller, Regneri, and Thater 2008), for which efficient algorithms (Knight and Graehl 2005) already exist to derive the best configuration (i.e., the best discourse tree). Hernault et al. (2010) present the publicly available HILDA system,3 which comes with a discourse segmenter and a parser based on Support Vector Machines (SVMs). Th</context>
</contexts>
<marker>Althaus, Duchier, Koller, Mehlhorn, Niehren, Thiel, 2003</marker>
<rawString>Althaus, Ernst, Denys Duchier, Alexander Koller, Kurt Mehlhorn, Joachim Niehren, and Sven Thiel. 2003. An efficient graph algorithm for dominance constraints. Journal of Algorithms, 48(1):194–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3941" citStr="Asher and Lascarides (2003)" startWordPosition="565" endWordPosition="568">Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is driven by sentence semantics. Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse structure. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is perhaps the most influential theory of discourse in computational linguistics. Although it was initially intended to be used in text generation, later it became popular as a framework for parsing the structure of a text (Taboada and Mann 2006). RST represents texts by labeled hierarchical structures, called Discourse Trees (DTs). For example</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Asher, Nicholas and Alex Lascarides, 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad 1997</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th European Chapter Meeting of the Association for Computational Linguistics, Workshop on Intelligent Scalable Test Summarization,</booktitle>
<pages>10--17</pages>
<location>Madrid.</location>
<marker>Barzilay, 1997, </marker>
<rawString>Barzilay, Regina and Michael Elhadad.1997. Using lexical chains for text summarization. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th European Chapter Meeting of the Association for Computational Linguistics, Workshop on Intelligent Scalable Test Summarization, pages 10–17, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Owen Rambow</author>
</authors>
<title>Identifying justifications in written dialogs by classifying text as argumentative.</title>
<date>2011</date>
<journal>International Journal of Semantic Computing,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="54025" citStr="Biran and Rambow 2011" startWordPosition="8376" endWordPosition="8379">We also measure the distances of the units in terms of the number of EDUs from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the discourse units. Discourse cues (e.g., because, but), when present, signal rhetorical relations between two text segments, and have been used as a primary source of information in earlier studies (Knott and Dale 1994; Marcu 2000a). However, recent studies (Hernault et al. 2010; Biran and Rambow 2011) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of cue phrases, since this approach is domain independent and capable of capturing non-lexical cues such as punctuation. In order to build a lexical N-gram dictionary empirically from the training corpus, we extract the first and last N tokens (NE11, 2,3}) of each discourse unit and rank them according to their mutual information with the two labels, Structure (S) and Relation (R). 401 Computational Linguistics Volume 41, Number 3 Table 1 Features used in our intra- and multi-sentential parsing </context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>Biran, Or and Owen Rambow. 2011. Identifying justifications in written dialogs by classifying text as argumentative. International Journal of Semantic Computing, 5(4):363–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kathleen McKeown</author>
<author>Owen Rambow</author>
</authors>
<title>Building and refining rhetorical-semantic relation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL’07,</booktitle>
<pages>428--435</pages>
<marker>Blair-Goldensohn, McKeown, Rambow, 2007</marker>
<rawString>Blair-Goldensohn, Sasha, Kathleen McKeown, and Owen Rambow. 2007. Building and refining rhetorical-semantic relation models. In Proceedings of the Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL’07, pages 428–435.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NY Rochester</author>
</authors>
<marker>Rochester, </marker>
<rawString>Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
</authors>
<title>Domain Adaptation of Natural Language Processing Systems.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Blitzer, 2008</marker>
<rawString>Blitzer, John. 2008. Domain Adaptation of Natural Language Processing Systems. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="78897" citStr="Breiman 1996" startWordPosition="12589" endWordPosition="12590">d (also known as logistic) function, respectively. The negative log-likelihood (NLL) of the model with l2 regularization for N data points (i.e., word–tokens) is given by N NLL(θ) = − yi log Sigm(©Txi) + (1 − yi) log (1 − Sigm(©Txi)) + A©T© (13) i=1 where yi is the gold label for word–token wi (represented by feature vector xi). We learn the model parameters © using the L-BFGS fitting algorithm, which is time- and spaceefficient. To avoid overfitting, we use 5-fold cross validation to learn the regularization strength parameter A from the training data. We also use a simple bagging technique (Breiman 1996) to deal with the sparsity of boundary (i.e., y = 1) tags. Note that our first attempt at the discourse segmentation task implemented a linearchain CRF model (Lafferty, McCallum, and Pereira 2001) to capture the sequential dependencies between the tags in a discriminative way. However, the binary Logistic Regression classifier, using the same set of features, not only outperforms the CRF model, but also reduces time and space complexity. One possible explanation for the low performance of the CRF model is that Markov dependencies between tags cannot be effectively captured due to the sparsity </context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Breiman, Leo. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse tagging reference manual.</title>
<date>2001</date>
<tech>Technical Report ISI-TR-545,</tech>
<institution>University of Southern California Information Sciences Institute.</institution>
<contexts>
<context position="6513" citStr="Carlson and Marcu (2001)" startWordPosition="960" endWordPosition="963">ei are the core parts of the relation and satellites are peripheral or supportive ones. For example, in Figure 1, Elaboration is a relation between a nucleus (EDU 4) and a satellite (EDU 5), and Contrast is a relation between two nuclei (EDUs 2 and 3). Carlson, Marcu, and Okurowski (2002) constructed the first large RST-annotated corpus (RST–DT) on Wall Street Journal articles from the Penn Treebank. Whereas Mann and Thompson (1988) had suggested about 25 relations, the RST–DT uses 53 mono-nuclear and 25 multi-nuclear relations. The relations are grouped into 16 coarse-grained categories; see Carlson and Marcu (2001) for a detailed description of the relations. Conventionally, rhetorical analysis in RST involves two subtasks: discourse segmentation is the task of breaking the text into a sequence of EDUs, and discourse parsing is the task of linking the discourse units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse parsing and rhetorical parsing interchangeably. While recent advances in automatic discourse segmentation have attained high accuracies (an F-score of 90.5% reported by Fisher and Roark [2007]), discourse parsing still poses significant challenges (Feng</context>
<context position="97222" citStr="Carlson and Marcu (2001)" startWordPosition="15534" endWordPosition="15537">n document-level (i.e., multi-sentential) parsing, we compare our approach with HILDA (Hernault et al. 2010) on the RST–DT corpus, and with the ILPbased approach (Subba and Di-Eugenio 2009) on the instructional corpus. The results for HILDA were obtained by running the system with default settings on the same inputs we provided to our system. Because we could not run the ILP-based system (not publicly available), we report the performance presented in their paper. Our experiments on the RST–DT corpus use the same 18 coarser coherence relations (see Figure 18 later in this article), defined by Carlson and Marcu (2001) and also used in SPADE and HILDA systems. More specifically, the relation set consists of 16 relation categories and two pseudo-relations, namely, Textual–Organization and Same–Unit. After attaching the nuclearity statuses (NS, SN, NN) to these relations, we obtain 41 distinct relations.15 Our experiments on the instructional corpus consider the same 26 primary relations (e.g., Goal:Act, Cause:Effect) used by Subba and Di-Eugenio (2009) and also treat the reversals of non-commutative relations as separate relations. That is, Goal–Act and Act–Goal are considered to be two different coherence r</context>
</contexts>
<marker>Carlson, Marcu, 2001</marker>
<rawString>Carlson, Lynn and Daniel Marcu. 2001. Discourse tagging reference manual. Technical Report ISI-TR-545, University of Southern California Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<date>2002</date>
<booktitle>RST Discourse Treebank (RST–DT) LDC2002T07. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>Carlson, Lynn, Daniel Marcu, and Mary Ellen Okurowski. 2002. RST Discourse Treebank (RST–DT) LDC2002T07. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yllias Chali</author>
<author>Shafiq Joty</author>
</authors>
<title>Word sense disambiguation using lexical cohesion.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007,</booktitle>
<pages>476--479</pages>
<location>Prague.</location>
<contexts>
<context position="60963" citStr="Chali and Joty 2007" startWordPosition="9548" endWordPosition="9551">xical chains spanning paragraphs. (b) and (c) Two possible DT structures. 404 Joty, Carenini, and Ng CODRA Figure 11 Extracting lexical chains. (a) A Lexical Semantic Relatedness Graph (LSRG) for five noun-tokens. (b) Resultant graph after performing WSD. The box at the bottom shows the lexical chains. In the preprocessing step, we extract the nouns from the document and lemmatize them using WordNet’s built-in morphy function (Fellbaum 1998). Then, by looking up in WordNet we expand each noun to all of its senses, and build a Lexical Semantic Relatedness Graph (LSRG) (Galley and McKeown 2003; Chali and Joty 2007). In an LSRG, the nodes represent noun-tokens with their candidate senses, and the weighted edges between senses of two different tokens represent one of the three semantic relations: repetition, synonym, and hypernym. For example, Figure 11a shows a partial LSRG, where the token bank has two possible senses, namely, money bank and river bank. Using the money bank sense, bank is connected with institution and company by hypernymy relations (edges marked with H), and with another bank by a repetition relation (edges marked with R). Similarly, using the river bank sense, it is connected with riv</context>
</contexts>
<marker>Chali, Joty, 2007</marker>
<rawString>Chali, Yllias and Shafiq Joty. 2007. Word sense disambiguation using lexical cohesion. In Proceedings of SemEval-2007, pages 476–479, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL’00,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="91520" citStr="Charniak 2000" startWordPosition="14642" endWordPosition="14643">s when comparing two systems, we use paired t-test on the F-scores to measure statistical significance and report the p-value. We ran HILDA with its default settings. For SPADE, we applied the same modifications to its default settings as described in Fisher and Roark (2007), which delivers significantly improved performance over its original version. Specifically, in our experiments on the RST–DT corpus, we trained SPADE using the human-annotated syntactic trees extracted from the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1994), and, during testing, we replaced the Charniak parser (Charniak 2000) with a more 12 Available from alt.qcri.org/tools/. 415 Computational Linguistics Volume 41, Number 3 Table 3 Discourse segmentation results of different models on the two corpora. Performances significantly superior to SPADE are denoted by *. accurate reranking parser (Charniak and Johnson 2005). However, because of the lack of gold syntactic trees in the instructional corpus, we trained SPADE in this corpus using the syntactic trees produced by the reranking parser. To avoid using the gold syntactic trees, we used the reranking parser in our system for both training and testing purposes. Thi</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL’00, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL’05,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="9474" citStr="Charniak and Johnson 2005" startWordPosition="1407" endWordPosition="1410"> DT. To cope with this limitation, we use the inferred (posterior) probabilities from our CRF parsing models in a probabilistic CKY-like bottom–up parsing algorithm (Jurafsky and Martin 2008), which is non-greedy and optimal. Furthermore, a simple modification of this parsing algorithm allows us to generate k-best (i.e., the k highest probability) parse hypotheses for each input text that could then be used in a reranker to improve over the initial ranking using additional (global) features of the discourse tree as evidence, a strategy that has been successfully explored in syntactic parsing (Charniak and Johnson 2005; Collins and Koo 2005). Third, most of the existing discourse parsers do not discriminate between intrasentential parsing (i.e., building the DTs for the individual sentences) and multisentential parsing (i.e., building the DT for the whole document). However, we argue that distinguishing between these two parsing conditions can result in more effective parsing. Two separate parsing models could exploit the fact that rhetorical relations 387 Computational Linguistics Volume 41, Number 3 are distributed differently intra-sententially versus multi-sententially. Also, they could independently ch</context>
<context position="91817" citStr="Charniak and Johnson 2005" startWordPosition="14682" endWordPosition="14685">livers significantly improved performance over its original version. Specifically, in our experiments on the RST–DT corpus, we trained SPADE using the human-annotated syntactic trees extracted from the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1994), and, during testing, we replaced the Charniak parser (Charniak 2000) with a more 12 Available from alt.qcri.org/tools/. 415 Computational Linguistics Volume 41, Number 3 Table 3 Discourse segmentation results of different models on the two corpora. Performances significantly superior to SPADE are denoted by *. accurate reranking parser (Charniak and Johnson 2005). However, because of the lack of gold syntactic trees in the instructional corpus, we trained SPADE in this corpus using the syntactic trees produced by the reranking parser. To avoid using the gold syntactic trees, we used the reranking parser in our system for both training and testing purposes. This syntactic parser was trained on the sections of the Penn Treebank not included in our test set. We applied the same canonical lexical head projection rules (Magerman 1995; Collins 2003) to lexicalize the syntactic trees as done in HILDA and SPADE. Note that previous studies (Fisher and Roark 20</context>
<context position="113996" citStr="Charniak and Johnson (2005)" startWordPosition="18137" endWordPosition="18140">t result tells that the parser has the base accuracy of 79.8%. The 2-best shows dramatic oracle-rate improvements (i.e., 4.65% absolute), meaning that often our parser generates the best tree as its top two outputs. 3-best and 4-best also show moderate improvements (about 2%). Things start to slow down afterwards, and we achieve oracle rates of 90.37% and 92.57% at 10−best and 20-best, respectively. The 30-best parsing gives an oracle score of 93.2%. The results of our k-best intra-sentential discourse parser demonstrate that a k-best reranking approach like that of Collins and Koo (2005) and Charniak and Johnson (2005) used for syntactic parsing can potentially improve the parsing accuracy even further by exploiting additional global features of the candidate discourse trees as evidence. The scenario is quite different at the document-level; Table 8 shows the k-best parsing results of TSP 1S-1S on the RST–DT test set. The improvements in oracle-rate are small at the document-level when compared with the sentence-level parsing. For example, the 2-best and the 5-best improve over the base accuracy by only 0.7 percentage points and 1.0 percentage points, respectively. The improvements get even slower after tha</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL’05, pages 173–180, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Towards coherent multi-document summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT’13,</booktitle>
<pages>1163--1173</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="129223" citStr="Christensen et al. (2013" startWordPosition="20677" endWordPosition="20680">ut of the system with minimal effort and let the system learn from that feedback. Another interesting future direction is to perform extrinsic evaluations of our system in downstream applications. One important application of rhetorical structure is text summarization, where a significant challenge is producing not only informative but also coherent summaries. A number of researchers have already investigated the utility of rhetorical structure for measuring text importance (i.e., informativeness) in summarization (Marcu 2000b; Daum´e and Marcu 2002; Louis, Joshi, and Nenkova 2010). Recently, Christensen et al. (2013, 2014) propose to perform sentence selection and ordering at the same time, and use constraints on discourse structure to make the summaries coherent. However, they represent the discourse as an unweighted directed graph, which is shallow and not sufficiently informative in most cases. Furthermore, their approach does not allow compression at the sentence level, which is often beneficial in summarization. In the future, we would like to investigate the utility of our rhetorical structure for performing sentence compression, selection, and ordering in a joint summarization process. Discourse s</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2013</marker>
<rawString>Christensen, Janara, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multi-document summarization. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT’13, pages 1163–1173, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland</author>
<author>Gagan Bansal</author>
<author>Mausam</author>
</authors>
<title>Hierarchical summarization: Scaling up multi-document summarization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL’13,</booktitle>
<pages>902--912</pages>
<location>Baltimore, MD.</location>
<marker>Christensen, Soderland, Bansal, Mausam, 2014</marker>
<rawString>Christensen, Janara, Stephen Soderland, Gagan Bansal, and Mausam. 2014. Hierarchical summarization: Scaling up multi-document summarization. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL’13, pages 902–912, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language Parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="92307" citStr="Collins 2003" startWordPosition="14764" endWordPosition="14765">rpora. Performances significantly superior to SPADE are denoted by *. accurate reranking parser (Charniak and Johnson 2005). However, because of the lack of gold syntactic trees in the instructional corpus, we trained SPADE in this corpus using the syntactic trees produced by the reranking parser. To avoid using the gold syntactic trees, we used the reranking parser in our system for both training and testing purposes. This syntactic parser was trained on the sections of the Penn Treebank not included in our test set. We applied the same canonical lexical head projection rules (Magerman 1995; Collins 2003) to lexicalize the syntactic trees as done in HILDA and SPADE. Note that previous studies (Fisher and Roark 2007; Soricut and Marcu 2003; Hernault et al. 2010) on discourse segmentation only report their performance on the RST–DT test set. To compare our results with them, we evaluated our model on the RST–DT test set. In addition, we showed a more general performance of SPADE and our system on the two corpora based on 10-fold cross validation.13 However, SPADE does not come with a training module for its segmenter. We reimplemented this module and verified its correctness by reproducing the r</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Collins, Michael. 2003. Head-driven statistical models for natural language Parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="9497" citStr="Collins and Koo 2005" startWordPosition="1411" endWordPosition="1414">tation, we use the inferred (posterior) probabilities from our CRF parsing models in a probabilistic CKY-like bottom–up parsing algorithm (Jurafsky and Martin 2008), which is non-greedy and optimal. Furthermore, a simple modification of this parsing algorithm allows us to generate k-best (i.e., the k highest probability) parse hypotheses for each input text that could then be used in a reranker to improve over the initial ranking using additional (global) features of the discourse tree as evidence, a strategy that has been successfully explored in syntactic parsing (Charniak and Johnson 2005; Collins and Koo 2005). Third, most of the existing discourse parsers do not discriminate between intrasentential parsing (i.e., building the DTs for the individual sentences) and multisentential parsing (i.e., building the DT for the whole document). However, we argue that distinguishing between these two parsing conditions can result in more effective parsing. Two separate parsing models could exploit the fact that rhetorical relations 387 Computational Linguistics Volume 41, Number 3 are distributed differently intra-sententially versus multi-sententially. Also, they could independently choose their own informat</context>
<context position="113964" citStr="Collins and Koo (2005)" startWordPosition="18132" endWordPosition="18135">f k-best parsing. The 1-best result tells that the parser has the base accuracy of 79.8%. The 2-best shows dramatic oracle-rate improvements (i.e., 4.65% absolute), meaning that often our parser generates the best tree as its top two outputs. 3-best and 4-best also show moderate improvements (about 2%). Things start to slow down afterwards, and we achieve oracle rates of 90.37% and 92.57% at 10−best and 20-best, respectively. The 30-best parsing gives an oracle score of 93.2%. The results of our k-best intra-sentential discourse parser demonstrate that a k-best reranking approach like that of Collins and Koo (2005) and Charniak and Johnson (2005) used for syntactic parsing can potentially improve the parsing accuracy even further by exploiting additional global features of the candidate discourse trees as evidence. The scenario is quite different at the document-level; Table 8 shows the k-best parsing results of TSP 1S-1S on the RST–DT test set. The improvements in oracle-rate are small at the document-level when compared with the sentence-level parsing. For example, the 2-best and the 5-best improve over the base accuracy by only 0.7 percentage points and 1.0 percentage points, respectively. The improv</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Collins, Michael and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="27564" citStr="Collobert et al. 2011" startWordPosition="4097" endWordPosition="4100">pendency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse parsing. However, as in Socher et al. (2013a, 2013b), word vectors (i.e., embeddings) are not learned explicitly for the task, rather they are taken from Collobert et al. (2011). Given the vectors of the words in an EDU, their model first composes them hierarchically based on a syntactic parse tree to get the vector representation for the EDU. Adjacen</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Collobert, Ronan, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Cristea</author>
<author>Nancy Ide</author>
<author>Laurent Romary</author>
</authors>
<title>Veins theory: A model of global discourse cohesion and coherence.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and of the 17th International Conference on Computational Linguistics (COLING/ACL’98),</booktitle>
<pages>281--285</pages>
<location>Montreal.</location>
<marker>Cristea, Ide, Romary, 1998</marker>
<rawString>Cristea, Dan, Nancy Ide, and Laurent Romary. 1998. Veins theory: A model of global discourse cohesion and coherence. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and of the 17th International Conference on Computational Linguistics (COLING/ACL’98), pages 281–285. Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Danlos</author>
</authors>
<title>D-STAG: A discourse analysis formalism based on synchronous TAGs.</title>
<date>2009</date>
<journal>TAL,</journal>
<volume>50</volume>
<issue>1</issue>
<contexts>
<context position="4063" citStr="Danlos (2009)" startWordPosition="584" endWordPosition="585">). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is driven by sentence semantics. Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse structure. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is perhaps the most influential theory of discourse in computational linguistics. Although it was initially intended to be used in text generation, later it became popular as a framework for parsing the structure of a text (Taboada and Mann 2006). RST represents texts by labeled hierarchical structures, called Discourse Trees (DTs). For example, consider the DT shown in Figure 1 for the following text: But he added: “Some people use the purchasers’ index as a lead</context>
</contexts>
<marker>Danlos, 2009</marker>
<rawString>Danlos, Laurence. 2009. D-STAG: A discourse analysis formalism based on synchronous TAGs. TAL, 50(1):111–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, ACL’07,</booktitle>
<pages>256--263</pages>
<location>Prague.</location>
<marker>Daum´e, 2007</marker>
<rawString>Daum´e, III, Hal. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, ACL’07, pages 256–263, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>449--456</pages>
<location>Philadelphia, PA.</location>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>Daum´e, III, Hal and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL ’02, pages 449–456, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Dinarelli</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Discriminative reranking for spoken language understanding.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing (TASLP),</journal>
<pages>20--526</pages>
<marker>Dinarelli, Moschitti, Riccardi, 2011</marker>
<rawString>Dinarelli, Marco, Alessandro Moschitti, and Giuseppe Riccardi. 2011. Discriminative reranking for spoken language understanding. IEEE Transactions on Audio, Speech and Language Processing (TASLP), 20:526–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David duVerle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>665--673</pages>
<contexts>
<context position="52937" citStr="duVerle and Prendinger (2009)" startWordPosition="8196" endWordPosition="8199"> is the set of features used in the parsing models, as summarized in Table 1. We categorize the features into seven groups and specify which groups are used in what parsing model. Notice that some of the features are used in both models. Most of the features have been explored in previous studies (e.g., Soricut and Marcu 2003; Sporleder and Lapata 2005; Hernault et al. 2010). However, we improve some of these as explained subsequently. The features are extracted from two adjacent discourse units Ut−1 and Ut. Organizational features encode useful information about text organization as shown by duVerle and Prendinger (2009). We measure the length of the discourse units as the number of EDUs and tokens in it. However, in order to better adjust to the length variations, rather than computing their absolute numbers in a unit, we choose to measure their relative numbers with respect to their total numbers in the two units. For example, if the two discourse units under consideration contain three EDUs in total, a unit containing two of the EDUs will have a relative EDU number of 0.67. We also measure the distances of the units in terms of the number of EDUs from the beginning and end of the sentence (or text in the m</context>
<context position="118048" citStr="duVerle and Prendinger (2009)" startWordPosition="18779" endWordPosition="18782">rformance over the previous set. Specifically, for sentence-level parsing, when we add the Organizational features with the Dominance set features, we achieve about 2 percentage points absolute improvements in nuclearity and relations. With N-gram features, the gain is even higher: 6 percentage points in relations and 3.5 percentage points in nuclearity for sentence-level parsing, and 3.8 percentage points in relations and 3.1 percentage points in nuclearity for document-level parsing. This demonstrates the utility of the N-gram features, which is also consistent with the previous findings of duVerle and Prendinger (2009) and Schilder (2002). The features extracted from Lexical chains (L-ch) have also proved to be useful for document-level parsing. They deliver absolute improvements of 2.7 percentage points, 2.9 percentage points, and 2.3 percentage points in span, nuclearity, and relations, 21 Text structural features are included in the Organizational features for multi-sentential parsing. 423 Computational Linguistics Volume 41, Number 3 Figure 17 Discourse trees generated by human annotator and our system for the text [what’s more,]e1 [he believes]e2 [seasonal swings in the auto industry this year aren’t o</context>
</contexts>
<marker>duVerle, Prendinger, 2009</marker>
<rawString>duVerle, David and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 665–673, Suntec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Egg</author>
<author>Alexander Koller</author>
<author>Joachim Niehren</author>
</authors>
<title>The constraint language for lambda structures.</title>
<date>2001</date>
<journal>Journal of Logic, Language and Information,</journal>
<volume>10</volume>
<issue>4</issue>
<marker>Egg, Koller, Niehren, 2001</marker>
<rawString>Egg, Markus, Alexander Koller, and Joachim Niehren. 2001. The constraint language for lambda structures. Journal of Logic, Language and Information, 10(4):457–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics -Volume 1, COLING ’96,</booktitle>
<pages>340--345</pages>
<location>Copenhagen.</location>
<contexts>
<context position="27161" citStr="Eisner (1996)" startWordPosition="4041" endWordPosition="4042">also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Eisner, Jason. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th Conference on Computational Linguistics -Volume 1, COLING ’96, pages 340–345, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet—An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="60788" citStr="Fellbaum 1998" startWordPosition="9519" endWordPosition="9520">Word Sense Disambiguation (WSD). P P P P P P P P 1 2 3 4 1 2 3 4 (b) (c) P P P P 1 2 3 4 (a) Figure 10 Correlation between lexical chains and discourse structure. (a) Lexical chains spanning paragraphs. (b) and (c) Two possible DT structures. 404 Joty, Carenini, and Ng CODRA Figure 11 Extracting lexical chains. (a) A Lexical Semantic Relatedness Graph (LSRG) for five noun-tokens. (b) Resultant graph after performing WSD. The box at the bottom shows the lexical chains. In the preprocessing step, we extract the nouns from the document and lemmatize them using WordNet’s built-in morphy function (Fellbaum 1998). Then, by looking up in WordNet we expand each noun to all of its senses, and build a Lexical Semantic Relatedness Graph (LSRG) (Galley and McKeown 2003; Chali and Joty 2007). In an LSRG, the nodes represent noun-tokens with their candidate senses, and the weighted edges between senses of two different tokens represent one of the three semantic relations: repetition, synonym, and hypernym. For example, Figure 11a shows a partial LSRG, where the token bank has two possible senses, namely, money bank and river bank. Using the money bank sense, bank is connected with institution and company by h</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane. 1998. WordNet—An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Text-level discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ’12,</booktitle>
<pages>60--68</pages>
<location>Jeju Island.</location>
<contexts>
<context position="7129" citStr="Feng and Hirst 2012" startWordPosition="1053" endWordPosition="1056">001) for a detailed description of the relations. Conventionally, rhetorical analysis in RST involves two subtasks: discourse segmentation is the task of breaking the text into a sequence of EDUs, and discourse parsing is the task of linking the discourse units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse parsing and rhetorical parsing interchangeably. While recent advances in automatic discourse segmentation have attained high accuracies (an F-score of 90.5% reported by Fisher and Roark [2007]), discourse parsing still poses significant challenges (Feng and Hirst 2012) and the performance of the existing discourse parsers (Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) is still considerably inferior compared with the human gold standard. Thus, the impact of rhetorical structure in downstream NLP applications is still very limited. The work we present in this article aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing discourse parsers. First, existing discourse parsers typically model the structure and the labels of a DT separately, and also do n</context>
<context position="31425" citStr="Feng and Hirst (2012)" startWordPosition="4690" endWordPosition="4693">like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT constituents, CODRA uses undirected conditional graphical models (i.e., CRFs) as its parsing models. To find the most probable DT, unlike most previous studies (Marcu 1999; Subba and Di-Eugenio 2009; Hernault et al. 2010), which adopt a greedy solution, CODRA applies an optimal CKY parsing algorithm to the inferred posterior probabilities (obtained from the CRFs) of all possible DT constituents. Furthermore, the parsing algorithm allows CODRA to generate a list of k</context>
<context position="70510" citStr="Feng and Hirst 2012" startWordPosition="11144" endWordPosition="11147">le parse trees. It is straightforward to generalize the above algorithm to produce k most probable DTs. Specifically, when filling up the dynamic programming tables, rather than storing a single best parse for each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates simultaneously. One can show that the time and space complexities of the k-best version of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005). Note that, in contrast to other document-level discourse parsers (Marcu 2000b; Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This approach of CODRA is also different from the sentence-level discourse parser SPADE (Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell D[i, j] in SPADE’s dynamic programming table stores D[i,j] = P([Ui(0), Um.(1), Uj(1)]) (11) where m* = argmax P([Ui(0), Um(1), Uj(1)]). Disregarding the relation label R while i&lt;m&lt;j populating D, this approach may find a discourse tr</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Feng, Vanessa and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ’12, pages 60–68, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>A linear-time bottom-up discourse parser with constraints and post-editing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14,</booktitle>
<pages>511--521</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="25918" citStr="Feng and Hirst (2014)" startWordPosition="3848" endWordPosition="3851">rendingerlab.net/hilda/. 391 Computational Linguistics Volume 41, Number 3 parsing, and both scenarios use a single uniform parsing model. Second, they take a greedy (i.e., sub-optimal) approach to construct a DT. Third, they disregard sequential dependencies between DT constituents. Furthermore, HILDA considers the structure and the labels of a DT separately. Our discourse parser CODRA, as described in the next section, addresses all these limitations. More recent work than ours also attempts to address some of the above-mentioned limitations of the existing discourse parsers. Similar to us, Feng and Hirst (2014) generate a document-level DT in two stages, where a multi-sentential parsing follows an intra-sentential one. At each stage, they iteratively use two separate linear-chain CRFs (Lafferty, McCallum, and Pereira 2001) in a cascade: one for predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsin</context>
<context position="65007" citStr="Feng and Hirst (2014)" startWordPosition="10239" endWordPosition="10242">ith this is to loop twice through the parsing process using two different parsing models—one trained with the complete feature set, and the other trained without the sub-structural features. We first build an initial, sub-optimal DT using the parsing model that is trained without the sub-structural features. This intermediate DT will now provide labels for the sub-structures. Next we can build a final, more accurate DT by using the complete parsing model. This idea of twopass discourse parsing, where the second pass performs post-editing using additional features, has recently been adopted by Feng and Hirst (2014) in their greedy parser. One could even continue doing post-editing multiple times until the DT converges. However, this could be very time consuming as each post-editing pass requires: (1) applying the parsing model to every possible unit sequence and computing the posterior marginals for all possible DT constituents, and (2) using the parsing algorithm to find the most probable DT. Recall from our earlier discussion in Section 4.1.3 that for n discourse units and M rhetorical relations, the first step requires O(M2n4) and O(M2n3) for intra- and multi-sentential parsing, respectively; we will</context>
</contexts>
<marker>Feng, Hirst, 2014</marker>
<rawString>Feng, Vanessa and Graeme Hirst. 2014. A linear-time bottom-up discourse parser with constraints and post-editing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14, pages 511–521, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, ACL’08,</booktitle>
<pages>959--967</pages>
<location>Columbus, OH.</location>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Finkel, Jenny Rose, Alex Kleeman, and Christopher Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, ACL’08, pages 959–967, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seeger Fisher</author>
<author>Brian Roark</author>
</authors>
<title>The utility of parse-derived features for automatic discourse segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, ACL’07,</booktitle>
<pages>488--495</pages>
<location>Prague.</location>
<contexts>
<context position="22543" citStr="Fisher and Roark (2007)" startWordPosition="3346" endWordPosition="3349">ep approach avoids illegal chunk sequences like a B–NUC followed by an I–SAT or a B–SAT followed by an I–NUC, and in this approach, it is easier to incorporate sentence-level properties like the constraint that a sentence must contain at least one nucleus. They examine whether shallow-syntactic features (e.g., POS and phrase tags) would be sufficient for these purposes. The evaluation on the RST–DT shows that the two-step approach outperforms the one-step approach, and its performance is comparable to that of SPADE, which requires relatively expensive full syntactic parses. In follow–up work, Fisher and Roark (2007) demonstrate over 4% absolute performance gain in discourse segmentation, by combining the features extracted from the syntactic tree with the ones derived via POS tagging and shallow syntactic parsing (i.e., chunking). Using quite a large number of features in a binary log-linear model, they achieve state-of-the-art performance in discourse segmentation on the RST–DT test set. In a different approach, Regneri, Egg, and Koller (2008) propose to use Underspecified Discourse Representation (UDR) as an intermediate representation for discourse parsing. Underspecified representations offer a singl</context>
<context position="71941" citStr="Fisher and Roark 2007" startWordPosition="11364" endWordPosition="11367">ely combined in a unified framework (Figure 2) to perform document-level rhetorical analysis. Recall that a key motivation for a two-stage10 parsing is that it allows us to capture the strong correlation between text structure and discourse structure in a scalable, modular, and flexible way. In the following, we describe two different approaches to model this correlation. 4.3.1 1S–1S (1 Sentence–1 Sub-tree). A key finding from previous studies on sentencelevel discourse analysis is that most sentences have a well-formed discourse sub-tree in the full document-level DT (Soricut and Marcu 2003; Fisher and Roark 2007). For example, Figure 13a shows 10 EDUs in three sentences (see boxes), where the DTs for the sentences obey their respective sentence boundaries. Our first approach, called 1S–1S (1 Sentence–1 Sub-tree), aims to maximally exploit this finding. It first constructs a DT for every sentence using our intra-sentential parser, and then it provides our multi-sentential parser with the sentence-level DTs to build the rhetorical parse for the whole document. 4.3.2 Sliding Window. Although the assumption made by 1S–1S clearly simplifies the parsing process, it completely ignores the cases where rhetori</context>
<context position="76956" citStr="Fisher and Roark 2007" startWordPosition="12240" endWordPosition="12243">hat one could try is: When both DTs segment the sentence into multiple sub-trees, pick the one with fewer sub-trees, and when only one of the DTs segment the sentence into multiple sub-trees, pick that one. At the end, the multi-sentential parser takes all these sentence-level sub-trees for a document, and builds a full rhetorical parse for the whole document. 5. The Discourse Segmenter Our discourse parser assumes that the input text has been already segmented into a sequence of EDUs. However, discourse segmentation is also a challenging problem, and previous studies (Soricut and Marcu 2003; Fisher and Roark 2007) have identified (a) (b) S (i) S1 S2 S3 (c) 2 (ii) 1 2 3 4 5 6 7 4 5 6 7 8 9 10 4 5 6 7 8 9 10 S1 S2 S2 S 3 S2 S3 1 2 3 4 5 6 7 4 5 6 7 8 9 10 410 Joty, Carenini, and Ng CODRA it as a primary source of inaccuracy for discourse parsing. Regardless of its importance in discourse parsing, discourse segmentation itself can be useful in several NLP applications, including sentence compression (Sporleder and Lapata 2005) and textual alignment in statistical machine translation (Stede 2011). Therefore, in CODRA, we have developed our own discourse segmenter, which not only achieves state-of-theart pe</context>
<context position="81754" citStr="Fisher and Roark 2007" startWordPosition="13046" endWordPosition="13049">ersion is also found to be rare, other variations of the production, depending on whether they include the lexical heads and how many non-terminals (one or two) they consider before and after the potential boundary, are examined one after another (see Fisher and Roark [2007] for details). In contrast, we compute the maximum likelihood estimates for a primary production (feature) and its other variations, and use those directly as features with/without binarizing the values. Shallow syntactic features like Chunk and POS tags have been shown to possess valuable clues for discourse segmentation (Fisher and Roark 2007; Sporleder and Lapata 2005). For example, it is less likely that an EDU boundary occurs within a chunk. We annotate the tokens of a sentence with chunk and POS tags using the state-of-the-art Illinois taggerll and encode these as features in our model. Note that the chunker assigns each token a tag using the BIO notation, where B stands for beginning of a particular phrase (e.g., noun or verb phrase), I stands for inside of a particular phrase, and O stands for outside of a particular phrase. The rationale for using the Illinois chunker is that it uses a larger set of tags (23 in total); thus</context>
<context position="86362" citStr="Fisher and Roark 2007" startWordPosition="13775" endWordPosition="13778">o not exclude any sentence in our discourse segmentation experiments. 6.2 Evaluation (and Agreement) Metrics In this subsection we describe the metrics used to measure both how much the annotators agree with each other, and how well the systems perform when their outputs are compared with human annotations for the discourse analysis tasks. 6.2.1 Metrics for Discourse Segmentation. Because sentence boundaries are considered to also be the EDU boundaries, we measure segmentation accuracy with respect to the intra-sentential segment boundaries, which is a standard method (Soricut and Marcu 2003; Fisher and Roark 2007). Specifically, if a sentence contains n EDUs, which corresponds to n − 1 intra-sentential segment boundaries, we measure the segmenter’s ability to correctly identify these n − 1 boundaries. Let h be the total number of intrasentential segment boundaries in the human annotation, m be the total number of intrasentential segment boundaries in the model output, and c be the total number of correct segment boundaries in the model output. Then, we measure Precision (P), Recall (R), and F-score for segmentation performance as follows: P= cm, R = ch, and F−score = 2PR P + R =2c (14) h + m 413 Comput</context>
<context position="90860" citStr="Fisher and Roark (2007)" startWordPosition="14540" endWordPosition="14543">iscourse segmentation), and the discourse trees are binary, then we get the same figures for precision, recall, and F-score. 6.3 Discourse Segmentation Evaluation In this section we present our experiments on discourse segmentation. 6.3.1 Experimental Set-up for Discourse Segmentation. We compare the performance of our discourse segmenter with the performance of the two publicly available discourse segmenters, namely, the discourse segmenters of the HILDA (Hernault et al. 2010) and SPADE (Soricut and Marcu 2003) systems. We also compare our results with the stateof-the-art results reported by Fisher and Roark (2007) on the RST–DT test set. In all our experiments when comparing two systems, we use paired t-test on the F-scores to measure statistical significance and report the p-value. We ran HILDA with its default settings. For SPADE, we applied the same modifications to its default settings as described in Fisher and Roark (2007), which delivers significantly improved performance over its original version. Specifically, in our experiments on the RST–DT corpus, we trained SPADE using the human-annotated syntactic trees extracted from the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1994), and, dur</context>
<context position="92419" citStr="Fisher and Roark 2007" startWordPosition="14781" endWordPosition="14784">k and Johnson 2005). However, because of the lack of gold syntactic trees in the instructional corpus, we trained SPADE in this corpus using the syntactic trees produced by the reranking parser. To avoid using the gold syntactic trees, we used the reranking parser in our system for both training and testing purposes. This syntactic parser was trained on the sections of the Penn Treebank not included in our test set. We applied the same canonical lexical head projection rules (Magerman 1995; Collins 2003) to lexicalize the syntactic trees as done in HILDA and SPADE. Note that previous studies (Fisher and Roark 2007; Soricut and Marcu 2003; Hernault et al. 2010) on discourse segmentation only report their performance on the RST–DT test set. To compare our results with them, we evaluated our model on the RST–DT test set. In addition, we showed a more general performance of SPADE and our system on the two corpora based on 10-fold cross validation.13 However, SPADE does not come with a training module for its segmenter. We reimplemented this module and verified its correctness by reproducing the results on the RST–DT test set. 6.3.2 Results for Discourse Segmentation. Table 3 shows the discourse segmentatio</context>
<context position="93841" citStr="Fisher and Roark (2007)" startWordPosition="15008" endWordPosition="15011">at the high segmentation accuracy reported by Hernault et al. (2010) is due to a less stringent evaluation metric. SPADE performs much better than HILDA with an absolute F-score improvement of 11.1%. Our segmenter DS outperforms SPADE with an absolute F-score improvement of 4.9% (p-value &lt; 2.4e-06), and also achieves comparable results to the ones of Fisher and Roark (2007) (F&amp;R), even though we use fewer features.14 Notice that human agreement for this task is quite high—namely, an F-score of 98.3 computed on the doubly-annotated portion of the RST–DT corpus mentioned in Section 6.1. Because Fisher and Roark (2007) only report their results on the RST–DT test set and we did not have access to their system, we compare our approach only with SPADE when evaluating on a whole corpus based on 10-fold cross validation. On the RST–DT corpus, our segmenter delivers an absolute F-score improvement of 3.8 percentage points, which represents a more than 25% relative error rate reduction. 13 Because the two tasks—discourse segmentation and intra-sentential parsing—operate at the sentence level, the cross validation was performed over sentences for their evaluation. 14 Because we did not have access to the system or</context>
</contexts>
<marker>Fisher, Roark, 2007</marker>
<rawString>Fisher, Seeger and Brian Roark. 2007. The utility of parse-derived features for automatic discourse segmentation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, ACL’07, pages 488–495, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Improving word sense disambiguation in lexical chaining.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI’03,</booktitle>
<pages>1486--1488</pages>
<location>Acapulco.</location>
<contexts>
<context position="60046" citStr="Galley and McKeown 2003" startWordPosition="9391" endWordPosition="9394">kipping paragraphs P2 and P3, while a second chain only spans P2 and P3. This situation makes it more likely that P2 and P3 should be linked in the DT before either of them is linked with another paragraph. Therefore, the DT structure in Figure 10b should be more likely than the structure in Figure 10c. One challenge in computing lexical chains is that words can have multiple senses, and semantic relationships depend on the sense rather than the word itself. Several methods have been proposed to compute lexical chains (Barzilay and Elhadad 1997; Hirst and St. Onge 1997; Silber and McCoy 2002; Galley and McKeown 2003). We follow the state-of-the-art approach proposed by Galley and McKeown (2003), which extracts lexical chains after performing Word Sense Disambiguation (WSD). P P P P P P P P 1 2 3 4 1 2 3 4 (b) (c) P P P P 1 2 3 4 (a) Figure 10 Correlation between lexical chains and discourse structure. (a) Lexical chains spanning paragraphs. (b) and (c) Two possible DT structures. 404 Joty, Carenini, and Ng CODRA Figure 11 Extracting lexical chains. (a) A Lexical Semantic Relatedness Graph (LSRG) for five noun-tokens. (b) Resultant graph after performing WSD. The box at the bottom shows the lexical chains.</context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>Galley, Michel and Kathleen McKeown. 2003. Improving word sense disambiguation in lexical chaining. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, IJCAI’03, pages 1486–1488, Acapulco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric Fosler-Lussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>562--569</pages>
<location>Sapporo.</location>
<contexts>
<context position="59091" citStr="Galley et al. 2003" startWordPosition="9230" endWordPosition="9233">Us ei:j and ej+1:k, respectively, we first compute the dominance set from the DS-LST of the sentence. We then extract the element from the set that holds across the EDUs j and j + 1. In our example, for the two units, containing EDUs e1 and e2, respectively, the relevant dominance set element is (1, efforts/NP)&gt;(2, to/S). We encode the syntactic labels and lexical heads of NH and NA, and the dominance relationship as features in our intra-sentential parsing model. Lexical chains (Morris and Hirst 1991) are sequences of semantically related words that can indicate topical boundaries in a text (Galley et al. 2003; Joty, Carenini, and Ng 2013). Features extracted from lexical chains are also shown to be useful for finding paragraph-level discourse structure (Sporleder and Lapata 2004). For example, consider the text with four paragraphs (P1 to P4) in Figure 10a. Now, let us assume that there is a lexical chain that spans the whole text, skipping paragraphs P2 and P3, while a second chain only spans P2 and P3. This situation makes it more likely that P2 and P3 should be linked in the DT before either of them is linked with another paragraph. Therefore, the DT structure in Figure 10b should be more likel</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>Galley, Michel, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics - Volume 1, ACL ’03, pages 562–569, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Giuseppe Riccardi</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow discourse parsing with conditional random fields.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing, IJCNLP’11,</booktitle>
<pages>1071--1079</pages>
<location>Chiang Mai.</location>
<contexts>
<context position="42149" citStr="Ghosh et al. 2011" startWordPosition="6355" endWordPosition="6358">vely, and s,c is the weight vector for the factors over the between-chain edges. A DCRF is a generalization of linear-chain CRFs (Lafferty, McCallum, and Pereira 2001) to represent complex interactions between output variables (i.e., labels), such as when performing multiple labeling tasks on the same sequence. Recently, there has been an explosion of interest in CRFs for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel, Kleeman, and Manning 2008), syntactic chunking (Sha and Pereira 2003), and discourse chunking (Ghosh et al. 2011) in accordance with the Penn Discourse Treebank (Prasad et al. 2008). DCRFs, being a discriminative approach to sequence modeling, have several advantages over their generative counterparts such as Hidden Markov Models (HMMs) and MRFs, which first model the joint distribution p(y,x|), and then infer the conditional distribution p(y|x, ). It has been advocated that discriminative models are generally more accurate than generative ones because they do not “waste resources” modeling complex distributions that are observed (i.e., p(x)); instead, they focus directly on modeling what we care about</context>
</contexts>
<marker>Ghosh, Johansson, Riccardi, Tonelli, 2011</marker>
<rawString>Ghosh, Sucheta, Richard Johansson, Giuseppe Riccardi, and Sara Tonelli. 2011. Shallow discourse parsing with conditional random fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing, IJCNLP’11, pages 1071–1079, Chiang Mai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Linguistic measures for automatic machine translation evaluation.</title>
<date>2010</date>
<booktitle>Machine Translation,</booktitle>
<pages>24--3</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Gim´enez, Jes´us and Lluis M`arquez. 2010. Linguistic measures for automatic machine translation evaluation. Machine Translation, 24(3–4):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Shafiq Joty</author>
<author>Lluis M`arquez</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Massimo Nicosia</author>
</authors>
<title>Learning to differentiate better from worse translations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>214--220</pages>
<location>Doha.</location>
<marker>Guzm´an, Joty, M`arquez, Moschitti, Nakov, Nicosia, 2014</marker>
<rawString>Guzm´an, Francisco, Shafiq Joty, Lluis M`arquez, Alessandro Moschitti, Preslav Nakov, and Massimo Nicosia. 2014a. Learning to differentiate better from worse translations. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214–220, Doha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Shafiq Joty</author>
<author>Lluis M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>Using discourse structure improves machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>687--698</pages>
<location>Baltimore, MD.</location>
<marker>Guzm´an, Joty, M`arquez, Nakov, 2014</marker>
<rawString>Guzm´an, Francisco, Shafiq Joty, Lluis M`arquez, and Preslav Nakov. 2014b. Using discourse structure improves machine translation evaluation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 687–698, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="2711" citStr="Halliday and Hasan 1976" startWordPosition="388" endWordPosition="391">iversity of British Columbia, Vancouver, BC, Canada, V6T 1Z4. E-mail: rng®cs.ubc.ca. Submission received: 11 May 2014; revised version received: 29 January 2015; accepted for publication: 18 March 2015. doi:10.1162/COLI a 00226 No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, a</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, Michael and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Document-wide decoding for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>1179--1190</pages>
<location>Jeju Island.</location>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Hardmeier, Christian, Joakim Nivre, and J¨org Tiedemann. 2012. Document-wide decoding for phrase-based statistical machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 1179–1190, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Helmut Prendinger</author>
<author>David duVerle</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>HILDA: A discourse parser using support vector machine classification. Dialogue and Discourse,</title>
<date>2010</date>
<contexts>
<context position="7257" citStr="Hernault et al. 2010" startWordPosition="1074" endWordPosition="1077">egmentation is the task of breaking the text into a sequence of EDUs, and discourse parsing is the task of linking the discourse units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse parsing and rhetorical parsing interchangeably. While recent advances in automatic discourse segmentation have attained high accuracies (an F-score of 90.5% reported by Fisher and Roark [2007]), discourse parsing still poses significant challenges (Feng and Hirst 2012) and the performance of the existing discourse parsers (Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) is still considerably inferior compared with the human gold standard. Thus, the impact of rhetorical structure in downstream NLP applications is still very limited. The work we present in this article aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing discourse parsers. First, existing discourse parsers typically model the structure and the labels of a DT separately, and also do not take into account the sequential dependencies between the DT constituents. However, for several NLP tasks, it has recently be</context>
<context position="13541" citStr="Hernault et al. 2010" startWordPosition="2009" endWordPosition="2012">in Section 2, we present our rhetorical analysis framework in Section 3. In Section 4, we describe our discourse parser. Then, in Section 5 we present our discourse segmenter. The experiments and analysis of results are presented in Section 6. Finally, we summarize our contributions with future directions in Section 7. 2. Related Work Rhetorical analysis has a long history—dating back to Mann and Thompson (1988), when RST was initially proposed as a useful linguistic method for describing natural texts, to more recent attempts to automatically extract the rhetorical structure of a given text (Hernault et al. 2010). In this section, we provide a brief overview of the computational approaches that follow RST as the theory of discourse, and that are related to our work; see the survey by Stede (2011) for a broader overview that also includes other theories of discourse. 388 Joty, Carenini, and Ng CODRA 2.1 Unsupervised and Rule-Based Approaches Although the most effective approaches to rhetorical analysis to date rely on supervised machine learning methods trained on human-annotated data, unsupervised methods have also been proposed, as they do not require human-annotated data and can be more easily appli</context>
<context position="23829" citStr="Hernault et al. (2010)" startWordPosition="3539" endWordPosition="3542">nguistic structure, and have been primarily used to deal with scope ambiguity in semantic structures (Reyle 1993; Egg, Koller, and Niehren 2001; Althaus et al. 2003; Koller, Regneri, and Thater 2008). Assuming that a UDR of a DT is already given in the form of a dominance graph (Althaus et al. 2003), Regneri, Egg, and Koller (2008) convert it into a more expressive and complete UDR representation called regular tree grammar (Koller, Regneri, and Thater 2008), for which efficient algorithms (Knight and Graehl 2005) already exist to derive the best configuration (i.e., the best discourse tree). Hernault et al. (2010) present the publicly available HILDA system,3 which comes with a discourse segmenter and a parser based on Support Vector Machines (SVMs). The discourse segmenter is a binary SVM classifier that uses the same lexico-syntactic features used in SPADE, but with more context (i.e., the lexico-syntactic features for the previous two words and the following two words). The discourse parser iteratively uses two SVM classifiers in a pipeline to build a DT. In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected uni</context>
<context position="31030" citStr="Hernault et al. 2010" startWordPosition="4634" endWordPosition="4637"> (EDUs or larger units) to relate (i.e., the structure), and what relations (i.e., the labels) to use in the process of building the DT. Specifically, discourse parsing requires: (1) a parsing model to explore the search space of possible structures and labels for their nodes, and (2) a parsing algorithm for selecting the best parse tree(s) among the candidates. A probabilistic parsing model like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT constituents, CODRA uses undirected conditional graphical models (i.e., CRF</context>
<context position="33748" citStr="Hernault et al. 2010" startWordPosition="5036" endWordPosition="5039">ble DT constituents for a given text (i.e., EDUs); then it uses a CKY parsing algorithm to combine those probabilities and find the most probable DT. Another crucial question related to parsing models is whether to use a single model or two different models for parsing at the sentence-level (i.e., intra-sentential) and at the document-level (i.e., multi-sentential). A simple and straightforward strategy would be to use a single unified parsing model for both intra- and multi-sentential parsing without distinguishing the two cases, as was previously done (Marcu 1999; Subba and Di-Eugenio 2009; Hernault et al. 2010). That approach has the advantages of making the parsing process easier, and the model gets more data to learn from. However, for a solution like ours, which tries to capture the interdependencies between constituents, this would be problematic with respect to scalability and inappropriate because of two modeling issues. More specifically, for scalability note that the number of valid trees grows exponentially with the number of EDUs in a document.&apos; Therefore, an exhaustive search over all the valid DTs is often infeasible, even for relatively small documents. For modeling, a single unified ap</context>
<context position="37529" citStr="Hernault et al. 2010" startWordPosition="5616" endWordPosition="5619"> for the second sentence in Figure 1 can be represented as {Elaboration–NS[4,4,5], Same–Unit–NN[4,5,6]}. Notice that in this representation, a relation R also specifies the nuclearity status of the discourse units involved, which can be one of Nucleus–Satellite (NS), Satellite–Nucleus (SN), or Nucleus–Nucleus (NN). Attaching nuclearity status to the relations allows us to perform the two subtasks of discourse parsing, relation identification and nuclearity assignment, simultaneously. A common assumption made for generating DTs effectively is that they are binary trees (Soricut and Marcu 2003; Hernault et al. 2010). That is, multi-nuclear relations (e.g., Joint, Same–Unit) involving more than two discourse units are mapped to a hierarchical right-branching binary tree. For example, a flat Joint(e1,e2,e3,e4) (Figure 4a) is mapped to a right-branching binary tree Joint(e1, Joint(e2, Joint(e3, e4))) (Figure 4b). Figure 4 Multi-nuclear relation and its corresponding binary tree representation. 395 Computational Linguistics Volume 41, Number 3 4.1 Parsing Models As mentioned before, the job of the intra- and multi-sentential parsing models of CODRA is to assign a probability to each of the constituents of al</context>
<context position="52685" citStr="Hernault et al. 2010" startWordPosition="8159" endWordPosition="8162">n order to avoid overfitting, we regularize the CRF models with l2 regularization and learn the model parameters using the limited-memory BFGS (L-BFGS) fitting algorithm. 4.1.4 Features Used in the Parsing Models. Crucial to parsing performance is the set of features used in the parsing models, as summarized in Table 1. We categorize the features into seven groups and specify which groups are used in what parsing model. Notice that some of the features are used in both models. Most of the features have been explored in previous studies (e.g., Soricut and Marcu 2003; Sporleder and Lapata 2005; Hernault et al. 2010). However, we improve some of these as explained subsequently. The features are extracted from two adjacent discourse units Ut−1 and Ut. Organizational features encode useful information about text organization as shown by duVerle and Prendinger (2009). We measure the length of the discourse units as the number of EDUs and tokens in it. However, in order to better adjust to the length variations, rather than computing their absolute numbers in a unit, we choose to measure their relative numbers with respect to their total numbers in the two units. For example, if the two discourse units under </context>
<context position="54001" citStr="Hernault et al. 2010" startWordPosition="8372" endWordPosition="8375">e EDU number of 0.67. We also measure the distances of the units in terms of the number of EDUs from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the discourse units. Discourse cues (e.g., because, but), when present, signal rhetorical relations between two text segments, and have been used as a primary source of information in earlier studies (Knott and Dale 1994; Marcu 2000a). However, recent studies (Hernault et al. 2010; Biran and Rambow 2011) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of cue phrases, since this approach is domain independent and capable of capturing non-lexical cues such as punctuation. In order to build a lexical N-gram dictionary empirically from the training corpus, we extract the first and last N tokens (NE11, 2,3}) of each discourse unit and rank them according to their mutual information with the two labels, Structure (S) and Relation (R). 401 Computational Linguistics Volume 41, Number 3 Table 1 Features used in our intra- and m</context>
<context position="56268" citStr="Hernault et al. 2010" startWordPosition="8769" endWordPosition="8772">umber of chains skipping both unit 1 and unit 2. Number of chains skipping unit 1 (or unit 2). 2 Contextual features Intra &amp; Multi-Sentential Previous and next feature vectors. 2 Sub-structural features Intra &amp; Multi-Sentential Root nodes of the left and right rhetorical sub-trees. More specifically, given an N-gram x, we compute its conditional entropy H with respect to S and R as follows:8 � c(x, s, r) H(S,R|x) _ − log (7) scS rER c(x) where c(x) is the empirical count of N-gram x, and c(x, s, r) is the joint empirical count of N-gram x with the labels s and r. This is in contrast to HILDA (Hernault et al. 2010), which ranks the N-grams by their frequencies in the training corpus. However, Blitzer 8 The higher the conditional entropy, the lower the mutual information, and vice versa. 402 Joty, Carenini, and Ng CODRA (2008) found mutual information to be more effective than frequency as a method for feature selection. Intuitively, the most informative discourse cues are not only the most frequent, but also the ones that are indicative of the labels in the training data. In addition to the lexical N-grams we also encode the POS tags of the first and last N tokens (NE{1, 2,3}) in a discourse unit as sha</context>
<context position="70489" citStr="Hernault et al. 2010" startWordPosition="11140" endWordPosition="11143"> list of k most probable parse trees. It is straightforward to generalize the above algorithm to produce k most probable DTs. Specifically, when filling up the dynamic programming tables, rather than storing a single best parse for each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates simultaneously. One can show that the time and space complexities of the k-best version of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005). Note that, in contrast to other document-level discourse parsers (Marcu 2000b; Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This approach of CODRA is also different from the sentence-level discourse parser SPADE (Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell D[i, j] in SPADE’s dynamic programming table stores D[i,j] = P([Ui(0), Um.(1), Uj(1)]) (11) where m* = argmax P([Ui(0), Um(1), Uj(1)]). Disregarding the relation label R while i&lt;m&lt;j populating D, this approach ma</context>
</contexts>
<marker>Hernault, Prendinger, duVerle, Ishizuka, 2010</marker>
<rawString>Hernault, Hugo, Helmut Prendinger, David duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser using support vector machine classification. Dialogue and Discourse, 1(3):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical Chains as Representation of Context for the Detection and Correction of Malapropisms.</title>
<date>1997</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database and Some of</booktitle>
<pages>305--332</pages>
<publisher>MIT press,</publisher>
<marker>Hirst, St-Onge, 1997</marker>
<rawString>Hirst, Graeme and David St-Onge. 1997. Lexical Chains as Representation of Context for the Detection and Correction of Malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database and Some of its Applications. MIT press, pages 305–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>Coherence and coreference.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--67</pages>
<contexts>
<context position="2724" citStr="Hobbs 1979" startWordPosition="392" endWordPosition="393">bia, Vancouver, BC, Canada, V6T 1Z4. E-mail: rng®cs.ubc.ca. Submission received: 11 May 2014; revised version received: 29 January 2015; accepted for publication: 18 March 2015. doi:10.1162/COLI a 00226 No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder </context>
</contexts>
<marker>Hobbs, 1979</marker>
<rawString>Hobbs, Jerry. 1979. Coherence and coreference. Cognitive Science, 3:67–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05,</booktitle>
<pages>53--64</pages>
<location>Stroudsburg, PA.</location>
<contexts>
<context position="70361" citStr="Huang and Chiang 2005" startWordPosition="11121" endWordPosition="11124">), respectively. A key advantage of using a probabilistic parsing algorithm like the one we use is that it allows us to generate a list of k most probable parse trees. It is straightforward to generalize the above algorithm to produce k most probable DTs. Specifically, when filling up the dynamic programming tables, rather than storing a single best parse for each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates simultaneously. One can show that the time and space complexities of the k-best version of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005). Note that, in contrast to other document-level discourse parsers (Marcu 2000b; Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This approach of CODRA is also different from the sentence-level discourse parser SPADE (Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell D[i, j] in SPADE’s dynamic programming table stores D[i,j] = P([Ui(0), Um.(1), Uj(1)</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Huang, Liang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, Parsing ’05, pages 53–64, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Representation learning for text-level discourse parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>13--24</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="28727" citStr="Ji and Eisenstein (2014)" startWordPosition="4283" endWordPosition="4286">se tree to get the vector representation for the EDU. Adjacent discourse units are then merged hierarchically to get the vector representations for the higher order discourse units. In every step, the merging is done using one binary (structure) and one multi-class (relation) classifier, each having a three-layer neural network architecture. The cost function for training the model is given by these two cascaded classifiers applied at different levels of the DT. Similar to our method, they use the classifier probabilities in a CKY-like parsing algorithm to find the global optimal DT. Finally, Ji and Eisenstein (2014) present a feature representation learning method in a shift–reduce discourse parser (Marcu 1999). Unlike DNNs, which learn non-linear feature transformations in a maximum likelihood model, they learn linear transformations of features in a max margin classification model. 3. Overview of Our Rhetorical Analysis Framework CODRA takes as input a raw text and produces a discourse tree that describes the text in terms of coherence relations that hold between adjacent discourse units (i.e., clauses, sentences) in the text. An example DT generated by an online demo of CODRA 392 Joty, Carenini, and N</context>
</contexts>
<marker>Ji, Eisenstein, 2014</marker>
<rawString>Ji, Yangfeng and Jacob Eisenstein. 2014. Representation learning for text-level discourse parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13–24, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>A novel discriminative framework for sentence-level discourse analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>904--915</pages>
<location>Jeju Island.</location>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>Joty, Shafiq, Giuseppe Carenini, and Raymond T. Ng. 2012. A novel discriminative framework for sentence-level discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 904–915, Jeju Island.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shafiq Joty</author>
</authors>
<title>Giuseppe Carenini,</title>
<location>and</location>
<marker>Joty, </marker>
<rawString>Joty, Shafiq, Giuseppe Carenini, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond T Ng</author>
</authors>
<title>Topic segmentation and labeling in asynchronous Conversations.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>47--521</pages>
<contexts>
<context position="59121" citStr="Ng 2013" startWordPosition="9237" endWordPosition="9238">st compute the dominance set from the DS-LST of the sentence. We then extract the element from the set that holds across the EDUs j and j + 1. In our example, for the two units, containing EDUs e1 and e2, respectively, the relevant dominance set element is (1, efforts/NP)&gt;(2, to/S). We encode the syntactic labels and lexical heads of NH and NA, and the dominance relationship as features in our intra-sentential parsing model. Lexical chains (Morris and Hirst 1991) are sequences of semantically related words that can indicate topical boundaries in a text (Galley et al. 2003; Joty, Carenini, and Ng 2013). Features extracted from lexical chains are also shown to be useful for finding paragraph-level discourse structure (Sporleder and Lapata 2004). For example, consider the text with four paragraphs (P1 to P4) in Figure 10a. Now, let us assume that there is a lexical chain that spans the whole text, skipping paragraphs P2 and P3, while a second chain only spans P2 and P3. This situation makes it more likely that P2 and P3 should be linked in the DT before either of them is linked with another paragraph. Therefore, the DT structure in Figure 10b should be more likely than the structure in Figure</context>
</contexts>
<marker>Ng, 2013</marker>
<rawString>Raymond T. Ng. 2013. Topic segmentation and labeling in asynchronous Conversations. Journal of Artificial Intelligence Research (JAIR), 47:521–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining intra- and multi-sentential rhetorical parsing for document-level discourse analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13,</booktitle>
<pages>486--496</pages>
<location>Sofia.</location>
<contexts>
<context position="101956" citStr="Joty et al. 2013" startWordPosition="16249" endWordPosition="16252">respectively.18 Our parser PAR-S reduces the errors by 76.1%, 62.4%, and 34.6% in span, nuclearity and relations, respectively. If we compare the performance of our intra-sentential discourse parser on the two corpora, we notice that our parser PAR-S is more accurate in finding the right tree 16 The parsing performance reported in Table 4 for CRF-NC is when the CRF parsing model is trained on a balanced data set (an equal number of instances with S=1 and S=0); Training on full but imbalanced data set gives slightly lower results. 17 Our EMNLP and ACL publications (Joty, Carenini, and Ng 2012; Joty et al. 2013) reported slightly lower parsing accuracies. Fixing a bug in the parsing algorithm accounts for the difference. 18 Subba and Di-Eugenio (2009) report their results based on an arbitrary split between training and test sets. Because we did not have access to their particular split, we compare our model’s performance based on 10-fold cross validation with their reported results. Also, because we did not have access to their system/output, we could not perform a significance test on the instructional corpus. 418 Joty, Carenini, and Ng CODRA Table 4 Intra-sentential parsing results based on manual</context>
<context position="133055" citStr="Joty et al. 2013" startWordPosition="21246" endWordPosition="21249">ecommended by several recent discourse theories (Wolf and Gibson 2005). Once we achieve similar performance on graph structures, we will perform extrinsic evaluations to determine their relative utility for various NLP tasks. Finally, we hope that the online demo, the source code of CODRA, and the evaluation metrics that we made publicly available in this work will facilitate other researchers in extending our work and in applying discourse parsing to their NLP tasks. Bibliographic Note Portions of this work were previously published in two conference proceedings (Joty, Carenini, and Ng 2012; Joty et al. 2013). This article significantly extends our previous work in several ways, most notably: (1) we extend the parsing algorithm to generate k-most probable parse hypotheses for each input text (Section 4.2); (2) we show the oracle accuracies for k-best discourse parsing both at the sentence level and at the document level (Section 6.4.4); (3) to support our claim, we compare our best results with several variations of our approach (see CRF-NC in Section 6.4.2, and CRF-O and CRF-T in Section 6.4.3); (4) we analyze the relative importance of different features for intra- and multi-sentential discourse</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Joty, Shafiq, Giuseppe Carenini, Raymond T. Ng, and Yashar Mehdad. 2013. Combining intra- and multi-sentential rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13, pages 486–496, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Francisco Guzm´an</author>
<author>Lluis M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>DiscoTK: Using discourse structure for machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14,</booktitle>
<pages>402--408</pages>
<location>Baltimore, MD.</location>
<marker>Joty, Guzm´an, M`arquez, Nakov, 2014</marker>
<rawString>Joty, Shafiq, Francisco Guzm´an, Lluis M`arquez, and Preslav Nakov. 2014. DiscoTK: Using discourse structure for machine translation evaluation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT ’14, pages 402–408, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James Martin</author>
</authors>
<title>Statistical parsing.</title>
<date>2008</date>
<booktitle>In Speech and Language Processing, chapter 14.</booktitle>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="9040" citStr="Jurafsky and Martin 2008" startWordPosition="1340" endWordPosition="1343">scriminative parsing models, expressed as Conditional Random Fields (CRFs) (Sutton, McCallum, and Rohanimanesh 2007), to infer the probability of all possible DT constituents. The CRF models effectively represent the structure and the label of a DT constituent jointly, and, whenever possible, capture the sequential dependencies. Second, existing discourse parsers typically apply greedy and sub-optimal parsing algorithms to build a DT. To cope with this limitation, we use the inferred (posterior) probabilities from our CRF parsing models in a probabilistic CKY-like bottom–up parsing algorithm (Jurafsky and Martin 2008), which is non-greedy and optimal. Furthermore, a simple modification of this parsing algorithm allows us to generate k-best (i.e., the k highest probability) parse hypotheses for each input text that could then be used in a reranker to improve over the initial ranking using additional (global) features of the discourse tree as evidence, a strategy that has been successfully explored in syntactic parsing (Charniak and Johnson 2005; Collins and Koo 2005). Third, most of the existing discourse parsers do not discriminate between intrasentential parsing (i.e., building the DTs for the individual </context>
<context position="67272" citStr="Jurafsky and Martin 2008" startWordPosition="10587" endWordPosition="10590">cal semantics, subjectivity, and TF.IDF-based cosine similarity. However, because such features did not improve parsing performance on our development set, they were excluded from our final set of features. 4.2 Parsing Algorithm The intra- and multi-sentential parsing models of CODRA assign a probability to every possible DT constituent in their respective parsing scenarios. The job of the parsing algorithm is then to find the k most probable DTs for a given text. We implement a probabilistic CKY-like bottom–up parsing algorithm that uses dynamic programming to compute the most likely parses (Jurafsky and Martin 2008). For simplicity, we first 406 Joty, Carenini, and Ng CODRA describe the specific case of generating the single most probable DT, then we describe how to generalize this algorithm to produce the k most probable DTs for a given text. Formally, the search problem for finding the most probable DT can be written as DT* = argmax P(DT|©) (8) DT where © specifies the parameters of the parsing model (intra- or multi-sentential). Given n discourse units, our parsing algorithm uses the upper-triangular portion of the nxn dynamic programming table D, where cell D[i, j] (for i &lt; j) stores: D[i,j] = P(r*[U</context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Jurafsky, Daniel and James Martin. 2008. Statistical parsing. In Speech and Language Processing, chapter 14. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>An overview of probabilistic tree transducers for natural language processing.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>3406</volume>
<pages>1--24</pages>
<publisher>Springer,</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="23726" citStr="Knight and Graehl 2005" startWordPosition="3523" endWordPosition="3526">rspecified representations offer a single compact representation to express possible ambiguities in a linguistic structure, and have been primarily used to deal with scope ambiguity in semantic structures (Reyle 1993; Egg, Koller, and Niehren 2001; Althaus et al. 2003; Koller, Regneri, and Thater 2008). Assuming that a UDR of a DT is already given in the form of a dominance graph (Althaus et al. 2003), Regneri, Egg, and Koller (2008) convert it into a more expressive and complete UDR representation called regular tree grammar (Koller, Regneri, and Thater 2008), for which efficient algorithms (Knight and Graehl 2005) already exist to derive the best configuration (i.e., the best discourse tree). Hernault et al. (2010) present the publicly available HILDA system,3 which comes with a discourse segmenter and a parser based on Support Vector Machines (SVMs). The discourse segmenter is a binary SVM classifier that uses the same lexico-syntactic features used in SPADE, but with more context (i.e., the lexico-syntactic features for the previous two words and the following two words). The discourse parser iteratively uses two SVM classifiers in a pipeline to build a DT. In each iteration, a binary classifier firs</context>
</contexts>
<marker>Knight, Graehl, 2005</marker>
<rawString>Knight, Kevin and Jonathan Graehl. 2005. An overview of probabilistic tree transducers for natural language processing. In Computational Linguistics and Intelligent Text Processing, volume 3406 of Lecture Notes in Computer Science. Springer, Berlin Heidelberg, pages 1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Robert Dale</author>
</authors>
<title>Using linguistic phenomena to motivate a set of coherence relations.</title>
<date>1994</date>
<booktitle>Discourse Processes,</booktitle>
<pages>18--35</pages>
<contexts>
<context position="3807" citStr="Knott and Dale (1994)" startWordPosition="545" endWordPosition="548">lation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is driven by sentence semantics. Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse structure. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is perhaps the most influential theory of discourse in computational linguistics. Although it was initially intended to be used in text generation, later it became popular as a framework for parsing the structure</context>
<context position="53940" citStr="Knott and Dale 1994" startWordPosition="8363" endWordPosition="8366">total, a unit containing two of the EDUs will have a relative EDU number of 0.67. We also measure the distances of the units in terms of the number of EDUs from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the discourse units. Discourse cues (e.g., because, but), when present, signal rhetorical relations between two text segments, and have been used as a primary source of information in earlier studies (Knott and Dale 1994; Marcu 2000a). However, recent studies (Hernault et al. 2010; Biran and Rambow 2011) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of cue phrases, since this approach is domain independent and capable of capturing non-lexical cues such as punctuation. In order to build a lexical N-gram dictionary empirically from the training corpus, we extract the first and last N tokens (NE11, 2,3}) of each discourse unit and rank them according to their mutual information with the two labels, Structure (S) and Relation (R). 401 Computational Linguistics </context>
</contexts>
<marker>Knott, Dale, 1994</marker>
<rawString>Knott, Alistair and Robert Dale. 1994. Using linguistic phenomena to motivate a set of coherence relations. Discourse Processes, 18:35–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Michaela Regneri</author>
<author>Stefan Thater</author>
</authors>
<title>Regular tree grammars as a formalism for scope underspecification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies,</booktitle>
<pages>218--226</pages>
<location>Columbus, OH.</location>
<marker>Koller, Regneri, Thater, 2008</marker>
<rawString>Koller, Alexander, Michaela Regneri, and Stefan Thater. 2008. Regular tree grammars as a formalism for scope underspecification. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies, pages 218–226, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA.</location>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Ivan Titov</author>
<author>Caroline Sporleder</author>
</authors>
<title>A Bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13,</booktitle>
<location>Sofia.</location>
<marker>Lazaridou, Titov, Sporleder, 2013</marker>
<rawString>Lazaridou, Angeliki, Ivan Titov, and Caroline Sporleder. 2013. A Bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Rumeng Li</author>
<author>Eduard Hovy</author>
</authors>
<title>Recursive deep models for discourse parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the</booktitle>
<contexts>
<context position="26772" citStr="Li et al. (2014)" startWordPosition="3984" endWordPosition="3987"> predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsing to achieve efficiency. They also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful appl</context>
</contexts>
<marker>Li, Li, Hovy, 2014</marker>
<rawString>Li, Jiwei, Rumeng Li, and Eduard Hovy. 2014. Recursive deep models for discourse parsing. In Proceedings of the 2014</rawString>
</citation>
<citation valid="false">
<booktitle>Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>2061--2069</pages>
<location>Doha.</location>
<marker></marker>
<rawString>Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061–2069, Doha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujian Li</author>
<author>Liang Wang</author>
<author>Ziqiang Cao</author>
<author>Wenjie Li</author>
</authors>
<title>Text-level discourse dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>25--35</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="26772" citStr="Li et al. (2014)" startWordPosition="3984" endWordPosition="3987"> predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsing to achieve efficiency. They also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful appl</context>
</contexts>
<marker>Li, Wang, Cao, Li, 2014</marker>
<rawString>Li, Sujian, Liang Wang, Ziqiang Cao, and Wenjie Li. 2014. Text-level discourse dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25–35, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Aravind Joshi</author>
<author>Ani Nenkova</author>
</authors>
<title>Discourse indicators for content selection in summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’10,</booktitle>
<pages>147--156</pages>
<location>Tokyo.</location>
<marker>Louis, Joshi, Nenkova, 2010</marker>
<rawString>Louis, Annie, Aravind Joshi, and Ani Nenkova. 2010. Discourse indicators for content selection in summarization. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’10, pages 147–156, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matouˇs Mach´aˇcek</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Results of the WMT14 metrics shared task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, MD.</location>
<marker>Mach´aˇcek, Bojar, 2014</marker>
<rawString>Mach´aˇcek, Matouˇs and Ondˇrej Bojar. 2014. Results of the WMT14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL’95,</booktitle>
<pages>276--283</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="92292" citStr="Magerman 1995" startWordPosition="14762" endWordPosition="14763">s on the two corpora. Performances significantly superior to SPADE are denoted by *. accurate reranking parser (Charniak and Johnson 2005). However, because of the lack of gold syntactic trees in the instructional corpus, we trained SPADE in this corpus using the syntactic trees produced by the reranking parser. To avoid using the gold syntactic trees, we used the reranking parser in our system for both training and testing purposes. This syntactic parser was trained on the sections of the Penn Treebank not included in our test set. We applied the same canonical lexical head projection rules (Magerman 1995; Collins 2003) to lexicalize the syntactic trees as done in HILDA and SPADE. Note that previous studies (Fisher and Roark 2007; Soricut and Marcu 2003; Hernault et al. 2010) on discourse segmentation only report their performance on the RST–DT test set. To compare our results with them, we evaluated our model on the RST–DT test set. In addition, we showed a more general performance of SPADE and our system on the two corpora based on 10-fold cross validation.13 However, SPADE does not come with a training module for its segmenter. We reimplemented this module and verified its correctness by re</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Magerman, David. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL’95, pages 276–283, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="4193" citStr="Mann and Thompson (1988)" startWordPosition="600" endWordPosition="603">on using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is driven by sentence semantics. Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse structure. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is perhaps the most influential theory of discourse in computational linguistics. Although it was initially intended to be used in text generation, later it became popular as a framework for parsing the structure of a text (Taboada and Mann 2006). RST represents texts by labeled hierarchical structures, called Discourse Trees (DTs). For example, consider the DT shown in Figure 1 for the following text: But he added: “Some people use the purchasers’ index as a leading indicator, some use it as a coincident indicator. But the thing it’s supposed to measure—manufacturing strength—it missed alto</context>
<context position="6325" citStr="Mann and Thompson (1988)" startWordPosition="933" endWordPosition="936"> thing it’s supposed to measure -- manufacturing strength -- (2) (3) (6) (4) (s) 386 Joty, Carenini, and Ng CODRA further distinguished based on their relative importance in the text: nuclei are the core parts of the relation and satellites are peripheral or supportive ones. For example, in Figure 1, Elaboration is a relation between a nucleus (EDU 4) and a satellite (EDU 5), and Contrast is a relation between two nuclei (EDUs 2 and 3). Carlson, Marcu, and Okurowski (2002) constructed the first large RST-annotated corpus (RST–DT) on Wall Street Journal articles from the Penn Treebank. Whereas Mann and Thompson (1988) had suggested about 25 relations, the RST–DT uses 53 mono-nuclear and 25 multi-nuclear relations. The relations are grouped into 16 coarse-grained categories; see Carlson and Marcu (2001) for a detailed description of the relations. Conventionally, rhetorical analysis in RST involves two subtasks: discourse segmentation is the task of breaking the text into a sequence of EDUs, and discourse parsing is the task of linking the discourse units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse parsing and rhetorical parsing interchangeably. While recent adva</context>
<context position="13335" citStr="Mann and Thompson (1988)" startWordPosition="1977" endWordPosition="1980">s. Finally, the oracle accuracy computed based on the k-best parse hypotheses generated by our parser demonstrates that a reranker could potentially improve the accuracy further. After discussing related work in Section 2, we present our rhetorical analysis framework in Section 3. In Section 4, we describe our discourse parser. Then, in Section 5 we present our discourse segmenter. The experiments and analysis of results are presented in Section 6. Finally, we summarize our contributions with future directions in Section 7. 2. Related Work Rhetorical analysis has a long history—dating back to Mann and Thompson (1988), when RST was initially proposed as a useful linguistic method for describing natural texts, to more recent attempts to automatically extract the rhetorical structure of a given text (Hernault et al. 2010). In this section, we provide a brief overview of the computational approaches that follow RST as the theory of discourse, and that are related to our work; see the survey by Stede (2011) for a broader overview that also includes other theories of discourse. 388 Joty, Carenini, and Ng CODRA 2.1 Unsupervised and Rule-Based Approaches Although the most effective approaches to rhetorical analys</context>
<context position="84672" citStr="Mann and Thompson (1988)" startWordPosition="13515" endWordPosition="13518">ST–DT (Carlson, Marcu, and Okurowski 2002), which contains discourse annotations for 385 Wall Street Journal news articles taken from the Penn Treebank corpus (Marcus, Marcinkiewicz, and Santorini 1994). The corpus is partitioned into a training set of 347 documents and a test set of 38 documents. A total of 53 documents selected from both training and test sets were annotated by two human annotators. We measure human agreements based on this doubly annotated data set. We used 25 documents from the training set as our development set. In RST–DT, the original 25 rhetorical relations defined by Mann and Thompson (1988) are further divided into a set of 18 coarser relation classes with 78 finer-grained relations (see Carlson and Marcu [2001] for details). Our second corpus is the instructional corpus prepared by Subba and Di-Eugenio (2009), which contains discourse annotations for 176 how-to manuals on home repair. The corpus was annotated with 26 informational relations (e.g., Preparation–Act, Act–Goal). For our experiments with the intra-sentential discourse parser, we extracted a sentence-level DT from a document-level DT by finding the sub-tree that exactly spans over the sentence. In RST–DT, by our coun</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann, William and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>A decision-based approach to rhetorical parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL’99,</booktitle>
<pages>365--372</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="18737" citStr="Marcu (1999)" startWordPosition="2792" endWordPosition="2793">tances stripped of their original discourse cues do not generalize well to implicit cases because they are linguistically quite different. Note that this approach to identifying discourse relations in the absence of manually labeled data does not fully solve the parsing problem (i.e., building DTs); rather, it only attempts to identify a small subset of coarser relations between two (flat) text segments (i.e., a tagging problem). Arguably, to perform a complete rhetorical analysis, one needs to use supervised machine learning techniques based on human-annotated data. 2.2 Supervised Approaches Marcu (1999) applies supervised machine learning techniques to build a discourse segmenter and a shift–reduce discourse parser. Both the segmenter and the parser rely on C4.5 decision tree classifiers (Poole and Mackworth 2010) to learn the rules automatically from the data. The discourse segmenter mainly uses discourse cues, shallowsyntactic (i.e., POS tags) and contextual features (i.e., neighboring words and their POS tags). To learn the shift–reduce actions, the discourse parser encodes five types of features: lexical (e.g., discourse cues), shallow-syntactic, textual similarity, operational (previous</context>
<context position="28824" citStr="Marcu 1999" startWordPosition="4298" endWordPosition="4299">to get the vector representations for the higher order discourse units. In every step, the merging is done using one binary (structure) and one multi-class (relation) classifier, each having a three-layer neural network architecture. The cost function for training the model is given by these two cascaded classifiers applied at different levels of the DT. Similar to our method, they use the classifier probabilities in a CKY-like parsing algorithm to find the global optimal DT. Finally, Ji and Eisenstein (2014) present a feature representation learning method in a shift–reduce discourse parser (Marcu 1999). Unlike DNNs, which learn non-linear feature transformations in a maximum likelihood model, they learn linear transformations of features in a max margin classification model. 3. Overview of Our Rhetorical Analysis Framework CODRA takes as input a raw text and produces a discourse tree that describes the text in terms of coherence relations that hold between adjacent discourse units (i.e., clauses, sentences) in the text. An example DT generated by an online demo of CODRA 392 Joty, Carenini, and Ng CODRA Segmenter Parser Segmentation model Sentences segmented into EDUs Algorithm model Intra-s</context>
<context position="30956" citStr="Marcu 1999" startWordPosition="4624" endWordPosition="4625"> discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and what relations (i.e., the labels) to use in the process of building the DT. Specifically, discourse parsing requires: (1) a parsing model to explore the search space of possible structures and labels for their nodes, and (2) a parsing algorithm for selecting the best parse tree(s) among the candidates. A probabilistic parsing model like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT c</context>
<context position="33698" citStr="Marcu 1999" startWordPosition="5030" endWordPosition="5031">he posterior probabilities of all possible DT constituents for a given text (i.e., EDUs); then it uses a CKY parsing algorithm to combine those probabilities and find the most probable DT. Another crucial question related to parsing models is whether to use a single model or two different models for parsing at the sentence-level (i.e., intra-sentential) and at the document-level (i.e., multi-sentential). A simple and straightforward strategy would be to use a single unified parsing model for both intra- and multi-sentential parsing without distinguishing the two cases, as was previously done (Marcu 1999; Subba and Di-Eugenio 2009; Hernault et al. 2010). That approach has the advantages of making the parsing process easier, and the model gets more data to learn from. However, for a solution like ours, which tries to capture the interdependencies between constituents, this would be problematic with respect to scalability and inappropriate because of two modeling issues. More specifically, for scalability note that the number of valid trees grows exponentially with the number of EDUs in a document.&apos; Therefore, an exhaustive search over all the valid DTs is often infeasible, even for relatively </context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, Daniel. 1999. A decision-based approach to rhetorical parsing. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL’99, pages 365–372, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of unrestricted texts: A surface-based approach.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--395</pages>
<contexts>
<context position="3049" citStr="Marcu 2000" startWordPosition="439" endWordPosition="440"> Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories</context>
<context position="14331" citStr="Marcu (2000" startWordPosition="2135" endWordPosition="2136">e (2011) for a broader overview that also includes other theories of discourse. 388 Joty, Carenini, and Ng CODRA 2.1 Unsupervised and Rule-Based Approaches Although the most effective approaches to rhetorical analysis to date rely on supervised machine learning methods trained on human-annotated data, unsupervised methods have also been proposed, as they do not require human-annotated data and can be more easily applied to new domains. Often, discourse connectives like but, because, and although convey clear information on the kind of relation linking the two text segments. In his early work, Marcu (2000a) presented a shallow rule-based approach relying on discourse connectives (or cues) and surface patterns. He used hand-coded rules, derived from an extensive corpus study, to break the text into EDUs and to build DTs for sentences first, then for paragraphs, and so on. Despite the fact that this work pioneered the field of rhetorical analysis, it has many limitations. First, identifying discourse connectives is a difficult task on its own, because (depending on the usage), the same phrase may or may not signal a discourse relation (Pitler and Nenkova 2009). For example, but can either signal</context>
<context position="53952" citStr="Marcu 2000" startWordPosition="8367" endWordPosition="8368">ing two of the EDUs will have a relative EDU number of 0.67. We also measure the distances of the units in terms of the number of EDUs from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the discourse units. Discourse cues (e.g., because, but), when present, signal rhetorical relations between two text segments, and have been used as a primary source of information in earlier studies (Knott and Dale 1994; Marcu 2000a). However, recent studies (Hernault et al. 2010; Biran and Rambow 2011) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of cue phrases, since this approach is domain independent and capable of capturing non-lexical cues such as punctuation. In order to build a lexical N-gram dictionary empirically from the training corpus, we extract the first and last N tokens (NE11, 2,3}) of each discourse unit and rank them according to their mutual information with the two labels, Structure (S) and Relation (R). 401 Computational Linguistics Volume 41, N</context>
<context position="70439" citStr="Marcu 2000" startWordPosition="11134" endWordPosition="11135">e use is that it allows us to generate a list of k most probable parse trees. It is straightforward to generalize the above algorithm to produce k most probable DTs. Specifically, when filling up the dynamic programming tables, rather than storing a single best parse for each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates simultaneously. One can show that the time and space complexities of the k-best version of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005). Note that, in contrast to other document-level discourse parsers (Marcu 2000b; Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This approach of CODRA is also different from the sentence-level discourse parser SPADE (Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell D[i, j] in SPADE’s dynamic programming table stores D[i,j] = P([Ui(0), Um.(1), Uj(1)]) (11) where m* = argmax P([Ui(0), Um(1), Uj(1)]). Disregarding the relation </context>
<context position="87388" citStr="Marcu (2000" startWordPosition="13945" endWordPosition="13946">es in the model output. Then, we measure Precision (P), Recall (R), and F-score for segmentation performance as follows: P= cm, R = ch, and F−score = 2PR P + R =2c (14) h + m 413 Computational Linguistics Volume 41, Number 3 Figure 15 A hypothetical system-generated DT for the two sentences in Figure 1. Figure 16 Measuring the accuracy of a discourse parser. (a) The human-annotated discourse tree. (b) The system-generated discourse tree. 6.2.2 Metrics for Discourse Parsing. To evaluate parsing performance, we use the standard unlabeled and labeled precision, recall, and F-score as proposed by Marcu (2000b). The unlabeled metric measures how accurate the discourse parser is in finding the right structure (i.e., the skeleton) of the DT, while the labeled metrics measure the parser’s ability to find the right labels (i.e., nuclearity statuses or relation labels) in addition to the right structure. Assume, for example, that given the two sentences of Figure 1, our system generates the DT shown in Figure 15. In Figure 16, we show the same gold DT shown in Figure 1 (on the left), and the same system-generated DT shown in Figure 15 (on the right), when the two trees are aligned. For the sake of illu</context>
<context position="88704" citStr="Marcu 2000" startWordPosition="14175" endWordPosition="14176">de two mistakes: (1) it broke the EDU marked 2–3 (Some people use the purchasers’ index as a leading indicator) in the human annotation into two separate EDUs, and (2) it could not identify EDU 5 (But the thing it’s supposed to measure) and EDU 6 (— manufacturing strength —) as two separate EDUs. Therefore, when we align the two annotations, we obtain seven EDUs in total. In Table 2, we list all constituents of the two DTs and their associated labels at the span, nuclei, and relation levels. The recall (R) and precision (P) figures are shown at the bottom of the table. Notice that, following (Marcu 2000b), the relation labels are assigned to the children nodes rather than to the parent nodes in the evaluation process to deal with non-binary trees in human annotations. To our knowledge, no implementation of some use it as a coincident indicator. 1�) as a leading indicator, Contrast Attribution Joint But he added: 1�) Elaboration &amp;quot;Some people use the purchasers’ index But the thing it’s supposed to measure -- manuFacturing strength -- 1�) it missed altogether lastmonth.&amp;quot; 1�) Contrast 12) 13 Attribution (1) Contrast Same-Unit Contrast (2-3) (4) (7) Elaboration (5) (6) (a) (1) Contrast (5-6) (7)</context>
<context position="129130" citStr="Marcu 2000" startWordPosition="20665" endWordPosition="20666">elop an interactive version of our system that will allow users to fix the output of the system with minimal effort and let the system learn from that feedback. Another interesting future direction is to perform extrinsic evaluations of our system in downstream applications. One important application of rhetorical structure is text summarization, where a significant challenge is producing not only informative but also coherent summaries. A number of researchers have already investigated the utility of rhetorical structure for measuring text importance (i.e., informativeness) in summarization (Marcu 2000b; Daum´e and Marcu 2002; Louis, Joshi, and Nenkova 2010). Recently, Christensen et al. (2013, 2014) propose to perform sentence selection and ordering at the same time, and use constraints on discourse structure to make the summaries coherent. However, they represent the discourse as an unweighted directed graph, which is shallow and not sufficiently informative in most cases. Furthermore, their approach does not allow compression at the sentence level, which is often beneficial in summarization. In the future, we would like to investigate the utility of our rhetorical structure for performin</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Marcu, Daniel. 2000a. The rhetorical parsing of unrestricted texts: A surface-based approach. Computational Linguistics, 26:395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3049" citStr="Marcu 2000" startWordPosition="439" endWordPosition="440"> Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories</context>
<context position="14331" citStr="Marcu (2000" startWordPosition="2135" endWordPosition="2136">e (2011) for a broader overview that also includes other theories of discourse. 388 Joty, Carenini, and Ng CODRA 2.1 Unsupervised and Rule-Based Approaches Although the most effective approaches to rhetorical analysis to date rely on supervised machine learning methods trained on human-annotated data, unsupervised methods have also been proposed, as they do not require human-annotated data and can be more easily applied to new domains. Often, discourse connectives like but, because, and although convey clear information on the kind of relation linking the two text segments. In his early work, Marcu (2000a) presented a shallow rule-based approach relying on discourse connectives (or cues) and surface patterns. He used hand-coded rules, derived from an extensive corpus study, to break the text into EDUs and to build DTs for sentences first, then for paragraphs, and so on. Despite the fact that this work pioneered the field of rhetorical analysis, it has many limitations. First, identifying discourse connectives is a difficult task on its own, because (depending on the usage), the same phrase may or may not signal a discourse relation (Pitler and Nenkova 2009). For example, but can either signal</context>
<context position="53952" citStr="Marcu 2000" startWordPosition="8367" endWordPosition="8368">ing two of the EDUs will have a relative EDU number of 0.67. We also measure the distances of the units in terms of the number of EDUs from the beginning and end of the sentence (or text in the multi-sentential case). Text structural features capture the correlation between text structure and rhetorical structure by counting the number of sentence and paragraph boundaries in the discourse units. Discourse cues (e.g., because, but), when present, signal rhetorical relations between two text segments, and have been used as a primary source of information in earlier studies (Knott and Dale 1994; Marcu 2000a). However, recent studies (Hernault et al. 2010; Biran and Rambow 2011) suggest that an empirically acquired lexical N-gram dictionary is more effective than a fixed list of cue phrases, since this approach is domain independent and capable of capturing non-lexical cues such as punctuation. In order to build a lexical N-gram dictionary empirically from the training corpus, we extract the first and last N tokens (NE11, 2,3}) of each discourse unit and rank them according to their mutual information with the two labels, Structure (S) and Relation (R). 401 Computational Linguistics Volume 41, N</context>
<context position="70439" citStr="Marcu 2000" startWordPosition="11134" endWordPosition="11135">e use is that it allows us to generate a list of k most probable parse trees. It is straightforward to generalize the above algorithm to produce k most probable DTs. Specifically, when filling up the dynamic programming tables, rather than storing a single best parse for each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates simultaneously. One can show that the time and space complexities of the k-best version of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005). Note that, in contrast to other document-level discourse parsers (Marcu 2000b; Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This approach of CODRA is also different from the sentence-level discourse parser SPADE (Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell D[i, j] in SPADE’s dynamic programming table stores D[i,j] = P([Ui(0), Um.(1), Uj(1)]) (11) where m* = argmax P([Ui(0), Um(1), Uj(1)]). Disregarding the relation </context>
<context position="87388" citStr="Marcu (2000" startWordPosition="13945" endWordPosition="13946">es in the model output. Then, we measure Precision (P), Recall (R), and F-score for segmentation performance as follows: P= cm, R = ch, and F−score = 2PR P + R =2c (14) h + m 413 Computational Linguistics Volume 41, Number 3 Figure 15 A hypothetical system-generated DT for the two sentences in Figure 1. Figure 16 Measuring the accuracy of a discourse parser. (a) The human-annotated discourse tree. (b) The system-generated discourse tree. 6.2.2 Metrics for Discourse Parsing. To evaluate parsing performance, we use the standard unlabeled and labeled precision, recall, and F-score as proposed by Marcu (2000b). The unlabeled metric measures how accurate the discourse parser is in finding the right structure (i.e., the skeleton) of the DT, while the labeled metrics measure the parser’s ability to find the right labels (i.e., nuclearity statuses or relation labels) in addition to the right structure. Assume, for example, that given the two sentences of Figure 1, our system generates the DT shown in Figure 15. In Figure 16, we show the same gold DT shown in Figure 1 (on the left), and the same system-generated DT shown in Figure 15 (on the right), when the two trees are aligned. For the sake of illu</context>
<context position="88704" citStr="Marcu 2000" startWordPosition="14175" endWordPosition="14176">de two mistakes: (1) it broke the EDU marked 2–3 (Some people use the purchasers’ index as a leading indicator) in the human annotation into two separate EDUs, and (2) it could not identify EDU 5 (But the thing it’s supposed to measure) and EDU 6 (— manufacturing strength —) as two separate EDUs. Therefore, when we align the two annotations, we obtain seven EDUs in total. In Table 2, we list all constituents of the two DTs and their associated labels at the span, nuclei, and relation levels. The recall (R) and precision (P) figures are shown at the bottom of the table. Notice that, following (Marcu 2000b), the relation labels are assigned to the children nodes rather than to the parent nodes in the evaluation process to deal with non-binary trees in human annotations. To our knowledge, no implementation of some use it as a coincident indicator. 1�) as a leading indicator, Contrast Attribution Joint But he added: 1�) Elaboration &amp;quot;Some people use the purchasers’ index But the thing it’s supposed to measure -- manuFacturing strength -- 1�) it missed altogether lastmonth.&amp;quot; 1�) Contrast 12) 13 Attribution (1) Contrast Same-Unit Contrast (2-3) (4) (7) Elaboration (5) (6) (a) (1) Contrast (5-6) (7)</context>
<context position="129130" citStr="Marcu 2000" startWordPosition="20665" endWordPosition="20666">elop an interactive version of our system that will allow users to fix the output of the system with minimal effort and let the system learn from that feedback. Another interesting future direction is to perform extrinsic evaluations of our system in downstream applications. One important application of rhetorical structure is text summarization, where a significant challenge is producing not only informative but also coherent summaries. A number of researchers have already investigated the utility of rhetorical structure for measuring text importance (i.e., informativeness) in summarization (Marcu 2000b; Daum´e and Marcu 2002; Louis, Joshi, and Nenkova 2010). Recently, Christensen et al. (2013, 2014) propose to perform sentence selection and ordering at the same time, and use constraints on discourse structure to make the summaries coherent. However, they represent the discourse as an unweighted directed graph, which is shallow and not sufficiently informative in most cases. Furthermore, their approach does not allow compression at the sentence level, which is often beneficial in summarization. In the future, we would like to investigate the utility of our rhetorical structure for performin</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Marcu, Daniel. 2000b. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL’02,</booktitle>
<pages>368--375</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="15703" citStr="Marcu and Echihabi (2002)" startWordPosition="2345" endWordPosition="2348">ttain high accuracy (Soricut and Marcu 2003). Third, DT structures do not always correspond to paragraph structures; for example, Sporleder and Lapata (2004) report that more than 20% of the paragraphs in the RST–DT corpus (Carlson, Marcu, and Okurowski 2002) do not correspond to a discourse unit in the DT. Fourth, discourse cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba and Di-Eugenio 2009) report even higher figures: About 60% of discourse relations are not explicitly signaled. Therefore, rather than relying on hand-coded rules based on discourse cues and surface patterns, recent approaches use machine learning techniques with a large se</context>
<context position="17007" citStr="Marcu and Echihabi (2002)" startWordPosition="2546" endWordPosition="2549">y signaled by discourse cues (e.g., Concession) and some do not (e.g., Background), there is a large middle ground of relations that may be signaled or not. For these “middle ground” relations, can we exploit features present in the signaled cases to automatically identify relations when they are not explicitly signaled? The idea is to use unambiguous discourse cues (e.g., although for Contrast, for example for Elaboration) to automatically label a large corpus with rhetorical relations that could then be used to train a supervised model.&apos; A series of previous studies have explored this idea. Marcu and Echihabi (2002) first attempted to identify four broad classes of relations: Contrast, Elaboration, Condition, and Cause–Explanation–Evidence. They used a naive Bayes classifier based on word pairs (w1, w2), where w1 occurs in the left segment, and w2 occurs in the right segment. Sporleder and Lascarides (2005) included other features (e.g., words and their stems, Part-of-Speech [POS] tags, positions, segment lengths) in a boosting-based classifier (i.e., BoosTexter [Schapire and Singer 2000]) to further improve relation classification accuracy. However, these studies evaluated classification performance on </context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Marcu, Daniel and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL’02, pages 368–375. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<marker>Marcus, Marcinkiewicz, Santorini, 1994</marker>
<rawString>Marcus, Mitchell, Mary Marcinkiewicz, and Beatrice Santorini. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Martin</author>
</authors>
<title>English Text: System and Structure.</title>
<date>1992</date>
<publisher>John Benjamins Publishing Company, Philadelphia/Amsterdam.</publisher>
<contexts>
<context position="3781" citStr="Martin (1992)" startWordPosition="542" endWordPosition="543">05), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is driven by sentence semantics. Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse structure. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is perhaps the most influential theory of discourse in computational linguistics. Although it was initially intended to be used in text generation, later it became popular as a framework</context>
</contexts>
<marker>Martin, 1992</marker>
<rawString>Martin, James, 1992. English Text: System and Structure. John Benjamins Publishing Company, Philadelphia/Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mstislav Maslennikov</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A multi-resolution framework for information extraction from free text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>592--599</pages>
<location>Prague.</location>
<contexts>
<context position="3404" citStr="Maslennikov and Chua 2007" startWordPosition="487" endWordPosition="490">ther to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is dr</context>
</contexts>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>Maslennikov, Mstislav and Tat-Seng Chua. 2007. A multi-resolution framework for information extraction from free text. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 592–599, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="52061" citStr="McCallum 2002" startWordPosition="8059" endWordPosition="8060">t all levels of a documentlevel discourse tree. Input: Sequence of units: (U1, U2, · · · Un), where Ux[0]:= start EDU ID of unit x, and Ux[1]:= end EDU ID of unit x. Output: List of adjacent units: L for i = 1 -+ n − 1 do all possible starting positions for the subsequence forj = i + 1 n do all possible ending positions for the subsequence for k = i j − 1 do all possible cut points within the subsequence Left = Ui[0] : Uk[1] Right = Uk+1[0] : Uj[1] L.append ((Left, Right)) end end end Both our intra- and multi-sentential parsing models are designed using MALLET’s graphical model toolkit GRMM (McCallum 2002). In order to avoid overfitting, we regularize the CRF models with l2 regularization and learn the model parameters using the limited-memory BFGS (L-BFGS) fitting algorithm. 4.1.4 Features Used in the Parsing Models. Crucial to parsing performance is the set of features used in the parsing models, as summarized in Table 1. We categorize the features into seven groups and specify which groups are used in what parsing model. Notice that some of the features are used in both models. Most of the features have been explored in previous studies (e.g., Soricut and Marcu 2003; Sporleder and Lapata 200</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, Andrew. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00,</booktitle>
<pages>591--598</pages>
<location>San Francisco, CA.</location>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>McCallum, Andrew, Dayne Freitag, and Fernando C. N. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML ’00, pages 591–598, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="27210" citStr="McDonald et al. 2005" startWordPosition="4047" endWordPosition="4050">ased on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, Ryan, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, ACL ’05, pages 91–98, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>523--530</pages>
<location>Stroudsburg, PA.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 523–530, Stroudsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="58980" citStr="Morris and Hirst 1991" startWordPosition="9212" endWordPosition="9215">r [2, 2, 3]. In order to extract dominance set features for two adjacent discourse units Ut−1 and Ut, containing EDUs ei:j and ej+1:k, respectively, we first compute the dominance set from the DS-LST of the sentence. We then extract the element from the set that holds across the EDUs j and j + 1. In our example, for the two units, containing EDUs e1 and e2, respectively, the relevant dominance set element is (1, efforts/NP)&gt;(2, to/S). We encode the syntactic labels and lexical heads of NH and NA, and the dominance relationship as features in our intra-sentential parsing model. Lexical chains (Morris and Hirst 1991) are sequences of semantically related words that can indicate topical boundaries in a text (Galley et al. 2003; Joty, Carenini, and Ng 2013). Features extracted from lexical chains are also shown to be useful for finding paragraph-level discourse structure (Sporleder and Lapata 2004). For example, consider the text with four paragraphs (P1 to P4) in Figure 10a. Now, let us assume that there is a lexical chain that spans the whole text, skipping paragraphs P2 and P3, while a second chain only spans P2 and P3. This situation makes it more likely that P2 and P3 should be linked in the DT before </context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of structure of text. Computational Linguistics, 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Murphy</author>
</authors>
<title>Machine Learning: A Probabilistic Perspective.</title>
<date>2012</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7948" citStr="Murphy 2012" startWordPosition="1182" endWordPosition="1183"> impact of rhetorical structure in downstream NLP applications is still very limited. The work we present in this article aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing discourse parsers. First, existing discourse parsers typically model the structure and the labels of a DT separately, and also do not take into account the sequential dependencies between the DT constituents. However, for several NLP tasks, it has recently been shown that joint models typically outperform independent or pipeline models (Murphy 2012, page 687). This is also supported in a recent study by Feng and Hirst (2012), in which the performance of a greedy bottom–up discourse parser improved when sequential dependencies were considered by using gold annotations for the neighboring (i.e., previous and next) discourse units as contextual features in the parsing model. To address this limitation of existing parsers, as the first contribution, we propose a novel discourse parser based on probabilistic discriminative parsing models, expressed as Conditional Random Fields (CRFs) (Sutton, McCallum, and Rohanimanesh 2007), to infer the pr</context>
<context position="20642" citStr="Murphy 2012" startWordPosition="3067" endWordPosition="3068">able DT structure for a sentence. SPADE was trained and tested on the RST–DT corpus. This work, by showing empirically the connection between syntax and discourse structure at the sentence level, has greatly influenced all major contributions in this area ever since. However, it is limited in several ways. First, SPADE does not produce a full-text (i.e., document-level) parse. Second, it applies a generative parsing model based on only lexico-syntactic features, whereas discriminative models are generally considered to be more accurate, and can incorporate arbitrary features more effectively (Murphy 2012). Third, the parsing model makes an independence assumption between the label and the structure of a DT constituent, and it ignores the sequential and the hierarchical dependencies between the DT constituents. Subsequent research addresses the question of how much syntax one really needs in rhetorical analysis. Sporleder and Lapata (2005) focus on the discourse chunking problem, comprising two subtasks: discourse segmentation and (flat) nuclearity assignment. They formulate discourse chunking in two alternative ways. First, one-step classification, where the discourse chunker, a multi-class cl</context>
<context position="40057" citStr="Murphy 2012" startWordPosition="6018" endWordPosition="6019">units Uj−1 and Uj, where M is the total number of relations in the relation set. The connections between adjacent nodes in a hidden layer encode sequential dependencies between the respective hidden nodes, and can enforce constraints such as the fact that a node must have a unique mother, namely, a Sj= 1 must not follow a Sj−1= 1. The connections between the two hidden layers model the structure and the relation of DT constituents jointly. Notice that the probabilistic graphical model shown in Figure 5 is a chain-structured undirected graphical model (also known as Markov Random Field or MRF [Murphy 2012]) with two hidden layers, i.e., structure chain and relation chain. It becomes a Dynamic Conditional Random Field (DCRF) (Sutton, McCallum, and Rohanimanesh 2007) when we directly model the hidden (output) variables by conditioning the clique potentials (i.e., factors) on the observed (input) variables: Figure 5 The intra-sentential parsing model of CODRA. P(R2:t, S2:t|x, s) = 1 t��-11 ϕ(Ri,Ri+1|x,s,r)iP(Si,Si+1|x,s,s)ω(Ri,Si|x,s,c) (1) Z(x, s) 11 i=2 R R R R R 2 3 j t-1 t S S S S S 2 3 j t-1 t U1 U U U U U 2 3 j t-1 t Relation sequence Structure sequence Unit sequence at level i 396 Jot</context>
<context position="42814" citStr="Murphy 2012" startWordPosition="6456" endWordPosition="6457">ad et al. 2008). DCRFs, being a discriminative approach to sequence modeling, have several advantages over their generative counterparts such as Hidden Markov Models (HMMs) and MRFs, which first model the joint distribution p(y,x|), and then infer the conditional distribution p(y|x, ). It has been advocated that discriminative models are generally more accurate than generative ones because they do not “waste resources” modeling complex distributions that are observed (i.e., p(x)); instead, they focus directly on modeling what we care about, namely, the distribution of labels given the data (Murphy 2012). Other key advantages include the ability to incorporate arbitrary overlapping local and global features, and the ability to relax strong independence assumptions. Furthermore, CRFs surmount the label bias problem (Lafferty, McCallum, and Pereira 2001) of the Maximum Entropy Markov Model (McCallum, Freitag, and Pereira 2000), which is considered to be a discriminative version of the HMM. 4.1.2 Training and Applying the Intra-Sentential Parsing Model. In order to obtain the probability of the constituents of all candidate DTs for a sentence, CODRA applies the intra-sentential parsing model (wi</context>
<context position="48989" citStr="Murphy 2012" startWordPosition="7538" endWordPosition="7539">ith the number of sentences in a document. For example, assuming that each sentence has a well-formed DT, for a document with n sentences, Algorithm 1 generates O(n3) sequences, where Figure 7 A gold discourse tree (left), and the 7 training instances it generates (right). NR = No Relation. 399 Computational Linguistics Volume 41, Number 3 the sequence at the bottom level has n units, each of the sequences at the second level has n-1 units, and so on. Because the DCRF model in Figure 5 has a “fat” chain structure, one could use the forward–backward algorithm for exact inference in this model (Murphy 2012). Forward–backward on a sequence containing T units costs O(TM2) time, where M is the number of relations in our relation set. This makes the chainstructured DCRF model impractical for multi-sentential parsing of long documents, since learning requires running inference on every training sequence with an overall time complexity of O(TM2n3) = O(M2n4) per document (Sutton and McCallum 2012). To address this problem, we have developed a simplified parsing model for multisentential parsing. Our model is shown in Figure 8. The two observed nodes Ut−1 and Ut are two adjacent (multi-sentential) disco</context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>Murphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. The MIT Press. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL ’04,</booktitle>
<pages>271--278</pages>
<location>Barcelona.</location>
<contexts>
<context position="130442" citStr="Pang and Lee 2004" startWordPosition="20855" endWordPosition="20858">ructure can also play important roles in sentiment analysis. A key research problem in sentiment analysis is extracting fine-grained opinions about different aspects of a product. Several recent papers (Somasundaran 2010; Lazaridou, Titov, and 427 Computational Linguistics Volume 41, Number 3 Sporleder 2013) exploited the rhetorical structure for this task. Another challenging problem is assessing the overall opinion expressed in a review because not all sentences in a review contribute equally to the overall sentiment. For example, some sentences are subjective, whereas others are objective (Pang and Lee 2004); some express the main claims, whereas others support them (Taboada et al. 2011); some express opinions about the main entity, whereas others are about the peripherals. Discourse structure could be useful to capture the relative weights of the discourse units towards the overall sentiment. For example, the nucleus and satellite distinction along with the rhetorical relations could be useful to infer the relative weights of the connecting discourse units. Among other applications of discourse structure, Machine Translation (MT) and its evaluation have received a resurgence of interest recently</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, Bo and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL ’04, pages 271–278. Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="14895" citStr="Pitler and Nenkova 2009" startWordPosition="2223" endWordPosition="2226">inking the two text segments. In his early work, Marcu (2000a) presented a shallow rule-based approach relying on discourse connectives (or cues) and surface patterns. He used hand-coded rules, derived from an extensive corpus study, to break the text into EDUs and to build DTs for sentences first, then for paragraphs, and so on. Despite the fact that this work pioneered the field of rhetorical analysis, it has many limitations. First, identifying discourse connectives is a difficult task on its own, because (depending on the usage), the same phrase may or may not signal a discourse relation (Pitler and Nenkova 2009). For example, but can either signal a Contrast discourse relation or can simply perform non-discourse acts. Second, discourse segmentation using only discourse connectives fails to attain high accuracy (Soricut and Marcu 2003). Third, DT structures do not always correspond to paragraph structures; for example, Sporleder and Lapata (2004) report that more than 20% of the paragraphs in the RST–DT corpus (Carlson, Marcu, and Okurowski 2002) do not correspond to a discourse unit in the DT. Fourth, discourse cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concess</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Pitler, Emily and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 13–16, Suntec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Poole</author>
<author>Alan Mackworth</author>
</authors>
<title>Artificial Intelligence: Foundations of Computational Agents.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18952" citStr="Poole and Mackworth 2010" startWordPosition="2821" endWordPosition="2824">he absence of manually labeled data does not fully solve the parsing problem (i.e., building DTs); rather, it only attempts to identify a small subset of coarser relations between two (flat) text segments (i.e., a tagging problem). Arguably, to perform a complete rhetorical analysis, one needs to use supervised machine learning techniques based on human-annotated data. 2.2 Supervised Approaches Marcu (1999) applies supervised machine learning techniques to build a discourse segmenter and a shift–reduce discourse parser. Both the segmenter and the parser rely on C4.5 decision tree classifiers (Poole and Mackworth 2010) to learn the rules automatically from the data. The discourse segmenter mainly uses discourse cues, shallowsyntactic (i.e., POS tags) and contextual features (i.e., neighboring words and their POS tags). To learn the shift–reduce actions, the discourse parser encodes five types of features: lexical (e.g., discourse cues), shallow-syntactic, textual similarity, operational (previous n shift–reduce operations), and rhetorical sub-structural features. Despite the fact that this work has pioneered many of today’s machine learning approaches to discourse parsing, it has all the limitations mention</context>
</contexts>
<marker>Poole, Mackworth, 2010</marker>
<rawString>Poole, David and Alan Mackworth, 2010. Artificial Intelligence: Foundations of Computational Agents. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2961--2968</pages>
<location>Marrakech.</location>
<contexts>
<context position="42217" citStr="Prasad et al. 2008" startWordPosition="6366" endWordPosition="6369">n-chain edges. A DCRF is a generalization of linear-chain CRFs (Lafferty, McCallum, and Pereira 2001) to represent complex interactions between output variables (i.e., labels), such as when performing multiple labeling tasks on the same sequence. Recently, there has been an explosion of interest in CRFs for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel, Kleeman, and Manning 2008), syntactic chunking (Sha and Pereira 2003), and discourse chunking (Ghosh et al. 2011) in accordance with the Penn Discourse Treebank (Prasad et al. 2008). DCRFs, being a discriminative approach to sequence modeling, have several advantages over their generative counterparts such as Hidden Markov Models (HMMs) and MRFs, which first model the joint distribution p(y,x|), and then infer the conditional distribution p(y|x, ). It has been advocated that discriminative models are generally more accurate than generative ones because they do not “waste resources” modeling complex distributions that are observed (i.e., p(x)); instead, they focus directly on modeling what we care about, namely, the distribution of labels given the data (Murphy 2012). O</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Prasad, Rashmi, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC), pages 2961–2968, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank as a resource for natural language generation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation,</booktitle>
<pages>25--32</pages>
<location>Birmingham.</location>
<contexts>
<context position="3171" citStr="Prasad et al. 2005" startWordPosition="456" endWordPosition="459">ht protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Mar</context>
</contexts>
<marker>Prasad, Joshi, Dinesh, Lee, Miltsakaki, Webber, 2005</marker>
<rawString>Prasad, Rashmi, Aravind Joshi, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, and Bonnie Webber. 2005. The Penn Discourse TreeBank as a resource for natural language generation. In Proceedings of the Corpus Linguistics Workshop on Using Corpora for Natural Language Generation, pages 25–32, Birmingham.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Markus Egg</author>
<author>Alexander Koller</author>
</authors>
<title>Efficient processing of underspecified discourse representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08,</booktitle>
<pages>245--248</pages>
<marker>Regneri, Egg, Koller, 2008</marker>
<rawString>Regneri, Michaela, Markus Egg, and Alexander Koller. 2008. Efficient processing of underspecified discourse representations. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short ’08, pages 245–248,</rawString>
</citation>
<citation valid="false">
<authors>
<author>OH Columbus</author>
</authors>
<marker>Columbus, </marker>
<rawString>Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Reyle</author>
</authors>
<title>Dealing with ambiguities by underspecification: Construction, representation and deduction.</title>
<date>1993</date>
<journal>Journal of Semantics,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="23319" citStr="Reyle 1993" startWordPosition="3458" endWordPosition="3459">gging and shallow syntactic parsing (i.e., chunking). Using quite a large number of features in a binary log-linear model, they achieve state-of-the-art performance in discourse segmentation on the RST–DT test set. In a different approach, Regneri, Egg, and Koller (2008) propose to use Underspecified Discourse Representation (UDR) as an intermediate representation for discourse parsing. Underspecified representations offer a single compact representation to express possible ambiguities in a linguistic structure, and have been primarily used to deal with scope ambiguity in semantic structures (Reyle 1993; Egg, Koller, and Niehren 2001; Althaus et al. 2003; Koller, Regneri, and Thater 2008). Assuming that a UDR of a DT is already given in the form of a dominance graph (Althaus et al. 2003), Regneri, Egg, and Koller (2008) convert it into a more expressive and complete UDR representation called regular tree grammar (Koller, Regneri, and Thater 2008), for which efficient algorithms (Knight and Graehl 2005) already exist to derive the best configuration (i.e., the best discourse tree). Hernault et al. (2010) present the publicly available HILDA system,3 which comes with a discourse segmenter and </context>
</contexts>
<marker>Reyle, 1993</marker>
<rawString>Reyle, Uwe. 1993. Dealing with ambiguities by underspecification: Construction, representation and deduction. Journal of Semantics, 10(2):123–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="17488" citStr="Schapire and Singer 2000" startWordPosition="2614" endWordPosition="2617">relations that could then be used to train a supervised model.&apos; A series of previous studies have explored this idea. Marcu and Echihabi (2002) first attempted to identify four broad classes of relations: Contrast, Elaboration, Condition, and Cause–Explanation–Evidence. They used a naive Bayes classifier based on word pairs (w1, w2), where w1 occurs in the left segment, and w2 occurs in the right segment. Sporleder and Lascarides (2005) included other features (e.g., words and their stems, Part-of-Speech [POS] tags, positions, segment lengths) in a boosting-based classifier (i.e., BoosTexter [Schapire and Singer 2000]) to further improve relation classification accuracy. However, these studies evaluated classification performance on the instances 1 We categorize this approach as unsupervised because it does not rely on human-annotated data. 389 Computational Linguistics Volume 41, Number 3 where rhetorical relations were originally signaled (i.e., the discourse cues were artificially removed), and did not verify how well this approach performs on the instances that are not originally signaled. Subsequent studies (Blair-Goldensohn, McKeown, and Rambow 2007; Sporleder 2007; Sporleder and Lascarides 2008) co</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Schapire, Robert E. and Yoram Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2–3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schauer</author>
<author>Udo Hahn</author>
</authors>
<title>Anaphoric cues for coherence relations.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing, RANLP ’01,</booktitle>
<pages>228--234</pages>
<contexts>
<context position="15996" citStr="Schauer and Hahn 2001" startWordPosition="2392" endWordPosition="2395"> in the DT. Fourth, discourse cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba and Di-Eugenio 2009) report even higher figures: About 60% of discourse relations are not explicitly signaled. Therefore, rather than relying on hand-coded rules based on discourse cues and surface patterns, recent approaches use machine learning techniques with a large set of informative features. While some rhetorical relations need to be explicitly signaled by discourse cues (e.g., Concession) and some do not (e.g., Background), there is a large middle ground of relations that may be signaled or not. For these “middle ground” relations, can we exploit featu</context>
</contexts>
<marker>Schauer, Hahn, 2001</marker>
<rawString>Schauer, Holger and Udo Hahn. 2001. Anaphoric cues for coherence relations. In Proceedings of the Conference on Recent Advances in Natural Language Processing, RANLP ’01, pages 228–234,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tzigov Chark</author>
</authors>
<marker>Chark, </marker>
<rawString>Tzigov Chark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Schilder</author>
</authors>
<title>Robust discourse parsing via discourse markers, topicality Computational Linguistics Volume 41, Number 3 and position.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="118068" citStr="Schilder (2002)" startWordPosition="18784" endWordPosition="18785">pecifically, for sentence-level parsing, when we add the Organizational features with the Dominance set features, we achieve about 2 percentage points absolute improvements in nuclearity and relations. With N-gram features, the gain is even higher: 6 percentage points in relations and 3.5 percentage points in nuclearity for sentence-level parsing, and 3.8 percentage points in relations and 3.1 percentage points in nuclearity for document-level parsing. This demonstrates the utility of the N-gram features, which is also consistent with the previous findings of duVerle and Prendinger (2009) and Schilder (2002). The features extracted from Lexical chains (L-ch) have also proved to be useful for document-level parsing. They deliver absolute improvements of 2.7 percentage points, 2.9 percentage points, and 2.3 percentage points in span, nuclearity, and relations, 21 Text structural features are included in the Organizational features for multi-sentential parsing. 423 Computational Linguistics Volume 41, Number 3 Figure 17 Discourse trees generated by human annotator and our system for the text [what’s more,]e1 [he believes]e2 [seasonal swings in the auto industry this year aren’t occurring at the same</context>
</contexts>
<marker>Schilder, 2002</marker>
<rawString>Schilder, Frank. 2002. Robust discourse parsing via discourse markers, topicality Computational Linguistics Volume 41, Number 3 and position. Natural Language Engineering, 8(3):235–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>134--141</pages>
<location>Edmonton.</location>
<contexts>
<context position="42105" citStr="Sha and Pereira 2003" startWordPosition="6347" endWordPosition="6350">elation edges and the structure edges, respectively, and s,c is the weight vector for the factors over the between-chain edges. A DCRF is a generalization of linear-chain CRFs (Lafferty, McCallum, and Pereira 2001) to represent complex interactions between output variables (i.e., labels), such as when performing multiple labeling tasks on the same sequence. Recently, there has been an explosion of interest in CRFs for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel, Kleeman, and Manning 2008), syntactic chunking (Sha and Pereira 2003), and discourse chunking (Ghosh et al. 2011) in accordance with the Penn Discourse Treebank (Prasad et al. 2008). DCRFs, being a discriminative approach to sequence modeling, have several advantages over their generative counterparts such as Hidden Markov Models (HMMs) and MRFs, which first model the joint distribution p(y,x|), and then infer the conditional distribution p(y|x, ). It has been advocated that discriminative models are generally more accurate than generative ones because they do not “waste resources” modeling complex distributions that are observed (i.e., p(x)); instead, they f</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha, Fei and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL-HLT’03, pages 134–141, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Silber</author>
<author>Kathleen McCoy</author>
</authors>
<title>Efficiently computed lexical chains as an intermediate representation for automatic text summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="60020" citStr="Silber and McCoy 2002" startWordPosition="9387" endWordPosition="9390">spans the whole text, skipping paragraphs P2 and P3, while a second chain only spans P2 and P3. This situation makes it more likely that P2 and P3 should be linked in the DT before either of them is linked with another paragraph. Therefore, the DT structure in Figure 10b should be more likely than the structure in Figure 10c. One challenge in computing lexical chains is that words can have multiple senses, and semantic relationships depend on the sense rather than the word itself. Several methods have been proposed to compute lexical chains (Barzilay and Elhadad 1997; Hirst and St. Onge 1997; Silber and McCoy 2002; Galley and McKeown 2003). We follow the state-of-the-art approach proposed by Galley and McKeown (2003), which extracts lexical chains after performing Word Sense Disambiguation (WSD). P P P P P P P P 1 2 3 4 1 2 3 4 (b) (c) P P P P 1 2 3 4 (a) Figure 10 Correlation between lexical chains and discourse structure. (a) Lexical chains spanning paragraphs. (b) and (c) Two possible DT structures. 404 Joty, Carenini, and Ng CODRA Figure 11 Extracting lexical chains. (a) A Lexical Semantic Relatedness Graph (LSRG) for five noun-tokens. (b) Resultant graph after performing WSD. The box at the bottom</context>
</contexts>
<marker>Silber, McCoy, 2002</marker>
<rawString>Silber, Gregory and Kathleen McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4):487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies.</title>
<date>2011</date>
<publisher>Morgan</publisher>
<contexts>
<context position="31392" citStr="Smith 2011" startWordPosition="4687" endWordPosition="4688">bilistic parsing model like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT constituents, CODRA uses undirected conditional graphical models (i.e., CRFs) as its parsing models. To find the most probable DT, unlike most previous studies (Marcu 1999; Subba and Di-Eugenio 2009; Hernault et al. 2010), which adopt a greedy solution, CODRA applies an optimal CKY parsing algorithm to the inferred posterior probabilities (obtained from the CRFs) of all possible DT constituents. Furthermore, the parsing algorithm all</context>
<context position="66265" citStr="Smith 2011" startWordPosition="10442" endWordPosition="10443">equires O(Mn3). In spite of the computational cost, the gain we attained in the subsequent passes was not significant for our development set. Therefore, we restrict our parser to only one-pass post-editing. Note that in parsing models where the score (i.e., likelihood) of a parse tree decomposes across local factors (e.g., the CRF-based syntactic parser of Finkel, Kleeman, and Manning [2008]), it is possible to define a semiring using the factors and the local scores (e.g., given by the inside algorithm). The CKY algorithm could then give the optimal parse tree in a single post-editing pass (Smith 2011). However, because our intra-sentential parsing model is designed to capture sequential dependencies between DT constituents, the score of a DT does not directly decompose across factors over discourse productions. Therefore, designing such a semiring was not possible in our case. In addition to these features, we also experimented with other features including WordNet-based lexical semantics, subjectivity, and TF.IDF-based cosine similarity. However, because such features did not improve parsing performance on our development set, they were excluded from our final set of features. 4.2 Parsing</context>
</contexts>
<marker>Smith, 2011</marker>
<rawString>Smith, Noah A. 2011. Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies. Morgan and Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>455--465</pages>
<location>Sofia.</location>
<contexts>
<context position="27469" citStr="Socher et al. 2013" startWordPosition="4083" endWordPosition="4086"> between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse parsing. However, as in Socher et al. (2013a, 2013b), word vectors (i.e., embeddings) are not learned explicitly for the task, rather they are taken from Collobert et al. (2011). Given the vectors of the words in an EDU, their model first composes them hiera</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Socher, Richard, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013a. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455–465, Sofia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="27469" citStr="Socher et al. 2013" startWordPosition="4083" endWordPosition="4086"> between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse parsing. However, as in Socher et al. (2013a, 2013b), word vectors (i.e., embeddings) are not learned explicitly for the task, rather they are taken from Collobert et al. (2011). Given the vectors of the words in an EDU, their model first composes them hiera</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Socher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
</authors>
<title>Discourse-Level Relations for Opinion Analysis.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pittsburgh, PA.</institution>
<contexts>
<context position="3290" citStr="Somasundaran 2010" startWordPosition="474" endWordPosition="475">nce structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., bec</context>
<context position="130044" citStr="Somasundaran 2010" startWordPosition="20799" endWordPosition="20800">ed directed graph, which is shallow and not sufficiently informative in most cases. Furthermore, their approach does not allow compression at the sentence level, which is often beneficial in summarization. In the future, we would like to investigate the utility of our rhetorical structure for performing sentence compression, selection, and ordering in a joint summarization process. Discourse structure can also play important roles in sentiment analysis. A key research problem in sentiment analysis is extracting fine-grained opinions about different aspects of a product. Several recent papers (Somasundaran 2010; Lazaridou, Titov, and 427 Computational Linguistics Volume 41, Number 3 Sporleder 2013) exploited the rhetorical structure for this task. Another challenging problem is assessing the overall opinion expressed in a review because not all sentences in a review contribute equally to the overall sentiment. For example, some sentences are subjective, whereas others are objective (Pang and Lee 2004); some express the main claims, whereas others support them (Taboada et al. 2011); some express opinions about the main entity, whereas others are about the peripherals. Discourse structure could be use</context>
</contexts>
<marker>Somasundaran, 2010</marker>
<rawString>Somasundaran, S. 2010. Discourse-Level Relations for Opinion Analysis. Ph.D. thesis, University of Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>149--156</pages>
<location>Edmonton.</location>
<contexts>
<context position="7207" citStr="Soricut and Marcu 2003" startWordPosition="1066" endWordPosition="1069"> analysis in RST involves two subtasks: discourse segmentation is the task of breaking the text into a sequence of EDUs, and discourse parsing is the task of linking the discourse units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse parsing and rhetorical parsing interchangeably. While recent advances in automatic discourse segmentation have attained high accuracies (an F-score of 90.5% reported by Fisher and Roark [2007]), discourse parsing still poses significant challenges (Feng and Hirst 2012) and the performance of the existing discourse parsers (Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) is still considerably inferior compared with the human gold standard. Thus, the impact of rhetorical structure in downstream NLP applications is still very limited. The work we present in this article aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing discourse parsers. First, existing discourse parsers typically model the structure and the labels of a DT separately, and also do not take into account the sequential dependencies between the DT constituents. </context>
<context position="15122" citStr="Soricut and Marcu 2003" startWordPosition="2255" endWordPosition="2258">tudy, to break the text into EDUs and to build DTs for sentences first, then for paragraphs, and so on. Despite the fact that this work pioneered the field of rhetorical analysis, it has many limitations. First, identifying discourse connectives is a difficult task on its own, because (depending on the usage), the same phrase may or may not signal a discourse relation (Pitler and Nenkova 2009). For example, but can either signal a Contrast discourse relation or can simply perform non-discourse acts. Second, discourse segmentation using only discourse connectives fails to attain high accuracy (Soricut and Marcu 2003). Third, DT structures do not always correspond to paragraph structures; for example, Sporleder and Lapata (2004) report that more than 20% of the paragraphs in the RST–DT corpus (Carlson, Marcu, and Okurowski 2002) do not correspond to a discourse unit in the DT. Fourth, discourse cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61</context>
<context position="19646" citStr="Soricut and Marcu (2003)" startWordPosition="2920" endWordPosition="2923">er mainly uses discourse cues, shallowsyntactic (i.e., POS tags) and contextual features (i.e., neighboring words and their POS tags). To learn the shift–reduce actions, the discourse parser encodes five types of features: lexical (e.g., discourse cues), shallow-syntactic, textual similarity, operational (previous n shift–reduce operations), and rhetorical sub-structural features. Despite the fact that this work has pioneered many of today’s machine learning approaches to discourse parsing, it has all the limitations mentioned in Section 1. The work of Marcu (1999) is considerably improved by Soricut and Marcu (2003). They present the publicly available SPADE system,Z which comes with probabilistic models for discourse segmentation and sentence-level discourse parsing. Their segmentation and parsing models are based on lexico-syntactic patterns (or features) extracted from the lexicalized syntactic tree of a sentence. The discourse parser uses an optimal parsing algorithm to find the most probable DT structure for a sentence. SPADE was trained and tested on the RST–DT corpus. This work, by showing empirically the connection between syntax and discourse structure at the sentence level, has greatly influenc</context>
<context position="30980" citStr="Soricut and Marcu 2003" startWordPosition="4626" endWordPosition="4629">arsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and what relations (i.e., the labels) to use in the process of building the DT. Specifically, discourse parsing requires: (1) a parsing model to explore the search space of possible structures and labels for their nodes, and (2) a parsing algorithm for selecting the best parse tree(s) among the candidates. A probabilistic parsing model like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT constituents, CODRA uses </context>
<context position="35420" citStr="Soricut and Marcu 2003" startWordPosition="5292" endWordPosition="5295">imilarly, the relative distributions 6 By the term “fat” we refer to CRFs with multiple (interconnected) chains of output variables. 7 For n + 1 EDUs, the number of valid discourse tree structures (i.e., not counting possible variations in the nuclearity and relation labels) is the Catalan number Cn. 394 Joty, Carenini, and Ng CODRA of Background, Contrast, Cause, and Explanation are different in the two parsing scenarios. On the other hand, different kinds of features are applicable and informative for intraversus multi-sentential parsing. For example, syntactic features like dominance sets (Soricut and Marcu 2003) are extremely useful for parsing at the sentence-level, but are not even applicable in the multi-sentential case. Likewise, lexical chain features (Sporleder and Lapata 2004), which are useful for multi-sentential parsing, are not applicable at the sentence level. Based on these above observations, CODRA comprises two separate modules: an intra-sentential parser and a multi-sentential parser, as shown in Figure 2. First, the intra-sentential parser produces one or more discourse sub-trees for each sentence. Then, the multi-sentential parser generates a full DT for the document from these sub-</context>
<context position="37506" citStr="Soricut and Marcu 2003" startWordPosition="5612" endWordPosition="5615">h j. For example, the DT for the second sentence in Figure 1 can be represented as {Elaboration–NS[4,4,5], Same–Unit–NN[4,5,6]}. Notice that in this representation, a relation R also specifies the nuclearity status of the discourse units involved, which can be one of Nucleus–Satellite (NS), Satellite–Nucleus (SN), or Nucleus–Nucleus (NN). Attaching nuclearity status to the relations allows us to perform the two subtasks of discourse parsing, relation identification and nuclearity assignment, simultaneously. A common assumption made for generating DTs effectively is that they are binary trees (Soricut and Marcu 2003; Hernault et al. 2010). That is, multi-nuclear relations (e.g., Joint, Same–Unit) involving more than two discourse units are mapped to a hierarchical right-branching binary tree. For example, a flat Joint(e1,e2,e3,e4) (Figure 4a) is mapped to a right-branching binary tree Joint(e1, Joint(e2, Joint(e3, e4))) (Figure 4b). Figure 4 Multi-nuclear relation and its corresponding binary tree representation. 395 Computational Linguistics Volume 41, Number 3 4.1 Parsing Models As mentioned before, the job of the intra- and multi-sentential parsing models of CODRA is to assign a probability to each of</context>
<context position="52635" citStr="Soricut and Marcu 2003" startWordPosition="8151" endWordPosition="8154">T’s graphical model toolkit GRMM (McCallum 2002). In order to avoid overfitting, we regularize the CRF models with l2 regularization and learn the model parameters using the limited-memory BFGS (L-BFGS) fitting algorithm. 4.1.4 Features Used in the Parsing Models. Crucial to parsing performance is the set of features used in the parsing models, as summarized in Table 1. We categorize the features into seven groups and specify which groups are used in what parsing model. Notice that some of the features are used in both models. Most of the features have been explored in previous studies (e.g., Soricut and Marcu 2003; Sporleder and Lapata 2005; Hernault et al. 2010). However, we improve some of these as explained subsequently. The features are extracted from two adjacent discourse units Ut−1 and Ut. Organizational features encode useful information about text organization as shown by duVerle and Prendinger (2009). We measure the length of the discourse units as the number of EDUs and tokens in it. However, in order to better adjust to the length variations, rather than computing their absolute numbers in a unit, we choose to measure their relative numbers with respect to their total numbers in the two uni</context>
<context position="57273" citStr="Soricut and Marcu 2003" startWordPosition="8924" endWordPosition="8927"> frequent, but also the ones that are indicative of the labels in the training data. In addition to the lexical N-grams we also encode the POS tags of the first and last N tokens (NE{1, 2,3}) in a discourse unit as shallow-syntactic features in our models. Figure 9 Dominance set features for intra-sentential discourse parsing. 403 Computational Linguistics Volume 41, Number 3 Lexico-syntactic features dominance sets extracted from the Discourse Segmented Lexicalized Syntactic Tree (DS-LST) of a sentence have been shown to be extremely effective for intra-sentential discourse parsing in SPADE (Soricut and Marcu 2003). Figure 9a shows the DS-LST (i.e., lexicalized syntactic tree with EDUs identified) for a sentence with three EDUs from the RST–DT corpus, and Figure 9b shows the corresponding discourse tree. In a DS-LST, each EDU except the one with the root node must have a head node NH that is attached to an attachment node NA residing in a separate EDU. A dominance set D (shown at the bottom of Figure 9a) contains these attachment points (shown in boxes) of the EDUs in a DS-LST. In addition to the syntactic and lexical information of the head and attachment nodes, each element in the dominance set also i</context>
<context position="70717" citStr="Soricut and Marcu 2003" startWordPosition="11176" endWordPosition="11179">se for each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates simultaneously. One can show that the time and space complexities of the k-best version of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005). Note that, in contrast to other document-level discourse parsers (Marcu 2000b; Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This approach of CODRA is also different from the sentence-level discourse parser SPADE (Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell D[i, j] in SPADE’s dynamic programming table stores D[i,j] = P([Ui(0), Um.(1), Uj(1)]) (11) where m* = argmax P([Ui(0), Um(1), Uj(1)]). Disregarding the relation label R while i&lt;m&lt;j populating D, this approach may find a discourse tree that is not globally optimal. 4.3 Document-Level Parsing Approaches Now that we have presented our intra-sentential and multi-sentential parsing components, we are ready to describe how they can be effect</context>
<context position="76932" citStr="Soricut and Marcu 2003" startWordPosition="12236" endWordPosition="12239">other simple heuristic that one could try is: When both DTs segment the sentence into multiple sub-trees, pick the one with fewer sub-trees, and when only one of the DTs segment the sentence into multiple sub-trees, pick that one. At the end, the multi-sentential parser takes all these sentence-level sub-trees for a document, and builds a full rhetorical parse for the whole document. 5. The Discourse Segmenter Our discourse parser assumes that the input text has been already segmented into a sequence of EDUs. However, discourse segmentation is also a challenging problem, and previous studies (Soricut and Marcu 2003; Fisher and Roark 2007) have identified (a) (b) S (i) S1 S2 S3 (c) 2 (ii) 1 2 3 4 5 6 7 4 5 6 7 8 9 10 4 5 6 7 8 9 10 S1 S2 S2 S 3 S2 S3 1 2 3 4 5 6 7 4 5 6 7 8 9 10 410 Joty, Carenini, and Ng CODRA it as a primary source of inaccuracy for discourse parsing. Regardless of its importance in discourse parsing, discourse segmentation itself can be useful in several NLP applications, including sentence compression (Sporleder and Lapata 2005) and textual alignment in statistical machine translation (Stede 2011). Therefore, in CODRA, we have developed our own discourse segmenter, which not only ach</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Soricut, Radu and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL’03, pages 149–156, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
</authors>
<title>Manually vs. automatically labelled data in discourse relation classification. Effects of example and feature selection.</title>
<date>2007</date>
<journal>LDV Forum,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Sporleder, 2007</marker>
<rawString>Sporleder, Caroline. 2007. Manually vs. automatically labelled data in discourse relation classification. Effects of example and feature selection. LDV Forum, 22(1):1–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic paragraph identification: A study across languages and domains.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP ’04,</booktitle>
<pages>72--79</pages>
<location>Barcelona.</location>
<contexts>
<context position="15235" citStr="Sporleder and Lapata (2004)" startWordPosition="2271" endWordPosition="2274">te the fact that this work pioneered the field of rhetorical analysis, it has many limitations. First, identifying discourse connectives is a difficult task on its own, because (depending on the usage), the same phrase may or may not signal a discourse relation (Pitler and Nenkova 2009). For example, but can either signal a Contrast discourse relation or can simply perform non-discourse acts. Second, discourse segmentation using only discourse connectives fails to attain high accuracy (Soricut and Marcu 2003). Third, DT structures do not always correspond to paragraph structures; for example, Sporleder and Lapata (2004) report that more than 20% of the paragraphs in the RST–DT corpus (Carlson, Marcu, and Okurowski 2002) do not correspond to a discourse unit in the DT. Fourth, discourse cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phra</context>
<context position="35595" citStr="Sporleder and Lapata 2004" startWordPosition="5317" endWordPosition="5320">iscourse tree structures (i.e., not counting possible variations in the nuclearity and relation labels) is the Catalan number Cn. 394 Joty, Carenini, and Ng CODRA of Background, Contrast, Cause, and Explanation are different in the two parsing scenarios. On the other hand, different kinds of features are applicable and informative for intraversus multi-sentential parsing. For example, syntactic features like dominance sets (Soricut and Marcu 2003) are extremely useful for parsing at the sentence-level, but are not even applicable in the multi-sentential case. Likewise, lexical chain features (Sporleder and Lapata 2004), which are useful for multi-sentential parsing, are not applicable at the sentence level. Based on these above observations, CODRA comprises two separate modules: an intra-sentential parser and a multi-sentential parser, as shown in Figure 2. First, the intra-sentential parser produces one or more discourse sub-trees for each sentence. Then, the multi-sentential parser generates a full DT for the document from these sub-trees. Both of our parsers have the same two components: a parsing model and a parsing algorithm. Whereas the two parsing models are rather different, the same parsing algorit</context>
<context position="59265" citStr="Sporleder and Lapata 2004" startWordPosition="9255" endWordPosition="9258">DUs j and j + 1. In our example, for the two units, containing EDUs e1 and e2, respectively, the relevant dominance set element is (1, efforts/NP)&gt;(2, to/S). We encode the syntactic labels and lexical heads of NH and NA, and the dominance relationship as features in our intra-sentential parsing model. Lexical chains (Morris and Hirst 1991) are sequences of semantically related words that can indicate topical boundaries in a text (Galley et al. 2003; Joty, Carenini, and Ng 2013). Features extracted from lexical chains are also shown to be useful for finding paragraph-level discourse structure (Sporleder and Lapata 2004). For example, consider the text with four paragraphs (P1 to P4) in Figure 10a. Now, let us assume that there is a lexical chain that spans the whole text, skipping paragraphs P2 and P3, while a second chain only spans P2 and P3. This situation makes it more likely that P2 and P3 should be linked in the DT before either of them is linked with another paragraph. Therefore, the DT structure in Figure 10b should be more likely than the structure in Figure 10c. One challenge in computing lexical chains is that words can have multiple senses, and semantic relationships depend on the sense rather th</context>
<context position="62527" citStr="Sporleder and Lapata (2004)" startWordPosition="9824" endWordPosition="9827">er their different senses are summed up and the one with the highest score is considered to be the right sense for the wordtoken. For example, if repetition and synonymy are weighted equally, and hypernymy is given half as much weight as either of them, the score of bank’s two senses are: 1 + 0.5 + 0.5 = 2 for the sense money bank and 1 + 0.5 = 1.5 for the sense river bank. Therefore, the selected sense for bank in this context is river bank. In case of a tie, we select the sense that is most frequent (i.e., the first sense in WordNet). Note that this approach to WSD is different from that of Sporleder and Lapata (2004), which takes a greedy approach. Finally, we prune the graph by only keeping the links that connect words with the selected senses. At the end of the process, we are left with the edges that form the actual lexical chains. For example, Figure 11b shows the result of pruning the graph in Figure 11a. The lexical chains extracted from the pruned graph are shown in the box at the bottom. Following Sporleder and Lapata (2004), for each chain element, we keep track of the location (i.e., sentence ID) in the text where that element was found, and exclude chains containing only one element. Given two </context>
</contexts>
<marker>Sporleder, Lapata, 2004</marker>
<rawString>Sporleder, Caroline and Mirella Lapata. 2004. Automatic paragraph identification: A study across languages and domains. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP ’04, pages 72–79, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Mirella Lapata</author>
</authors>
<title>Discourse chunking and its application to sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural</booktitle>
<contexts>
<context position="3100" citStr="Sporleder and Lapata 2005" startWordPosition="445" endWordPosition="448">of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different vie</context>
<context position="20982" citStr="Sporleder and Lapata (2005)" startWordPosition="3115" endWordPosition="3118"> not produce a full-text (i.e., document-level) parse. Second, it applies a generative parsing model based on only lexico-syntactic features, whereas discriminative models are generally considered to be more accurate, and can incorporate arbitrary features more effectively (Murphy 2012). Third, the parsing model makes an independence assumption between the label and the structure of a DT constituent, and it ignores the sequential and the hierarchical dependencies between the DT constituents. Subsequent research addresses the question of how much syntax one really needs in rhetorical analysis. Sporleder and Lapata (2005) focus on the discourse chunking problem, comprising two subtasks: discourse segmentation and (flat) nuclearity assignment. They formulate discourse chunking in two alternative ways. First, one-step classification, where the discourse chunker, a multi-class classifier, assigns to each token one of the four labels: (1) B–NUC (beginning of a nucleus), (2) I–NUC (inside a nucleus), (3) B– SAT (beginning of a satellite), and (4) I–SAT (inside a satellite). Therefore, this approach performs discourse segmentation and nuclearity assignment simultaneously. Second, 2 http://www.isi.edu/licensed-sw/spa</context>
<context position="52662" citStr="Sporleder and Lapata 2005" startWordPosition="8155" endWordPosition="8158">kit GRMM (McCallum 2002). In order to avoid overfitting, we regularize the CRF models with l2 regularization and learn the model parameters using the limited-memory BFGS (L-BFGS) fitting algorithm. 4.1.4 Features Used in the Parsing Models. Crucial to parsing performance is the set of features used in the parsing models, as summarized in Table 1. We categorize the features into seven groups and specify which groups are used in what parsing model. Notice that some of the features are used in both models. Most of the features have been explored in previous studies (e.g., Soricut and Marcu 2003; Sporleder and Lapata 2005; Hernault et al. 2010). However, we improve some of these as explained subsequently. The features are extracted from two adjacent discourse units Ut−1 and Ut. Organizational features encode useful information about text organization as shown by duVerle and Prendinger (2009). We measure the length of the discourse units as the number of EDUs and tokens in it. However, in order to better adjust to the length variations, rather than computing their absolute numbers in a unit, we choose to measure their relative numbers with respect to their total numbers in the two units. For example, if the two</context>
<context position="77374" citStr="Sporleder and Lapata 2005" startWordPosition="12335" endWordPosition="12338">umes that the input text has been already segmented into a sequence of EDUs. However, discourse segmentation is also a challenging problem, and previous studies (Soricut and Marcu 2003; Fisher and Roark 2007) have identified (a) (b) S (i) S1 S2 S3 (c) 2 (ii) 1 2 3 4 5 6 7 4 5 6 7 8 9 10 4 5 6 7 8 9 10 S1 S2 S2 S 3 S2 S3 1 2 3 4 5 6 7 4 5 6 7 8 9 10 410 Joty, Carenini, and Ng CODRA it as a primary source of inaccuracy for discourse parsing. Regardless of its importance in discourse parsing, discourse segmentation itself can be useful in several NLP applications, including sentence compression (Sporleder and Lapata 2005) and textual alignment in statistical machine translation (Stede 2011). Therefore, in CODRA, we have developed our own discourse segmenter, which not only achieves state-of-theart performance as shown later, but also reduces the time complexity by using fewer features. 5.1 Segmentation Model The discourse segmenter in CODRA implements a binary classifier to decide for each word–token (except the last) in a sentence, whether to place an EDU boundary after that token. We use a maximum entropy model to build a discriminative classifier. More specifically, we use a Logistic Regression classifier w</context>
<context position="81782" citStr="Sporleder and Lapata 2005" startWordPosition="13050" endWordPosition="13053"> be rare, other variations of the production, depending on whether they include the lexical heads and how many non-terminals (one or two) they consider before and after the potential boundary, are examined one after another (see Fisher and Roark [2007] for details). In contrast, we compute the maximum likelihood estimates for a primary production (feature) and its other variations, and use those directly as features with/without binarizing the values. Shallow syntactic features like Chunk and POS tags have been shown to possess valuable clues for discourse segmentation (Fisher and Roark 2007; Sporleder and Lapata 2005). For example, it is less likely that an EDU boundary occurs within a chunk. We annotate the tokens of a sentence with chunk and POS tags using the state-of-the-art Illinois taggerll and encode these as features in our model. Note that the chunker assigns each token a tag using the BIO notation, where B stands for beginning of a particular phrase (e.g., noun or verb phrase), I stands for inside of a particular phrase, and O stands for outside of a particular phrase. The rationale for using the Illinois chunker is that it uses a larger set of tags (23 in total); thus it is more informative than</context>
</contexts>
<marker>Sporleder, Lapata, 2005</marker>
<rawString>Sporleder, Caroline and Mirella Lapata. 2005. Discourse chunking and its application to sentence compression. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural</rawString>
</citation>
<citation valid="false">
<booktitle>Language Processing, HLT-EMNLP’05,</booktitle>
<pages>257--264</pages>
<location>Vancouver.</location>
<marker></marker>
<rawString>Language Processing, HLT-EMNLP’05, pages 257–264, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Exploiting linguistic cues to classify rhetorical relations.</title>
<date>2005</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP),</booktitle>
<pages>157--166</pages>
<contexts>
<context position="17304" citStr="Sporleder and Lascarides (2005)" startWordPosition="2590" endWordPosition="2593">ey are not explicitly signaled? The idea is to use unambiguous discourse cues (e.g., although for Contrast, for example for Elaboration) to automatically label a large corpus with rhetorical relations that could then be used to train a supervised model.&apos; A series of previous studies have explored this idea. Marcu and Echihabi (2002) first attempted to identify four broad classes of relations: Contrast, Elaboration, Condition, and Cause–Explanation–Evidence. They used a naive Bayes classifier based on word pairs (w1, w2), where w1 occurs in the left segment, and w2 occurs in the right segment. Sporleder and Lascarides (2005) included other features (e.g., words and their stems, Part-of-Speech [POS] tags, positions, segment lengths) in a boosting-based classifier (i.e., BoosTexter [Schapire and Singer 2000]) to further improve relation classification accuracy. However, these studies evaluated classification performance on the instances 1 We categorize this approach as unsupervised because it does not rely on human-annotated data. 389 Computational Linguistics Volume 41, Number 3 where rhetorical relations were originally signaled (i.e., the discourse cues were artificially removed), and did not verify how well thi</context>
</contexts>
<marker>Sporleder, Lascarides, 2005</marker>
<rawString>Sporleder, Caroline and Alex Lascarides. 2005. Exploiting linguistic cues to classify rhetorical relations. In Proceedings of Recent Advances in Natural Language Processing (RANLP), pages 157–166, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="15903" citStr="Sporleder and Lascarides (2008)" startWordPosition="2376" endWordPosition="2379">agraphs in the RST–DT corpus (Carlson, Marcu, and Okurowski 2002) do not correspond to a discourse unit in the DT. Fourth, discourse cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba and Di-Eugenio 2009) report even higher figures: About 60% of discourse relations are not explicitly signaled. Therefore, rather than relying on hand-coded rules based on discourse cues and surface patterns, recent approaches use machine learning techniques with a large set of informative features. While some rhetorical relations need to be explicitly signaled by discourse cues (e.g., Concession) and some do not (e.g., Background), there is a large middle ground of rel</context>
<context position="18085" citStr="Sporleder and Lascarides 2008" startWordPosition="2695" endWordPosition="2698">sTexter [Schapire and Singer 2000]) to further improve relation classification accuracy. However, these studies evaluated classification performance on the instances 1 We categorize this approach as unsupervised because it does not rely on human-annotated data. 389 Computational Linguistics Volume 41, Number 3 where rhetorical relations were originally signaled (i.e., the discourse cues were artificially removed), and did not verify how well this approach performs on the instances that are not originally signaled. Subsequent studies (Blair-Goldensohn, McKeown, and Rambow 2007; Sporleder 2007; Sporleder and Lascarides 2008) confirm that classifiers trained on instances stripped of their original discourse cues do not generalize well to implicit cases because they are linguistically quite different. Note that this approach to identifying discourse relations in the absence of manually labeled data does not fully solve the parsing problem (i.e., building DTs); rather, it only attempts to identify a small subset of coarser relations between two (flat) text segments (i.e., a tagging problem). Arguably, to perform a complete rhetorical analysis, one needs to use supervised machine learning techniques based on human-an</context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>Sporleder, Caroline and Alex Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineering, 14(3):369–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>The Potsdam Commentary Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL-04 Workshop on Discourse Annotation,</booktitle>
<pages>96--102</pages>
<location>Barcelona.</location>
<contexts>
<context position="16008" citStr="Stede 2004" startWordPosition="2396" endWordPosition="2397">course cues are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba and Di-Eugenio 2009) report even higher figures: About 60% of discourse relations are not explicitly signaled. Therefore, rather than relying on hand-coded rules based on discourse cues and surface patterns, recent approaches use machine learning techniques with a large set of informative features. While some rhetorical relations need to be explicitly signaled by discourse cues (e.g., Concession) and some do not (e.g., Background), there is a large middle ground of relations that may be signaled or not. For these “middle ground” relations, can we exploit features present </context>
<context position="111050" citStr="Stede 2004" startWordPosition="17650" endWordPosition="17651">two document-level parsing approaches reveals that TSP SW significantly outperforms TSP 1-1 only in finding the right structure on both corpora (p-value &lt;0.01). Not surprisingly, the improvement is higher on the instructional corpus. A likely explanation is that the instructional corpus contains more leaky boundaries (12%), allowing the sliding window approach to be more effective in finding those, without inducing much noise for the labels. This demonstrates the potential of TSP SW for data sets with even more leaky boundaries, e.g., the Dutch (Vliet and Redeker 2011) and the German Potsdam (Stede 2004) corpora. However, it would be interesting to see how other heuristics to do consolidation in the cross condition (Section 4.3.2) perform. To analyze errors made by TSP SW, we looked at some poorly parsed examples and found that although TSP SW finds more correct structures, a corresponding improvement in labeling relations is not present because in some cases, it tends to induce noise from the neighboring sentences for the labels. For example, when parsing is performed on the first sentence in Figure 1 in isolation using 1S-1S, our parser rightly identifies the Contrast relation between EDUs </context>
</contexts>
<marker>Stede, 2004</marker>
<rawString>Stede, Manfred. 2004. The Potsdam Commentary Corpus. In Proceedings of the ACL-04 Workshop on Discourse Annotation, pages 96–102, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>Discourse Processing. Synthesis Lectures on Human Language Technologies.</title>
<date>2011</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="13728" citStr="Stede (2011)" startWordPosition="2044" endWordPosition="2045">analysis of results are presented in Section 6. Finally, we summarize our contributions with future directions in Section 7. 2. Related Work Rhetorical analysis has a long history—dating back to Mann and Thompson (1988), when RST was initially proposed as a useful linguistic method for describing natural texts, to more recent attempts to automatically extract the rhetorical structure of a given text (Hernault et al. 2010). In this section, we provide a brief overview of the computational approaches that follow RST as the theory of discourse, and that are related to our work; see the survey by Stede (2011) for a broader overview that also includes other theories of discourse. 388 Joty, Carenini, and Ng CODRA 2.1 Unsupervised and Rule-Based Approaches Although the most effective approaches to rhetorical analysis to date rely on supervised machine learning methods trained on human-annotated data, unsupervised methods have also been proposed, as they do not require human-annotated data and can be more easily applied to new domains. Often, discourse connectives like but, because, and although convey clear information on the kind of relation linking the two text segments. In his early work, Marcu (2</context>
<context position="77444" citStr="Stede 2011" startWordPosition="12346" endWordPosition="12347"> discourse segmentation is also a challenging problem, and previous studies (Soricut and Marcu 2003; Fisher and Roark 2007) have identified (a) (b) S (i) S1 S2 S3 (c) 2 (ii) 1 2 3 4 5 6 7 4 5 6 7 8 9 10 4 5 6 7 8 9 10 S1 S2 S2 S 3 S2 S3 1 2 3 4 5 6 7 4 5 6 7 8 9 10 410 Joty, Carenini, and Ng CODRA it as a primary source of inaccuracy for discourse parsing. Regardless of its importance in discourse parsing, discourse segmentation itself can be useful in several NLP applications, including sentence compression (Sporleder and Lapata 2005) and textual alignment in statistical machine translation (Stede 2011). Therefore, in CODRA, we have developed our own discourse segmenter, which not only achieves state-of-theart performance as shown later, but also reduces the time complexity by using fewer features. 5.1 Segmentation Model The discourse segmenter in CODRA implements a binary classifier to decide for each word–token (except the last) in a sentence, whether to place an EDU boundary after that token. We use a maximum entropy model to build a discriminative classifier. More specifically, we use a Logistic Regression classifier with parameter ©: P(yjw, ©) = Ber(yj Sigm(©Tx)) (12) where the output y</context>
</contexts>
<marker>Stede, 2011</marker>
<rawString>Stede, Manfred. 2011. Discourse Processing. Synthesis Lectures on Human Language Technologies. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajen Subba</author>
<author>Barbara Di-Eugenio</author>
</authors>
<title>An effective discourse parser that uses rich linguistic information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL’09,</booktitle>
<pages>566--574</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="7234" citStr="Subba and Di-Eugenio 2009" startWordPosition="1070" endWordPosition="1073">s two subtasks: discourse segmentation is the task of breaking the text into a sequence of EDUs, and discourse parsing is the task of linking the discourse units (EDUs and larger units) into a labeled tree. In this article, we use the terms discourse parsing and rhetorical parsing interchangeably. While recent advances in automatic discourse segmentation have attained high accuracies (an F-score of 90.5% reported by Fisher and Roark [2007]), discourse parsing still poses significant challenges (Feng and Hirst 2012) and the performance of the existing discourse parsers (Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) is still considerably inferior compared with the human gold standard. Thus, the impact of rhetorical structure in downstream NLP applications is still very limited. The work we present in this article aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing discourse parsers. First, existing discourse parsers typically model the structure and the labels of a DT separately, and also do not take into account the sequential dependencies between the DT constituents. However, for several NLP ta</context>
<context position="16050" citStr="Subba and Di-Eugenio 2009" startWordPosition="2400" endWordPosition="2403">ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba and Di-Eugenio 2009) report even higher figures: About 60% of discourse relations are not explicitly signaled. Therefore, rather than relying on hand-coded rules based on discourse cues and surface patterns, recent approaches use machine learning techniques with a large set of informative features. While some rhetorical relations need to be explicitly signaled by discourse cues (e.g., Concession) and some do not (e.g., Background), there is a large middle ground of relations that may be signaled or not. For these “middle ground” relations, can we exploit features present in the signaled cases to automatically ide</context>
<context position="24646" citStr="Subba and Di-Eugenio (2009)" startWordPosition="3665" endWordPosition="3668"> that uses the same lexico-syntactic features used in SPADE, but with more context (i.e., the lexico-syntactic features for the previous two words and the following two words). The discourse parser iteratively uses two SVM classifiers in a pipeline to build a DT. In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label. Using this simple method, they report promising results in document-level discourse parsing on the RST–DT. For a different genre, instructional texts, Subba and Di-Eugenio (2009) propose a shift–reduce discourse parser that relies on a classifier for relation labeling. Their classifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a large set of features including the linguistically rich compositional semantics coming from a semantic parser. They demonstrate that including compositional semantics with other features improves the performance of the classifier, thus, also improves the performance of the parser. Both HILDA and the ILP-based approach of Subba and Di-Eugenio (2009) are limited in several ways. First, they do not differentiate</context>
<context position="31007" citStr="Subba and Di-Eugenio 2009" startWordPosition="4630" endWordPosition="4633">ining which discourse units (EDUs or larger units) to relate (i.e., the structure), and what relations (i.e., the labels) to use in the process of building the DT. Specifically, discourse parsing requires: (1) a parsing model to explore the search space of possible structures and labels for their nodes, and (2) a parsing algorithm for selecting the best parse tree(s) among the candidates. A probabilistic parsing model like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT constituents, CODRA uses undirected conditional grap</context>
<context position="33725" citStr="Subba and Di-Eugenio 2009" startWordPosition="5032" endWordPosition="5035"> probabilities of all possible DT constituents for a given text (i.e., EDUs); then it uses a CKY parsing algorithm to combine those probabilities and find the most probable DT. Another crucial question related to parsing models is whether to use a single model or two different models for parsing at the sentence-level (i.e., intra-sentential) and at the document-level (i.e., multi-sentential). A simple and straightforward strategy would be to use a single unified parsing model for both intra- and multi-sentential parsing without distinguishing the two cases, as was previously done (Marcu 1999; Subba and Di-Eugenio 2009; Hernault et al. 2010). That approach has the advantages of making the parsing process easier, and the model gets more data to learn from. However, for a solution like ours, which tries to capture the interdependencies between constituents, this would be problematic with respect to scalability and inappropriate because of two modeling issues. More specifically, for scalability note that the number of valid trees grows exponentially with the number of EDUs in a document.&apos; Therefore, an exhaustive search over all the valid DTs is often infeasible, even for relatively small documents. For modeli</context>
<context position="70467" citStr="Subba and Di-Eugenio 2009" startWordPosition="11136" endWordPosition="11139"> it allows us to generate a list of k most probable parse trees. It is straightforward to generalize the above algorithm to produce k most probable DTs. Specifically, when filling up the dynamic programming tables, rather than storing a single best parse for each sub-tree, we store and keep track (i.e., using back-pointers) of k-best candidates simultaneously. One can show that the time and space complexities of the k-best version of the algorithm are O(n3Mk2 log k) and O(k2n), respectively (Huang and Chiang 2005). Note that, in contrast to other document-level discourse parsers (Marcu 2000b; Subba and Di-Eugenio 2009; Hernault et al. 2010; Feng and Hirst 2012, 2014), which use a greedy algorithm, CODRA finds a discourse tree that is globally optimal.9 This approach of CODRA is also different from the sentence-level discourse parser SPADE (Soricut and Marcu 2003). SPADE first finds the tree structure that is globally optimal, then it assigns the most probable relations to the internal nodes. More specifically, the cell D[i, j] in SPADE’s dynamic programming table stores D[i,j] = P([Ui(0), Um.(1), Uj(1)]) (11) where m* = argmax P([Ui(0), Um(1), Uj(1)]). Disregarding the relation label R while i&lt;m&lt;j populati</context>
<context position="73490" citStr="Subba and Di-Eugenio (2009)" startWordPosition="11639" endWordPosition="11642"> 4 5 6 7 ? ? S 2 8 9 10 S 3 (b) (a) Figure 13 Two possible DTs for three sentences. sentence boundaries. For example, in the DT shown in Figure 13b, sentence S2 does not have a well-formed sub-tree because some of its units attach to the left (i.e., 4–5 and 6) and some to the right (i.e., 7). Vliet and Redeker (2011) call these cases “leaky” boundaries. Although we find fewer than 5% of the sentences in the RST–DT have leaky boundaries, in other corpora this can be true for a larger portion of the sentences. For example, we observe that over 12% of the sentences in the instructional corpus of Subba and Di-Eugenio (2009) have leaky boundaries. However, we notice that in most cases where DT structures violate sentence boundaries, its units are merged with the units of its adjacent sentences, as in Figure 13b. For example, this is true for 75% of the leaky cases in our development set containing 20 news articles from the RST–DT and for 79% of the leaky cases in our development set containing 20 how-to manuals from the instructional corpus. Based on this observation, we propose a sliding window approach. In this approach, our intra-sentential parser works with a window of two consecutive sentences, and builds a </context>
<context position="84896" citStr="Subba and Di-Eugenio (2009)" startWordPosition="13550" endWordPosition="13553">titioned into a training set of 347 documents and a test set of 38 documents. A total of 53 documents selected from both training and test sets were annotated by two human annotators. We measure human agreements based on this doubly annotated data set. We used 25 documents from the training set as our development set. In RST–DT, the original 25 rhetorical relations defined by Mann and Thompson (1988) are further divided into a set of 18 coarser relation classes with 78 finer-grained relations (see Carlson and Marcu [2001] for details). Our second corpus is the instructional corpus prepared by Subba and Di-Eugenio (2009), which contains discourse annotations for 176 how-to manuals on home repair. The corpus was annotated with 26 informational relations (e.g., Preparation–Act, Act–Goal). For our experiments with the intra-sentential discourse parser, we extracted a sentence-level DT from a document-level DT by finding the sub-tree that exactly spans over the sentence. In RST–DT, by our count, 7,321 out of 7,673 sentences in the training set, 951 out of 991 sentences in the test set, and 1,114 out of 1, 208 sentences in the doubly-annotated set have a well-formed DT. On the other hand, 3,032 out of 3,430 senten</context>
<context position="96334" citStr="Subba and Di-Eugenio (2009)" startWordPosition="15393" endWordPosition="15396">uation In this section we present our experiments on discourse parsing. First, we describe the experimental set-up. Then, we present the results of the parsers. While presenting the performance of our discourse parser, we show a breakdown of intra-sentential versus inter-sentential results, in addition to the aggregated results at the document level. 6.4.1 Experimental Set-up for Discourse Parsing. In our experiments on sentence-level (i.e., intra-sentential) discourse parsing, we compare our approach with SPADE (Soricut and Marcu 2003) on the RST–DT corpus, and with the ILP-based approach of Subba and Di-Eugenio (2009) on the instructional corpus, because they are the state of the art in their respective genres. For SPADE, we applied the same modifications to its default settings as described in Section 6.3.1, which leads to improved performance. Similarly, in our experiments on document-level (i.e., multi-sentential) parsing, we compare our approach with HILDA (Hernault et al. 2010) on the RST–DT corpus, and with the ILPbased approach (Subba and Di-Eugenio 2009) on the instructional corpus. The results for HILDA were obtained by running the system with default settings on the same inputs we provided to our</context>
<context position="97663" citStr="Subba and Di-Eugenio (2009)" startWordPosition="15596" endWordPosition="15599">ance presented in their paper. Our experiments on the RST–DT corpus use the same 18 coarser coherence relations (see Figure 18 later in this article), defined by Carlson and Marcu (2001) and also used in SPADE and HILDA systems. More specifically, the relation set consists of 16 relation categories and two pseudo-relations, namely, Textual–Organization and Same–Unit. After attaching the nuclearity statuses (NS, SN, NN) to these relations, we obtain 41 distinct relations.15 Our experiments on the instructional corpus consider the same 26 primary relations (e.g., Goal:Act, Cause:Effect) used by Subba and Di-Eugenio (2009) and also treat the reversals of non-commutative relations as separate relations. That is, Goal–Act and Act–Goal are considered to be two different coherence relations. Attaching the nuclearity statuses to these relations provides 76 distinct relations. Based on our experiments on the development set, the size of the automatically built bi-gram and tri-gram dictionaries was set to 95% of their total number of items, and the size of the unigram dictionary was set to 100%. Note that the unigram dictionary contains only special tags denoting EDU, sentence, and paragraph boundaries. 15 Not all rel</context>
</contexts>
<marker>Subba, Di-Eugenio, 2009</marker>
<rawString>Subba, Rajen and Barbara Di-Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL’09, pages 566–574, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An introduction to conditional random fields. Foundations and Trends</title>
<date>2012</date>
<booktitle>in Machine Learning,</booktitle>
<volume>4</volume>
<issue>4</issue>
<marker>Sutton, McCallum, 2012</marker>
<rawString>Sutton, Charles and Andrew McCallum. 2012. An introduction to conditional random fields. Foundations and Trends in Machine Learning, 4(4):267–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>8--693</pages>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Sutton, Charles, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. Journal of Machine Learning Research (JMLR), 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
</authors>
<title>Discourse markers as signals (or not) of rhetorical relations.</title>
<date>2006</date>
<journal>Journal of Pragmatics,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="16022" citStr="Taboada 2006" startWordPosition="2398" endWordPosition="2399">are sometimes ambiguous; for example, but can signal Contrast, Antithesis and Concession, and so on. Finally, a more serious problem with the rule-based approach is that often rhetorical relations are not explicitly signaled by discourse cues. For example, in RST–DT, Marcu and Echihabi (2002) found that only 61 out of 238 Contrast relations and 79 out of 307 Cause–Explanation relations were explicitly signaled by cue phrases. In the British National Corpus, Sporleder and Lascarides (2008) report that half of the sentences lack a discourse cue. Other studies (Schauer and Hahn 2001; Stede 2004; Taboada 2006; Subba and Di-Eugenio 2009) report even higher figures: About 60% of discourse relations are not explicitly signaled. Therefore, rather than relying on hand-coded rules based on discourse cues and surface patterns, recent approaches use machine learning techniques with a large set of informative features. While some rhetorical relations need to be explicitly signaled by discourse cues (e.g., Concession) and some do not (e.g., Background), there is a large middle ground of relations that may be signaled or not. For these “middle ground” relations, can we exploit features present in the signale</context>
</contexts>
<marker>Taboada, 2006</marker>
<rawString>Taboada, Maite. 2006. Discourse markers as signals (or not) of rhetorical relations. Journal of Pragmatics, 38(4):567–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="130523" citStr="Taboada et al. 2011" startWordPosition="20868" endWordPosition="20871">blem in sentiment analysis is extracting fine-grained opinions about different aspects of a product. Several recent papers (Somasundaran 2010; Lazaridou, Titov, and 427 Computational Linguistics Volume 41, Number 3 Sporleder 2013) exploited the rhetorical structure for this task. Another challenging problem is assessing the overall opinion expressed in a review because not all sentences in a review contribute equally to the overall sentiment. For example, some sentences are subjective, whereas others are objective (Pang and Lee 2004); some express the main claims, whereas others support them (Taboada et al. 2011); some express opinions about the main entity, whereas others are about the peripherals. Discourse structure could be useful to capture the relative weights of the discourse units towards the overall sentiment. For example, the nucleus and satellite distinction along with the rhetorical relations could be useful to infer the relative weights of the connecting discourse units. Among other applications of discourse structure, Machine Translation (MT) and its evaluation have received a resurgence of interest recently. A workshop dedicated to discourse in machine translation was arranged recently </context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Taboada, Maite, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods for sentiment analysis. Computational Linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>William C Mann</author>
</authors>
<title>Rhetorical structure theory: Looking back and moving ahead.</title>
<date>2006</date>
<booktitle>Discourse Studies,</booktitle>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="4441" citStr="Taboada and Mann 2006" startWordPosition="641" endWordPosition="644">course relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is driven by sentence semantics. Webber (2004) and Danlos (2009) extend sentence grammar to formalize discourse structure. Rhetorical Structure Theory (RST), proposed by Mann and Thompson (1988), is perhaps the most influential theory of discourse in computational linguistics. Although it was initially intended to be used in text generation, later it became popular as a framework for parsing the structure of a text (Taboada and Mann 2006). RST represents texts by labeled hierarchical structures, called Discourse Trees (DTs). For example, consider the DT shown in Figure 1 for the following text: But he added: “Some people use the purchasers’ index as a leading indicator, some use it as a coincident indicator. But the thing it’s supposed to measure—manufacturing strength—it missed altogether last month.” The leaves of a DT correspond to contiguous atomic text spans, called elementary discourse units (EDUs; six in the example). EDUs are clause-like units that serve as building blocks. Adjacent EDUs are connected by coherence rela</context>
</contexts>
<marker>Taboada, Mann, 2006</marker>
<rawString>Taboada, Maite and William C. Mann. 2006. Rhetorical structure theory: Looking back and moving ahead. Discourse Studies, 8(3):423–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical</title>
<date>2002</date>
<contexts>
<context position="3376" citStr="Teufel and Moens 2002" startWordPosition="483" endWordPosition="486">uses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Repres</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Teufel, Simone and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>