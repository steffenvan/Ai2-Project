<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.70995">
Articles
</note>
<title confidence="0.9088255">
Discriminative Reranking for Natural
Language Parsing
</title>
<author confidence="0.992438">
Michael Collins*
</author>
<affiliation confidence="0.995641">
Massachusetts Institute of Technology
</affiliation>
<author confidence="0.994796">
Terry Koo*
</author>
<affiliation confidence="0.981118">
Massachusetts Institute of Technology
</affiliation>
<bodyText confidence="0.990595">
This article considers approaches which rerank the output of an existing probabilistic parser.
The base parser produces a set of candidate parses for each input sentence, with associated
probabilities that define an initial ranking of these parses. A second model then attempts to
improve upon this initial ranking, using additional features of the tree as evidence. The strength
of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without
concerns about how these features interact or overlap and without the need to define a
derivation or a generative model which takes these features into account. We introduce a new
method for the reranking task, based on the boosting approach to ranking problems described in
Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank.
The method combined the log-likelihood under a baseline model (that of Collins [1999]) with
evidence from an additional 500,000 features over parse trees that were not included in the
original model. The new model achieved 89.75% F-measure, a 13% relative decrease in F-
measure error over the baseline model’s score of 88.2%. The article also introduces a new
algorithm for the boosting approach which takes advantage of the sparsity of the feature space in
the parsing data. Experiments show significant efficiency gains for the new algorithm over the
obvious implementation of the boosting approach. We argue that the method is an appealing
alternative—in terms of both simplicity and efficiency—to work on feature selection methods
within log-linear (maximum-entropy) models. Although the experiments in this article are on
natural language parsing (NLP), the approach should be applicable to many other NLP
problems which are naturally framed as ranking tasks, for example, speech recognition, machine
translation, or natural language generation.
</bodyText>
<sectionHeader confidence="0.975737" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999923142857143">
Machine-learning approaches to natural language parsing have recently shown some
success in complex domains such as news wire text. Many of these methods fall into
the general category of history-based models, in which a parse tree is represented as a
derivation (sequence of decisions) and the probability of the tree is then calculated as a
product of decision probabilities. While these approaches have many advantages, it
can be awkward to encode some constraints within this framework. In the ideal case,
the designer of a statistical parser would be able to easily add features to the model
</bodyText>
<note confidence="0.8088016">
* MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), the Stata Center, Building 32,
32 Vassar Street, Cambridge, MA 02139. Email: mcollins@csail.mit.edu, maestro@mit.edu.
Submission received: 15th October 2003; Accepted for publication: 29th April 2004
0 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.99992886">
that are believed to be useful in discriminating among candidate trees for a sentence.
In practice, however, adding new features to a generative or history-based model can
be awkward: The derivation in the model must be altered to take the new features into
account, and this can be an intricate task.
This article considers approaches which rerank the output of an existing
probabilistic parser. The base parser produces a set of candidate parses for each
input sentence, with associated probabilities that define an initial ranking of these
parses. A second model then attempts to improve upon this initial ranking, using
additional features of the tree as evidence. The strength of our approach is that it
allows a tree to be represented as an arbitrary set of features, without concerns about
how these features interact or overlap and without the need to define a derivation
which takes these features into account.
We introduce a new method for the reranking task, based on the boosting
approach to ranking problems described in Freund et al. (1998). The algorithm can be
viewed as a feature selection method, optimizing a particular loss function (the
exponential loss function) that has been studied in the boosting literature. We applied
the boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus,
Santorini, and Marcinkiewicz 1993). The method combines the log-likelihood under a
baseline model (that of Collins [1999]) with evidence from an additional 500,000
features over parse trees that were not included in the original model. The baseline
model achieved 88.2% F-measure on this task. The new model achieves 89.75% F-
measure, a 13% relative decrease in F-measure error.
Although the experiments in this article are on natural language parsing, the
approach should be applicable to many other natural language processing (NLP)
problems which are naturally framed as ranking tasks, for example, speech
recognition, machine translation, or natural language generation. See Collins (2002a)
for an application of the boosting approach to named entity recognition, and Walker,
Rambow, and Rogati (2001) for the application of boosting techniques for ranking in
the context of natural language generation.
The article also introduces a new, more efficient algorithm for the boosting
approach which takes advantage of the sparse nature of the feature space in the
parsing data. Other NLP tasks are likely to have similar characteristics in terms of
sparsity. Experiments show an efficiency gain of a factor of 2,600 for the new algorithm
over the obvious implementation of the boosting approach. Efficiency issues are
important, because the parsing task is a fairly large problem, involving around one
million parse trees and over 500,000 features. The improved algorithm can perform
100,000 rounds of feature selection on our task in a few hours with current processing
speeds. The 100,000 rounds of feature selection require computation equivalent to
around 40 passes over the entire training set (as opposed to 100,000 passes for the
“naive”implementation).
The problems with history-based models and the desire to be able to specify
features as arbitrary predicates of the entire tree have been noted before. In particular,
previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della
Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al. 2002) has investigated the
use of Markov random fields (MRFs) or log-linear models as probabilistic models with
global features for parsing and other NLP tasks. (Log-linear models are often referred
to as maximum-entropy models in the NLP literature.) Similar methods have also been
proposed for machine translation (Och and Ney 2002) and language understanding in
dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman,
Hastie, and Tibshirani 1998) has drawn connections between log-linear models and
</bodyText>
<page confidence="0.990889">
26
</page>
<note confidence="0.933176">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.999894133333333">
boosting for classification problems. One contribution of our research is to draw
similar connections between the two approaches to ranking problems.
We argue that the efficient boosting algorithm introduced in this article is an
attractive alternative to maximum-entropy models, in particular, feature selection
methods that have been proposed in the literature on maximum-entropy models. The
earlier methods for maximum-entropy feature selection methods (Ratnaparkhi,
Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra,
Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require
several full passes over the training set for each round of feature selection, suggesting
that at least for the parsing data, the improved boosting algorithm is several orders of
magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to
these earlier methods for feature selection, as well as the more recent work of
McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004).
The remainder of this article is structured as follows. Section 2 reviews history-
based models for NLP and highlights the perceived shortcomings of history-based
models which motivate the reranking approaches described in the remainder of the
article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000;
Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty
2001; Collins, Schapire, and Singer 2002) that derives connections between boosting
and maximum-entropy models for the simpler case of classification problems; this
work forms the basis for the reranking methods. Section 4 describes how these
approaches can be generalized to ranking problems. We introduce loss functions for
boosting and MRF approaches and discuss optimization methods. We also derive the
efficient algorithm for boosting in this section. Section 5 gives experimental results,
investigating the performance improvements on parsing, efficiency issues, and the
effect of various parameters of the boosting algorithm. Section 6 discusses related work
in more detail. Finally, section 7 gives conclusions.
The reranking models in this article were originally introduced in Collins (2000). In
this article we give considerably more detail in terms of the algorithms involved, their
justification, and their performance in experiments on natural language parsing.
</bodyText>
<sectionHeader confidence="0.993075" genericHeader="method">
2. History-Based Models
</sectionHeader>
<bodyText confidence="0.999955583333334">
Before discussing the reranking approaches, we describe history-based models (Black
et al. 1992). They are important for a few reasons. First, several of the best-performing
parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997,
1999; Henderson 2003) are cases of history-based models. Many systems applied to
part-of-speech tagging, speech recognition, and other language or speech tasks also fall
into this class of model. Second, a particular history-based model (that of Collins
[1999]) is used as the initial model for our approach. Finally, it is important to describe
history-based models—and to explain their limitations—to motivate our departure
from them.
Parsing can be framed as a supervised learning task, to induce a function f : X--+Y
given training examples (xi, yi), where xi Z X, yi Z Y. We define GEN(x)ÎY to be the
set of candidates for a given input x. In the parsing problem x is a sentence, and
</bodyText>
<footnote confidence="0.96789325">
1 Note, however, that log-linear models which employ regularization methods instead of feature
selection—see, for example, Johnson et al. (1999) and Lafferty, McCallum, and Pereira (2001)—are likely
to be comparable in terms of efficiency to our feature selection approach. See section 6.3 for more
discussion.
</footnote>
<page confidence="0.99457">
27
</page>
<note confidence="0.803603">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.98277325">
GEN(x) is a set of candidate trees for that sentence. A particular characteristic of the
problem is the complexity of GEN(x) : GEN(x) can be very large, and each member of
GEN(x) has a rich internal structure. This contrasts with “typical”classification prob-
lems in which GEN(x) is a fixed, small set, for example, f-1,+11 in binary
classification problems.
In probabilistic approaches, a model is defined which assigns a probability P(x, y)
to each (x, y) pair.2 The most likely parse for each sentence x is then arg maxyEGEN(x)
P(x, y). This leaves the question of how to define P(x,y). In history-based approaches,
a one-to-one mapping is defined between each pair (x, y) and a decision sequence
(d1... dn). The sequence (d1... dn) can be thought of as the sequence of moves that build
(x, y) in some canonical order. Given this mapping, the probability of a tree can be
written as
</bodyText>
<equation confidence="0.983177">
P(x, y) = 11 P(di)F(d1... di-1))
i=1...n
</equation>
<bodyText confidence="0.999818125">
Here, (d1... di-1) is the history for the ith decision. F is a function which groups histo-
ries into equivalence classes, thereby making independence assumptions in the model.
Probabilistic context-free grammars (PCFGs) are one example of a history-based
model. The decision sequence (d1... dn) is defined as the sequence of rule expansions in
a top-down, leftmost derivation of the tree. The history is equivalent to a partially built
tree, and F picks out the nonterminal being expanded (i.e., the leftmost nonterminal in
the fringe of this tree), making the assumption that P(diId1... di-1) depends only on
the nonterminal being expanded. In the resulting model a tree with rule expansions
</bodyText>
<equation confidence="0.793202">
Qn P(biIAi).
(Ai—+bi) is assigned a probability i=1
</equation>
<bodyText confidence="0.650737">
Our base model, that of Collins (1999), is also a history-based model. It can be con-
sidered to be a type of PCFG, where the rules are lexicalized. An example rule would be
</bodyText>
<equation confidence="0.970257">
VP(saw) -&gt; VBD(saw) NP-C(her) NP(today)
</equation>
<bodyText confidence="0.999843333333333">
Lexicalization leads to a very large number of rules; to make the number of parameters
manageable, the generation of the right-hand side of a rule is broken down into a number
of decisions, as follows:
</bodyText>
<listItem confidence="0.999620166666667">
• First the head nonterminal (VBD in the above example) is chosen.
• Next, left and right subcategorization frames are chosen
({} and {NP-C}).
• Nonterminal sequences to the left and right of the VBD are chosen
(an empty sequence to the left, (NP-C, NP) to the right).
• Finally, the lexical heads of the modifiers are chosen (her and today).
</listItem>
<bodyText confidence="0.508278666666667">
2 To be more precise, generative probabilistic models assign joint probabilities P(x,y) to each (x,y) pair.
Similar arguments apply to conditional history-based models, which define conditional probabilities
P(y I x) through a definition
</bodyText>
<equation confidence="0.864965">
P(y I x) = 11 P(di I F(d1... di-1, x))
i=1...n
</equation>
<bodyText confidence="0.982258666666667">
where d1... dn are again the decisions made in building a parse, and F is a function that groups histories
into equivalence classes. Note that x is added to the domain of F (the context on which decisions are
conditioned). See Ratnaparkhi (1997) for one example of a method using this approach.
</bodyText>
<page confidence="0.994216">
28
</page>
<note confidence="0.938433">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.982089888888889">
Figure 1 illustrates this process. Each of the above decisions has an associated
probability conditioned on the left-hand side of the rule (VP(saw)) and other infor-
mation in some cases.
History-based approaches lead to models in which the log-probability of a parse
tree can be written as a linear sum of parameters ak multiplied by features hk. Each
feature hk(x, y) is the count of a different “event”or fragment within the tree. As an
example, consider a PCFG with rules (Ak—&gt;Ok) for 1 &lt; k &lt; m. If hk(x,y) is the number
of times (Ak—&gt;Ok) is seen in the tree, and ak = log P(OklAk) is the parameter associated
with that rule, then
</bodyText>
<equation confidence="0.994435">
m akhk(x, y)
X
log P(x, y) =
k=1
</equation>
<bodyText confidence="0.99990575">
All models considered in this article take this form, although in the boosting models
the score for a parse is not a log-probability. The features hk define an m-dimensional
vector of counts which represent the tree. The parameters ak represent the influence of
each feature on the score of a tree.
A drawback of history-based models is that the choice of derivation has a
profound influence on the parameterization of the model. (Similar observations have
been made in the related cases of belief networks [Pearl 1988], and language models
for speech recognition [Rosenfeld 1997].) When designing a model, it would be
desirable to have a framework in which features can be easily added to the model.
Unfortunately, with history-based models adding new features often requires a
modification of the underlying derivations in the model. Modifying the derivation to
include a new feature type can be a laborious task. In an ideal situation we would be
able to encode arbitrary features hk, without having to worry about formulating a
derivation that included these features.
To take a concrete example, consider part-of-speech tagging using a hidden
Markov model (HMM). We might have the intuition that almost every sentence has at
least one verb and therefore that sequences including at least one verb should have
increased scores under the model. Encoding this constraint in a compact way in an
HMM takes some ingenuity. The obvious approach—to add to each state the
information about whether or not a verb has been generated in the history—doubles
</bodyText>
<figureCaption confidence="0.750997">
Figure 1
</figureCaption>
<bodyText confidence="0.621966">
The sequence of decisions involved in generating the right-hand side of a lexical rule.
</bodyText>
<page confidence="0.99665">
29
</page>
<note confidence="0.760506">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.481236">
the number of states (and parameters) in the model. In contrast, it would be trivial to
implement a feature hk(x,y) which is 1 if y contains a verb, 0 otherwise.
</bodyText>
<listItem confidence="0.739122">
3. Logistic Regression and Boosting
</listItem>
<bodyText confidence="0.999959111111111">
We now turn to machine-learning methods for the ranking task. In this section we
review two methods for binary classification problems: logistic regression (or
maximum-entropy) models and boosting. These methods form the basis for the
reranking approaches described in later sections of the article. Maximum-entropy
models are a very popular method within the computational linguistics community;
see, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article which
introduces the models and motivates them. Boosting approaches to classification have
received considerable attention in the machine-learning community since the intro-
duction of AdaBoost by Freund and Schapire (1997).
Boosting algorithms, and in particular the relationship between boosting
algorithms and maximum-entropy models, are perhaps not familiar topics in the
NLP literature. However there has recently been much work drawing connections
between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy
and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001;
Collins, Schapire, and Singer 2002); in this section we review this work. Much of this
work has focused on binary classification problems, and this section is also restricted
to problems of this type. Later in the article we show how several of the ideas can be
carried across to reranking problems.
</bodyText>
<subsectionHeader confidence="0.991143">
3.1 Binary Classification Problems
</subsectionHeader>
<bodyText confidence="0.981633">
The general setup for binary classification problems is as follows:
</bodyText>
<listItem confidence="0.9973596">
• The “input domain”(set of possible inputs) is X.
• The “output domain”(set of possible labels) is simply a set of two
labels, Y = {-1, +1}.3
• The training set is an array of n labeled examples,
((x1, y1), (x2, y2), . .., (xn, yn)), where each xi E X, yi E Y.
• Input examples are represented through m “features,”which are
functions hk : X—&gt; R for k = 1, ... , m. It is also sometimes convenient
to think of an example x as being represented by an m-dimensional
“feature vector”7(x) = (h1(x), h2(x), ... , hm(x)).
• Finally, there is a parameter vector, a = (a1,. . . , am),
</listItem>
<bodyText confidence="0.69289925">
where each ak E R, hence a¯ is an m-dimensional real-valued vector.
We show that both logistic regression and boosting implement a linear, or hyperplane,
classifier. This means that given an input example x and parameter values ¯a, the
output from the classifier is
</bodyText>
<equation confidence="0.425425">
sign (F(x, ¯a)) (1)
</equation>
<page confidence="0.6701505">
3 It turns out to be convenient to define Y = {-1, +1} rather than Y = {0, +1}, for example.
30
</page>
<bodyText confidence="0.8994035">
Collins and Koo Discriminative Reranking for NLP
where
</bodyText>
<equation confidence="0.980761166666667">
m
EFðx, ¯aÞ 1/4 akhkðxÞ 1/4 a¯ &apos; 7ðxÞ ð2Þ
k1/41
Here a¯ • 7ðxÞ is the inner or dot product between the vectors a¯ and 7ðxÞ, and sign(z) =
1 if z &gt; 0, sign(z) = —1 otherwise. Geometrically, the examples x are represented as
vectors (�(x) in some m-dimensional vector space, and the parameters a¯ define a
</equation>
<bodyText confidence="0.999836888888889">
hyperplane which passes through the origin4 of the space and has a¯ as its normal.
Points lying on one side of this hyperplane are classified as +1; points on the other side
are classified as —1. The central question in learning is how to set the parameters ¯a,
given the training examples bðx1, y1Þ, ðx2, y2Þ, ... ,ðxn, ynÞÀ. Logistic regression and
boosting involve different algorithms and criteria for training the parameters ¯a, but
recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and
Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins,
Schapire, and Singer 2002) has shown that the methods have strong similarities. The
next section describes parameter estimation methods.
</bodyText>
<subsectionHeader confidence="0.987444">
3.2 Loss Functions for Logistic Regression and Boosting
</subsectionHeader>
<bodyText confidence="0.999721833333333">
A central idea in both logistic regression and boosting is that of a loss function, which
drives the parameter estimation methods of the two approaches. This section describes
loss functions for binary classification. Later in the article, we introduce loss functions
for reranking tasks which are closely related to the loss functions for classification tasks.
First, consider a logistic regression model. The parameters of the model a¯ are used
to define a conditional probability
</bodyText>
<equation confidence="0.980896666666667">
eyFðx, ¯aÞ
Pðy j x, ¯aÞ 1/4 ð3Þ
1 þ eyFðx, ¯aÞ
</equation>
<bodyText confidence="0.9999438">
where Fðx, ¯aÞ is as defined in equation (2). Some form of maximum-likelihood
estimation is often used for parameter estimation. The parameters are chosen to
maximize the log-likelihood of the training set; equivalently: we talk (to emphasize the
similarities to the boosting approach) about minimizing the negative log-likelihood.
The negative log-likelihood, LogLoss(¯a), is defined as
</bodyText>
<equation confidence="0.9734725">
n n � eyiFðxi, ¯aÞ n \1 + e
ELogLoss ð¯aÞ 1/4 � Elog Pðyi j xi, ¯aÞ 1/4 � loa)) to y;F(xi, ¯aÞ1
i1/41 i1/41 i1/41
ð4Þ
</equation>
<bodyText confidence="0.9985745">
There are many methods in the literature for minimizing LogLoss(¯a) with respect to
¯a, for example, generalized or improved iterative scaling (Berger, Della Pietra, and
</bodyText>
<footnote confidence="0.9957195">
4 It might seem to be a restriction to have the hyperplane passing through the origin of the space. However
if a constant “bias”feature hmþ1ðxÞ 1/4 1 for all x is added to the representation, a hyperplane passing
through the origin in this new space is equivalent to a hyperplane in general position in the original
m-dimensional space.
</footnote>
<page confidence="0.99969">
31
</page>
<note confidence="0.301804">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.941808">
Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient
methods (Malouf 2002). In the next section we describe feature selection methods, as
described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra,
and Lafferty (1997).
Once the parameters a¯ are estimated on training examples, the output for an
example x is the most likely label under the model,
</bodyText>
<equation confidence="0.713874">
arg max P(y l x, ¯a) = arg max yF(x, ¯a) = sign(F(x, ¯a)) (5)
yZY yZ{—1,+1}
</equation>
<bodyText confidence="0.8751545">
where as before, sign (z) = 1 if z &gt; 0, sign (z) = —1 otherwise. Thus we see that the
logistic regression model implements a hyperplane classifier.
In boosting, a different loss function is used, namely, ExpLoss(¯a), which is defined
as
</bodyText>
<equation confidence="0.996469333333333">
n
EExpLoss(¯a) = e—yiF(xi,¯a) (6)
i=1
</equation>
<bodyText confidence="0.9997134">
This loss function is minimized using a feature selection method, which we describe in
the next section.
There are strong similarities between LogLoss (equation (4)) and ExpLoss
(equation (6)). In making connections between the two functions, it is useful to
consider a third function of the parameters and training examples,
</bodyText>
<equation confidence="0.985178666666667">
n
EError(¯a) = gyiF(xi, ¯a) &lt; 0Ä (7)
i=1
</equation>
<bodyText confidence="0.9994985">
where gpÄ is one if p is true, zero otherwise. Error(¯a) is the number of incorrectly
classified training examples under parameter values ¯a.
Finally, it will be useful to define the margin on the ith training example, given
parameter values ¯a, as
</bodyText>
<equation confidence="0.990967090909091">
Mi(¯a) = yiF(xi, ¯a) (8)
With these definitions, the three loss functions can be written in the following form:
n
ELogLoss(¯a) = f (Mi(¯a)), where f (z) = log(1 + e—z)
i=1
n
EExpLoss(¯a) = f (Mi(¯a)), where f (z) = e—z
i=1
n
EError(¯a) = f (Mi(¯a)), where f (z) = gz &lt; 0Ä
i=1
</equation>
<bodyText confidence="0.9840135">
The three loss functions differ only in their choice of an underlying “potential
function”of the margins, f(z). This function is f(z) = log (1 + e—z), f(z) = e—z, or
</bodyText>
<page confidence="0.996899">
32
</page>
<note confidence="0.63637">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.998140052631579">
f (z) = Qz &lt; 01 for LogLoss, ExpLoss, and Error, respectively. The f(z) functions penalize
nonpositive margins on training examples. The simplest function, f (z) = Qz &lt; 01, gives
a cost of one if a margin is negative (an error is made), zero otherwise. ExpLoss and
LogLoss involve definitions for f(z) which quickly tend to zero as z Y oo but heavily
penalize increasingly negative margins.
Figure 2 shows plots for the three definitions of f (z). The functions f (z) = e—z and
f (z) = log (1 + e—z) are both upper bounds on the error function, so that mini-
mizing either LogLoss or ExpLoss can be seen as minimizing an upper bound on
the number of training errors. (Note that minimizing Error(¯�) itself is known to be
at least NP-hard if no parameter settings can achieve zero errors on the training
set; see, for example, Hoffgen, van Horn, and Simon [1995].) As z Y oo, the func-
tions f (z) = e—z and f (z) = log(1 + e—z) become increasingly similar, because log
(1 + e—z) Y e—z as e—z Y 0. For negative z, the two functions behave quite differently.
f (z) = e—z shows an exponentially growing cost function as z Y — oo. In contrast, as
z Y —oo it can be seen that log(1 + e—z) Y log(e—z) = —z, so this function shows
asymptotically linear growth for negative z. As a final remark, note that both f (z) =
e—z and f (z) = log(1 + e—z) are convex in z, with the result that LogLoss(¯�) and
ExpLoss(¯�) are convex in the parameters ¯�. This means that there are no problems
with local minima when optimizing these two loss functions.
</bodyText>
<subsectionHeader confidence="0.995428">
3.3 Feature Selection Methods
</subsectionHeader>
<bodyText confidence="0.999189">
In this article we concentrate on feature selection methods: algorithms which aim to
make progress in minimizing the loss functions LogLoss(¯�) and ExpLoss(¯�) while
using a small number of features (equivalently, ensuring that most parameter values in
</bodyText>
<figureCaption confidence="0.865477">
Figure 2
</figureCaption>
<bodyText confidence="0.993337">
Potential functions underlying ExpLoss, LogLoss, and Error. The graph labeled ExpLoss is a plot
of f (z) = e—z for z = [—1.5...1.5]; LogLoss shows a similar plot for f (z) = log(1 + e—z); Error is a
plot of f (z) = Qz &lt; 01.
</bodyText>
<page confidence="0.996571">
33
</page>
<note confidence="0.277921">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.913941111111111">
a¯ are zero). Roughly speaking, the motivation for using a small number of features is
the hope that this will prevent overfitting in the models.
Feature selection methods have been proposed in the maximum-entropy literature
by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and
Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and
Ward 1997, 1998; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004). The
most basic approach—for example see Ratnaparkhi, Roukos, and Ward (1994) and
Berger, Della Pietra, and Della Pietra (1996)—involves selection of a single feature at
each iteration, followed by an update to the entire model, as follows:
</bodyText>
<construct confidence="0.525955846153846">
Step 1: Throughout the algorithm, maintain a set of active features. Initialize this set to
be empty.
Step 2: Choose a feature from outside of the set of active features which has the largest
estimated impact in terms of reducing the loss function LogLoss, and add this to the
active feature set.
Step 3: Minimize LogLoss(¯a) with respect to the set of active features; that is, allow
only the active features to take nonzero parameter values when minimizing LogLoss.
Return to Step 2.
Methods in the boosting literature (see, for example, Schapire and Singer [1999]) can
be considered to be feature selection methods of the following form:
Step 1: Start with all parameter values set to zero.
Step 2: Choose a feature which has largest estimated impact in terms of reducing the
loss function ExpLoss.
</construct>
<bodyText confidence="0.964534647058824">
Step 3: Update the parameter for the feature chosen at Step 2 in such a way as to
minimize ExpLoss(¯a) with respect to this one parameter. All other parameter values
are left fixed. Return to Step 2.
The difference with this latter “boosting”approach is that in Step 3, only one
parameter value is adjusted, namely, the parameter corresponding to the newly chosen
feature. Note that in this framework, the same feature may be chosen at more than one
iteration.5 The maximum-entropy feature selection method can be quite inefficient,
as the entire model is updated at each step. For example, Ratnaparkhi (1998) quotes
times of around 30 hours for 500 rounds of feature selection on a prepositional-
phrase attachment task. These experiments were performed in 1998, when pro-
cessors were no doubt considerably slower than those available today. However,
the PP attachment task is much smaller than the parsing task that we are address-
ing: Our task involves around 1,000,000 examples, with perhaps a few hundred features
per example, and 100,000 rounds of feature selection; this compares to 20,000 exam-
ples, 16 features per example, and 500 rounds of feature selection for the PP attach-
ment task in Ratnaparkhi (1998). As an estimate, assuming that computational
complexity scales linearly in these factors,6 our task is 1,000,000
</bodyText>
<figure confidence="0.512097">
20,000 x 320
16 x 100,000
500 = 200,000
</figure>
<footnote confidence="0.9981406">
5 That is, the feature may be repeatedly updated, although the same feature will never be chosen
in consecutive iterations, because after an update the model is minimized with respect to the
selected feature.
6 We believe this is a realistic assumption, as each round of feature selection takes O(nf) time, where n
is the number of training examples, and f is the number of active features on each example.
</footnote>
<page confidence="0.998242">
34
</page>
<note confidence="0.637917">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.999974846153846">
as large as the PP attachment task. These figures suggest that the maximum-entropy
feature selection approach may be infeasible for large-scale tasks such as the one in this
article.
The fact that the boosting approach does not update the entire model at each
round of feature selection may be a disadvantage in terms of the number of features or
the test data accuracy of the final model. There is reason for concern that Step 2 will at
some iterations mistakenly choose features which are apparently useful in reducing
the loss function, but which would have little utility if the entire model had been
optimized at the previous iteration of Step 3. However, previous empirical results for
boosting have shown that it is a highly effective learning method, suggesting that this
is not in fact a problem for the approach. Given the previous strong results for the
boosting approach, and for reasons of computational efficiency, we pursue the
boosting approach to feature selection in this article.
</bodyText>
<subsectionHeader confidence="0.759287">
3.4 Statistical Justification for the Methods
</subsectionHeader>
<bodyText confidence="0.998629696969697">
Minimization of LogLoss is most often justified as a parametric, maximum-likelihood
(ML) approach to estimation. Thus this approach benefits from the usual guarantees
for ML estimation: If the distribution generating examples is within the class of
distributions specified by the log-linear form, then in the limit as the sample size goes
to infinity, the model will be optimal in the sense of convergence to the true underlying
distribution generating examples. As far as we are aware, behavior of the models for
finite sample sizes is less well understood. In particular, while feature selection
methods have often been proposed for maximum-entropy models, little theoretical
justification (in terms of guarantees about generalization) has been given for them. It
seems intuitive that a model with a smaller number of parameters will require fewer
samples for convergence, but this is not necessarily the case, and at present this
intuition lacks a theoretical basis. Feature selection methods can probably be
motivated either from a Bayesian perspective (through a prior favoring models with
a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit
perspective (models with fewer parameters are less likely to fit the data by chance), but
this requires additional research.
The statistical justification for boosting approaches is quite different. Boosting
algorithms were originally developed within the PAC framework (Valiant 1984) for
machine learning, specifically to address questions regarding the equivalence of weak
and strong learning. Freund and Schapire (1997) originally introduced AdaBoost and
gave a first set of statistical guarantees for the algorithm. Schapire et al. (1998) gave a
second set of guarantees based on the analysis of margins on training examples. Both
papers assume that a fixed distribution D(x, y) is generating both training and test
examples and that the goal is to find a hypothesis with a small number of expected
errors with respect to this distribution. The form of the distribution is not assumed to
be known, and in this sense the guarantees are nonparametric, or “distribution free.”Freund and Schapire (1997) show that if the weak learning assumption holds (i.e.,
roughly speaking, a feature with error rate better than chance can be found for any
distribution over the sample space X x {-1, +1}), then the training error for the
ExpLoss method decreases rapidly enough for there to be good generalization to test
examples. Schapire et al. (1998) show that under the same assumption, minimization of
ExpLoss using the feature selection method ensures that the distribution of margins on
training data develops in such a way that good generalization performance on test
examples is guaranteed.
</bodyText>
<page confidence="0.996457">
35
</page>
<figure confidence="0.339978">
Computational Linguistics Volume 31, Number 1
</figure>
<subsectionHeader confidence="0.962001">
3.5 Boosting with Complex Feature Spaces
</subsectionHeader>
<bodyText confidence="0.995978456521739">
Thus far in this article we have presented boosting as a feature selection approach. In
this section, we note that there is an alternative view of boosting in which it is
described as a method for combining multiple models, for example, as a method for
forming a linear combination of decision trees. We consider only the simpler, feature
selection view of boosting in this article. This section is included for completeness and
because the more general view of boosting may be relevant to future work on boosting
approaches for parse reranking (note, however, that the discussion in this section is not
essential to the rest of the article, so the reader may safely skip this section if she or he
wishes to do so).
In feature selection approaches, as described in this article, the set of possible
features hkðxÞ for k = 1, ... , m is taken to be a fixed set of relatively simple functions. In
particular, we have assumed that m is relatively small (for example, small enough for
algorithms that require O(m) time or space to be feasible). More generally, however,
boosting can be applied in more complex settings. For example, a common use of
boosting is to form a linear combination of decision trees. In this case each example x is
represented as a number of attribute-value pairs, and each “feature”hk(x) is a complete
decision tree built on predicates over the attribute values in x. In this case the number
of features m is huge: There are as many features as there are decision trees over the
given set of attributes, thus m grows exponentially quickly with the number of
attributes that are used to represent an example x. Boosting may even be applied in
situations in which the number of features is infinite. For example, it may be used to
form a linear combination of neural networks. In this case each feature hk(x)
corresponds to a different parameter setting within the (infinite) set of possible
parameter settings for the neural network.
In more complex settings such as boosting of decision trees or neural networks, it
is generally not feasible to perform an exhaustive search (with O(m) time complexity)
for the feature which has the greatest impact on the exponential7 loss function. Instead,
an approximate search is performed. In boosting approaches, this approximate search
is achieved through a protocol in which at each round of boosting, a “distribution”over the training examples is maintained. The distribution can be interpreted as
assigning an importance weight to each training example, most importantly giving
higher weight to examples which are incorrectly classified. At each round of boosting
the distribution is passed to an algorithm such as a decision tree or neural network
learning method, which attempts to return a feature (a decision tree, or a neural
network parameter setting) which has a relatively low error rate with respect to the
distribution. The feature that is returned is then incorporated into the linear
combination of features. The algorithm which generates a classifier given a
distribution over the examples (for example, the decision tree induction method) is
usually referred to as “the weak learner.”The weak learner generally uses an
approximate (for example, greedy) method to find a function with a low error rate
with respect to the distribution. Freund and Schapire (1997) show that provided that at
each round of boosting the weak learner returns a feature with greater than (50 + e) %
accuracy for some fixed e, the number of training errors falls exponentially quickly
with the number of rounds of boosting. This fast drop in training errors translates to
statistical bounds on generalization performance (Freund and Schapire 1997).
7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman
et al. (2000) and Duffy and Helmbold (1999).
</bodyText>
<page confidence="0.960803">
36
</page>
<note confidence="0.533899">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.999935">
Under this view of boosting, the feature selection methods in this article are a
particularly simple case in which the weak learner can afford to exhaustively search
through the space of possible features. Future work on reranking approaches might
consider other approaches—such as boosting of decision trees—which can effectively
consider more complex features.
</bodyText>
<sectionHeader confidence="0.874987" genericHeader="method">
4. Reranking Approaches
</sectionHeader>
<bodyText confidence="0.9999794">
This section describes how the ideas from classification problems can be extended to
reranking tasks. A baseline statistical parser is used to generate N-best output both for
its training set and for test data sentences. Each candidate parse for a sentence is
represented as a feature vector which includes the log-likelihood under the baseline
model, as well as a large number of additional features. The additional features can in
principle be any predicates over sentence/tree pairs. Evidence from the initial log-
likelihood and the additional features is combined using a linear model. Parameter
estimation becomes a problem of learning how to combine these different sources of
information. The boosting algorithm we use is related to the generalization of boosting
methods to ranking problems in Freund et al. (1998); we also introduce an approach
related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994),
Papineni, Roukos, and Ward (1997, 1998), Johnson et al. (1999), Riezler et al. (2002), and
Och and Ney (2002).
Section 4.1 gives a formal definition of the reranking problem. Section 4.2
introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss
functions in section 3.2. Section 4.3 describes a general approach to feature selection
methods with these loss functions. Section 4.4 describes a first algorithm for the
exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm
for the case of ExpLoss. Finally, section 4.6 describes issues in feature selection
algorithms for the LogLoss function.
</bodyText>
<subsectionHeader confidence="0.969568">
4.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.997698">
We use the following notation in the rest of this article:
</bodyText>
<listItem confidence="0.90034325">
• si is the ith sentence in the training set. There are n sentences in training
data, so that 1 &lt; i &lt; n.
• xi,j is the jth parse of the ith sentence. There are ni parses for the ith
sentence, so that 1 &lt; i &lt; n and 1 &lt; j &lt; ni. Each xi,j contains both the
</listItem>
<bodyText confidence="0.93245475">
tree and the underlying sentence (i.e., each xi,j is a pair bsi, ti,jÀ,
where si is the ith sentence in the training data, and ti,j is the jth tree
for this sentence). We assume that the parses are distinct, that is, that
xi,j 0 xi,j¶ for j 0 j¶.
</bodyText>
<listItem confidence="0.916224833333333">
• Score(xi,j) is the “score”for parse xi,j, a measure of the similarity of
xi,j to the gold-standard parse. For example, Score(xi,j) might be the
F-measure accuracy of parse xi,j compared to the gold-standard parse
for si.
• Q(xi,j) is the probability that the base parsing model assigns to parse xi,j.
L(xi,j) = log Q(xi,j) is the log-probability.
</listItem>
<page confidence="0.95312">
37
</page>
<figure confidence="0.299016">
Computational Linguistics Volume 31, Number 1
</figure>
<listItem confidence="0.9955564">
• Without loss of generality, we assume xi,1 to be the highest-scoring
parse for the ith sentence.8 More precisely, for all i, 2 &lt; j &lt; ni,
Score(xi,1) &gt; Score(xi, j). Note that xi,1 may not be identical to the
gold-standard parse; in some cases the parser may fail to propose
the correct parse anywhere in its list of candidates.9
</listItem>
<bodyText confidence="0.996759">
Thus our training data consist of a set of parses, {xi,j : i = 1, ... , n, j = 1, ... , niJ,
together with scores Score(xi,j) and log-probabilities L(xi,j).
We represent candidate parse trees through m features, hk for k = 1,. . . , m. Each hk
is an indicator function, for example,
</bodyText>
<equation confidence="0.977648">
hk(x) = 1 if x contains the rule bS —&gt; NP VPÀ
0 otherwise
</equation>
<bodyText confidence="0.999818">
We show that the restriction to binary-valued features is important for the simplicity
and efficiency of the algorithms.10 We also assume a vector of m + 1 parameters, a¯ =
{a0, a1, ... , am}. Each ai can take any value in the reals. The ranking function for a
parse tree x implied by a parameter vector a¯ is defined as
</bodyText>
<equation confidence="0.993680333333333">
m
F(x, ¯a) = a0L(x) + X akhk(x)
k=1
</equation>
<bodyText confidence="0.928524333333333">
Given a new test sentence s, with parses xj for j = 1, ... , N, the output of the model is
the highest-scoring tree under the ranking function
arg max
</bodyText>
<subsubsectionHeader confidence="0.434521">
xZjx1 ... xNJ
</subsubsectionHeader>
<bodyText confidence="0.7694293">
Thus F(x, ¯a) can be interpreted as a measure of how plausible a parse x is, with higher
scores meaning that x is more plausible. Competing parses for the same sentence are
ranked in order of plausibility by this function. We can recover the base ranking
function—the log-likelihood L(x)—by setting a0 to a positive constant and setting all
other parameter values to be zero. Our intention is to use the training examples to pick
parameter values which improve upon this initial ranking.
We now discuss how to set these parameters. First we discuss loss functions
Loss(¯a) which can be used to drive the training process. We then go on to describe
feature selection methods for the different loss functions.
8 In the event that multiple parses get the (same) highest score, the parse with the highest value of log-
likelihood L under the baseline model is taken as xi,1. In the event that two parses have the same score
and the same log-likelihood—which occurred rarely if ever in our experiments—we make a random
choice between the two parses.
9 This is not necessarily a significant issue if an application using the output from the parser is sensitive to
improvements in evaluation measures such as precision and recall that give credit for partial matches
between the parser’s output and the correct parse. In this case, it is important only that the precision/
recall for xi,1 is significantly higher than that of the baseline parser, that is, that there is some “head room”for the reranking module in terms of precision and recall.
10 In particular, this restriction allows closed-form parameter updates for the models based on ExpLoss that
we consider. Note that features tracking the counts of different rules can be simulated through several
features which take value one if a rule is seen &gt; 1 time, &gt; 2 times &gt; 3 times, and so on.
</bodyText>
<equation confidence="0.495603">
F(x, ¯a)
</equation>
<page confidence="0.974821">
38
</page>
<note confidence="0.702385">
Collins and Koo Discriminative Reranking for NLP
</note>
<subsectionHeader confidence="0.911348">
4.2 Loss Functions for Ranking Problems
</subsectionHeader>
<bodyText confidence="0.8424635">
4.2.1 Ranking Errors and Margins. The loss functions we consider are all related to
the number of ranking errors a function F makes on the training set. The ranking error
rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best
parse:
</bodyText>
<equation confidence="0.886685666666667">
ni ni
XError (¯a) = X XgF(xi,1, ¯a) &lt; F(xi,j, ¯a)Ä = X gF(xi,1, ¯a) — F(xi,j, ¯a) &lt; 0Ä
i j=2 i j=2
</equation>
<bodyText confidence="0.999752">
where again, gpÄ is one if p is true, zero otherwise. In the ranking problem we define the
margin for each example xi,j such that i = 1,. . . , n, j = 2,. . . , ni, as
</bodyText>
<equation confidence="0.999607">
Mij(¯a) = F(xi,1, ¯a) — F(xi,j, ¯a)
</equation>
<bodyText confidence="0.8725675">
Thus Mij(¯a) is the difference in ranking score between the correct parse of a sentence
and a competing parse xi,j. It follows that
</bodyText>
<equation confidence="0.955030333333333">
ni
XError(¯a) = X gMij(¯a) &lt; 0Ä
i j=2
</equation>
<bodyText confidence="0.971103666666667">
The ranking error is zero if all margins are positive. The loss functions we discuss all
turn out to be direct functions of the margins on training examples.
4.2.2 Log-Likelihood. The first loss function is that suggested by Markov random
fields. As suggested by Ratnaparkhi, Roukos, and Ward (1994) and Johnson et al.
(1999), the conditional probability of xi,q being the correct parse for the ith sentence is
defined as
</bodyText>
<equation confidence="0.98108975">
eF(xi,q, ¯a)
P(xi,q I si, ¯a) =
ni
P
j=1
Given a new test sentence s, with parses xj for j = 1,. . . , N, the most likely tree is
eF(xi,j, ¯a)
arg max
xj
eF(xq,¯a)
= arg max F(xj, ¯a)
xj
eF(xj,¯a)
N
P
q=1
</equation>
<bodyText confidence="0.983320666666667">
Hence once the parameters are trained, the ranking function is used to order candidate
trees for test examples.
The log-likelihood of the training data is
</bodyText>
<equation confidence="0.519481333333333">
X Xlog P(xi,1 I si, ¯a) = eF(xi,1, ¯a)
i i log ni (xi,j, ¯a)
Pj=1 eF
</equation>
<bodyText confidence="0.6250425">
Under maximum-likelihood estimation, the parameters a¯ would be set to maxi-
mize the log-likelihood. Equivalently, we again talk about minimizing the negative
</bodyText>
<page confidence="0.991417">
39
</page>
<note confidence="0.275156">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.8929095">
log-likelihood. Some manipulation shows that the negative log-likelihood is a function
of the margins on training data:
</bodyText>
<equation confidence="0.992131">
eFðxi,1, ¯aÞ—log ni eFðxi,j, ¯aÞ �j
— —log
�j1/41 ini e ðFðxi,1, aÞ—Fðxi,j, aÞÞ
nini
logð1 þXe ðFðxi,1,¯aÞ—Fðxi,j, aÞÞ 1/4Xlogð1 þ X emi,jð¯aÞÞ ð9Þ
j1/42
j1/42
i
XLogLoss ð¯aÞ 1/4
i
X1/4
i
</equation>
<bodyText confidence="0.9967355">
Note the similarity of equation (9) to the LogLoss function for classification in equa-
tion (4).
</bodyText>
<subsubsectionHeader confidence="0.877704">
4.2.3 Exponential Loss. The next loss function is based on the boosting method
</subsubsectionHeader>
<bodyText confidence="0.9974645">
described in Schapire and Singer (1999). It is a special case of the general ranking
methods described in Freund et al. (1998), with the ranking “feedback”being a simple
binary distinction between the highest-scoring parse and the other parses. Again, the
loss function is a function of the margins on training data:
</bodyText>
<equation confidence="0.910826">
ni ni
XExpLossð¯aÞ 1/4 X Xe�ðFðxi,1, ¯aÞ�Fðxi,j, ¯aÞÞ 1/4 X eMi,jð¯aÞ ð10Þ
ij1/42 i j1/42
</equation>
<bodyText confidence="0.99997475">
Note the similarity of equation (10) to the ExpLoss function for classification
in equation (6). It can be shown that ExpLossð¯aÞ &gt; Errorð¯aÞ, so that minimizing
ExpLossð¯aÞ is closely related to minimizing the number of ranking errors.11 This
follows from the fact that for any x, e�x &gt; gx &lt; 01, and therefore that
</bodyText>
<equation confidence="0.999364666666667">
X ni Xe�Mi,jð¯aÞ � ni gMi,jð¯aÞ &lt; 01
i X i X
j1/42 j1/42
</equation>
<bodyText confidence="0.999989333333333">
We generalize the ExpLoss function slightly, by allowing a weight for each example
xi,j, for i = 1, ... , n, j = 2, ... , ni. We use Si,j to refer to this weight. In particular, in some
experiments in this article, we use the following definition:
</bodyText>
<equation confidence="0.725014">
Si,j 1/4 Scoreðxi,1Þ — Scoreðxi,jÞ ð11Þ
</equation>
<footnote confidence="0.93028">
11 Note that LogLoss is not a direct upper bound on the number of ranking errors, although it can be shown
that it is a (relatively loose) upper bound on the number of times the correct parse is not the highest-
ranked parse on the model. The latter observation follows from the property that the correct parse must
be highest ranked if its probability is greater than 0.5.
</footnote>
<page confidence="0.993768">
40
</page>
<note confidence="0.552808">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.999995">
where, as defined in section 4.1, Score(xi,j) is some measure of the “goodness”of a
parse, such as the F-measure (see section 5 for the exact definition of Score used in our
experiments). The definition for ExpLoss is modified to be
</bodyText>
<equation confidence="0.946050666666667">
ni
EExpLoss(¯a) = Si,je�Mi,j(¯a)
i j=2
</equation>
<bodyText confidence="0.986252">
This definition now takes into account the importance, Si,j, of each example. It is an
upper bound on the following quantity:
</bodyText>
<equation confidence="0.9212165">
ni
Si,jgMi,j(¯a) G 0Ä
</equation>
<bodyText confidence="0.999773333333333">
which is the number of errors weighted by the factors Si,j. The original definition of
ExpLoss in equation (10) can be recovered by setting Si,j = 1 for all i, j (i.e., by giving
equal weight to all examples). In our experiments we found that a definition of Si,j such
as that in equation (11) gave improved performance on development data, presumably
because it takes into account the relative cost of different ranking errors in training-
data examples.
</bodyText>
<subsectionHeader confidence="0.557871">
4.3 A General Approach to Feature Selection
</subsectionHeader>
<bodyText confidence="0.999989333333334">
At this point we have definitions for ExpLoss and LogLoss which are analogous to the
definitions in section 3.2 for binary classification tasks. Section 3.3 introduced the idea
of feature selection methods; the current section gives a more concrete description of
the methods used in our experiments.
The goal of feature selection methods is to find a small subset of the features that
contribute most to reducing the loss function. The methods we consider are greedy, at
each iteration picking the feature hk with additive weight d which has the most impact
on the loss function. In general, a separate set of instances is used in cross-validation to
choose the stopping point, that is, to decide on the number of features in the model.
At this point we introduce some notation concerning feature selection methods.
We define Upd(¯a,k,d) to be an updated parameter vector, with the same parameter
values as a¯ with the exception of ak, which is incremented by d:
</bodyText>
<equation confidence="0.998306">
Upd(¯a,k,d) = {a0,a1,...,ak + d,...,am}
</equation>
<bodyText confidence="0.999664">
The d parameter can potentially take any value in the reals. The loss for the updated
model is Loss(Upd(¯a, k,d)). Assuming we greedily pick a single feature with some
weight to update the model, and given that the current parameter settings are ¯a, the
optimal feature/weight pair (k*, d*) is
</bodyText>
<equation confidence="0.8472896">
(k*, d*) = arg min Loss (Upd(¯a, k, d))
k, d
j=2
E
i
</equation>
<page confidence="0.985997">
41
</page>
<table confidence="0.743232">
Computational Linguistics Volume 31, Number 1
The feature selection algorithms we consider take the following form (¯at is the
parameter vector at the tth iteration):
Step 1: Initialize ¯a0 to some value. (This will generally involve values of zero for
a1, ... , am and a nonzero value for a0, for example, ¯a0 = {1, 0, 0, ...}.)
Step 2: For t = 1 to N (the number of iterations N will be chosen by cross-
validation)
a: Find (k*, d*) = arg mink,d Loss(Upd (¯at-1, k, d))
b: Set ¯at = Upd(¯at-1,k*,d*)
</table>
<bodyText confidence="0.928472222222222">
Note that this is essentially the idea behind the “boosting”approach to feature se-
lection introduced in section 3.3. In contrast, the feature selection method of Berger,
Della Pietra, and Della Pietra (1996), also described in section 3.3, would involve
updating parameter values for all selected features at step 2b.
The main computation for both loss functions involves searching for the optimal
feature/weight pair (k*, d*). In both cases we take a two-step approach to solving this
problem. In the first step the optimal update for each feature hk is calculated. We
define BestWt(k, ¯a) as the optimal update for the kth feature (it must be calculated for
all features k = 1, ... , m):
</bodyText>
<equation confidence="0.607461">
BestWt(k, ¯a) = arg min dLoss(Upd(¯a, k,d))
</equation>
<bodyText confidence="0.835923">
The next step is to calculate the Loss for each feature with its optimal update, which
we will call
BestLoss(k, ¯a) = min Loss(Upd(¯a, k, d)) = Loss(Upd(¯a, k, BestWt(k, ¯a)))
d
BestWt and BestLoss for each feature having been computed, the optimal feature/
weight pair can be found:
k* =arg min kBestLoss(k, ¯a), d* = BestWt(k*, ¯a)
The next sections describe how BestWt and BestLoss can be computed for the two loss
functions.
</bodyText>
<subsectionHeader confidence="0.958313">
4.4 Feature Selection for ExpLoss
</subsectionHeader>
<bodyText confidence="0.968645">
At the first iteration, a0 is set to optimize ExpLoss (recall that L(xi,j) is the log-
likelihood for parse xi,j under the base parsing model):
</bodyText>
<figure confidence="0.944281333333333">
ni
X Si,je-(a[L(xi,1)-L(xi,j)]) (12)
a0 = arg min
a
i
j=2
</figure>
<footnote confidence="0.6439475">
In initial experiments we found that this step was crucial to the performance of the
method (as opposed to simply setting a0 = 1, for example). It ensures that the
</footnote>
<page confidence="0.994488">
42
</page>
<note confidence="0.794167">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.9999414">
contribution of the log-likelihood feature is well-calibrated with respect to the
exponential loss function. In our implementation a0 was optimized using simple brute-
force search. All values of a0 between 0.001 and 10 at increments of 0.001 were tested,
and the value which minimized the function in equation (12) was chosen.12
Feature selection then proceeds to search for values of the remaining param-
eters, a1, ... , am. (Note that it might be preferable to also allow a0 to be adjusted as
features are added; we leave this to future work.) This requires calculation of the
terms BestWt(k, ¯a) and BestLoss(k, ¯a) for each feature. For binary-valued features
these values have closed-form solutions, which is computationally very convenient.
We now describe the form of these updates. See appendix A for how the updates
can be derived (the derivation is essentially the same as that in Schapire and Singer
[1999]).
First, we note that for any feature, [hk(xi,1) — hk(xi,j)] can take on three values: +1,
—1, or 0 (this follows from our assumption of binary-valued feature values). For each
k we define the following sets:
</bodyText>
<equation confidence="0.9836345">
Ak = {(i,j) : [hk(xi,1) — hk(xi,j)] = 11
A—k = {(i,j) : [hk(xi,1) — hk(xi,j)] = —11
</equation>
<bodyText confidence="0.962525">
Thus A+k is the set of training examples in which the kth feature is seen in the correct
parse but not in the competing parse; A—k is the set in which the kth feature is seen in
the incorrect but not the correct parse.
Based on these definitions, we next define Wk and W—k as follows:
</bodyText>
<equation confidence="0.963603285714286">
XW� k = Si,je—Mi,j(¯a) (13)
(i,j)ZAk
XW—k = si,je—Mi,j(¯a) (14)
(i,j)ZA—k
Given these definitions, it can be shown (see appendix A) that
BestWt(k, ¯a) = 1 log Wk (15)
Wk
</equation>
<bodyText confidence="0.567971">
and
BestLoss(k, ¯a) = Z — (rWk+ Wk ~2 (16)
</bodyText>
<footnote confidence="0.583487">
12 A more precise approach, for example, binary search, could also be used to solve this optimization
problem. We used the methods that searches through a set of fixed values for simplicity, implicitly
assuming that a precision of 0.001 was sufficient for our problem.
</footnote>
<page confidence="0.998429">
43
</page>
<note confidence="0.480991">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.994191">
where Z 1/4 Ei Enij1/42 SId
- e Mi,jð6Þ 1/4 ExpLossðaÞ is a constant (for fixed a) which appears
in the BestLoss for all features and therefore does not affect their ranking.
As Schapire and Singer (1999) point out, the updates in equation (15) can be
problematic, as they are undefined (infinite) when either Wþk or Wk is zero. Following
Schapire and Singer (1999), we introduce smoothing through a parameter E and the
following new definition of BestWt:
BestWtðk, aÞ 1/4 1 log Wþk þ EZ ð17Þ
2 Wk þ EZ
The smoothing parameter E is chosen through optimization on a development set.
See Figure 3 for a direct implementation of the feature selection method for
ExpLoss. We use an array of values
</bodyText>
<equation confidence="0.9319075">
Gk 1/4q jWþ ~W~
k k
</equation>
<bodyText confidence="0.990725444444444">
to indicate the gain of each feature (i.e., the impact that choosing this feature will have
on the ExpLoss function). The features are ranked by this quantity. It can be seen that
almost all of the computation involves the calculation of Z and Wþk and Wk for each
feature hk. Once these values have been computed, the optimal feature and its update
can be chosen.
4.5 A New, More Efficient Algorithm for ExpLoss
This section presents a new algorithm which is equivalent to the ExpLoss algo-
rithm in Figure 3, but can be vastly more efficient for problems with sparse fea-
ture spaces. In the experimental section of this article we show that it is almost
2,700 times more efficient for our task than the algorithm in Figure 3. The efficiency
of the different algorithms is important in the parsing problem. The training data we
eventually used contained around 36,000 sentences, with an average of 27 parses per
sentence, giving around 1,000,000 parse trees in total. There were over 500,000 dif-
ferent features.
The new algorithm is also applicable, with minor modifications, to boosting
approaches for classification problems in which the representation also involves sparse
binary features (for example, the text classification problems in Schapire and Singer
[2000]). As far as we are aware, the new algorithm has not appeared elsewhere in the
boosting literature.
Figure 4 shows the improved boosting algorithm. Inspection of the algorithm in
Figure 3 shows that only margins on examples in the sets Aþk~ and A- are modified
when a feature k* is selected. The feature space in many NLP problems is very sparse
(most features only appear on relatively few training examples, or equivalently, most
training examples will have only a few nonzero features). It follows that in many cases,
the sets Aþk~ and A- will be much smaller than the overall size of the training set.
Therefore when updating the model from a to Updð¯�, k*, S*Þ, the values Wþk and Wk
remain unchanged for many features and do not need to be recalculated. In fact, only
</bodyText>
<page confidence="0.99709">
44
</page>
<note confidence="0.877136">
Collins and Koo Discriminative Reranking for NLP
</note>
<figureCaption confidence="0.871602">
Figure 3
</figureCaption>
<bodyText confidence="0.9802425">
A naive algorithm for the boosting loss function.
features which co-occur with k* on some example must be updated. The algorithm in
Figure 4 recalculates the values of A k and A—k only for those features which co-occur
with the selected feature k*.
To achieve this, the algorithm relies on a second pair of indices. For all i, 2 &lt; j &lt; ni,
we define
</bodyText>
<equation confidence="0.9882225">
B i,j= {k : [hk(xi,1) — hk(xi,j)] = 1 1
B—i,j = {k : [hk(xi,1) — hk(xi,j)] = —11 (18)
</equation>
<page confidence="0.993967">
45
</page>
<figure confidence="0.797013">
Computational Linguistics Volume 31, Number 1
Input
</figure>
<listItem confidence="0.982208742857143">
• Examples xi for i = 1 , a, = , drawn from some set X.
• Weights Si,) representing importance of examples.
• Initial model log-likelihoods L(xi,j), for all examples ai.j.
• Feature functions h0: X {0, 1} for k =1.....on.
• Smoothing parameter e. (usually chosen by optimization on development
data).
• Number of rounds N (usually chosen by optimization on development
data).
Initialize
• Set ao = argmin„ 2 Si,jei,d-L(xi,i)].
• Set ak = 0 for k =1 ,
• For all 1, 2 &lt;j &lt; n. set margins = ao [L(xi,i) - L(x,i,j)].
• For all k =1 m.
• Set A = [hk(Xi,l) h5(X1,1)1 =11.
• Set A = (i, j) [hk(xi,i) - h5 (x1)] =
• For all i, 2 &lt;j &lt; 01,
• Set B,TF.i = {k : [hk(xi,i)- hk(xi = 11.
• Set -}k : [ht(xi ) - hk (xi .4)1 =
• Calculate Z and 117,;, WC, Gle - fork = 1 , m using
the algorithm in Figure 3.
Repeat for t = 1.....N
, +eZ
• Choose k* = argmaxk C5 and 6* - log
• For (i, j) E A.
• Set A = (cEA&apos; -6* - set /kJ = + 6*, and set
Z = Z A.
• For kG B;P1, 14/;,P = Wk7F + A.
• For k E B1, W = 1477 + A.
• For (i, j) E A.
• Set A = (cE°&amp;quot; ,+6* - e-A4&apos; , set M0 = M, 6*, and set
Z = Z
• For k = 14E„-F A.
• For k E Wk7 = Wk + A
• For features k whose values of IicP and/or 147„7 have changed, update Gle.
• at = k*. 1*)
</listItem>
<subsectionHeader confidence="0.367775">
Output Final parameter setting dtiv.
</subsectionHeader>
<bodyText confidence="0.524829">
Figure 4
An improved algorithm for the boosting loss function.
</bodyText>
<page confidence="0.996888">
46
</page>
<note confidence="0.862622">
Collins and Koo Discriminative Reranking for NLP
</note>
<figureCaption confidence="0.609538333333333">
So Bi,j+ and Bi,j~ are indices from training examples to features. With the algorithm in
Figure 4, updating the values of Wþk and Wk for the features which co-occur with k*
involves the following number of steps:
</figureCaption>
<equation confidence="0.8781425">
XC 1/4 ðjBþ i,jj þ jB-jjÞ þ X ðjBþ i,jj þ jB�i,jjÞ ð19Þ
ði,jÞZAþk� ði,jÞZAS,
</equation>
<bodyText confidence="0.9976225">
In contrast, the naive algorithm requires a pass over the entire training set, which
requires the following number of steps:
</bodyText>
<equation confidence="0.977929">
n ni
XT 1/4 X ðjBþ i,jj þ jB-jÞ ð20Þ
i1/41 j1/42
</equation>
<bodyText confidence="0.961765363636364">
The relative efficiency of the two algorithms depends on the value of C/T at each
iteration. In the worst case, when every feature chosen appears on every training
example, then C/T = 1, and the two algorithms essentially have the same running time.
However in sparse feature spaces there is reason to believe that C/T will be small for
most iterations. In section 5.4.3 we show that this is the case for our experiments.
4.6 Feature Selection for LogLoss
We now describe an approach that was implemented for LogLoss. At the first iteration,
a0 is set to one. Feature selection then searches for values of the remaining parameters,
a1, ... , am. We now describe how to calculate the optimal update for a feature k with
the LogLoss function. First we recap the definition of the probability of a particular
parse xi,q given parameter settings ¯a:
</bodyText>
<equation confidence="0.986373">
eFðxi,q,¯aÞ
Pðxi,q j si, ¯aÞ 1/4 P ni eFðxi,j,¯aÞ
j1/41
Recall that the log-loss is
XLogLossð¯aÞ 1/4 — log Pðxi,1 j si, ¯aÞ
i
</equation>
<bodyText confidence="0.9995748">
Unfortunately, unlike the case of ExpLoss, in general an analytic solution for BestWt
does not exist. However, we can define an iterative solution using techniques from
iterative scaling (Della Pietra, Della Pietra, and Lafferty 1997). We first define ˜
number of times that feature k is seen in the best parse, and ˜pkð¯aÞ, the expected number
of times under the model that feature k is seen:
</bodyText>
<equation confidence="0.9019805">
ni
X˜hk 1/4 Xhkðxi,1Þ, ˜pkð¯aÞ 1/4 X hkðxi,jÞPðxi,j j si, ¯aÞ
i i j1/41
hk, the
</equation>
<page confidence="0.987573">
47
</page>
<table confidence="0.904937666666667">
Computational Linguistics Volume 31, Number 1
Iterative scaling then defines the following update d˜
d˜ = log ˜hk
˜pk(¯a)
While in general it is not true that d˜ = BestWt(k, ¯a), it can be shown that this update
leads to an improvement in the LogLoss (i.e., that LogLoss(Upd(¯a, k, ˜d)) &lt; LogLoss ¯a)),
with equality holding only when ¯ak is already at the optimal value, in other words,
when arg mind LogLoss(Upd(¯a, k, d)) = 0. This suggests the following iterative method
for finding BestWt(k, ¯a):
Step 1: Initialization: Set d = 0 and ¯a&apos; = ¯a, calculate ˜hk.
Step 2: Repeat until convergence of d:
a: Calculate p˜k(¯a&apos;).
b: d&lt;--d + log
c: ¯a&apos;&lt;-- Upd(¯a,k, d).
Step 3: Return BestWt(k, ¯a) = d.
</table>
<bodyText confidence="0.960880913043478">
Given this method for calculating BestWt(k, ¯a), BestLoss(k, ¯a) can be calculated as
Loss(k, BestWt(k, ¯a)). Note that this is only one of a number of methods for finding
BestWt(k, ¯a): Given that this is a one-parameter, convex optimization problem, it is a
fairly simple task, and there are many methods which could be used.
Unfortunately there does not appear to be an efficient algorithm for LogLoss that is
analogous to the ExpLoss algorithm in Figure 4 (at least if the feature selection method
is required to pick the feature with highest impact on the loss function at each
iteration). A similar observation for LogLoss can be made, in that when the model is
updated with a feature/weight pair (k*, d*), many features will have their values for
BestWt and BestLoss unchanged. Only those features which co-occur with k* on some
example will need to have their values of BestWt and BestLoss updated. However, this
observation does not lead to an efficient algorithm: Updating these values is much
more expensive than in the ExpLoss case. The procedure for finding the optimal value
BestWt(k, ¯a) must be applied for each feature which co-occurs with the chosen feature
k*. For example, the iterative scaling procedure described above must be applied for a
number of features. For each feature, this will involve recalculation of the distribution
{P(xi,1 1 si),P(xi,2 I si), ...,P(xi,ni I si)I for each example i on which the feature occurs.13 It
takes only one feature that is seen on all training examples for the algorithm to involve
recalculation of P(xi,j I si) for the entire training set. This contrasts with the simple
updates in the improved boosting algorithm (W+k = W+k + D and Wk = Wk + D). In
fact in the parsing experiments, we were forced to give up on the LogLoss feature
selection methods because of their inefficiency (see section 6.4 for more discussion about
efficiency).
</bodyText>
<footnote confidence="0.940747">
13 This is not a failure of iterative scaling alone: Given that in the general case, closed-form solutions for
BestWt and BestLoss do not exist, it is hard to imagine a method that computes these values exactly
without some kind of iterative method which requires repeatedly visiting the examples on which a
feature is seen.
</footnote>
<bodyText confidence="0.3759805">
˜hk
˜pk(¯a&apos;).
</bodyText>
<page confidence="0.986009">
48
</page>
<note confidence="0.769416">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.99962825">
Note, however, that approximate methods for finding the best feature and
updating its weight may lead to efficient algorithms. Appendix B gives a sketch of one
such approach, which is based on results from Collins, Schapire, and Singer (2002). We
did not test this method; we leave this to future work.
</bodyText>
<sectionHeader confidence="0.984348" genericHeader="method">
5. Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.977988">
5.1 Generation of Parsing Data Sets
</subsectionHeader>
<bodyText confidence="0.999835833333333">
We used the Penn Wall Street Journal treebank (Marcus, Santorini, and
Marcinkiewicz 1993) as training and test data. Sections 2–21 inclusive (around
40,000 sentences) were used as training data, section 23 was used as the final test set.
Of the 40,000 training sentences, the first 36,000 were used as the main training set.
The remaining 4,000 sentences were used as development data and to cross-validate
the number of rounds (features) in the model. Model 2 of Collins (1999) was used to
parse both the training and test data, producing multiple hypotheses for each
sentence. We achieved this by disabling dynamic programming in the parser and
choosing a relatively narrow beam width of 1,000. The resulting parser returns all
parses that fall within the beam. The number of such parses varies sentence by
sentence.
In order to gain a representative set of training data, the 36,000 training sentences
were parsed in 2,000 sentence chunks, each chunk being parsed with a model trained
on the remaining 34,000 sentences (this prevented the initial model from being
unrealistically “good”on the training sentences). The 4,000 development sentences
were parsed with a model trained on the 36,000 training sentences. Section 23 was
parsed with a model trained on all 40,000 sentences.
In the experiments we used the following definition for the Score of the parse:
</bodyText>
<equation confidence="0.989079">
Score(xi,j) = F-me1 oe(xi,j) x Size(xi,j)
</equation>
<bodyText confidence="0.9998914">
where F-measure(xi,j) is the F1 score14 of the parse when compared to the gold-
standard parse (a value between 0 and 100), and Size(xi,j) is the number of constituents
in the gold-standard parse for the ith sentence. Hence the Score function is sensitive to
both the accuracy of the parse, and also the number of constituents in the gold-
standard parse.
</bodyText>
<subsectionHeader confidence="0.836486">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.98627025">
The following types of features were included in the model. We will use the rule VP —&gt;
PP VBD NP NP SBAR with head VBD as an example. Note that the output of our
baseline parser produces syntactic trees with headword annotations (see Collins
[1999]) for a description of the rules used to find headwords).
</bodyText>
<footnote confidence="0.885118">
14 Note that in the rare cases in which the baseline parser produces no constituents, the precision is
undefined; in these cases we defined the F-measure to be 0.
</footnote>
<page confidence="0.998181">
49
</page>
<figure confidence="0.386357">
Computational Linguistics Volume 31, Number 1
</figure>
<figureCaption confidence="0.4611925">
Rules. These include all context-free rules in the
tree, for example, VP —&gt; PP VBD NP NP SBAR.
Bigrams. These are adjacent pairs of nonterminals
to the left and right of the head. As shown, the
example rule would contribute the bigrams
(Right,VP,NP,NP), (Right,VP,NP,SBAR),
(Right,VP,SBAR,STOP) to the right of the head
and (Left,VP,PP,STOP) to the left of the head.
Grandparent rules. Same as Rules, but also
including the nonterminal above the rule.
</figureCaption>
<bodyText confidence="0.968552894736842">
Two-level rules. Same as Rules, but also
including the entire rule above the rule.
Two-level bigrams. Same as Bigrams, but
also including the entire rule above the rule.
Trigrams. All trigrams within the rule. The
example rule would contribute the trigrams
(VP, STOP, PP, VBD!), (VP, PP, VBD!, NP),
(VP, VBD!, NP, NP), (VP, NP, NP, SBAR),
and (VP,NP, SBAR, STOP) (! is used to mark
the head of the rule).
Grandparent bigrams. Same as Bigrams,
but also including the nonterminal above
the bigrams.
Lexical bigrams.
Same as Bigrams,
but with the lexical
heads of the two
nonterminals also
included.
</bodyText>
<page confidence="0.849409">
50
</page>
<note confidence="0.666366">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.999612783783784">
Head Modifiers. All head-modifier pairs, with
the grandparent nonterminal also included.
An adj flag is also included, which is one if
the modifier is adjacent to the head, zero
otherwise. As an example, say the nonterminal
dominating the example rule is S. The
example rule would contribute (Left, S, VP,
VBD, PP, adj = 1), (Right, S, VP, VBD, NP,
adj = 1), (Right, S, VP, VBD, NP, adj = 0),
and (Right, S, VP, VBD, SBAR, adj = 0).
PPs. Lexical trigrams involving the heads
of arguments of prepositional phrases.
The example shown at right would
contribute the trigram (NP, NP, PP, NP,
president, of, U.S.), in addition to the
relation (NP, NP, PP, NP, of, U.S.), which
ignores the headword of the constituent
being modified by the PP. The three
nonterminals (for example, NP, NP, PP)
identify the parent of the entire phrase,
the nonterminal of the head of the phrase,
and the nonterminal label for the PP.
Distance head modifiers. Features involving the distance between
headwords. For example, assume dist is the number of words between
the headwords of the VBD and SBAR in the (VP, VBD, SBAR) head-modifier
relation in the above rule. This relation would then generate features
(VP, VBD, SBAR, = dist), and (VP, VBD, SBAR, &lt; x) for all dist &lt; x &lt; 9 and
(VP, VBD, SBAR, &gt; x) for all 1 &lt; x &lt; dist.
Further lexicalization. In order to generate more features, a second pass
was made in which all nonterminals were augmented with their lexical
heads when these headwords were closed-class words. All features
apart from head modifiers, PPs, and distance head modifiers were then
generated with these augmented nonterminals.
All of these features were initially generated, but only features seen on at least
one parse for at least five different sentences were included in the final model (this
count cutoff was implemented to keep the number of features down to a tractable
number).
</bodyText>
<subsectionHeader confidence="0.763388">
5.3 Applying the Reranking Methods
</subsectionHeader>
<bodyText confidence="0.999276">
The ExpLoss method was trained with several values for the smoothing parameter e:
{0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075}. For each value of e, the
method was run for 100,000 rounds on the training data. The implementation was
such that the feature updates for all 100,000 rounds for each training run were
recorded in a file. This made it simple to test the model on development data for all
values of N between 0 and 100,000.
</bodyText>
<page confidence="0.993356">
51
</page>
<note confidence="0.493408">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.9918185">
The different values of &amp; and N were compared on development data through the
following criterion:
</bodyText>
<equation confidence="0.9846345">
X Score(zi) (21)
i
</equation>
<bodyText confidence="0.999929454545455">
where Score is as defined above, and zi is the output of the model on the ith
development set example. The &amp;, N values which maximized this quantity were used
to define the final model applied to the test data (section 23 of the treebank). The
optimal values were &amp; = 0.0025 and N = 90,386, at which point 11,673 features had
nonzero values (note that the feature selection techniques may result in a given feature
being updated more than once). The computation took roughly 3–4 hours on a
machine with a 1.6 GHz pentium processor and around 2 GB of memory.
Table 1 shows results for the method. The model of Collins (1999) was the base
model; the ExpLoss model gave a 1.5% absolute improvement over this method. The
method gives very similar accuracy to the model of Charniak (2000), which also uses a
rich set of initial features in addition to Charniak’s (1997) original model.
The LogLoss method was too inefficient to run on the full data set. Instead we
made some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parse
trees) and 52,294 features.15 On an older machine (an order of magnitude or more
slower than the machine used for the final tests) the boosting method took 40 minutes
for 10,000 rounds on this data set. The LogLoss method took 20 hours to complete
3,500 rounds (a factor of about 85 times slower). This was in spite of various heuristics
that were implemented in an attempt to speed up LogLoss: for example, selecting
multiple features at each round or recalculating the statistics for only the best K
features for some small K at the previous round of feature selection. In initial
experiments we found ExpLoss to give similar, perhaps slightly better, accuracy than
LogLoss.
</bodyText>
<subsectionHeader confidence="0.875058">
5.4 Further Experiments
</subsectionHeader>
<bodyText confidence="0.995746153846154">
This section describes further experiments investigating various aspects of the
boosting algorithm: the effect of the &amp; and N parameters, learning curves, the choice
of the Si,j weights, and efficiency issues.
5.4.1 The Effect of the a and N Parameters. Figure 5 shows the learning curve on
development data for the optimal value of &amp; (0.0025). The accuracy shown is the
performance relative to the baseline method of using the probability from the
generative model alone in ranking parses, where the measure in equation (21) is used
to measure performance. For example, a score of 101.5 indicates a 1.5% increase in this
score. The learning curve is initially steep, eventually flattening off, but reaching its
peak value after a large number (90,386) of rounds of feature selection.
Table 2 indicates how the peak performance varies with the smoothing parameter
&amp;. Figure 6 shows learning curves for various values of &amp;. It can be seen that values
other than &amp; = 0.0025 can lead to undertraining or overtraining of the model.
</bodyText>
<page confidence="0.6958265">
15 All features described above except distance head modifiers and further lexicalization were included.
52
</page>
<note confidence="0.961993">
Collins and Koo Discriminative Reranking for NLP
</note>
<tableCaption confidence="0.993485">
Table 1
</tableCaption>
<bodyText confidence="0.57954775">
Results on section 23 of the WSJ Treebank. “LR”is labeled recall; “LP”is labeled precision;
“CBs”is the average number of crossing brackets per sentence; “0 CBs”is the percentage of
sentences with 0 crossing brackets; “2 CBs”is the percentage of sentences with two or more
crossing brackets. All the results in this table are for models trained and tested on the same data,
using the same evaluation metric. Note that the ExpLoss results are very slightly different from
the original results published in Collins (2000). We recently reimplemented the boosting code
and reran the experiments, and minor differences in the code and a values tested on
development data led to minor improvements in the results.
</bodyText>
<table confidence="0.996333615384615">
Model &lt; 40 Words (2,245 sentences)
LR LP CBs 0 CBs 2 CBs
Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1%
Collins 1999 88.5% 88.7% 0.92 66.7% 87.1%
Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6%
ExpLoss 90.2% 90.4% 0.73 71.2% 90.2%
Model &lt; 100 Words (2,416 sentences)
LR LP CBs 0 CBs 2 CBs
Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2%
Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% —
Collins 1999 88.1% 88.3% 1.06 64.0% 85.1%
Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7%
ExpLoss 89.6% 89.9% 0.86 68.7% 88.3%
</table>
<figureCaption confidence="0.794596">
Figure 5
</figureCaption>
<bodyText confidence="0.7275105">
Learning curve on development data for the optimal value for a (0.0025). The y-axis is the level of
accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting.
</bodyText>
<page confidence="0.9955">
53
</page>
<table confidence="0.431354">
Computational Linguistics Volume 31, Number 1
</table>
<tableCaption confidence="0.959405">
Table 2
</tableCaption>
<table confidence="0.957674916666667">
Peak performance achieved for various values of F. ‘‘Best N”refers to the number of
rounds at which peak development set accuracy was reached. ‘‘Best score”indicates the
relative performance, compared to the baseline method, at the optimal value for N.
E Best N Best score
0.0001 29,471 101.743
0.00025 22,468 101.849
0.0005 48,795 101.845
0.00075 43,386 101.809
0.001 43,975 101.849
0.0025 90,386 101.864
0.005 66,378 101.824
0.0075 80,746 101.722
</table>
<subsubsectionHeader confidence="0.807853">
5.4.2 The Effect of the St,j Weights on Examples. In section 4.2.3 we introduced the
</subsubsectionHeader>
<bodyText confidence="0.996345">
idea of weights Si,j representing the importance of examples. Thus far, in the
experiments in this article, we have used the definition
</bodyText>
<equation confidence="0.927361">
Si,j 1/4 Scoreðxi,1Þ — Scoreðxi,jÞ ð22Þ
</equation>
<bodyText confidence="0.9278590625">
thereby weighting examples in proportion to their difference in score from the correct
parse for the sentence in question. In this section we compare this approach to a
default definition of Si,j, namely,
Si,j 1/4 1 ð23Þ
Using this definition, we trained the ExpLoss method on the same training set for
several values of the smoothing parameter a and evaluated the performance on
development data. Table 3 compares the peak performance achieved under the two
definitions of Si,j on the development set. It can be seen that the definition in equation
(22) outperforms the simpler method in equation (23). Figure 7 shows the learning
curves for the optimal values of a for the two methods. It can be seen that the learning
curve for the definition of Si,j in equation (22) consistently dominates the curve for the
simpler definition.
5.4.3 Efficiency Gains. Section 4.5 introduced an efficient algorithm for optimizing
ExpLoss. In this section we explore the empirical gains in efficiency seen on the
parsing data sets in this article.
We first define the quantity T as follows:
</bodyText>
<equation confidence="0.908535666666667">
ni
XT 1/4 X ðjBþ i,jj þ jB�i,jjÞ
i j1/42
</equation>
<page confidence="0.997643">
54
</page>
<note confidence="0.885325">
Collins and Koo Discriminative Reranking for NLP
</note>
<figureCaption confidence="0.939031">
Figure 6
</figureCaption>
<bodyText confidence="0.998855555555555">
Learning curves on development data for various values of &amp;. In each case the y-axis is the
level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of
boosting. The three graphs compare the curve for &amp; = 0.0025 (the optimal value) to (from top
to bottom) &amp; = 0.0001, &amp; = 0.0075, and &amp; = 0.001. The top graph shows that &amp; = 0.0001 leads to
undersmoothing (overtraining). Initially the graph is higher than that for &amp; = 0.0025, but on later
rounds the performance starts to decrease. The middle graph shows that &amp; = 0.0075 leads to
oversmoothing (undertraining). The graph shows consistently lower performance than that for
&amp; = 0.0025. The bottom graph shows that there is little difference in performance for &amp; = 0.001
versus &amp; = 0.0025.
</bodyText>
<page confidence="0.996469">
55
</page>
<note confidence="0.455645">
Computational Linguistics Volume 31, Number 1
</note>
<tableCaption confidence="0.992099">
Table 3
</tableCaption>
<table confidence="0.996292272727273">
Peak performance achieved for various values of a for Si,j 1/4 Scoreðxi,1Þ — Scoreðxi,jÞ (column
labeled “weighted”) and Si,j 1/4 1 (column labeled “unweighted”).
E Best score (weighted) Best score (unweighted)
0.0001 101.743 101.744
0.00025 101.849 101.754
0.0005 101.845 101.778
0.00075 101.809 101.762
0.001 101.849 101.778
0.0025 101.864 101.699
0.005 101.824 101.610
0.0075 101.722 101.604
</table>
<bodyText confidence="0.999671166666667">
This is a measure of the number of updates to the Wþk and Wk variables required in
making a pass over the entire training set. Thus it is a measure of the amount of
computation that the naive algorithm for ExpLoss, presented in Figure 3, requires for
each round of feature selection.
Next, say the improved algorithm in Figure 4 selects feature k* on the t th round of
feature selection. Then we define the following quantity:
</bodyText>
<equation confidence="0.545976">
XCt 1/4 ðjBþ i,jj þ jB� i,jjÞ þ X ðjBþ i,jj þ jB-jÞ
ði,jÞEAþk� ði,jÞEA�k�
</equation>
<figureCaption confidence="0.8606078">
This is a measure of the number of summations required by the improved algorithm in
Figure 4 at the t th round of feature selection.
Figure 7
Performance versus number of rounds of boosting for Si,j 1/4 Scoreðxi,1Þ — Scoreðxi,jÞ
(curve labeled “Weighted”) and Si,j 1/4 1 (curve labeled “Not Weighted”).
</figureCaption>
<page confidence="0.986265">
56
</page>
<note confidence="0.841476">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.998512">
We are now in a position to compare the running times of the two algorithms. We
define the following quantities:
</bodyText>
<equation confidence="0.981056">
n Ct (24)
X T
Work(n) =
t=1
nT
Savings(n) = n
Ct
t=1
Savings(a, b) = (1+b—a)Tb
Ct
t=a
</equation>
<bodyText confidence="0.997733611111111">
Here, Work(n) is the computation required for n rounds of feature selection, where
a single unit of computation corresponds to a pass over the entire training set.
Savings(n) tracks the relative efficiency of the two algorithms as a function of the
number of features, n. For example, if Savings(100) = 1,200, this signifies that for the
first 100 rounds of feature selection, the improved algorithm is 1,200 times as efficient
as the naive algorithm. Finally, Savings(a, b) indicates the relative efficiency between
rounds a and b, inclusive, of feature selection. For example, Savings(11, 100) = 83
signifies that between rounds 11 and 100 inclusive of the algorithm, the improved
algorithm was 83 times as efficient.
Figures 8 and 9 show graphs of Work(n) and Savings(n) versus n. The savings
from the improved algorithm are dramatic. In 100,000 rounds of feature selection,
the improved algorithm requires total computation that is equivalent to a mere 37.1
passes over the training set. This is a saving of a factor of 2,692 over the naive algorithm.
Table 4 shows the value of Savings(a,b) for various values of (a,b). It can be
seen that the performance gains are significantly larger in later rounds of feature
selection, presumably because in later stages relatively infrequent features are being
selected. Even so, there are still savings of a factor of almost 50 in the early stages of
the method.
</bodyText>
<sectionHeader confidence="0.998535" genericHeader="method">
6. Related Work
</sectionHeader>
<subsectionHeader confidence="0.971046">
6.1 History-Based Models with Complex Features
</subsectionHeader>
<bodyText confidence="0.99813">
Charniak (2000) describes a parser which incorporates additional features into a
previously developed parser, that of Charniak (1997). The method gives substantial
improvements over the original parser and results which are very close to the results
of the boosting method we have described in this article (see section 5 for experimental
results comparing the two methods). Our features are in many ways similar to those of
Charniak (2000). The model in Charniak (2000) is quite different, however. The
additional features are incorporated using a method inspired by maximum-entropy
models (e.g., the model of Ratnaparkhi [1997]).
Ratnaparkhi (1997) describes the use of maximum-entropy techniques ap-
plied to parsing. Log-linear models are used to estimate the conditional probabilities
P(di I (D (d1,...,di_1)) in a history-based parser. As a result the model can take into
account quite a rich set of features in the history.
</bodyText>
<page confidence="0.991765">
57
</page>
<figure confidence="0.674960333333333">
Computational Linguistics Volume 31, Number 1
Figure 8
Work(n)(y-axis) versus n (x-axis).
</figure>
<figureCaption confidence="0.961855">
Figure 9
</figureCaption>
<bodyText confidence="0.198876">
Savings(n)(y-axis) versus n(x-axis).
</bodyText>
<page confidence="0.99753">
58
</page>
<note confidence="0.975302">
Collins and Koo Discriminative Reranking for NLP
</note>
<tableCaption confidence="0.999117">
Table 4
</tableCaption>
<table confidence="0.984707666666667">
Values of Savings (a, b) for various values of a, b.
a–b Savings (a, b)
1–100,000 2,692.7
1–10 48.6
11–100 83.5
101–1,000 280.0
1,001–10,000 1,263.9
10,001–50,000 2,920.2
50,001–100,000 4,229.8
</table>
<bodyText confidence="0.988561333333333">
Both approaches still rely on decomposing a parse tree into a sequence of
decisions, and we would argue that the techniques described in this article have more
flexibility in terms of the features that can be included in the model.
</bodyText>
<subsectionHeader confidence="0.973285">
6.2 Joint Log-Linear Models
</subsectionHeader>
<bodyText confidence="0.93337775">
Abney (1997) describes the application of log-linear models to stochastic head-
driven phrase structure grammars (HPSGs). Della Pietra, Della Pietra, and Lafferty
(1997) describe feature selection methods for log-linear models, and Rosenfeld
(1997) describes application of these methods to language modeling for speech
recognition. These methods all emphasize models which define a joint proba-
bility over the space of all parse trees (or structures in question): For this reason
we describe these approaches as “Joint log-linear models.”The probability of a tree
xi,j is
</bodyText>
<equation confidence="0.988763">
Pð eF(x;,jÞ ð27Þ
xii) — P eFðxÞ
xEZ
</equation>
<bodyText confidence="0.99592">
Here Z is the (infinite) set of possible trees, and the denominator cannot be
calculated explicitly. This is a problem for parameter estimation, in which an estimate
of the denominator is required, and Monte Carlo methods have been proposed
(Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a
technique for estimation of this value. Our sense is that these methods can be
computationally expensive. Notice that the joint likelihood in equation (27) is not a
direct function of the margins on training examples, and its relation to error
rate is therefore not so clear as in the discriminative approaches described in this
article.
</bodyText>
<subsectionHeader confidence="0.992466">
6.3 Conditional Log-Linear Models
</subsectionHeader>
<bodyText confidence="0.9995806">
Ratnaparkhi, Roukos, and Ward (1994), Johnson et al. (1999), and Riezler et al. (2002)
suggest training log-linear models (i.e., the LogLoss function in equation (9)) for
parsing problems. Ratnaparkhi, Roukos, and Ward (1994) use feature selection
techniques for the task. Johnson et al. (1999) and Riezler et al. (2002) do not use a
feature selection technique, employing instead an objective function which includes a
</bodyText>
<page confidence="0.995062">
59
</page>
<note confidence="0.453443">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.53014">
Gaussian prior on the parameter values, thereby penalizing parameter values which
become too large:
</bodyText>
<equation confidence="0.826091333333333">
2
¯a* 1/4 arg minðLogLossð¯aÞ þ E ak ð28Þ
k1/40 ...m 72k
</equation>
<bodyText confidence="0.998070666666667">
Closed-form updates under iterative scaling are not possible with this objective
function; instead, optimization algorithms such as gradient descent or conjugate
gradient methods are used to estimate parameter values.
In more recent work, Lafferty, McCallum, and Pereira (2001) describe the use of
conditional Markov random fields (CRFs) for tagging tasks such as named entity
recognition or part-of-speech tagging (hidden Markov models are a common method
applied to these tasks). CRFs employ the objective function in equation (28). A key
insight of Lafferty, McCallum, and Pereira (2001) is that when features are of a
significantly local nature, the gradient of the function in equation (28) can be calculated
efficiently using dynamic programming, even in cases in which the set of candidates
involves all possible tagged sequences and is therefore exponential in size. See also Sha
and Pereira (2003) for more recent work on CRFs.
Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter
values which achieve the global minimum of the objective function in equation (28)) is
a plausible alternative to the feature selection approaches described in the current
article or to the feature selection methods previously applied to log-linear models. The
Gaussian prior (i.e., the Pk a2k/72k penalty) has been found in practice to be very
effective in combating overfitting of the parameters to the training data (Chen and
Rosenfeld 1999; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al.
2002). The function in equation (28) can be optimized using variants of gradient
descent, which in practice require tens or at most hundreds of passes over the training
data (see, e.g., Sha and Pereira 2003). Thus log-linear models with a Gaussian prior are
likely to be comparable in terms of efficiency to the feature selection approach
described in this article (in the experimental section, we showed that for the parse-
reranking task, the efficient boosting algorithm requires computation that is equivalent
to around 40 passes over the training data).
Note, however, that the two methods will differ considerably in terms of the
sparsity of the resulting reranker. Whereas the feature selection approach leads to
around 11,000 (2%) of the features in our model having nonzero parameter values,
log-linear models with Gaussian priors typically have very few nonzero parameters
(see, e.g., Riezler and Vasserman 2004). This may be important in some domains, for
example, those in which there are a very large number of features and this large
number leads to difficulties in terms of memory requirements or computation time.
</bodyText>
<subsectionHeader confidence="0.917795">
6.4 Feature Selection Methods
</subsectionHeader>
<bodyText confidence="0.999461142857143">
A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi
1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003;
Riezler and Vasserman 2004) describe feature selection approaches for log-linear
models applied to NLP problems. Earlier work (Berger, Della Pietra, and Della Pietra
1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested
methods that added a feature at a time to the model and updated all parameters in the
current model at each step (for more detail, see section 3.3).
</bodyText>
<page confidence="0.994555">
60
</page>
<note confidence="0.847683">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.99992338">
Assuming that selection of a feature takes one pass over the training set and that
fitting a model takes p passes over the training set, these methods require f x (p + 1)
passes over the training set, where f is the number of features selected. In our
experiments, f z 10,000. It is difficult to estimate the value for p, but assuming (very
conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over
the training set. This is around 1,000 times as much computation as that required for
the efficient boosting algorithm applied to our data, suggesting that the feature
selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998),
and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the
parsing task.
More recent work (McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004)
has considered methods for speeding up the feature selection methods described in
Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della
Pietra, and Lafferty (1997). McCallum (2003) and Riezler and Vasserman (2004)
describe approaches that add k features at each step, where k is some constant greater
than one. The running time for these methods is therefore O(f x (p + 1)1k). Riezler
and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal
performance. McCallum (2003) uses a value of k = 1,000. Zhou et al. (2003) use a
different heuristic that avoids having to recompute the gain for every feature at every
iteration.
We would argue that the alternative feature selection methods in the current
article may be preferable on the grounds of both efficiency and simplicity. Even with
large values of k in the approach of McCallum (2003) and Riezler and Vasserman
(2004) (e.g., k = 1,000), the approach we describe is likely to be at least as efficient as
these alternative approaches. In terms of simplicity, the methods in McCallum (2003)
and Riezler and Vasserman (2004) require selection of a number of free parameters
governing the behavior of the algorithm: the value for k, the value for a regularizer
constant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and the
precision with which the model is optimized at each stage of feature selection
(McCallum [2003] describes using “just a few BFGS iterations”at each stage). In
contrast, our method requires a single parameter to be chosen (the value for the e
smoothing parameter) and makes a single approximation (that only a single feature is
updated at each round of feature selection). The latter approximation is particularly
important, as it leads to the efficient algorithm in Figure 4, which avoids a pass over
the training set at each iteration of feature selection (note that in sparse feature spaces, f
rounds of feature selection in our approach can take considerably fewer than f passes
over the training set, in contrast to other work on feature selection within log-linear
models).
Note that there are other important differences among the approaches. Both Della
Pietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods that
induce conjunctions of “base”features, in a way similar to decision tree learners. Thus
a relatively small number of base features can lead to a very large number of possible
conjoined features. In future work it might be interesting to consider these kinds of
approaches for the parsing problem. Another difference is that both McCallum, and
Riezler and Vasserman, describe approaches that use a regularizer in addition to
feature selection: McCallum uses a two-norm regularizer; Riezler and Vasserman use a
one-norm regularizer.
Finally, note that other feature selection methods have been proposed within the
machine-learning community: for example, “filter”methods, in which feature
selection is performed as a preprocessing step before applying a learning method,
</bodyText>
<page confidence="0.996781">
61
</page>
<note confidence="0.598285">
Computational Linguistics Volume 31, Number 1
</note>
<bodyText confidence="0.972631638888889">
and backward selection methods (Koller and Sahami 1996), in which initially all
features are added to the model and features are then incrementally removed from the
model.
6.5 Boosting, Perceptron, and Support Vector Machine Approaches for Ranking
Problems
Freund et al. (1998) introduced a formulation of boosting for ranking problems. The
problem we have considered is a special case of the problem in Freund et al. (1998), in
that we have considered a binary distinction between candidates (i.e., the best parse
vs. other parses), whereas Freund et al. consider learning full or partial orderings over
candidates. The improved algorithm that we introduced in Figure 4 is, however, a new
algorithm that could perhaps be generalized to the full problem of Freund et al. (1998);
we leave this to future research.
Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003)
describe experiments on tagging tasks using the ExpLoss function, in contrast to the
LogLoss function used in Lafferty, McCallum, and Pereira (2001). Altun, Hofmann,
and Johnson (2003) describe how dynamic programming methods can be used to
calculate gradients of the ExpLoss function even in cases in which the set of candidates
again includes all possible tagged sequences, a set which grows exponentially in size
with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann
(2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact
on accuracy for the tagging task in question.
Perceptron-based algorithms, or the voted perceptron approach of Freund and
Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins
(2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron
algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a)
directly compares the boosting and perceptron approaches on a named entity task;
and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which
allow representations of parse trees or labeled sequences in very-high-dimensional
spaces.
Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to
ranking problems and apply support vector machines (SVMs) using tree-adjoining
grammar (Joshi, Levy, and Takahashi 1975) features to the parsing data sets we have
described in this article, with good empirical results.
See Collins (2004) for a discussion of many of these methods, including an
overview of statistical bounds for the boosting, perceptron, and SVM methods, as well
as a discussion of the computational issues involved in the different algorithms.
</bodyText>
<sectionHeader confidence="0.966553" genericHeader="method">
7. Conclusions
</sectionHeader>
<bodyText confidence="0.999979">
This article has introduced a new algorithm, based on boosting approaches in machine
learning, for ranking problems in natural language processing. The approach gives a
13% relative reduction in error on parsing Wall Street Journal data. While in this article
the experimental focus has been on parsing, many other problems in natural language
processing or speech recognition can also be framed as reranking problems, so the
methods described should be quite broadly applicable. The boosting approach to
ranking has been applied to named entity segmentation (Collins 2002a) and natural
language generation (Walker, Rambow, and Rogati 2001). The key characteristics of
the approach are the use of global features and of a training criterion (optimization
</bodyText>
<page confidence="0.995705">
62
</page>
<note confidence="0.820338">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.998611416666667">
problem) that is discriminative and closely related to the task at hand (i.e., parse
accuracy).
In addition, the article introduced a new algorithm for the boosting approach
which takes advantage of the sparse nature of the feature space in the parsing data that
we use. Other NLP tasks are likely to have similar characteristics in terms of sparsity.
Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for
the new algorithm over the obvious implementation of the boosting approach. We
would argue that the improved boosting algorithm is a natural alternative to
maximum-entropy or (conditional) log-linear models. The article has drawn connec-
tions between boosting and maximum-entropy models in terms of the optimization
problems that they involve, the algorithms used, their relative efficiency, and their
performance in empirical tests.
</bodyText>
<sectionHeader confidence="0.979271" genericHeader="method">
Appendix A: Derivation of Updates for ExpLoss
</sectionHeader>
<bodyText confidence="0.885570545454545">
This appendix gives a derivation of the optimal updates for ExpLoss. The derivation is
very close to that in Schapire and Singer (1999). Recall that for parameter values ¯a, we
need to compute BestWtðk, ¯aÞ and BestLossðk, ¯aÞ for k 1/4 1, ... , m, where
BestWtðk, ¯aÞ 1/4 arg min ExpLossðUpdð¯a, k, dÞÞ
d
and
BestLossðk, ¯aÞ 1/4 ExpLossðUpdð¯a, k, BestWtðk, ¯aÞÞÞ
The first thing to note is that an update in parameters from a¯ to Updð¯a, k,dÞÞ
results in a simple additive update to the ranking function F:
Fðxi,j, Updð¯a, k, dÞÞ 1/4 Fðxi,j, aÞ þ dhkðxi,jÞ
It follows that the margin on example ði, jÞ also has a simple update:
</bodyText>
<equation confidence="0.769481818181818">
Mi,jðUpdð¯a,k,dÞÞ 1/4 Fðxi,1,Updð¯a,k, dÞÞ — Fðxi,j,Updð¯a,k, dÞÞ
1/4 Fðxi,1, ¯aÞ — Fðxi,j, ¯aÞ þ d1/2hkðxi,1Þ — hkðxi,jÞ]
1/4 Mi,jð¯aÞ þ d1/2hkðxi,1Þ — hkðxi,jÞ]
The updated ExpLoss function can then be written as
ni
E Si,je�Mi,jðUpdð¯a,k,dÞÞ
ExpLoss ðUpdð¯a,k, dÞÞ 1/4
i j1/42
ni
E1/4 ESi,jeMi,jðaÞ-d1/2hkðxi,1Þ-hkðxi,jÞ]
i j1/42
</equation>
<bodyText confidence="0.737091666666667">
Next, we note that 1/2hkðxi,1Þ — hkðxi,jÞ] can take on three values: +1, —1, or 0. We split the
training sample into three sets depending on this value:
Aþk 1/4 fði,jÞ : 1/2hkðxi,1Þ — hkðxi,jÞ] 1/4 1g
</bodyText>
<page confidence="0.991393">
63
</page>
<note confidence="0.474331">
Computational Linguistics Volume 31, Number 1
</note>
<equation confidence="0.953828357142857">
A—k = {(i,j) : [hk(xi,1) — hk(xi,j)] = —11
A0 k = {(i,j) : [hk(xi,1) — hk(xi,j)] = 01
Given these definitions, we define Wk+, Wk—, and Wk0 as
XW+ k = Si,je—Mi,j(¯a)
(i,j)&amp;A+k
XW—k = Si,je—Mi,j(¯a)
(i,j)&amp;A—k
XW0 k = Si,je—Mi,j(¯a)
(i,j)&amp;A0k
ExpLoss is now rewritten in terms of these quantities:
ExpLoss(Upd(¯a, k, d)) = X XSi,je—Mi,j(¯a)—d + XSi,je—Mi,j(¯a)+d + Si,je—Mi,j(¯a)
(i,j)&amp;A+k (i,j)&amp;A—k (i,j)&amp;A0k
= e—dW+k + edW—k + W0 (A.1)
k
</equation>
<bodyText confidence="0.999022">
To find the value of d that minimizes this loss, we set the derivative of (A.1) with
respect to d to zero, giving the following solution:
</bodyText>
<figure confidence="0.923608133333333">
1 W+ k
BestWt(k, ¯a) = 2 log W—
k
Plugging this value of d back into (A.1) gives the best loss:
BestLoss(k, ¯a) = 2 q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
W+ k W— + W0
k k
q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
= 2 W+ k W— + Z — W+ k — W—
k k
Cqqffiffiffiffiffiffiffiffiffiffiffiffiffiffi
Wk Wk
= Z —
~2
(A.2)
</figure>
<bodyText confidence="0.6016594">
where Z = ExpLoss(¯a) = Pi Pni 2 Si,je—Mi,j(¯a) is a constant (for constant ¯a) which
appears in the BestLoss for all features and therefore does not affect their ranking.
Appendix B: An Alternative Method for LogLoss
In this appendix we sketch an alternative approach for feature selection in LogLoss
that is potentially an efficient method, at the cost of introducing an approximation
</bodyText>
<page confidence="0.997001">
64
</page>
<note confidence="0.821972">
Collins and Koo Discriminative Reranking for NLP
</note>
<bodyText confidence="0.980578111111111">
in the feature selection method. Until now, we have defined BestLossðk, ¯aÞ to be the
minimum of the loss given that the kth feature is updated an optimal amount:
BestLossðk, ¯aÞ 1/4 min LogLossðUpdð¯a,k, dÞÞ
d
In this section we sketch a different approach, based on results from Collins, Schapire,
and Singer (2002), which leads to an algorithm very similar to that for ExpLoss in
Figures 3 and 4. Take the following definitions (note the similarity to the definitions in
equations (13), (14), (15), and (16), with only the definitions for Wk+ and Wk~ being
altered):
</bodyText>
<table confidence="0.984679">
EWþ k 1/4 Eqi,j, Wk 1/4 eMi,jð¯aÞ
ði,jÞ&amp;Aþ ði,jÞ&amp;Ak qi,j, where qi,j 1/4 1 þ Pni a Mi,qð¯aÞ ðB.1Þ
k q1/42
1 BestWtðk, ¯aÞ 1/4 2 log Wk ðB.2Þ
k
BestLossðk, ¯aÞ 1/4 LogLossð¯aÞ — 2 Cqffiffiffiffiffiffiffiffiffiffiffiffiffiffi
Wk —Wk I ðB.3Þ
</table>
<bodyText confidence="0.97989848">
Note that the ExpLoss computations can be recovered by replacing qi,j in equation
(B.1) with qi,j 1/4 emi,jð¯aÞ. This is the only essential difference between the new
algorithm and the ExpLoss method.
Results from Collins, Schapire and Singer (2002) show that under these definitions
the following guarantee holds:
LogLossðUpdð¯a,k, BestWtðk, ¯aÞÞÞ &lt; BestLossðk, ¯aÞ
So it can be seen that the update from a¯ to Updð¯a, k, BestWtðk, ¯aÞÞ is guaranteed to
decrease LogLoss by at least ffiffiffiffiffiffiffi
(Wk -W, )2. From these results, the algorithms in Figures 3
and 4 could be altered to take the revised definitions of Wþk and Wk into account.
Selecting the feature with the minimum value of BestLossðk, ¯aÞ at each iteration leads
to the largest guaranteed decrease in LogLoss. Note that this is now an approximation,
in that BestLossðk, ¯a) is an upper bound on the log-likelihood which may or may not be
tight. There are convergence guarantees for the method, however, in that as the
number of rounds of feature selection goes to infinity, the LogLoss approaches its
minimum value.
The algorithms in Figures 3 and 4 could be modified to take the alternative
definitions of Wþk and Wk into account, thereby being modified to optimize LogLoss
instead of ExpLoss. The denominator terms in the qi,j definitions in equation (B.1) may
complicate the algorithms somewhat, but it should still be possible to derive relatively
efficient algorithms using the technique.
For a full derivation of the modified updates and for quite technical convergence
proofs, see Collins, Schapire and Singer (2002). We give a sketch of the argument here.
First, we show that
LogLossðUpdð¯a, k, dÞÞ &lt; LogLossð¯a — Wþk — Wk þ Wþk e~d þ Wk edÞ ðB.4Þ
</bodyText>
<page confidence="0.996948">
65
</page>
<table confidence="0.927757625">
Computational Linguistics Volume 31, Number 1
This can be derived as follows (in this derivation we use gkðxi,jÞ 1/4 hkðxi,1Þ — hkðxi,jÞÞ:
LogLossðUpdð¯a,k, dÞÞ
1/4 LogLossð¯aÞ þ LogLossðUpdð¯a, k, dÞÞ — LogLossð¯aÞ
X1/4 LogLossð¯aÞ þ log 1 þ Pni !
i j1/42 e Mi,jð¯aÞ—dgkðxi,jÞ
1 þ Pn1/42 e Mi,jð¯aÞ
j
X1/4 LogLossð¯aÞ þ 1 Pnij1/42e Mi,jð¯aÞ—dgkðxi,jÞ
i log 1 þ i
ni a mwða) + 1 + n eMi,jð¯aÞ
Pj1/42 E 7
X1/4 LogLossð¯aÞ þ 0 1
i nini
log 1 — X qi,j þ X qi,je�dgkðxi,jÞ A
j1/42 j1/42 ðB.5Þ
</table>
<figure confidence="0.8070612">
ni ni
X� LogLoss ð¯aÞ �
i
X qi,j þ X
j1/42 i
X qi,je�dgkðxi,jÞ
j1/42
ðB.6Þ
1/4 LogLossð¯aÞ — ðW0k þ Wþk þ Wk Þ þ W0k þ Wþk e~d þ Wk ed
1/4 LogLossð¯aÞ — Wþk — Wk þ Wþk e~d þ Wk ed
</figure>
<bodyText confidence="0.79873275">
Equation (B.6) can be derived from equation (B.5) through the bound logð1 + xÞ &lt; x
for all x.
The second step is to minimize the right-hand side of the bound in equation (B.4)
with respect to d. It can be verified that the minimum is found at
</bodyText>
<equation confidence="0.435582666666667">
1 Wþ k
d 1/4 2 log W~
k
</equation>
<bodyText confidence="0.462159">
at which value the right-hand side of equation (B.4) is equal to
</bodyText>
<table confidence="0.88393">
LogLossðdÞ — C q~2
q ffiffiffiffiffiffiffi ffiffiffiffiffiffiffi
Wþ ~W~
k k
</table>
<sectionHeader confidence="0.994126" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997071125">
Thanks to Rob Schapire and Yoram Singer for
useful discussions on boosting algorithms and
to Mark Johnson for useful discussions about
linear models for parse ranking. Steve Abney
and Fernando Pereira gave useful feedback on
earlier drafts of this work. Finally, thanks to
the anonymous reviewers for several useful
comments.
</bodyText>
<sectionHeader confidence="0.994288" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997900444444444">
Abney, Steven. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597–618.
Altun, Yasemin, Thomas Hofmann, and
Mark Johnson. 2003. Discriminative
learning for label sequences via boosting.
In Advances in Neural Information Processing
Systems (NIPS 15), Vancouver.
Altun, Yasemin, Mark Johnson, and
Thomas Hofmann. 2003. Loss functions
and optimization methods for
discriminative learning of label sequences.
In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2003),
Sapporo, Japan.
Berger, Adam L., Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language
</reference>
<page confidence="0.984893">
66
</page>
<note confidence="0.939476">
Collins and Koo Discriminative Reranking for NLP
</note>
<reference confidence="0.999568762711865">
processing. Computational Linguistics,
22(1):39–71.
Black, Ezra, Frederick Jelinek, John Lafferty,
David Magerman, Robert Mercer, and Salim
Roukos. 1992. Towards history-based
grammars: Using richer models for
probabilistic parsing. In Proceedings of the
Fifth DARPA Speech and Natural Language
Workshop, Harriman, NY.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. Proceedings of the 14th National
Conference on Artificial Intelligence, Menlo
Park, CA. AAAI Press/MIT Press.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL-2000, Seattle.
Chen, Stanley F., and Ronald Rosenfeld.
1999. A gaussian prior for smoothing
maximum entropy models. Technical
Report CMU-CS-99-108, Computer
Science Department, Carnegie Mellon
University.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
and Eighth Conference of the European Chapter
of the Association for Computational
Linguistics, pages 16–23, Madrid.
Collins, Michael. 1999. Head-Driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In Proceedings of the 17th International
Conference on Machine Learning (ICML 2000),
Stanford, CA. Morgan Kaufmann,
San Francisco.
Collins, Michael. 2002a. Ranking algorithms
for named-entity extraction: Boosting and
the voted perceptron. In ACL 2002:
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Collins, Michael. 2002b. Discriminative
training methods for hidden Markov
models: Theory and experiments with the
perceptron algorithm. In Proceedings of
EMNLP 2002, Philadelphia.
Collins, Michael. 2004. Parameter estimation
for statistical parsing models: Theory and
practice of distribution-free methods. In
Harry Bunt, John Carroll, and Giorgio Satta,
editors, New Developments in Parsing
Technology. Kluwer.
Collins, Michael and Nigel Duffy. 2001.
Convolution kernels for natural language.
In Advances in Neural Information Processing
Systems (NIPS 14), Vancouver.
Collins, Michael, and Nigel Duffy. 2002.
New ranking algorithms for parsing and
tagging: Kernels over discrete structures,
and the voted perceptron. In ACL 2002:
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Collins, Michael, Robert E. Schapire, and
Yoram Singer. 2002. Logistic regression,
AdaBoost and Bregman distances. Machine
Learning, 48(1/2/3):253–285.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(4):380–393.
Duffy, Nigel and David Helmbold. 1999.
Potential boosters? In Advances in
Neural Information Processing Systems
(NIPS 12), Denver.
Freund, Yoav, Raj Iyer, Robert E. Schapire,
and Yoram Singer. 1998. An efficient
boosting algorithm for combining
preferences. In Machine Learning:
Proceedings of the 15th International
Conference, Madison, WI.
Freund, Yoav and Robert E. Schapire. 1997. A
decision-theoretic generalization of on-line
learning and an application to boosting.
Journal of Computer and System Sciences,
55(1):119–139.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277–296.
Friedman, Jerome H., Trevor Hastie, and
Robert Tibshirani. 2000. Additive logistic
regression: A statistical view of boosting.
Annals of Statistics, 38(2):337–374.
Henderson, James. 2003. Inducing history
representations for broad coverage
statistical parsing. In Proceedings of the Joint
Meeting of the North American Chapter of the
Association for Computational Linguistics and
the Human Language Technology Conference
(HLT-NAACL 2003), pages 103–110,
Edmonton, Alberta, Canada.
Hoffgen, Klauss U., Kevin S. van Horn, and
Hans U. Simon. 1995. Robust trainability of
single neurons. Journal of Computer and
System Sciences, 50:114–125.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
“unification-based”grammars. In
Proceedings of ACL 1999, College
Park, MD.
</reference>
<page confidence="0.972523">
67
</page>
<reference confidence="0.994822915966387">
Computational Linguistics Volume 31, Number 1
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Science 10,
no. 1:136–163.
Koller, Daphne, and Mehran Sahami. 1996.
Toward optimal feature selection. In
Proceedings of the 13th International
Conference on Machine Learning (ICML),
pages 284–292, Bari, Italy, July.
Lafferty, John. 1999. Additive models,
boosting, and inference for generalized
divergences. In Proceedings of the 12th
Annual Conference on Computational Learning
Theory (COLT’99), Santa Cruz, CA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of ICML 2001,
Williamstown, MA.
Lebanon, Guy and John Lafferty. 2001.
Boosting and maximum likelihood for
exponential models. In Advances in
Neural Information Processing Systems
(NIPS 14), Vancouver.
Malouf, Robert. 2002. A comparison of
algorithms for maximum entropy
parameter estimation. In Proceedings of the
Sixth Conference on Natural Language
Learning (CoNNL-2002), Taipei, Taiwan.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19(2):313–330.
Mason, Llew, Peter L. Bartlett, and Jonathan
Baxter. 1999. Direct optimization of margins
improves generalization in combined
classifiers. In Advances in Neural Information
Processing Systems (NIPS 12), Denver.
McCallum, Andrew. 2003. Efficiently
inducing features of conditional random
fields. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence (UAI
2003), Acapulco.
Och, Franz Josef, and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In ACL 2002: Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 295–302,
Philadelphia.
Papineni, Kishore A., Salim Roukos, and R. T.
Ward. 1997. Feature-based language
understanding. In Proceedings of
EuroSpeech’97, vol. 3, pages 1435–1438,
Rhodes, Greece.
Papineni, Kishore A., Salim Roukos, and
R. T. Ward. 1998. Maximum likelihood
and discriminative training of direct
translation models. In Proceedings of the
1998 IEEE International Conference on
Acoustics, Speech and Signal Processing,
vol. 1, pages 189–192, Seattle.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems. Morgan Kaufmann, San
Mateo, CA.
Ratnaparkhi, Adwait. 1997. A linear observed
time statistical parser based on maximum
entropy models. In Proceedings of the Second
Conference on Empirical Methods in Natural
Language Processing, Brown University,
Providence, RI.
Ratnaparkhi, Adwait. 1998. Maximum Entropy
Models for Natural Language Ambiguity
Resolution. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Ratnaparkhi, Adwait, Salim Roukos, and
R. T. Ward. 1994. A maximum entropy
model for parsing. In Proceedings of the
International Conference on Spoken Language
Processing, pages 803–806, Yokohama,
Japan.
Riezler, Stefan, Tracy H. King,
Ronald M. Kaplan, Richard Crouch,
John T. Maxwell III, and Mark Johnson.
2002. Parsing the Wall Street Journal
using a lexical-functional grammar
and discriminative estimation
techniques. In ACL 2002: Proceedings
of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Riezler, Stefan and Alexander Vasserman.
2004. Incremental feature selection and
l1 regularization for relaxed
maximum-entropy modeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP’04),
Barcelona, Spain.
Rosenfeld, Ronald. 1997. A whole sentence
maximum entropy language model. In
Proceedings of the IEEE Workshop on
Speech Recognition and Understanding,
Santa Barbara, CA, December.
Schapire, Robert E., Yoav Freund, Peter
Bartlett, and W. S. Lee. 1998. Boosting the
margin: A new explanation for the
effectiveness of voting methods. Annals of
Statistics, 26(5):1651–1686.
Schapire, Robert E. and Yoram Singer. 1999.
Improved boosting algorithms using
confidence-rated predictions. Machine
Learning, 37(3):297–336.
Schapire, Robert E. and Yoram Singer. 2000.
BoosTexter: A boosting-based system for
text categorization. Machine Learning,
39(2/3):135–168.
</reference>
<page confidence="0.993097">
68
</page>
<note confidence="0.813133">
Collins and Koo Discriminative Reranking for NLP
</note>
<reference confidence="0.998551884615385">
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields. In
Proceedings of HLT-NAACL 2003,
Edmonton, Alberta, Canada.
Shen, Libin, Anoop Sarkar, and Aravind K.
Joshi. 2003. Using LTAG based features in
parse reranking. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
Sapporo, Japan.
Valiant, Leslie G. 1984. A theory of the
learnable. Communications of the ACM,
27(11):1134–1142.
Walker, Marilyn, Owen Rambow, and
Monica Rogati. 2001. SPoT: A trainable
sentence planner. In Proceedings of the Second
Meeting of the North American Chapter of the
Association for Computational
Linguistics (NAACL 2001), Pittsburgh.
Zhou, Yaqian, Fuliang Weng, Lide Wu, and
Hauke Schmidt. 2003. A fast algorithm for
feature selection in conditional maximum
entropy modeling. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
Sapporo, Japan.
</reference>
<page confidence="0.99931">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838522">
<title confidence="0.962995">Articles Discriminative Reranking for Natural Language Parsing</title>
<affiliation confidence="0.99815">Massachusetts Institute of Technology Massachusetts Institute of Technology</affiliation>
<abstract confidence="0.995169761904762">This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the model. The new model achieved 89.75% a 13% relative decrease in measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="6361" citStr="Abney 1997" startWordPosition="972" endWordPosition="973">, involving around one million parse trees and over 500,000 features. The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds. The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the “naive”implementation). The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before. In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al. 2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks. (Log-linear models are often referred to as maximum-entropy models in the NLP literature.) Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-line</context>
<context position="80489" citStr="Abney (1997)" startWordPosition="13496" endWordPosition="13497">re 8 Work(n)(y-axis) versus n (x-axis). Figure 9 Savings(n)(y-axis) versus n(x-axis). 58 Collins and Koo Discriminative Reranking for NLP Table 4 Values of Savings (a, b) for various values of a, b. a–b Savings (a, b) 1–100,000 2,692.7 1–10 48.6 11–100 83.5 101–1,000 280.0 1,001–10,000 1,263.9 10,001–50,000 2,920.2 50,001–100,000 4,229.8 Both approaches still rely on decomposing a parse tree into a sequence of decisions, and we would argue that the techniques described in this article have more flexibility in terms of the features that can be included in the model. 6.2 Joint Log-Linear Models Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs). Della Pietra, Della Pietra, and Lafferty (1997) describe feature selection methods for log-linear models, and Rosenfeld (1997) describes application of these methods to language modeling for speech recognition. These methods all emphasize models which define a joint probability over the space of all parse trees (or structures in question): For this reason we describe these approaches as “Joint log-linear models.”The probability of a tree xi,j is Pð eF(x;,jÞ ð27Þ xii) — P eFðxÞ xEZ Here Z</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Abney, Steven. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasemin Altun</author>
<author>Thomas Hofmann</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative learning for label sequences via boosting.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS 15),</booktitle>
<location>Vancouver.</location>
<marker>Altun, Hofmann, Johnson, 2003</marker>
<rawString>Altun, Yasemin, Thomas Hofmann, and Mark Johnson. 2003. Discriminative learning for label sequences via boosting. In Advances in Neural Information Processing Systems (NIPS 15), Vancouver.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yasemin Altun</author>
</authors>
<location>Mark Johnson, and</location>
<marker>Altun, </marker>
<rawString>Altun, Yasemin, Mark Johnson, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Loss functions and optimization methods for discriminative learning of label sequences.</title>
<date>2003</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="90590" citStr="Hofmann (2003)" startWordPosition="15090" endWordPosition="15091">1998) introduced a formulation of boosting for ranking problems. The problem we have considered is a special case of the problem in Freund et al. (1998), in that we have considered a binary distinction between candidates (i.e., the best parse vs. other parses), whereas Freund et al. consider learning full or partial orderings over candidates. The improved algorithm that we introduced in Figure 4 is, however, a new algorithm that could perhaps be generalized to the full problem of Freund et al. (1998); we leave this to future research. Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003) describe experiments on tagging tasks using the ExpLoss function, in contrast to the LogLoss function used in Lafferty, McCallum, and Pereira (2001). Altun, Hofmann, and Johnson (2003) describe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy </context>
</contexts>
<marker>Hofmann, 2003</marker>
<rawString>Thomas Hofmann. 2003. Loss functions and optimization methods for discriminative learning of label sequences. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2003), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, Adam L., Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
<author>David Magerman</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: Using richer models for probabilistic parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifth DARPA Speech and Natural Language Workshop,</booktitle>
<location>Harriman, NY.</location>
<contexts>
<context position="9573" citStr="Black et al. 1992" startWordPosition="1438" endWordPosition="1441">imental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm. Section 6 discusses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from the</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1992</marker>
<rawString>Black, Ezra, Frederick Jelinek, John Lafferty, David Magerman, Robert Mercer, and Salim Roukos. 1992. Towards history-based grammars: Using richer models for probabilistic parsing. In Proceedings of the Fifth DARPA Speech and Natural Language Workshop, Harriman, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>Proceedings of the 14th National Conference on Artificial Intelligence,</booktitle>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Menlo Park, CA.</location>
<contexts>
<context position="9717" citStr="Charniak 1997" startWordPosition="1462" endWordPosition="1463">ithm. Section 6 discusses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X--+Y given training examples (xi, yi), where xi Z X, yi Z Y. W</context>
<context position="72587" citStr="Charniak 1997" startWordPosition="12192" endWordPosition="12193">he percentage of sentences with 0 crossing brackets; “2 CBs”is the percentage of sentences with two or more crossing brackets. All the results in this table are for models trained and tested on the same data, using the same evaluation metric. Note that the ExpLoss results are very slightly different from the original results published in Collins (2000). We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and a values tested on development data led to minor improvements in the results. Model &lt; 40 Words (2,245 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1% Collins 1999 88.5% 88.7% 0.92 66.7% 87.1% Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6% ExpLoss 90.2% 90.4% 0.73 71.2% 90.2% Model &lt; 100 Words (2,416 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2% Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% — Collins 1999 88.1% 88.3% 1.06 64.0% 85.1% Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7% ExpLoss 89.6% 89.9% 0.86 68.7% 88.3% Figure 5 Learning curve on development data for the optimal value for a (0.0025). The y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of roun</context>
<context position="79028" citStr="Charniak (1997)" startWordPosition="13269" endWordPosition="13270">. This is a saving of a factor of 2,692 over the naive algorithm. Table 4 shows the value of Savings(a,b) for various values of (a,b). It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected. Even so, there are still savings of a factor of almost 50 in the early stages of the method. 6. Related Work 6.1 History-Based Models with Complex Features Charniak (2000) describes a parser which incorporates additional features into a previously developed parser, that of Charniak (1997). The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods). Our features are in many ways similar to those of Charniak (2000). The model in Charniak (2000) is quite different, however. The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]). Ratnaparkhi (1997) describes the use of maximum-entropy techniques applied to parsing. Log-linear mod</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, Eugene. 1997. Statistical parsing with a context-free grammar and word statistics. Proceedings of the 14th National Conference on Artificial Intelligence, Menlo Park, CA. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="69651" citStr="Charniak (2000)" startWordPosition="11707" endWordPosition="11708"> the final model applied to the test data (section 23 of the treebank). The optimal values were &amp; = 0.0025 and N = 90,386, at which point 11,673 features had nonzero values (note that the feature selection techniques may result in a given feature being updated more than once). The computation took roughly 3–4 hours on a machine with a 1.6 GHz pentium processor and around 2 GB of memory. Table 1 shows results for the method. The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method. The method gives very similar accuracy to the model of Charniak (2000), which also uses a rich set of initial features in addition to Charniak’s (1997) original model. The LogLoss method was too inefficient to run on the full data set. Instead we made some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parse trees) and 52,294 features.15 On an older machine (an order of magnitude or more slower than the machine used for the final tests) the boosting method took 40 minutes for 10,000 rounds on this data set. The LogLoss method took 20 hours to complete 3,500 rounds (a factor of about 85 times slower). This was in spite of various heuristic</context>
<context position="72672" citStr="Charniak 2000" startWordPosition="12206" endWordPosition="12207">ences with two or more crossing brackets. All the results in this table are for models trained and tested on the same data, using the same evaluation metric. Note that the ExpLoss results are very slightly different from the original results published in Collins (2000). We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and a values tested on development data led to minor improvements in the results. Model &lt; 40 Words (2,245 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1% Collins 1999 88.5% 88.7% 0.92 66.7% 87.1% Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6% ExpLoss 90.2% 90.4% 0.73 71.2% 90.2% Model &lt; 100 Words (2,416 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2% Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% — Collins 1999 88.1% 88.3% 1.06 64.0% 85.1% Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7% ExpLoss 89.6% 89.9% 0.86 68.7% 88.3% Figure 5 Learning curve on development data for the optimal value for a (0.0025). The y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting. 53 Computational Linguistics Volume 31, Number 1 Table 2 Peak perform</context>
<context position="78910" citStr="Charniak (2000)" startWordPosition="13253" endWordPosition="13254">tion, the improved algorithm requires total computation that is equivalent to a mere 37.1 passes over the training set. This is a saving of a factor of 2,692 over the naive algorithm. Table 4 shows the value of Savings(a,b) for various values of (a,b). It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected. Even so, there are still savings of a factor of almost 50 in the early stages of the method. 6. Related Work 6.1 History-Based Models with Complex Features Charniak (2000) describes a parser which incorporates additional features into a previously developed parser, that of Charniak (1997). The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods). Our features are in many ways similar to those of Charniak (2000). The model in Charniak (2000) is quite different, however. The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratn</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL-2000, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108,</tech>
<institution>Computer Science Department, Carnegie Mellon University.</institution>
<contexts>
<context position="83861" citStr="Chen and Rosenfeld 1999" startWordPosition="14014" endWordPosition="14017">s and is therefore exponential in size. See also Sha and Pereira (2003) for more recent work on CRFs. Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter values which achieve the global minimum of the objective function in equation (28)) is a plausible alternative to the feature selection approaches described in the current article or to the feature selection methods previously applied to log-linear models. The Gaussian prior (i.e., the Pk a2k/72k penalty) has been found in practice to be very effective in combating overfitting of the parameters to the training data (Chen and Rosenfeld 1999; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al. 2002). The function in equation (28) can be optimized using variants of gradient descent, which in practice require tens or at most hundreds of passes over the training data (see, e.g., Sha and Pereira 2003). Thus log-linear models with a Gaussian prior are likely to be comparable in terms of efficiency to the feature selection approach described in this article (in the experimental section, we showed that for the parsereranking task, the efficient boosting algorithm requires computation that is equivalent to around 40</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Chen, Stanley F., and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Computer Science Department, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="9737" citStr="Collins 1997" startWordPosition="1465" endWordPosition="1466">sses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X--+Y given training examples (xi, yi), where xi Z X, yi Z Y. We define GEN(x)ÎY to</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 16–23, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="12496" citStr="Collins (1999)" startWordPosition="1914" endWordPosition="1915">model. Probabilistic context-free grammars (PCFGs) are one example of a history-based model. The decision sequence (d1... dn) is defined as the sequence of rule expansions in a top-down, leftmost derivation of the tree. The history is equivalent to a partially built tree, and F picks out the nonterminal being expanded (i.e., the leftmost nonterminal in the fringe of this tree), making the assumption that P(diId1... di-1) depends only on the nonterminal being expanded. In the resulting model a tree with rule expansions Qn P(biIAi). (Ai—+bi) is assigned a probability i=1 Our base model, that of Collins (1999), is also a history-based model. It can be considered to be a type of PCFG, where the rules are lexicalized. An example rule would be VP(saw) -&gt; VBD(saw) NP-C(her) NP(today) Lexicalization leads to a very large number of rules; to make the number of parameters manageable, the generation of the right-hand side of a rule is broken down into a number of decisions, as follows: • First the head nonterminal (VBD in the above example) is chosen. • Next, left and right subcategorization frames are chosen ({} and {NP-C}). • Nonterminal sequences to the left and right of the VBD are chosen (an empty seq</context>
<context position="63418" citStr="Collins (1999)" startWordPosition="10650" endWordPosition="10651">inger (2002). We did not test this method; we leave this to future work. 5. Experimental Evaluation 5.1 Generation of Parsing Data Sets We used the Penn Wall Street Journal treebank (Marcus, Santorini, and Marcinkiewicz 1993) as training and test data. Sections 2–21 inclusive (around 40,000 sentences) were used as training data, section 23 was used as the final test set. Of the 40,000 training sentences, the first 36,000 were used as the main training set. The remaining 4,000 sentences were used as development data and to cross-validate the number of rounds (features) in the model. Model 2 of Collins (1999) was used to parse both the training and test data, producing multiple hypotheses for each sentence. We achieved this by disabling dynamic programming in the parser and choosing a relatively narrow beam width of 1,000. The resulting parser returns all parses that fall within the beam. The number of such parses varies sentence by sentence. In order to gain a representative set of training data, the 36,000 training sentences were parsed in 2,000 sentence chunks, each chunk being parsed with a model trained on the remaining 34,000 sentences (this prevented the initial model from being unrealistic</context>
<context position="69491" citStr="Collins (1999)" startWordPosition="11680" endWordPosition="11681">re is as defined above, and zi is the output of the model on the ith development set example. The &amp;, N values which maximized this quantity were used to define the final model applied to the test data (section 23 of the treebank). The optimal values were &amp; = 0.0025 and N = 90,386, at which point 11,673 features had nonzero values (note that the feature selection techniques may result in a given feature being updated more than once). The computation took roughly 3–4 hours on a machine with a 1.6 GHz pentium processor and around 2 GB of memory. Table 1 shows results for the method. The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method. The method gives very similar accuracy to the model of Charniak (2000), which also uses a rich set of initial features in addition to Charniak’s (1997) original model. The LogLoss method was too inefficient to run on the full data set. Instead we made some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parse trees) and 52,294 features.15 On an older machine (an order of magnitude or more slower than the machine used for the final tests) the boosting method took 40 minutes for 10,00</context>
<context position="72629" citStr="Collins 1999" startWordPosition="12199" endWordPosition="12200">brackets; “2 CBs”is the percentage of sentences with two or more crossing brackets. All the results in this table are for models trained and tested on the same data, using the same evaluation metric. Note that the ExpLoss results are very slightly different from the original results published in Collins (2000). We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and a values tested on development data led to minor improvements in the results. Model &lt; 40 Words (2,245 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1% Collins 1999 88.5% 88.7% 0.92 66.7% 87.1% Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6% ExpLoss 90.2% 90.4% 0.73 71.2% 90.2% Model &lt; 100 Words (2,416 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2% Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% — Collins 1999 88.1% 88.3% 1.06 64.0% 85.1% Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7% ExpLoss 89.6% 89.9% 0.86 68.7% 88.3% Figure 5 Learning curve on development data for the optimal value for a (0.0025). The y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting. 53 Computational Linguisti</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning (ICML 2000),</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="9278" citStr="Collins (2000)" startWordPosition="1399" endWordPosition="1400">or the reranking methods. Section 4 describes how these approaches can be generalized to ranking problems. We introduce loss functions for boosting and MRF approaches and discuss optimization methods. We also derive the efficient algorithm for boosting in this section. Section 5 gives experimental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm. Section 6 discusses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other lang</context>
<context position="72328" citStr="Collins (2000)" startWordPosition="12147" endWordPosition="12148">ther lexicalization were included. 52 Collins and Koo Discriminative Reranking for NLP Table 1 Results on section 23 of the WSJ Treebank. “LR”is labeled recall; “LP”is labeled precision; “CBs”is the average number of crossing brackets per sentence; “0 CBs”is the percentage of sentences with 0 crossing brackets; “2 CBs”is the percentage of sentences with two or more crossing brackets. All the results in this table are for models trained and tested on the same data, using the same evaluation metric. Note that the ExpLoss results are very slightly different from the original results published in Collins (2000). We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and a values tested on development data led to minor improvements in the results. Model &lt; 40 Words (2,245 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1% Collins 1999 88.5% 88.7% 0.92 66.7% 87.1% Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6% ExpLoss 90.2% 90.4% 0.73 71.2% 90.2% Model &lt; 100 Words (2,416 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2% Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% — Collins 1999 88.1% 88.3% 1.06 64.0% 85.1% Char</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Collins, Michael. 2000. Discriminative reranking for natural language parsing. In Proceedings of the 17th International Conference on Machine Learning (ICML 2000), Stanford, CA. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5063" citStr="Collins (2002" startWordPosition="771" endWordPosition="772">er a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The baseline model achieved 88.2% F-measure on this task. The new model achieves 89.75% Fmeasure, a 13% relative decrease in F-measure error. Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation. See Collins (2002a) for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati (2001) for the application of boosting techniques for ranking in the context of natural language generation. The article also introduces a new, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data. Other NLP tasks are likely to have similar characteristics in terms of sparsity. Experiments show an efficiency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting appro</context>
<context position="91391" citStr="Collins (2002" startWordPosition="15214" endWordPosition="15215">scribe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question. Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces. Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply support vector machines (SVMs) using tree-adjoining grammar (Joshi, Levy, and T</context>
<context position="92934" citStr="Collins 2002" startWordPosition="15443" endWordPosition="15444">he different algorithms. 7. Conclusions This article has introduced a new algorithm, based on boosting approaches in machine learning, for ranking problems in natural language processing. The approach gives a 13% relative reduction in error on parsing Wall Street Journal data. While in this article the experimental focus has been on parsing, many other problems in natural language processing or speech recognition can also be framed as reranking problems, so the methods described should be quite broadly applicable. The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker, Rambow, and Rogati 2001). The key characteristics of the approach are the use of global features and of a training criterion (optimization 62 Collins and Koo Discriminative Reranking for NLP problem) that is discriminative and closely related to the task at hand (i.e., parse accuracy). In addition, the article introduced a new algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data that we use. Other NLP tasks are likely to have similar characteristics in terms of sparsity. Experiments s</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002a. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP 2002,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5063" citStr="Collins (2002" startWordPosition="771" endWordPosition="772">er a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The baseline model achieved 88.2% F-measure on this task. The new model achieves 89.75% Fmeasure, a 13% relative decrease in F-measure error. Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation. See Collins (2002a) for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati (2001) for the application of boosting techniques for ranking in the context of natural language generation. The article also introduces a new, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data. Other NLP tasks are likely to have similar characteristics in terms of sparsity. Experiments show an efficiency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting appro</context>
<context position="91391" citStr="Collins (2002" startWordPosition="15214" endWordPosition="15215">scribe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question. Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces. Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply support vector machines (SVMs) using tree-adjoining grammar (Joshi, Levy, and T</context>
<context position="92934" citStr="Collins 2002" startWordPosition="15443" endWordPosition="15444">he different algorithms. 7. Conclusions This article has introduced a new algorithm, based on boosting approaches in machine learning, for ranking problems in natural language processing. The approach gives a 13% relative reduction in error on parsing Wall Street Journal data. While in this article the experimental focus has been on parsing, many other problems in natural language processing or speech recognition can also be framed as reranking problems, so the methods described should be quite broadly applicable. The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker, Rambow, and Rogati 2001). The key characteristics of the approach are the use of global features and of a training criterion (optimization 62 Collins and Koo Discriminative Reranking for NLP problem) that is discriminative and closely related to the task at hand (i.e., parse accuracy). In addition, the article introduced a new algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data that we use. Other NLP tasks are likely to have similar characteristics in terms of sparsity. Experiments s</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002b. Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm. In Proceedings of EMNLP 2002, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2004</date>
<booktitle>New Developments in Parsing Technology.</booktitle>
<editor>In Harry Bunt, John Carroll, and Giorgio Satta, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="92122" citStr="Collins (2004)" startWordPosition="15319" endWordPosition="15320">ergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces. Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply support vector machines (SVMs) using tree-adjoining grammar (Joshi, Levy, and Takahashi 1975) features to the parsing data sets we have described in this article, with good empirical results. See Collins (2004) for a discussion of many of these methods, including an overview of statistical bounds for the boosting, perceptron, and SVM methods, as well as a discussion of the computational issues involved in the different algorithms. 7. Conclusions This article has introduced a new algorithm, based on boosting approaches in machine learning, for ranking problems in natural language processing. The approach gives a 13% relative reduction in error on parsing Wall Street Journal data. While in this article the experimental focus has been on parsing, many other problems in natural language processing or sp</context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Collins, Michael. 2004. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In Harry Bunt, John Carroll, and Giorgio Satta, editors, New Developments in Parsing Technology. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS 14),</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="91428" citStr="Collins and Duffy (2001" startWordPosition="15218" endWordPosition="15221">ing methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question. Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces. Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply support vector machines (SVMs) using tree-adjoining grammar (Joshi, Levy, and Takahashi 1975) features to the parsin</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, Michael and Nigel Duffy. 2001. Convolution kernels for natural language. In Advances in Neural Information Processing Systems (NIPS 14), Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia.</location>
<marker>Collins, Duffy, 2002</marker>
<rawString>Collins, Michael, and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<date>2002</date>
<booktitle>Logistic regression, AdaBoost and Bregman distances. Machine Learning,</booktitle>
<pages>48--1</pages>
<marker>Collins, Schapire, Singer, 2002</marker>
<rawString>Collins, Michael, Robert E. Schapire, and Yoram Singer. 2002. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48(1/2/3):253–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Vincent Della Pietra Stephen</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Pietra, Stephen, Lafferty, 1997</marker>
<rawString>Della Pietra, Stephen, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel Duffy</author>
<author>David Helmbold</author>
</authors>
<title>Potential boosters?</title>
<date>1999</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS 12),</booktitle>
<location>Denver.</location>
<contexts>
<context position="8420" citStr="Duffy and Helmbold 1999" startWordPosition="1275" endWordPosition="1278"> boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods. Section 4 describes how these approaches can be generalized to ranking problems. We introduce loss functions for boosting and MRF approaches and discuss optimization methods. We also derive the efficient algorithm for boosting in this section. Section 5 gives experimental results, investigating the performance improvements on pa</context>
<context position="17443" citStr="Duffy and Helmbold 1999" startWordPosition="2720" endWordPosition="2723">, Berger, Della Pietra, and Della Pietra (1996) for an early article which introduces the models and motivates them. Boosting approaches to classification have received considerable attention in the machine-learning community since the introduction of AdaBoost by Freund and Schapire (1997). Boosting algorithms, and in particular the relationship between boosting algorithms and maximum-entropy models, are perhaps not familiar topics in the NLP literature. However there has recently been much work drawing connections between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002); in this section we review this work. Much of this work has focused on binary classification problems, and this section is also restricted to problems of this type. Later in the article we show how several of the ideas can be carried across to reranking problems. 3.1 Binary Classification Problems The general setup for binary classification problems is as follows: • The “input domain”(set of possible inputs) is X. • The “output domain”(set of possible labels) is simply a set of two labels, Y = {-</context>
<context position="19756" citStr="Duffy and Helmbold 1999" startWordPosition="3132" endWordPosition="3135">epresented as vectors (�(x) in some m-dimensional vector space, and the parameters a¯ define a hyperplane which passes through the origin4 of the space and has a¯ as its normal. Points lying on one side of this hyperplane are classified as +1; points on the other side are classified as —1. The central question in learning is how to set the parameters ¯a, given the training examples bðx1, y1Þ, ðx2, y2Þ, ... ,ðxn, ynÞÀ. Logistic regression and boosting involve different algorithms and criteria for training the parameters ¯a, but recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) has shown that the methods have strong similarities. The next section describes parameter estimation methods. 3.2 Loss Functions for Logistic Regression and Boosting A central idea in both logistic regression and boosting is that of a loss function, which drives the parameter estimation methods of the two approaches. This section describes loss functions for binary classification. Later in the article, we introduce loss functions for reranking tasks which are closely related to the loss functions</context>
<context position="36678" citStr="Duffy and Helmbold (1999)" startWordPosition="5933" endWordPosition="5936">y) method to find a function with a low error rate with respect to the distribution. Freund and Schapire (1997) show that provided that at each round of boosting the weak learner returns a feature with greater than (50 + e) % accuracy for some fixed e, the number of training errors falls exponentially quickly with the number of rounds of boosting. This fast drop in training errors translates to statistical bounds on generalization performance (Freund and Schapire 1997). 7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman et al. (2000) and Duffy and Helmbold (1999). 36 Collins and Koo Discriminative Reranking for NLP Under this view of boosting, the feature selection methods in this article are a particularly simple case in which the weak learner can afford to exhaustively search through the space of possible features. Future work on reranking approaches might consider other approaches—such as boosting of decision trees—which can effectively consider more complex features. 4. Reranking Approaches This section describes how the ideas from classification problems can be extended to reranking tasks. A baseline statistical parser is used to generate N-best </context>
</contexts>
<marker>Duffy, Helmbold, 1999</marker>
<rawString>Duffy, Nigel and David Helmbold. 1999. Potential boosters? In Advances in Neural Information Processing Systems (NIPS 12), Denver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>In Machine Learning: Proceedings of the 15th International Conference,</booktitle>
<location>Madison, WI.</location>
<contexts>
<context position="941" citStr="Freund et al. (1998)" startWordPosition="140" endWordPosition="143"> input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75% F-measure, a 13% relative decrease in Fmeasure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for</context>
<context position="4101" citStr="Freund et al. (1998)" startWordPosition="626" endWordPosition="629">idate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). The algorithm can be viewed as a feature selection method, optimizing a particular loss function (the exponential loss function) that has been studied in the boosting literature. We applied the boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus, Santorini, and Marcinkiewicz 1993). The method combines the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The baseline model achieved 88.2% F-measure on this task. The new model achieves 89.75% Fmeas</context>
<context position="37940" citStr="Freund et al. (1998)" startWordPosition="6125" endWordPosition="6128">est data sentences. Each candidate parse for a sentence is represented as a feature vector which includes the log-likelihood under the baseline model, as well as a large number of additional features. The additional features can in principle be any predicates over sentence/tree pairs. Evidence from the initial loglikelihood and the additional features is combined using a linear model. Parameter estimation becomes a problem of learning how to combine these different sources of information. The boosting algorithm we use is related to the generalization of boosting methods to ranking problems in Freund et al. (1998); we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994), Papineni, Roukos, and Ward (1997, 1998), Johnson et al. (1999), Riezler et al. (2002), and Och and Ney (2002). Section 4.1 gives a formal definition of the reranking problem. Section 4.2 introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss functions in section 3.2. Section 4.3 describes a general approach to feature selection methods with these loss functions. Section 4.4 describes a first algorithm for the exponential loss (ExpLoss) function; s</context>
<context position="45233" citStr="Freund et al. (1998)" startWordPosition="7431" endWordPosition="7434"> Number 1 log-likelihood. Some manipulation shows that the negative log-likelihood is a function of the margins on training data: eFðxi,1, ¯aÞ—log ni eFðxi,j, ¯aÞ �j — —log �j1/41 ini e ðFðxi,1, aÞ—Fðxi,j, aÞÞ nini logð1 þXe ðFðxi,1,¯aÞ—Fðxi,j, aÞÞ 1/4Xlogð1 þ X emi,jð¯aÞÞ ð9Þ j1/42 j1/42 i XLogLoss ð¯aÞ 1/4 i X1/4 i Note the similarity of equation (9) to the LogLoss function for classification in equation (4). 4.2.3 Exponential Loss. The next loss function is based on the boosting method described in Schapire and Singer (1999). It is a special case of the general ranking methods described in Freund et al. (1998), with the ranking “feedback”being a simple binary distinction between the highest-scoring parse and the other parses. Again, the loss function is a function of the margins on training data: ni ni XExpLossð¯aÞ 1/4 X Xe�ðFðxi,1, ¯aÞ�Fðxi,j, ¯aÞÞ 1/4 X eMi,jð¯aÞ ð10Þ ij1/42 i j1/42 Note the similarity of equation (10) to the ExpLoss function for classification in equation (6). It can be shown that ExpLossð¯aÞ &gt; Errorð¯aÞ, so that minimizing ExpLossð¯aÞ is closely related to minimizing the number of ranking errors.11 This follows from the fact that for any x, e�x &gt; gx &lt; 01, and therefore that X n</context>
<context position="89981" citStr="Freund et al. (1998)" startWordPosition="14989" endWordPosition="14992">izer; Riezler and Vasserman use a one-norm regularizer. Finally, note that other feature selection methods have been proposed within the machine-learning community: for example, “filter”methods, in which feature selection is performed as a preprocessing step before applying a learning method, 61 Computational Linguistics Volume 31, Number 1 and backward selection methods (Koller and Sahami 1996), in which initially all features are added to the model and features are then incrementally removed from the model. 6.5 Boosting, Perceptron, and Support Vector Machine Approaches for Ranking Problems Freund et al. (1998) introduced a formulation of boosting for ranking problems. The problem we have considered is a special case of the problem in Freund et al. (1998), in that we have considered a binary distinction between candidates (i.e., the best parse vs. other parses), whereas Freund et al. consider learning full or partial orderings over candidates. The improved algorithm that we introduced in Figure 4 is, however, a new algorithm that could perhaps be generalized to the full problem of Freund et al. (1998); we leave this to future research. Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofma</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Freund, Yoav, Raj Iyer, Robert E. Schapire, and Yoram Singer. 1998. An efficient boosting algorithm for combining preferences. In Machine Learning: Proceedings of the 15th International Conference, Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and an application to boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>55</volume>
<issue>1</issue>
<contexts>
<context position="17110" citStr="Freund and Schapire (1997)" startWordPosition="2674" endWordPosition="2677"> we review two methods for binary classification problems: logistic regression (or maximum-entropy) models and boosting. These methods form the basis for the reranking approaches described in later sections of the article. Maximum-entropy models are a very popular method within the computational linguistics community; see, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article which introduces the models and motivates them. Boosting approaches to classification have received considerable attention in the machine-learning community since the introduction of AdaBoost by Freund and Schapire (1997). Boosting algorithms, and in particular the relationship between boosting algorithms and maximum-entropy models, are perhaps not familiar topics in the NLP literature. However there has recently been much work drawing connections between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002); in this section we review this work. Much of this work has focused on binary classification problems, and this section is also restricted to problems of this type. Lat</context>
<context position="31568" citStr="Freund and Schapire (1997)" startWordPosition="5094" endWordPosition="5097"> Feature selection methods can probably be motivated either from a Bayesian perspective (through a prior favoring models with a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit perspective (models with fewer parameters are less likely to fit the data by chance), but this requires additional research. The statistical justification for boosting approaches is quite different. Boosting algorithms were originally developed within the PAC framework (Valiant 1984) for machine learning, specifically to address questions regarding the equivalence of weak and strong learning. Freund and Schapire (1997) originally introduced AdaBoost and gave a first set of statistical guarantees for the algorithm. Schapire et al. (1998) gave a second set of guarantees based on the analysis of margins on training examples. Both papers assume that a fixed distribution D(x, y) is generating both training and test examples and that the goal is to find a hypothesis with a small number of expected errors with respect to this distribution. The form of the distribution is not assumed to be known, and in this sense the guarantees are nonparametric, or “distribution free.”Freund and Schapire (1997) show that if the w</context>
<context position="36164" citStr="Freund and Schapire (1997)" startWordPosition="5847" endWordPosition="5850">learning method, which attempts to return a feature (a decision tree, or a neural network parameter setting) which has a relatively low error rate with respect to the distribution. The feature that is returned is then incorporated into the linear combination of features. The algorithm which generates a classifier given a distribution over the examples (for example, the decision tree induction method) is usually referred to as “the weak learner.”The weak learner generally uses an approximate (for example, greedy) method to find a function with a low error rate with respect to the distribution. Freund and Schapire (1997) show that provided that at each round of boosting the weak learner returns a feature with greater than (50 + e) % accuracy for some fixed e, the number of training errors falls exponentially quickly with the number of rounds of boosting. This fast drop in training errors translates to statistical bounds on generalization performance (Freund and Schapire 1997). 7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman et al. (2000) and Duffy and Helmbold (1999). 36 Collins and Koo Discriminative Reranking for NLP Under this view of boosting, the</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Freund, Yoav and Robert E. Schapire. 1997. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="91315" citStr="Freund and Schapire (1999)" startWordPosition="15201" endWordPosition="15204">ion used in Lafferty, McCallum, and Pereira (2001). Altun, Hofmann, and Johnson (2003) describe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question. Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces. Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to ranking problems and apply sup</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Freund, Yoav and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
</authors>
<title>Additive logistic regression: A statistical view of boosting.</title>
<date>2000</date>
<journal>Annals of Statistics,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="36648" citStr="Friedman et al. (2000)" startWordPosition="5928" endWordPosition="5931">oximate (for example, greedy) method to find a function with a low error rate with respect to the distribution. Freund and Schapire (1997) show that provided that at each round of boosting the weak learner returns a feature with greater than (50 + e) % accuracy for some fixed e, the number of training errors falls exponentially quickly with the number of rounds of boosting. This fast drop in training errors translates to statistical bounds on generalization performance (Freund and Schapire 1997). 7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman et al. (2000) and Duffy and Helmbold (1999). 36 Collins and Koo Discriminative Reranking for NLP Under this view of boosting, the feature selection methods in this article are a particularly simple case in which the weak learner can afford to exhaustively search through the space of possible features. Future work on reranking approaches might consider other approaches—such as boosting of decision trees—which can effectively consider more complex features. 4. Reranking Approaches This section describes how the ideas from classification problems can be extended to reranking tasks. A baseline statistical pars</context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2000</marker>
<rawString>Friedman, Jerome H., Trevor Hastie, and Robert Tibshirani. 2000. Additive logistic regression: A statistical view of boosting. Annals of Statistics, 38(2):337–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Meeting of the North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conference (HLT-NAACL</booktitle>
<pages>103--110</pages>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="9760" citStr="Henderson 2003" startWordPosition="1468" endWordPosition="1469"> more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X--+Y given training examples (xi, yi), where xi Z X, yi Z Y. We define GEN(x)ÎY to be the set of candidat</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>Henderson, James. 2003. Inducing history representations for broad coverage statistical parsing. In Proceedings of the Joint Meeting of the North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conference (HLT-NAACL 2003), pages 103–110, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klauss U Hoffgen</author>
<author>Kevin S van Horn</author>
<author>Hans U Simon</author>
</authors>
<title>Robust trainability of single neurons.</title>
<date>1995</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>50--114</pages>
<marker>Hoffgen, van Horn, Simon, 1995</marker>
<rawString>Hoffgen, Klauss U., Kevin S. van Horn, and Hans U. Simon. 1995. Robust trainability of single neurons. Journal of Computer and System Sciences, 50:114–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based”grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL 1999, College Park, MD. Computational Linguistics Volume 31, Number</booktitle>
<volume>1</volume>
<contexts>
<context position="6429" citStr="Johnson et al. 1999" startWordPosition="981" endWordPosition="984"> features. The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds. The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the “naive”implementation). The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before. In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al. 2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks. (Log-linear models are often referred to as maximum-entropy models in the NLP literature.) Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-linear models and 26 Collins and Koo Discriminative Reranking for NLP bo</context>
<context position="10572" citStr="Johnson et al. (1999)" startWordPosition="1598" endWordPosition="1601">ular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X--+Y given training examples (xi, yi), where xi Z X, yi Z Y. We define GEN(x)ÎY to be the set of candidates for a given input x. In the parsing problem x is a sentence, and 1 Note, however, that log-linear models which employ regularization methods instead of feature selection—see, for example, Johnson et al. (1999) and Lafferty, McCallum, and Pereira (2001)—are likely to be comparable in terms of efficiency to our feature selection approach. See section 6.3 for more discussion. 27 Computational Linguistics Volume 31, Number 1 GEN(x) is a set of candidate trees for that sentence. A particular characteristic of the problem is the complexity of GEN(x) : GEN(x) can be very large, and each member of GEN(x) has a rich internal structure. This contrasts with “typical”classification problems in which GEN(x) is a fixed, small set, for example, f-1,+11 in binary classification problems. In probabilistic approache</context>
<context position="38120" citStr="Johnson et al. (1999)" startWordPosition="6152" endWordPosition="6155"> additional features. The additional features can in principle be any predicates over sentence/tree pairs. Evidence from the initial loglikelihood and the additional features is combined using a linear model. Parameter estimation becomes a problem of learning how to combine these different sources of information. The boosting algorithm we use is related to the generalization of boosting methods to ranking problems in Freund et al. (1998); we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994), Papineni, Roukos, and Ward (1997, 1998), Johnson et al. (1999), Riezler et al. (2002), and Och and Ney (2002). Section 4.1 gives a formal definition of the reranking problem. Section 4.2 introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss functions in section 3.2. Section 4.3 describes a general approach to feature selection methods with these loss functions. Section 4.4 describes a first algorithm for the exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm for the case of ExpLoss. Finally, section 4.6 describes issues in feature selection algorithms for the LogLoss function. 4.1 Proble</context>
<context position="43890" citStr="Johnson et al. (1999)" startWordPosition="7195" endWordPosition="7198">e define the margin for each example xi,j such that i = 1,. . . , n, j = 2,. . . , ni, as Mij(¯a) = F(xi,1, ¯a) — F(xi,j, ¯a) Thus Mij(¯a) is the difference in ranking score between the correct parse of a sentence and a competing parse xi,j. It follows that ni XError(¯a) = X gMij(¯a) &lt; 0Ä i j=2 The ranking error is zero if all margins are positive. The loss functions we discuss all turn out to be direct functions of the margins on training examples. 4.2.2 Log-Likelihood. The first loss function is that suggested by Markov random fields. As suggested by Ratnaparkhi, Roukos, and Ward (1994) and Johnson et al. (1999), the conditional probability of xi,q being the correct parse for the ith sentence is defined as eF(xi,q, ¯a) P(xi,q I si, ¯a) = ni P j=1 Given a new test sentence s, with parses xj for j = 1,. . . , N, the most likely tree is eF(xi,j, ¯a) arg max xj eF(xq,¯a) = arg max F(xj, ¯a) xj eF(xj,¯a) N P q=1 Hence once the parameters are trained, the ranking function is used to order candidate trees for test examples. The log-likelihood of the training data is X Xlog P(xi,1 I si, ¯a) = eF(xi,1, ¯a) i i log ni (xi,j, ¯a) Pj=1 eF Under maximum-likelihood estimation, the parameters a¯ would be set to max</context>
<context position="81836" citStr="Johnson et al. (1999)" startWordPosition="13703" endWordPosition="13706"> estimation, in which an estimate of the denominator is required, and Monte Carlo methods have been proposed (Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a technique for estimation of this value. Our sense is that these methods can be computationally expensive. Notice that the joint likelihood in equation (27) is not a direct function of the margins on training examples, and its relation to error rate is therefore not so clear as in the discriminative approaches described in this article. 6.3 Conditional Log-Linear Models Ratnaparkhi, Roukos, and Ward (1994), Johnson et al. (1999), and Riezler et al. (2002) suggest training log-linear models (i.e., the LogLoss function in equation (9)) for parsing problems. Ratnaparkhi, Roukos, and Ward (1994) use feature selection techniques for the task. Johnson et al. (1999) and Riezler et al. (2002) do not use a feature selection technique, employing instead an objective function which includes a 59 Computational Linguistics Volume 31, Number 1 Gaussian prior on the parameter values, thereby penalizing parameter values which become too large: 2 ¯a* 1/4 arg minðLogLossð¯aÞ þ E ak ð28Þ k1/40 ...m 72k Closed-form updates under iterati</context>
<context position="83882" citStr="Johnson et al. 1999" startWordPosition="14018" endWordPosition="14021">ntial in size. See also Sha and Pereira (2003) for more recent work on CRFs. Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter values which achieve the global minimum of the objective function in equation (28)) is a plausible alternative to the feature selection approaches described in the current article or to the feature selection methods previously applied to log-linear models. The Gaussian prior (i.e., the Pk a2k/72k penalty) has been found in practice to be very effective in combating overfitting of the parameters to the training data (Chen and Rosenfeld 1999; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al. 2002). The function in equation (28) can be optimized using variants of gradient descent, which in practice require tens or at most hundreds of passes over the training data (see, e.g., Sha and Pereira 2003). Thus log-linear models with a Gaussian prior are likely to be comparable in terms of efficiency to the feature selection approach described in this article (in the experimental section, we showed that for the parsereranking task, the efficient boosting algorithm requires computation that is equivalent to around 40 passes over the trai</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based”grammars. In Proceedings of ACL 1999, College Park, MD. Computational Linguistics Volume 31, Number 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon S Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Science</journal>
<volume>10</volume>
<pages>1--136</pages>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, Aravind K., Leon S. Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Science 10, no. 1:136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Mehran Sahami</author>
</authors>
<title>Toward optimal feature selection.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th International Conference on Machine Learning (ICML),</booktitle>
<pages>284--292</pages>
<location>Bari, Italy,</location>
<contexts>
<context position="89759" citStr="Koller and Sahami 1996" startWordPosition="14955" endWordPosition="14958">e kinds of approaches for the parsing problem. Another difference is that both McCallum, and Riezler and Vasserman, describe approaches that use a regularizer in addition to feature selection: McCallum uses a two-norm regularizer; Riezler and Vasserman use a one-norm regularizer. Finally, note that other feature selection methods have been proposed within the machine-learning community: for example, “filter”methods, in which feature selection is performed as a preprocessing step before applying a learning method, 61 Computational Linguistics Volume 31, Number 1 and backward selection methods (Koller and Sahami 1996), in which initially all features are added to the model and features are then incrementally removed from the model. 6.5 Boosting, Perceptron, and Support Vector Machine Approaches for Ranking Problems Freund et al. (1998) introduced a formulation of boosting for ranking problems. The problem we have considered is a special case of the problem in Freund et al. (1998), in that we have considered a binary distinction between candidates (i.e., the best parse vs. other parses), whereas Freund et al. consider learning full or partial orderings over candidates. The improved algorithm that we introdu</context>
</contexts>
<marker>Koller, Sahami, 1996</marker>
<rawString>Koller, Daphne, and Mehran Sahami. 1996. Toward optimal feature selection. In Proceedings of the 13th International Conference on Machine Learning (ICML), pages 284–292, Bari, Italy, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
</authors>
<title>Additive models, boosting, and inference for generalized divergences.</title>
<date>1999</date>
<booktitle>In Proceedings of the 12th Annual Conference on Computational Learning Theory (COLT’99),</booktitle>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="17418" citStr="Lafferty 1999" startWordPosition="2718" endWordPosition="2719">ee, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article which introduces the models and motivates them. Boosting approaches to classification have received considerable attention in the machine-learning community since the introduction of AdaBoost by Freund and Schapire (1997). Boosting algorithms, and in particular the relationship between boosting algorithms and maximum-entropy models, are perhaps not familiar topics in the NLP literature. However there has recently been much work drawing connections between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002); in this section we review this work. Much of this work has focused on binary classification problems, and this section is also restricted to problems of this type. Later in the article we show how several of the ideas can be carried across to reranking problems. 3.1 Binary Classification Problems The general setup for binary classification problems is as follows: • The “input domain”(set of possible inputs) is X. • The “output domain”(set of possible labels) is simply a </context>
<context position="19731" citStr="Lafferty 1999" startWordPosition="3130" endWordPosition="3131">xamples x are represented as vectors (�(x) in some m-dimensional vector space, and the parameters a¯ define a hyperplane which passes through the origin4 of the space and has a¯ as its normal. Points lying on one side of this hyperplane are classified as +1; points on the other side are classified as —1. The central question in learning is how to set the parameters ¯a, given the training examples bðx1, y1Þ, ðx2, y2Þ, ... ,ðxn, ynÞÀ. Logistic regression and boosting involve different algorithms and criteria for training the parameters ¯a, but recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) has shown that the methods have strong similarities. The next section describes parameter estimation methods. 3.2 Loss Functions for Logistic Regression and Boosting A central idea in both logistic regression and boosting is that of a loss function, which drives the parameter estimation methods of the two approaches. This section describes loss functions for binary classification. Later in the article, we introduce loss functions for reranking tasks which are closely rela</context>
</contexts>
<marker>Lafferty, 1999</marker>
<rawString>Lafferty, John. 1999. Additive models, boosting, and inference for generalized divergences. In Proceedings of the 12th Annual Conference on Computational Learning Theory (COLT’99), Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML</booktitle>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001,</rawString>
</citation>
<citation valid="false">
<location>Williamstown, MA.</location>
<marker></marker>
<rawString>Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Lebanon</author>
<author>John Lafferty</author>
</authors>
<title>Boosting and maximum likelihood for exponential models.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS 14),</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="8481" citStr="Lebanon and Lafferty 2001" startWordPosition="1284" endWordPosition="1287">icient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods. Section 4 describes how these approaches can be generalized to ranking problems. We introduce loss functions for boosting and MRF approaches and discuss optimization methods. We also derive the efficient algorithm for boosting in this section. Section 5 gives experimental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameter</context>
<context position="17504" citStr="Lebanon and Lafferty 2001" startWordPosition="2729" endWordPosition="2732">y article which introduces the models and motivates them. Boosting approaches to classification have received considerable attention in the machine-learning community since the introduction of AdaBoost by Freund and Schapire (1997). Boosting algorithms, and in particular the relationship between boosting algorithms and maximum-entropy models, are perhaps not familiar topics in the NLP literature. However there has recently been much work drawing connections between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002); in this section we review this work. Much of this work has focused on binary classification problems, and this section is also restricted to problems of this type. Later in the article we show how several of the ideas can be carried across to reranking problems. 3.1 Binary Classification Problems The general setup for binary classification problems is as follows: • The “input domain”(set of possible inputs) is X. • The “output domain”(set of possible labels) is simply a set of two labels, Y = {-1, +1}.3 • The training set is an array of n labeled examples</context>
<context position="19817" citStr="Lebanon and Lafferty 2001" startWordPosition="3141" endWordPosition="3144">ace, and the parameters a¯ define a hyperplane which passes through the origin4 of the space and has a¯ as its normal. Points lying on one side of this hyperplane are classified as +1; points on the other side are classified as —1. The central question in learning is how to set the parameters ¯a, given the training examples bðx1, y1Þ, ðx2, y2Þ, ... ,ðxn, ynÞÀ. Logistic regression and boosting involve different algorithms and criteria for training the parameters ¯a, but recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) has shown that the methods have strong similarities. The next section describes parameter estimation methods. 3.2 Loss Functions for Logistic Regression and Boosting A central idea in both logistic regression and boosting is that of a loss function, which drives the parameter estimation methods of the two approaches. This section describes loss functions for binary classification. Later in the article, we introduce loss functions for reranking tasks which are closely related to the loss functions for classification tasks. First, consider a logistic regress</context>
</contexts>
<marker>Lebanon, Lafferty, 2001</marker>
<rawString>Lebanon, Guy and John Lafferty. 2001. Boosting and maximum likelihood for exponential models. In Advances in Neural Information Processing Systems (NIPS 14), Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning (CoNNL-2002),</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="21723" citStr="Malouf 2002" startWordPosition="3446" endWordPosition="3447">g LogLoss(¯a) with respect to ¯a, for example, generalized or improved iterative scaling (Berger, Della Pietra, and 4 It might seem to be a restriction to have the hyperplane passing through the origin of the space. However if a constant “bias”feature hmþ1ðxÞ 1/4 1 for all x is added to the representation, a hyperplane passing through the origin in this new space is equivalent to a hyperplane in general position in the original m-dimensional space. 31 Computational Linguistics Volume 31, Number 1 Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient methods (Malouf 2002). In the next section we describe feature selection methods, as described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra, and Lafferty (1997). Once the parameters a¯ are estimated on training examples, the output for an example x is the most likely label under the model, arg max P(y l x, ¯a) = arg max yF(x, ¯a) = sign(F(x, ¯a)) (5) yZY yZ{—1,+1} where as before, sign (z) = 1 if z &gt; 0, sign (z) = —1 otherwise. Thus we see that the logistic regression model implements a hyperplane classifier. In boosting, a different loss function is used, namely, ExpLoss(¯a), wh</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Malouf, Robert. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference on Natural Language Learning (CoNNL-2002), Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llew Mason</author>
<author>Peter L Bartlett</author>
<author>Jonathan Baxter</author>
</authors>
<title>Direct optimization of margins improves generalization in combined classifiers.</title>
<date>1999</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS 12),</booktitle>
<location>Denver.</location>
<marker>Mason, Bartlett, Baxter, 1999</marker>
<rawString>Mason, Llew, Peter L. Bartlett, and Jonathan Baxter. 1999. Direct optimization of margins improves generalization in combined classifiers. In Advances in Neural Information Processing Systems (NIPS 12), Denver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Efficiently inducing features of conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI 2003),</booktitle>
<location>Acapulco.</location>
<contexts>
<context position="8016" citStr="McCallum (2003)" startWordPosition="1217" endWordPosition="1218">entropy models. The earlier methods for maximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of clas</context>
<context position="26012" citStr="McCallum 2003" startWordPosition="4208" endWordPosition="4209"> is a plot of f (z) = e—z for z = [—1.5...1.5]; LogLoss shows a similar plot for f (z) = log(1 + e—z); Error is a plot of f (z) = Qz &lt; 01. 33 Computational Linguistics Volume 31, Number 1 a¯ are zero). Roughly speaking, the motivation for using a small number of features is the hope that this will prevent overfitting in the models. Feature selection methods have been proposed in the maximum-entropy literature by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004). The most basic approach—for example see Ratnaparkhi, Roukos, and Ward (1994) and Berger, Della Pietra, and Della Pietra (1996)—involves selection of a single feature at each iteration, followed by an update to the entire model, as follows: Step 1: Throughout the algorithm, maintain a set of active features. Initialize this set to be empty. Step 2: Choose a feature from outside of the set of active features which has the largest estimated impact in terms of reducing the loss function LogLoss, and add this to the active feature set. Step 3: Minimi</context>
<context position="85247" citStr="McCallum 2003" startWordPosition="14234" endWordPosition="14235">on approach leads to around 11,000 (2%) of the features in our model having nonzero parameter values, log-linear models with Gaussian priors typically have very few nonzero parameters (see, e.g., Riezler and Vasserman 2004). This may be important in some domains, for example, those in which there are a very large number of features and this large number leads to difficulties in terms of memory requirements or computation time. 6.4 Feature Selection Methods A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems. Earlier work (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested methods that added a feature at a time to the model and updated all parameters in the current model at each step (for more detail, see section 3.3). 60 Collins and Koo Discriminative Reranking for NLP Assuming that selection of a feature takes one pass over the training set and that fitting a model takes p passes over the training </context>
<context position="86535" citStr="McCallum 2003" startWordPosition="14445" endWordPosition="14446">f is the number of features selected. In our experiments, f z 10,000. It is difficult to estimate the value for p, but assuming (very conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over the training set. This is around 1,000 times as much computation as that required for the efficient boosting algorithm applied to our data, suggesting that the feature selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the parsing task. More recent work (McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) has considered methods for speeding up the feature selection methods described in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997). McCallum (2003) and Riezler and Vasserman (2004) describe approaches that add k features at each step, where k is some constant greater than one. The running time for these methods is therefore O(f x (p + 1)1k). Riezler and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal performance. McCallum (2003) uses a value </context>
<context position="88863" citStr="McCallum (2003)" startWordPosition="14826" endWordPosition="14827"> a single feature is updated at each round of feature selection). The latter approximation is particularly important, as it leads to the efficient algorithm in Figure 4, which avoids a pass over the training set at each iteration of feature selection (note that in sparse feature spaces, f rounds of feature selection in our approach can take considerably fewer than f passes over the training set, in contrast to other work on feature selection within log-linear models). Note that there are other important differences among the approaches. Both Della Pietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods that induce conjunctions of “base”features, in a way similar to decision tree learners. Thus a relatively small number of base features can lead to a very large number of possible conjoined features. In future work it might be interesting to consider these kinds of approaches for the parsing problem. Another difference is that both McCallum, and Riezler and Vasserman, describe approaches that use a regularizer in addition to feature selection: McCallum uses a two-norm regularizer; Riezler and Vasserman use a one-norm regularizer. Finally, note that other feature selection met</context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>McCallum, Andrew. 2003. Efficiently inducing features of conditional random fields. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI 2003), Acapulco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Philadelphia.</location>
<contexts>
<context position="6780" citStr="Och and Ney 2002" startWordPosition="1035" endWordPosition="1038">d models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before. In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al. 2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks. (Log-linear models are often referred to as maximum-entropy models in the NLP literature.) Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-linear models and 26 Collins and Koo Discriminative Reranking for NLP boosting for classification problems. One contribution of our research is to draw similar connections between the two approaches to ranking problems. We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the l</context>
<context position="38167" citStr="Och and Ney (2002)" startWordPosition="6161" endWordPosition="6164">in principle be any predicates over sentence/tree pairs. Evidence from the initial loglikelihood and the additional features is combined using a linear model. Parameter estimation becomes a problem of learning how to combine these different sources of information. The boosting algorithm we use is related to the generalization of boosting methods to ranking problems in Freund et al. (1998); we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994), Papineni, Roukos, and Ward (1997, 1998), Johnson et al. (1999), Riezler et al. (2002), and Och and Ney (2002). Section 4.1 gives a formal definition of the reranking problem. Section 4.2 introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss functions in section 3.2. Section 4.3 describes a general approach to feature selection methods with these loss functions. Section 4.4 describes a first algorithm for the exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm for the case of ExpLoss. Finally, section 4.6 describes issues in feature selection algorithms for the LogLoss function. 4.1 Problem Definition We use the following notation in t</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, Franz Josef, and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>R T Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In Proceedings of EuroSpeech’97,</booktitle>
<volume>3</volume>
<pages>1435--1438</pages>
<location>Rhodes, Greece.</location>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>Papineni, Kishore A., Salim Roukos, and R. T. Ward. 1997. Feature-based language understanding. In Proceedings of EuroSpeech’97, vol. 3, pages 1435–1438, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>R T Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>189--192</pages>
<location>Seattle.</location>
<marker>Papineni, Roukos, Ward, 1998</marker>
<rawString>Papineni, Kishore A., Salim Roukos, and R. T. Ward. 1998. Maximum likelihood and discriminative training of direct translation models. In Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, vol. 1, pages 189–192, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="15031" citStr="Pearl 1988" startWordPosition="2353" endWordPosition="2354"> P(OklAk) is the parameter associated with that rule, then m akhk(x, y) X log P(x, y) = k=1 All models considered in this article take this form, although in the boosting models the score for a parse is not a log-probability. The features hk define an m-dimensional vector of counts which represent the tree. The parameters ak represent the influence of each feature on the score of a tree. A drawback of history-based models is that the choice of derivation has a profound influence on the parameterization of the model. (Similar observations have been made in the related cases of belief networks [Pearl 1988], and language models for speech recognition [Rosenfeld 1997].) When designing a model, it would be desirable to have a framework in which features can be easily added to the model. Unfortunately, with history-based models adding new features often requires a modification of the underlying derivations in the model. Modifying the derivation to include a new feature type can be a laborious task. In an ideal situation we would be able to encode arbitrary features hk, without having to worry about formulating a derivation that included these features. To take a concrete example, consider part-of-</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Pearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="9702" citStr="Ratnaparkhi 1997" startWordPosition="1460" endWordPosition="1461">the boosting algorithm. Section 6 discusses related work in more detail. Finally, section 7 gives conclusions. The reranking models in this article were originally introduced in Collins (2000). In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing. 2. History-Based Models Before discussing the reranking approaches, we describe history-based models (Black et al. 1992). They are important for a few reasons. First, several of the best-performing parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997, 1999; Henderson 2003) are cases of history-based models. Many systems applied to part-of-speech tagging, speech recognition, and other language or speech tasks also fall into this class of model. Second, a particular history-based model (that of Collins [1999]) is used as the initial model for our approach. Finally, it is important to describe history-based models—and to explain their limitations—to motivate our departure from them. Parsing can be framed as a supervised learning task, to induce a function f : X--+Y given training examples (xi, yi), where xi</context>
<context position="13745" citStr="Ratnaparkhi (1997)" startWordPosition="2128" endWordPosition="2129">o the right). • Finally, the lexical heads of the modifiers are chosen (her and today). 2 To be more precise, generative probabilistic models assign joint probabilities P(x,y) to each (x,y) pair. Similar arguments apply to conditional history-based models, which define conditional probabilities P(y I x) through a definition P(y I x) = 11 P(di I F(d1... di-1, x)) i=1...n where d1... dn are again the decisions made in building a parse, and F is a function that groups histories into equivalence classes. Note that x is added to the domain of F (the context on which decisions are conditioned). See Ratnaparkhi (1997) for one example of a method using this approach. 28 Collins and Koo Discriminative Reranking for NLP Figure 1 illustrates this process. Each of the above decisions has an associated probability conditioned on the left-hand side of the rule (VP(saw)) and other information in some cases. History-based approaches lead to models in which the log-probability of a parse tree can be written as a linear sum of parameters ak multiplied by features hk. Each feature hk(x, y) is the count of a different “event”or fragment within the tree. As an example, consider a PCFG with rules (Ak—&gt;Ok) for 1 &lt; k &lt; m. </context>
<context position="79545" citStr="Ratnaparkhi (1997)" startWordPosition="13348" endWordPosition="13349">er which incorporates additional features into a previously developed parser, that of Charniak (1997). The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods). Our features are in many ways similar to those of Charniak (2000). The model in Charniak (2000) is quite different, however. The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]). Ratnaparkhi (1997) describes the use of maximum-entropy techniques applied to parsing. Log-linear models are used to estimate the conditional probabilities P(di I (D (d1,...,di_1)) in a history-based parser. As a result the model can take into account quite a rich set of features in the history. 57 Computational Linguistics Volume 31, Number 1 Figure 8 Work(n)(y-axis) versus n (x-axis). Figure 9 Savings(n)(y-axis) versus n(x-axis). 58 Collins and Koo Discriminative Reranking for NLP Table 4 Values of Savings (a, b) for various values of a, b. a–b Savings (a, b) 1–100,000 2,692.7 1–10 48.6 11–100 83.5 101–1,000 </context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Ratnaparkhi, Adwait. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="27712" citStr="Ratnaparkhi (1998)" startWordPosition="4489" endWordPosition="4490">ExpLoss. Step 3: Update the parameter for the feature chosen at Step 2 in such a way as to minimize ExpLoss(¯a) with respect to this one parameter. All other parameter values are left fixed. Return to Step 2. The difference with this latter “boosting”approach is that in Step 3, only one parameter value is adjusted, namely, the parameter corresponding to the newly chosen feature. Note that in this framework, the same feature may be chosen at more than one iteration.5 The maximum-entropy feature selection method can be quite inefficient, as the entire model is updated at each step. For example, Ratnaparkhi (1998) quotes times of around 30 hours for 500 rounds of feature selection on a prepositionalphrase attachment task. These experiments were performed in 1998, when processors were no doubt considerably slower than those available today. However, the PP attachment task is much smaller than the parsing task that we are addressing: Our task involves around 1,000,000 examples, with perhaps a few hundred features per example, and 100,000 rounds of feature selection; this compares to 20,000 examples, 16 features per example, and 500 rounds of feature selection for the PP attachment task in Ratnaparkhi (19</context>
<context position="72856" citStr="Ratnaparkhi 1998" startWordPosition="12239" endWordPosition="12240">esults are very slightly different from the original results published in Collins (2000). We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and a values tested on development data led to minor improvements in the results. Model &lt; 40 Words (2,245 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1% Collins 1999 88.5% 88.7% 0.92 66.7% 87.1% Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6% ExpLoss 90.2% 90.4% 0.73 71.2% 90.2% Model &lt; 100 Words (2,416 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2% Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% — Collins 1999 88.1% 88.3% 1.06 64.0% 85.1% Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7% ExpLoss 89.6% 89.9% 0.86 68.7% 88.3% Figure 5 Learning curve on development data for the optimal value for a (0.0025). The y-axis is the level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting. 53 Computational Linguistics Volume 31, Number 1 Table 2 Peak performance achieved for various values of F. ‘‘Best N”refers to the number of rounds at which peak development set accuracy was reached. ‘‘Best score”indicates the relative performance, comp</context>
<context position="85185" citStr="Ratnaparkhi 1998" startWordPosition="14225" endWordPosition="14226">e sparsity of the resulting reranker. Whereas the feature selection approach leads to around 11,000 (2%) of the features in our model having nonzero parameter values, log-linear models with Gaussian priors typically have very few nonzero parameters (see, e.g., Riezler and Vasserman 2004). This may be important in some domains, for example, those in which there are a very large number of features and this large number leads to difficulties in terms of memory requirements or computation time. 6.4 Feature Selection Methods A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems. Earlier work (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested methods that added a feature at a time to the model and updated all parameters in the current model at each step (for more detail, see section 3.3). 60 Collins and Koo Discriminative Reranking for NLP Assuming that selection of a feature takes one pass over the training </context>
<context position="86730" citStr="Ratnaparkhi (1998)" startWordPosition="14474" endWordPosition="14475">uld require 30,000 passes over the training set. This is around 1,000 times as much computation as that required for the efficient boosting algorithm applied to our data, suggesting that the feature selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the parsing task. More recent work (McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) has considered methods for speeding up the feature selection methods described in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997). McCallum (2003) and Riezler and Vasserman (2004) describe approaches that add k features at each step, where k is some constant greater than one. The running time for these methods is therefore O(f x (p + 1)1k). Riezler and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal performance. McCallum (2003) uses a value of k = 1,000. Zhou et al. (2003) use a different heuristic that avoids having to recompute the gain for every feature at every iteration. We would argue that the alternative feature selection met</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Ratnaparkhi, Adwait. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Salim Roukos</author>
<author>R T Ward</author>
</authors>
<title>A maximum entropy model for parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>803--806</pages>
<location>Yokohama, Japan.</location>
<marker>Ratnaparkhi, Roukos, Ward, 1994</marker>
<rawString>Ratnaparkhi, Adwait, Salim Roukos, and R. T. Ward. 1994. A maximum entropy model for parsing. In Proceedings of the International Conference on Spoken Language Processing, pages 803–806, Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="6451" citStr="Riezler et al. 2002" startWordPosition="985" endWordPosition="988">ed algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds. The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the “naive”implementation). The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before. In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al. 2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilistic models with global features for parsing and other NLP tasks. (Log-linear models are often referred to as maximum-entropy models in the NLP literature.) Similar methods have also been proposed for machine translation (Och and Ney 2002) and language understanding in dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-linear models and 26 Collins and Koo Discriminative Reranking for NLP boosting for classificat</context>
<context position="38143" citStr="Riezler et al. (2002)" startWordPosition="6156" endWordPosition="6159">he additional features can in principle be any predicates over sentence/tree pairs. Evidence from the initial loglikelihood and the additional features is combined using a linear model. Parameter estimation becomes a problem of learning how to combine these different sources of information. The boosting algorithm we use is related to the generalization of boosting methods to ranking problems in Freund et al. (1998); we also introduce an approach related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994), Papineni, Roukos, and Ward (1997, 1998), Johnson et al. (1999), Riezler et al. (2002), and Och and Ney (2002). Section 4.1 gives a formal definition of the reranking problem. Section 4.2 introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss functions in section 3.2. Section 4.3 describes a general approach to feature selection methods with these loss functions. Section 4.4 describes a first algorithm for the exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm for the case of ExpLoss. Finally, section 4.6 describes issues in feature selection algorithms for the LogLoss function. 4.1 Problem Definition We use the</context>
<context position="81863" citStr="Riezler et al. (2002)" startWordPosition="13708" endWordPosition="13711">timate of the denominator is required, and Monte Carlo methods have been proposed (Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a technique for estimation of this value. Our sense is that these methods can be computationally expensive. Notice that the joint likelihood in equation (27) is not a direct function of the margins on training examples, and its relation to error rate is therefore not so clear as in the discriminative approaches described in this article. 6.3 Conditional Log-Linear Models Ratnaparkhi, Roukos, and Ward (1994), Johnson et al. (1999), and Riezler et al. (2002) suggest training log-linear models (i.e., the LogLoss function in equation (9)) for parsing problems. Ratnaparkhi, Roukos, and Ward (1994) use feature selection techniques for the task. Johnson et al. (1999) and Riezler et al. (2002) do not use a feature selection technique, employing instead an objective function which includes a 59 Computational Linguistics Volume 31, Number 1 Gaussian prior on the parameter values, thereby penalizing parameter values which become too large: 2 ¯a* 1/4 arg minðLogLossð¯aÞ þ E ak ð28Þ k1/40 ...m 72k Closed-form updates under iterative scaling are not possible</context>
<context position="83942" citStr="Riezler et al. 2002" startWordPosition="14027" endWordPosition="14030">ent work on CRFs. Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter values which achieve the global minimum of the objective function in equation (28)) is a plausible alternative to the feature selection approaches described in the current article or to the feature selection methods previously applied to log-linear models. The Gaussian prior (i.e., the Pk a2k/72k penalty) has been found in practice to be very effective in combating overfitting of the parameters to the training data (Chen and Rosenfeld 1999; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al. 2002). The function in equation (28) can be optimized using variants of gradient descent, which in practice require tens or at most hundreds of passes over the training data (see, e.g., Sha and Pereira 2003). Thus log-linear models with a Gaussian prior are likely to be comparable in terms of efficiency to the feature selection approach described in this article (in the experimental section, we showed that for the parsereranking task, the efficient boosting algorithm requires computation that is equivalent to around 40 passes over the training data). Note, however, that the two methods will differ </context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Riezler, Stefan, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques. In ACL 2002: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
</authors>
<title>Incremental feature selection and l1 regularization for relaxed maximum-entropy modeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="8070" citStr="Riezler and Vasserman (2004)" startWordPosition="1224" endWordPosition="1227">aximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the</context>
<context position="26059" citStr="Riezler and Vasserman 2004" startWordPosition="4214" endWordPosition="4217"> [—1.5...1.5]; LogLoss shows a similar plot for f (z) = log(1 + e—z); Error is a plot of f (z) = Qz &lt; 01. 33 Computational Linguistics Volume 31, Number 1 a¯ are zero). Roughly speaking, the motivation for using a small number of features is the hope that this will prevent overfitting in the models. Feature selection methods have been proposed in the maximum-entropy literature by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004). The most basic approach—for example see Ratnaparkhi, Roukos, and Ward (1994) and Berger, Della Pietra, and Della Pietra (1996)—involves selection of a single feature at each iteration, followed by an update to the entire model, as follows: Step 1: Throughout the algorithm, maintain a set of active features. Initialize this set to be empty. Step 2: Choose a feature from outside of the set of active features which has the largest estimated impact in terms of reducing the loss function LogLoss, and add this to the active feature set. Step 3: Minimize LogLoss(¯a) with respect to the set of activ</context>
<context position="84857" citStr="Riezler and Vasserman 2004" startWordPosition="14170" endWordPosition="14173"> efficiency to the feature selection approach described in this article (in the experimental section, we showed that for the parsereranking task, the efficient boosting algorithm requires computation that is equivalent to around 40 passes over the training data). Note, however, that the two methods will differ considerably in terms of the sparsity of the resulting reranker. Whereas the feature selection approach leads to around 11,000 (2%) of the features in our model having nonzero parameter values, log-linear models with Gaussian priors typically have very few nonzero parameters (see, e.g., Riezler and Vasserman 2004). This may be important in some domains, for example, those in which there are a very large number of features and this large number leads to difficulties in terms of memory requirements or computation time. 6.4 Feature Selection Methods A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems. Earlier work (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; </context>
<context position="86582" citStr="Riezler and Vasserman 2004" startWordPosition="14451" endWordPosition="14454">ted. In our experiments, f z 10,000. It is difficult to estimate the value for p, but assuming (very conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over the training set. This is around 1,000 times as much computation as that required for the efficient boosting algorithm applied to our data, suggesting that the feature selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the parsing task. More recent work (McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) has considered methods for speeding up the feature selection methods described in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997). McCallum (2003) and Riezler and Vasserman (2004) describe approaches that add k features at each step, where k is some constant greater than one. The running time for these methods is therefore O(f x (p + 1)1k). Riezler and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal performance. McCallum (2003) uses a value of k = 1,000. Zhou et al. (2003) use a differen</context>
</contexts>
<marker>Riezler, Vasserman, 2004</marker>
<rawString>Riezler, Stefan and Alexander Vasserman. 2004. Incremental feature selection and l1 regularization for relaxed maximum-entropy modeling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP’04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A whole sentence maximum entropy language model.</title>
<date>1997</date>
<booktitle>In Proceedings of the IEEE Workshop on Speech Recognition and Understanding,</booktitle>
<location>Santa Barbara, CA,</location>
<contexts>
<context position="15092" citStr="Rosenfeld 1997" startWordPosition="2361" endWordPosition="2362">n m akhk(x, y) X log P(x, y) = k=1 All models considered in this article take this form, although in the boosting models the score for a parse is not a log-probability. The features hk define an m-dimensional vector of counts which represent the tree. The parameters ak represent the influence of each feature on the score of a tree. A drawback of history-based models is that the choice of derivation has a profound influence on the parameterization of the model. (Similar observations have been made in the related cases of belief networks [Pearl 1988], and language models for speech recognition [Rosenfeld 1997].) When designing a model, it would be desirable to have a framework in which features can be easily added to the model. Unfortunately, with history-based models adding new features often requires a modification of the underlying derivations in the model. Modifying the derivation to include a new feature type can be a laborious task. In an ideal situation we would be able to encode arbitrary features hk, without having to worry about formulating a derivation that included these features. To take a concrete example, consider part-of-speech tagging using a hidden Markov model (HMM). We might ha</context>
<context position="80723" citStr="Rosenfeld (1997)" startWordPosition="13527" endWordPosition="13528">692.7 1–10 48.6 11–100 83.5 101–1,000 280.0 1,001–10,000 1,263.9 10,001–50,000 2,920.2 50,001–100,000 4,229.8 Both approaches still rely on decomposing a parse tree into a sequence of decisions, and we would argue that the techniques described in this article have more flexibility in terms of the features that can be included in the model. 6.2 Joint Log-Linear Models Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs). Della Pietra, Della Pietra, and Lafferty (1997) describe feature selection methods for log-linear models, and Rosenfeld (1997) describes application of these methods to language modeling for speech recognition. These methods all emphasize models which define a joint probability over the space of all parse trees (or structures in question): For this reason we describe these approaches as “Joint log-linear models.”The probability of a tree xi,j is Pð eF(x;,jÞ ð27Þ xii) — P eFðxÞ xEZ Here Z is the (infinite) set of possible trees, and the denominator cannot be calculated explicitly. This is a problem for parameter estimation, in which an estimate of the denominator is required, and Monte Carlo methods have been proposed</context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>Rosenfeld, Ronald. 1997. A whole sentence maximum entropy language model. In Proceedings of the IEEE Workshop on Speech Recognition and Understanding, Santa Barbara, CA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoav Freund</author>
<author>Peter Bartlett</author>
<author>W S Lee</author>
</authors>
<title>Boosting the margin: A new explanation for the effectiveness of voting methods.</title>
<date>1998</date>
<journal>Annals of Statistics,</journal>
<volume>26</volume>
<issue>5</issue>
<contexts>
<context position="31688" citStr="Schapire et al. (1998)" startWordPosition="5112" endWordPosition="5115">h a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit perspective (models with fewer parameters are less likely to fit the data by chance), but this requires additional research. The statistical justification for boosting approaches is quite different. Boosting algorithms were originally developed within the PAC framework (Valiant 1984) for machine learning, specifically to address questions regarding the equivalence of weak and strong learning. Freund and Schapire (1997) originally introduced AdaBoost and gave a first set of statistical guarantees for the algorithm. Schapire et al. (1998) gave a second set of guarantees based on the analysis of margins on training examples. Both papers assume that a fixed distribution D(x, y) is generating both training and test examples and that the goal is to find a hypothesis with a small number of expected errors with respect to this distribution. The form of the distribution is not assumed to be known, and in this sense the guarantees are nonparametric, or “distribution free.”Freund and Schapire (1997) show that if the weak learning assumption holds (i.e., roughly speaking, a feature with error rate better than chance can be found for any</context>
</contexts>
<marker>Schapire, Freund, Bartlett, Lee, 1998</marker>
<rawString>Schapire, Robert E., Yoav Freund, Peter Bartlett, and W. S. Lee. 1998. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5):1651–1686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="45146" citStr="Schapire and Singer (1999)" startWordPosition="7415" endWordPosition="7418">valently, we again talk about minimizing the negative 39 Computational Linguistics Volume 31, Number 1 log-likelihood. Some manipulation shows that the negative log-likelihood is a function of the margins on training data: eFðxi,1, ¯aÞ—log ni eFðxi,j, ¯aÞ �j — —log �j1/41 ini e ðFðxi,1, aÞ—Fðxi,j, aÞÞ nini logð1 þXe ðFðxi,1,¯aÞ—Fðxi,j, aÞÞ 1/4Xlogð1 þ X emi,jð¯aÞÞ ð9Þ j1/42 j1/42 i XLogLoss ð¯aÞ 1/4 i X1/4 i Note the similarity of equation (9) to the LogLoss function for classification in equation (4). 4.2.3 Exponential Loss. The next loss function is based on the boosting method described in Schapire and Singer (1999). It is a special case of the general ranking methods described in Freund et al. (1998), with the ranking “feedback”being a simple binary distinction between the highest-scoring parse and the other parses. Again, the loss function is a function of the margins on training data: ni ni XExpLossð¯aÞ 1/4 X Xe�ðFðxi,1, ¯aÞ�Fðxi,j, ¯aÞÞ 1/4 X eMi,jð¯aÞ ð10Þ ij1/42 i j1/42 Note the similarity of equation (10) to the ExpLoss function for classification in equation (6). It can be shown that ExpLossð¯aÞ &gt; Errorð¯aÞ, so that minimizing ExpLossð¯aÞ is closely related to minimizing the number of ranking err</context>
<context position="53213" citStr="Schapire and Singer (1999)" startWordPosition="8815" endWordPosition="8818">shown (see appendix A) that BestWt(k, ¯a) = 1 log Wk (15) Wk and BestLoss(k, ¯a) = Z — (rWk+ Wk ~2 (16) 12 A more precise approach, for example, binary search, could also be used to solve this optimization problem. We used the methods that searches through a set of fixed values for simplicity, implicitly assuming that a precision of 0.001 was sufficient for our problem. 43 Computational Linguistics Volume 31, Number 1 where Z 1/4 Ei Enij1/42 SId - e Mi,jð6Þ 1/4 ExpLossðaÞ is a constant (for fixed a) which appears in the BestLoss for all features and therefore does not affect their ranking. As Schapire and Singer (1999) point out, the updates in equation (15) can be problematic, as they are undefined (infinite) when either Wþk or Wk is zero. Following Schapire and Singer (1999), we introduce smoothing through a parameter E and the following new definition of BestWt: BestWtðk, aÞ 1/4 1 log Wþk þ EZ ð17Þ 2 Wk þ EZ The smoothing parameter E is chosen through optimization on a development set. See Figure 3 for a direct implementation of the feature selection method for ExpLoss. We use an array of values Gk 1/4q jWþ ~W~ k k to indicate the gain of each feature (i.e., the impact that choosing this feature will hav</context>
<context position="94221" citStr="Schapire and Singer (1999)" startWordPosition="15641" endWordPosition="15644">ng data for the new algorithm over the obvious implementation of the boosting approach. We would argue that the improved boosting algorithm is a natural alternative to maximum-entropy or (conditional) log-linear models. The article has drawn connections between boosting and maximum-entropy models in terms of the optimization problems that they involve, the algorithms used, their relative efficiency, and their performance in empirical tests. Appendix A: Derivation of Updates for ExpLoss This appendix gives a derivation of the optimal updates for ExpLoss. The derivation is very close to that in Schapire and Singer (1999). Recall that for parameter values ¯a, we need to compute BestWtðk, ¯aÞ and BestLossðk, ¯aÞ for k 1/4 1, ... , m, where BestWtðk, ¯aÞ 1/4 arg min ExpLossðUpdð¯a, k, dÞÞ d and BestLossðk, ¯aÞ 1/4 ExpLossðUpdð¯a, k, BestWtðk, ¯aÞÞÞ The first thing to note is that an update in parameters from a¯ to Updð¯a, k,dÞÞ results in a simple additive update to the ranking function F: Fðxi,j, Updð¯a, k, dÞÞ 1/4 Fðxi,j, aÞ þ dhkðxi,jÞ It follows that the margin on example ði, jÞ also has a simple update: Mi,jðUpdð¯a,k,dÞÞ 1/4 Fðxi,1,Updð¯a,k, dÞÞ — Fðxi,j,Updð¯a,k, dÞÞ 1/4 Fðxi,1, ¯aÞ — Fðxi,j, ¯aÞ þ d1/2hkð</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>Schapire, Robert E. and Yoram Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<marker>Schapire, Singer, 2000</marker>
<rawString>Schapire, Robert E. and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<location>Edmonton, Alberta, Canada.</location>
<contexts>
<context position="83309" citStr="Sha and Pereira (2003)" startWordPosition="13928" endWordPosition="13931">be the use of conditional Markov random fields (CRFs) for tagging tasks such as named entity recognition or part-of-speech tagging (hidden Markov models are a common method applied to these tasks). CRFs employ the objective function in equation (28). A key insight of Lafferty, McCallum, and Pereira (2001) is that when features are of a significantly local nature, the gradient of the function in equation (28) can be calculated efficiently using dynamic programming, even in cases in which the set of candidates involves all possible tagged sequences and is therefore exponential in size. See also Sha and Pereira (2003) for more recent work on CRFs. Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter values which achieve the global minimum of the objective function in equation (28)) is a plausible alternative to the feature selection approaches described in the current article or to the feature selection methods previously applied to log-linear models. The Gaussian prior (i.e., the Pk a2k/72k penalty) has been found in practice to be very effective in combating overfitting of the parameters to the training data (Chen and Rosenfeld 1999; Johnson et al. 1999; Lafferty, McCallum, and P</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha, Fei and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL 2003, Edmonton, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using LTAG based features in parse reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<location>Sapporo, Japan.</location>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Shen, Libin, Anoop Sarkar, and Aravind K. Joshi. 2003. Using LTAG based features in parse reranking. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP 2003), Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<volume>27</volume>
<issue>11</issue>
<contexts>
<context position="31430" citStr="Valiant 1984" startWordPosition="5077" endWordPosition="5078">fewer samples for convergence, but this is not necessarily the case, and at present this intuition lacks a theoretical basis. Feature selection methods can probably be motivated either from a Bayesian perspective (through a prior favoring models with a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit perspective (models with fewer parameters are less likely to fit the data by chance), but this requires additional research. The statistical justification for boosting approaches is quite different. Boosting algorithms were originally developed within the PAC framework (Valiant 1984) for machine learning, specifically to address questions regarding the equivalence of weak and strong learning. Freund and Schapire (1997) originally introduced AdaBoost and gave a first set of statistical guarantees for the algorithm. Schapire et al. (1998) gave a second set of guarantees based on the analysis of margins on training examples. Both papers assume that a fixed distribution D(x, y) is generating both training and test examples and that the goal is to find a hypothesis with a small number of expected errors with respect to this distribution. The form of the distribution is not ass</context>
</contexts>
<marker>Valiant, 1984</marker>
<rawString>Valiant, Leslie G. 1984. A theory of the learnable. Communications of the ACM, 27(11):1134–1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Owen Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>SPoT: A trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2001),</booktitle>
<location>Pittsburgh.</location>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>Walker, Marilyn, Owen Rambow, and Monica Rogati. 2001. SPoT: A trainable sentence planner. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2001), Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaqian Zhou</author>
<author>Fuliang Weng</author>
<author>Lide Wu</author>
<author>Hauke Schmidt</author>
</authors>
<title>A fast algorithm for feature selection in conditional maximum entropy modeling.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="8036" citStr="Zhou et al. (2003)" startWordPosition="1219" endWordPosition="1222">he earlier methods for maximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004). The remainder of this article is structured as follows. Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems;</context>
<context position="26030" citStr="Zhou et al. 2003" startWordPosition="4210" endWordPosition="4213"> (z) = e—z for z = [—1.5...1.5]; LogLoss shows a similar plot for f (z) = log(1 + e—z); Error is a plot of f (z) = Qz &lt; 01. 33 Computational Linguistics Volume 31, Number 1 a¯ are zero). Roughly speaking, the motivation for using a small number of features is the hope that this will prevent overfitting in the models. Feature selection methods have been proposed in the maximum-entropy literature by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004). The most basic approach—for example see Ratnaparkhi, Roukos, and Ward (1994) and Berger, Della Pietra, and Della Pietra (1996)—involves selection of a single feature at each iteration, followed by an update to the entire model, as follows: Step 1: Throughout the algorithm, maintain a set of active features. Initialize this set to be empty. Step 2: Choose a feature from outside of the set of active features which has the largest estimated impact in terms of reducing the loss function LogLoss, and add this to the active feature set. Step 3: Minimize LogLoss(¯a) wit</context>
<context position="85265" citStr="Zhou et al. 2003" startWordPosition="14236" endWordPosition="14239">ds to around 11,000 (2%) of the features in our model having nonzero parameter values, log-linear models with Gaussian priors typically have very few nonzero parameters (see, e.g., Riezler and Vasserman 2004). This may be important in some domains, for example, those in which there are a very large number of features and this large number leads to difficulties in terms of memory requirements or computation time. 6.4 Feature Selection Methods A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) describe feature selection approaches for log-linear models applied to NLP problems. Earlier work (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested methods that added a feature at a time to the model and updated all parameters in the current model at each step (for more detail, see section 3.3). 60 Collins and Koo Discriminative Reranking for NLP Assuming that selection of a feature takes one pass over the training set and that fitting a model takes p passes over the training set, these methods</context>
<context position="86553" citStr="Zhou et al. 2003" startWordPosition="14447" endWordPosition="14450"> of features selected. In our experiments, f z 10,000. It is difficult to estimate the value for p, but assuming (very conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over the training set. This is around 1,000 times as much computation as that required for the efficient boosting algorithm applied to our data, suggesting that the feature selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the parsing task. More recent work (McCallum 2003; Zhou et al. 2003; Riezler and Vasserman 2004) has considered methods for speeding up the feature selection methods described in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997). McCallum (2003) and Riezler and Vasserman (2004) describe approaches that add k features at each step, where k is some constant greater than one. The running time for these methods is therefore O(f x (p + 1)1k). Riezler and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal performance. McCallum (2003) uses a value of k = 1,000. Zhou</context>
</contexts>
<marker>Zhou, Weng, Wu, Schmidt, 2003</marker>
<rawString>Zhou, Yaqian, Fuliang Weng, Lide Wu, and Hauke Schmidt. 2003. A fast algorithm for feature selection in conditional maximum entropy modeling. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP 2003), Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>