<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000069">
<title confidence="0.9957525">
Trimming CFG Parse Trees for Sentence Compression Using Machine
Learning Approaches
</title>
<author confidence="0.999817">
Yuya Unno1 Takashi Ninomiya2 Yusuke Miyao1 Jun’ichi Tsujii134
</author>
<affiliation confidence="0.999939333333333">
1Department of Computer Science, University of Tokyo
2Information Technology Center, University of Tokyo
3School of Informatics, University of Manchester
</affiliation>
<address confidence="0.898853">
4SORST, JST
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
</address>
<email confidence="0.9913535">
{unno, yusuke, tsujii}@is.s.u-tokyo.ac.jp
ninomi@r.dl.itc.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.993805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969333333333">
Sentence compression is a task of creating
a short grammatical sentence by removing
extraneous words or phrases from an origi-
nal sentence while preserving its meaning.
Existing methods learn statistics on trim-
ming context-free grammar (CFG) rules.
However, these methods sometimes elim-
inate the original meaning by incorrectly
removing important parts of sentences, be-
cause trimming probabilities only depend
on parents’ and daughters’ non-terminals
in applied CFG rules. We apply a maxi-
mum entropy model to the above method.
Our method can easily include various
features, for example, other parts of a
parse tree or words the sentences contain.
We evaluated the method using manually
compressed sentences and human judg-
ments. We found that our method pro-
duced more grammatical and informative
compressed sentences than other methods.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999856568627451">
In most automatic summarization approaches, text
is summarized by extracting sentences from a
given document without modifying the sentences
themselves. Although these methods have been
significantly improved to extract good sentences
as summaries, they are not intended to shorten sen-
tences; i.e., the output often has redundant words
or phrases. These methods cannot be used to
make a shorter sentence from an input sentence or
for other applications such as generating headline
news (Dorr et al., 2003) or messages for the small
screens of mobile devices. We need to compress
sentences to obtain short and useful summaries.
This task is called sentence compression.
While several methods have been proposed for
sentence compression (Witbrock and Mittal, 1999;
Jing and McKeown, 1999; Vandeghinste and Pan,
2004), this paper focuses on Knight and Marcu’s
noisy-channel model (Knight and Marcu, 2000)
and presents an extension of their method. They
developed a probabilistic model for trimming a
CFG parse tree of an input sentence. Their
method drops words of input sentences but does
not change their order or change the words. They
use a parallel corpus that contains pairs of origi-
nal and compressed sentences. The method makes
CFG parse trees of both original and compressed
sentences and learns trimming probabilities from
these pairs. Although their method is concise and
well-defined, its accuracy is still unsatisfactory.
Their method has two problems. One is that prob-
abilities are calculated only from the frequencies
of applied CFG rules, and other characteristics like
whether the phrase includes negative words cannot
be introduced. The other problem is that the parse
trees of original and compressed sentences some-
times do not correspond.
To solve the former problem, we apply a maxi-
mum entropy model to Knight and Marcu’s model
to introduce machine learning features that are de-
fined not only for CFG rules but also for other
characteristics in a parse tree, such as the depth
from the root node or words it contains. To solve
the latter problem, we introduce a novel matching
method, the bottom-up method, to learn compli-
cated relations of two unmatched trees.
We evaluated each algorithm using the Ziff-
Davis corpus, which has long and short sentence
pairs. We compared our method with Knight and
Marcu’s method in terms of F-measures, bigram
F-measures, BLEU scores and human judgments.
</bodyText>
<page confidence="0.968283">
850
</page>
<note confidence="0.890239">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850–857,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.887059" genericHeader="introduction">
2 Background
2.1 The Noisy-Channel Model for Sentence
Compression
</sectionHeader>
<bodyText confidence="0.984493297297297">
Knight and Marcu proposed a sentence compres-
sion method using a noisy-channel model (Knight
and Marcu, 2000). This model assumes that along
sentence was originally a short one and that the
longer sentence was generated because some un-
necessary words were added. Given a long sen-
tence l, it finds a short sentence s that maximizes
P(s|l). This is equivalent to finding the s that
maximizes P(s) · P(l|s) in Bayes’ Rule.
The expression P(s) is the source model, which
gives the probability that s is the original short
string. When s is ungrammatical, P(s) becomes
small. The expression P(l|s) is the channel
model, which gives the probability that s is ex-
panded to l. When s does not include important
words of l, P(l|s) has a low value.
In the Knight and Marcu’s model, a proba-
bilistic context-free grammar (PCFG) score and a
word-bigram score are incorporated as the source
model. To estimate the channel model, Knight
and Marcu used the Ziff-Davis parallel corpus,
which contains long sentences and corresponding
short sentences compressed by humans. Note that
each compressed sentence is a subsequence of the
corresponding original sentence. They first parse
both the original and compressed sentences using
a CFG parser to create parse trees. When two
nodes of the original and compressed trees have
the same non-terminals, and the daughter nodes of
the compressed tree are a subsequence of the orig-
inal tree, they count the node pair as a joint event.
For example, in Figure 1, the original parse tree
contains a rule rl = (B  D E F), and the com-
pressed parse tree contains rs = (B  D F).
They assume that rs was expanded into rl, and
count the node pairs as joint events. The expan-
sion probability of two rules is given by:
</bodyText>
<equation confidence="0.996637">
Pe p.d (rl|rs) = count(joint(rl, rs))
count(rs) �
</equation>
<bodyText confidence="0.99650975">
Finally, new subtrees grow from new daugh-
ter nodes in each expanded node. In Figure 1,
(E (G g) (H h)) grows from E. The PCFG
scores, P,fg, of these subtrees are calculated.
Then, each probability is assumed to be indepen-
dent of the others, and the channel model, P(l|s),
is calculated as the product of all expansion prob-
abilities of joint events and PCFG scores of new
</bodyText>
<figureCaption confidence="0.9985665">
Figure 1: Examples of original and compressed
parse trees.
</figureCaption>
<bodyText confidence="0.53393">
subtrees:
</bodyText>
<equation confidence="0.9988915">
P(l|s) = � �Pe p��d(rl|rs) · P,fg(r),
(rl,r3)ER rER&apos;
</equation>
<bodyText confidence="0.999936833333333">
where R is the set of rule pairs, and R&apos; is the set
of generation rules in new subtrees.
To compress an input sentence, they create a
tree with the highest score of all possible trees.
They pack all possible trees in a shared-forest
structure (Langkilde, 2000). The forest structure
is represented by an AND-OR tree, and it con-
tains many tree structures. The forest represen-
tation saves memory and makes calculation faster
because the trees share sub structures, and this can
reduce the total number of calculations.
They normalize each log probability using the
length of the compressed sentence; that is, they di-
vide the log probability by the length of the com-
pressed sentence.
Turner and Charniak (Turner and Charniak,
2005) added some special rules and applied this
method to unsupervised learning to overcome the
lack of training data. However their model also
has the same problem. McDonald (McDonald,
2006) independently proposed a new machine
learning approach. He does not trim input parse
trees but uses rich features about syntactic trees
and improved performance.
</bodyText>
<subsectionHeader confidence="0.996589">
2.2 Maximum Entropy Model
</subsectionHeader>
<bodyText confidence="0.977280888888889">
The maximum entropy model (Berger et al., 1996)
estimates a probability distribution from training
data. The model creates the most “uniform” distri-
bution within the constraints given by users. The
distribution with the maximum entropy is consid-
ered the most uniform.
Given two finite sets of event variables, X and
Y, we estimate their joint probability distribution,
P(x, y). An output, y ( Y), is produced, and
</bodyText>
<figure confidence="0.999345105263158">
A
A
f
d
G
H
d f
D
B
E
F
C
c
D
B
F
g h
C
c
</figure>
<page confidence="0.997205">
851
</page>
<bodyText confidence="0.993454916666667">
contextual information, x (E X), is observed. To
represent whether the event (x, y) satisfies a cer-
tain feature, we introduce a feature function. A
feature function fi returns 1 iff the event (x, y) sat-
isfies the feature i and returns 0 otherwise.
Given training data {(x1, y1), · · · , (xn, yn)},
we assume that the expectation of fi on the dis-
tribution of the model conforms to that on the em-
pirical probability distribution P(x, y). We select
the probability distribution that satisfies these con-
straints of all feature functions and maximizes its
entropy, H(P) = − Ex,y P(x, y) log (P(x, y)).
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.8868335">
3.1 Maximum Entropy Model for Sentence
Compression
</subsectionHeader>
<bodyText confidence="0.99957537037037">
We describe a maximum entropy method as a
natural extension of Knight and Marcu’s noisy-
channel model (Knight and Marcu, 2000). Knight
and Marcu’s method uses only mother and daugh-
ter local relations in CFG parse trees. Therefore,
it sometimes eliminates the meanings of the origi-
nal sentences. For example, their method cannot
distinguish “never” and “always” because these
two adverbs are assigned the same non-terminals
in parse trees. However, if “never” is removed
from a sentence, the meaning of the sentence com-
pletely changes. Turner and Charniak (Turner and
Charniak, 2005) revised and improved Knight and
Marcu’s algorithm; however, their algorithm also
uses only mother and daughter relations and has
the same problem. We use other information as
feature functions of the maximum entropy model,
and this model can deal with many features more
appropriately than using simple frequency.
Suppose that we trim a node in the original full
parse tree. For example, suppose we have a mother
node A and daughter nodes (B C D) that are de-
rived using a CFG rule. We must leave at least one
non-terminal in the daughter nodes. The trim can-
didates of this rule are the members of the set of
subsequences, Y, of (B C D), or the seven non-
terminal sequences below:
</bodyText>
<equation confidence="0.49883175">
Y = {B, C, D, BC, BD, CD, BCD}.
For each y (E Y), such as (B C), the trimming
probability, P(y|Y) = Ptrim(A —* B C|A —*
B C D), is calculated by using the maximum en-
</equation>
<bodyText confidence="0.630067384615385">
tropy model. We assume that these joint events are
independent of each other and calculate the proba-
bility that an original sentence, l, is compressed to
Description
1 the mother node
2 the current node
3 the daughter node sequence in the original sentence
and which daughters are removed
4 the daughter node sequence in the compressed sen-
tence
5 the number of daughter nodes
6 the depth from the root
7 the daughter non-terminals that are removed
8 the daughter terminals that are removed
9 whether the daughters are “negative adverbs”, and
removed
10 tri-gram of daughter nodes
11 only one daughter exists, and its non-terminal is the
same as that of the current node
12 only one daughter exists, and its non-terminal is the
same as that of the mother node
13 how many daughter nodes are removed
14 the number of terminals the current node contains
15 whether the head daughter is removed
16 the left-most and the right-most daughters
17 the left and the right siblings
</bodyText>
<tableCaption confidence="0.95595">
Table 1: Features for maximum entropy model.
</tableCaption>
<bodyText confidence="0.9731675">
s as the product of all trimming probabilities, like
in Knight and Marcu’s method.
</bodyText>
<equation confidence="0.999513">
P(s|l) = � Ptrim(rs|rl),
(r3,rj)ER
</equation>
<bodyText confidence="0.9998424">
where R is the set of compressed and original rule
pairs in joint events. Note that our model does not
use Bayes’ Rule or any language models.
For example, in Figure 1, the trimming proba-
bility is calculated as below:
</bodyText>
<equation confidence="0.9968005">
P(s|l) = Ptrim(A —* B C|A —* B C)
·Ptrim(B —* D F|B — *D E F).
</equation>
<bodyText confidence="0.999529882352941">
To represent all summary candidates, we cre-
ate a compression forest as Knight and Marcu did.
We select the tree assigned the highest probability
from the forest.
Features in the maximum entropy model are de-
fined for a tree node and its surroundings. When
we process one node, or one non-terminal x, we
call it the current node. We focus on not only x
and its daughter nodes, but its mother node, its
sibling nodes, terminals of its subtree and so on.
The features we used are listed in Table 1.
Knight and Marcu divided the log probabilities
by the length of the summary. We extend this idea
so that we can change the output length flexibly.
We introduce a length parameter, α, and define a
score 5α as 5α(s) = length(s)α log P(s|l), where
l is an input sentence to be shortened, and s is a
</bodyText>
<page confidence="0.990057">
852
</page>
<bodyText confidence="0.9994636">
summary candidate. Because log P(s|l) is nega-
tive, short sentences obtain a high score for large
α, and long ones get a low score. The parameter
α can be negative or positive, and we can use it to
control the average length of outputs.
</bodyText>
<subsectionHeader confidence="0.998056">
3.2 Bottom-Up Method
</subsectionHeader>
<bodyText confidence="0.967324090909091">
As explained in Section 2.1, in Knight and
Marcu’s method, both original and compressed
sentences are parsed, and correspondences of CFG
rules are identified. However, when the daugh-
ter nodes of a compressed rule are not a subse-
quence of the daughter nodes in the original one,
the method cannot learn this joint event. A com-
plex sentence is a typical example. A complex
sentence is a sentence that includes another sen-
tence as a part. An example of a parse tree of a
complex sentence and its compressed version is
shown in Figure 2. When we extract joint events
from these two trees, we cannot match the two
root nodes because the sequence of the daughter
nodes of the root node of the compressed parse
tree, (NP ADVP VP .), is not a subsequence
of the daughter nodes of the original parse tree,
(S , NP VP .). Turner and Charniak (Turner and
Charniak, 2005) solve this problem by appending
special rules that are applied when a mother node
and its daughter node have the same label. How-
ever, there are several types of such problems like
Figure 2. We need to extract these structures from
a training corpus.
We propose a bottom-up method to solve the
problem explained above. In our method, only
original sentences are parsed, and the parse trees
of compressed sentences are extracted from the
original parse trees. An example of this method
is shown in Figure 3. The original sentence is ‘d
g h f c’, and its compressed sentence is ‘d g c’.
First, each terminal in the parse tree of the original
sentence is marked if it exists in the compressed
sentence. In the figure, the marked terminals are
represented by circles. Second, each non-terminal
in the original parse tree is marked if it has at least
one marked terminal in its sub-trees. These are
represented as bold boxes in the figure. If non-
terminals contain marked non-terminals in their
sub-trees, these non-terminals are also marked re-
cursively. These marked non-terminals and termi-
nals compose a tree structure like that on the right-
hand side in the figure. These non-terminals rep-
resent joint events at each node.
</bodyText>
<figure confidence="0.8621965">
top top
I never think so
</figure>
<figureCaption confidence="0.999545333333333">
Figure 2: Example of parse tree pair that cannot
be matched.
Figure 3: Example of bottom-up method.
</figureCaption>
<bodyText confidence="0.976732">
Note that this “tree” is not guaranteed to be
a grammatical “parse tree” by the CFG gram-
mar. For example, from the tree of Figure 2,
</bodyText>
<equation confidence="0.866292">
(S (S · · · ) (, , ) (NP I) (VP said) (..)), a new
tree, (S (S · · · ) (..)), is extracted. However, the
rule (S —* S .) is ungrammatical.
</equation>
<sectionHeader confidence="0.999405" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.996107">
4.1 Evaluation Method
</subsectionHeader>
<bodyText confidence="0.99990225">
We evaluated each sentence compression method
using word F-measures, bigram F-measures, and
BLEU scores (Papineni et al., 2002). BLEU scores
are usually used for evaluating machine transla-
tion quality. A BLEU score is defined as the
weighted geometric average of n-gram precisions
with length penalties. We used from unigram to
4-gram precisions and uniform weights for the
BLEU scores.
ROUGE (Lin, 2004) is a set of recall-based cri-
teria that is mainly used for evaluating summa-
rization tasks. ROUGE-N uses average N-gram re-
call, and ROUGE-1 is word recall. ROUGE-L uses
the length of the longest common subsequence
(LCS) of the original and summarized sentences.
In our model, the length of the LCS is equal to
the number of common words, and ROUGE-L is
equal to the unigram F-measure because words
are not rearranged. ROUGE-L and ROUGE-1 are
supposed to be appropriate for the headline gener-
</bodyText>
<figure confidence="0.998533578947369">
.
.
ADVP
NP
VP
I never think so
I said
,
S
S
S
,
NP VP
.
NP ADVP VP
.
D
d
G
S
B
E
H
h
A
F
f
C
c
D
d
g
B
G
E
A
C
c
</figure>
<page confidence="0.996127">
853
</page>
<bodyText confidence="0.998179615384615">
ation task (Lin, 2004). This is not our task, but it
is the most similar task in his paper.
We also evaluated the methods using human
judgments. The evaluator is not the author but not
a native English speaker. The judgment used the
same criteria as those in Knight and Marcu’s meth-
ods. We performed two experiments. In the first
experiment, evaluators scored from 1 to 5 points
the grammaticality of the compressed sentence. In
the second one, they scored from 1 to 5 points
how well the compressed sentence contained the
important words of the original one.
We used the parallel corpus used in Ref. (Knight
and Marcu, 2000). This corpus consists of sen-
tence pairs extracted automatically from the Ziff-
Davis corpus, a set of newspaper articles about
computer products. This corpus has 1087 sentence
pairs. Thirty-two of these sentences were used for
the human judgments in Knight and Marcu’s ex-
periment, and the same sentences were used for
our human judgments. The rest of the sentences
were randomly shuffled, and 527 sentence pairs
were used as a training corpus, 263 pairs as a de-
velopment corpus, and 264 pairs as a test corpus.
To parse these corpora, we used Charniak and
Johnson’s parser (Charniak and Johnson, 2005).
</bodyText>
<subsectionHeader confidence="0.999429">
4.2 Settings of Two Experiments
</subsectionHeader>
<bodyText confidence="0.999888434782609">
We experimented with/without goal sentence
length for summaries.
In the first experiment, the system was given
only a sentence and no sentence length informa-
tion. The sentence compression problem without
the length information is a general task, but evalu-
ating it is difficult because the correct length of a
summary is not generally defined even by humans.
The following example shows this.
Original:“A font, on the other hand, is a subcate-
gory of a typeface, such as Helvetica Bold or Hel-
vetica Medium.”
Human: “A font is a subcategory of a typeface,
such as Helvetica Bold.”
System: “A font is a subcategory of a typeface.”
The “such as” phrase is removed in this sys-
tem output, but it is not removed in the human
summary. Neither result is wrong, but in such
situations, the evaluation score of the system de-
creases. This is because the compression rate of
each algorithm is different, and evaluation scores
are affected by the lengths of system outputs. For
this reason, results with different lengths cannot be
</bodyText>
<figure confidence="0.976082">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Compression ratio
</figure>
<figureCaption confidence="0.9742375">
Figure 5: Bigram F-measures and compression
ratios.
</figureCaption>
<bodyText confidence="0.9990838">
compared easily. We therefore examined the rela-
tions between the average compression ratios and
evaluation scores for all methods by changing the
system summary length with the different length
parameter α introduced in Section 3.1.
In the second experiment, the system was given
a sentence and the length for the compressed sen-
tence. We compressed each input sentence to the
length of the sentence in its goal summary. This
sentence compression problem is easier than that
in which the system can generate sentences of any
length. We selected the highest-scored sentence
from the sentences of length l. Note that the re-
calls, precisions and F-measures have the same
scores in this setting.
</bodyText>
<subsectionHeader confidence="0.993698">
4.3 Results of Experiments
</subsectionHeader>
<bodyText confidence="0.999944333333333">
The results of the experiment without the sen-
tence length information are shown in Figure 4,
5 and 6. Noisy-channel indicates the results of the
noisy-channel model, ME indicates the results of
the maximum-entropy method, and ME + bottom-
up indicates the results of the maximum-entropy
</bodyText>
<figure confidence="0.998061416666667">
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Noisy-channel
ME
ME + bottom-up
</figure>
<figureCaption confidence="0.971499">
Figure 4: F-measures and compression ratios.
</figureCaption>
<figure confidence="0.999452125">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Compression ratio
Bigram F-measure
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Noisy-channel
ME
ME + bottom-up
F-measure
</figure>
<page confidence="0.621396">
854
</page>
<figure confidence="0.9952075">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Compression ratio
</figure>
<figureCaption confidence="0.978781">
Figure 6: BLEU scores and compression ratios.
</figureCaption>
<figure confidence="0.656059">
Noisy-channel ME ME+bottom-up
</figure>
<figureCaption confidence="0.9653745">
Figure 7: Results of experiments with length in-
formation.
</figureCaption>
<bodyText confidence="0.999758304347826">
method with the bottom-up method. We used the
length parameter, α, introduced in Section 3.1, and
obtained a set of summaries with different aver-
age lengths. We plotted the compression ratios
and three scores in the figures. In these figures,
a compression ratio is the ratio of the total num-
ber of words in compressed sentences to the total
number of words in the original sentences.
In these figures, our maximum entropy meth-
ods obtained higher scores than the noisy-channel
model at all compression ratios. The maximum
entropy method with the bottom-up method obtain
the highest scores on these three measures.
The results of the experiment with the sentence
length information are shown in Figure 7. In this
experiment, the scores of the maximum entropy
methods were higher than the scores of the noisy-
channel model. The maximum entropy method
with the bottom-up method achieved the highest
scores on each measure.
The results of the human judgments are shown
in Table 2. In this experiment, each length of out-
put is same as the length of goal sentence. The
</bodyText>
<table confidence="0.9949708">
Method Grammar Importance
Human 4.94 4.31
Noisy-channel 3.81 3.38
ME 3.88 3.38
ME + bottom-up 4.22 4.06
</table>
<tableCaption confidence="0.999683">
Table 2: Results of human judgments.
</tableCaption>
<bodyText confidence="0.999885454545455">
maximum entropy with the bottom-up method ob-
tained the highest scores of the three methods. We
did t-tests (5% significance). Between the noisy-
channel model and the maximum entropy with the
bottom-up method, importance is significantly dif-
ferent but grammaticality is not. Between the hu-
man and the maximum entropy with the bottom-
up method, grammaticality is significantly differ-
ent but importance is not. There are no significant
differences between the noisy-channel model and
the maximum entropy model.
</bodyText>
<subsectionHeader confidence="0.95409">
4.3.1 Problem of Negative Adverbs
</subsectionHeader>
<bodyText confidence="0.999981066666667">
One problem of the noisy-channel model is that
it cannot distinguish the meanings of removed
words. That is, it sometimes removes semantically
important words, such as “not” and “never”, be-
cause the expansion probability depends only on
non-terminals of parent and daughter nodes.
For example, our test corpus includes 15 sen-
tences that contain “not”. The noisy-channel
model removed six “not”s, and the meanings of
the sentences were reversed. However, the two
maximum entropy methods removed only one
“not” because they have “negative adverb” as a
feature in their models. The first example in Ta-
ble 3 shows one of these sentences. In this exam-
ple, only Noisy-channel removed “not”.
</bodyText>
<subsectionHeader confidence="0.995747">
4.3.2 Effect of Bottom-Up Method
</subsectionHeader>
<bodyText confidence="0.999937933333333">
Our bottom-up method achieved the highest
accuracy, in terms of F-measures, bigram F-
measures, BLEU scores and human judgments.
The results were fairly good, especially when it
summarized complex sentences, which have sen-
tences as parts. The second example in Table 3 is
a typical complex sentence. In this example, only
ME + bottom-up correctly remove “he said”.
Most of the complex sentences were correctly
compressed by the bottom-up method, but a few
sentences like the third example in Table 3 were
not. In this example, the original sentence was
parsed as shown in Figure 8 (left). If this sen-
tence is compressed to the human output, its parse
tree has to be like that in Figure 8 (middle) using
</bodyText>
<figure confidence="0.998923772727273">
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Noisy-channel
ME
ME + bottom-up
0.9
0.8
0.7
0.6
0.5
0.4
F-measure
bigram-F-measure
BLEU
BLEU score
</figure>
<page confidence="0.995647">
855
</page>
<bodyText confidence="0.808080421052632">
Original a file or application ”alias ”similar in effect to the ms-dos path
statement provides a visible icon in
folders where an aliased application
does not actually reside .
Human a file or application alias provides
a visible icon in folders where an
aliased application does not actually
reside .
Noisy- a similar in effect to ms-dos
channel statement provides a visible icon in
folders where an aliased application
does reside .
ME a or application alias statement
provides a visible icon in folders
where an aliased application does not
actually reside .
ME + a file or application statement
bottom-up provides a visible icon in folders
where an aliased application does not
</bodyText>
<table confidence="0.980311833333333">
actually reside .
Original the user can then abort the
transmission , he said .
Human the user can then abort the
transmission .
Noisy- the user can abort the transmission
channel said .
ME the user can abort the transmission
said .
ME + the user can then abort the
bottom-up transmission .
Original it is likely that both companies will
work on integrating multimedia with
database technologies .
Human both companies will work on
integrating multimedia with database
technologies .
Noisy- it is likely that both companies will
channel work on integrating .
ME it is likely that both companies will
work on integrating .
ME + it is will work on integrating
bottom-up multimedia with database technologies
.
</table>
<tableCaption confidence="0.999885">
Table 3: Examples of compressed sentences.
</tableCaption>
<bodyText confidence="0.967838272727273">
our method. When a parse tree is too long from
the root to the leaves like this, some nodes are
trimmed but others are not because we assume that
each trimming probability is independent. The
compressed sentence is ungrammatical, as in the
third example in Table 3.
We have to constrain such ungrammatical sen-
tences or introduce another rule that reconstructs
a short tree as in Figure 8 (right). That is, we in-
troduce a new transformation rule that compresses
(A1 (B (C (A2 ··· )))) to (A2 ··· ).
</bodyText>
<subsectionHeader confidence="0.998764">
4.4 Comparison with Original Results
</subsectionHeader>
<bodyText confidence="0.99931325">
We compared our results with Knight and Marcu’s
original results. They implemented two methods:
one is the noisy-channel model and the other is
a decision-based model. Each model produced
32 compressed sentences, and we calculated F-
measures, bigram F-measures, and BLEU scores.
We used the length parameter α = 0.5 for the
maximum-entropy method and α = −0.25 for
</bodyText>
<figureCaption confidence="0.9656025">
Figure 8: Parse trees of complicated complex sen-
tences.
</figureCaption>
<table confidence="0.9990771">
Method Comp. F-measure bigram F- BLEU
measure
Noisy- 70.19% 68.80 55.96 44.54
channel
Decision- 57.26% 71.25 61.93 58.21
based
ME 66.51% 73.10 62.86 53.51
ME + 58.14% 78.58 70.30 65.26
bottom-up
Human 53.59%
</table>
<tableCaption confidence="0.999986">
Table 4: Comparison with original results.
</tableCaption>
<bodyText confidence="0.999895733333333">
the maximum-entropy method with the bottom-up
method. These two values were determined using
experiments on the development set, which did not
contain the 32 test sentences.
The results are shown in Table 4. Noisy-channel
indicates the results of Knight and Marcu’s noisy-
channel model, and Decision-based indicates the
results of Knight and Marcu’s decision-based
model. Comp. indicates the compression ratio of
each result. Our two methods achieved higher ac-
curacy than the noisy-channel model. The results
of the decision-based model and our maximum-
entropy method were not significantly different.
Our maximum-entropy method with the bottom-
up method achieved the highest accuracy.
</bodyText>
<subsectionHeader confidence="0.999429">
4.5 Corpus Size and Output Accuracy
</subsectionHeader>
<bodyText confidence="0.985624166666667">
In general, using more training data improves the
accuracy of outputs and using less data results in
low accuracy. Our experiment has the problem
that the training corpus was small. To study the re-
lation between training corpus size and accuracy,
we experimented using different training corpus
sizes and compared accuracy of the output.
Figure 9 shows the relations between training
corpus size and three scores, F-measures, bigram
F-measures and BLEU scores, when we used the
maximum entropy method with the bottom-up
method. This graph suggests that the accuracy in-
</bodyText>
<figure confidence="0.972036133333333">
S
It
VP
is ADJP SBAR SBAR
both companies both companies both companies
will ... will ... will ...
(left) (middle) (right)
S
VP
S
likely that S
S
856
0 100 200 300 400 500 600 700 800
Size of training corpus
</figure>
<figureCaption confidence="0.9835095">
Figure 9: Relation between training corpus size
and evaluation score.
</figureCaption>
<bodyText confidence="0.998446666666667">
creases when the corpus size is increased. Over
about 600 sentences, the increase becomes slower.
The graph shows that the training corpus was
large enough for this study. However, if we intro-
duced other specific features, such as lexical fea-
tures, a larger corpus would be required.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999385">
We presented a maximum entropy model to ex-
tend the sentence compression methods described
by Knight and Marcu (Knight and Marcu, 2000).
Our proposals are two-fold. First, our maxi-
mum entropy model allows us to incorporate var-
ious characteristics, such as a mother node or the
depth from a root node, into a probabilistic model
for determining which part of an input sentence
is removed. Second, our bottom-up method of
matching original and compressed parse trees can
match tree structures that cannot be matched using
Knight and Marcu’s method.
The experimental results show that our maxi-
mum entropy method improved the accuracy of
sentence compression as determined by three eval-
uation criteria: F-measures, bigram F-measures
and BLEU scores. Using our bottom-up method
further improved accuracy and produced short
summaries that could not be produced by previ-
ous methods. However, we need to modify this
model to appropriately process more complicated
sentences because some sentences were not cor-
rectly summarized. Human judgments showed
that the maximum entropy model with the bottom-
up method provided more grammatical and more
informative summaries than other methods.
Though our training corpus was small, our ex-
periments demonstrated that the data was suffi-
cient. To improve our approaches, we can intro-
duce more feature functions, especially more se-
mantic or lexical features, and to deal with these
features, we need a larger corpus.
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999823666666667">
We would like to thank Prof. Kevin Knight and
Prof. Daniel Marcu for providing their parallel
corpus and the experimental results.
</bodyText>
<sectionHeader confidence="0.999185" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999742975">
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22(1):39–71.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proc. ofACL’05, pages 173–180.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge Trim-
mer: A Parse-and-Trim Approach to Headline Gen-
eration. In Proc. of DUC 2003, pages 1–8.
H. Jing and K. R. McKeown. 1999. The decomposi-
tion of human-written summary sentences. In Proc.
of SIGIR’99, pages 129–136.
K. Knight and D. Marcu. 2000. Statistics-Based Sum-
marization - Step One: Sentence Compression. In
Proc. ofAAAI/IAAI’00, pages 703–710.
I. Langkilde. 2000. Forest-Based Statistical Sentence
Generation. In Proc. of NAACL’00, pages 170–177.
C. Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Text Summarization
Branches Out: Proc. of ACL’04 Workshop, pages
74–81.
R. McDonald. 2006. Discriminative Sentence Com-
pression with Soft Syntactic Evidence. In Proc. of
EACL’06.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of ACL’02, pages 311–
318.
J. Turner and E. Charniak. 2005. Supervised and Un-
supervised Learning for Sentence Compression. In
Proc. ofACL’05, pages 290–297.
V. Vandeghinste and Y. Pan. 2004. Sentence Com-
pression for Automated Subtitling: A Hybrid Ap-
proach. In Text Summarization Branches Out: Proc.
ofACL’04 Workshop, pages 89–95.
M. J. Witbrock and V. O. Mittal. 1999. Ultra-
Summarization: A Statistical Approach to Generat-
ing Highly Condensed Non-Extractive Summaries.
In Proc. of SIGIR’99, pages 315–316.
</reference>
<figure confidence="0.9992077">
BLEU score
F-measure
bigram F-measure
Score 0.85
0.8
0.75
0.7
0.65
0.6
0.55
</figure>
<page confidence="0.932681">
857
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699707">
<title confidence="0.999929">Trimming CFG Parse Trees for Sentence Compression Using Machine Learning Approaches</title>
<author confidence="0.999307">Takashi Yusuke Jun’ichi</author>
<affiliation confidence="0.973965">of Computer Science, University of Tokyo Technology Center, University of Tokyo of Informatics, University of Manchester JST</affiliation>
<address confidence="0.943622">Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan</address>
<email confidence="0.8929415">yusuke,ninomi@r.dl.itc.u-tokyo.ac.jp</email>
<abstract confidence="0.999224909090909">Sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning. Existing methods learn statistics on trimming context-free grammar (CFG) rules. However, these methods sometimes eliminate the original meaning by incorrectly removing important parts of sentences, because trimming probabilities only depend on parents’ and daughters’ non-terminals in applied CFG rules. We apply a maximum entropy model to the above method. Our method can easily include various features, for example, other parts of a parse tree or words the sentences contain. We evaluated the method using manually compressed sentences and human judgments. We found that our method produced more grammatical and informative compressed sentences than other methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="7334" citStr="Berger et al., 1996" startWordPosition="1172" endWordPosition="1175">e each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by users. The distribution with the maximum entropy is considered the most uniform. Given two finite sets of event variables, X and Y, we estimate their joint probability distribution, P(x, y). An output, y ( Y), is produced, and A A f d G H d f D B E F C c D B F g h C c 851 contextual information, x (E X), is observed. To represent whether the event (x, y) satisfies a certain feature, we introduce a feature function. A feature function fi returns 1 iff the </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL’05,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="17047" citStr="Charniak and Johnson, 2005" startWordPosition="2924" endWordPosition="2927">f. (Knight and Marcu, 2000). This corpus consists of sentence pairs extracted automatically from the ZiffDavis corpus, a set of newspaper articles about computer products. This corpus has 1087 sentence pairs. Thirty-two of these sentences were used for the human judgments in Knight and Marcu’s experiment, and the same sentences were used for our human judgments. The rest of the sentences were randomly shuffled, and 527 sentence pairs were used as a training corpus, 263 pairs as a development corpus, and 264 pairs as a test corpus. To parse these corpora, we used Charniak and Johnson’s parser (Charniak and Johnson, 2005). 4.2 Settings of Two Experiments We experimented with/without goal sentence length for summaries. In the first experiment, the system was given only a sentence and no sentence length information. The sentence compression problem without the length information is a general task, but evaluating it is difficult because the correct length of a summary is not generally defined even by humans. The following example shows this. Original:“A font, on the other hand, is a subcategory of a typeface, such as Helvetica Bold or Helvetica Medium.” Human: “A font is a subcategory of a typeface, such as Helve</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-Fine nBest Parsing and MaxEnt Discriminative Reranking. In Proc. ofACL’05, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
<author>D Zajic</author>
<author>R Schwartz</author>
</authors>
<title>Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation.</title>
<date>2003</date>
<booktitle>In Proc. of DUC</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1786" citStr="Dorr et al., 2003" startWordPosition="252" endWordPosition="255">hat our method produced more grammatical and informative compressed sentences than other methods. 1 Introduction In most automatic summarization approaches, text is summarized by extracting sentences from a given document without modifying the sentences themselves. Although these methods have been significantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases. These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al., 2003) or messages for the small screens of mobile devices. We need to compress sentences to obtain short and useful summaries. This task is called sentence compression. While several methods have been proposed for sentence compression (Witbrock and Mittal, 1999; Jing and McKeown, 1999; Vandeghinste and Pan, 2004), this paper focuses on Knight and Marcu’s noisy-channel model (Knight and Marcu, 2000) and presents an extension of their method. They developed a probabilistic model for trimming a CFG parse tree of an input sentence. Their method drops words of input sentences but does not change their o</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation. In Proc. of DUC 2003, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K R McKeown</author>
</authors>
<title>The decomposition of human-written summary sentences.</title>
<date>1999</date>
<booktitle>In Proc. of SIGIR’99,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="2066" citStr="Jing and McKeown, 1999" startWordPosition="295" endWordPosition="298">hese methods have been significantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases. These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al., 2003) or messages for the small screens of mobile devices. We need to compress sentences to obtain short and useful summaries. This task is called sentence compression. While several methods have been proposed for sentence compression (Witbrock and Mittal, 1999; Jing and McKeown, 1999; Vandeghinste and Pan, 2004), this paper focuses on Knight and Marcu’s noisy-channel model (Knight and Marcu, 2000) and presents an extension of their method. They developed a probabilistic model for trimming a CFG parse tree of an input sentence. Their method drops words of input sentences but does not change their order or change the words. They use a parallel corpus that contains pairs of original and compressed sentences. The method makes CFG parse trees of both original and compressed sentences and learns trimming probabilities from these pairs. Although their method is concise and well-</context>
</contexts>
<marker>Jing, McKeown, 1999</marker>
<rawString>H. Jing and K. R. McKeown. 1999. The decomposition of human-written summary sentences. In Proc. of SIGIR’99, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-Based Summarization - Step One: Sentence Compression.</title>
<date>2000</date>
<booktitle>In Proc. ofAAAI/IAAI’00,</booktitle>
<pages>703--710</pages>
<contexts>
<context position="2182" citStr="Knight and Marcu, 2000" startWordPosition="312" endWordPosition="315">ten sentences; i.e., the output often has redundant words or phrases. These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al., 2003) or messages for the small screens of mobile devices. We need to compress sentences to obtain short and useful summaries. This task is called sentence compression. While several methods have been proposed for sentence compression (Witbrock and Mittal, 1999; Jing and McKeown, 1999; Vandeghinste and Pan, 2004), this paper focuses on Knight and Marcu’s noisy-channel model (Knight and Marcu, 2000) and presents an extension of their method. They developed a probabilistic model for trimming a CFG parse tree of an input sentence. Their method drops words of input sentences but does not change their order or change the words. They use a parallel corpus that contains pairs of original and compressed sentences. The method makes CFG parse trees of both original and compressed sentences and learns trimming probabilities from these pairs. Although their method is concise and well-defined, its accuracy is still unsatisfactory. Their method has two problems. One is that probabilities are calculat</context>
<context position="4017" citStr="Knight and Marcu, 2000" startWordPosition="604" endWordPosition="607">-up method, to learn complicated relations of two unmatched trees. We evaluated each algorithm using the ZiffDavis corpus, which has long and short sentence pairs. We compared our method with Knight and Marcu’s method in terms of F-measures, bigram F-measures, BLEU scores and human judgments. 850 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850–857, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Background 2.1 The Noisy-Channel Model for Sentence Compression Knight and Marcu proposed a sentence compression method using a noisy-channel model (Knight and Marcu, 2000). This model assumes that along sentence was originally a short one and that the longer sentence was generated because some unnecessary words were added. Given a long sentence l, it finds a short sentence s that maximizes P(s|l). This is equivalent to finding the s that maximizes P(s) · P(l|s) in Bayes’ Rule. The expression P(s) is the source model, which gives the probability that s is the original short string. When s is ungrammatical, P(s) becomes small. The expression P(l|s) is the channel model, which gives the probability that s is expanded to l. When s does not include important words o</context>
<context position="8531" citStr="Knight and Marcu, 2000" startWordPosition="1390" endWordPosition="1393">tion fi returns 1 iff the event (x, y) satisfies the feature i and returns 0 otherwise. Given training data {(x1, y1), · · · , (xn, yn)}, we assume that the expectation of fi on the distribution of the model conforms to that on the empirical probability distribution P(x, y). We select the probability distribution that satisfies these constraints of all feature functions and maximizes its entropy, H(P) = − Ex,y P(x, y) log (P(x, y)). 3 Methods 3.1 Maximum Entropy Model for Sentence Compression We describe a maximum entropy method as a natural extension of Knight and Marcu’s noisychannel model (Knight and Marcu, 2000). Knight and Marcu’s method uses only mother and daughter local relations in CFG parse trees. Therefore, it sometimes eliminates the meanings of the original sentences. For example, their method cannot distinguish “never” and “always” because these two adverbs are assigned the same non-terminals in parse trees. However, if “never” is removed from a sentence, the meaning of the sentence completely changes. Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu’s algorithm; however, their algorithm also uses only mother and daughter relations and has the same probl</context>
<context position="16447" citStr="Knight and Marcu, 2000" startWordPosition="2824" endWordPosition="2827">ion task (Lin, 2004). This is not our task, but it is the most similar task in his paper. We also evaluated the methods using human judgments. The evaluator is not the author but not a native English speaker. The judgment used the same criteria as those in Knight and Marcu’s methods. We performed two experiments. In the first experiment, evaluators scored from 1 to 5 points the grammaticality of the compressed sentence. In the second one, they scored from 1 to 5 points how well the compressed sentence contained the important words of the original one. We used the parallel corpus used in Ref. (Knight and Marcu, 2000). This corpus consists of sentence pairs extracted automatically from the ZiffDavis corpus, a set of newspaper articles about computer products. This corpus has 1087 sentence pairs. Thirty-two of these sentences were used for the human judgments in Knight and Marcu’s experiment, and the same sentences were used for our human judgments. The rest of the sentences were randomly shuffled, and 527 sentence pairs were used as a training corpus, 263 pairs as a development corpus, and 264 pairs as a test corpus. To parse these corpora, we used Charniak and Johnson’s parser (Charniak and Johnson, 2005)</context>
<context position="27602" citStr="Knight and Marcu, 2000" startWordPosition="4675" endWordPosition="4678">... will ... (left) (middle) (right) S VP S likely that S S 856 0 100 200 300 400 500 600 700 800 Size of training corpus Figure 9: Relation between training corpus size and evaluation score. creases when the corpus size is increased. Over about 600 sentences, the increase becomes slower. The graph shows that the training corpus was large enough for this study. However, if we introduced other specific features, such as lexical features, a larger corpus would be required. 5 Conclusion We presented a maximum entropy model to extend the sentence compression methods described by Knight and Marcu (Knight and Marcu, 2000). Our proposals are two-fold. First, our maximum entropy model allows us to incorporate various characteristics, such as a mother node or the depth from a root node, into a probabilistic model for determining which part of an input sentence is removed. Second, our bottom-up method of matching original and compressed parse trees can match tree structures that cannot be matched using Knight and Marcu’s method. The experimental results show that our maximum entropy method improved the accuracy of sentence compression as determined by three evaluation criteria: F-measures, bigram F-measures and BL</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-Based Summarization - Step One: Sentence Compression. In Proc. ofAAAI/IAAI’00, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>Forest-Based Statistical Sentence Generation.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL’00,</booktitle>
<pages>170--177</pages>
<contexts>
<context position="6444" citStr="Langkilde, 2000" startWordPosition="1033" endWordPosition="1034">g, of these subtrees are calculated. Then, each probability is assumed to be independent of the others, and the channel model, P(l|s), is calculated as the product of all expansion probabilities of joint events and PCFG scores of new Figure 1: Examples of original and compressed parse trees. subtrees: P(l|s) = � �Pe p��d(rl|rs) · P,fg(r), (rl,r3)ER rER&apos; where R is the set of rule pairs, and R&apos; is the set of generation rules in new subtrees. To compress an input sentence, they create a tree with the highest score of all possible trees. They pack all possible trees in a shared-forest structure (Langkilde, 2000). The forest structure is represented by an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their mode</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>I. Langkilde. 2000. Forest-Based Statistical Sentence Generation. In Proc. of NAACL’00, pages 170–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proc. of ACL’04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="15214" citStr="Lin, 2004" startWordPosition="2586" endWordPosition="2587">ple, from the tree of Figure 2, (S (S · · · ) (, , ) (NP I) (VP said) (..)), a new tree, (S (S · · · ) (..)), is extracted. However, the rule (S —* S .) is ungrammatical. 4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F-measures, bigram F-measures, and BLEU scores (Papineni et al., 2002). BLEU scores are usually used for evaluating machine translation quality. A BLEU score is defined as the weighted geometric average of n-gram precisions with length penalties. We used from unigram to 4-gram precisions and uniform weights for the BLEU scores. ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks. ROUGE-N uses average N-gram recall, and ROUGE-1 is word recall. ROUGE-L uses the length of the longest common subsequence (LCS) of the original and summarized sentences. In our model, the length of the LCS is equal to the number of common words, and ROUGE-L is equal to the unigram F-measure because words are not rearranged. ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener. . ADVP NP VP I never think so I said , S S S , NP VP . NP ADVP VP . D d G S B E H h A F f C c D d g B G E A </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out: Proc. of ACL’04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative Sentence Compression with Soft Syntactic Evidence.</title>
<date>2006</date>
<booktitle>In Proc. of EACL’06.</booktitle>
<contexts>
<context position="7098" citStr="McDonald, 2006" startWordPosition="1138" endWordPosition="1139">y an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by users. The distribution with the maximum entropy is considered the most uniform. Given two finite sets of event variables, X and Y, we estimate their joint probability distribution, P(x, y). An output, y ( Y), is produced, </context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative Sentence Compression with Soft Syntactic Evidence. In Proc. of EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="14937" citStr="Papineni et al., 2002" startWordPosition="2540" endWordPosition="2543">gure. These non-terminals represent joint events at each node. top top I never think so Figure 2: Example of parse tree pair that cannot be matched. Figure 3: Example of bottom-up method. Note that this “tree” is not guaranteed to be a grammatical “parse tree” by the CFG grammar. For example, from the tree of Figure 2, (S (S · · · ) (, , ) (NP I) (VP said) (..)), a new tree, (S (S · · · ) (..)), is extracted. However, the rule (S —* S .) is ungrammatical. 4 Experiment 4.1 Evaluation Method We evaluated each sentence compression method using word F-measures, bigram F-measures, and BLEU scores (Papineni et al., 2002). BLEU scores are usually used for evaluating machine translation quality. A BLEU score is defined as the weighted geometric average of n-gram precisions with length penalties. We used from unigram to 4-gram precisions and uniform weights for the BLEU scores. ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks. ROUGE-N uses average N-gram recall, and ROUGE-1 is word recall. ROUGE-L uses the length of the longest common subsequence (LCS) of the original and summarized sentences. In our model, the length of the LCS is equal to the number of </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proc. of ACL’02, pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Supervised and Unsupervised Learning for Sentence Compression. In</title>
<date>2005</date>
<booktitle>Proc. ofACL’05,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="6912" citStr="Turner and Charniak, 2005" startWordPosition="1107" endWordPosition="1110">n input sentence, they create a tree with the highest score of all possible trees. They pack all possible trees in a shared-forest structure (Langkilde, 2000). The forest structure is represented by an AND-OR tree, and it contains many tree structures. The forest representation saves memory and makes calculation faster because the trees share sub structures, and this can reduce the total number of calculations. They normalize each log probability using the length of the compressed sentence; that is, they divide the log probability by the length of the compressed sentence. Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. However their model also has the same problem. McDonald (McDonald, 2006) independently proposed a new machine learning approach. He does not trim input parse trees but uses rich features about syntactic trees and improved performance. 2.2 Maximum Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data. The model creates the most “uniform” distribution within the constraints given by users. The distribution with the maxim</context>
<context position="8987" citStr="Turner and Charniak, 2005" startWordPosition="1461" endWordPosition="1464">ximum Entropy Model for Sentence Compression We describe a maximum entropy method as a natural extension of Knight and Marcu’s noisychannel model (Knight and Marcu, 2000). Knight and Marcu’s method uses only mother and daughter local relations in CFG parse trees. Therefore, it sometimes eliminates the meanings of the original sentences. For example, their method cannot distinguish “never” and “always” because these two adverbs are assigned the same non-terminals in parse trees. However, if “never” is removed from a sentence, the meaning of the sentence completely changes. Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu’s algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. We use other information as feature functions of the maximum entropy model, and this model can deal with many features more appropriately than using simple frequency. Suppose that we trim a node in the original full parse tree. For example, suppose we have a mother node A and daughter nodes (B C D) that are derived using a CFG rule. We must leave at least one non-terminal in the daughter nodes. The trim candidates of this rule are the members of th</context>
<context position="13159" citStr="Turner and Charniak, 2005" startWordPosition="2224" endWordPosition="2227">e of the daughter nodes in the original one, the method cannot learn this joint event. A complex sentence is a typical example. A complex sentence is a sentence that includes another sentence as a part. An example of a parse tree of a complex sentence and its compressed version is shown in Figure 2. When we extract joint events from these two trees, we cannot match the two root nodes because the sequence of the daughter nodes of the root node of the compressed parse tree, (NP ADVP VP .), is not a subsequence of the daughter nodes of the original parse tree, (S , NP VP .). Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. However, there are several types of such problems like Figure 2. We need to extract these structures from a training corpus. We propose a bottom-up method to solve the problem explained above. In our method, only original sentences are parsed, and the parse trees of compressed sentences are extracted from the original parse trees. An example of this method is shown in Figure 3. The original sentence is ‘d g h f c’, and its compressed sentence is ‘d g c’. First, each ter</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>J. Turner and E. Charniak. 2005. Supervised and Unsupervised Learning for Sentence Compression. In Proc. ofACL’05, pages 290–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vandeghinste</author>
<author>Y Pan</author>
</authors>
<title>Sentence Compression for Automated Subtitling: A Hybrid Approach.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proc. ofACL’04 Workshop,</booktitle>
<pages>89--95</pages>
<contexts>
<context position="2095" citStr="Vandeghinste and Pan, 2004" startWordPosition="299" endWordPosition="302">ignificantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases. These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al., 2003) or messages for the small screens of mobile devices. We need to compress sentences to obtain short and useful summaries. This task is called sentence compression. While several methods have been proposed for sentence compression (Witbrock and Mittal, 1999; Jing and McKeown, 1999; Vandeghinste and Pan, 2004), this paper focuses on Knight and Marcu’s noisy-channel model (Knight and Marcu, 2000) and presents an extension of their method. They developed a probabilistic model for trimming a CFG parse tree of an input sentence. Their method drops words of input sentences but does not change their order or change the words. They use a parallel corpus that contains pairs of original and compressed sentences. The method makes CFG parse trees of both original and compressed sentences and learns trimming probabilities from these pairs. Although their method is concise and well-defined, its accuracy is stil</context>
</contexts>
<marker>Vandeghinste, Pan, 2004</marker>
<rawString>V. Vandeghinste and Y. Pan. 2004. Sentence Compression for Automated Subtitling: A Hybrid Approach. In Text Summarization Branches Out: Proc. ofACL’04 Workshop, pages 89–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Witbrock</author>
<author>V O Mittal</author>
</authors>
<title>UltraSummarization: A Statistical Approach to Generating Highly Condensed Non-Extractive Summaries.</title>
<date>1999</date>
<booktitle>In Proc. of SIGIR’99,</booktitle>
<pages>315--316</pages>
<contexts>
<context position="2042" citStr="Witbrock and Mittal, 1999" startWordPosition="291" endWordPosition="294">nces themselves. Although these methods have been significantly improved to extract good sentences as summaries, they are not intended to shorten sentences; i.e., the output often has redundant words or phrases. These methods cannot be used to make a shorter sentence from an input sentence or for other applications such as generating headline news (Dorr et al., 2003) or messages for the small screens of mobile devices. We need to compress sentences to obtain short and useful summaries. This task is called sentence compression. While several methods have been proposed for sentence compression (Witbrock and Mittal, 1999; Jing and McKeown, 1999; Vandeghinste and Pan, 2004), this paper focuses on Knight and Marcu’s noisy-channel model (Knight and Marcu, 2000) and presents an extension of their method. They developed a probabilistic model for trimming a CFG parse tree of an input sentence. Their method drops words of input sentences but does not change their order or change the words. They use a parallel corpus that contains pairs of original and compressed sentences. The method makes CFG parse trees of both original and compressed sentences and learns trimming probabilities from these pairs. Although their met</context>
</contexts>
<marker>Witbrock, Mittal, 1999</marker>
<rawString>M. J. Witbrock and V. O. Mittal. 1999. UltraSummarization: A Statistical Approach to Generating Highly Condensed Non-Extractive Summaries. In Proc. of SIGIR’99, pages 315–316.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>