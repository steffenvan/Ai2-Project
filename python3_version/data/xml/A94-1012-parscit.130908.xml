<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.9980465">
Combination of Symbolic and Statistical Approaches for
Grammatical Knowledge Acquisition
</title>
<author confidence="0.994858">
Masaki KIYONO* and Jun&apos;ichi TSUJII
</author>
<affiliation confidence="0.998587">
Centre for Computational Linguistics
University of Manchester Institute of Science and Technology
</affiliation>
<address confidence="0.958862">
PO Box 88, Manchester, M60 1QD, United Kingdom
</address>
<email confidence="0.920378">
kiyonoOccl.umist.ac.uk, tsujiiOccl.umist.ac.uk
</email>
<sectionHeader confidence="0.998046" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992263157895">
The framework we adopted for customiz-
ing linguistic knowledge to individual ap-
plication domains is an integration of sym-
bolic and statistical approaches. In or-
der to acquire domain specific knowledge,
we have previously proposed a rule-based
mechanism to hypothesize missing knowl-
edge from partial parsing results of unsuc-
cessfully parsed sentences. In this paper,
we focus on the statistical process which se-
lects plausible knowledge from a set of hy-
potheses generated from the whole corpus.
In particular, we introduce two statistical
measures of hypotheses, Local Plausibility
and Global Plausibility, and describe how
these measures are determined iteratively.
The proposed method will be incorporated
into the tool kit for linguistic knowledge
acquisition which we are now developing.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9767255">
Current technologies in natural language process-
ing are not so mature as to make general purpose
systems applicable to any domains; therefore rapid
customization of linguistic knowledge to the sub-
language of an application domain is vital for the
development of practical systems. In the currently
working systems, such customization has been car-
ried out manually by linguists or lexicographers with
time-consuming effort.
We have already proposed a mechanism which
acquires sublanguage-specific linguistic knowledge
from parsing failures and which can be used as a
tool for linguistic knowledge customization (Kiyono
and Tsujii, 1993; Kiyono and Tsujii, 1994). Our ap-
proach is characterized by a mixture of symbolic and
statistical approaches to grammatical knowledge ac-
quisition. Unlike probabilistic parsing, proposed by
(Fujisaki et al., 1989; Briscoe and Carroll, 1993),
*also a staff member of Matsushita Electric Industrial
Co. ,Ltd., Shinagawa, Tokyo, JAPAN.
which assumes the prior existence of comprehensive
linguistic knowledge, our system can suggest new
pieces of knowledge including CFG rules, subcate-
gorization frames, and other lexical features. It also
differs from previous proposals on lexical acquisi-
tion using statistical measures such as (Church et
al., 1991; Brent, 1991; Brown et al., 1993) which ei-
ther deny the prior existence of linguistic knowledge
or use linguistic knowledge in ad hoc ways.
Our system consists of two components: (1) the
rule-based component, which detects incompleteness
of the existing knowledge and generates a set of hy-
potheses of new knowledge and (2) the corpus-based
component which selects plausible hypotheses on the
basis of their statistical behaviour. As the rule-based
component has been explained in our previous pa-
pers, in this paper we focus on the corpus-based
component.
After giving a brief explanation of the framework,
we describe a data structure called Hypothesis Graph
which plays a crucial role in the corpus-based pro-
cess, and then introduce two statistical measures of
hypotheses, Global Plausibility and Local Plausibil-
ity, which are iteratively determined to select a set
of plausible hypotheses. An experiment which shows
the effectiveness of our method is also given.
</bodyText>
<sectionHeader confidence="0.974754" genericHeader="method">
2 The System Organization
</sectionHeader>
<subsectionHeader confidence="0.990533">
2.1 Hypothesis Generation
</subsectionHeader>
<bodyText confidence="0.999882857142857">
Figure 1 shows the framework of our system. When
the parser fails to analyse a sentence, the Hypothe-
sis Generator (HG) produces hypotheses of missing
knowledge each of which could rectify the defects of
the current grammar. As the parser is a sort of Chart
Parser and maintains partial parsing results in the
form of inactive and active edges, a parsing failure
means that no inactive edge of category S spanning
the whole sentence exists.
The HG tries to introduce an inactive edge of S by
making hypotheses of missing linguistic knowledge.
It generates hypotheses of rewriting rules which col-
lect existing sequences of inactive edges into an ex-
pected category. It also calls itself recursively to in-
</bodyText>
<page confidence="0.996134">
72
</page>
<figureCaption confidence="0.7144275">
Rule-Based Component Corpus-Based Component
Figure 1: Framework of Grammar Acquisition
</figureCaption>
<bodyText confidence="0.884139352941176">
troduce necessary inactive edges for each rule of the
expected category whose application is prevented
due to the lack of necessary inactive edges. The
simplest form of the algorithm is shown below.
[Algorithm] An inactive edge [ie(A) : x 0, xn] can
be introduced, with label A, between word po-
sitions xo and xn by each of the hypotheses gen-
erated from the following two steps.
[Step 1] For each sequence of inactive edges,
[ie(B 1) : so, x1], . . . , POO : n -1, xni,
spanning from so to xn, generates a new
rule.
A Bi , • • , Brz
[Step 21 For each existing rule of form A
A1, • • ,A, finds an incomplete sequence
of inactive edges, [ie(Ai) : xo, Sib • • •
[ie(Ai_i): xi-2, xj_i], [ie(Ai+i) : xi,
</bodyText>
<equation confidence="0.982492">
. . ,
{ie(A) : x _1, xi,], and calls this algo-
rithm for [ie(Ai) : xi_i, xi].
</equation>
<bodyText confidence="0.9997396">
This algorithm has been further augmented in or-
der to treat sentences which contain more than one
construction not covered by the current version of
the grammar and to generate hypotheses concern-
ing complex features like subcategorization frames.
</bodyText>
<subsectionHeader confidence="0.999824">
2.2 Hypothesis Filtering
</subsectionHeader>
<bodyText confidence="0.999598428571429">
The greater number of the hypotheses generated by
the algorithm are linguistically unnatural, because
the algorithm does not embody any linguistic prin-
ciple to judge the appropriateness of hypotheses, and
therefore we introduced a set of criteria to filter out
unnatural hypotheses (Kiyono and Tsujii, 1993; Kiy-
ono and Tsujii, 1994). This includes, for example,
</bodyText>
<listItem confidence="0.993147285714286">
• The maximum number of daughter constituents
of a rule is set to 3.
• Supposing that the current version of the gram-
mar contains all the category conversion rules,
a unary rule with one daughter constituent is
not generated.
• Using generalizations embodied in the current
version of the grammar, a rule containing a se-
quence of constituents which can be collected
into a larger constituent by the current version
of grammar is not generated.
• Distinguishing non-lexical categories from lexi-
cal categories, a rule whose mother category is
a lexical category is not generated.
</listItem>
<bodyText confidence="0.8940415">
These criteria significantly reduce the number of
hypotheses to be generated.
</bodyText>
<subsectionHeader confidence="0.999767">
2.3 Hypothesis Graph
</subsectionHeader>
<bodyText confidence="0.998068261904762">
As the criteria which the HG uses to filter out un-
natural hypotheses are solely based on the forms of
hypotheses, they cannot identify the &amp;quot;correct&amp;quot; hy-
potheses on their own. The correct ones are rather
chosen by the Hypothesis Selector (HS), which re-
sorts to examining the statistical behaviour of hy-
potheses throughout a given corpus.
A straightforward method is to count the fre-
quency of hypotheses, but this simple method does
not work, because hypotheses are not independent of
each other. A hypothesis is either competing with or
complementary to other hypotheses generated from
the same sentence. A group of hypotheses generated
for restoring the same inactive edge constitutes a set
of competing hypotheses and only one of them con-
tributes to the correct structure of the sentence. On
the other hand, two groups of hypotheses which are
generated to treat two different parts of the same
sentence stand in complementary relationships.
A hypothesis should be recognized as being cor-
rect, only when no other competing hypothesis is
more plausible. That is, even if a hypothesis is gen-
erated frequently, it should not be chosen as the
correct one, if more plausible competing hypothe-
ses are always generated together with it. On the
other hand, even if a hypothesis is generated only
once, it should be chosen as the correct one, if there
is no other competing hypothesis.
In order to realize the above conception, the HS
maintains mutual relationships among hypotheses
as an AND-OR graph. In a graph, AND nodes
and OR nodes express complementary relationships
and competing relationships, respectively. A node is
shared, when different recursion steps in the HG try
to restore the same inactive edge. Figure 2 shows the
AND-OR graph for the hypotheses generated from
the sentence &amp;quot;Failing students looked embarrassed&amp;quot;
when the current version of grammar does not con-
tain rules for participles. The top node is an AND
node which has two groups of hypotheses that treat
two different parts of the sentence, i.e. &amp;quot;failing stu-
dents&amp;quot; and &amp;quot;looked embarrased&amp;quot;.
</bodyText>
<figure confidence="0.984148956521739">
(Hypothesis DB)
Hypotheses
Sentence
Corpus
Parser
Parsing Result
Hypothesis
Generator
Grammar
Humantinteraction
( Plausible )
Hypotheses
Hypothesis
Selector
73
Sentence: Failing students looked embarrassed. 3.2 Initial Estimation of Local Plausibility
HP1: NPVP,NP (&amp;quot;failing students&amp;quot;)
11P2: AD J = [failing]
HP3: VP VP, VP (&amp;quot;looked embarrassed&amp;quot;)
HP4: ADV [embarrassed]
11P5: N [embarrassed]
HP6: AD J [embarrassed]
(AND)
</figure>
<figureCaption confidence="0.997273">
Figure 2: AND-OR Graph of Hypotheses
</figureCaption>
<sectionHeader confidence="0.986193" genericHeader="method">
3 Statistical Analysis
</sectionHeader>
<subsectionHeader confidence="0.99764">
3.1 Two Measures of Plausibility
</subsectionHeader>
<bodyText confidence="0.9999135">
The HS uses two measures of plausibility of hypothe-
ses. One is computed for an instance hypothesis and
the other is for a generic hypothesis. (See 3.3 for the
relationship between the two types of hypotheses.)
</bodyText>
<listItem confidence="0.890196285714286">
(1) Local Plausibility: This value shows how
plausible an instance hypothesis is as grammat-
ical knowledge to contribute to the correct anal-
ysis of a unsuccessfully parsed sentence.
(2) Global Plausibility: This value shows how
plausible the hypothesis of the generic form is
as grammatical knowledge to be acquired.
</listItem>
<bodyText confidence="0.996479090909091">
As we describe in the following section, the Lo-
cal Plausibility (LP) of an instance hypothesis is
computed on the basis of the values of the Global
Plausibility (GP) of the generic hyoptheses which
are linked to instance hypotheses in the same hy-
pothesis graph. On the other hand, the GP of a
generic hypothesis is computed from the LP values
of its instance hypotheses across the whole corpus.
Intuitively speaking, the GP of a generic hypoth-
esis is high if its instances are frequently generated
and if they receive high LP values, while the LP of
a instance hypothesis is high if the GP of the cor-
responding generic hypothesis is high and if the GP
values of the generic hypotheses corresponding to its
competing hypotheses are low. Because of this mu-
tual dependence between LP and GP, they cannot
be computed in a single step but rather computed
iteratively by repeating the following steps until the
halt condition is satisfied.
[Step 1] Estimates the initial values of LP.
[Step 2] Calculates GP values from LP values.
[Step 3] Checks the halt condition.
[Step 4] Calculates LP values from GP values and
GOTO [Step 2].
If the current version of the grammar is reason-
ably comprehensive, pieces of linguistic knowledge
which have to be acquired are likely to be lex-
ical or idiosyncratic. That is, we assume that
sublanguage-specificity tends to be manifested by
unknown words, new usages of existing words, and
syntactic constructions idiosyncratic to the sublan-
guage. In order to quantify such plausibility, the
following value is given to each hypothesis.
</bodyText>
<equation confidence="0.999284">
LP W(Hypoi) x H(Hypoi)
(Hypoi = 1 )
W(S) x H(S)
</equation>
<bodyText confidence="0.99996925">
This value shows the proportion of the syntactic
structure in the whole sentence which is not covered
by the hypothesis. It ranges from 0 to 1 and gets
larger if the hypothesis rectifies a smaller part of
the sentence. W(Hypoi), the width of the hypothe-
sis, is defined as the word count of the subtree and
H(Hypoi), the height, is defined as the shortest path
from lexical nodes to the top node of the subtree.
</bodyText>
<subsectionHeader confidence="0.8654435">
3.3 Generic Hypothesis and Global
Plausibility
</subsectionHeader>
<bodyText confidence="0.999740771428571">
The GP of a hypothesis is computed based on the
LP values of its instance hypotheses, but the rela-
tionship between a generic hypothesis and its in-
stances is not straightforward because we adopted
a unification-based grammar formalism. For exam-
ple, the instance hypothesis of NP = VP, NP in
Figure 2 contains not only this CFG skeleton but
also further feature descriptions of the three con-
stituents which include specific surface words like
&amp;quot;failing&amp;quot; and &amp;quot;students&amp;quot;. Unless we generalize them,
we cannot obtain the generic form of this instance
hypothesis, and therefore cannot judge whether the
hypotheses generated from different sentences are
identical.
Such generalization of instance hypotheses re-
quires an induciive mechanism for judging which
parts of the feature specification are common to all
instance hypotheses and should be included in a hy-
pothesis of the generic form. This kind of induc-
tion is beyond the scope of the current framework,
because such induction may need a lot of time and
space if it is carried out from scratch. We first gather
a set of instance hypotheses which are likely to be
instances of the same generic hypothesis which, in
turn, is likely to be &amp;quot;correct&amp;quot; linguistic knowledge.
Our current framework uses a simple definition of
generic hypotheses and their instances. That is, if
two rule hypotheses have the same CFG skeleton,
then they are judged to be instances of the same
generic hypotheses. As for lexical hypotheses, we
use a set of fixed templates of lexical entries in or-
der to acquire detailed knowledge like subcategoriza-
tion frames. Features which are not included in the
templates are ignored in the judgement of whether
generic hypotheses are identical.
</bodyText>
<page confidence="0.991084">
74
</page>
<figure confidence="0.987708333333333">
TOP
0.05 0.1 0.9
(a) Collection of Global Plausibility
TOP
0.06 0.94
(b) Distribution of Local Plausibility
</figure>
<figureCaption confidence="0.999991">
Figure 3: Calculation of Local Plausibility
</figureCaption>
<bodyText confidence="0.999401">
The GP of a generic hypotheses is defined as being
the probability of the event that at least one instance
hypothesis recovers the true cause of a parsing fail-
ure, and it is computed by the following formula
when a set of its instance hypotheses is identified.
In the formula, HP is a generic hypothesis and HPi
are its instances.
</bodyText>
<equation confidence="0.994215">
GP(HP) = 1—H(1— LP(HPi))
</equation>
<bodyText confidence="0.999970833333333">
The more instance hypotheses are generated, the
closer to 1 GP(HP) becomes. If one of the instances
is regarded to be recovering the true cause of a pars-
ing failure, the GP of the generic hypothesis is as-
signed 1, because the hypothesis is indispensable to
the analysis of the corpus.
</bodyText>
<subsectionHeader confidence="0.993614">
3.4 Local Plausibility
</subsectionHeader>
<bodyText confidence="0.999914772727272">
The calculation of LP is carried out on each hypoth-
esis graph based on the assumption that an instance
hypothesis or a set of instance hypotheses which re-
covers the true cause(s) of the parsing failure should
exist in the graph. This assumption means that the
top node of a hypothesis graph is assigned 1 as its
LP value.
The LP value assigned to a node is to be dis-
tributed to its daughter nodes by considering the
GP values of the corresponding generic hypotheses.
For example, the daughter nodes of an OR node,
which constitute a set of competing hypotheses, re-
ceive their LP values which are dividents of the LP
value of the mother node proportional to their GP
values.
However, as GP is defined only for hypotheses,
we first determine the GP values of all nodes in a
hypothesis graph in a bottom-up manner, starting
from the tip nodes of the graph to which instance
hypotheses are attached. Therefore, [Step 2] in the
statistical analysis is further divided into the follow-
ing three steps.
</bodyText>
<subsectionHeader confidence="0.731305">
[Step 2-1] Bottom-up Calculation of GP
</subsectionHeader>
<bodyText confidence="0.719002857142857">
The GP value of an intermediate node is deter-
mined as follows (See Figure 3(a)).
• The GP value of an OR node is computed by the
following formula based on the GP values of the
daughter nodes, which corresponds to the prob-
ability that at least one of the daughter nodes
represents &amp;quot;correct&amp;quot; grammatical knowledge.
</bodyText>
<equation confidence="0.969246">
GP(OR)= 1—H(1 — GP(Nodei))
i.1
</equation>
<listItem confidence="0.93742625">
• The GP value of an AND node is computed by
the following formula, which corresponds to the
probability that all the daughter nodes repre-
sent &amp;quot;correct&amp;quot; grammatical knowledge.
</listItem>
<equation confidence="0.4813735">
fri
GP(AND)= GP(Nodei)
</equation>
<subsectionHeader confidence="0.311659">
[Step 2-2] Deletion of Hypotheses
</subsectionHeader>
<bodyText confidence="0.999818631578947">
The nodes which have significantly smaller GP
values than the highest one among the daughter
nodes of the same mother OR node (less than one
tenth, in our current implementation) will be re-
moved from the hypothesis graph. For example,
HP2 in Figure 3 was considered to be much less plau-
sible than HP4 and removed from the graph.
As a node in a hypothesis graph could have more
than one mother nodes, the hypothesis deletion is
realized by removing the link between the node rep-
resenting the hypothesis and one of its mother OR
nodes (not removing the node itself). For example,
in Figure 3, when HP4 is removed in comparison
with HP2 or HP3, the link between HP and the
OR node is removed, while the link between HP4
and the AND node still remains.
The deletion of less viable nodes accelerates the
convergence of the iterative process of computing
GP and LP.
</bodyText>
<page confidence="0.996653">
75
</page>
<bodyText confidence="0.879045">
[Step 2-3] Top-down Calculation of LP
This step distributes the LP assigned to the top
node (that is, 1) to the nodes below in a top-
down way according to the following rules (See Fig-
ure 3(b)).
</bodyText>
<listItem confidence="0.821136166666667">
• The LP value of an OR node is distributed to its
daughter nodes proportional to their GP values
so that the sum of their LP values is the same
as that of the OR, node because the daughter
nodes of the same OR node represent mutually
exclusive hypotheses.
</listItem>
<equation confidence="0.741971">
GP(Nodei)
LP(Nodea)- Ein„1GP(Nodej)LP(OR)
</equation>
<listItem confidence="0.9191915">
• The LP value of an AND node is distributed to
its daughter nodes with the same values.
</listItem>
<equation confidence="0.979078">
LP(Node) = LP(AND)
</equation>
<bodyText confidence="0.999868">
If a hypothesis has more than one mother nodes
and its LP can be calculated through several paths,
the sum of those is given to the hypothesis. For
example, the value for HP4 in Figure 3 is 0.56 +
0.38 = 0.94.
As we discussed before, these newly computed LP
values are used to compute the GP values at [Step
2] in the next cycle of iteration.
</bodyText>
<subsectionHeader confidence="0.971882">
3.5 Halt Condition
</subsectionHeader>
<bodyText confidence="0.999517733333333">
The iterative calculation process is regarded to have
converged if the GP values of all the generic hypothe-
ses do not change in comparison with the previous
cycle, but as it possibly takes a lot of time for the
process to reach such a situation, we use an easier
condition to stop the process. That is, we count
the number of deleted instance hypotheses at each
cycle and terminate the iteration when no instance
hypothesis is deleted in a number of consecutive it-
erations. Actually, the process halts after 5 zero-
deletion cycles in our current implementation.
When the interative process terminates, the hy-
potheses with high GP values are presented as the
final candidates of new knowledge to be added to
the current version of grammar.
</bodyText>
<sectionHeader confidence="0.983566" genericHeader="method">
4 Preliminary Experiment
</sectionHeader>
<bodyText confidence="0.970072272727273">
In order to demonstrate how the HS works, we car-
ried out a preliminary experiment with 1,000 sen-
tences in the UNIX on-line manual (approximately
one fifth of the whole manual). As the initial knowl-
edge for the experiment, we prepared a grammar set
which contains 120 rules covering English basic ex-
pressions and deliberately removed rules for partici-
ples in order to check whether the HS can discover
adequate rules. The input data to the statistical pro-
cess is a set of 5,906 instance hypotheses generated
from 282 unsuccessfully parsed sentences.
</bodyText>
<table confidence="0.999362592592593">
# GP Generic Hypothesis N
1 1.000000 vp =&gt; vp,p. 20
1 1.000000 up =&gt; vp,np. 26
1 1.000000 n =&gt; [&apos;double-quote&apos;] . 1
1 1.000000 n =&gt; [filename]. 8
1 1.000000 v =&gt; [archived]. 1
1 1.000000 n =&gt; [directory] . 4
1 1.000000 n =&gt; [&apos;EOF&apos;]. 2
1 1.000000 adj =&gt; [&apos;non-printing&apos;]. 1
1 1.000000 n =&gt; [patimames] . 1
1 1.000000 n =&gt; [cpp] . 3
1 1.000000 n =&gt; [&apos;NEWLINE&apos;] . 3
1 1.000000 n =&gt; [&apos; .cshrc&apos;]. 2
1 1.000000 n =&gt; [backslash] . 2
1 1.000000 n =&gt; [aliases]. 2
1 1.000000 adj =&gt; [nonseekable]. 1
1 1.000000 n =&gt; [wordlist]. 2
1 1.000000 n =&gt; [login]. 3
138 0.925960 n =&gt; PTERM1 . 2
142 0.913933 n =&gt; [cmdtool]. 1
173 0.750000 n =&gt; [&apos;command-line&apos;]. 2
189 0.683594 n =&gt; [filenames]. 1
232 0.500000 adj =&gt; [backquoted] • 1
318 0.336694 up =&gt; np,np. 12
546 0.000000 adj =&gt; [blocking]. 1
546 0.000000 adj =&gt; [invisible]. 1
#: Rank, N: Number of instance hypotheses
</table>
<tableCaption confidence="0.999683">
Table 1: List of &amp;quot;Correct&amp;quot; Hypotheses
</tableCaption>
<bodyText confidence="0.999923133333333">
The statistical process removed 4,034 instance hy-
potheses and stopped after 63 cycles of the iterative
computation of GP and LP. The instance hypotheses
were grouped into 2,876 generic hypotheses and the
GP values of 2,331 generic hypotheses were reduced
to 0 by the hypothesis deletion.
Table 1 is the list of &amp;quot;correct&amp;quot; hypotheses picked
up from the whole list of generic hypotheses sorted
by GP values. The hypothesis for participles, up =&gt;
vp ,fl, is one of the 128 hypotheses whose GP val-
ues are 1. This table also shows that quite a few
&amp;quot;correct&amp;quot; lexical hypotheses are in higher positions
because lexical knowledge for unknown words is in-
dispensable to the successful parsing of the corpus.
The distribution of &amp;quot;correct&amp;quot; hypotheses within
the whole list is shown in Table 2. The fact that
&amp;quot;correct&amp;quot; hypotheses exist more in higher ranges
supports our mechanism. Although some of the
&amp;quot;correct&amp;quot; ones have zero GP values, they do not di-
minish our framework because most of them are the
hypotheses treating participles as adjectives, which
are the alternative hypotheses of up =&gt; vp ,np.
The parameter which we can adjust to select
more plausible hypotheses is the threshold for the
hypothesis deletion. Generally speaking, giving a
higher threshold causes an increase of the number
of deleted hypotheses and therefore accelerates the
convergence of the iterative process. In the experi-
ment, however, the use of one fifth as the threshold
instead of one tenth did not bring a major difference.
</bodyText>
<page confidence="0.954592">
76
</page>
<table confidence="0.999888">
Range Rule Lexical Correct
Hypothesis Hypothesis Hypothesis
1— 100 19 81 35
101— 200 41 59 25
201— 300 55 45 13
301— 400 76 24 5
401— 500 95 5 0
501-1000 473 27 9
1001-2000 770 230 12
2001-2876 653 223 9
Total 2182 694 108
</table>
<tableCaption confidence="0.993313">
Table 2: Distribution of &amp;quot;Correct&amp;quot; Hypotheses
</tableCaption>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.949931222222222">
The statistical analysis discussed in this paper is
based on the assumption that types of linguistic
knowledge to be acquired are:
[1] Knowledge for syntactic constructions which is
used frequently in the given sublanguage.
[2] Lexical knowledge such as subcategorization
frames and number properties, which is often
idiosyncratic to the given sublanguage.
[3] Knowledge which belongs neither to [1] nor to
[2], but is indispensable to the given corpus.
[1] implies that knowledge for less frequent con-
structions can be ignored at the initial stage of lin-
guistic knowledge customization. Such knowledge
will be discovered after major defects of the current
grammar are rectified, because the GP of a generic
hypothesis is defined as being sensitive to the fre-
quency of the hypothesis.
[2] means that we assume that the set of initially
provided grammar rules has a comprehensive cov-
erage of English basic expressions. This assump-
tion is reflected in the way of the initial estimation
of LP values. Also note that only when this as-
sumption is satisfied, can the HG produce a reason-
able set of hypotheses. On the other hand, because
of this assumption, our framework can learn struc-
turally complex and linguistically meaningful lexical
descriptions, like a subcategorization frame.
[3] is reflected in the way of the computation of
GP values. A generic hypothesis one of whose in-
stances occurs as a single possible hypothesis that
can recover a parsing failure will have the GP value
of 1, even though its frequency is very low.
The computation mechanism of GP and LP bears
a resemblance to the EM algorithm(Dempster et al.,
1977; Brown et al., 1993), which iteratively com-
putes maximum likelihood estimates from incom-
plete data. As the purpose of our statistical analysis
is to choose &amp;quot;correct&amp;quot; hypotheses from a hypothe-
sis set which contains unnatural hypotheses as well,
our motivation is different from that of the EM algo-
rithm. However, if we consider that the hypothesis
deletion is maxmizing the plausibility of &amp;quot;correct&amp;quot;
hypotheses, the computation procedures of both al-
gorithms have a strong similarity.
The grammatical knowledge acquisition method
proposed in this paper will be incorporated into the
tool kit for linguistic knowledge customization which
we are now developing. In the practical use of our
method, a grammar maintainer will be shown a list
of hypotheses with high GP values and renew the
current version of grammatical knowledge. The re-
newed knowledge will be used in the next cycle of
hypothesis generation and selection to achieve the
gradual enlargement of linguistic knowledge.
</bodyText>
<sectionHeader confidence="0.997826" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99999775">
We would like to thank our colleagues in UMIST
who gave us many usuful comments. We also want to
thank Mr Tsumura and Dr Kawakami of Matsushita,
who allowed the first author to study at UMIST.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999306909090909">
Michael R. Brent. 1991. Automatic Acquisition
of Subcategorization Frames from Untagged Text.
In Proc. of the 29st ACL meeting, pages 209-214.
Ted Briscoe and John Carroll. 1993. General-
ized Probabilistic LR Parsing of Natural Lan-
guage (Corpora) with Unification-Based Gram-
mars. Computational Linguistics, 19(1):25-59.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguis-
tics, 19(2):263-311.
Kenneth Church, William Gale, Patrick Hanks, and
Donald Hindle. 1991. Using Statistics in Lexi-
cal Analysis. In Uri Zernik, editor, Lexical Acqui-
sition: Exploiting On-Line Resources to Build a
Lexicon, chapter 6, pages 115-164. Lawrence Erl-
baum Associates.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via
the EM Algorithm. Journal of the Royal Statisti-
cal Society, 39(B):1-38.
T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and T.
Nishino. 1989. A Probabilistic Parsing Method
for Sentence Disambiguation. In Proc. of the Int.
Workshop on Parsing Technologies, pages 105-
114. Carnegie-Mellon University.
Masaki Kiyono and Jun&apos;ichi Tsujii. 1993. Linguistic
Knowledge Acquisition from Parsing Failures. In
Proc. of EACL-93, pages 222-231.
Masaki Kiyono and Jun&apos;ichi Tsujii. 1994. Hypothe-
sis Selection in Grammar Acquisition. In Proc. of
COLING-94.
</reference>
<page confidence="0.999121">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720460">
<title confidence="0.9889985">Combination of Symbolic and Statistical Approaches for Grammatical Knowledge Acquisition</title>
<author confidence="0.736346">KIYONO TSUJII</author>
<affiliation confidence="0.9992945">Centre for Computational Linguistics University of Manchester Institute of Science and Technology</affiliation>
<address confidence="0.999935">88, Manchester, M60 1QD, United Kingdom</address>
<email confidence="0.996568">kiyonoOccl.umist.ac.uk,tsujiiOccl.umist.ac.uk</email>
<abstract confidence="0.9992435">The framework we adopted for customizing linguistic knowledge to individual application domains is an integration of symbolic and statistical approaches. In order to acquire domain specific knowledge, we have previously proposed a rule-based mechanism to hypothesize missing knowledge from partial parsing results of unsuccessfully parsed sentences. In this paper, we focus on the statistical process which selects plausible knowledge from a set of hypotheses generated from the whole corpus. In particular, we introduce two statistical of hypotheses, Plausibility Plausibility, describe how these measures are determined iteratively. The proposed method will be incorporated into the tool kit for linguistic knowledge acquisition which we are now developing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>Automatic Acquisition of Subcategorization Frames from Untagged Text.</title>
<date>1991</date>
<booktitle>In Proc. of the 29st ACL meeting,</booktitle>
<pages>209--214</pages>
<contexts>
<context position="2423" citStr="Brent, 1991" startWordPosition="345" endWordPosition="346">acterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN. which assumes the prior existence of comprehensive linguistic knowledge, our system can suggest new pieces of knowledge including CFG rules, subcategorization frames, and other lexical features. It also differs from previous proposals on lexical acquisition using statistical measures such as (Church et al., 1991; Brent, 1991; Brown et al., 1993) which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways. Our system consists of two components: (1) the rule-based component, which detects incompleteness of the existing knowledge and generates a set of hypotheses of new knowledge and (2) the corpus-based component which selects plausible hypotheses on the basis of their statistical behaviour. As the rule-based component has been explained in our previous papers, in this paper we focus on the corpus-based component. After giving a brief explanation of the framework, we des</context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>Michael R. Brent. 1991. Automatic Acquisition of Subcategorization Frames from Untagged Text. In Proc. of the 29st ACL meeting, pages 209-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="2004" citStr="Briscoe and Carroll, 1993" startWordPosition="282" endWordPosition="285">evelopment of practical systems. In the currently working systems, such customization has been carried out manually by linguists or lexicographers with time-consuming effort. We have already proposed a mechanism which acquires sublanguage-specific linguistic knowledge from parsing failures and which can be used as a tool for linguistic knowledge customization (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN. which assumes the prior existence of comprehensive linguistic knowledge, our system can suggest new pieces of knowledge including CFG rules, subcategorization frames, and other lexical features. It also differs from previous proposals on lexical acquisition using statistical measures such as (Church et al., 1991; Brent, 1991; Brown et al., 1993) which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways. Our system consists of two components: (1) the rule-b</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized Probabilistic LR Parsing of Natural Language (Corpora) with Unification-Based Grammars. Computational Linguistics, 19(1):25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="2444" citStr="Brown et al., 1993" startWordPosition="347" endWordPosition="350">a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN. which assumes the prior existence of comprehensive linguistic knowledge, our system can suggest new pieces of knowledge including CFG rules, subcategorization frames, and other lexical features. It also differs from previous proposals on lexical acquisition using statistical measures such as (Church et al., 1991; Brent, 1991; Brown et al., 1993) which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways. Our system consists of two components: (1) the rule-based component, which detects incompleteness of the existing knowledge and generates a set of hypotheses of new knowledge and (2) the corpus-based component which selects plausible hypotheses on the basis of their statistical behaviour. As the rule-based component has been explained in our previous papers, in this paper we focus on the corpus-based component. After giving a brief explanation of the framework, we describe a data structur</context>
<context position="23068" citStr="Brown et al., 1993" startWordPosition="3853" endWordPosition="3856">ssumption is satisfied, can the HG produce a reasonable set of hypotheses. On the other hand, because of this assumption, our framework can learn structurally complex and linguistically meaningful lexical descriptions, like a subcategorization frame. [3] is reflected in the way of the computation of GP values. A generic hypothesis one of whose instances occurs as a single possible hypothesis that can recover a parsing failure will have the GP value of 1, even though its frequency is very low. The computation mechanism of GP and LP bears a resemblance to the EM algorithm(Dempster et al., 1977; Brown et al., 1993), which iteratively computes maximum likelihood estimates from incomplete data. As the purpose of our statistical analysis is to choose &amp;quot;correct&amp;quot; hypotheses from a hypothesis set which contains unnatural hypotheses as well, our motivation is different from that of the EM algorithm. However, if we consider that the hypothesis deletion is maxmizing the plausibility of &amp;quot;correct&amp;quot; hypotheses, the computation procedures of both algorithms have a strong similarity. The grammatical knowledge acquisition method proposed in this paper will be incorporated into the tool kit for linguistic knowledge custo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
<author>Patrick Hanks</author>
<author>Donald Hindle</author>
</authors>
<title>Using Statistics in Lexical Analysis. In</title>
<date>1991</date>
<booktitle>Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, chapter 6,</booktitle>
<pages>115--164</pages>
<editor>Uri Zernik, editor,</editor>
<contexts>
<context position="2410" citStr="Church et al., 1991" startWordPosition="341" endWordPosition="344"> Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN. which assumes the prior existence of comprehensive linguistic knowledge, our system can suggest new pieces of knowledge including CFG rules, subcategorization frames, and other lexical features. It also differs from previous proposals on lexical acquisition using statistical measures such as (Church et al., 1991; Brent, 1991; Brown et al., 1993) which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways. Our system consists of two components: (1) the rule-based component, which detects incompleteness of the existing knowledge and generates a set of hypotheses of new knowledge and (2) the corpus-based component which selects plausible hypotheses on the basis of their statistical behaviour. As the rule-based component has been explained in our previous papers, in this paper we focus on the corpus-based component. After giving a brief explanation of the fram</context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1991</marker>
<rawString>Kenneth Church, William Gale, Patrick Hanks, and Donald Hindle. 1991. Using Statistics in Lexical Analysis. In Uri Zernik, editor, Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, chapter 6, pages 115-164. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="23047" citStr="Dempster et al., 1977" startWordPosition="3849" endWordPosition="3852">e that only when this assumption is satisfied, can the HG produce a reasonable set of hypotheses. On the other hand, because of this assumption, our framework can learn structurally complex and linguistically meaningful lexical descriptions, like a subcategorization frame. [3] is reflected in the way of the computation of GP values. A generic hypothesis one of whose instances occurs as a single possible hypothesis that can recover a parsing failure will have the GP value of 1, even though its frequency is very low. The computation mechanism of GP and LP bears a resemblance to the EM algorithm(Dempster et al., 1977; Brown et al., 1993), which iteratively computes maximum likelihood estimates from incomplete data. As the purpose of our statistical analysis is to choose &amp;quot;correct&amp;quot; hypotheses from a hypothesis set which contains unnatural hypotheses as well, our motivation is different from that of the EM algorithm. However, if we consider that the hypothesis deletion is maxmizing the plausibility of &amp;quot;correct&amp;quot; hypotheses, the computation procedures of both algorithms have a strong similarity. The grammatical knowledge acquisition method proposed in this paper will be incorporated into the tool kit for lingu</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisaki</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A Probabilistic Parsing Method for Sentence Disambiguation.</title>
<date>1989</date>
<booktitle>In Proc. of the Int. Workshop on Parsing Technologies,</booktitle>
<pages>105--114</pages>
<institution>Carnegie-Mellon University.</institution>
<contexts>
<context position="1976" citStr="Fujisaki et al., 1989" startWordPosition="278" endWordPosition="281">main is vital for the development of practical systems. In the currently working systems, such customization has been carried out manually by linguists or lexicographers with time-consuming effort. We have already proposed a mechanism which acquires sublanguage-specific linguistic knowledge from parsing failures and which can be used as a tool for linguistic knowledge customization (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN. which assumes the prior existence of comprehensive linguistic knowledge, our system can suggest new pieces of knowledge including CFG rules, subcategorization frames, and other lexical features. It also differs from previous proposals on lexical acquisition using statistical measures such as (Church et al., 1991; Brent, 1991; Brown et al., 1993) which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways. Our system consists of tw</context>
</contexts>
<marker>Fujisaki, Jelinek, Cocke, Black, Nishino, 1989</marker>
<rawString>T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and T. Nishino. 1989. A Probabilistic Parsing Method for Sentence Disambiguation. In Proc. of the Int. Workshop on Parsing Technologies, pages 105-114. Carnegie-Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Kiyono</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Linguistic Knowledge Acquisition from Parsing Failures.</title>
<date>1993</date>
<booktitle>In Proc. of EACL-93,</booktitle>
<pages>222--231</pages>
<contexts>
<context position="1764" citStr="Kiyono and Tsujii, 1993" startWordPosition="247" endWordPosition="250">hnologies in natural language processing are not so mature as to make general purpose systems applicable to any domains; therefore rapid customization of linguistic knowledge to the sublanguage of an application domain is vital for the development of practical systems. In the currently working systems, such customization has been carried out manually by linguists or lexicographers with time-consuming effort. We have already proposed a mechanism which acquires sublanguage-specific linguistic knowledge from parsing failures and which can be used as a tool for linguistic knowledge customization (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN. which assumes the prior existence of comprehensive linguistic knowledge, our system can suggest new pieces of knowledge including CFG rules, subcategorization frames, and other lexical features. It also differs from previous proposals on lexical acquisition using sta</context>
<context position="5572" citStr="Kiyono and Tsujii, 1993" startWordPosition="870" endWordPosition="873">alls this algorithm for [ie(Ai) : xi_i, xi]. This algorithm has been further augmented in order to treat sentences which contain more than one construction not covered by the current version of the grammar and to generate hypotheses concerning complex features like subcategorization frames. 2.2 Hypothesis Filtering The greater number of the hypotheses generated by the algorithm are linguistically unnatural, because the algorithm does not embody any linguistic principle to judge the appropriateness of hypotheses, and therefore we introduced a set of criteria to filter out unnatural hypotheses (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). This includes, for example, • The maximum number of daughter constituents of a rule is set to 3. • Supposing that the current version of the grammar contains all the category conversion rules, a unary rule with one daughter constituent is not generated. • Using generalizations embodied in the current version of the grammar, a rule containing a sequence of constituents which can be collected into a larger constituent by the current version of grammar is not generated. • Distinguishing non-lexical categories from lexical categories, a rule whose mother category is a l</context>
</contexts>
<marker>Kiyono, Tsujii, 1993</marker>
<rawString>Masaki Kiyono and Jun&apos;ichi Tsujii. 1993. Linguistic Knowledge Acquisition from Parsing Failures. In Proc. of EACL-93, pages 222-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Kiyono</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Hypothesis Selection in Grammar Acquisition.</title>
<date>1994</date>
<booktitle>In Proc. of COLING-94.</booktitle>
<contexts>
<context position="1790" citStr="Kiyono and Tsujii, 1994" startWordPosition="251" endWordPosition="254">uage processing are not so mature as to make general purpose systems applicable to any domains; therefore rapid customization of linguistic knowledge to the sublanguage of an application domain is vital for the development of practical systems. In the currently working systems, such customization has been carried out manually by linguists or lexicographers with time-consuming effort. We have already proposed a mechanism which acquires sublanguage-specific linguistic knowledge from parsing failures and which can be used as a tool for linguistic knowledge customization (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). Our approach is characterized by a mixture of symbolic and statistical approaches to grammatical knowledge acquisition. Unlike probabilistic parsing, proposed by (Fujisaki et al., 1989; Briscoe and Carroll, 1993), *also a staff member of Matsushita Electric Industrial Co. ,Ltd., Shinagawa, Tokyo, JAPAN. which assumes the prior existence of comprehensive linguistic knowledge, our system can suggest new pieces of knowledge including CFG rules, subcategorization frames, and other lexical features. It also differs from previous proposals on lexical acquisition using statistical measures such as </context>
<context position="5598" citStr="Kiyono and Tsujii, 1994" startWordPosition="874" endWordPosition="878">ie(Ai) : xi_i, xi]. This algorithm has been further augmented in order to treat sentences which contain more than one construction not covered by the current version of the grammar and to generate hypotheses concerning complex features like subcategorization frames. 2.2 Hypothesis Filtering The greater number of the hypotheses generated by the algorithm are linguistically unnatural, because the algorithm does not embody any linguistic principle to judge the appropriateness of hypotheses, and therefore we introduced a set of criteria to filter out unnatural hypotheses (Kiyono and Tsujii, 1993; Kiyono and Tsujii, 1994). This includes, for example, • The maximum number of daughter constituents of a rule is set to 3. • Supposing that the current version of the grammar contains all the category conversion rules, a unary rule with one daughter constituent is not generated. • Using generalizations embodied in the current version of the grammar, a rule containing a sequence of constituents which can be collected into a larger constituent by the current version of grammar is not generated. • Distinguishing non-lexical categories from lexical categories, a rule whose mother category is a lexical category is not gen</context>
</contexts>
<marker>Kiyono, Tsujii, 1994</marker>
<rawString>Masaki Kiyono and Jun&apos;ichi Tsujii. 1994. Hypothesis Selection in Grammar Acquisition. In Proc. of COLING-94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>