<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9911815">
The Role of Lexico-Semantic Feedback in Open-Domain Textual
Question-Answering
</title>
<author confidence="0.991166">
Sanda Harabagiu, Dan Moldovan
Marius Pas¸ca, Rada Mihalcea, Mihai Surdeanu,
R˘azvan Bunescu, Roxana Girju, Vasile Rus and Paul Mor˘arescu
</author>
<affiliation confidence="0.8289155">
Department of Computer Science and Engineering
Southern Methodist University
</affiliation>
<address confidence="0.824757">
Dallas, TX 75275-0122
</address>
<email confidence="0.992356">
sanda @engr.smu.edu
</email>
<sectionHeader confidence="0.995472" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901181818182">
This paper presents an open-domain
textual Question-Answering system
that uses several feedback loops to en-
hance its performance. These feedback
loops combine in a new way statistical
results with syntactic, semantic or
pragmatic information derived from
texts and lexical databases. The paper
presents the contribution of each feed-
back loop to the overall performance of
76% human-assessed precise answers.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9978426">
Open-domain textual Question-Answering
(Q&amp;A), as defined by the TREC competitions1,
is the task of identifying in large collections of
documents a text snippet where the answer to
a natural language question lies. The answer
is constrained to be found either in a short (50
bytes) or a long (250 bytes) text span. Frequently,
keywords extracted from the natural language
question are either within the text span or in
its immediate vicinity, forming a text para-
graph. Since such paragraphs must be identified
throughout voluminous collections, automatic
and autonomous Q&amp;A systems incorporate an
index of the collection as well as a paragraph
retrieval mechanism.
</bodyText>
<footnote confidence="0.8398215">
Recent results from the TREC evaluations
((Kwok et al., 2000) (Radev et al., 2000) (Allen
1The Text REtrieval Conference (TREC) is a series of
workshops organized by the National Institute of Standards
and Technology (NIST), designed to advance the state-of-
the-art in information retrieval (IR)
</footnote>
<bodyText confidence="0.99994545">
et al., 2000)) show that Information Retrieval (IR)
techniques alone are not sufficient for finding an-
swers with high precision. In fact, more and more
systems adopt architectures in which the seman-
tics of the questions are captured prior to para-
graph retrieval (e.g. (Gaizauskas and Humphreys,
2000) (Harabagiu et al., 2000)) and used later in
extracting the answer (cf. (Abney et al., 2000)).
When processing a natural language question two
goals must be achieved. First we need to know
what is the expected answer type; in other words,
we need to know what we are looking for. Sec-
ond, we need to know where to look for the an-
swer, e.g. we must identify the question keywords
to be used in the paragraph retrieval.
The expected answer type is determined based
on the question stem, e.g. who, where or how
much and eventually one of the question concepts,
when the stem is ambiguous (for example what),
as described in (Harabagiu et al., 2000) (Radev et
al., 2000) (Srihari and Li, 2000). However finding
question keywords that retrieve all candidate an-
swers cannot be achieved only by deriving some
of the words used in the question. Frequently,
question reformulations use different words, but
imply the same answer. Moreover, many equiv-
alent answers are phrased differently. In this pa-
per we argue that the answer to complex natural
language questions cannot be extracted with sig-
nificant precision from large collections of texts
unless several lexico-semantic feedback loops are
allowed.
In Section 2 we survey the related work
whereas in Section 3 we describe the feedback
loops that refine the search for correct answers.
Section 4 presents the approach of devising key-
word alternations whereas Section 5 details the
recognition of question reformulations. Section 6
evaluates the results of the Q&amp;A system and Sec-
tion 7 summarizes the conclusions.
</bodyText>
<sectionHeader confidence="0.998765" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999876">
Mechanisms for open-domain textual Q&amp;A were
not discovered in the vacuum. The 90s witnessed
a constant improvement of IR systems, deter-
mined by the availability of large collections of
texts and the TREC evaluations. In parallel, In-
formation Extraction (IE) techniques were devel-
oped under the TIPSTER Message Understand-
ing Conference (MUC) competitions. Typically,
IE systems identify information of interest in a
text and map it to a predefined, target represen-
tation, known as template. Although simple com-
binations of IR and IE techniques are not practical
solutions for open-domain textual Q&amp;A because
IE systems are based on domain-specific knowl-
edge, their contribution to current open-domain
Q&amp;A methods is significant. For example, state-
of-the-art Named Entity (NE) recognizers devel-
oped for IE systems were readily available to be
incorporated in Q&amp;A systems and helped recog-
nize names of people, organizations, locations or
dates.
Assuming that it is very likely that the answer
is a named entity, (Srihari and Li, 2000) describes
a NE-supported Q&amp;A system that functions quite
well when the expected answer type is one of the
categories covered by the NE recognizer. Un-
fortunately this system is not fully autonomous,
as it depends on IR results provided by exter-
nal search engines. Answer extractions based on
NE recognizers were also developed in the Q&amp;A
presented in (Abney et al., 2000) (Radev et al.,
2000) (Gaizauskas and Humphreys, 2000). As
noted in (Voorhees and Tice, 2000), Q&amp;A sys-
tems that did not include NE recognizers per-
formed poorly in the TREC evaluations, espe-
cially in the short answer category. Some Q&amp;A
systems, like (Moldovan et al., 2000) relied both
on NE recognizers and some empirical indicators.
However, the answer does not always belong
to a category covered by the NE recognizer. For
such cases several approaches have been devel-
oped. The first one, presented in (Harabagiu et
al., 2000), the answer type is derived from a large
answer taxonomy. A different approach, based on
statistical techniques was proposed in (Radev et
al., 2000). (Cardie et al., 2000) presents a method
of extracting answers as noun phrases in a novel
way. Answer extraction based on grammatical
information is also promoted by the system de-
scribed in (Clarke et al., 2000).
One of the few Q&amp;A systems that takes into
account morphological, lexical and semantic al-
ternations of terms is described in (Ferret et al.,
2000). To our knowledge, none of the cur-
rent open-domain Q&amp;A systems use any feed-
back loops to generate lexico-semantic alterna-
tions. This paper shows that such feedback loops
enhance significantly the performance of open-
domain textual Q&amp;A systems.
</bodyText>
<sectionHeader confidence="0.998206" genericHeader="method">
3 Textual Q&amp;A Feedback Loops
</sectionHeader>
<bodyText confidence="0.993217028169014">
Before initiating the search for the answer to a
natural language question we take into account
the fact that it is very likely that the same ques-
tion or a very similar one has been posed to the
system before, and thus those results can be used
again. To find such cached questions, we measure
the similarity to the previously processed ques-
tions and when a reformulation is identified, the
system returns the corresponding cached correct
answer, as illustrated in Figure 1.
When no reformulations are detected, the
search for answers is based on the conjecture that
the eventual answer is likely to be found in a
text paragraph that (a) contains the most repre-
sentative question concepts and (b) includes a tex-
tual concept of the same category as the expected
answer. Since the current retrieval technology
does not model semantic knowledge, we break
down this search into a boolean retrieval, based
on some question keywords and a filtering mech-
anism, that retains only those passages containing
the expected answer type. Both the question key-
words and the expected answer type are identified
by using the dependencies derived from the ques-
tion parse.
By implementing our own version of the pub-
licly available Collins parser (Collins, 1996), we
also learned a dependency model that enables the
mapping of parse trees into sets of binary rela-
tions between the head-word of each constituent
and its sibling-words. For example, the parse tree
of TREC-9 question Q210: “How many dogs pull
a sled in the Iditarod ?” is:
For each possible constituent in a parse tree,
rules first described in (Magerman, 1995) and
(Jelinek et al., 1994) identify the head-child and
propagate the head-word to its parent. For the
parse of question Q210 the propagation is:
When the propagation is over, head-modifier
relations are extracted, generating the following
dependency structure, called question semantic
form in (Harabagiu et al., 2000).
COUNT dogs pull sled Iditarod
In the structure above, COUNT represents the
expected answer type, replacing the question stem
“how many”. Few question stems are unambigu-
ous (e.g. who, when). If the question stem is am-
biguous, the expected answer type is determined
by the concept from the question semantic form
that modifies the stem. This concept is searched
in an ANSWER TAXONOMY comprising several
tops linked to a significant number of WordNet
noun and verb hierarchies. Each top represents
one of the possible expected answer types imple-
mented in our system (e.g. PERSON, PRODUCT,
NUMERICAL VALUE, COUNT, LOCATION). We
encoded a total of 38 possible answer types.
In addition, the question keywords used for
paragraph retrieval are also derived from the ques-
tion semantic form. The question keywords are
organized in an ordered list which first enumer-
ates the named entities and the question quota-
tions, then the concepts that triggered the recogni-
tion of the expected answer type followed by all
adjuncts, in a left-to-right order, and finally the
question head. The conjunction of the keywords
represents the boolean query applied to the doc-
ument index. (Moldovan et al., 2000) details the
empirical methods used in our system for trans-
forming a natural language question into an IR
query.
</bodyText>
<figureCaption confidence="0.995002">
Figure 1: Feedbacks for the Answer Search.
</figureCaption>
<bodyText confidence="0.999988666666667">
It is well known that one of the disadvantages
of boolean retrieval is that it returns either too
many or too few documents. However, for ques-
tion answering, this is an advantage, exploited by
the first feedback loop represented in Figure 1.
Feedback loop 1 is triggered when the number of
retrieved paragraphs is either smaller than a min-
imal value or larger than a maximal value deter-
mined beforehand for each answer type. Alterna-
tively, when the number of paragraphs is within
limits, those paragraphs that do not contain at
least one concept of the same semantic category
as the expected answer type are filtered out. The
remaining paragraphs are parsed and their depen-
dency structures, called answer semantic forms,
</bodyText>
<figure confidence="0.998113303030303">
NP
NP
NP
WRB
NN IN DT
NNS VBP DT
JJ
NNP
dogs pull a sled in the
How
many
Iditarod
VP
NP
PP
S
S (pull)
VP (pull)
NP (sled)
PP (Iditarod)
NP (dogs)
NP (Iditarod)
WRB
NNP (Iditarod)
VBP
pull a sled in the Iditarod
dogs
NP (sled)
JJ NNS
DT NN IN DT
How
many
No
Cached Questions
Cached Answers
Question
REFORMULATION
PARSE
Retrieval
Index
LOOP 1
Yes
Expected Answer Type
in paragraph
No
Yes
Question Semantic Form
No
Lexical
Alternations
S-UNIFICATIONS
Answer Semantic Form
No
Semantic
Alternations
Answer Logical Form
Question Logical Form
ABDUCTIVE PROOF
Expected Answer Type
Question Keywords
Yes LOOP 3
Min&lt;Number Paragraphs&lt;Max No
LOOP 2
Filter out paragraph
Yes
Yes
</figure>
<bodyText confidence="0.947309487179487">
are derived.
Feedback loop 2 illustrated in Figure 1 is acti-
vated when the question semantic form and the
answer semantic form cannot by unified. The uni-
fication involves three steps:
Step 1: The recognition of the expected answer
type. The first step marks all possible concepts
that are answer candidates. For example, in the
case of TREC -9 question Q243: “Where did the
ukulele originate ?”, the expected answer type is
LOCATION. In the paragraph “the ukulele intro-
duced from Portugal into the Hawaiian islands”
contains two named entities of the category LO-
CATION and both are marked accordingly.
Step 2: The identification of the question con-
cepts. The second step identifies the question
words, their synonyms, morphological deriva-
tions or WordNet hypernyms in the answer se-
mantic form.
Step 3: The assessment of the similarities of
dependencies. In the third step, two classes of
similar dependencies are considered, generating
unifications of the question and answer semantic
forms:
Class L2-1: there is a one-to-one mapping be-
tween the binary dependencies of the question
and binary dependencies from the answer seman-
tic form. Moreover, these dependencies largely
cover the question semantic form2. An example
is:
We find an entailment between producing, or
making and selling goods, derived from Word-
Net, since synsetmake, produce, create has the
genus manufacture, defined in the gloss of its ho-
momorphic nominalization as “for sale”. There-
fore the semantic form of question Q261 and its
illustrated answer are similar.
Class L2-2: Either the question semantic form
or the answer semantic form contain new con-
</bodyText>
<footnote confidence="0.629647">
2Some modifiers might be missing from the answer.
</footnote>
<bodyText confidence="0.999928060606061">
cepts, that impose a bridging inference. The
knowledge used for inference is of lexical nature
and is later employed for abductions that justify
the correctness of the answer. For example:
Nouns head and government are constituents of
a possible paraphrase of president, i.e. “head of
government”. However, only world knowledge
can justify the answer, since there are countries
where the prime minister is the head of govern-
ment. Presupposing this inference, the semantic
form of the question and answer are similar.
Feedback loop 3 from Figure 1 brings forward
additional semantic information. Two classes of
similar dependencies are considered for the ab-
duction of answers, performed in a manner simi-
lar to the justifications described in (Harabagiu et
al., 2000).
Class L3-1: is characterized by the need for
contextual information, brought forward by ref-
erence resolution. In the following example, a
chain of coreference links Bill Gates and Mi-
crosoft founder in the candidate answer:
Class L3-2: Paraphrases and additional infor-
mation produce significant differences between
the question semantic form and the answer se-
mantic form. However, semantic information
contributes to the normalization of the answer
dependencies until they can be unified with the
question dependencies. For example, if (a) a vol-
cano IS-A mountain; (b) lava IS-PART of vol-
cano, and moreover it is a part coming from the
inside; and (c) fragments of lava have all the prop-
erties of lava, the following question semantic
</bodyText>
<figure confidence="0.99150555">
Q261: What company sells most greetings cards ?
ORGANIZATION sells greeting cards most
maker greeting cards largest
ORGANIZATION(Hallmark)
&amp;quot;Hallmark remains the largest maker of greeting cards&amp;quot;
Q231: Who was the president of Vichy France ?
PERSON president France Vichy
head government France Vichy
PERSON(Marshall Philippe Petain)
&amp;quot;Marshall Philippe Petain, head of Vichy France
government&amp;quot;
Q318: Where did Bill Gates go to college?
ORGANIZATION go college Bill Gates
dropout founder Microsoft
ORGANIZATION=college(Harvard)
Bill Gates
&amp;quot;Harvard dropout and Microsoft founder&amp;quot;
Answer Question
Answer Question
Answer Question
</figure>
<bodyText confidence="0.7766445">
form and answer semantic form can be unified:
The resulting normalized dependencies are:
</bodyText>
<equation confidence="0.465275">
TEMPERATURE(300 degrees)
</equation>
<bodyText confidence="0.999911090909091">
The semantic information and the world
knowledge needed for the above unifications are
available from WordNet (Miller, 1995). More-
over, this knowledge can be translated in ax-
iomatic form and used for abductive proofs. Each
of the feedback loops provide the retrieval en-
gine with new alternations of the question key-
words. Feedback loop 2 considers morphological
and lexical alternations whereas Feedback loop 3
uses semantic alternations. The method of gener-
ating the alternations is detailed in Section 4.
</bodyText>
<sectionHeader confidence="0.998491" genericHeader="method">
4 Keyword Alternations
</sectionHeader>
<bodyText confidence="0.999864315789474">
To enhance the chance of finding the answer to
a question, each feedback loop provides with
a different set of keyword alternations. Such
alternations can be classified according to the
linguistic knowledge they are based upon:
1.Morphological Alternations. When lexical
alternations are necessary because no answer
was found yet, the first keyword that is altered
is determined by the question word that either
prompted the expected answer type or is in the
same semantic class with the expected answer
type. For example, in the case of question
Q209: “Who invented the paper clip ?”, the
expected answer type is PERSON and so is the
subject of the verb invented , lexicalized as the
nominalization inventor. Moreover, since our
retrieval mechanism does not stem keywords, all
the inflections of the verb are also considered.
Therefore, the initial query is expanded into:
</bodyText>
<footnote confidence="0.9281015">
QUERY(Q209):�paper AND clip AND (invented OR
inventor OR invent OR invents)
</footnote>
<listItem confidence="0.9977823">
2. Lexical Alternations. WordNet encodes a
wealth of semantic information that is easily
mined. Seven types of semantic relations span
concepts, enabling the retrieval of synonyms
and other semantically related terms. Such
alternations improve the recall of the answer
paragraphs. For example, in the case of question
Q221: “Who killed Martin Luther King ?”,
by considering the synonym of killer, the noun
assassin, the Q&amp;A system retrieved paragraphs
</listItem>
<bodyText confidence="0.8310202">
with the correct answer. Similarly, for the
question Q206: “How far is the moon ?”, since
the adverb far is encoded in WordNet as being an
attribute of distance, by adding this noun to the
retrieval keywords, a correct answer is found.
</bodyText>
<listItem confidence="0.952774">
3. Semantic Alternations and Paraphrases. We
</listItem>
<bodyText confidence="0.9245599375">
define as semantic alternations of a keyword
those words or collocations from WordNet that
(a) are not members of any WordNet synsets
containing the original keyword; and (b) have a
chain of WordNet relations or bigram relations
that connect it to the original keyword. These
relations can be translated in axiomatic form and
thus participate to the abductive backchaining
from the answer to the question - to justify
the answer. For example semantic alternations
involving only WordNet relations were used in
the case of question Q258: “Where do lobsters
like to live ?”. Since in WordNet the verb prefer
has verb like as a hypernym, and moreover,
its glossed definition is liking better, the query
becomes:
Sometimes multiple keywords are replaced by
a semantic alternation. Sometimes these alterna-
tions are similar to the relations between multi-
term paraphrases and single terms, other time they
simply are semantically related terms. In the case
of question Q210: “How many dogs pull a sled
in the Iditarod ?”, since the definition of Word-
Net sense 2 of noun harness contains the bigram
“pull cart” and both sled and cart are forms of
vehicles, the alternation of the pair of keywords
pull, slide is rendered by harness. Only when
this feedback is received, the paragraph contain-
ing the correct answer is retrieved.
To decide which keywords should be expanded
and what form of alternations should be used we
rely on a set of heuristics which complement the
</bodyText>
<figure confidence="0.984724">
Q361: How hot does the inside of an active volcano get ?
TEMPERATURE get inside volcano active
fragments lava belched out mountain
TEMPERATURE(300 degrees)
&amp;quot;lava fragments belched out of the mountain were as hot
300 degrees Fahrenheit&amp;quot;
lava/
[inside volcano]
mountain/volcano active/
[lava belched out]
belched out
QUERY(Q258):lobsters AND (like OR prefer) AND live
Answer Question
</figure>
<bodyText confidence="0.9995768">
heuristics that select the question keywords and
generate the queries (as described in (Moldovan
et al., 2000)):
Heuristic 1: Whenever the first feedback loop re-
quires the addition of the main verb of the ques-
tion as a query keyword, generate all verb conju-
gations as well as its nominalizations.
Heuristic 2: Whenever the second feedback loop
requires lexical alternations, collect from Word-
Net all the synset elements of the direct hyper-
nyms and direct hyponyms of verbs and nomi-
nalizations that are used in the query. If multiple
verbs are used, expand them in a left-to-right or-
der.
Heuristic 3: Whenever the third feedback loop
imposes semantic alternations expressed as para-
phrases, if a verb and its direct object from the
question are selected as query keywords, search
for other verb-object pairs semantically related to
the query pair. When new pairs are located in
the glosses of a synset , expand the query verb-
object pair with all the elements from .
Another set of possible alternations, defined by
the existence of lexical relations between pairs
of words from different question are used to de-
tect question reformulations. The advantage of
these different forms of alternations is that they
enable the resolution of similar questions through
answer caching instead of normal Q&amp;A process-
ing.
</bodyText>
<sectionHeader confidence="0.939488" genericHeader="method">
5 Question Reformulations
</sectionHeader>
<bodyText confidence="0.999988571428571">
In TREC-9 243 questions were reformulations of
54 inquiries, thus asking for the same answer. The
reformulation classes contained variable number
of questions, ranging from two to eight questions.
Two examples of reformulation classes are listed
in Table 1. To classify questions in reformulation
groups, we used the algorithm:
</bodyText>
<footnote confidence="0.9108319">
Reformulation Classes(new question, old questions)
1. For each question from old questions
2. Compute similarity(question,new question)
3. Build a new similarity matrix such that
it is generated by adding to the matrix for the
old questions a new row and a new column
representing the similarities computed at step 2.
4. Find the transitive closures for the set
old questions new question
5. Result: reformulation classes as transitive closures.
</footnote>
<bodyText confidence="0.9810222">
In Figure 2 we represent the similarity matrix
for six questions that were successively posed to
the answer engine. Since question reformulations
are transitive relations, if at a step questions
and are found similar and already belongs
to , a reformulation class previously discovered
(i.e. a group of at least two similar questions),
then question is also included in . Figure 2
illustrates the transitive closures for reformula-
tions at each of the five steps from the succession
of six questions. To be noted that at step 4 no new
similarities were found, thus is not found sim-
ilar to at this step. However, at step 5, since
is found similar to both and ,results
similar to all the other questions but .
</bodyText>
<footnote confidence="0.996352454545454">
Q397:When was the Brandenburg Gate in Berlin built?
Q814:When was Berlin’s Brandenburg gate erected?
Q-411:What tourist attractions are there in Reims?
Q-711:What are the names of the tourist attractions
in Reims?
Q-712:What do most tourists visit in Reims?
Q-713:What attracts tourists to Reims?
Q-714:What are tourist attractions in Reims?
Q-715:What could I see in Reims?
Q-716:What is worth seeing in Reims?
Q-717:What can one see in Reims?
</footnote>
<tableCaption confidence="0.942416428571429">
Table 1: Two classes of TREC-9 question refor-
mulations.
Step 1: {Q1, Q21
Step 2: {Q1, Q21 {Q31
Step 3: {Q1, Q2, Q41 {Q31
Step 4: {Q1, Q2, Q41 {Q31 {Q51
Step 5: {Q1, Q2, Q4, Q5, Q61 {Q31
</tableCaption>
<figureCaption confidence="0.9671085">
Figure 2: Building reformulation classes with a
similarity matrix.
</figureCaption>
<bodyText confidence="0.988071">
The algorithm that measures the similarity be-
tween two questions is:
</bodyText>
<construct confidence="0.494983">
Algorithm Similarity(Q, Q’)
Input: a pair of question represented as two word strings:
</construct>
<note confidence="0.283454">
Q: and Q’:
</note>
<page confidence="0.662641">
1. Apply a part-of-speech tagger on both questions:
Tag(Q):
Tag(Q’):
</page>
<listItem confidence="0.85647375">
2. Setnr matches=0
3. Identify quadruples such that
if and are content words with
and Lexical relation holds then increase nr matches
</listItem>
<figure confidence="0.966509238095238">
Q1 Q2 Q3 Q4 Q5 Q6
Q1
Q2
Q3
Q4
Q5
Q6
0 0
0
0
0
0
0
0
1 1
0
0
0
1
1
0
0
0
0
0
0 1
0
0
0
1 0
0
0
0
1
0
0
0
0
1
4. Relax the Lexical relation and goto step 3;
5. If (nr matchesnumber of content words
then Q and Q’ are similar
</figure>
<bodyText confidence="0.996431764705882">
The Lexical relation between a pair of con-
tent words is initially considered to be a string
identity. In later loops starting at step 3 one of
the following three possible relaxations of Lex-
ical relation are allowed: (a) common morpho-
logical root (e.g. owner and owns, from question
Q742: “Who is the owner of CNN ?” and ques-
tion Q417: “Who owns CNN ?” respectively);
(b) WordNet synonyms (e.g. gestation and preg-
nancy from question Q763: “How long is hu-
man gestation ?” and question Q765: “A nor-
mal human pregnancy lasts how many months
?”, respectively) or (c) WordNet hypernyms (e.g.
the verbs erect and build from question Q814:
“When was Berlin’s Brandenburg gate erected ?”
and question Q397: “When was the Brandenburg
Gate in Berlin built ?” respectively).
</bodyText>
<sectionHeader confidence="0.993256" genericHeader="evaluation">
6 Performance evaluation
</sectionHeader>
<bodyText confidence="0.992729666666667">
To evaluate the role of lexico-semantic feedback
loops in an open-domain textual Q&amp;A system
we have relied on the 890 questions employed
in the TREC-8 and TREC-9 Q&amp;A evaluations.
In TREC, for each question the performance was
computed by the reciprocal value of the rank
(RAR) of the highest-ranked correct answer given
by the system. Given that only the first five an-
swers were considered in the TREC evaluations, i
f the RAR is defined as its value is
1 if the first answer is correct; 0.5 if the second an-
swer was correct, but not the first one; 0.33 when
the correct answer was on the third position; 0.25
if the fourth answer was correct; 0.2 when the fifth
answer was correct and 0 if none of the first five
answers were correct. The Mean Reciprocal An-
swer Rank (MRAR) is used to compute the over-
all performance of the systems participating in the
TREC evaluation In ad-
dition, TREC-9 imposed the constraint that an an-
swer is considered correct only when the textual
context from the document that contains it can
account for it. When the human assessors were
convinced this constraint was satisfied, they con-
sidered the RAR to be strict, otherwise, the RAR
was considered lenient.
Table 2 summarizes the MRARs provided by
</bodyText>
<table confidence="0.993217">
MRAR MRAR
lenient strict
Short answer 0.599 0.580
Long answer 0.778 0.760
</table>
<tableCaption confidence="0.999322">
Table 2: NIST-evaluated performance
</tableCaption>
<bodyText confidence="0.997177875">
NIST for the system on which we evaluated the
role of lexico-semantic feedbacks. Table 3 lists
the quantitative analysis of the feedback loops.
Loop 1 was generated more often than any other
loop. However, the small overall average number
of feedback loops that have been carried out in-
dicate that the fact they port little overhead to the
Q&amp;A system.
</bodyText>
<table confidence="0.7416384">
Average Maximal
number number
Loop 1 1.384 7
Loop 2 1.15 3
Loop 3 1.07 5
</table>
<tableCaption confidence="0.81603">
Table 3: Number of feedbacks on the TREC test
</tableCaption>
<bodyText confidence="0.942983833333333">
data
More interesting is the qualitative analysis of
the effect of the feedback loops on the Q&amp;A eval-
uation. Overall, the precision increases substan-
tially when all loops were enabled, as illustrated
in Table 4.
</bodyText>
<table confidence="0.9995185">
L1 L2 L3 MRAR MRAR
short long
No No No 0.321 0.385
Yes No No 0.451 0.553
No Yes No 0.490 0.592
Yes Yes No 0.554 0.676
No No Yes 0.347 0.419
Yes No Yes 0.488 0.589
No Yes Yes 0.510 0.629
Yes Yes Yes 0.568 0.737
</table>
<tableCaption confidence="0.995134">
Table 4: Effect of feedbacks on accuracy.
L1=Loop 1; L2=Loop 2; L3=Loop 3.
</tableCaption>
<bodyText confidence="0.999781294117647">
Individually, the effect of Loop 1 has an ac-
curacy increase of over 40%, the effect of Loop
2 had an enhancement of more than 52% while
Loop 3 produced an enhancement of only 8%. Ta-
ble 4 lists also the combined effect of the feed-
backs, showing that when all feedbacks are en-
abled, for short answers we obtained an MRAR of
0.568, i.e. 76% increase over Q&amp;A without feed-
backs. The MRAR for long answers had a sim-
ilar increase of 91%. Because we also used the
answer caching technique, we gained more than
1% for short answers and almost 3% for long an-
swers, obtaining the result listed in Table 2. In our
experiments, from the total of 890 TREC ques-
tions, lexical alternations were used for 129 ques-
tions and the semantic alternations were needed
only for 175 questions.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999992388888889">
This paper has presented a Q&amp;/A system that em-
ploys several feedback mechanisms that provide
lexical and semantic alternations to the question
keywords. By relying on large, open-domain lin-
guistic resources such as WordNet we enabled a
more precise approach of searching and mining
answers from large collections of texts. Evalua-
tions indicate that when all three feedback loops
are enabled we reached an enhancement of al-
most 76% for short answers and 91% for long an-
swers, respectively, over the case when there are
no feedback loops. In addition, a small increase
is produced by relying on cached answers of sim-
ilar questions. Our results so far indicate that
the usage of feedback loops that produce alter-
nations is significantly more efficient than multi-
word indexing or annotations of large corpora
with predicate-argument information.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849054054054">
Steve Abney, Michael Collins, and Amit Singhal. Answer
extraction. In Proceedings of the 6th Applied Natural
Language Processing Conference (ANLP-2000), pages
296–301, Seattle, Washington, 2000.
James Allen, Margaret Connell, W. Bruce Croft, Fan-Fang
Feng, David Fisher and Xioayan Li. INQUERY in
TREC-9. Proceedings of the Text Retrieval Conference
(TREC-9), pages 504–510, 2000.
Claire Cardie, Vincent Ng, David Pierce, Chris Buckley. Ex-
amining the role of statistical and linguistic knowledge
sources in a general-knowledge que stion answering sys-
tem. In Proceedings of the 6th Applied Natural Lan-
guage Processing Conference (ANLP-2000), pages 180–
187, Seattle, Washington, 2000.
C.L. Clarke, Gordon V. Cormak, D.I.E. Kisman and T.R. Ly-
nam. Question Answering by passage selection. Pro-
ceedings of the Text Retrieval Conference (TREC-9),
pages 65–76, 2000.
Michael Collins. A New Statistical Parser Based on Bigram
Lexical Dependencies. In Proceedings of the 34th An-
nual Meeting of the Association for Computational Lin-
guistics, ACL-96, pages 184–191, 1996.
Olivier Ferret, Brigitte Grau, Martine Hurault-Plantet,
Gabriel Illouz, Christian Jacquemin, Nicolas Masson and
Paule Lecuyer. QALC- the question-answering system of
LIMSI-CNRS. Proceedings of the Text Retrieval Confer-
ence (TREC-9), pages 316–326, 2000.
Robert Gaizauskas and Kevin Humphreys. A com-
bined IR/NLP approach to question answering against
large text collections. In Proceedings of the 6th
Content-Based Multimedia Information Access Confer-
ence (RIAO-2000), pages 1288–1304, Paris, France,
2000.
Sanda Harabagiu, Marius Pas¸ca and Steven Maiorano. Ex-
periments with Open-Domain Textual Question Answer-
ing. In the Proceedings of the 18th International Con-
ference on Computational Linguistics (COLING-2000),
pages 292–298, 2000.
Frederick Jelinek, John Lafferty, Dan Magerman, Robert
Mercer, Adwait Ratnaparkhi and Selim Roukos. Deci-
sion tree parsing using a hidden derivational model. In
Proceedings of the 1994 Human Language Technology
Workshop, pages 272–277, 1994.
K.L. Kwok, L. Grunfeld, N. Dinstl and M. Chan. TREC-
9 Cross Language, Web and Question-Answering Track
Experiments using PIRCS. Proceedings of the Text Re-
trieval Conference (TREC-9), pages 26–35, 2000.
Dan Magerman. Statistical decision-tree models of parsing.
In Proceedings of the 33rd Annual Meeting of the Associ-
ation for Computational Linguistics, ACL-95, pages 276–
283, 1995.
George A. Miller. WordNet: A Lexical Database. Commu-
nication oftheACM, vol 38: No11, pages 39–41, Novem-
ber 1995.
Dan Moldovan, Sanda Harabagiu, Marius Pas¸ca, Rada
Mihalcea, Richard Goodrum, Roxana Girju and Vasile
Rus. The Structure and Performance of an Open-Domain
Question Answering System. Proceedings of the 38th
Annual Meeting of the Association for Comoutational
Linguistics (ACL-2000), pages 563–570, 2000.
Dragomir Radev, John Prager, and V. Samn. Ranking sus-
pected answers to natural language questions using pre-
dictive annotation. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP-2000),
pages 150–157, Seattle, Washington, 2000.
Rohini Srihari and W. Li. A question answering system
supported by information extraction. In Proceedings of
the 6th Applied Natural Language Processing Conference
(ANLP-2000), Seattle, Washington, 2000.
Ellen M. Voorhees and Dawn Tice. Building a question an-
swering test collection. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR-2000),
Athens, Greece, 2000.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629023">
<title confidence="0.997114">The Role of Lexico-Semantic Feedback in Open-Domain Textual Question-Answering</title>
<author confidence="0.863505">Sanda Harabagiu</author>
<author confidence="0.863505">Dan Moldovan Rada Mihalcea</author>
<author confidence="0.863505">Mihai Surdeanu</author>
<author confidence="0.863505">Roxana Girju Bunescu</author>
<author confidence="0.863505">Vasile Rus Mor˘arescu</author>
<affiliation confidence="0.997047">Department of Computer Science and Engineering Southern Methodist University</affiliation>
<address confidence="0.998861">Dallas, TX 75275-0122</address>
<email confidence="0.994086">sanda@engr.smu.edu</email>
<abstract confidence="0.996171">This paper presents an open-domain textual Question-Answering system that uses several feedback loops to enhance its performance. These feedback loops combine in a new way statistical results with syntactic, semantic or pragmatic information derived from texts and lexical databases. The paper presents the contribution of each feedback loop to the overall performance of 76% human-assessed precise answers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steve Abney</author>
<author>Michael Collins</author>
<author>Amit Singhal</author>
</authors>
<title>Answer extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000),</booktitle>
<pages>296--301</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="2119" citStr="Abney et al., 2000" startWordPosition="312" endWordPosition="315"> et al., 2000) (Allen 1The Text REtrieval Conference (TREC) is a series of workshops organized by the National Institute of Standards and Technology (NIST), designed to advance the state-ofthe-art in information retrieval (IR) et al., 2000)) show that Information Retrieval (IR) techniques alone are not sufficient for finding answers with high precision. In fact, more and more systems adopt architectures in which the semantics of the questions are captured prior to paragraph retrieval (e.g. (Gaizauskas and Humphreys, 2000) (Harabagiu et al., 2000)) and used later in extracting the answer (cf. (Abney et al., 2000)). When processing a natural language question two goals must be achieved. First we need to know what is the expected answer type; in other words, we need to know what we are looking for. Second, we need to know where to look for the answer, e.g. we must identify the question keywords to be used in the paragraph retrieval. The expected answer type is determined based on the question stem, e.g. who, where or how much and eventually one of the question concepts, when the stem is ambiguous (for example what), as described in (Harabagiu et al., 2000) (Radev et al., 2000) (Srihari and Li, 2000). Ho</context>
<context position="5001" citStr="Abney et al., 2000" startWordPosition="780" endWordPosition="783">ers developed for IE systems were readily available to be incorporated in Q&amp;A systems and helped recognize names of people, organizations, locations or dates. Assuming that it is very likely that the answer is a named entity, (Srihari and Li, 2000) describes a NE-supported Q&amp;A system that functions quite well when the expected answer type is one of the categories covered by the NE recognizer. Unfortunately this system is not fully autonomous, as it depends on IR results provided by external search engines. Answer extractions based on NE recognizers were also developed in the Q&amp;A presented in (Abney et al., 2000) (Radev et al., 2000) (Gaizauskas and Humphreys, 2000). As noted in (Voorhees and Tice, 2000), Q&amp;A systems that did not include NE recognizers performed poorly in the TREC evaluations, especially in the short answer category. Some Q&amp;A systems, like (Moldovan et al., 2000) relied both on NE recognizers and some empirical indicators. However, the answer does not always belong to a category covered by the NE recognizer. For such cases several approaches have been developed. The first one, presented in (Harabagiu et al., 2000), the answer type is derived from a large answer taxonomy. A different a</context>
</contexts>
<marker>Abney, Collins, Singhal, 2000</marker>
<rawString>Steve Abney, Michael Collins, and Amit Singhal. Answer extraction. In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000), pages 296–301, Seattle, Washington, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Margaret Connell</author>
<author>W Bruce Croft</author>
</authors>
<title>Fan-Fang Feng, David Fisher and Xioayan Li.</title>
<date>2000</date>
<booktitle>INQUERY in TREC-9. Proceedings of the Text Retrieval Conference (TREC-9),</booktitle>
<pages>504--510</pages>
<marker>Allen, Connell, Croft, 2000</marker>
<rawString>James Allen, Margaret Connell, W. Bruce Croft, Fan-Fang Feng, David Fisher and Xioayan Li. INQUERY in TREC-9. Proceedings of the Text Retrieval Conference (TREC-9), pages 504–510, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Vincent Ng</author>
<author>David Pierce</author>
<author>Chris Buckley</author>
</authors>
<title>Examining the role of statistical and linguistic knowledge sources in a general-knowledge que stion answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000),</booktitle>
<pages>180--187</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="5701" citStr="Cardie et al., 2000" startWordPosition="896" endWordPosition="899">d Tice, 2000), Q&amp;A systems that did not include NE recognizers performed poorly in the TREC evaluations, especially in the short answer category. Some Q&amp;A systems, like (Moldovan et al., 2000) relied both on NE recognizers and some empirical indicators. However, the answer does not always belong to a category covered by the NE recognizer. For such cases several approaches have been developed. The first one, presented in (Harabagiu et al., 2000), the answer type is derived from a large answer taxonomy. A different approach, based on statistical techniques was proposed in (Radev et al., 2000). (Cardie et al., 2000) presents a method of extracting answers as noun phrases in a novel way. Answer extraction based on grammatical information is also promoted by the system described in (Clarke et al., 2000). One of the few Q&amp;A systems that takes into account morphological, lexical and semantic alternations of terms is described in (Ferret et al., 2000). To our knowledge, none of the current open-domain Q&amp;A systems use any feedback loops to generate lexico-semantic alternations. This paper shows that such feedback loops enhance significantly the performance of opendomain textual Q&amp;A systems. 3 Textual Q&amp;A Feedb</context>
</contexts>
<marker>Cardie, Ng, Pierce, Buckley, 2000</marker>
<rawString>Claire Cardie, Vincent Ng, David Pierce, Chris Buckley. Examining the role of statistical and linguistic knowledge sources in a general-knowledge que stion answering system. In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000), pages 180– 187, Seattle, Washington, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Clarke</author>
<author>Gordon V Cormak</author>
<author>D I E Kisman</author>
<author>T R Lynam</author>
</authors>
<title>Question Answering by passage selection.</title>
<date>2000</date>
<booktitle>Proceedings of the Text Retrieval Conference (TREC-9),</booktitle>
<pages>65--76</pages>
<contexts>
<context position="5890" citStr="Clarke et al., 2000" startWordPosition="928" endWordPosition="931">00) relied both on NE recognizers and some empirical indicators. However, the answer does not always belong to a category covered by the NE recognizer. For such cases several approaches have been developed. The first one, presented in (Harabagiu et al., 2000), the answer type is derived from a large answer taxonomy. A different approach, based on statistical techniques was proposed in (Radev et al., 2000). (Cardie et al., 2000) presents a method of extracting answers as noun phrases in a novel way. Answer extraction based on grammatical information is also promoted by the system described in (Clarke et al., 2000). One of the few Q&amp;A systems that takes into account morphological, lexical and semantic alternations of terms is described in (Ferret et al., 2000). To our knowledge, none of the current open-domain Q&amp;A systems use any feedback loops to generate lexico-semantic alternations. This paper shows that such feedback loops enhance significantly the performance of opendomain textual Q&amp;A systems. 3 Textual Q&amp;A Feedback Loops Before initiating the search for the answer to a natural language question we take into account the fact that it is very likely that the same question or a very similar one has be</context>
</contexts>
<marker>Clarke, Cormak, Kisman, Lynam, 2000</marker>
<rawString>C.L. Clarke, Gordon V. Cormak, D.I.E. Kisman and T.R. Lynam. Question Answering by passage selection. Proceedings of the Text Retrieval Conference (TREC-9), pages 65–76, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, ACL-96,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="7553" citStr="Collins, 1996" startWordPosition="1206" endWordPosition="1207"> that (a) contains the most representative question concepts and (b) includes a textual concept of the same category as the expected answer. Since the current retrieval technology does not model semantic knowledge, we break down this search into a boolean retrieval, based on some question keywords and a filtering mechanism, that retains only those passages containing the expected answer type. Both the question keywords and the expected answer type are identified by using the dependencies derived from the question parse. By implementing our own version of the publicly available Collins parser (Collins, 1996), we also learned a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words. For example, the parse tree of TREC-9 question Q210: “How many dogs pull a sled in the Iditarod ?” is: For each possible constituent in a parse tree, rules first described in (Magerman, 1995) and (Jelinek et al., 1994) identify the head-child and propagate the head-word to its parent. For the parse of question Q210 the propagation is: When the propagation is over, head-modifier relations are extracted, generating the followi</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. A New Statistical Parser Based on Bigram Lexical Dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, ACL-96, pages 184–191, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
<author>Brigitte Grau</author>
<author>Martine Hurault-Plantet</author>
<author>Gabriel Illouz</author>
<author>Christian Jacquemin</author>
<author>Nicolas Masson</author>
<author>Paule Lecuyer</author>
</authors>
<title>QALC- the question-answering system of LIMSI-CNRS.</title>
<date>2000</date>
<booktitle>Proceedings of the Text Retrieval Conference (TREC-9),</booktitle>
<pages>316--326</pages>
<contexts>
<context position="6038" citStr="Ferret et al., 2000" startWordPosition="953" endWordPosition="956">er. For such cases several approaches have been developed. The first one, presented in (Harabagiu et al., 2000), the answer type is derived from a large answer taxonomy. A different approach, based on statistical techniques was proposed in (Radev et al., 2000). (Cardie et al., 2000) presents a method of extracting answers as noun phrases in a novel way. Answer extraction based on grammatical information is also promoted by the system described in (Clarke et al., 2000). One of the few Q&amp;A systems that takes into account morphological, lexical and semantic alternations of terms is described in (Ferret et al., 2000). To our knowledge, none of the current open-domain Q&amp;A systems use any feedback loops to generate lexico-semantic alternations. This paper shows that such feedback loops enhance significantly the performance of opendomain textual Q&amp;A systems. 3 Textual Q&amp;A Feedback Loops Before initiating the search for the answer to a natural language question we take into account the fact that it is very likely that the same question or a very similar one has been posed to the system before, and thus those results can be used again. To find such cached questions, we measure the similarity to the previously </context>
</contexts>
<marker>Ferret, Grau, Hurault-Plantet, Illouz, Jacquemin, Masson, Lecuyer, 2000</marker>
<rawString>Olivier Ferret, Brigitte Grau, Martine Hurault-Plantet, Gabriel Illouz, Christian Jacquemin, Nicolas Masson and Paule Lecuyer. QALC- the question-answering system of LIMSI-CNRS. Proceedings of the Text Retrieval Conference (TREC-9), pages 316–326, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gaizauskas</author>
<author>Kevin Humphreys</author>
</authors>
<title>A combined IR/NLP approach to question answering against large text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Content-Based Multimedia Information Access Conference (RIAO-2000),</booktitle>
<pages>1288--1304</pages>
<location>Paris, France,</location>
<contexts>
<context position="2027" citStr="Gaizauskas and Humphreys, 2000" startWordPosition="296" endWordPosition="299">as a paragraph retrieval mechanism. Recent results from the TREC evaluations ((Kwok et al., 2000) (Radev et al., 2000) (Allen 1The Text REtrieval Conference (TREC) is a series of workshops organized by the National Institute of Standards and Technology (NIST), designed to advance the state-ofthe-art in information retrieval (IR) et al., 2000)) show that Information Retrieval (IR) techniques alone are not sufficient for finding answers with high precision. In fact, more and more systems adopt architectures in which the semantics of the questions are captured prior to paragraph retrieval (e.g. (Gaizauskas and Humphreys, 2000) (Harabagiu et al., 2000)) and used later in extracting the answer (cf. (Abney et al., 2000)). When processing a natural language question two goals must be achieved. First we need to know what is the expected answer type; in other words, we need to know what we are looking for. Second, we need to know where to look for the answer, e.g. we must identify the question keywords to be used in the paragraph retrieval. The expected answer type is determined based on the question stem, e.g. who, where or how much and eventually one of the question concepts, when the stem is ambiguous (for example wha</context>
<context position="5055" citStr="Gaizauskas and Humphreys, 2000" startWordPosition="788" endWordPosition="791">available to be incorporated in Q&amp;A systems and helped recognize names of people, organizations, locations or dates. Assuming that it is very likely that the answer is a named entity, (Srihari and Li, 2000) describes a NE-supported Q&amp;A system that functions quite well when the expected answer type is one of the categories covered by the NE recognizer. Unfortunately this system is not fully autonomous, as it depends on IR results provided by external search engines. Answer extractions based on NE recognizers were also developed in the Q&amp;A presented in (Abney et al., 2000) (Radev et al., 2000) (Gaizauskas and Humphreys, 2000). As noted in (Voorhees and Tice, 2000), Q&amp;A systems that did not include NE recognizers performed poorly in the TREC evaluations, especially in the short answer category. Some Q&amp;A systems, like (Moldovan et al., 2000) relied both on NE recognizers and some empirical indicators. However, the answer does not always belong to a category covered by the NE recognizer. For such cases several approaches have been developed. The first one, presented in (Harabagiu et al., 2000), the answer type is derived from a large answer taxonomy. A different approach, based on statistical techniques was proposed </context>
</contexts>
<marker>Gaizauskas, Humphreys, 2000</marker>
<rawString>Robert Gaizauskas and Kevin Humphreys. A combined IR/NLP approach to question answering against large text collections. In Proceedings of the 6th Content-Based Multimedia Information Access Conference (RIAO-2000), pages 1288–1304, Paris, France, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Marius Pas¸ca</author>
<author>Steven Maiorano</author>
</authors>
<title>Experiments with Open-Domain Textual Question Answering.</title>
<date>2000</date>
<booktitle>In the Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000),</booktitle>
<pages>292--298</pages>
<marker>Harabagiu, Pas¸ca, Maiorano, 2000</marker>
<rawString>Sanda Harabagiu, Marius Pas¸ca and Steven Maiorano. Experiments with Open-Domain Textual Question Answering. In the Proceedings of the 18th International Conference on Computational Linguistics (COLING-2000), pages 292–298, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
<author>Dan Magerman</author>
<author>Robert Mercer</author>
</authors>
<title>Adwait Ratnaparkhi and Selim Roukos. Decision tree parsing using a hidden derivational model.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 Human Language Technology Workshop,</booktitle>
<pages>272--277</pages>
<contexts>
<context position="7943" citStr="Jelinek et al., 1994" startWordPosition="1271" endWordPosition="1274">answer type. Both the question keywords and the expected answer type are identified by using the dependencies derived from the question parse. By implementing our own version of the publicly available Collins parser (Collins, 1996), we also learned a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words. For example, the parse tree of TREC-9 question Q210: “How many dogs pull a sled in the Iditarod ?” is: For each possible constituent in a parse tree, rules first described in (Magerman, 1995) and (Jelinek et al., 1994) identify the head-child and propagate the head-word to its parent. For the parse of question Q210 the propagation is: When the propagation is over, head-modifier relations are extracted, generating the following dependency structure, called question semantic form in (Harabagiu et al., 2000). COUNT dogs pull sled Iditarod In the structure above, COUNT represents the expected answer type, replacing the question stem “how many”. Few question stems are unambiguous (e.g. who, when). If the question stem is ambiguous, the expected answer type is determined by the concept from the question semantic </context>
</contexts>
<marker>Jelinek, Lafferty, Magerman, Mercer, 1994</marker>
<rawString>Frederick Jelinek, John Lafferty, Dan Magerman, Robert Mercer, Adwait Ratnaparkhi and Selim Roukos. Decision tree parsing using a hidden derivational model. In Proceedings of the 1994 Human Language Technology Workshop, pages 272–277, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Kwok</author>
<author>L Grunfeld</author>
<author>N Dinstl</author>
<author>M Chan</author>
</authors>
<title>TREC9 Cross Language, Web and Question-Answering Track Experiments using PIRCS.</title>
<date>2000</date>
<booktitle>Proceedings of the Text Retrieval Conference (TREC-9),</booktitle>
<pages>26--35</pages>
<contexts>
<context position="1493" citStr="Kwok et al., 2000" startWordPosition="213" endWordPosition="216">ing in large collections of documents a text snippet where the answer to a natural language question lies. The answer is constrained to be found either in a short (50 bytes) or a long (250 bytes) text span. Frequently, keywords extracted from the natural language question are either within the text span or in its immediate vicinity, forming a text paragraph. Since such paragraphs must be identified throughout voluminous collections, automatic and autonomous Q&amp;A systems incorporate an index of the collection as well as a paragraph retrieval mechanism. Recent results from the TREC evaluations ((Kwok et al., 2000) (Radev et al., 2000) (Allen 1The Text REtrieval Conference (TREC) is a series of workshops organized by the National Institute of Standards and Technology (NIST), designed to advance the state-ofthe-art in information retrieval (IR) et al., 2000)) show that Information Retrieval (IR) techniques alone are not sufficient for finding answers with high precision. In fact, more and more systems adopt architectures in which the semantics of the questions are captured prior to paragraph retrieval (e.g. (Gaizauskas and Humphreys, 2000) (Harabagiu et al., 2000)) and used later in extracting the answer</context>
</contexts>
<marker>Kwok, Grunfeld, Dinstl, Chan, 2000</marker>
<rawString>K.L. Kwok, L. Grunfeld, N. Dinstl and M. Chan. TREC9 Cross Language, Web and Question-Answering Track Experiments using PIRCS. Proceedings of the Text Retrieval Conference (TREC-9), pages 26–35, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Magerman</author>
</authors>
<title>Statistical decision-tree models of parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL-95,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="7916" citStr="Magerman, 1995" startWordPosition="1268" endWordPosition="1269">taining the expected answer type. Both the question keywords and the expected answer type are identified by using the dependencies derived from the question parse. By implementing our own version of the publicly available Collins parser (Collins, 1996), we also learned a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words. For example, the parse tree of TREC-9 question Q210: “How many dogs pull a sled in the Iditarod ?” is: For each possible constituent in a parse tree, rules first described in (Magerman, 1995) and (Jelinek et al., 1994) identify the head-child and propagate the head-word to its parent. For the parse of question Q210 the propagation is: When the propagation is over, head-modifier relations are extracted, generating the following dependency structure, called question semantic form in (Harabagiu et al., 2000). COUNT dogs pull sled Iditarod In the structure above, COUNT represents the expected answer type, replacing the question stem “how many”. Few question stems are unambiguous (e.g. who, when). If the question stem is ambiguous, the expected answer type is determined by the concept </context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>Dan Magerman. Statistical decision-tree models of parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL-95, pages 276– 283, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database. Communication oftheACM, vol 38: No11,</title>
<date>1995</date>
<pages>39--41</pages>
<contexts>
<context position="14999" citStr="Miller, 1995" startWordPosition="2392" endWordPosition="2393">esident France Vichy head government France Vichy PERSON(Marshall Philippe Petain) &amp;quot;Marshall Philippe Petain, head of Vichy France government&amp;quot; Q318: Where did Bill Gates go to college? ORGANIZATION go college Bill Gates dropout founder Microsoft ORGANIZATION=college(Harvard) Bill Gates &amp;quot;Harvard dropout and Microsoft founder&amp;quot; Answer Question Answer Question Answer Question form and answer semantic form can be unified: The resulting normalized dependencies are: TEMPERATURE(300 degrees) The semantic information and the world knowledge needed for the above unifications are available from WordNet (Miller, 1995). Moreover, this knowledge can be translated in axiomatic form and used for abductive proofs. Each of the feedback loops provide the retrieval engine with new alternations of the question keywords. Feedback loop 2 considers morphological and lexical alternations whereas Feedback loop 3 uses semantic alternations. The method of generating the alternations is detailed in Section 4. 4 Keyword Alternations To enhance the chance of finding the answer to a question, each feedback loop provides with a different set of keyword alternations. Such alternations can be classified according to the linguist</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. WordNet: A Lexical Database. Communication oftheACM, vol 38: No11, pages 39–41, November 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pas¸ca</author>
<author>Rada Mihalcea</author>
<author>Richard Goodrum</author>
</authors>
<title>Roxana Girju and Vasile Rus. The Structure and Performance of an Open-Domain Question Answering System.</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Annual Meeting of the Association for Comoutational Linguistics (ACL-2000),</booktitle>
<pages>563--570</pages>
<marker>Moldovan, Harabagiu, Pas¸ca, Mihalcea, Goodrum, 2000</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pas¸ca, Rada Mihalcea, Richard Goodrum, Roxana Girju and Vasile Rus. The Structure and Performance of an Open-Domain Question Answering System. Proceedings of the 38th Annual Meeting of the Association for Comoutational Linguistics (ACL-2000), pages 563–570, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>John Prager</author>
<author>V Samn</author>
</authors>
<title>Ranking suspected answers to natural language questions using predictive annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000),</booktitle>
<pages>150--157</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="1514" citStr="Radev et al., 2000" startWordPosition="217" endWordPosition="220">ions of documents a text snippet where the answer to a natural language question lies. The answer is constrained to be found either in a short (50 bytes) or a long (250 bytes) text span. Frequently, keywords extracted from the natural language question are either within the text span or in its immediate vicinity, forming a text paragraph. Since such paragraphs must be identified throughout voluminous collections, automatic and autonomous Q&amp;A systems incorporate an index of the collection as well as a paragraph retrieval mechanism. Recent results from the TREC evaluations ((Kwok et al., 2000) (Radev et al., 2000) (Allen 1The Text REtrieval Conference (TREC) is a series of workshops organized by the National Institute of Standards and Technology (NIST), designed to advance the state-ofthe-art in information retrieval (IR) et al., 2000)) show that Information Retrieval (IR) techniques alone are not sufficient for finding answers with high precision. In fact, more and more systems adopt architectures in which the semantics of the questions are captured prior to paragraph retrieval (e.g. (Gaizauskas and Humphreys, 2000) (Harabagiu et al., 2000)) and used later in extracting the answer (cf. (Abney et al., </context>
<context position="5022" citStr="Radev et al., 2000" startWordPosition="784" endWordPosition="787">systems were readily available to be incorporated in Q&amp;A systems and helped recognize names of people, organizations, locations or dates. Assuming that it is very likely that the answer is a named entity, (Srihari and Li, 2000) describes a NE-supported Q&amp;A system that functions quite well when the expected answer type is one of the categories covered by the NE recognizer. Unfortunately this system is not fully autonomous, as it depends on IR results provided by external search engines. Answer extractions based on NE recognizers were also developed in the Q&amp;A presented in (Abney et al., 2000) (Radev et al., 2000) (Gaizauskas and Humphreys, 2000). As noted in (Voorhees and Tice, 2000), Q&amp;A systems that did not include NE recognizers performed poorly in the TREC evaluations, especially in the short answer category. Some Q&amp;A systems, like (Moldovan et al., 2000) relied both on NE recognizers and some empirical indicators. However, the answer does not always belong to a category covered by the NE recognizer. For such cases several approaches have been developed. The first one, presented in (Harabagiu et al., 2000), the answer type is derived from a large answer taxonomy. A different approach, based on sta</context>
</contexts>
<marker>Radev, Prager, Samn, 2000</marker>
<rawString>Dragomir Radev, John Prager, and V. Samn. Ranking suspected answers to natural language questions using predictive annotation. In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000), pages 150–157, Seattle, Washington, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohini Srihari</author>
<author>W Li</author>
</authors>
<title>A question answering system supported by information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000),</booktitle>
<location>Seattle, Washington,</location>
<contexts>
<context position="2715" citStr="Srihari and Li, 2000" startWordPosition="420" endWordPosition="423">(cf. (Abney et al., 2000)). When processing a natural language question two goals must be achieved. First we need to know what is the expected answer type; in other words, we need to know what we are looking for. Second, we need to know where to look for the answer, e.g. we must identify the question keywords to be used in the paragraph retrieval. The expected answer type is determined based on the question stem, e.g. who, where or how much and eventually one of the question concepts, when the stem is ambiguous (for example what), as described in (Harabagiu et al., 2000) (Radev et al., 2000) (Srihari and Li, 2000). However finding question keywords that retrieve all candidate answers cannot be achieved only by deriving some of the words used in the question. Frequently, question reformulations use different words, but imply the same answer. Moreover, many equivalent answers are phrased differently. In this paper we argue that the answer to complex natural language questions cannot be extracted with significant precision from large collections of texts unless several lexico-semantic feedback loops are allowed. In Section 2 we survey the related work whereas in Section 3 we describe the feedback loops th</context>
<context position="4630" citStr="Srihari and Li, 2000" startWordPosition="718" endWordPosition="721">in a text and map it to a predefined, target representation, known as template. Although simple combinations of IR and IE techniques are not practical solutions for open-domain textual Q&amp;A because IE systems are based on domain-specific knowledge, their contribution to current open-domain Q&amp;A methods is significant. For example, stateof-the-art Named Entity (NE) recognizers developed for IE systems were readily available to be incorporated in Q&amp;A systems and helped recognize names of people, organizations, locations or dates. Assuming that it is very likely that the answer is a named entity, (Srihari and Li, 2000) describes a NE-supported Q&amp;A system that functions quite well when the expected answer type is one of the categories covered by the NE recognizer. Unfortunately this system is not fully autonomous, as it depends on IR results provided by external search engines. Answer extractions based on NE recognizers were also developed in the Q&amp;A presented in (Abney et al., 2000) (Radev et al., 2000) (Gaizauskas and Humphreys, 2000). As noted in (Voorhees and Tice, 2000), Q&amp;A systems that did not include NE recognizers performed poorly in the TREC evaluations, especially in the short answer category. Som</context>
</contexts>
<marker>Srihari, Li, 2000</marker>
<rawString>Rohini Srihari and W. Li. A question answering system supported by information extraction. In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP-2000), Seattle, Washington, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Dawn Tice</author>
</authors>
<title>Building a question answering test collection.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-2000),</booktitle>
<location>Athens, Greece,</location>
<contexts>
<context position="5094" citStr="Voorhees and Tice, 2000" startWordPosition="795" endWordPosition="798">d helped recognize names of people, organizations, locations or dates. Assuming that it is very likely that the answer is a named entity, (Srihari and Li, 2000) describes a NE-supported Q&amp;A system that functions quite well when the expected answer type is one of the categories covered by the NE recognizer. Unfortunately this system is not fully autonomous, as it depends on IR results provided by external search engines. Answer extractions based on NE recognizers were also developed in the Q&amp;A presented in (Abney et al., 2000) (Radev et al., 2000) (Gaizauskas and Humphreys, 2000). As noted in (Voorhees and Tice, 2000), Q&amp;A systems that did not include NE recognizers performed poorly in the TREC evaluations, especially in the short answer category. Some Q&amp;A systems, like (Moldovan et al., 2000) relied both on NE recognizers and some empirical indicators. However, the answer does not always belong to a category covered by the NE recognizer. For such cases several approaches have been developed. The first one, presented in (Harabagiu et al., 2000), the answer type is derived from a large answer taxonomy. A different approach, based on statistical techniques was proposed in (Radev et al., 2000). (Cardie et al.</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Ellen M. Voorhees and Dawn Tice. Building a question answering test collection. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-2000), Athens, Greece, 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>