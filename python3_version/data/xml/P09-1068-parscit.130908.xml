<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.965983">
Unsupervised Learning of Narrative Schemas and their Participants
</title>
<author confidence="0.918206">
Nathanael Chambers and Dan Jurafsky
</author>
<affiliation confidence="0.803717">
Stanford University, Stanford, CA 94305
</affiliation>
<email confidence="0.993725">
{natec,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.993745" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998705625">
We describe an unsupervised system for learn-
ing narrative schemas, coherent sequences or sets
of events (arrested(POLICE,SUSPECT), convicted(
JUDGE, SUSPECT)) whose arguments are filled
with participant semantic roles defined over words
(JUDGE = {judge, jury, court}, POLICE = {police,
agent, authorities}). Unlike most previous work in
event structure or semantic role learning, our sys-
tem does not use supervised techniques, hand-built
knowledge, or predefined classes of events or roles.
Our unsupervised learning algorithm uses corefer-
ring arguments in chains of verbs to learn both rich
narrative event structure and argument roles. By
jointly addressing both tasks, we improve on pre-
vious results in narrative/frame learning and induce
rich frame-specific semantic roles.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999810051282051">
This paper describes a new approach to event se-
mantics that jointly learns event relations and their
participants from unlabeled corpora.
The early years of natural language processing
(NLP) took a “top-down” approach to language
understanding, using representations like scripts
(Schank and Abelson, 1977) (structured represen-
tations of events, their causal relationships, and
their participants) and frames to drive interpreta-
tion of syntax and word use. Knowledge structures
such as these provided the interpreter rich infor-
mation about many aspects of meaning.
The problem with these rich knowledge struc-
tures is that the need for hand construction, speci-
ficity, and domain dependence prevents robust and
flexible language understanding. Instead, mod-
ern work on understanding has focused on shal-
lower representations like semantic roles, which
express at least one aspect of the semantics of
events and have proved amenable to supervised
learning from corpora like PropBank (Palmer et
al., 2005) and Framenet (Baker et al., 1998). Un-
fortunately, creating these supervised corpora is an
expensive and difficult multi-year effort, requiring
complex decisions about the exact set of roles to
be learned. Even unsupervised attempts to learn
semantic roles have required a pre-defined set of
roles (Grenager and Manning, 2006) and often a
hand-labeled seed corpus (Swier and Stevenson,
2004; He and Gildea, 2006).
In this paper, we describe our attempts to learn
script-like information about the world, including
both event structures and the roles of their partic-
ipants, but without pre-defined frames, roles, or
tagged corpora.
Consider the following Narrative Schema, to be
defined more formally later. The events on the left
follow a set of participants through a series of con-
nected events that constitute a narrative:
</bodyText>
<figure confidence="0.7354025">
Events Roles
A = Police
B = Suspect
C = Plea
D = Jury
D sentence B
</figure>
<bodyText confidence="0.9987273">
Being able to robustly learn sets of related
events (left) and frame-specific role information
about the argument types that fill them (right)
could assist a variety of NLP applications, from
question answering to machine translation.
Our previous work (Chambers and Jurafsky,
2008) relied on the intuition that in a coherent text,
any two events that are about the same participants
are likely to be part of the same story or narra-
tive. The model learned simple aspects of nar-
rative structure (‘narrative chains’) by extracting
events that share a single participant, the protag-
onist. In this paper we extend this work to rep-
resent sets of situation-specific events not unlike
scripts, caseframes (Bean and Riloff, 2004), and
FrameNet frames (Baker et al., 1998). This paper
shows that verbs in distinct narrative chains can be
merged into an improved single narrative schema,
while the shared arguments across verbs can pro-
vide rich information for inducing semantic roles.
</bodyText>
<figure confidence="0.99611925">
A search B
A arrest B
B plead C
D acquit B D convict B
</figure>
<page confidence="0.97211">
602
</page>
<note confidence="0.9996055">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602–610,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.967996" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999474">
This paper addresses two areas of work in event
semantics, narrative event chains and semantic
role labeling. We begin by highlighting areas in
both that can mutually inform each other through
a narrative schema model.
</bodyText>
<subsectionHeader confidence="0.91866">
2.1 Narrative Event Chains
</subsectionHeader>
<bodyText confidence="0.99991775">
Narrative Event Chains are partially ordered sets
of events that all involve the same shared par-
ticipant, the protagonist (Chambers and Jurafsky,
2008). A chain contains a set of verbs represent-
ing events, and for each verb, the grammatical role
filled by the shared protagonist.
An event is a verb together with its constellation
of arguments. An event slot is a tuple of an event
and a particular argument slot (grammatical rela-
tion), represented as a pair (v, d) where v is a verb
and d E {subject, object, prep}. A chain is a tu-
ple (L, O) where L is a set of event slots and O is
a partial (temporal) ordering. We will write event
slots in shorthand as (X pleads) or (pleads X) for
(pleads, subject) and (pleads, object). Below is
an example chain modeling criminal prosecution.
</bodyText>
<equation confidence="0.994081571428571">
L = (X pleads), (X admits), (convicted X), (sentenced X)
O = {(pleads, convicted), (convicted, sentenced), ...}
A graphical view is often more intuitive:
(X admits)
(X pleads)
(convicted X)
(sentenced X)
</equation>
<bodyText confidence="0.999185896551725">
In this example, the protagonist of the chain
is the person being prosecuted and the other un-
specified event slots remain unfilled and uncon-
strained. Chains in the Chambers and Jurafsky
(2008) model are ordered; in this paper rather than
address the ordering task we focus on event and ar-
gument induction, leaving ordering as future work.
The Chambers and Jurafsky (2008) model
learns chains completely unsupervised, (albeit af-
ter parsing and resolving coreference in the text)
by counting pairs of verbs that share corefer-
ring arguments within documents and computing
the pointwise mutual information (PMI) between
these verb-argument pairs. The algorithm creates
chains by clustering event slots using their PMI
scores, and we showed this use of co-referring ar-
guments improves event relatedness.
Our previous work, however, has two major
limitations. First, the model did not express
any information about the protagonist, such as its
type or role. Role information (such as knowing
whether a filler is a location, a person, a particular
class of people, or even an inanimate object) could
crucially inform learning and inference. Second,
the model only represents one participant (the pro-
tagonist). Representing the other entities involved
in all event slots in the narrative could potentially
provide valuable information. We discuss both of
these extensions next.
</bodyText>
<subsectionHeader confidence="0.589075">
2.1.1 The Case for Arguments
</subsectionHeader>
<bodyText confidence="0.999920846153846">
The Chambers and Jurafsky (2008) narrative
chains do not specify what type of argument fills
the role of protagonist. Chain learning and clus-
tering is based only on the frequency with which
two verbs share arguments, ignoring any features
of the arguments themselves.
Take this example of an actual chain from an
article in our training data. Given this chain of five
events, we want to choose other events most likely
to occur in this scenario.
One of the top scoring event slots is (fly X). Nar-
rative chains incorrectly favor (fly X) because it is
observed during training with all five event slots,
although not frequently with any one of them. An
event slot like (charge X) is much more plausible,
but is unfortunately scored lower by the model.
Representing the types of the arguments can
help solve this problem. Few types of arguments
are shared between the chain and (fly X). How-
ever, (charge X) shares many arguments with (ac-
cuse X), (search X) and (suspect X) (e.g., criminal
and suspect). Even more telling is that these argu-
ments are jointly shared (the same or coreferent)
across all three events. Chains represent coherent
scenarios, not just a set of independent pairs, so we
want to model argument overlap across all pairs.
</bodyText>
<subsectionHeader confidence="0.496767">
2.1.2 The Case for Joint Chains
</subsectionHeader>
<bodyText confidence="0.996168333333333">
The second problem with narrative chains is that
they make judgments only between protagonist ar-
guments, one slot per event. All entities and slots
</bodyText>
<figure confidence="0.998328454545454">
hunt
use
suspect
accuse
search
fly
charge
admits
pleads
convicted
sentenced
</figure>
<page confidence="0.998386">
603
</page>
<bodyText confidence="0.999593615384615">
in the space of events should be jointly considered
when making event relatedness decisions.
As an illustration, consider the verb arrest.
Which verb is more related, convict or capture?
A narrative chain might only look at the objects
of these verbs and choose the one with the high-
est score, usually choosing convict. But in this
case the subjects offer additional information; the
subject of arrest (police) is different from that of
convict (judge). A more informed decision prefers
capture because both the objects (suspect) and
subjects (police) are identical. This joint reason-
ing is absent from the narrative chain model.
</bodyText>
<subsectionHeader confidence="0.99893">
2.2 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.999938368421052">
The task of semantic role learning and labeling
is to identify classes of entities that fill predicate
slots; semantic roles seem like they’d be a good
model for the kind of argument types we’d like
to learn for narratives. Most work on semantic
role labeling, however, is supervised, using Prop-
bank (Palmer et al., 2005), FrameNet (Baker et
al., 1998) or VerbNet (Kipper et al., 2000) as
gold standard roles and training data. More re-
cent learning work has applied bootstrapping ap-
proaches (Swier and Stevenson, 2004; He and
Gildea, 2006), but these still rely on a hand la-
beled seed corpus as well as a pre-defined set of
roles. Grenegar and Manning (2006) use the EM
algorithm to learn PropBank roles from unlabeled
data, and unlike bootstrapping, they don’t need a
labeled corpus from which to start. However, they
do require a predefined set of roles (arg0, arg1,
etc.) to define the domain of their probabilistic
model.
Green and Dorr (2005) use WordNet’s graph
structure to cluster its verbs into FrameNet frames,
using glosses to name potential slots. We differ in
that we attempt to learn frame-like narrative struc-
ture from untagged newspaper text. Most sim-
ilar to us, Alishahi and Stevenson (2007) learn
verb specific semantic profiles of arguments us-
ing WordNet classes to define the roles. We learn
situation-specific classes of roles shared by multi-
ple verbs.
Thus, two open goals in role learning include
(1) unsupervised learning and (2) learning the
roles themselves rather than relying on pre-defined
role classes. As just described, Chambers and Ju-
rafsky (2008) offers an unsupervised approach to
event learning (goal 1), but lacks semantic role
knowledge (goal 2). The following sections de-
scribe a model that addresses both goals.
</bodyText>
<sectionHeader confidence="0.991646" genericHeader="method">
3 Narrative Schemas
</sectionHeader>
<bodyText confidence="0.999969333333333">
The next sections introduce typed narrative chains
and chain merging, extensions that allow us to
jointly learn argument roles with event structure.
</bodyText>
<subsectionHeader confidence="0.998673">
3.1 Typed Narrative Chains
</subsectionHeader>
<bodyText confidence="0.999980789473685">
The first step in describing a narrative schema is to
extend the definition of a narrative chain to include
argument types. We now constrain the protagonist
to be of a certain type or role. A Typed Narrative
Chain is a partially ordered set of event slots that
share an argument, but now the shared argument
is a role defined by being a member of a set of
types R. These types can be lexical units (such as
observed head words), noun clusters, or other se-
mantic representations. We use head words in the
examples below, but we also evaluate with argu-
ment clustering by mapping head words to mem-
ber clusters created with the CBC clustering algo-
rithm (Pantel and Lin, 2002).
We define a typed narrative chain as a tuple
(L, P, O) with L and O the set of event slots
and partial ordering as before. Let P be a set of
argument types (head words) representing a single
role. An example is given here:
</bodyText>
<equation confidence="0.998225">
L = {(hunt X), (X use), (suspect X), (accuse X), (search X)}
P = {person, government, company, criminal, ...}
O = {(use, hunt), (suspect, search), (suspect, accuse) ... }
</equation>
<subsectionHeader confidence="0.999688">
3.2 Learning Argument Types
</subsectionHeader>
<bodyText confidence="0.971225666666667">
As mentioned above, narrative chains are learned
by parsing the text, resolving coreference, and ex-
tracting chains of events that share participants. In
our new model, argument types are learned simul-
taneously with narrative chains by finding salient
words that represent coreferential arguments. We
record counts of arguments that are observed with
each pair of event slots, build the referential set
for each word from its coreference chain, and then
represent each observed argument by the most fre-
quent head word in its referential set (ignoring pro-
nouns and mapping entity mentions with person
pronouns to a constant PERSON identifier).
As an example, the following contains four
worker mentions:
But for a growing proportion of U.S. workers, the troubles re-
ally set in when they apply for unemployment benefits. Many
workers find their benefits challenged.
</bodyText>
<page confidence="0.914357">
604
</page>
<equation confidence="0.998481666666667">
L = {X arrest, X charge, X raid, X seize,
X confiscate, X detain, X deport }
P = {police, agent, authority, government}
</equation>
<figureCaption confidence="0.904442">
Figure 1: A typed narrative chain. The four top
arguments are given. The ordering O is not shown.
</figureCaption>
<bodyText confidence="0.999893125">
The four bolded terms are coreferential and
(hopefully) identified by coreference. Our algo-
rithm chooses the head word of each phrase and
ignores the pronouns. It then chooses the most
frequent head word as the most salient mention.
In this example, the most salient term is workers.
If any pair of event slots share arguments from this
set, we count workers. In this example, the pair (X
find) and (X apply) shares an argument (they and
workers). The pair ((X find),(X apply)) is counted
once for narrative chain induction, and ((X find),
(X apply), workers) once for argument induction.
Figure 1 shows the top occurring words across
all event slot pairs in a criminal scenario chain.
This chain will be part of a larger narrative
schema, described in section 3.4.
</bodyText>
<subsectionHeader confidence="0.998038">
3.3 Event Slot Similarity with Arguments
</subsectionHeader>
<bodyText confidence="0.9989098">
We now formalize event slot similarity with argu-
ments. Narrative chains as defined in (Chambers
and Jurafsky, 2008) score a new event slot (f, g)
against a chain of size n by summing over the
scores between all pairs:
</bodyText>
<equation confidence="0.9916555">
chainsim(C, (f, g)) _ �n sim((ei, di) , (f, g)) (1)
i=1
</equation>
<bodyText confidence="0.998717857142857">
where C is a narrative chain, f is a verb with
grammatical argument g, and sim(e, e&apos;) is the
pointwise mutual information pmi(e, e&apos;). Grow-
ing a chain by one adds the highest scoring event.
We extend this function to include argument
types by defining similarity in the context of a spe-
cific argument a:
</bodyText>
<equation confidence="0.760952">
sim((e, d) , (e0, d0) , a) _
pmi((e, d) , (e0, d0)) + A log freq((e, d) , (e0, d0) , a)
(2)
</equation>
<bodyText confidence="0.949826625">
where A is a constant weighting factor and
freq(b, b&apos;, a) is the corpus count of a filling the
arguments of events b and b&apos;. We then score the
entire chain for a particular argument:
sim((ei, di) , (ej, dj) , a) (3)
Using this chain score, we finally extend
chainsim to score a new event slot based on the
argument that maximizes the entire chain’s score:
</bodyText>
<equation confidence="0.9705575">
chainsim0(C, (f, g)) _
sim((ei,di) , (f, g) , a)) (4)
</equation>
<bodyText confidence="0.993956">
The argument is now directly influencing event
slot similarity scores. We will use this definition
in the next section to build Narrative Schemas.
</bodyText>
<subsectionHeader confidence="0.93241">
3.4 Narrative Schema: Multiple Chains
</subsectionHeader>
<bodyText confidence="0.9998505">
Whereas a narrative chain is a set of event slots,
a Narrative Schema is a set of typed narrative
chains. A schema thus models all actors in a set
of events. If (push X) is in one chain, (Y push) is
in another. This allows us to model a document’s
entire narrative, not just one main actor.
</bodyText>
<subsubsectionHeader confidence="0.487235">
3.4.1 The Model
</subsubsectionHeader>
<bodyText confidence="0.99998055">
A narrative schema is defined as a 2-tuple N =
(E, C) with E a set of events (here defined as
verbs) and C a set of typed chains over the
event slots. We represent an event as a verb v
and its grammatical argument positions D„ C_
{subject, object, prep}. Thus, each event slot
(v, d) for all d E D„ belongs to a chain c E C
in the schema. Further, each c must be unique for
each slot of a single verb. Using the criminal pros-
ecution domain as an example, a narrative schema
in this domain is built as in figure 2.
The three dotted boxes are graphical represen-
tations of the typed chains that are combined in
this schema. The first represents the event slots in
which the criminal is involved, the second the po-
lice, and the third is a court or judge. Although our
representation uses a set of chains, it is equivalent
to represent a schema as a constraint satisfaction
problem between (e, d) event slots. The next sec-
tion describes how to learn these schemas.
</bodyText>
<subsectionHeader confidence="0.972983">
3.4.2 Learning Narrative Schemas
</subsectionHeader>
<bodyText confidence="0.999671090909091">
Previous work on narrative chains focused on re-
latedness scores between pairs of verb arguments
(event slots). The clustering step which built
chains depended on these pairwise scores. Narra-
tive schemas use a generalization of the entire verb
with all of its arguments. A joint decision can be
made such that a verb is added to a schema if both
its subject and object are assigned to chains in the
schema with high confidence.
For instance, it may be the case that (Y
pull over) scores well with the ‘police’ chain in
</bodyText>
<equation confidence="0.81723">
n−1�
i=1
score(C, a) _
�n
j=i+1
�n
i=1
(score(C, a) +
max
a
</equation>
<page confidence="0.897281">
605
</page>
<figureCaption confidence="0.985511">
Figure 2: Merging typed chains into a single unordered Narrative Schema.
</figureCaption>
<figure confidence="0.998309666666667">
police,
agent
arrest
charge
criminal,
suspect
guilty,
innocent
plead
convict
police,agent judge,jury
sentence
criminal,suspect
arrest
charge
judge,
jury
sentence
convict
convict
sentence
arrest
charge
plead
</figure>
<bodyText confidence="0.934885230769231">
figure 3. However, the object of (pull over A)
is not present in any of the other chains. Police
pull over cars, but this schema does not have a
chain involving cars. In contrast, (Y search) scores
well with the ‘police’ chain and (search X) scores
well in the ‘defendant’ chain too. Thus, we want
to favor search instead of pull over because the
schema is already modeling both arguments.
This intuition leads us to our event relatedness
function for the entire narrative schema N, not
just one chain. Instead of asking which event slot
(v, d) is a best fit, we ask if v is best by considering
all slots at once:
</bodyText>
<equation confidence="0.995759">
narsim(N, v) _
chainsim&apos;(c, (v, d))) (5)
</equation>
<bodyText confidence="0.9999488">
where CN is the set of chains in our narrative N. If
(v, d) does not have strong enough similarity with
any chain, it creates a new one with base score Q.
The Q parameter balances this decision of adding
to an existing chain in N or creating a new one.
</bodyText>
<subsectionHeader confidence="0.969827">
3.4.3 Building Schemas
</subsectionHeader>
<bodyText confidence="0.9975312">
We use equation 5 to build schemas from the set
of events as opposed to the set of event slots that
previous work on narrative chains used. In Cham-
bers and Jurafsky (2008), narrative chains add the
best (e, d) based on the following:
</bodyText>
<equation confidence="0.894855">
chainsim(c, (vj, gj)) (6)
</equation>
<bodyText confidence="0.9996595">
where m is the number of seen event slots in the
corpus and (vj, gj) is the jth such possible event
slot. Schemas are now learned by adding events
that maximize equation 5:
</bodyText>
<equation confidence="0.884834">
narsim(N, vj) (7)
</equation>
<bodyText confidence="0.996057666666667">
where |v |is the number of observed verbs and vj
is the jth such verb. Verbs are incrementally added
to a narrative schema by strength of similarity.
</bodyText>
<figure confidence="0.668036">
defendant, nichols,
smith, simpson
police, agent,
authorities, government
license
</figure>
<figureCaption confidence="0.735333">
immigrant, reporter,
cavalo, migrant, alien
Figure 3: Graphical view of an unordered schema
automatically built starting from the verb ‘arrest’.
A Q value that encouraged splitting was used.
</figureCaption>
<sectionHeader confidence="0.942172" genericHeader="method">
4 Sample Narrative Schemas
</sectionHeader>
<bodyText confidence="0.99993685">
Figures 3 and 4 show two criminal schemas
learned completely automatically from the NYT
portion of the Gigaword Corpus (Graff, 2002).
We parse the text into dependency graphs and re-
solve coreferences. The figures result from learn-
ing over the event slot counts. In addition, figure 5
shows six of the top 20 scoring narrative schemas
learned by our system. We artificially required the
clustering procedure to stop (and sometimes con-
tinue) at six events per schema. Six was chosen
as the size to enable us to compare to FrameNet
in the next section; the mean number of verbs in
FrameNet frames is between five and six. A low
Q was chosen to limit chain splitting. We built a
new schema starting from each verb that occurs in
more than 3000 and less than 50,000 documents
in the NYT section. This amounted to approxi-
mately 1800 verbs from which we show the top
20. Not surprisingly, most of the top schemas con-
cern business, politics, crime, or food.
</bodyText>
<sectionHeader confidence="0.984428" genericHeader="method">
5 Frames and Roles
</sectionHeader>
<bodyText confidence="0.9942355">
Most previous work on unsupervised semantic
role labeling assumes that the set of possible
</bodyText>
<figure confidence="0.989884083333333">
raid
arrest
charge
seize
confiscate
detain
deport
max(Q, max
CEC,v
�
dEDv
max
j:0&lt;j&lt;m
max
j:0&lt;j&lt;|v|
606
A produce B A E {company, inc, corp, microsoft, B trade C A E {}
A sell B iraq, co, unit, maker, ...} B fell C B E {dollar, share, index, mark, currency,
A manufacture B A *quote B stock, yield, price, pound, ...}
A *market B B E {drug, product, system, test, B fall C C E {friday, most, year, percent, thursday
A distribute B software, funds, movie, ...} B -slip C monday, share, week, dollar, ...}
A -develop B B rise C
A boil B A E {wash, heat, thinly, onion, note} A detain B A E {police, agent, officer, authorities,
A slice B A confiscate B troops, official, investigator, ... }
A -peel B A seize B
B E {potato, onion, mushroom, clove, A raid B B E {suspect, government, journalist,
A saute B A search B monday, member, citizen, client,... }
orange, gnocchi } A arrest B
A cook B
A chop B
A *uphold B A E {court, judge, justice, panel, osteen, A own B A E {company, investor, trader, corp,
A *challenge B circuit, nicolau, sporkin, majority, ...} A *borrow B enron, inc, government, bank, itt, ...}
A rule B A sell B
A enforce B B E {law, ban, rule, constitutionality, A buy back B B E {share, stock, stocks, bond, company,
A *overturn B conviction, ruling, lawmaker, tax, ...} A buy B security, team, funds, house, ... }
A *strike down B A *repurchase B
</figure>
<figureCaption confidence="0.706063571428571">
Figure 5: Six of the top 20 scored Narrative Schemas. Events and arguments in italics were marked
misaligned by FrameNet definitions. * indicates verbs not in FrameNet. - indicates verb senses not in
FameNet.
defendant, nichols,
smith, simpson
jury, juror, court,
judge, tribunal, senate
</figureCaption>
<figure confidence="0.608457">
sentence
</figure>
<figureCaption confidence="0.738822">
Figure 4: Graphical view of an unordered schema
</figureCaption>
<bodyText confidence="0.962826916666667">
automatically built from the verb ‘convict’. Each
node shape is a chain in the schema.
classes is very small (i.e, PropBank roles ARG0
and ARG1) and is known in advance. By con-
trast, our approach induces sets of entities that ap-
pear in the argument positions of verbs in a nar-
rative schema. Our model thus does not assume
the set of roles is known in advance, and it learns
the roles at the same time as clustering verbs into
frame-like schemas. The resulting sets of entities
(such as {police, agent, authorities, government}
or {court, judge, justice}) can be viewed as a kind
of schema-specific semantic role.
How can this unsupervised method of learning
roles be evaluated? In Section 6 we evaluate the
schemas together with their arguments in a cloze
task. In this section we perform a more qualitative
evalation by comparing our schema to FrameNet.
FrameNet (Baker et al., 1998) is a database of
frames, structures that characterize particular sit-
uations. A frame consists of a set of events (the
verbs and nouns that describe them) and a set
of frame-specific semantic roles called frame el-
ements that can be arguments of the lexical units
in the frame. FrameNet frames share commonali-
ties with narrative schemas; both represent aspects
of situations in the world, and both link semanti-
cally related words into frame-like sets in which
each predicate draws its argument roles from a
frame-specific set. They differ in that schemas fo-
cus on events in a narrative, while frames focus on
events that share core participants. Nonetheless,
the fact that FrameNet defines frame-specific ar-
gument roles suggests that comparing our schemas
and roles to FrameNet would be elucidating.
We took the 20 learned narrative schemas de-
scribed in the previous section and used FrameNet
to perform qualitative evaluations on three aspects
of schema: verb groupings, linking structure (the
mapping of each argument role to syntactic sub-
ject or object), and the roles themselves (the set of
entities that constitutes the schema roles).
Verb groupings To compare a schema’s event
selection to a frame’s lexical units, we first map
the top 20 schemas to the FrameNet frames that
have the largest overlap with each schema’s six
verbs. We were able to map 13 of our 20 narra-
tives to FrameNet (for the remaining 7, no frame
contained more than one of the six verbs). The
remaining 13 schemas contained 6 verbs each for
a total of 78 verbs. 26 of these verbs, however,
did not occur in FrameNet, either at all, or with
the correct sense. Of the remaining 52 verb map-
pings, 35 (67%) occurred in the closest FrameNet
frame or in a frame one link away. 17 verbs (33%)
deliberate
deadlocked
found
convict
acquit
</bodyText>
<page confidence="0.994358">
607
</page>
<bodyText confidence="0.999912492957747">
occurred in a different frame than the one chosen.
We examined the 33% of verbs that occurred in
a different frame. Most occurred in related frames,
but did not have FrameNet links between them.
For instance, one schema includes the causal verb
trade with unaccusative verbs of change like rise
and fall. FrameNet separates these classes of verbs
into distinct frames, distinguishing motion frames
from caused-motion frames.
Even though trade and rise are in different
FrameNet frames, they do in fact have the narra-
tive relation that our system discovered. Of the 17
misaligned events, we judged all but one to be cor-
rect in a narrative sense. Thus although not exactly
aligned with FrameNet’s notion of event clusters,
our induction algorithm seems to do very well.
Linking structure Next, we compare a
schema’s linking structure, the grammatical
relation chosen for each verb event. We thus
decide, e.g., if the object of the verb arrest (arrest
B) plays the same role as the object of detain
(detain B), or if the subject of detain (B detain)
would have been more appropriate.
We evaluated the clustering decisions of the 13
schemas (78 verbs) that mapped to frames. For
each chain in a schema, we identified the frame
element that could correctly fill the most verb ar-
guments in the chain. The remaining arguments
were considered incorrect. Because we assumed
all verbs to be transitive, there were 156 arguments
(subjects and objects) in the 13 schema. Of these
156 arguments, 151 were correctly clustered to-
gether, achieving 96.8% accuracy.
The schema in figure 5 with events detain, seize,
arrest, etc. shows some of these errors. The object
of all of these verbs is an animate theme, but con-
fiscate B and raid B are incorrect; people cannot
be confiscated/raided. They should have been split
into their own chain within the schema.
Argument Roles Finally, we evaluate the
learned sets of entities that fill the argument slots.
As with the above linking evaluation, we first iden-
tify the best frame element for each argument. For
example, the events in the top left schema of fig-
ure 5 map to the Manufacturing frame. Argument
B was identified as the Product frame element. We
then evaluate the top 10 arguments in the argument
set, judging whether each is a reasonable filler of
the role. In our example, drug and product are cor-
rect Product arguments. An incorrect argument is
test, as it was judged that a test is not a product.
We evaluated all 20 schemas. The 13 mapped
schemas used their assigned frames, and we cre-
ated frame element definitions for the remaining 7
that were consistent with the syntactic positions.
There were 400 possible arguments (20 schemas,
2 chains each), and 289 were judged correct for a
precision of 72%. This number includes Person
and Organization names as correct fillers. A more
conservative metric removing these classes results
in 259 (65%) correct.
Most of the errors appear to be from parsing
mistakes. Several resulted from confusing objects
with adjuncts. Others misattached modifiers, such
as including most as an argument. The cooking
schema appears to have attached verbal arguments
learned from instruction lists (wash, heat, boil).
Two schemas require situations as arguments, but
the dependency graphs chose as arguments the
subjects of the embedded clauses, resulting in 20
incorrect arguments in these schema.
</bodyText>
<sectionHeader confidence="0.998004" genericHeader="method">
6 Evaluation: Cloze
</sectionHeader>
<bodyText confidence="0.99953175">
The previous section compared our learned knowl-
edge to current work in event and role semantics.
We now provide a more formal evaluation against
untyped narrative chains. The two main contribu-
tions of schema are (1) adding typed arguments
and (2) considering joint chains in one model. We
evaluate each using the narrative cloze test as in
(Chambers and Jurafsky, 2008).
</bodyText>
<subsectionHeader confidence="0.978806">
6.1 Narrative Cloze
</subsectionHeader>
<bodyText confidence="0.999975307692308">
The cloze task (Taylor, 1953) evaluates human un-
derstanding of lexical units by removing a random
word from a sentence and asking the subject to
guess what is missing. The narrative cloze is a
variation on this idea that removes an event slot
from a known narrative chain.Performance is mea-
sured by the position of the missing event slot in a
system’s ranked guess list.
This task is particularly attractive for narrative
schemas (and chains) because it aligns with one
of the original ideas behind Schankian scripts,
namely that scripts help humans ‘fill in the blanks’
when language is underspecified.
</bodyText>
<subsectionHeader confidence="0.997701">
6.2 Training and Test Data
</subsectionHeader>
<bodyText confidence="0.999805">
We count verb pairs and shared arguments over
the NYT portion of the Gigaword Corpus (years
1994-2004), approximately one million articles.
</bodyText>
<page confidence="0.984633">
608
</page>
<figure confidence="0.8505995">
Narrative Cloze Test
Training Data from 1994−X
</figure>
<figureCaption confidence="0.9693095">
Figure 6: Results with varying sizes of training
data.
</figureCaption>
<bodyText confidence="0.9999845">
We parse the text into typed dependency graphs
with the Stanford Parser (de Marneffe et al., 2006),
recording all verbs with subject, object, or prepo-
sitional typed dependencies. Unlike in (Chambers
and Jurafsky, 2008), we lemmatize verbs and ar-
gument head words. We use the OpenNLP1 coref-
erence engine to resolve entity mentions.
The test set is the same as in (Chambers and Ju-
rafsky, 2008). 100 random news articles were se-
lected from the 2001 NYT section of the Gigaword
Corpus. Articles that did not contain a protagonist
with five or more events were ignored, leaving a
test set of 69 articles. We used a smaller develop-
ment set of size 17 to tune parameters.
</bodyText>
<subsectionHeader confidence="0.987193">
6.3 Typed Chains
</subsectionHeader>
<bodyText confidence="0.9999614">
The first evaluation compares untyped against
typed narrative event chains. The typed model
uses equation 4 for chain clustering. The dotted
line ‘Chain’ and solid ‘Typed Chain’ in figure 6
shows the average ranked position over the test set.
The untyped chains plateau and begin to worsen
as the amount of training data increases, but the
typed model is able to improve for some time af-
ter. We see a 6.9% gain at 2004 when both lines
trend upwards.
</bodyText>
<subsectionHeader confidence="0.961419">
6.4 Narrative Schema
</subsectionHeader>
<bodyText confidence="0.999148">
The second evaluation compares the performance
of the narrative schema model against single nar-
rative chains. We ignore argument types and use
untyped chains in both (using equation 1 instead
</bodyText>
<footnote confidence="0.951449">
1http://opennlp.sourceforge.net/
</footnote>
<bodyText confidence="0.999746833333333">
of 4). The dotted line ‘Chain’ and solid ‘Schema’
show performance results in figure 6. Narrative
Schemas have better ranked scores in all data sizes
and follow the previous experiment in improving
results as more data is added even though untyped
chains trend upward. We see a 3.3% gain at 2004.
</bodyText>
<subsectionHeader confidence="0.975723">
6.5 Typed Narrative Schema
</subsectionHeader>
<bodyText confidence="0.999976416666667">
The final evaluation combines schemas with ar-
gument types to measure overall gain. We eval-
uated with both head words and CBC clusters
as argument representations. Not only do typed
chains and schemas outperform untyped chains,
combining the two gives a further performance
boost. Clustered arguments improve the re-
sults further, helping with sparse argument counts
(‘Typed Schema’ in figure 6 uses CBC argu-
ments). Overall, using all the data (by year 2004)
shows a 10.1% improvement over untyped narra-
tive chains.
</bodyText>
<sectionHeader confidence="0.999617" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999956304347826">
Our significant improvement in the cloze evalua-
tion shows that even though narrative cloze does
not evaluate argument types, jointly modeling the
arguments with events improves event cluster-
ing. Likewise, the FrameNet comparison suggests
that modeling related events helps argument learn-
ing. The tasks mutually inform each other. Our
argument learning algorithm not only performs
unsupervised induction of situation-specific role
classes, but the resulting roles and linking struc-
tures may also offer the possibility of (unsuper-
vised) FrameNet-style semantic role labeling.
Finding the best argument representation is an
important future direction. The performance of
our noun clusters in figure 6 showed that while the
other approaches leveled off, clusters continually
improved with more data. The exact balance be-
tween lexical units, clusters, or more general (tra-
ditional) semantic roles remains to be solved, and
may be application specific.
We hope in the future to show that a range of
NLU applications can benefit from the rich infer-
ential structures that narrative schemas provide.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998975">
This work is funded in part by NSF (IIS-0811974).
We thank the reviewers and the Stanford NLP
Group for helpful suggestions.
</bodyText>
<figure confidence="0.999061357142857">
1995 1996 1997 1 998 1999 2000 2001 2002 2003 2004
Ranked Position
1350
1300
1250
1200
1150
1100
1050
1000
Chain
Typed Chain
Schema
Typed Schema
</figure>
<page confidence="0.991487">
609
</page>
<sectionHeader confidence="0.995134" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850196078432">
Afra Alishahi and Suzanne Stevenson. 2007. A com-
putational usage-based model for learning general
properties of semantic roles. In The 2nd European
Cognitive Science Conference, Delphi, Greece.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages
86–90, San Francisco, California. Morgan Kauf-
mann Publishers.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. Proc. of HLT/NAACL, pages 297–
304.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08, Hawaii, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449–454.
David Graff. 2002. English Gigaword. Linguistic
Data Consortium.
Rebecca Green and Bonnie J. Dorr. 2005. Frame se-
mantic enhancement of lexical-semantic resources.
In ACL-SIGLEX Workshop on Deep Lexical Acqui-
sition, pages 57–66.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In EMNLP.
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary re-
port. Technical Report 891, University of Rochester.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of AAAI-2000, Austin, TX.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1):71–106.
Patrick Pantel and Dekang Lin. 2002. Document clus-
tering with committees. In ACM Conference on Re-
search and Development in Information Retrieval,
pages 199–206, Tampere, Finland.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erl-
baum.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In EMNLP.
Wilson L. Taylor. 1953. Cloze procedure: a new tool
for measuring readability. Journalism Quarterly,
30:415–433.
</reference>
<page confidence="0.997042">
610
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.562269">
<title confidence="0.994475">Unsupervised Learning of Narrative Schemas and their Participants</title>
<author confidence="0.783219">Chambers Jurafsky</author>
<affiliation confidence="0.603608">Stanford University, Stanford, CA 94305</affiliation>
<abstract confidence="0.999561352941176">We describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury, Unlike most previous work in event structure or semantic role learning, our system does not use supervised techniques, hand-built knowledge, or predefined classes of events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Afra Alishahi</author>
<author>Suzanne Stevenson</author>
</authors>
<title>A computational usage-based model for learning general properties of semantic roles.</title>
<date>2007</date>
<booktitle>In The 2nd European Cognitive Science Conference,</booktitle>
<location>Delphi, Greece.</location>
<contexts>
<context position="10065" citStr="Alishahi and Stevenson (2007)" startWordPosition="1616" endWordPosition="1619"> seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just described, Chambers and Jurafsky (2008) offers an unsupervised approach to event learning (goal 1), but lacks semantic role knowledge (goal 2). The following sections describe a model that addresses both goals. 3 Narrative Schemas The next sections introduce typed na</context>
</contexts>
<marker>Alishahi, Stevenson, 2007</marker>
<rawString>Afra Alishahi and Suzanne Stevenson. 2007. A computational usage-based model for learning general properties of semantic roles. In The 2nd European Cognitive Science Conference, Delphi, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>ACL-98,</booktitle>
<pages>86--90</pages>
<editor>In Christian Boitet and Pete Whitelock, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, California.</location>
<contexts>
<context position="2007" citStr="Baker et al., 1998" startWordPosition="284" endWordPosition="287">ive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Con</context>
<context position="3628" citStr="Baker et al., 1998" startWordPosition="546" endWordPosition="549">ould assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles. A search B A arrest B B plead C D acquit B D convict B 602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602–610, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Background This paper addresses two areas of work in event semantics, narrative event chains and semantic role labeling. We begin by highlighting areas in both that can mu</context>
<context position="9209" citStr="Baker et al., 1998" startWordPosition="1472" endWordPosition="1475">t of arrest (police) is different from that of convict (judge). A more informed decision prefers capture because both the objects (suspect) and subjects (police) are identical. This joint reasoning is absent from the narrative chain model. 2.2 Semantic Role Labeling The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use </context>
<context position="22704" citStr="Baker et al., 1998" startWordPosition="3847" endWordPosition="3850">bs in a narrative schema. Our model thus does not assume the set of roles is known in advance, and it learns the roles at the same time as clustering verbs into frame-like schemas. The resulting sets of entities (such as {police, agent, authorities, government} or {court, judge, justice}) can be viewed as a kind of schema-specific semantic role. How can this unsupervised method of learning roles be evaluated? In Section 6 we evaluate the schemas together with their arguments in a cloze task. In this section we perform a more qualitative evalation by comparing our schema to FrameNet. FrameNet (Baker et al., 1998) is a database of frames, structures that characterize particular situations. A frame consists of a set of events (the verbs and nouns that describe them) and a set of frame-specific semantic roles called frame elements that can be arguments of the lexical units in the frame. FrameNet frames share commonalities with narrative schemas; both represent aspects of situations in the world, and both link semantically related words into frame-like sets in which each predicate draws its argument roles from a frame-specific set. They differ in that schemas focus on events in a narrative, while frames f</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Christian Boitet and Pete Whitelock, editors, ACL-98, pages 86–90, San Francisco, California. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>Proc. of HLT/NAACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="3586" citStr="Bean and Riloff, 2004" startWordPosition="539" endWordPosition="542">t the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles. A search B A arrest B B plead C D acquit B D convict B 602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602–610, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Background This paper addresses two areas of work in event semantics, narrative event chains and semantic role labeling. We begin</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>David Bean and Ellen Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. Proc. of HLT/NAACL, pages 297– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08,</booktitle>
<location>Hawaii, USA.</location>
<contexts>
<context position="3147" citStr="Chambers and Jurafsky, 2008" startWordPosition="465" endWordPosition="468">les of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following Narrative Schema, to be defined more formally later. The events on the left follow a set of participants through a series of connected events that constitute a narrative: Events Roles A = Police B = Suspect C = Plea D = Jury D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, whil</context>
<context position="4465" citStr="Chambers and Jurafsky, 2008" startWordPosition="686" endWordPosition="689">es. A search B A arrest B B plead C D acquit B D convict B 602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602–610, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Background This paper addresses two areas of work in event semantics, narrative event chains and semantic role labeling. We begin by highlighting areas in both that can mutually inform each other through a narrative schema model. 2.1 Narrative Event Chains Narrative Event Chains are partially ordered sets of events that all involve the same shared participant, the protagonist (Chambers and Jurafsky, 2008). A chain contains a set of verbs representing events, and for each verb, the grammatical role filled by the shared protagonist. An event is a verb together with its constellation of arguments. An event slot is a tuple of an event and a particular argument slot (grammatical relation), represented as a pair (v, d) where v is a verb and d E {subject, object, prep}. A chain is a tuple (L, O) where L is a set of event slots and O is a partial (temporal) ordering. We will write event slots in shorthand as (X pleads) or (pleads X) for (pleads, subject) and (pleads, object). Below is an example chain</context>
<context position="6732" citStr="Chambers and Jurafsky (2008)" startWordPosition="1057" endWordPosition="1060">previous work, however, has two major limitations. First, the model did not express any information about the protagonist, such as its type or role. Role information (such as knowing whether a filler is a location, a person, a particular class of people, or even an inanimate object) could crucially inform learning and inference. Second, the model only represents one participant (the protagonist). Representing the other entities involved in all event slots in the narrative could potentially provide valuable information. We discuss both of these extensions next. 2.1.1 The Case for Arguments The Chambers and Jurafsky (2008) narrative chains do not specify what type of argument fills the role of protagonist. Chain learning and clustering is based only on the frequency with which two verbs share arguments, ignoring any features of the arguments themselves. Take this example of an actual chain from an article in our training data. Given this chain of five events, we want to choose other events most likely to occur in this scenario. One of the top scoring event slots is (fly X). Narrative chains incorrectly favor (fly X) because it is observed during training with all five event slots, although not frequently with a</context>
<context position="10437" citStr="Chambers and Jurafsky (2008)" startWordPosition="1673" endWordPosition="1677">WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just described, Chambers and Jurafsky (2008) offers an unsupervised approach to event learning (goal 1), but lacks semantic role knowledge (goal 2). The following sections describe a model that addresses both goals. 3 Narrative Schemas The next sections introduce typed narrative chains and chain merging, extensions that allow us to jointly learn argument roles with event structure. 3.1 Typed Narrative Chains The first step in describing a narrative schema is to extend the definition of a narrative chain to include argument types. We now constrain the protagonist to be of a certain type or role. A Typed Narrative Chain is a partially ord</context>
<context position="13907" citStr="Chambers and Jurafsky, 2008" startWordPosition="2261" endWordPosition="2264">r of event slots share arguments from this set, we count workers. In this example, the pair (X find) and (X apply) shares an argument (they and workers). The pair ((X find),(X apply)) is counted once for narrative chain induction, and ((X find), (X apply), workers) once for argument induction. Figure 1 shows the top occurring words across all event slot pairs in a criminal scenario chain. This chain will be part of a larger narrative schema, described in section 3.4. 3.3 Event Slot Similarity with Arguments We now formalize event slot similarity with arguments. Narrative chains as defined in (Chambers and Jurafsky, 2008) score a new event slot (f, g) against a chain of size n by summing over the scores between all pairs: chainsim(C, (f, g)) _ �n sim((ei, di) , (f, g)) (1) i=1 where C is a narrative chain, f is a verb with grammatical argument g, and sim(e, e&apos;) is the pointwise mutual information pmi(e, e&apos;). Growing a chain by one adds the highest scoring event. We extend this function to include argument types by defining similarity in the context of a specific argument a: sim((e, d) , (e0, d0) , a) _ pmi((e, d) , (e0, d0)) + A log freq((e, d) , (e0, d0) , a) (2) where A is a constant weighting factor and fre</context>
<context position="18320" citStr="Chambers and Jurafsky (2008)" startWordPosition="3076" endWordPosition="3080">chema N, not just one chain. Instead of asking which event slot (v, d) is a best fit, we ask if v is best by considering all slots at once: narsim(N, v) _ chainsim&apos;(c, (v, d))) (5) where CN is the set of chains in our narrative N. If (v, d) does not have strong enough similarity with any chain, it creates a new one with base score Q. The Q parameter balances this decision of adding to an existing chain in N or creating a new one. 3.4.3 Building Schemas We use equation 5 to build schemas from the set of events as opposed to the set of event slots that previous work on narrative chains used. In Chambers and Jurafsky (2008), narrative chains add the best (e, d) based on the following: chainsim(c, (vj, gj)) (6) where m is the number of seen event slots in the corpus and (vj, gj) is the jth such possible event slot. Schemas are now learned by adding events that maximize equation 5: narsim(N, vj) (7) where |v |is the number of observed verbs and vj is the jth such verb. Verbs are incrementally added to a narrative schema by strength of similarity. defendant, nichols, smith, simpson police, agent, authorities, government license immigrant, reporter, cavalo, migrant, alien Figure 3: Graphical view of an unordered sch</context>
<context position="28270" citStr="Chambers and Jurafsky, 2008" startWordPosition="4777" endWordPosition="4780">uments learned from instruction lists (wash, heat, boil). Two schemas require situations as arguments, but the dependency graphs chose as arguments the subjects of the embedded clauses, resulting in 20 incorrect arguments in these schema. 6 Evaluation: Cloze The previous section compared our learned knowledge to current work in event and role semantics. We now provide a more formal evaluation against untyped narrative chains. The two main contributions of schema are (1) adding typed arguments and (2) considering joint chains in one model. We evaluate each using the narrative cloze test as in (Chambers and Jurafsky, 2008). 6.1 Narrative Cloze The cloze task (Taylor, 1953) evaluates human understanding of lexical units by removing a random word from a sentence and asking the subject to guess what is missing. The narrative cloze is a variation on this idea that removes an event slot from a known narrative chain.Performance is measured by the position of the missing event slot in a system’s ranked guess list. This task is particularly attractive for narrative schemas (and chains) because it aligns with one of the original ideas behind Schankian scripts, namely that scripts help humans ‘fill in the blanks’ when la</context>
<context position="29560" citStr="Chambers and Jurafsky, 2008" startWordPosition="4988" endWordPosition="4992">verb pairs and shared arguments over the NYT portion of the Gigaword Corpus (years 1994-2004), approximately one million articles. 608 Narrative Cloze Test Training Data from 1994−X Figure 6: Results with varying sizes of training data. We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006), recording all verbs with subject, object, or prepositional typed dependencies. Unlike in (Chambers and Jurafsky, 2008), we lemmatize verbs and argument head words. We use the OpenNLP1 coreference engine to resolve entity mentions. The test set is the same as in (Chambers and Jurafsky, 2008). 100 random news articles were selected from the 2001 NYT section of the Gigaword Corpus. Articles that did not contain a protagonist with five or more events were ignored, leaving a test set of 69 articles. We used a smaller development set of size 17 to tune parameters. 6.3 Typed Chains The first evaluation compares untyped against typed narrative event chains. The typed model uses equation 4 for chain clustering. The dotted line ‘Chain’ and solid ‘Typed Chain’ in figure 6 shows the average ranked position over the test set. The untyped chains plateau and begin to worsen as the amount of tr</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of ACL-08, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC-06, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium.</title>
<date>2002</date>
<contexts>
<context position="19182" citStr="Graff, 2002" startWordPosition="3219" endWordPosition="3220">equation 5: narsim(N, vj) (7) where |v |is the number of observed verbs and vj is the jth such verb. Verbs are incrementally added to a narrative schema by strength of similarity. defendant, nichols, smith, simpson police, agent, authorities, government license immigrant, reporter, cavalo, migrant, alien Figure 3: Graphical view of an unordered schema automatically built starting from the verb ‘arrest’. A Q value that encouraged splitting was used. 4 Sample Narrative Schemas Figures 3 and 4 show two criminal schemas learned completely automatically from the NYT portion of the Gigaword Corpus (Graff, 2002). We parse the text into dependency graphs and resolve coreferences. The figures result from learning over the event slot counts. In addition, figure 5 shows six of the top 20 scoring narrative schemas learned by our system. We artificially required the clustering procedure to stop (and sometimes continue) at six events per schema. Six was chosen as the size to enable us to compare to FrameNet in the next section; the mean number of verbs in FrameNet frames is between five and six. A low Q was chosen to limit chain splitting. We built a new schema starting from each verb that occurs in more th</context>
</contexts>
<marker>Graff, 2002</marker>
<rawString>David Graff. 2002. English Gigaword. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Green</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Frame semantic enhancement of lexical-semantic resources.</title>
<date>2005</date>
<booktitle>In ACL-SIGLEX Workshop on Deep Lexical Acquisition,</booktitle>
<pages>57--66</pages>
<contexts>
<context position="9804" citStr="Green and Dorr (2005)" startWordPosition="1574" endWordPosition="1577">meNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just descri</context>
</contexts>
<marker>Green, Dorr, 2005</marker>
<rawString>Rebecca Green and Bonnie J. Dorr. 2005. Frame semantic enhancement of lexical-semantic resources. In ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 57–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Unsupervised discovery of a statistical verb lexicon.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2299" citStr="Grenager and Manning, 2006" startWordPosition="327" endWordPosition="330">robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following Narrative Schema, to be defined more formally later. The events on the left follow a set of participants through a series of connected events that constitute a narrative: Events Roles A = Police B = Suspect C = Plea D = Jury D sentence B Being able to robustly learn sets </context>
</contexts>
<marker>Grenager, Manning, 2006</marker>
<rawString>Trond Grenager and Christopher D. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shan He</author>
<author>Daniel Gildea</author>
</authors>
<title>Self-training and co-training for semantic role labeling: Primary report.</title>
<date>2006</date>
<tech>Technical Report 891,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="2385" citStr="He and Gildea, 2006" startWordPosition="341" endWordPosition="344"> on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following Narrative Schema, to be defined more formally later. The events on the left follow a set of participants through a series of connected events that constitute a narrative: Events Roles A = Police B = Suspect C = Plea D = Jury D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types </context>
<context position="9396" citStr="He and Gildea, 2006" startWordPosition="1503" endWordPosition="1506">t reasoning is absent from the narrative chain model. 2.2 Semantic Role Labeling The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untag</context>
</contexts>
<marker>He, Gildea, 2006</marker>
<rawString>Shan He and Daniel Gildea. 2006. Self-training and co-training for semantic role labeling: Primary report. Technical Report 891, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI-2000,</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="9242" citStr="Kipper et al., 2000" startWordPosition="1478" endWordPosition="1481">t from that of convict (judge). A more informed decision prefers capture because both the objects (suspect) and subjects (police) are identical. This joint reasoning is absent from the narrative chain model. 2.2 Semantic Role Labeling The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to clus</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of AAAI-2000, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1973" citStr="Palmer et al., 2005" startWordPosition="278" endWordPosition="281">heir participants) and frames to drive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined fra</context>
<context position="9178" citStr="Palmer et al., 2005" startWordPosition="1467" endWordPosition="1470">ditional information; the subject of arrest (police) is different from that of convict (judge). A more informed decision prefers capture because both the objects (suspect) and subjects (police) are identical. This joint reasoning is absent from the narrative chain model. 2.2 Semantic Role Labeling The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic mo</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Document clustering with committees.</title>
<date>2002</date>
<booktitle>In ACM Conference on Research and Development in Information Retrieval,</booktitle>
<pages>199--206</pages>
<location>Tampere, Finland.</location>
<contexts>
<context position="11476" citStr="Pantel and Lin, 2002" startWordPosition="1852" endWordPosition="1855"> to extend the definition of a narrative chain to include argument types. We now constrain the protagonist to be of a certain type or role. A Typed Narrative Chain is a partially ordered set of event slots that share an argument, but now the shared argument is a role defined by being a member of a set of types R. These types can be lexical units (such as observed head words), noun clusters, or other semantic representations. We use head words in the examples below, but we also evaluate with argument clustering by mapping head words to member clusters created with the CBC clustering algorithm (Pantel and Lin, 2002). We define a typed narrative chain as a tuple (L, P, O) with L and O the set of event slots and partial ordering as before. Let P be a set of argument types (head words) representing a single role. An example is given here: L = {(hunt X), (X use), (suspect X), (accuse X), (search X)} P = {person, government, company, criminal, ...} O = {(use, hunt), (suspect, search), (suspect, accuse) ... } 3.2 Learning Argument Types As mentioned above, narrative chains are learned by parsing the text, resolving coreference, and extracting chains of events that share participants. In our new model, argument</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Document clustering with committees. In ACM Conference on Research and Development in Information Retrieval, pages 199–206, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, plans, goals and understanding. Lawrence Erlbaum.</title>
<date>1977</date>
<contexts>
<context position="1280" citStr="Schank and Abelson, 1977" startWordPosition="173" endWordPosition="176">events or roles. Our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles. By jointly addressing both tasks, we improve on previous results in narrative/frame learning and induce rich frame-specific semantic roles. 1 Introduction This paper describes a new approach to event semantics that jointly learns event relations and their participants from unlabeled corpora. The early years of natural language processing (NLP) took a “top-down” approach to language understanding, using representations like scripts (Schank and Abelson, 1977) (structured representations of events, their causal relationships, and their participants) and frames to drive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, plans, goals and understanding. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert S Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2363" citStr="Swier and Stevenson, 2004" startWordPosition="337" endWordPosition="340">n understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to be learned. Even unsupervised attempts to learn semantic roles have required a pre-defined set of roles (Grenager and Manning, 2006) and often a hand-labeled seed corpus (Swier and Stevenson, 2004; He and Gildea, 2006). In this paper, we describe our attempts to learn script-like information about the world, including both event structures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following Narrative Schema, to be defined more formally later. The events on the left follow a set of participants through a series of connected events that constitute a narrative: Events Roles A = Police B = Suspect C = Plea D = Jury D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information abo</context>
<context position="9374" citStr="Swier and Stevenson, 2004" startWordPosition="1499" endWordPosition="1502">e) are identical. This joint reasoning is absent from the narrative chain model. 2.2 Semantic Role Labeling The task of semantic role learning and labeling is to identify classes of entities that fill predicate slots; semantic roles seem like they’d be a good model for the kind of argument types we’d like to learn for narratives. Most work on semantic role labeling, however, is supervised, using Propbank (Palmer et al., 2005), FrameNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrativ</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>Robert S. Swier and Suzanne Stevenson. 2004. Unsupervised semantic role labelling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson L Taylor</author>
</authors>
<title>Cloze procedure: a new tool for measuring readability.</title>
<date>1953</date>
<journal>Journalism Quarterly,</journal>
<pages>30--415</pages>
<contexts>
<context position="28321" citStr="Taylor, 1953" startWordPosition="4787" endWordPosition="4788">mas require situations as arguments, but the dependency graphs chose as arguments the subjects of the embedded clauses, resulting in 20 incorrect arguments in these schema. 6 Evaluation: Cloze The previous section compared our learned knowledge to current work in event and role semantics. We now provide a more formal evaluation against untyped narrative chains. The two main contributions of schema are (1) adding typed arguments and (2) considering joint chains in one model. We evaluate each using the narrative cloze test as in (Chambers and Jurafsky, 2008). 6.1 Narrative Cloze The cloze task (Taylor, 1953) evaluates human understanding of lexical units by removing a random word from a sentence and asking the subject to guess what is missing. The narrative cloze is a variation on this idea that removes an event slot from a known narrative chain.Performance is measured by the position of the missing event slot in a system’s ranked guess list. This task is particularly attractive for narrative schemas (and chains) because it aligns with one of the original ideas behind Schankian scripts, namely that scripts help humans ‘fill in the blanks’ when language is underspecified. 6.2 Training and Test Dat</context>
</contexts>
<marker>Taylor, 1953</marker>
<rawString>Wilson L. Taylor. 1953. Cloze procedure: a new tool for measuring readability. Journalism Quarterly, 30:415–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>