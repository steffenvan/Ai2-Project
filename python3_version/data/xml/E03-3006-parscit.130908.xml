<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.984497">
Reversing Controlled Document Authoring to Normalize Documents
</title>
<author confidence="0.886094">
Aurelien Max
</author>
<affiliation confidence="0.652374">
Groupe d&apos;Etude pour la Traduction Automatique (GETA)
Xerox Research Centre Europe (XRCE)
Grenoble, France
</affiliation>
<email confidence="0.991913">
aurelien.max@imag.fr
</email>
<sectionHeader confidence="0.995508" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999609">
This paper introduces document nor-
malization, and addresses the issue of
whether controlled document authoring
systems can be used in a reverse mode
to normalize legacy documents. A
paradigm for deep content analysis us-
ing such a system is proposed, and an
architecture for a document normaliza-
tion system is described.
</bodyText>
<sectionHeader confidence="0.99833" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997532608695652">
Controlled Document Authoring is a field of re-
search in NLP that is concerned with the interac-
tive production of documents in limited domains.
The aim of systems implementing controlled doc-
ument authoring is to allow the user to specify an
underlying semantic representation of the docu-
ment that is well-formed and complete relative to
its class of documents. This representation is then
used to produce a fully controlled version of the
document, possibly in several languages. We dis-
tinguish controlled document authoring systems
from what is referred to in (Reiter and Dale, 2000)
as computer as authoring aid, which are Natu-
ral Language Generation systems intended to pro-
duce initial drafts or only routine factual sections
of documents, in that the former can be used to
produce high-quality final versions of documents
without the need for further hand-editing.
The question which motivated our work was
the following: can we reuse the resources of an
existing controlled document authoring system to
analyze documents from the same class of docu-
ments? If so, we could obtain the semantic struc-
ture corresponding to a raw document, and then
produce from it a completely controlled version.
If the raw document is bigger in scope from the
documents that the authoring system models, then
something similar to document summarization by
content recognition and reformulation would be
done. Incomplete representations after automatic
analysis could be interactively completed, thus re-
entering controlled document authoring. Produc-
ing the document from the semantic representa-
tion in several languages would do some kind of
normalizing translation of the original document
(Max, 2003). We call the process of reconstructing
such a semantic representation (and re-generating
controlled text in the same language), which is
common to all the above cases, document normal-
ization.
In this paper, we will first attempt to argue why
document normalization could be of some use in
the real world. We will then introduce our ap-
proach to document normalization, and describe
a possible implementation. We will conclude by
introducing our future work.
</bodyText>
<sectionHeader confidence="0.771961" genericHeader="introduction">
2 Why normalize documents?
</sectionHeader>
<bodyText confidence="0.999946142857143">
Text normalization often refers to techniques used
to disambiguate text to facilitate its analysis
(Mikheev, 2000). The definition for document
normalization that we propose can have much
more impact on the surface form of documents.
In order to propose application domains for doc-
ument normalization, we attempted to identify do-
</bodyText>
<page confidence="0.998253">
33
</page>
<bodyText confidence="0.999576535714286">
mains where documents of the same nature but
from different origins where compiled into homo-
geneous collections. We focussed our attention on
the pharmaceutical domain, which produces sev-
eral yearly compendiums of drug leaflets, as for
example the French Vidal de la Famille (OVP Edi-
tions du VIDAL, 1998).
Producing pharmaceutical documents is the
responsibility of the pharmaceutical companies
which market the drugs. A study we conducted on
a corpus of 50 patient pharmaceutical leaflets for
pain relievers (Max, 2002) collected on drug ven-
dor websites revealed several types of variations.
The first observation was that the structures of the
leaflets could vary considerably. For comparable
drugs, we found that for example warning-related
information could be presented in different ways.
One of them was to divide them into two sections,
Warnings and Side effects, another one had a three-
section division into Drug interaction precautions,
Warnings, and Alcohol warning. In the first case,
drug interaction precautions effectively appeared
in the more general Warnings section (You should
ask your doctor before taking aspirin if you are
taking medicines for...) Conversely, possible side
effects, which are in a separate section in the first
case, were found in the Warnings section in the
second case (If ringing in the ears or a loss of
hearing occurs...) A related type of variation con-
cerns the focus which is given to certain types of
content. A warning specific to alcohol is needed
for patient taking aspirin as alcohol consumption
may cause stomach bleeding in this circumstance.
While some leaflets presented a separate section,
Alcohol warnings, others simply mentioned the re-
lated possible side effect, stomach bleeding, in the
appropriate section.
In spite of these differences in structure, leaflets
in the subset we have studied usually express
the same types of content, that is, the commu-
nicative intentions expressed by the authors of
the leaflets are similar. However, this content
can be expressed in a variety of ways. A fac-
tor analysis of stylistic variation in a corpus of
342 patient leaflets (Paiva, 2000) revealed that two
important factors opposed abstraction (e.g. use
of agentless passives and nominalizations) to in-
volvement/directness (e.g. use of 1st and 2nd
persons and imperatives) and full reference to
pronominalized reference.
Our study also showed that similar communica-
tive intentions could be expressed in a variety of
ways conveying more or less subtle semantic dis-
tinctions. We argue that for documents of such
an important nature, consistency of expression and
of information presentation can not only be bene-
ficial to the reader but also necessary to allow a
clear and unambiguous understanding of the com-
municative intentions contained in different doc-
uments. Controlled document authoring systems
can guarantee that the documents they produce are
consistent as the production of the text is under
the control of the system. An authoring system
for drug leaflets conforming to Le Vidal specifi-
cations has been developed (Brun et al., 2000),
showing that new documents could be written in
a fully controlled way. But most existing docu-
ments, if they conform to some specifications, do
not have these desirable properties across differ-
ent drug vendors. Our research thus addresses this
complementary issue: can we reuse the modelling
of documents of such systems to analyze existing
legacy documents from the same class of docu-
ments?
Document normalization implies analyzing a
legacy document into a semantically well-formed
content representation, and producing a normal-
ized version from that content representation.
This expresses predefined communicative content
present in the input document, in a structurally and
linguistically controlled way. Predefined content
reveals communicative intentions, which should
ideally be described by an expert of the discourse
domain.
</bodyText>
<sectionHeader confidence="0.985434" genericHeader="method">
3 Controlled document authoring
</sectionHeader>
<bodyText confidence="0.9997509">
There has been a recent trend to investigate con-
trolled document authoring, e.g. (Power and Scott,
1998; Dymetman et al., 2000), where the focus
is on obtaining document content representations
by interaction with the user/author and producing
multilingual versions of the final document from
them. Typically, the user of these systems has
to select possible semantic choices in active fields
present in the evolving text of the document in the
user&apos;s language. These selections iteratively refine
</bodyText>
<page confidence="0.996039">
34
</page>
<figure confidence="0.985750214285714">
list0fRroductWarnings(AllergyWarning, DurationWarning)::productWarnings(TypeOfSymptom, ActiveIngredient)-e ---&gt;
[&apos;PRODUCT WARN NGS&apos;
AllergyWarning::allergyWarning(ActiveIngredient)-e,
DurationWarning::durationWarning(TypeOfSymptom)-e.
doNotTakeInCase0fAllergy(Ingredient)::allergyWarning(Ingredient)-e ---&gt;
[&apos;DO NOT TAKE THIS DRUG IF YOU ARE ALLERGIC TO
Ingredient::activeIngredient-e.
doNotTakeForMore(Number, TimeUnit, PWWarnIng)::durationWarning(TypeOfSymptom) ---&gt;
[&apos; This product should not be taken for more than
Number::integer-e, TimeUnit::timeUnit-e,
[&apos; without consulting a doctor.
PWWarning::persistOrWorsenWarning(TypeOfSymptom).
consultIfPainPerslstsOrGetsWorse::persistOrWorsenWarning(paln) ---&gt;
[&apos;Consult your doctor if pain persists or gets worse.&apos;].
</figure>
<figureCaption confidence="0.99997">
Figure 1: MDA grammar extract for the product warning section of a patient drug leaflet.
</figureCaption>
<bodyText confidence="0.998630516129032">
the document content until it is complete.
In the Multilingual Document Authoring
(MDA) system (Dymetman et al., 2000), the
specification of well-formed document content
representations can be recursively described in a
grammar formalism that is a variant of Definite
Clause Grammars (Pereira and Warren, 1980).
Figure 1 shows a simple MDA grammar extract
for the product warning section of a patient leaflet.
The first rule reads as follows: the semantic struc-
ture list0fProductWarning(AllergyWarning,
DurationWarning) is of type productWarn-
ings(TypeOfSymptom, ActiveIngredient), and
is made up of the terminal string &amp;quot;PRODUCT
WARNINGS&amp;quot;, an element of type allergyWarn-
ing(Activelngredient) element, and an element
of type durationWarning(TypeOfSymptom).
Semantic constraints are established through
the use of shared type-parameters: for example,
TypeOf Symptom constrains the element of type
durationWarning. Text strings can appear in
right-hand sides of rules, which allows to asso-
ciate text realizations to content representations by
a traversing of the leaves of their tree.1 The gran-
ularity of text fragments that is allowed in rules
is not necessarily as fine-grained as predicate-
argument structures of sentences commonly used
in NLG. This approach proved to be adequate
for classes of documents where certain choices
could be rendered as entire text passages (e.g.
pregnancy warnings, disclaimers, etc.) and where
</bodyText>
<footnote confidence="0.6842185">
1Some details such as morphological-level constraints
have been omitted for lack of space.
</footnote>
<bodyText confidence="0.99950175">
a more fine-grained representation would not be
needed, thus offering an interesting intermediate
level between full NLG and templates (Reiter,
1995).
</bodyText>
<sectionHeader confidence="0.953473" genericHeader="method">
4 A paradigm for deep content analysis
</sectionHeader>
<subsectionHeader confidence="0.99268">
4.1 Fuzzy inverted generation
</subsectionHeader>
<bodyText confidence="0.999913884615385">
Content analysis is often viewed as a parsing pro-
cess where semantic interpretation is derived from
syntactic structures (Allen, 1995). In practice,
however, building broad-coverage syntactically-
driven parsing grammars that are robust to the
variation in the input is a very difficult task. Fur-
thermore, we have already argued that for the pur-
pose of document normalization we would like to
match texts that do not carry significant commu-
nicative differences in a given class of documents
but may be of quite different surface forms. There-
fore, we propose to concentrate on what counts
as a well-formed document semantic representa-
tion rather than on surface properties of text, as the
space of possible content representations is vastly
more restricted than the space of possible texts.
Bridging the gap between deep content and sur-
face text can be done by using the textual pre-
dictions made by the generator of an MDA sys-
tem from well-formed content representations. In-
deed, an MDA system can be used as a device
for enumerating well-formed document represen-
tations in a constrained domain and associating
texts with them. If we can compute a relevant
measure of semantic similarity between the text
produced for any document content representation
</bodyText>
<page confidence="0.995945">
35
</page>
<bodyText confidence="0.999981615384615">
and the text of a legacy document, we could possi-
bly consider the representations with the best sim-
ilarity scores as those best corresponding to the
legacy document under analysis. As this kind of
analysis uses predictions made by a natural lan-
guage generator, we named it inverted generation
(Max and Dymetman, 2002). And because a gen-
erator will seriously undergenerate with respect to
all the texts that could be normalized to the same
communicative intention, we made this process
fuzzy by matching documents at a more abstract
level than on raw text to evaluate commonality of
communicative content.
</bodyText>
<subsectionHeader confidence="0.979437">
4.2 Implementing fuzzy inverted generation
using MDA
</subsectionHeader>
<bodyText confidence="0.999984518518519">
We use the formalism of the MDA authoring sys-
tem (Dymetman et al., 2000) to implement fuzzy
inverted generation, as it offers a close coupling
between semantic modelling and text generation.2
In this context, an input document will be used as
an information source to reconstruct the semantic
choices that a human author would have made if
she had created the document most similar to the
input document in terms of communicative con-
tent using MDA. The space of virtual documents3
for a given class of documents being potentially
huge, we will want to implement a heuristic search
procedure to find the best candidates. The confi-
dence in the analysis will depend on the quality of
the match and the similarity measure used, which
suggests that in practice such a normalization task
could hardly be done without at least some inter-
vention from a human expert.
The search for candidate content representa-
tions begins under the assumption that the input
document belongs to the class of documents mod-
eled by the MDA grammar used. Starting from
the root type of the MDA grammar, partial content
representations are iteratively produced by per-
forming steps of derivation on the typed abstract
trees. This corresponds to instantiating a variable
with a value compatible with its type (which is
</bodyText>
<footnote confidence="0.993636666666667">
2MDA grammars being Prolog programs, any Prolog
predicate could be called from the rules. We ignored this
powerful feature of MDA grammars and thus used a simpli-
fied formalism.
3We call virtual documents documents that can be pre-
dicted by the semantic model but do not exist a priori.
</footnote>
<bodyText confidence="0.999942371428571">
what is done interactively in the authoring mode).
A similarity measure is computed between the in-
put document and the set of all the virtual docu-
ments that could be produced from a given partial
content representation. This similarity measure is
used as the evaluation function of an admissible
heuristic search (Nilsson, 1998) that returns the
candidate content representations in decreasing or-
der of similarity with the input document. In order
to guarantee that the search is admissible, it has
to implement a best-first strategy, and use an opti-
mistic evaluation function that decreases as search
progresses and that is an overestimate of the simi-
larity between the best attainable virtual document
and the input document.
In order to allow the computation of the simi-
larity function between a partial content represen-
tation (a node in our search space) and an input
document, some account of the properties of at-
tainable virtual documents has to be percolated to
the semantic types in the grammar. We call pro-
file a representation of a text document that can
be used to measure some semantic content sim-
ilarity. A profile must have the property that it
can be computed for text strings appearing in rules
of the MDA grammar and percolated to semantic
types in the grammar up to the root type. A profile
for a type gives an account of the profiles of all
the terminals attainable from it, in such a way that
the similarity function used will overestimate the
value of the similarity between the best attainable
virtual document and the input document. We will
show in the next section how this can be realized
in a practical normalization system using an MDA
grammar.
</bodyText>
<sectionHeader confidence="0.9615455" genericHeader="method">
5 A possible implementation of a
document normalization system
</sectionHeader>
<subsectionHeader confidence="0.99949">
5.1 System architecture
</subsectionHeader>
<bodyText confidence="0.99998025">
In this section we describe the architecture of
the document normalization system that we have
started to develop. An MDA grammar is first
compiled to associate profiles with all its seman-
tic types. This compiled version of the grammar
is used in conjunction with the profile computed
for the input document in a first pass analysis.
The aim of this first pass analysis, implementing
</bodyText>
<page confidence="0.991641">
36
</page>
<bodyText confidence="0.9998885">
fuzzy inverted generation, is to isolate a limited
set of candidate content representations. A sec-
ond pass analysis is then applied on those candi-
dates, which are then actual texts associated with
their content representation. Ultimately, interac-
tive disambiguation takes place to select the best
candidate among those that could not be filtered
out automatically.
</bodyText>
<subsectionHeader confidence="0.998599">
5.2 Profile construction
</subsectionHeader>
<bodyText confidence="0.99091">
Profile definition Profiles give an account of
text content and are compared to evaluate content
similarity. We defined our notion of content simi-
larity from the fact, broadly accepted in the infor-
mation retrieval community, that the more terms
(and related terms) are shared by two texts, the
more likely they are to be about the same topic.
Text content can be roughly approximated by a
vector containing all lemmatized forms of words
and their associated number of occurrences. We
call such a vector the lexical profile of a text. It has
been shown that using sets of synonyms instead
of word forms could improve similarity measures
(Gonzalo et al., 1998), so we use synset profiles to
account for lexico- semantic variation.
Text profile construction Words in text frag-
ments are first lemmatized and their part-of-
speech is disambiguated using the morphologi-
cal analysis tools of XRCE. Their correspond-
ing set of synonyms is then looked up through a
lexico-semantic interface, and the corresponding
synset key is used to index the word or expres-
sion. We have developed an annotation graph-
ical interface that allows a human to annotate
strings in MDA grammars by choosing the ap-
propriate synset in the default lexico-semantic re-
source, WordNet (Miller et al., 1993), or to define
new sets of synonyms in the absence of availability
of a more specific resource. The annotation inter-
face also allows the annotator to specify a value
of infonnativity for the indexed synsets4, which is
taken into account when computing profile simi-
larity. The set of synsets which have been used to
4We thought that the kind of informativity for words that
was needed required some expertise on the class of docu-
ments, and was therefore not easily derivable from corpus
statistics. We nevertheless intend to evaluate informativity
measures derived from term frequencies.
index the text fragments found in the MDA gram-
mar is then used as a target set when building the
profile for an input document.
Profile similarity computation We want to
evaluate how much content is common to an in-
put document and a set of virtual documents, but
for our purpose we do not want this measure to be
penalized by unshared content. Furthermore, we
want to use this measure as the evaluation function
of our search procedure, so it has to be optimistic
when applied to partial representations. Thus we
chose a simple intersection measure between two
lexical profiles, weighted by the informativity of
the synsets involved. This measure is given by
the following formula, where oecspi(item) is the
number of occurrences of item in profile P1, and
inf (item) is its informativity:
</bodyText>
<equation confidence="0.998149333333333">
sim(P1, P2) = Emin(occsp, (item),
itemEP1,P2
OCC8p2(ite717,)) * in! (item)
</equation>
<subsectionHeader confidence="0.961359">
5.3 Grammar precompilation
</subsectionHeader>
<bodyText confidence="0.999980833333333">
A given semantic type can have several realiza-
tions, which correspond to a collection of virtual
texts. The synset profile of a type has to give an
account of the maximum number of occurrences
of elements from a synset that can be obtained by
deriving this type in any possible way. The synset
profile for an expansion of a type (a right-hand
side of a rule) can be obtained by taking the bag-
union (which sums the number of occurrences for
each element in the profiles) of the synset profiles
of all the elements in the expansion. Obtaining the
profile for a type can then be done by taking the
maximum of the profiles of all its expansions. We
call this operation, which takes for each element
its maximum number of occurrences in the expan-
sions of the type, the union-max of the profiles of
all the expansions for a type. This reflects the fact
that, whatever the derivation that is made from a
type, elements from a given synset cannot appear
in a text produced from that derivation more than
a given number of times.
The grammar precompilation algorithm shown
on figure 2 uses a fixpoint approach. At each it-
eration, the profiles for all the semantic types are
</bodyText>
<page confidence="0.998288">
37
</page>
<table confidence="0.923186538461538">
currentIteration &lt;- 0
maximumNumber0fIterations &lt;- number of semantic types in the grammar
thereWasAnUpdate &lt;- true
create an empty profile for every semantic type
REPEAT WHILE therewasAnUpdate is true AND currentIteration &lt;= maxNumber0fIterations
FOR ALL semantic types in the grammar
FOR ALL their expansions
build the profile for that expansion given the current profiles
set the profile for that type to be the union-max of itself and the profile for the expansion
IF (currentIteration = maxNumber0fIterations)
set all changing numbers of occurrences for elements in the profiles to an infinity value
update thereWasAnUpdate appropriately
currentIteration &lt;- currentIteration + 1
</table>
<figureCaption confidence="0.972766">
Figure 2: Algorithm for percolating profiles in the grammar
</figureCaption>
<bodyText confidence="0.999866272727273">
built, given the current values of the profiles in-
volved in their construction. If no profile update
has been done during an iteration, then a fixpoint
has been reached and all the synset elements have
been percolated up to the root semantic type. If
updates are still made after a certain number of
iterations, which corresponds to the number of se-
mantic types in the grammar, that is, the depth of
the longest derivation without repetition, then the
corresponding updated values will tend to infinity
(this corresponds to the case of recursive types).
</bodyText>
<subsectionHeader confidence="0.969381">
5.4 Automatic selection of candidates
</subsectionHeader>
<bodyText confidence="0.9870210625">
A first pass analysis implements fuzzy inverted
generation. The most promising candidate content
representations are expanded first. Their profile
is the bag-union of the profiles for the types of
all their uninstantiated variables (the unspecified
parts) and the profiles for their text fragments (the
known parts). The intersection similarity measure
can only decrease or remain constant as a partial
content representation is further refined, thus sat-
isfying the constraint for the admissibility of the
search. The search terminates when a given num-
ber of complete candidates have been found.5
This first pass restricts the search space from
a huge collection of virtual documents to a com-
paratively smaller number of concrete textual doc-
uments, associated with their semantic structure.
Candidates differ in at least one semantic choice,
so the various alternatives can be rescored lo-
5 This number has to be determined empirically for a given
source of documents so that it guarantees that the correct can-
didate is retained.
cally using more fine-grained measures. An ap-
proach can be to search for evidence of the pres-
ence of some text passages produced by compet-
ing semantic choices in the input document, and to
rescore them appropriately. Given the constraints
on the domain of the input documents, we hope
that simple features will help significantly in dis-
ambiguating candidates, as for example distance
constraints which have been shown to participate
significantly in the evaluation of text similarity
over short passages (Hatzivassiloglou et al., 1999).
</bodyText>
<subsectionHeader confidence="0.843339">
5.5 Interactive disambiguation
</subsectionHeader>
<bodyText confidence="0.9999655">
Due to its limitations, the proposed approach
cannot guarantee that the correct candidate can
be selected automatically. Recognizing simi-
lar communicative intentions challenges simple
text matching techniques, and can require expert
knowledge that is difficult to obtain a priori and to
encode into automatic disambiguation rules. We
therefore propose that automatic selection of can-
didates be done down to a level of confidence that
would be determined so as to guarantee that the
correct document is retained. We then envisage
several modes of intervention from an expert. One
is to display the texts corresponding to possible
alternatives among which the expert could select
the correct one in the light of highlighted passages
of the document that obtained good scores during
the second pass analysis. Supervised learning of
new formulations could then be done by allowing
the expert to augment the generative power of the
MDA grammar used by adding alternative termi-
</bodyText>
<page confidence="0.996147">
38
</page>
<figure confidence="0.98772875">
1ist0fProcluctWarnings
doNotTakelfAllergy doNotTakeForMore
duration consultlfSymptomsPersistOrGetWorse
number days
</figure>
<figureCaption confidence="0.999942">
Figure 3: The abstract tree for the product warning section of the normalized document on figure 4
</figureCaption>
<bodyText confidence="0.96927775">
indications:
For the temporary relief of minor aches and pains associated .oith
headache, a cold, muscular aches, backache, toothache,
premenstrual and menstrual cramps and the minor pain of arthritis
</bodyText>
<sectionHeader confidence="0.470458" genericHeader="method">
Directions:
</sectionHeader>
<bodyText confidence="0.974126666666667">
Aduks and chidren 12 years Ct. age and older: 2 tablets ..Mth a full
glass of water every 6 hours Wnile symptoms persist, or as directed
by a doctor. Do not exceed 8 tablets in 24 hays.
</bodyText>
<table confidence="0.881506142857143">
Do not give to chidren under 12 years of age.
Ingredients:
Active Ingredients: Each tablet contains:: Aspirin (500 mg),
Caffeine (32 mg)
Inactive Ingredients: Hydroxypropyl Methylcellulose,
Microcrystalline Cellulose, Polyethylene Glycol, Sodium Lauryl
Sulfate, Starch
</table>
<subsectionHeader confidence="0.622279">
Drug Interaction Precautions:
</subsectionHeader>
<bodyText confidence="0.993446653846154">
Do not take this product if you are taking a prescription crug for
anticoagulation (thinning the blood), diabetes, gout or arthritis unless
directed by a doctor.
Warni-igs:
Children and teenagers should not
chicken pox or flu symptoms
Reye syndrome, a rare but seri
associated with asprni. Do not ta e this prodict if you have
asthma, an all gy.to aspirin, stomach problems (such as heartburn,
upset stomach, or stomach pain) that persist or recur, uters or
bleeding problems, or if ringing in the ears or a low of heering
occurs, unless directed by a doctor. Do not take this product for pain
for more than 10 days unless drected by a doctor. If pain plgtzgr
gets worse, if nem symptoms occa, or if redness ors.elling is
present ,sçft a docti because these could be sigrs of a serious
condition. As Mth ary drug. If you are pregnant or nu ng a baby,
seek the advice of a health professional before using th oduct. II
is especially important not to use asprin cluing the I
months of pregnancy unless specrically drected to do so by a
doctor because it may cause problems lithe trihom child or
complications dtrricp delivery. Keep this end all drugs out of the
reach of children. In case of accidental overdose, seek professional
assistance or contact a poison control center immediately.
Alcohol Warnrig: If .54:1u consume 3 or more alcoholic crinks every
day, ask you doctor Whether you should take aspirin or other pain
relievers or fever reducers. Aspirin may cause stomach bleeding.
</bodyText>
<sectionHeader confidence="0.954065" genericHeader="method">
INDICATIONS
</sectionHeader>
<bodyText confidence="0.85077175">
For the temporary relief of minor aches and pains, including:
- muscular aches
- headaches
-toothaches [SOME ELEMENTS OMITTED HERE]
</bodyText>
<sectionHeader confidence="0.878161" genericHeader="method">
ACTIVE INGREDIENTS
</sectionHeader>
<table confidence="0.910863266666667">
Each tablet contains:
- 500 mg of aspirin
- 32 mg of caffeine
DIRECTIONS
Children above 12 and adults: Take 2 tablets with a full glass of
water. This drug can betaken every 6 hours Mile symptoms persist.
Do not exceed 8 tablets in 24 hours.
Children under 12: CONSULT A DOCTOR BEFORE GIVING THIS
PRODUCT TO CHILDREN UNDER 12.
POSSIBLE SIDE-EFFECTS
Consult a doctor immediately if any of the follorying side effects
occur, as they could be signs of a serious condition:
- new symptoms
- ringing in the ears
.1.; [SOME ELEMENTS OMITTED HERE]
WARNINGS
Drug Ma-actions. A doctor shoul consulted before taking this
drug if you are already taking a prescri. on drug for at least one of
the following:
- anticoagulation
-diabetes [SOME ELEMENTS 0 TED HERE]
Product warnings. DONO T &apos;TAKE- T IS&apos;D RUG IF YOU ARE
ALLERGIC TO ASPIRIN. This product should not be taken for more
than 10 days .Aithout consulting a doctor. Consult yo doctor f pain
persists or gets worse.,
Particular conditions. A doctor should be consu •d before taking
this drug if rDu have any of the following condir • s
- asthma
777111377176731MI17- [SOME ELEMENTS OMITTED HERE]
Children and teenagers. Aspirin should be administered under
</table>
<figureCaption confidence="0.429704071428571">
medical advice only to children and teenagers with symptoms of a
virus infection, as it can increase the risks of a serious illness called
Reye&apos;s Syncrome.
Pregnancy. Pregnant vyomen should consult a doctor before taking
this drug. Using aspirin during the last 3 months of pregnancy may
cause problems to the unborn child or complications during delivery.
Alcohol. A doctor should be consulted about the risks associated
wtth alcohol consumption before taking this drug, because aspirin
may cause stomach bleeding.
is medicine for
doctor is consulted about
!bless reported to he
Figure 4: Anacin patient leaflet: on the left, the online version found on www.drugstore.com; on the
right, a normalized version
</figureCaption>
<page confidence="0.997251">
39
</page>
<bodyText confidence="0.992322428571429">
nal strings.6 Another mode would be to re-enter
the authoring mode of MDA to allow the expert
to finish the normalization manually, which would
be necessary in the case of incomplete input doc-
uments.
pervision of his PhD. This work is supported by a
grant from ANRT.
</bodyText>
<note confidence="0.28923">
References
James Allen. 1995. Natural Language Understanding. Benjamin/Cummings
Publishing, 2nd edition.
</note>
<subsectionHeader confidence="0.667843">
5.6 Normalization example
</subsectionHeader>
<bodyText confidence="0.989142875">
Figure 3 shows the abstract tree obtained after nor-
malization that corresponds to the product warn-
ings section of the normalized leaflet on figure 47
using a complete grammar that includes the extract
on figure 1. Text fragments used as evidence to
construct this abstract tree have been highlighted
on the input document and connected to their nor-
malized reformulations.
</bodyText>
<sectionHeader confidence="0.98699" genericHeader="discussions">
6 Discussion and future work
</sectionHeader>
<bodyText confidence="0.9968248">
Our research work has still a lot of questions to
address, some of which requiring a full implemen-
tation of our prototype system. First and fore-
most, the issue of what allows documents from a
given class to be normalized, and what the impli-
cations are, have to be more formally defined, be-
fore the issues of scalability and portability can be
addressed. Then the notion of level of confidence
for the automatic analysis has to be defined taking
as parameters the class of documents, the grammar
used, and the source of the input documents.
The human expert involved in the interactive
part guarantees the validity of the whole normal-
ization process. It is in fact an interesting charac-
teristic of our approach, as the result of a normal-
ization can be inspected by comparing two doc-
uments as those on figure 4. It is however very
important to minimize the time and efforts needed
from the expert, so to have the system perform
as much filtering as possible. To this end, the
reuse of the interactive disambiguation of difficult
cases through supervised learning seems particu-
larly important.
Acknowledgements The author wishes to thank
Marc Dymetman and Christian Boitet for their su-
</bodyText>
<footnote confidence="0.933692571428572">
6This non-determinism would simply be ignored in the
authoring mode, and alternative formulations present in
the semantic structure of a candidate content representation
would ultimately be changed to their normalized formulation.
7Our system being under development, the normalized
version of the leaflet shown has been disambiguated manu-
ally and is given for illustration purpose only.
</footnote>
<reference confidence="0.999547162790698">
Caroline Brun, Marc Dymetman, and Veronika Lux. 2000. Document Struc-
ture and Multilingual Authoring. In Proceedings of INLG 2000, Mitzpe
Ramon, Israel.
Marc Dymetman, Veronika Lux, and Aarne Ranta. 2000. XML and Multilin-
gual Document Authoring: Convergent Trends. In Proceedings of COL-
INC Gernhuty.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan Cigarran. 1998. Index-
ing with WordNet Synsets Can Improve Text Retrieval. In Proceedings of
the COLING/ACL Workshop on the Usage of WordNet in Natural Language
Processing Systems, Montreal, Canada.
Vasileios Hatzivassiloglou, Judith L. Klavans, and Eleazar Eskin. 1999. De-
tecting Text Similarity over Short Passages: Exploring Linguistic Feature
Combinations via Machine Learning. In Proceedings of EMNLP/VLC&amp;quot;99,
College Park, USA.
Aurdlien Max and Marc Dymetman. 2002. Document Content Analysis
through Inverted Generation. In Proceedings of the workshop on Using
(and Acquiring) Linguistic (and World) Knowledge for Information Access
of the AAAI Spring Symposium Series, Stanford University, USA.
Aurelien Max. 2002. Normalisation de Documents par Analyse du Contents
it l&apos;Aide dun Modele Semantique et dun Generateur. In Proceedings of
TAIN-RECITAL 2002, Nancy, France.
Aurolien Max. 2003. Multi-language Machine Translation through Docu-
ment Normalization. To appear in the proceedings of the EACI:03 EAMT
workshop, Budapest, Hungary.
Andrei Mikheev. 2000. Document centered approach to text normalization. In
Research and Development in Information Retrieval, pages 136-143.
G. Miller, R. Berckwith, C. Fellbaum, D. Gross, K. Miller, and R. Tengi. 1993.
Five papers on Won/Net. Princeton University Press.
Nils J. Nilsson. 1998. A rtificial Intelligence: a New Synthesis. Morgan Kauf-
mann.
OVP Editions du VIDAL, editor. 1998. Le VIDAL de la famille. Hachette,
Paris.
Daniel S. Paiva. 2000. Investing Style in a Corpus of Pharmaceutical Leaflets:
Result of a Factor Analysis. In Proceedings of the ACL Student Research
Workshop, Hong Kong.
Fernando Pereira and David Warren. 1980. Definite Clauses for Language
Analysis. Artificial Intelligence, 13.
Richard Power and Donia Scott. 1998. Multilingual Authoring using Feed-
back Texts. In Proceedings of COLING/ACL-98, Montreal, Canada.
Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation
Systems. Cambridge University Press.
Ehud Reiter. 1995. NLG Vs Templates. In Proceedings of ENLGW-95, Lei-
den, The Netherlands.
</reference>
<page confidence="0.998633">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.617003">
<title confidence="0.999864">Reversing Controlled Document Authoring to Normalize Documents</title>
<author confidence="0.988458">Aurelien Max</author>
<affiliation confidence="0.8020635">Groupe d&apos;Etude pour la Traduction Automatique (GETA) Xerox Research Centre Europe (XRCE)</affiliation>
<address confidence="0.985213">Grenoble, France</address>
<email confidence="0.996252">aurelien.max@imag.fr</email>
<abstract confidence="0.9949382">This paper introduces document normalization, and addresses the issue of whether controlled document authoring systems can be used in a reverse mode to normalize legacy documents. A paradigm for deep content analysis using such a system is proposed, and an architecture for a document normalization system is described.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
<author>Marc Dymetman</author>
<author>Veronika Lux</author>
</authors>
<title>Document Structure and Multilingual Authoring.</title>
<date>2000</date>
<booktitle>In Proceedings of INLG</booktitle>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="6139" citStr="Brun et al., 2000" startWordPosition="951" endWordPosition="954">nveying more or less subtle semantic distinctions. We argue that for documents of such an important nature, consistency of expression and of information presentation can not only be beneficial to the reader but also necessary to allow a clear and unambiguous understanding of the communicative intentions contained in different documents. Controlled document authoring systems can guarantee that the documents they produce are consistent as the production of the text is under the control of the system. An authoring system for drug leaflets conforming to Le Vidal specifications has been developed (Brun et al., 2000), showing that new documents could be written in a fully controlled way. But most existing documents, if they conform to some specifications, do not have these desirable properties across different drug vendors. Our research thus addresses this complementary issue: can we reuse the modelling of documents of such systems to analyze existing legacy documents from the same class of documents? Document normalization implies analyzing a legacy document into a semantically well-formed content representation, and producing a normalized version from that content representation. This expresses predefin</context>
</contexts>
<marker>Brun, Dymetman, Lux, 2000</marker>
<rawString>Caroline Brun, Marc Dymetman, and Veronika Lux. 2000. Document Structure and Multilingual Authoring. In Proceedings of INLG 2000, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
<author>Veronika Lux</author>
<author>Aarne Ranta</author>
</authors>
<title>XML and Multilingual Document Authoring: Convergent Trends.</title>
<date>2000</date>
<booktitle>In Proceedings of COLINC Gernhuty.</booktitle>
<contexts>
<context position="7132" citStr="Dymetman et al., 2000" startWordPosition="1096" endWordPosition="1099">e class of documents? Document normalization implies analyzing a legacy document into a semantically well-formed content representation, and producing a normalized version from that content representation. This expresses predefined communicative content present in the input document, in a structurally and linguistically controlled way. Predefined content reveals communicative intentions, which should ideally be described by an expert of the discourse domain. 3 Controlled document authoring There has been a recent trend to investigate controlled document authoring, e.g. (Power and Scott, 1998; Dymetman et al., 2000), where the focus is on obtaining document content representations by interaction with the user/author and producing multilingual versions of the final document from them. Typically, the user of these systems has to select possible semantic choices in active fields present in the evolving text of the document in the user&apos;s language. These selections iteratively refine 34 list0fRroductWarnings(AllergyWarning, DurationWarning)::productWarnings(TypeOfSymptom, ActiveIngredient)-e ---&gt; [&apos;PRODUCT WARN NGS&apos; AllergyWarning::allergyWarning(ActiveIngredient)-e, DurationWarning::durationWarning(TypeOfSym</context>
<context position="8480" citStr="Dymetman et al., 2000" startWordPosition="1240" endWordPosition="1243">O Ingredient::activeIngredient-e. doNotTakeForMore(Number, TimeUnit, PWWarnIng)::durationWarning(TypeOfSymptom) ---&gt; [&apos; This product should not be taken for more than Number::integer-e, TimeUnit::timeUnit-e, [&apos; without consulting a doctor. PWWarning::persistOrWorsenWarning(TypeOfSymptom). consultIfPainPerslstsOrGetsWorse::persistOrWorsenWarning(paln) ---&gt; [&apos;Consult your doctor if pain persists or gets worse.&apos;]. Figure 1: MDA grammar extract for the product warning section of a patient drug leaflet. the document content until it is complete. In the Multilingual Document Authoring (MDA) system (Dymetman et al., 2000), the specification of well-formed document content representations can be recursively described in a grammar formalism that is a variant of Definite Clause Grammars (Pereira and Warren, 1980). Figure 1 shows a simple MDA grammar extract for the product warning section of a patient leaflet. The first rule reads as follows: the semantic structure list0fProductWarning(AllergyWarning, DurationWarning) is of type productWarnings(TypeOfSymptom, ActiveIngredient), and is made up of the terminal string &amp;quot;PRODUCT WARNINGS&amp;quot;, an element of type allergyWarning(Activelngredient) element, and an element of </context>
<context position="12080" citStr="Dymetman et al., 2000" startWordPosition="1794" endWordPosition="1797">cores as those best corresponding to the legacy document under analysis. As this kind of analysis uses predictions made by a natural language generator, we named it inverted generation (Max and Dymetman, 2002). And because a generator will seriously undergenerate with respect to all the texts that could be normalized to the same communicative intention, we made this process fuzzy by matching documents at a more abstract level than on raw text to evaluate commonality of communicative content. 4.2 Implementing fuzzy inverted generation using MDA We use the formalism of the MDA authoring system (Dymetman et al., 2000) to implement fuzzy inverted generation, as it offers a close coupling between semantic modelling and text generation.2 In this context, an input document will be used as an information source to reconstruct the semantic choices that a human author would have made if she had created the document most similar to the input document in terms of communicative content using MDA. The space of virtual documents3 for a given class of documents being potentially huge, we will want to implement a heuristic search procedure to find the best candidates. The confidence in the analysis will depend on the qu</context>
</contexts>
<marker>Dymetman, Lux, Ranta, 2000</marker>
<rawString>Marc Dymetman, Veronika Lux, and Aarne Ranta. 2000. XML and Multilingual Document Authoring: Convergent Trends. In Proceedings of COLINC Gernhuty.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
<author>Irina Chugur</author>
<author>Juan Cigarran</author>
</authors>
<title>Indexing with WordNet Synsets Can Improve Text Retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING/ACL Workshop on the Usage of WordNet in Natural Language Processing Systems,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="16776" citStr="Gonzalo et al., 1998" startWordPosition="2574" endWordPosition="2577">ccount of text content and are compared to evaluate content similarity. We defined our notion of content similarity from the fact, broadly accepted in the information retrieval community, that the more terms (and related terms) are shared by two texts, the more likely they are to be about the same topic. Text content can be roughly approximated by a vector containing all lemmatized forms of words and their associated number of occurrences. We call such a vector the lexical profile of a text. It has been shown that using sets of synonyms instead of word forms could improve similarity measures (Gonzalo et al., 1998), so we use synset profiles to account for lexico- semantic variation. Text profile construction Words in text fragments are first lemmatized and their part-ofspeech is disambiguated using the morphological analysis tools of XRCE. Their corresponding set of synonyms is then looked up through a lexico-semantic interface, and the corresponding synset key is used to index the word or expression. We have developed an annotation graphical interface that allows a human to annotate strings in MDA grammars by choosing the appropriate synset in the default lexico-semantic resource, WordNet (Miller et a</context>
</contexts>
<marker>Gonzalo, Verdejo, Chugur, Cigarran, 1998</marker>
<rawString>Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan Cigarran. 1998. Indexing with WordNet Synsets Can Improve Text Retrieval. In Proceedings of the COLING/ACL Workshop on the Usage of WordNet in Natural Language Processing Systems, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Eleazar Eskin</author>
</authors>
<title>Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC&amp;quot;99,</booktitle>
<location>College Park, USA.</location>
<contexts>
<context position="22932" citStr="Hatzivassiloglou et al., 1999" startWordPosition="3576" endWordPosition="3579"> for a given source of documents so that it guarantees that the correct candidate is retained. cally using more fine-grained measures. An approach can be to search for evidence of the presence of some text passages produced by competing semantic choices in the input document, and to rescore them appropriately. Given the constraints on the domain of the input documents, we hope that simple features will help significantly in disambiguating candidates, as for example distance constraints which have been shown to participate significantly in the evaluation of text similarity over short passages (Hatzivassiloglou et al., 1999). 5.5 Interactive disambiguation Due to its limitations, the proposed approach cannot guarantee that the correct candidate can be selected automatically. Recognizing similar communicative intentions challenges simple text matching techniques, and can require expert knowledge that is difficult to obtain a priori and to encode into automatic disambiguation rules. We therefore propose that automatic selection of candidates be done down to a level of confidence that would be determined so as to guarantee that the correct document is retained. We then envisage several modes of intervention from an </context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, and Eleazar Eskin. 1999. Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning. In Proceedings of EMNLP/VLC&amp;quot;99, College Park, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurdlien Max</author>
<author>Marc Dymetman</author>
</authors>
<title>Document Content Analysis through Inverted Generation.</title>
<date>2002</date>
<booktitle>In Proceedings of the workshop on Using (and Acquiring) Linguistic (and World) Knowledge for Information Access of the AAAI Spring Symposium Series,</booktitle>
<location>Stanford University, USA.</location>
<contexts>
<context position="11667" citStr="Max and Dymetman, 2002" startWordPosition="1727" endWordPosition="1730">ent representations. Indeed, an MDA system can be used as a device for enumerating well-formed document representations in a constrained domain and associating texts with them. If we can compute a relevant measure of semantic similarity between the text produced for any document content representation 35 and the text of a legacy document, we could possibly consider the representations with the best similarity scores as those best corresponding to the legacy document under analysis. As this kind of analysis uses predictions made by a natural language generator, we named it inverted generation (Max and Dymetman, 2002). And because a generator will seriously undergenerate with respect to all the texts that could be normalized to the same communicative intention, we made this process fuzzy by matching documents at a more abstract level than on raw text to evaluate commonality of communicative content. 4.2 Implementing fuzzy inverted generation using MDA We use the formalism of the MDA authoring system (Dymetman et al., 2000) to implement fuzzy inverted generation, as it offers a close coupling between semantic modelling and text generation.2 In this context, an input document will be used as an information s</context>
</contexts>
<marker>Max, Dymetman, 2002</marker>
<rawString>Aurdlien Max and Marc Dymetman. 2002. Document Content Analysis through Inverted Generation. In Proceedings of the workshop on Using (and Acquiring) Linguistic (and World) Knowledge for Information Access of the AAAI Spring Symposium Series, Stanford University, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurelien Max</author>
</authors>
<title>Normalisation de Documents par Analyse du Contents it l&apos;Aide dun Modele Semantique et dun Generateur.</title>
<date>2002</date>
<booktitle>In Proceedings of TAIN-RECITAL 2002,</booktitle>
<location>Nancy, France.</location>
<contexts>
<context position="3569" citStr="Max, 2002" startWordPosition="547" endWordPosition="548"> to propose application domains for document normalization, we attempted to identify do33 mains where documents of the same nature but from different origins where compiled into homogeneous collections. We focussed our attention on the pharmaceutical domain, which produces several yearly compendiums of drug leaflets, as for example the French Vidal de la Famille (OVP Editions du VIDAL, 1998). Producing pharmaceutical documents is the responsibility of the pharmaceutical companies which market the drugs. A study we conducted on a corpus of 50 patient pharmaceutical leaflets for pain relievers (Max, 2002) collected on drug vendor websites revealed several types of variations. The first observation was that the structures of the leaflets could vary considerably. For comparable drugs, we found that for example warning-related information could be presented in different ways. One of them was to divide them into two sections, Warnings and Side effects, another one had a threesection division into Drug interaction precautions, Warnings, and Alcohol warning. In the first case, drug interaction precautions effectively appeared in the more general Warnings section (You should ask your doctor before ta</context>
</contexts>
<marker>Max, 2002</marker>
<rawString>Aurelien Max. 2002. Normalisation de Documents par Analyse du Contents it l&apos;Aide dun Modele Semantique et dun Generateur. In Proceedings of TAIN-RECITAL 2002, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurolien Max</author>
</authors>
<title>Multi-language Machine Translation through Document Normalization.</title>
<date>2003</date>
<booktitle>the proceedings of the EACI:03 EAMT workshop,</booktitle>
<location>Budapest, Hungary.</location>
<note>To appear in</note>
<contexts>
<context position="2232" citStr="Max, 2003" startWordPosition="339" endWordPosition="340">obtain the semantic structure corresponding to a raw document, and then produce from it a completely controlled version. If the raw document is bigger in scope from the documents that the authoring system models, then something similar to document summarization by content recognition and reformulation would be done. Incomplete representations after automatic analysis could be interactively completed, thus reentering controlled document authoring. Producing the document from the semantic representation in several languages would do some kind of normalizing translation of the original document (Max, 2003). We call the process of reconstructing such a semantic representation (and re-generating controlled text in the same language), which is common to all the above cases, document normalization. In this paper, we will first attempt to argue why document normalization could be of some use in the real world. We will then introduce our approach to document normalization, and describe a possible implementation. We will conclude by introducing our future work. 2 Why normalize documents? Text normalization often refers to techniques used to disambiguate text to facilitate its analysis (Mikheev, 2000).</context>
</contexts>
<marker>Max, 2003</marker>
<rawString>Aurolien Max. 2003. Multi-language Machine Translation through Document Normalization. To appear in the proceedings of the EACI:03 EAMT workshop, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Document centered approach to text normalization.</title>
<date>2000</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="2831" citStr="Mikheev, 2000" startWordPosition="432" endWordPosition="433">ent (Max, 2003). We call the process of reconstructing such a semantic representation (and re-generating controlled text in the same language), which is common to all the above cases, document normalization. In this paper, we will first attempt to argue why document normalization could be of some use in the real world. We will then introduce our approach to document normalization, and describe a possible implementation. We will conclude by introducing our future work. 2 Why normalize documents? Text normalization often refers to techniques used to disambiguate text to facilitate its analysis (Mikheev, 2000). The definition for document normalization that we propose can have much more impact on the surface form of documents. In order to propose application domains for document normalization, we attempted to identify do33 mains where documents of the same nature but from different origins where compiled into homogeneous collections. We focussed our attention on the pharmaceutical domain, which produces several yearly compendiums of drug leaflets, as for example the French Vidal de la Famille (OVP Editions du VIDAL, 1998). Producing pharmaceutical documents is the responsibility of the pharmaceutic</context>
</contexts>
<marker>Mikheev, 2000</marker>
<rawString>Andrei Mikheev. 2000. Document centered approach to text normalization. In Research and Development in Information Retrieval, pages 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Berckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
<author>R Tengi</author>
</authors>
<title>Five papers on Won/Net.</title>
<date>1993</date>
<publisher>Princeton University Press.</publisher>
<contexts>
<context position="17385" citStr="Miller et al., 1993" startWordPosition="2673" endWordPosition="2676"> al., 1998), so we use synset profiles to account for lexico- semantic variation. Text profile construction Words in text fragments are first lemmatized and their part-ofspeech is disambiguated using the morphological analysis tools of XRCE. Their corresponding set of synonyms is then looked up through a lexico-semantic interface, and the corresponding synset key is used to index the word or expression. We have developed an annotation graphical interface that allows a human to annotate strings in MDA grammars by choosing the appropriate synset in the default lexico-semantic resource, WordNet (Miller et al., 1993), or to define new sets of synonyms in the absence of availability of a more specific resource. The annotation interface also allows the annotator to specify a value of infonnativity for the indexed synsets4, which is taken into account when computing profile similarity. The set of synsets which have been used to 4We thought that the kind of informativity for words that was needed required some expertise on the class of documents, and was therefore not easily derivable from corpus statistics. We nevertheless intend to evaluate informativity measures derived from term frequencies. index the tex</context>
</contexts>
<marker>Miller, Berckwith, Fellbaum, Gross, Miller, Tengi, 1993</marker>
<rawString>G. Miller, R. Berckwith, C. Fellbaum, D. Gross, K. Miller, and R. Tengi. 1993. Five papers on Won/Net. Princeton University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nils J Nilsson</author>
</authors>
<title>A rtificial Intelligence: a New Synthesis.</title>
<date>1998</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="13909" citStr="Nilsson, 1998" startWordPosition="2099" endWordPosition="2100"> 2MDA grammars being Prolog programs, any Prolog predicate could be called from the rules. We ignored this powerful feature of MDA grammars and thus used a simplified formalism. 3We call virtual documents documents that can be predicted by the semantic model but do not exist a priori. what is done interactively in the authoring mode). A similarity measure is computed between the input document and the set of all the virtual documents that could be produced from a given partial content representation. This similarity measure is used as the evaluation function of an admissible heuristic search (Nilsson, 1998) that returns the candidate content representations in decreasing order of similarity with the input document. In order to guarantee that the search is admissible, it has to implement a best-first strategy, and use an optimistic evaluation function that decreases as search progresses and that is an overestimate of the similarity between the best attainable virtual document and the input document. In order to allow the computation of the similarity function between a partial content representation (a node in our search space) and an input document, some account of the properties of attainable v</context>
</contexts>
<marker>Nilsson, 1998</marker>
<rawString>Nils J. Nilsson. 1998. A rtificial Intelligence: a New Synthesis. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Le VIDAL de la famille. Hachette,</booktitle>
<editor>OVP Editions du VIDAL, editor.</editor>
<location>Paris.</location>
<marker>1998</marker>
<rawString>OVP Editions du VIDAL, editor. 1998. Le VIDAL de la famille. Hachette, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Paiva</author>
</authors>
<title>Investing Style in a Corpus of Pharmaceutical Leaflets: Result of a Factor Analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="5185" citStr="Paiva, 2000" startWordPosition="805" endWordPosition="806">s alcohol consumption may cause stomach bleeding in this circumstance. While some leaflets presented a separate section, Alcohol warnings, others simply mentioned the related possible side effect, stomach bleeding, in the appropriate section. In spite of these differences in structure, leaflets in the subset we have studied usually express the same types of content, that is, the communicative intentions expressed by the authors of the leaflets are similar. However, this content can be expressed in a variety of ways. A factor analysis of stylistic variation in a corpus of 342 patient leaflets (Paiva, 2000) revealed that two important factors opposed abstraction (e.g. use of agentless passives and nominalizations) to involvement/directness (e.g. use of 1st and 2nd persons and imperatives) and full reference to pronominalized reference. Our study also showed that similar communicative intentions could be expressed in a variety of ways conveying more or less subtle semantic distinctions. We argue that for documents of such an important nature, consistency of expression and of information presentation can not only be beneficial to the reader but also necessary to allow a clear and unambiguous under</context>
</contexts>
<marker>Paiva, 2000</marker>
<rawString>Daniel S. Paiva. 2000. Investing Style in a Corpus of Pharmaceutical Leaflets: Result of a Factor Analysis. In Proceedings of the ACL Student Research Workshop, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>David Warren</author>
</authors>
<title>Definite Clauses for Language Analysis.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<contexts>
<context position="8672" citStr="Pereira and Warren, 1980" startWordPosition="1267" endWordPosition="1270">eUnit::timeUnit-e, [&apos; without consulting a doctor. PWWarning::persistOrWorsenWarning(TypeOfSymptom). consultIfPainPerslstsOrGetsWorse::persistOrWorsenWarning(paln) ---&gt; [&apos;Consult your doctor if pain persists or gets worse.&apos;]. Figure 1: MDA grammar extract for the product warning section of a patient drug leaflet. the document content until it is complete. In the Multilingual Document Authoring (MDA) system (Dymetman et al., 2000), the specification of well-formed document content representations can be recursively described in a grammar formalism that is a variant of Definite Clause Grammars (Pereira and Warren, 1980). Figure 1 shows a simple MDA grammar extract for the product warning section of a patient leaflet. The first rule reads as follows: the semantic structure list0fProductWarning(AllergyWarning, DurationWarning) is of type productWarnings(TypeOfSymptom, ActiveIngredient), and is made up of the terminal string &amp;quot;PRODUCT WARNINGS&amp;quot;, an element of type allergyWarning(Activelngredient) element, and an element of type durationWarning(TypeOfSymptom). Semantic constraints are established through the use of shared type-parameters: for example, TypeOf Symptom constrains the element of type durationWarning.</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Fernando Pereira and David Warren. 1980. Definite Clauses for Language Analysis. Artificial Intelligence, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Power</author>
<author>Donia Scott</author>
</authors>
<title>Multilingual Authoring using Feedback Texts.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7108" citStr="Power and Scott, 1998" startWordPosition="1092" endWordPosition="1095"> documents from the same class of documents? Document normalization implies analyzing a legacy document into a semantically well-formed content representation, and producing a normalized version from that content representation. This expresses predefined communicative content present in the input document, in a structurally and linguistically controlled way. Predefined content reveals communicative intentions, which should ideally be described by an expert of the discourse domain. 3 Controlled document authoring There has been a recent trend to investigate controlled document authoring, e.g. (Power and Scott, 1998; Dymetman et al., 2000), where the focus is on obtaining document content representations by interaction with the user/author and producing multilingual versions of the final document from them. Typically, the user of these systems has to select possible semantic choices in active fields present in the evolving text of the document in the user&apos;s language. These selections iteratively refine 34 list0fRroductWarnings(AllergyWarning, DurationWarning)::productWarnings(TypeOfSymptom, ActiveIngredient)-e ---&gt; [&apos;PRODUCT WARN NGS&apos; AllergyWarning::allergyWarning(ActiveIngredient)-e, DurationWarning::d</context>
</contexts>
<marker>Power, Scott, 1998</marker>
<rawString>Richard Power and Donia Scott. 1998. Multilingual Authoring using Feedback Texts. In Proceedings of COLING/ACL-98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1130" citStr="Reiter and Dale, 2000" startWordPosition="168" endWordPosition="171">on system is described. 1 Introduction Controlled Document Authoring is a field of research in NLP that is concerned with the interactive production of documents in limited domains. The aim of systems implementing controlled document authoring is to allow the user to specify an underlying semantic representation of the document that is well-formed and complete relative to its class of documents. This representation is then used to produce a fully controlled version of the document, possibly in several languages. We distinguish controlled document authoring systems from what is referred to in (Reiter and Dale, 2000) as computer as authoring aid, which are Natural Language Generation systems intended to produce initial drafts or only routine factual sections of documents, in that the former can be used to produce high-quality final versions of documents without the need for further hand-editing. The question which motivated our work was the following: can we reuse the resources of an existing controlled document authoring system to analyze documents from the same class of documents? If so, we could obtain the semantic structure corresponding to a raw document, and then produce from it a completely control</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>NLG Vs Templates.</title>
<date>1995</date>
<booktitle>In Proceedings of ENLGW-95,</booktitle>
<location>Leiden, The Netherlands.</location>
<contexts>
<context position="10025" citStr="Reiter, 1995" startWordPosition="1462" endWordPosition="1463">g of the leaves of their tree.1 The granularity of text fragments that is allowed in rules is not necessarily as fine-grained as predicateargument structures of sentences commonly used in NLG. This approach proved to be adequate for classes of documents where certain choices could be rendered as entire text passages (e.g. pregnancy warnings, disclaimers, etc.) and where 1Some details such as morphological-level constraints have been omitted for lack of space. a more fine-grained representation would not be needed, thus offering an interesting intermediate level between full NLG and templates (Reiter, 1995). 4 A paradigm for deep content analysis 4.1 Fuzzy inverted generation Content analysis is often viewed as a parsing process where semantic interpretation is derived from syntactic structures (Allen, 1995). In practice, however, building broad-coverage syntacticallydriven parsing grammars that are robust to the variation in the input is a very difficult task. Furthermore, we have already argued that for the purpose of document normalization we would like to match texts that do not carry significant communicative differences in a given class of documents but may be of quite different surface fo</context>
</contexts>
<marker>Reiter, 1995</marker>
<rawString>Ehud Reiter. 1995. NLG Vs Templates. In Proceedings of ENLGW-95, Leiden, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>