<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996525">
Multi-level Structured Models for Document-level Sentiment Classification
</title>
<author confidence="0.992446">
Ainur Yessenalina
</author>
<affiliation confidence="0.9974375">
Dept. of Computer Science
Cornell University
</affiliation>
<address confidence="0.542693">
Ithaca, NY, USA
</address>
<email confidence="0.998185">
ainur@cs.cornell.edu
</email>
<author confidence="0.997">
Yisong Yue
</author>
<affiliation confidence="0.9974815">
Dept. of Computer Science
Cornell University
</affiliation>
<address confidence="0.542699">
Ithaca, NY, USA
</address>
<email confidence="0.998193">
yyue@cs.cornell.edu
</email>
<author confidence="0.99253">
Claire Cardie
</author>
<affiliation confidence="0.9974635">
Dept. of Computer Science
Cornell University
</affiliation>
<address confidence="0.54304">
Ithaca, NY, USA
</address>
<email confidence="0.998934">
cardie@cs.cornell.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751045454546">
In this paper, we investigate structured mod-
els for document-level sentiment classifica-
tion. When predicting the sentiment of a sub-
jective document (e.g., as positive or nega-
tive), it is well known that not all sentences
are equally discriminative or informative. But
identifying the useful sentences automatically
is itself a difficult learning problem. This pa-
per proposes a joint two-level approach for
document-level sentiment classification that
simultaneously extracts useful (i.e., subjec-
tive) sentences and predicts document-level
sentiment based on the extracted sentences.
Unlike previous joint learning methods for
the task, our approach (1) does not rely on
gold standard sentence-level subjectivity an-
notations (which may be expensive to obtain),
and (2) optimizes directly for document-level
performance. Empirical evaluations on movie
reviews and U.S. Congressional floor debates
show improved performance over previous ap-
proaches.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972818181818">
Sentiment classification is a well-studied and active
research area (Pang and Lee, 2008). One of the main
challenges for document-level sentiment categoriza-
tion is that not every part of the document is equally
informative for inferring the sentiment of the whole
document. Objective statements interleaved with the
subjective statements can be confusing for learning
methods, and subjective statements with conflicting
sentiment further complicate the document catego-
rization task. For example, authors of movie reviews
often devote large sections to (largely objective) de-
scriptions of the plot (Pang and Lee, 2004). In ad-
dition, an overall positive review might still include
some negative opinions about an actor or the plot.
Early research on document-level sentiment clas-
sification employed conventional machine learning
techniques for text categorization (Pang et al., 2002).
These methods, however, assume that documents are
represented via a flat feature vector (e.g., a bag-of-
words). As a result, their ability to identify and ex-
ploit subjectivity (or other useful) information at the
sentence-level is limited.
And although researchers subsequently proposed
methods for incorporating sentence-level subjectiv-
ity information, existing techniques have some un-
desirable properties. First, they typically require
gold standard sentence-level annotations (McDon-
ald et al. (2007), Mao and Lebanon (2006)). But
the cost of acquiring such labels can be prohibitive.
Second, some solutions for incorporating sentence-
level information lack mechanisms for controlling
how errors propagate from the subjective sentence
identification subtask to the main document classifi-
cation task (Pang and Lee, 2004). Finally, solutions
that attempt to handle the error propagation problem
have done so by explicitly optimizing for the best
combination of document- and sentence-level clas-
sification accuracy (McDonald et al., 2007). Opti-
mizing for this compromise, when the real goal is
to maximize only the document-level accuracy, can
potentially hurt document-level performance.
In this paper, we propose a joint two-level model
to address the aforementioned concerns. We formu-
late our training objective to directly optimize for
</bodyText>
<page confidence="0.938693">
1046
</page>
<note confidence="0.8162895">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1046–1056,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99926">
document-level accuracy. Further, we do not require
gold standard sentence-level labels for training. In-
stead, our training method treats sentence-level la-
bels as hidden variables and jointly learns to predict
the document label and those (subjective) sentences
that best “explain” it, thus controlling the propaga-
tion of incorrect sentence labels. And by directly
optimizing for document-level accuracy, our model
learns to solve the sentence extraction subtask only
to the extent required for accurately classifying doc-
ument sentiment. A software implementation of our
method is also publicly available.1
For the rest of the paper, we will discuss re-
lated work, motivate and describe our model, present
an empirical evaluation on movie reviews and U.S.
Congressional floor debates datasets and close with
discussion and conclusions.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999686703703704">
Pang and Lee (2004) first showed that sentence-
level extraction can improve document-level per-
formance. They used a cascaded approach by
first filtering out objective sentences and perform-
ing subjectivity extractions using a global min-cut
inference. Afterward, the subjective extracts were
converted into inputs for the document-level senti-
ment classifier. One advantage of their approach
is that it avoids the need for explicit subjectiv-
ity annotations. However, like other cascaded ap-
proaches (e.g., Thomas et al. (2006), Mao and
Lebanon (2006)), it can be difficult to control how
errors propagate from the sentence-level subtask to
the main document classification task.
Instead of taking a cascaded approach, one can
directly modify the training of flat document clas-
sifiers using lower level information. For instance,
Zaidan et al. (2007) used human annotators to mark
the “annotator rationales”, which are text spans that
support the document’s sentiment label. These an-
notator rationales are then used to formulate addi-
tional constraints during SVM training to ensure that
the resulting document classifier is less confident in
classifying a document that does not contain the ra-
tionale versus the original document. Yessenalina et
al. (2010) extended this approach to use automati-
cally generated rationales.
</bodyText>
<footnote confidence="0.930337">
1http://projects.yisongyue.com/svmsle/
</footnote>
<bodyText confidence="0.999937342857143">
A natural approach to avoid the pitfalls associ-
ated with cascaded methods is to use joint two-
level models that simultaneously solve the sentence-
level and document-level tasks (e.g., McDonald et
al. (2007), Zaidan and Eisner (2008)). Since these
models are trained jointly, the sentence-level pre-
dictions affect the document-level predictions and
vice-versa. However, such approaches typically
require sentence-level annotations during training,
which can be expensive to acquire. Furthermore,
the training objectives are usually formulated as a
compromise between sentence-level and document-
level performance. If the goal is to predict well at the
document-level, then these approaches are solving a
much harder problem that is not exactly aligned with
maximizing document-level accuracy.
Recently, researchers within both Natural Lan-
guage Processing (e.g., Petrov and Klein (2007),
Chang et al. (2010), Clarke et al. (2010)) and
other fields (e.g., Felzenszwalb et al. (2008), Yu
and Joachims (2009)) have analyzed joint multi-
level models (i.e., models that simultaneously solve
the main prediction task along with important sub-
tasks) that are trained using limited or no explicit
lower level annotations. Similar to our approach, the
lower level labels are treated as hidden or latent vari-
ables during training. Although the training process
is non-trivial (and in particular requires a good ini-
tialization of the hidden variables), it avoids the need
for human annotations for the lower level subtasks.
Some researchers have also recently applied hidden
variable models to sentiment analysis, but they were
focused on classifying either phrase-level (Choi and
Cardie, 2008) or sentence-level polarity (Nakagawa
et al., 2010).
</bodyText>
<sectionHeader confidence="0.983655" genericHeader="method">
3 Extracting Hidden Explanations
</sectionHeader>
<bodyText confidence="0.9998938">
In this paper, we take the view that each document
has a subset of sentences that best explains its sen-
timent. Consider the “annotator rationales” gener-
ated by human judges for the movie reviews dataset
(Zaidan et al., 2007). Each rationale is a text span
that was identified to support (or explain) its parent
document’s sentiment. Thus, these rationales can be
interpreted as (something close to) a ground truth la-
beling of the explanatory segments. Using a dataset
where each document contains only its rationales,
</bodyText>
<page confidence="0.989146">
1047
</page>
<construct confidence="0.445386">
Algorithm 1 Inference Algorithm for (2)
</construct>
<listItem confidence="0.972730444444444">
1: Input: x
2: Output: (y, s)
3: s+ &lt;-- argmaxsES(x) ~wTΨ(x, +1, s)
4: s− &lt;-- argmaxsES(x) ~wT Ψ(x, −1, s)
5: if ~wT Ψ(x, +1, s+) &gt; ~wT Ψ(x, −1, s−) then
6: Return (+1, s+)
7: else
8: Return (−1, s−)
9: end if
</listItem>
<bodyText confidence="0.99942175">
cross validation experiments using an SVM classi-
fier yields 97.44% accuracy – as opposed to 86.33%
accuracy when using the full text of the original doc-
uments. Clearly, extracting the best supporting seg-
ments can offer a tremendous performance boost.
We are interested in settings where human-
extracted explanations such as annotator rationales
might not be readily available, or are imperfect. As
such, we will formulate the set of extracted sen-
tences as latent or hidden variables in our model.
Viewing the extracted sentences as latent variables
will pose no new challenges during prediction, since
the model is expected to predict all labels at test
time. We will leverage recent advances in training
latent variable SVMs (Yu and Joachims, 2009) to ar-
rive at an effective training procedure.
</bodyText>
<sectionHeader confidence="0.994953" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.998876555555555">
In this section, we present a two-level document
classification model. Although our model makes
predictions at both the document and sentence lev-
els, it will be trained (and evaluated) only with re-
spect to document-level performance. We begin
by presenting the feature structure and inference
method. We will then describe a supervised train-
ing algorithm based on structural SVMs, and finally
discuss some extensions and design decisions.
Let x denote a document, y = +1 denote the sen-
timent (for us, a binary positive or negative polarity)
of a document, and s denote a subset of explanatory
sentences in x. Let Ψ(x, y, s) denote a joint fea-
ture map that outputs features describing the qual-
ity of predicting sentiment y using explanation s for
document x. We focus on linear models, so given a
(learned) weight vector ~w, we can write the quality
of predicting y (with explanation s) as
</bodyText>
<equation confidence="0.932418">
F(x, y, s; ~w) = ~wT Ψ(x, y, s), (1)
and a document-level sentiment classifier as
h(x; ~w) = argmax
y=±1
</equation>
<bodyText confidence="0.99987525">
where S(x) denotes the collection of feasible expla-
nations (e.g., subsets of sentences) for x.
Let xj denote the j-th sentence of x. We propose
the following instantiation of (1),
</bodyText>
<equation confidence="0.9812875">
~wTΨ(x, y, s) =
1 � y � ~wT polψpol(xj) + ~wT subjψsubj(xj), (3)
N(x)
jEs
</equation>
<bodyText confidence="0.962857333333333">
where the first term in the summation captures the
quality of predicting polarity y on sentences in s,
the second term captures the quality of predicting s
as the subjective sentences, and N(x) is a normaliz-
ing factor (which will be discussed in more detail in
Section 4.3). We represent the weight vector as
and
and
denote the polarity and
subjectivity features of sentence xj, respectively.
Note that
and
are disjoint by construc-
tion, i.e.,
= 0. We will present extensions
in Section 4.5.
For example, suppose
and
were both
feature vectors. Then we might learn
a high weight for the feature corresponding to the
</bodyText>
<equation confidence="0.890745363636363">
word
in
ψpol(xj)
ψsubj(xj)
ψpol
ψsubj
ψTpolψsubj
ψpol
ψsubj
bag-of-words
“think”
</equation>
<bodyText confidence="0.978305285714286">
ψsubj since that word is indicative
of the sentence being subjective (but not necessarily
indicating positive or negative polarity).
explains a negative polari
ty prediction.
We now specify the structure of S(x). In this pa-
per, we use a cardinality constraint,
</bodyText>
<subsectionHeader confidence="0.998953">
4.1 Making Predictions
</subsectionHeader>
<bodyText confidence="0.9967494">
Algorithm 1 describes our inference procedure. Re-
call from (2) that our hypothesis function predicts
the sentiment label that maximizes (3). To do this,
we compare the best set of sentences that explains
a positive polarity prediction with the best set that
</bodyText>
<equation confidence="0.778847875">
S(x) = �s � �1, . . . , �x�� : �s� � f(�x�)�, (5)
max F(x, y, s; ~w), (2)
sES(x)
�
� ~wpol ,
~wsubj
w=
(4)
</equation>
<page confidence="0.739947">
1048
</page>
<bodyText confidence="0.322276">
Algorithm 2 Training Algorithm for OP 1
</bodyText>
<listItem confidence="0.976989727272727">
1: Input: {(x1, y1), ... , (xN, yN)} //training data
2: Input: C //regularization parameter
3: Input: (s1, ... , sN) //initial guess
4: w~ ← SSVMSolve(C, {(xi, yi, si)}Ni�1)
5: while w~ not converged do
6: for i = 1, ... , N do
7: si ← argmaxsES(xi) ~wTΨ(xi, yi, s)
8: end for
9: w~ ← SSVMSolve(C, {(xi, yi, si)}Ni�1)
10: end while
11: Return w~
</listItem>
<bodyText confidence="0.99994475">
where f(|x|) is a function that depends only on the
number of sentences in x. For example, a simple
function is f(|x|) = |x |· 0.3, indicating that at most
30% of the sentences in x can be subjective.
Using this definition of S(x), we can then com-
pute the best set of subjective sentences for each
possible y by computing the joint subjectivity and
polarity score of each sentence xj in isolation,
</bodyText>
<equation confidence="0.99886675">
~wTΨ(xi, yi, si) ≥ max
siES(xi)
~wT Ψ(xi, −yi, s�)+1−ξi,
y · ~wTpolψpol(xj) + ~wTsubjψsubj(xj),
</equation>
<bodyText confidence="0.8300282">
and selecting the top
ass (or fewer, if there
are fewer than
f(|x|)
f(|x|) that have positive joint score).
</bodyText>
<subsectionHeader confidence="0.988133">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.92699525">
For training, we will use an approach based on latent
variable structural SVMs (Yu an
d Joachims, 2009).
Optimization Problem 1.
</bodyText>
<equation confidence="0.963009">
s.t.
∀i :
~wT Ψ(xi, yi, s) ≥
max ~wT Ψ(xi, −yi, s�) + 1 − ξi (7)
siES(xi)
</equation>
<bodyText confidence="0.9924659">
OP 1 optimizes the standard SVM training objec-
tive for binary classification. Each training example
has a corresponding constraint (7), which is quanti-
fied over the best possible explanation of the train-
ing polarity label. Note that we never observe the
true explanation for the training labels; they are the
hidden or latent variables. The hidden variables are
also ignored in the objective function.
1049 As a result, one can interpret OP 1 to be directly
optimizing atrade-off between model complexity
(as measured using the 2-norm) and document-level
classification error in the training set. This has two
main advantages over related training approaches.
First, it solves the multi-level problem jointly as op-
posed to separately, which avoids introducing diffi-
cult to control propagation errors. Second, it does
not require solving the sentence-level task perfectly,
and also does not require precise sentence-level
training labels. In other words, our goal is to learn to
identify the informative (subjective) sentences that
best explain the training labels to the extent required
for good document classification performance.
OP 1 is non-convex because of the constraints (7).
To solve OP 1, we use the combination of the CCCP
algorithm (Yuille and Rangarajan, 2003) with cut-
ting plan
where si is some fixed explanation (e.g., an initial
guess of the best explanation). Then OP 1 reduces
to a standard structural SVM, which can be solved
efficiently (Joachims et al., 2009). Algorithm 2 de-
scribes our training procedure. Starting with an ini-
tial guess si for each training example, the training
procedure alternates between solving an instance of
the resulting structural SVM (called SSVMSolve in
Algorithm 2) using the currently best known expla-
nations si (Line 9), and making a new guess of the
best explanations (Line 7). Yu and Joachims (2009)
showed that this alternating procedure for training
latent variable structural SVMs is an instance of the
CCCP procedure (Yuille and Rangarajan, 2003), and
so is guaranteed to converge to a local optimum.
For our experiments, we do not train until conver-
gence, but instead use performance on a validation
set to choose the halting iteration. Since OP 1 is non-
convex, agood initialization is necessary. To gener-
ate the initial explanations, one can use an off-the-
shelf sentiment classifier such as OpinionFinder2
(Wilson et al., 2005). For some datasets, there ex-
ist documents with annotated sentences, which we
e training of structural SVMs (Joachims et
</bodyText>
<equation confidence="0.947868928571429">
al., 2009), as proposed in Yu and Joachims (2009).
Suppose each constraint (7) is replaced by
2http://www.cs.pitt.edu/mpqa/
opinionfinderrelease/
N
2
k~wk2 + 1
N
ξi (6)
min
~w,ξ&gt;0
i=1
max
sESi
</equation>
<bodyText confidence="0.9929105">
can treat either as the ground truth or another (very
good) initial guess of the explanatory sentences.
</bodyText>
<subsectionHeader confidence="0.999033">
4.3 Feature Representation
</subsectionHeader>
<bodyText confidence="0.9885059375">
Like any machine learning approach, we must spec-
ify a useful set of features for the ψ vectors described
above. We will consider two types of features.
Bag-of-words. Perhaps the simplest approach is
to define ψ using a bag-of-words feature representa-
tion, with one feature corresponding to each word in
the active lexicon of the corpus. Using such a feature
representation might allow us to learn which words
have high polarity (e.g., “great”) and which are in-
dicative of subjective sentences (e.g., “opinion”).
Sentence properties. We can incorporate many
useful features to describe sentence subjectivity. For
example, subjective sentences might densely popu-
late the end of a document, or exhibit spatial co-
herence (so features describing previous sentences
might be useful for classifying the current sentence).
Such features cannot be compactly incorporated into
flat models that ignore the document structure.
For our experiments, we normalize each ψsubj
and ψpol to have unit 2-norm.
Joint Feature Normalization. Another design
decision is the choice of normalization N(x) in (3).
Two straightforward choices are N(x) = f(|x|) and
p
N(x) = f(|x|), where f(|x|) is the size con-
straint as described in (5). In our experiments we
tried both and found the square root normalization
to work better in practice; therefore all the experi-
p
mental results are reported using N(x) = f(|x|).
The appendix contains an analysis that sheds light
on when square root normalization can be useful.
</bodyText>
<subsectionHeader confidence="0.912962">
4.4 Incorporating Proximity Information
</subsectionHeader>
<bodyText confidence="0.999745333333333">
As mentioned in Section 4.3, it is possible (and
likely) for subjective sentences to exhibit spatial co-
herence (e.g., they might tend to group together).
To exploit this structure, we will expand the feature
space of ψsubj to include both the words of the cur-
rent and previous sentence as follows,
</bodyText>
<equation confidence="0.8821205">
�
� ψsubj(xj)
ψsubj(x, j) = .
ψsubj(xj`)
The corresponding weight vector can be written as
�
�i
wsubj =.
~~wsubj
wprevSubj
</equation>
<bodyText confidence="0.999995666666667">
By adding these features, we are essentially assum-
ing that the words of the previous sentence are pre-
dictive of the subjectivity of the current sentence.
Alternative approaches include explicitly ac-
counting for this structure by treating subjective
sentence extraction as a sequence-labeling problem,
such as in McDonald et al. (2007). Such struc-
ture formulations can be naturally encoded in the
joint feature map. Note that the inference procedure
in Algorthm 1 is still tractable, since it reduces to
comparing the best sequence of subjective/objective
sentences that explains a positive sentiment versus
the best sequence that explains a negative sentiment.
For this study, we chose not to examine this more
expressive yet more complex structure.
</bodyText>
<subsectionHeader confidence="0.876196">
4.5 Extensions
</subsectionHeader>
<bodyText confidence="0.96816">
Though our initial model (3) is simple and intuitive,
performance can depend heavily on the quality of
latent variable initialization and the quality of the
feature structure design. Consider the case where
the initialization contains only objective sentences
that do not convey any sentiment. Then all the fea-
tures initially available during training are gener-
ated from these objective sentences and are thus use-
less for sentiment classification. In other words, too
much useful information has been suppressed for
the model to make effective decisions. To hedge
against learning poor models due to using a poor
initialization and/or a suboptimal feature structure,
we now propose extensions that incorporate infor-
mation from the entire document.
We identify the following desirable properties that
any such extended model should satisfy:
(A) The model should be linear.
(B) The model should be trained jointly.
(C) The component that models the entire docu-
ment should influence which sentences are ex-
tracted.
The first property stems from the fact that our ap-
proach relies on linear models. The second property
is desirable since joint training avoids error propaga-
tion that can be difficult to control. The third prop-
erty deals with the information suppression issue.
</bodyText>
<page confidence="0.967531">
1050
</page>
<subsectionHeader confidence="0.531443">
4.5.1 Regularizing Relative to a Prior
</subsectionHeader>
<bodyText confidence="0.964521">
We first consider a model that satisfies properties
(A) and (C). Using the representation in (4), we pro-
pose a training procedure that regularize ~wpol rela-
tive to a prior model. Suppose we have a weight
vector ~w0 which indicated the a priori guess of the
contribution of each corresponding feature, then we
can train our model using OP 2,
Optimization Problem 2.
</bodyText>
<equation confidence="0.996394333333333">
s.t. ∀i :
~wTΨ(xi, yi, s) ≥
~wTΨ(xi, −yi, sI) + 1 − ξi
</equation>
<bodyText confidence="0.999904666666667">
where ~wdoc denotes a weight vector trained to clas-
sify the polarity of entire documents. Then one can
interpret OP 2 as enforcing that the polarity weights
~wpol not be too far from ~wdoc. Note that ~w0 must be
available before training. Therefore this approach
does not satisfy property (B).
</bodyText>
<subsectionHeader confidence="0.739062">
4.5.2 Extended Feature Space
</subsectionHeader>
<bodyText confidence="0.999990636363636">
One simple way to satisfy all three aforemen-
tioned properties is to jointly model not only po-
larity and subjectivity of the extracted sentences,
but also polarity of the entire document. Let ~wdoc
denote the weight vector used to model the polar-
ity of entire document x (so the document polarity
score is then ~wTdocψpol(x)). We can also incorporate
this weight vector into our structured model to com-
pute a smoothed polarity score of each sentence via
~wTdocψpol(xj). Following this intuition, we propose
the following structured model,
</bodyText>
<equation confidence="0.969404">
~wT Ψ(x, y, s) =
��
�E~wTsubjψsubj(xj) �+ y · ~wTdocψpol(x)
jEs
</equation>
<bodyText confidence="0.992838">
where the weight vector is now
</bodyText>
<equation confidence="0.9368295">
�
w~ = �
</equation>
<bodyText confidence="0.9647884">
this model via OP 1 achieves that
is
(1) used to model the polarity of the entire docu-
ment, and (2) used to compute a smoothed estimate
of the polarity of the extracted sentences. This sat-
isfies all three properties (A), (B), an
Training
~wdoc
d (C), although
other approaches are also possible.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.982807">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99993012">
We evaluate our methods using the Movie Reviews
and U.S. Congressional Floor Debates datasets, fol-
lowing the setup used in previous work for compar-
ison purposes.3
Movie Reviews. We use the movie reviews
dataset from Zaidan et al. (2007) that was originally
released by Pang and Lee (2004). This version con-
tains annotated rationales for each review, which we
use to generate an additional initialization during
training (described below). We follow exactly the
experimental setup used in Zaidan et al. (2007).4
U.S. Congressional Floor Debates. We also
use the U.S. Congressional floor debates transcripts
from Thomas et al. (2006). The data was extracted
from GovTrack (http://govtrack.us), which has all
available transcripts of U.S. floor debates in the
House of Representatives in 2005. As in previ-
ous work, only debates with discussions of “con-
troversial” bills were considered (where the los-
ing side had at least 20% of the speeches). The
goal is to predict the vote (“yea” or “nay”) for the
speaker of each speech segment. For our experi-
ments, we evaluate our methods using the speaker-
based speech-segment classification setting as de-
scribed in Thomas et al. (2006).5
</bodyText>
<footnote confidence="0.988705555555556">
3Datasets in the required format for SVM&amp;quot;&apos; are available at
http://www.cs.cornell.edu/˜ainur/data.html
4Since the rationale annotations are available for nine out of
10 folds, we used the 10-th fold as the blind test set. We trained
nine different models on subsets of size eight, used the remain-
ing fold as the validation set, and then measured the average
performance on the final test set.
5In the other setting described in Thomas et al. (2006)
(segment-based speech-segment classification), around 39% of
</footnote>
<figure confidence="0.997873387096774">
�
�
�1:
jEs
y
N(x)
�
~wTpolψpol(x) + cψpol(xj))
j
�
wd
N
1k~w − ~w0k2 + C
N
min
~w,ξ&gt;0
i=1
ξi
max
sESi
max
s&apos;ES(xi)
For our experiments, we use
� ~wdoc
0 1 ,
~w0 =
1
+ N(x)
~wpol �
~wsubj �.
~wdoc
</figure>
<page confidence="0.990195">
1051
</page>
<tableCaption confidence="0.863069333333333">
Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates
(bottom) datasets using SVMsle, SVMsle w/ Prior and SVMsle
fs with and without proximity features.
</tableCaption>
<table confidence="0.999528727272727">
INITIALIZATION SVMsle + Prox.Feat. SVMsle + Prox.Feat. SVMsle + Prox.Feat.
w/ Prior fs
Random 30% 87.22 85.44 87.61 87.56 89.50 88.22
Last 30% 89.72 * 88.83 90.50 * 90.00 * 91.06 * 91.22 *
OpinionFinder 91.28 * 90.89 * 91.72 * 93.22 * 92.50 * 92.39 *
Annot.Rationales 91.61 * 92.00 * 92.67 * 92.00 * 92.28 * 93.22 *
INITIALIZATION SVMsle + Prox.Feat. SVMsle + Prox.Feat. SVMsle fs + Prox.Feat.
w/ Prior
Random 30% 78.84 73.14 78.49 76.40 77.33 73.84
Last 30% 73.26 73.95 71.51 73.60 67.79 73.37
OpinionFinder 77.33 79.53 77.09 78.60 77.67 77.09
</table>
<bodyText confidence="0.96893965">
– For Movie Reviews, the SVM baseline accuracy is 88.56%. A * (or *) indicates statically significantly better performance than
baseline according to the paired t-test with P &lt; 0.001 (or P &lt; 0.05).
– For U.S. Congressional Floor Debates, the SVM baseline accuracy is 70.00%. Statistical significance cannot be calculated because
the data comes in a single split.
Since our training procedure solves a non-convex
optimization problem, it requires an initial guess of
the explanatory sentences. We use an explanatory
set size (5) of 30% of the number of sentences in
each document, L = F0.3 · |x|], with a lower cap of
1. We generate initializations using OpinionFinder
(Wilson et al., 2005), which were shown to be a
reasonable substitute for human annotations in the
Movie Reviews dataset (Yessenalina et al., 2010).6
We consider two additional (baseline) methods
for initialization: using a random set of sentences,
and using the last 30% of sentence in the document.
In the Movie Reviews dataset, we also use sentences
containing human-annotator rationales as a final ini-
tialization option. No such manual annotations are
available for the Congressional Debates.
</bodyText>
<subsectionHeader confidence="0.997022">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.980037888888889">
We evaluate three versions of our model: the ini-
tial model (3) which we call SVMsle (SVMs for
Sentiment classification with Latent Explanations),
SVMsle regularized relative to a prior as described in
the documents in the whole dataset contain only 1-3 sentences,
making it an uninteresting setting to analyze with our model.
6We select all sentences whose majority vote of Opinion-
Finder word-level polarities matches the document’s sentiment.
If there are fewer than L sentences, we add sentences starting
from the end of the document. If there are more, we remove
sentences starting from the beginning of the document.
Section 4.5.1 which we refer to as SVMsle w/ Prior,7
and the feature smoothing model described in Sec-
tion 4.5.2 which we call SVMsle
fs . Due to the diffi-
culty of selecting a good prior, we expect SVMsle
fs to
exhibit the most robust performance.
Table 1 shows a comparison of our proposed
methods on the two datasets. We observe that
SVMsle
fs provides both strong and robust perfor-
mance. The performance of SVMsle is generally bet-
ter when trained using a prior than not in the Movie
Reviews dataset. Both extensions appear to hurt
performance in the U.S. Congressional Floor De-
bates dataset. Using OpinionFinder to initialize our
training procedure offers good performance across
both datasets, whereas the baseline initializations
exhibit more erratic performance behavior.8 Unsur-
prisingly, initializing using human annotations (in
the Movie Reviews dataset) can offer further im-
provement. Adding proximity features (as described
in Section 4.4) in general seems to improve perfor-
mance when using a good initialization, and hurts
performance otherwise.
</bodyText>
<footnote confidence="0.997922333333333">
7We either used the same value of C to train both standard
SVM model and SVM�&amp;quot; w/ Prior or used the best standard
SVM model on the validation set to train SVM&amp;quot;&apos; w/ Prior. We
chose the combination that works the best on the validation set.
8Using the random initialization on the U.S. Congressional
Floor Debates dataset offers surprisingly good performance.
</footnote>
<page confidence="0.997184">
1052
</page>
<tableCaption confidence="0.8129012">
Table 2: Comparison of SVMsle
fs with previous work on
the Movie Reviews dataset. We considered two settings:
when human annotations are available (Annot. Labels),
and when they are unavailable (No Annot. Labels).
</tableCaption>
<table confidence="0.81083995">
METHOD
SVM
Zaidan et al. (2007)
SVM���
��
SVM���
�� + Prox.Feat.
Baseline
Annot.
Labels
ACC
88.56
92.20
92.28
93.22
No Annot. Yessenalina et al. (2010) 91.78
Labels SVM��� Figure 1: Overlap of extracted sentences from different
�� 92.50
SVM��� 92.39 SV�ls models on the Movie Reviews training set.
�� +Prox.Feat.
</table>
<tableCaption confidence="0.999874">
Table 3: Comparison of SVMsle
</tableCaption>
<bodyText confidence="0.557365428571429">
fs with previous work on
the U.S. Congressional Floor Debates dataset for the
speaker-based segment classification task.
METHOD
SVM
Thomas et al. (2006)
Bansal et al. (2008)
</bodyText>
<equation confidence="0.84284275">
SVM���
��
SVM���
�� + Prox.Feat.
</equation>
<bodyText confidence="0.99615925">
Tables 2 and 3 show a comparison of SVM���
�� with
previous work on the Movie Reviews and U.S. Con-
gressional Floor Debates datasets, respectively. For
the Movie Reviews dataset, we considered two set-
tings: when human annotations are available, and
when they are not (in which case we initialized using
OpinionFinder). For the U.S. Congressional Floor
Debates dataset we used only the latter setting, since
there are no annotations available for this dataset. In
all cases we observe SVM���
�� showing improved per-
formance compared to previous results.
Training details. We tried around 10 different
values for C parameter, and selected the final model
based on the validation set. The training proce-
dure alternates between training a standard struc-
tural SVM model and using the subsequent model
to re-label the latent variables. We selected the halt-
ing iteration of the training procedure using the val-
idation set. When initializing using human annota-
tions for the Movie Reviews dataset, the halting iter-
ation is typically the first iteration, whereas the halt-
ing iteration is typically chosen from a later iteration
</bodyText>
<figureCaption confidence="0.970099">
Figure 2: Test accuracy on the Movie Reviews dataset for
SVMsle
</figureCaption>
<bodyText confidence="0.924057043478261">
fs while varying extraction size.
when initializing using OpinionFinder.
Figure 1 shows the per-iteration overlap of ex-
tracted sentences from SVM���
�� models initialized us-
ing OpinionFinder and human annotations on the
Movie Reviews training set. We can see that train-
ing has approximately converged after about 10 it-
erations.9 We can also see that both models itera-
tively learn to extract sentences that are more similar
to each other than their respective initializations (the
overlap between the two initializations is 57%). This
is an indicator that our learning problem, despite be-
ing non-convex and having multiple local optima,
has a reasonably large “good” region that can be ap-
proached using different initialization methods.
Varying the extraction size. Figure 2 shows how
accuracy on the test set of SVM���
�� changes on the
Movie Reviews dataset as a function of varying the
extraction size f(|x|) from (5). We can see that per-
formance changes smoothly10 (and so is robust), and
that one might see further improvement from more
</bodyText>
<footnote confidence="0.97632825">
9The number of iterations required to converge is an upper
bound on the number of iterations from which to choose the
halting iteration (based on a validation set).
10The smoothness will depend on the initialization.
</footnote>
<figure confidence="0.940877">
Baseline
Prior work
Our work
ACC
70.00
71.28
75.00
77.67
77.09
</figure>
<page confidence="0.98612">
1053
</page>
<tableCaption confidence="0.53713">
Table 4: Example ”yea” speech with Latent Explanations from the U.S. Congressional Floor Debates dataset predicted
by SVMsle
fs with OpinionFinder initialization. Latent Explanations are preceded by solid circles with numbers denoting
</tableCaption>
<bodyText confidence="0.69146175">
their preference order (1 being most preferred by SVMsle
fs ). The five least subjective sentences are preceded by circles
with numbers denoting the subjectivity order (1 being least subjective according to SVMsle
fs ).
</bodyText>
<construct confidence="0.6876184">
0 Mr. Speaker, I am proud to stand
on the house floor today to speak in
favor of the Stem Cell Research En-
hancement Act, legislation which will
bring hope to millions ofpeople suffer-
ing from disease in this nation. 0 I
want to thank Congresswoman Degette
and Congressman Castle for their tire-
less work in bringing this bill to the
house floor for a vote.
</construct>
<bodyText confidence="0.917831689655172">
D The discovery of embryonic stem
cells is a major scientific breakthrough.
OO Embryonic stem cells have the po-
tential to form any cell type in the
human body. This could have pro-
found implications for diseases such as
Alzheimer’s, Parkinson’s, various forms
of brain and spinal cord disorders, dia-
betes, and many types of cancer. © Ac-
cording to the Coalition for the Ad-
vancement of Medical Research, there
are at least 58 diseases which could po-
tentially be cured through stem cell re-
search.
That is why more than 200 major
patient groups, scientists, and medical
research groups and 80 Nobel Laure-
ates support the Stem Cell Research En-
hancement Act. OO They know that this
legislation will give us a chance to find
cures to diseases affecting 100 million
Americans.
I want to make clear that I oppose re-
productive cloning, as we all do. I have
voted against it in the past. 0 However,
that is vastly differentfrom stem cell re-
search and as an ovarian cancer sur-
vivor, I am notgoing to stand in the way
of science.
</bodyText>
<note confidence="0.4461415">
Permitting peer-reviewed Federal
funds to be used for this research,
</note>
<figureCaption confidence="0.756384125">
combined with public oversight of these
activities, is our best assurance that
research will be of the highest quality
and performed with the greatest dignity
and moral responsibility. The policy
President Bush announced in August
2001 has limited access to stem cell
lines and has stalled scientific progress.
</figureCaption>
<bodyText confidence="0.991824275862069">
As a cancer survivor, I know the des-
peration these families feel as they wait
for a cure. ® This congress must not
stand in the way of that progress. 0 We
have an opportunity to change the lives
of millions, and I hope we take it. 0 I
urge my colleagues to support this leg-
islation.
careful tuning of the size constraint.
Examining an example prediction. Our pro-
posed methods are not designed to extract inter-
pretable explanations, but examining the extracted
explanations might still yield meaningful informa-
tion. Table 4 contains an example speech from the
U.S. Congressional Floor Debates test set, with La-
tent Explanations found by SVM���
�� highlighted in
boldface. This speech was made in support of the
Stem Cell Research Enhancement Act. For com-
parison, Table 4 also shows the five least subjective
sentences according to SVM���
�� . Notice that most of
these “objective” sentences can plausibly belong to
speeches made in opposition to bills that limit stem
cell research funding. That is, they do not clearly in-
dicate the speaker’s stance towards the specific bill
in question. We can thus see that our approach can
indeed learn to infer sentences that are essential to
understanding the document-level sentiment.
</bodyText>
<sectionHeader confidence="0.999837" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999946032258064">
Making good structural assumptions simplifies the
development process. Compared to methods that
modify the training of flat document classifiers (e.g.,
Zaidan et al. (2007)), our approach uses fewer pa-
rameters, leading to a more compact and faster train-
ing stage. Compared to methods that use a cascaded
approach (e.g., Pang and Lee (2004)), our approach
is more robust to errors in the lower-level subtask
due to being a joint model.
Introducing latent variables makes the training
procedure more flexible by not requiring lower-level
labels, but does require a good initialization (i.e., a
reasonable substitute for the lower-level labels). We
believe that the widespread availability of off-the-
shelf sentiment lexicons and software, despite being
developed for a different domain, makes this issue
less of a concern, and in fact creates an opportunity
for approaches like ours to have real impact.
One can incorporate many types of sentence-level
information that cannot be directly incorporated into
a flat model. Examples include scores from another
sentence-level classifier (e.g., from Nakagawa et. al
(2010)) or combining phrase-level polarity scores
(e.g., from Choi and Cardie (2008)) for each sen-
tence, or features that describe the position of the
sentence in the document.
Most prior work on the U.S. Congressional Floor
Debates dataset focused on using relationships be-
tween speakers such as agreement (Thomas et al.,
2006; Bansal et al., 2008), and used a global min-
cut inference procedure. However, they require all
</bodyText>
<page confidence="0.988593">
1054
</page>
<bodyText confidence="0.99998528125">
test instances to be known in advance (i.e., their for-
mulations are transductive). Our method is not lim-
ited to the transductive setting, and instead exploits
a different and complementary structure: the latent
explanation (i.e., only some sentences in the speech
are indicative of the speaker’s vote).
In a sense, the joint feature structure used in
our model is the simplest that could be used. Our
model makes no explicit structural dependencies be-
tween sentences, so the choice of whether to extract
each sentence is essentially made independently of
other sentences in the document. More sophisticated
structures can be used if appropriate. For instance,
one can formulate the sentence extraction task as
a sequence labeling problem similar to (McDonald
et al., 2007), or use a more expressive graphical
model such as in (Pang and Lee, 2004; Thomas et
al., 2006). So long as the global inference proce-
dure is tractable or has a good approximation al-
gorithm, then the training procedure is guaranteed
to converge with rigorous generalization guarantees
(Finley and Joachims, 2008). Since any formulation
of the extraction subtask will suppress information
for the main document-level task, one must take care
to properly incorporate smoothing if necessary.
Another interesting direction is training models to
predict not only sentiment polarity, but also whether
a document is objective. For example, one can pose
a three class problem (“positive”, “negative”, “ob-
jective”), where objective documents might not nec-
essarily have a good set of (subjective) explanatory
sentences, similar to (Chang et al., 2010).
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977125">
We have presented latent variable structured mod-
els for the document sentiment classification task.
These models do not rely on sentence-level an-
notations, and are trained jointly (over both the
document and sentence levels) to directly optimize
document-level accuracy. Experiments on two stan-
dard sentiment analysis datasets showed improved
performance over previous results.
Our approach can, in principle, be applied to any
classification task that is well modeled by jointly
solving an extraction subtask. However, as evi-
denced by our experiments, proper training does re-
quire a reasonable initial guess of the extracted ex-
planations, as well as ways to mitigate the risk of
the extraction subtask suppressing too much infor-
mation (such as via feature smoothing).
</bodyText>
<sectionHeader confidence="0.996666" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<reference confidence="0.679431222222222">
This work was supported in part by National Science
Foundation Grants BCS-0904822, BCS-0624277, IIS-
0535099; by a gift from Google; and by the Department
of Homeland Security under ONR Grant N0014-07-1-
0152. The second author was also supported in part by
a Microsoft Research Graduate Fellowship. The authors
thank Yejin Choi, Thorsten Joachims, Nikos Karampatzi-
akis, Lillian Lee, Chun-Nam Yu, and the anonymous re-
viewers for their helpful comments.
</reference>
<sectionHeader confidence="0.873071" genericHeader="conclusions">
Appendix
</sectionHeader>
<bodyText confidence="0.524997666666667">
Recall that all the ψsubj and ψpol vectors have unit 2-
norm, which is assumed here to be desirable. We now
p
show that using N(x) = f(|x|) achieves a similar
property for Ψ(x, y, s). We can write the squared 2-norm
of Ψ(x, y, s) as
</bodyText>
<equation confidence="0.844450666666667">
 |Ψ (x, y, s)|2 = ⎡1
2 y ψpol(xj) + ψsubj(xj)
N(x) jEs
</equation>
<bodyText confidence="0.981191">
where the last equality follows from the fact that
</bodyText>
<equation confidence="0.755799">
ψpol(xj)T ψsubj(xj) = 0,
</equation>
<bodyText confidence="0.998999666666667">
due to the two vectors using disjoint feature spaces by
construction. The summation of the ψpol(xj) terms is
written as
</bodyText>
<equation confidence="0.9846736">
ψpol(xj)Tψpol(xj) (8)
1 ≤ f(|x|),
⎤2
⎦
2
(jEs
jEs iEs
X=
jEs
ψpol(xj)Tψpol(xi)
</equation>
<bodyText confidence="0.91927">
where (8) follows from the sparsity assumption that
</bodyText>
<equation confidence="0.913591">
∀i =6 j : ψpol(xj)T ψpol(xi) ≈ 0.
</equation>
<bodyText confidence="0.563875333333333">
A similar argument applies for the ψsubj(xj) terms.
p
Thus, by choosing N(x) = f(|x|) the joint feature
</bodyText>
<footnote confidence="0.4749775">
vectors Ψ(x, y, s) will have approximately equal magni-
tude as measured using the 2-norm.
</footnote>
<figure confidence="0.997668083333333">
⎡
⎢ ⎣
1
=
f(|x|)
⎛ ⎞X ψpol (xj ) + X ψsubj (xj)
jEs (jEs
2
⎞ 2⎤
⎠ ⎦ ,
⎥
X≈jEs
</figure>
<page confidence="0.982377">
1055
</page>
<sectionHeader confidence="0.994426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999717686746988">
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework. In In-
ternational Conference on Computational Linguistics
(COLING).
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative learning over con-
strained latent representations. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Empirical Methods in
Natural Language Processing (EMNLP).
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world’s response. In ACL Conference on Natural Lan-
guage Learning (CoNLL), July.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. 2008. A discriminatively trained, multiscale,
deformable part model. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR).
Thomas Finley and Thorsten Joachims. 2008. Train-
ing structural svms when exact inference is intractable.
In International Conference on Machine Learning
(ICML).
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu.
2009. Cutting plane training of structural svms. Ma-
chine Learning, 77(1):27–59.
Yi Mao and Guy Lebanon. 2006. Isotonic conditional
random fields and local sentiment flow. In Neural In-
formation Processing Systems (NIPS).
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using crfs with hidden variables. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL).
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Empirical Methods in
Natural Language Processing (EMNLP).
Slav Petrov and Dan Klein. 2007. Discriminative log-
linear grammars with latent variables. In Neural In-
formation Processing Systems (NIPS).
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Empirical
Methods in Natural Language Processing (EMNLP).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Empirical Methods in Natural
Language Processing (EMNLP).
Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010.
Automatically generating annotator rationales to im-
prove sentiment classification. In Annual Meeting of
the Association for Computational Linguistics (ACL).
Chun-Nam Yu and Thorsten Joachims. 2009. Learning
structural svms with latent variables. In International
Conference on Machine Learning (ICML).
Alan L. Yuille and Anand Rangarajan. 2003. The
concave-convex procedure. Neural Computation,
15(4):915–936, April.
Omar F. Zaidan and Jason Eisner. 2008. Modeling an-
notators: a generative approach to learning from an-
notator rationales. In Empirical Methods in Natural
Language Processing (EMNLP).
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2007. Using “annotator rationales” to improve ma-
chine learning for text categorization. In Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL).
</reference>
<page confidence="0.993176">
1056
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.084544">
<title confidence="0.99934">Multi-level Structured Models for Document-level Sentiment Classification</title>
<author confidence="0.819745">Ainur</author>
<affiliation confidence="0.9619565">Dept. of Computer Cornell</affiliation>
<address confidence="0.705847">Ithaca, NY,</address>
<email confidence="0.999273">ainur@cs.cornell.edu</email>
<author confidence="0.587272">Yisong</author>
<affiliation confidence="0.9687945">Dept. of Computer Cornell</affiliation>
<address confidence="0.596224">Ithaca, NY,</address>
<email confidence="0.999428">yyue@cs.cornell.edu</email>
<author confidence="0.917335">Claire</author>
<affiliation confidence="0.967427">Dept. of Computer Cornell</affiliation>
<address confidence="0.690169">Ithaca, NY,</address>
<email confidence="0.999689">cardie@cs.cornell.edu</email>
<abstract confidence="0.993280608695652">In this paper, we investigate structured models for document-level sentiment classification. When predicting the sentiment of a subjective document (e.g., as positive or negative), it is well known that not all sentences are equally discriminative or informative. But identifying the useful sentences automatically is itself a difficult learning problem. This paper proposes a joint two-level approach for document-level sentiment classification that simultaneously extracts useful (i.e., subjective) sentences and predicts document-level sentiment based on the extracted sentences. Unlike previous joint learning methods for the task, our approach (1) does not rely on gold standard sentence-level subjectivity annotations (which may be expensive to obtain), and (2) optimizes directly for document-level performance. Empirical evaluations on movie reviews and U.S. Congressional floor debates show improved performance over previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work was supported in part by National Science Foundation Grants BCS-0904822, BCS-0624277, IIS0535099; by a gift from Google; and by the Department of Homeland Security under ONR Grant N0014-07-1-0152. The second author was also supported in part by a Microsoft Research Graduate Fellowship. The authors thank Yejin Choi, Thorsten Joachims, Nikos Karampatziakis, Lillian Lee, Chun-Nam Yu, and the anonymous reviewers for their helpful comments.</title>
<marker></marker>
<rawString>This work was supported in part by National Science Foundation Grants BCS-0904822, BCS-0624277, IIS0535099; by a gift from Google; and by the Department of Homeland Security under ONR Grant N0014-07-1-0152. The second author was also supported in part by a Microsoft Research Graduate Fellowship. The authors thank Yejin Choi, Thorsten Joachims, Nikos Karampatziakis, Lillian Lee, Chun-Nam Yu, and the anonymous reviewers for their helpful comments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Claire Cardie</author>
<author>Lillian Lee</author>
</authors>
<title>The power of negative thinking: Exploiting label disagreement in the min-cut classification framework.</title>
<date>2008</date>
<booktitle>In International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="28283" citStr="Bansal et al. (2008)" startWordPosition="4509" endWordPosition="4512">when human annotations are available (Annot. Labels), and when they are unavailable (No Annot. Labels). METHOD SVM Zaidan et al. (2007) SVM��� �� SVM��� �� + Prox.Feat. Baseline Annot. Labels ACC 88.56 92.20 92.28 93.22 No Annot. Yessenalina et al. (2010) 91.78 Labels SVM��� Figure 1: Overlap of extracted sentences from different �� 92.50 SVM��� 92.39 SV�ls models on the Movie Reviews training set. �� +Prox.Feat. Table 3: Comparison of SVMsle fs with previous work on the U.S. Congressional Floor Debates dataset for the speaker-based segment classification task. METHOD SVM Thomas et al. (2006) Bansal et al. (2008) SVM��� �� SVM��� �� + Prox.Feat. Tables 2 and 3 show a comparison of SVM��� �� with previous work on the Movie Reviews and U.S. Congressional Floor Debates datasets, respectively. For the Movie Reviews dataset, we considered two settings: when human annotations are available, and when they are not (in which case we initialized using OpinionFinder). For the U.S. Congressional Floor Debates dataset we used only the latter setting, since there are no annotations available for this dataset. In all cases we observe SVM��� �� showing improved performance compared to previous results. Training detai</context>
<context position="35725" citStr="Bansal et al., 2008" startWordPosition="5727" endWordPosition="5730">s an opportunity for approaches like ours to have real impact. One can incorporate many types of sentence-level information that cannot be directly incorporated into a flat model. Examples include scores from another sentence-level classifier (e.g., from Nakagawa et. al (2010)) or combining phrase-level polarity scores (e.g., from Choi and Cardie (2008)) for each sentence, or features that describe the position of the sentence in the document. Most prior work on the U.S. Congressional Floor Debates dataset focused on using relationships between speakers such as agreement (Thomas et al., 2006; Bansal et al., 2008), and used a global mincut inference procedure. However, they require all 1054 test instances to be known in advance (i.e., their formulations are transductive). Our method is not limited to the transductive setting, and instead exploits a different and complementary structure: the latent explanation (i.e., only some sentences in the speech are indicative of the speaker’s vote). In a sense, the joint feature structure used in our model is the simplest that could be used. Our model makes no explicit structural dependencies between sentences, so the choice of whether to extract each sentence is </context>
</contexts>
<marker>Bansal, Cardie, Lee, 2008</marker>
<rawString>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
<author>Vivek Srikumar</author>
</authors>
<title>Discriminative learning over constrained latent representations.</title>
<date>2010</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="6866" citStr="Chang et al. (2010)" startWordPosition="974" endWordPosition="977">ce-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g., Petrov and Klein (2007), Chang et al. (2010), Clarke et al. (2010)) and other fields (e.g., Felzenszwalb et al. (2008), Yu and Joachims (2009)) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtas</context>
<context position="37417" citStr="Chang et al., 2010" startWordPosition="5991" endWordPosition="5994">guaranteed to converge with rigorous generalization guarantees (Finley and Joachims, 2008). Since any formulation of the extraction subtask will suppress information for the main document-level task, one must take care to properly incorporate smoothing if necessary. Another interesting direction is training models to predict not only sentiment polarity, but also whether a document is objective. For example, one can pose a three class problem (“positive”, “negative”, “objective”), where objective documents might not necessarily have a good set of (subjective) explanatory sentences, similar to (Chang et al., 2010). 7 Conclusion We have presented latent variable structured models for the document sentiment classification task. These models do not rely on sentence-level annotations, and are trained jointly (over both the document and sentence levels) to directly optimize document-level accuracy. Experiments on two standard sentiment analysis datasets showed improved performance over previous results. Our approach can, in principle, be applied to any classification task that is well modeled by jointly solving an extraction subtask. However, as evidenced by our experiments, proper training does require a r</context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek Srikumar. 2010. Discriminative learning over constrained latent representations. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7640" citStr="Choi and Cardie, 2008" startWordPosition="1093" endWordPosition="1096">that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to sentiment analysis, but they were focused on classifying either phrase-level (Choi and Cardie, 2008) or sentence-level polarity (Nakagawa et al., 2010). 3 Extracting Hidden Explanations In this paper, we take the view that each document has a subset of sentences that best explains its sentiment. Consider the “annotator rationales” generated by human judges for the movie reviews dataset (Zaidan et al., 2007). Each rationale is a text span that was identified to support (or explain) its parent document’s sentiment. Thus, these rationales can be interpreted as (something close to) a ground truth labeling of the explanatory segments. Using a dataset where each document contains only its rational</context>
<context position="35460" citStr="Choi and Cardie (2008)" startWordPosition="5683" endWordPosition="5686">tialization (i.e., a reasonable substitute for the lower-level labels). We believe that the widespread availability of off-theshelf sentiment lexicons and software, despite being developed for a different domain, makes this issue less of a concern, and in fact creates an opportunity for approaches like ours to have real impact. One can incorporate many types of sentence-level information that cannot be directly incorporated into a flat model. Examples include scores from another sentence-level classifier (e.g., from Nakagawa et. al (2010)) or combining phrase-level polarity scores (e.g., from Choi and Cardie (2008)) for each sentence, or features that describe the position of the sentence in the document. Most prior work on the U.S. Congressional Floor Debates dataset focused on using relationships between speakers such as agreement (Thomas et al., 2006; Bansal et al., 2008), and used a global mincut inference procedure. However, they require all 1054 test instances to be known in advance (i.e., their formulations are transductive). Our method is not limited to the transductive setting, and instead exploits a different and complementary structure: the latent explanation (i.e., only some sentences in the</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In ACL Conference on Natural Language Learning (CoNLL),</booktitle>
<contexts>
<context position="6888" citStr="Clarke et al. (2010)" startWordPosition="978" endWordPosition="981">affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g., Petrov and Klein (2007), Chang et al. (2010), Clarke et al. (2010)) and other fields (e.g., Felzenszwalb et al. (2008), Yu and Joachims (2009)) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers h</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In ACL Conference on Natural Language Learning (CoNLL), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Felzenszwalb</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>A discriminatively trained, multiscale, deformable part model.</title>
<date>2008</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="6940" citStr="Felzenszwalb et al. (2008)" startWordPosition="986" endWordPosition="989">-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g., Petrov and Klein (2007), Chang et al. (2010), Clarke et al. (2010)) and other fields (e.g., Felzenszwalb et al. (2008), Yu and Joachims (2009)) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to </context>
</contexts>
<marker>Felzenszwalb, McAllester, Ramanan, 2008</marker>
<rawString>Pedro Felzenszwalb, David McAllester, and Deva Ramanan. 2008. A discriminatively trained, multiscale, deformable part model. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Finley</author>
<author>Thorsten Joachims</author>
</authors>
<title>Training structural svms when exact inference is intractable.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="36888" citStr="Finley and Joachims, 2008" startWordPosition="5913" endWordPosition="5916">entences, so the choice of whether to extract each sentence is essentially made independently of other sentences in the document. More sophisticated structures can be used if appropriate. For instance, one can formulate the sentence extraction task as a sequence labeling problem similar to (McDonald et al., 2007), or use a more expressive graphical model such as in (Pang and Lee, 2004; Thomas et al., 2006). So long as the global inference procedure is tractable or has a good approximation algorithm, then the training procedure is guaranteed to converge with rigorous generalization guarantees (Finley and Joachims, 2008). Since any formulation of the extraction subtask will suppress information for the main document-level task, one must take care to properly incorporate smoothing if necessary. Another interesting direction is training models to predict not only sentiment polarity, but also whether a document is objective. For example, one can pose a three class problem (“positive”, “negative”, “objective”), where objective documents might not necessarily have a good set of (subjective) explanatory sentences, similar to (Chang et al., 2010). 7 Conclusion We have presented latent variable structured models for </context>
</contexts>
<marker>Finley, Joachims, 2008</marker>
<rawString>Thomas Finley and Thorsten Joachims. 2008. Training structural svms when exact inference is intractable. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Thomas Finley</author>
<author>Chun-Nam Yu</author>
</authors>
<title>Cutting plane training of structural svms.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="14583" citStr="Joachims et al., 2009" startWordPosition="2280" endWordPosition="2283">sk perfectly, and also does not require precise sentence-level training labels. In other words, our goal is to learn to identify the informative (subjective) sentences that best explain the training labels to the extent required for good document classification performance. OP 1 is non-convex because of the constraints (7). To solve OP 1, we use the combination of the CCCP algorithm (Yuille and Rangarajan, 2003) with cutting plan where si is some fixed explanation (e.g., an initial guess of the best explanation). Then OP 1 reduces to a standard structural SVM, which can be solved efficiently (Joachims et al., 2009). Algorithm 2 describes our training procedure. Starting with an initial guess si for each training example, the training procedure alternates between solving an instance of the resulting structural SVM (called SSVMSolve in Algorithm 2) using the currently best known explanations si (Line 9), and making a new guess of the best explanations (Line 7). Yu and Joachims (2009) showed that this alternating procedure for training latent variable structural SVMs is an instance of the CCCP procedure (Yuille and Rangarajan, 2003), and so is guaranteed to converge to a local optimum. For our experiments,</context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>Thorsten Joachims, Thomas Finley, and Chun-Nam Yu. 2009. Cutting plane training of structural svms. Machine Learning, 77(1):27–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Mao</author>
<author>Guy Lebanon</author>
</authors>
<title>Isotonic conditional random fields and local sentiment flow.</title>
<date>2006</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2742" citStr="Mao and Lebanon (2006)" startWordPosition="376" endWordPosition="379">n employed conventional machine learning techniques for text categorization (Pang et al., 2002). These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subjectivity information, existing techniques have some undesirable properties. First, they typically require gold standard sentence-level annotations (McDonald et al. (2007), Mao and Lebanon (2006)). But the cost of acquiring such labels can be prohibitive. Second, some solutions for incorporating sentencelevel information lack mechanisms for controlling how errors propagate from the subjective sentence identification subtask to the main document classification task (Pang and Lee, 2004). Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document- and sentence-level classification accuracy (McDonald et al., 2007). Optimizing for this compromise, when the real goal is to maximize only the document-leve</context>
<context position="5153" citStr="Mao and Lebanon (2006)" startWordPosition="727" endWordPosition="730"> debates datasets and close with discussion and conclusions. 2 Related Work Pang and Lee (2004) first showed that sentencelevel extraction can improve document-level performance. They used a cascaded approach by first filtering out objective sentences and performing subjectivity extractions using a global min-cut inference. Afterward, the subjective extracts were converted into inputs for the document-level sentiment classifier. One advantage of their approach is that it avoids the need for explicit subjectivity annotations. However, like other cascaded approaches (e.g., Thomas et al. (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance, Zaidan et al. (2007) used human annotators to mark the “annotator rationales”, which are text spans that support the document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifyi</context>
</contexts>
<marker>Mao, Lebanon, 2006</marker>
<rawString>Yi Mao and Guy Lebanon. 2006. Isotonic conditional random fields and local sentiment flow. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2718" citStr="McDonald et al. (2007)" startWordPosition="371" endWordPosition="375"> sentiment classification employed conventional machine learning techniques for text categorization (Pang et al., 2002). These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subjectivity information, existing techniques have some undesirable properties. First, they typically require gold standard sentence-level annotations (McDonald et al. (2007), Mao and Lebanon (2006)). But the cost of acquiring such labels can be prohibitive. Second, some solutions for incorporating sentencelevel information lack mechanisms for controlling how errors propagate from the subjective sentence identification subtask to the main document classification task (Pang and Lee, 2004). Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document- and sentence-level classification accuracy (McDonald et al., 2007). Optimizing for this compromise, when the real goal is to maximiz</context>
<context position="6168" citStr="McDonald et al. (2007)" startWordPosition="878" endWordPosition="881">he document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document. Yessenalina et al. (2010) extended this approach to use automatically generated rationales. 1http://projects.yisongyue.com/svmsle/ A natural approach to avoid the pitfalls associated with cascaded methods is to use joint twolevel models that simultaneously solve the sentencelevel and document-level tasks (e.g., McDonald et al. (2007), Zaidan and Eisner (2008)). Since these models are trained jointly, the sentence-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, resea</context>
<context position="18218" citStr="McDonald et al. (2007)" startWordPosition="2861" endWordPosition="2864"> to group together). To exploit this structure, we will expand the feature space of ψsubj to include both the words of the current and previous sentence as follows, � � ψsubj(xj) ψsubj(x, j) = . ψsubj(xj`) The corresponding weight vector can be written as � �i wsubj =. ~~wsubj wprevSubj By adding these features, we are essentially assuming that the words of the previous sentence are predictive of the subjectivity of the current sentence. Alternative approaches include explicitly accounting for this structure by treating subjective sentence extraction as a sequence-labeling problem, such as in McDonald et al. (2007). Such structure formulations can be naturally encoded in the joint feature map. Note that the inference procedure in Algorthm 1 is still tractable, since it reduces to comparing the best sequence of subjective/objective sentences that explains a positive sentiment versus the best sequence that explains a negative sentiment. For this study, we chose not to examine this more expressive yet more complex structure. 4.5 Extensions Though our initial model (3) is simple and intuitive, performance can depend heavily on the quality of latent variable initialization and the quality of the feature stru</context>
<context position="36576" citStr="McDonald et al., 2007" startWordPosition="5862" endWordPosition="5865">exploits a different and complementary structure: the latent explanation (i.e., only some sentences in the speech are indicative of the speaker’s vote). In a sense, the joint feature structure used in our model is the simplest that could be used. Our model makes no explicit structural dependencies between sentences, so the choice of whether to extract each sentence is essentially made independently of other sentences in the document. More sophisticated structures can be used if appropriate. For instance, one can formulate the sentence extraction task as a sequence labeling problem similar to (McDonald et al., 2007), or use a more expressive graphical model such as in (Pang and Lee, 2004; Thomas et al., 2006). So long as the global inference procedure is tractable or has a good approximation algorithm, then the training procedure is guaranteed to converge with rigorous generalization guarantees (Finley and Joachims, 2008). Since any formulation of the extraction subtask will suppress information for the main document-level task, one must take care to properly incorporate smoothing if necessary. Another interesting direction is training models to predict not only sentiment polarity, but also whether a doc</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using crfs with hidden variables.</title>
<date>2010</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="7691" citStr="Nakagawa et al., 2010" startWordPosition="1100" endWordPosition="1103">along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to sentiment analysis, but they were focused on classifying either phrase-level (Choi and Cardie, 2008) or sentence-level polarity (Nakagawa et al., 2010). 3 Extracting Hidden Explanations In this paper, we take the view that each document has a subset of sentences that best explains its sentiment. Consider the “annotator rationales” generated by human judges for the movie reviews dataset (Zaidan et al., 2007). Each rationale is a text span that was identified to support (or explain) its parent document’s sentiment. Thus, these rationales can be interpreted as (something close to) a ground truth labeling of the explanatory segments. Using a dataset where each document contains only its rationales, 1047 Algorithm 1 Inference Algorithm for (2) 1:</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using crfs with hidden variables. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1951" citStr="Pang and Lee, 2004" startWordPosition="265" endWordPosition="268">n Sentiment classification is a well-studied and active research area (Pang and Lee, 2008). One of the main challenges for document-level sentiment categorization is that not every part of the document is equally informative for inferring the sentiment of the whole document. Objective statements interleaved with the subjective statements can be confusing for learning methods, and subjective statements with conflicting sentiment further complicate the document categorization task. For example, authors of movie reviews often devote large sections to (largely objective) descriptions of the plot (Pang and Lee, 2004). In addition, an overall positive review might still include some negative opinions about an actor or the plot. Early research on document-level sentiment classification employed conventional machine learning techniques for text categorization (Pang et al., 2002). These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subject</context>
<context position="4626" citStr="Pang and Lee (2004)" startWordPosition="650" endWordPosition="653">) sentences that best “explain” it, thus controlling the propagation of incorrect sentence labels. And by directly optimizing for document-level accuracy, our model learns to solve the sentence extraction subtask only to the extent required for accurately classifying document sentiment. A software implementation of our method is also publicly available.1 For the rest of the paper, we will discuss related work, motivate and describe our model, present an empirical evaluation on movie reviews and U.S. Congressional floor debates datasets and close with discussion and conclusions. 2 Related Work Pang and Lee (2004) first showed that sentencelevel extraction can improve document-level performance. They used a cascaded approach by first filtering out objective sentences and performing subjectivity extractions using a global min-cut inference. Afterward, the subjective extracts were converted into inputs for the document-level sentiment classifier. One advantage of their approach is that it avoids the need for explicit subjectivity annotations. However, like other cascaded approaches (e.g., Thomas et al. (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-</context>
<context position="21970" citStr="Pang and Lee (2004)" startWordPosition="3479" endWordPosition="3482">is model via OP 1 achieves that is (1) used to model the polarity of the entire document, and (2) used to compute a smoothed estimate of the polarity of the extracted sentences. This satisfies all three properties (A), (B), an Training ~wdoc d (C), although other approaches are also possible. 5 Experiments 5.1 Experimental Setup We evaluate our methods using the Movie Reviews and U.S. Congressional Floor Debates datasets, following the setup used in previous work for comparison purposes.3 Movie Reviews. We use the movie reviews dataset from Zaidan et al. (2007) that was originally released by Pang and Lee (2004). This version contains annotated rationales for each review, which we use to generate an additional initialization during training (described below). We follow exactly the experimental setup used in Zaidan et al. (2007).4 U.S. Congressional Floor Debates. We also use the U.S. Congressional floor debates transcripts from Thomas et al. (2006). The data was extracted from GovTrack (http://govtrack.us), which has all available transcripts of U.S. floor debates in the House of Representatives in 2005. As in previous work, only debates with discussions of “controversial” bills were considered (wher</context>
<context position="34606" citStr="Pang and Lee (2004)" startWordPosition="5556" endWordPosition="5559">imit stem cell research funding. That is, they do not clearly indicate the speaker’s stance towards the specific bill in question. We can thus see that our approach can indeed learn to infer sentences that are essential to understanding the document-level sentiment. 6 Discussion Making good structural assumptions simplifies the development process. Compared to methods that modify the training of flat document classifiers (e.g., Zaidan et al. (2007)), our approach uses fewer parameters, leading to a more compact and faster training stage. Compared to methods that use a cascaded approach (e.g., Pang and Lee (2004)), our approach is more robust to errors in the lower-level subtask due to being a joint model. Introducing latent variables makes the training procedure more flexible by not requiring lower-level labels, but does require a good initialization (i.e., a reasonable substitute for the lower-level labels). We believe that the widespread availability of off-theshelf sentiment lexicons and software, despite being developed for a different domain, makes this issue less of a concern, and in fact creates an opportunity for approaches like ours to have real impact. One can incorporate many types of sent</context>
<context position="36649" citStr="Pang and Lee, 2004" startWordPosition="5876" endWordPosition="5879">., only some sentences in the speech are indicative of the speaker’s vote). In a sense, the joint feature structure used in our model is the simplest that could be used. Our model makes no explicit structural dependencies between sentences, so the choice of whether to extract each sentence is essentially made independently of other sentences in the document. More sophisticated structures can be used if appropriate. For instance, one can formulate the sentence extraction task as a sequence labeling problem similar to (McDonald et al., 2007), or use a more expressive graphical model such as in (Pang and Lee, 2004; Thomas et al., 2006). So long as the global inference procedure is tractable or has a good approximation algorithm, then the training procedure is guaranteed to converge with rigorous generalization guarantees (Finley and Joachims, 2008). Since any formulation of the extraction subtask will suppress information for the main document-level task, one must take care to properly incorporate smoothing if necessary. Another interesting direction is training models to predict not only sentiment polarity, but also whether a document is objective. For example, one can pose a three class problem (“pos</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1422" citStr="Pang and Lee, 2008" startWordPosition="188" endWordPosition="191">classification that simultaneously extracts useful (i.e., subjective) sentences and predicts document-level sentiment based on the extracted sentences. Unlike previous joint learning methods for the task, our approach (1) does not rely on gold standard sentence-level subjectivity annotations (which may be expensive to obtain), and (2) optimizes directly for document-level performance. Empirical evaluations on movie reviews and U.S. Congressional floor debates show improved performance over previous approaches. 1 Introduction Sentiment classification is a well-studied and active research area (Pang and Lee, 2008). One of the main challenges for document-level sentiment categorization is that not every part of the document is equally informative for inferring the sentiment of the whole document. Objective statements interleaved with the subjective statements can be confusing for learning methods, and subjective statements with conflicting sentiment further complicate the document categorization task. For example, authors of movie reviews often devote large sections to (largely objective) descriptions of the plot (Pang and Lee, 2004). In addition, an overall positive review might still include some nega</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2215" citStr="Pang et al., 2002" startWordPosition="303" endWordPosition="306"> document. Objective statements interleaved with the subjective statements can be confusing for learning methods, and subjective statements with conflicting sentiment further complicate the document categorization task. For example, authors of movie reviews often devote large sections to (largely objective) descriptions of the plot (Pang and Lee, 2004). In addition, an overall positive review might still include some negative opinions about an actor or the plot. Early research on document-level sentiment classification employed conventional machine learning techniques for text categorization (Pang et al., 2002). These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subjectivity information, existing techniques have some undesirable properties. First, they typically require gold standard sentence-level annotations (McDonald et al. (2007), Mao and Lebanon (2006)). But the cost of acquiring such labels can be prohibitive. Second, some</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative loglinear grammars with latent variables.</title>
<date>2007</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="6845" citStr="Petrov and Klein (2007)" startWordPosition="970" endWordPosition="973">ained jointly, the sentence-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g., Petrov and Klein (2007), Chang et al. (2010), Clarke et al. (2010)) and other fields (e.g., Felzenszwalb et al. (2008), Yu and Joachims (2009)) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for t</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Discriminative loglinear grammars with latent variables. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5129" citStr="Thomas et al. (2006)" startWordPosition="723" endWordPosition="726">S. Congressional floor debates datasets and close with discussion and conclusions. 2 Related Work Pang and Lee (2004) first showed that sentencelevel extraction can improve document-level performance. They used a cascaded approach by first filtering out objective sentences and performing subjectivity extractions using a global min-cut inference. Afterward, the subjective extracts were converted into inputs for the document-level sentiment classifier. One advantage of their approach is that it avoids the need for explicit subjectivity annotations. However, like other cascaded approaches (e.g., Thomas et al. (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance, Zaidan et al. (2007) used human annotators to mark the “annotator rationales”, which are text spans that support the document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is les</context>
<context position="22313" citStr="Thomas et al. (2006)" startWordPosition="3530" endWordPosition="3533"> our methods using the Movie Reviews and U.S. Congressional Floor Debates datasets, following the setup used in previous work for comparison purposes.3 Movie Reviews. We use the movie reviews dataset from Zaidan et al. (2007) that was originally released by Pang and Lee (2004). This version contains annotated rationales for each review, which we use to generate an additional initialization during training (described below). We follow exactly the experimental setup used in Zaidan et al. (2007).4 U.S. Congressional Floor Debates. We also use the U.S. Congressional floor debates transcripts from Thomas et al. (2006). The data was extracted from GovTrack (http://govtrack.us), which has all available transcripts of U.S. floor debates in the House of Representatives in 2005. As in previous work, only debates with discussions of “controversial” bills were considered (where the losing side had at least 20% of the speeches). The goal is to predict the vote (“yea” or “nay”) for the speaker of each speech segment. For our experiments, we evaluate our methods using the speakerbased speech-segment classification setting as described in Thomas et al. (2006).5 3Datasets in the required format for SVM&amp;quot;&apos; are available</context>
<context position="28262" citStr="Thomas et al. (2006)" startWordPosition="4505" endWordPosition="4508">idered two settings: when human annotations are available (Annot. Labels), and when they are unavailable (No Annot. Labels). METHOD SVM Zaidan et al. (2007) SVM��� �� SVM��� �� + Prox.Feat. Baseline Annot. Labels ACC 88.56 92.20 92.28 93.22 No Annot. Yessenalina et al. (2010) 91.78 Labels SVM��� Figure 1: Overlap of extracted sentences from different �� 92.50 SVM��� 92.39 SV�ls models on the Movie Reviews training set. �� +Prox.Feat. Table 3: Comparison of SVMsle fs with previous work on the U.S. Congressional Floor Debates dataset for the speaker-based segment classification task. METHOD SVM Thomas et al. (2006) Bansal et al. (2008) SVM��� �� SVM��� �� + Prox.Feat. Tables 2 and 3 show a comparison of SVM��� �� with previous work on the Movie Reviews and U.S. Congressional Floor Debates datasets, respectively. For the Movie Reviews dataset, we considered two settings: when human annotations are available, and when they are not (in which case we initialized using OpinionFinder). For the U.S. Congressional Floor Debates dataset we used only the latter setting, since there are no annotations available for this dataset. In all cases we observe SVM��� �� showing improved performance compared to previous re</context>
<context position="35703" citStr="Thomas et al., 2006" startWordPosition="5723" endWordPosition="5726">n, and in fact creates an opportunity for approaches like ours to have real impact. One can incorporate many types of sentence-level information that cannot be directly incorporated into a flat model. Examples include scores from another sentence-level classifier (e.g., from Nakagawa et. al (2010)) or combining phrase-level polarity scores (e.g., from Choi and Cardie (2008)) for each sentence, or features that describe the position of the sentence in the document. Most prior work on the U.S. Congressional Floor Debates dataset focused on using relationships between speakers such as agreement (Thomas et al., 2006; Bansal et al., 2008), and used a global mincut inference procedure. However, they require all 1054 test instances to be known in advance (i.e., their formulations are transductive). Our method is not limited to the transductive setting, and instead exploits a different and complementary structure: the latent explanation (i.e., only some sentences in the speech are indicative of the speaker’s vote). In a sense, the joint feature structure used in our model is the simplest that could be used. Our model makes no explicit structural dependencies between sentences, so the choice of whether to ext</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="15491" citStr="Wilson et al., 2005" startWordPosition="2427" endWordPosition="2430">e 9), and making a new guess of the best explanations (Line 7). Yu and Joachims (2009) showed that this alternating procedure for training latent variable structural SVMs is an instance of the CCCP procedure (Yuille and Rangarajan, 2003), and so is guaranteed to converge to a local optimum. For our experiments, we do not train until convergence, but instead use performance on a validation set to choose the halting iteration. Since OP 1 is nonconvex, agood initialization is necessary. To generate the initial explanations, one can use an off-theshelf sentiment classifier such as OpinionFinder2 (Wilson et al., 2005). For some datasets, there exist documents with annotated sentences, which we e training of structural SVMs (Joachims et al., 2009), as proposed in Yu and Joachims (2009). Suppose each constraint (7) is replaced by 2http://www.cs.pitt.edu/mpqa/ opinionfinderrelease/ N 2 k~wk2 + 1 N ξi (6) min ~w,ξ&gt;0 i=1 max sESi can treat either as the ground truth or another (very good) initial guess of the explanatory sentences. 4.3 Feature Representation Like any machine learning approach, we must specify a useful set of features for the ψ vectors described above. We will consider two types of features. Bag</context>
<context position="25012" citStr="Wilson et al., 2005" startWordPosition="3990" endWordPosition="3993">indicates statically significantly better performance than baseline according to the paired t-test with P &lt; 0.001 (or P &lt; 0.05). – For U.S. Congressional Floor Debates, the SVM baseline accuracy is 70.00%. Statistical significance cannot be calculated because the data comes in a single split. Since our training procedure solves a non-convex optimization problem, it requires an initial guess of the explanatory sentences. We use an explanatory set size (5) of 30% of the number of sentences in each document, L = F0.3 · |x|], with a lower cap of 1. We generate initializations using OpinionFinder (Wilson et al., 2005), which were shown to be a reasonable substitute for human annotations in the Movie Reviews dataset (Yessenalina et al., 2010).6 We consider two additional (baseline) methods for initialization: using a random set of sentences, and using the last 30% of sentence in the document. In the Movie Reviews dataset, we also use sentences containing human-annotator rationales as a final initialization option. No such manual annotations are available for the Congressional Debates. 5.2 Experimental Results We evaluate three versions of our model: the initial model (3) which we call SVMsle (SVMs for Senti</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Automatically generating annotator rationales to improve sentiment classification.</title>
<date>2010</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5858" citStr="Yessenalina et al. (2010)" startWordPosition="835" endWordPosition="838">ubtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance, Zaidan et al. (2007) used human annotators to mark the “annotator rationales”, which are text spans that support the document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document. Yessenalina et al. (2010) extended this approach to use automatically generated rationales. 1http://projects.yisongyue.com/svmsle/ A natural approach to avoid the pitfalls associated with cascaded methods is to use joint twolevel models that simultaneously solve the sentencelevel and document-level tasks (e.g., McDonald et al. (2007), Zaidan and Eisner (2008)). Since these models are trained jointly, the sentence-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, t</context>
<context position="25138" citStr="Yessenalina et al., 2010" startWordPosition="4010" endWordPosition="4013"> 0.05). – For U.S. Congressional Floor Debates, the SVM baseline accuracy is 70.00%. Statistical significance cannot be calculated because the data comes in a single split. Since our training procedure solves a non-convex optimization problem, it requires an initial guess of the explanatory sentences. We use an explanatory set size (5) of 30% of the number of sentences in each document, L = F0.3 · |x|], with a lower cap of 1. We generate initializations using OpinionFinder (Wilson et al., 2005), which were shown to be a reasonable substitute for human annotations in the Movie Reviews dataset (Yessenalina et al., 2010).6 We consider two additional (baseline) methods for initialization: using a random set of sentences, and using the last 30% of sentence in the document. In the Movie Reviews dataset, we also use sentences containing human-annotator rationales as a final initialization option. No such manual annotations are available for the Congressional Debates. 5.2 Experimental Results We evaluate three versions of our model: the initial model (3) which we call SVMsle (SVMs for Sentiment classification with Latent Explanations), SVMsle regularized relative to a prior as described in the documents in the who</context>
<context position="27918" citStr="Yessenalina et al. (2010)" startWordPosition="4452" endWordPosition="4455">t standard SVM model on the validation set to train SVM&amp;quot;&apos; w/ Prior. We chose the combination that works the best on the validation set. 8Using the random initialization on the U.S. Congressional Floor Debates dataset offers surprisingly good performance. 1052 Table 2: Comparison of SVMsle fs with previous work on the Movie Reviews dataset. We considered two settings: when human annotations are available (Annot. Labels), and when they are unavailable (No Annot. Labels). METHOD SVM Zaidan et al. (2007) SVM��� �� SVM��� �� + Prox.Feat. Baseline Annot. Labels ACC 88.56 92.20 92.28 93.22 No Annot. Yessenalina et al. (2010) 91.78 Labels SVM��� Figure 1: Overlap of extracted sentences from different �� 92.50 SVM��� 92.39 SV�ls models on the Movie Reviews training set. �� +Prox.Feat. Table 3: Comparison of SVMsle fs with previous work on the U.S. Congressional Floor Debates dataset for the speaker-based segment classification task. METHOD SVM Thomas et al. (2006) Bansal et al. (2008) SVM��� �� SVM��� �� + Prox.Feat. Tables 2 and 3 show a comparison of SVM��� �� with previous work on the Movie Reviews and U.S. Congressional Floor Debates datasets, respectively. For the Movie Reviews dataset, we considered two setti</context>
</contexts>
<marker>Yessenalina, Choi, Cardie, 2010</marker>
<rawString>Ainur Yessenalina, Yejin Choi, and Claire Cardie. 2010. Automatically generating annotator rationales to improve sentiment classification. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural svms with latent variables.</title>
<date>2009</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="6964" citStr="Yu and Joachims (2009)" startWordPosition="990" endWordPosition="993">ches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g., Petrov and Klein (2007), Chang et al. (2010), Clarke et al. (2010)) and other fields (e.g., Felzenszwalb et al. (2008), Yu and Joachims (2009)) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to sentiment analysis, but </context>
<context position="9247" citStr="Yu and Joachims, 2009" startWordPosition="1363" endWordPosition="1366">l text of the original documents. Clearly, extracting the best supporting segments can offer a tremendous performance boost. We are interested in settings where humanextracted explanations such as annotator rationales might not be readily available, or are imperfect. As such, we will formulate the set of extracted sentences as latent or hidden variables in our model. Viewing the extracted sentences as latent variables will pose no new challenges during prediction, since the model is expected to predict all labels at test time. We will leverage recent advances in training latent variable SVMs (Yu and Joachims, 2009) to arrive at an effective training procedure. 4 Model In this section, we present a two-level document classification model. Although our model makes predictions at both the document and sentence levels, it will be trained (and evaluated) only with respect to document-level performance. We begin by presenting the feature structure and inference method. We will then describe a supervised training algorithm based on structural SVMs, and finally discuss some extensions and design decisions. Let x denote a document, y = +1 denote the sentiment (for us, a binary positive or negative polarity) of a</context>
<context position="14957" citStr="Yu and Joachims (2009)" startWordPosition="2341" endWordPosition="2344">P algorithm (Yuille and Rangarajan, 2003) with cutting plan where si is some fixed explanation (e.g., an initial guess of the best explanation). Then OP 1 reduces to a standard structural SVM, which can be solved efficiently (Joachims et al., 2009). Algorithm 2 describes our training procedure. Starting with an initial guess si for each training example, the training procedure alternates between solving an instance of the resulting structural SVM (called SSVMSolve in Algorithm 2) using the currently best known explanations si (Line 9), and making a new guess of the best explanations (Line 7). Yu and Joachims (2009) showed that this alternating procedure for training latent variable structural SVMs is an instance of the CCCP procedure (Yuille and Rangarajan, 2003), and so is guaranteed to converge to a local optimum. For our experiments, we do not train until convergence, but instead use performance on a validation set to choose the halting iteration. Since OP 1 is nonconvex, agood initialization is necessary. To generate the initial explanations, one can use an off-theshelf sentiment classifier such as OpinionFinder2 (Wilson et al., 2005). For some datasets, there exist documents with annotated sentence</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam Yu and Thorsten Joachims. 2009. Learning structural svms with latent variables. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan L Yuille</author>
<author>Anand Rangarajan</author>
</authors>
<title>The concave-convex procedure.</title>
<date>2003</date>
<journal>Neural Computation,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="14376" citStr="Yuille and Rangarajan, 2003" startWordPosition="2244" endWordPosition="2247">ng approaches. First, it solves the multi-level problem jointly as opposed to separately, which avoids introducing difficult to control propagation errors. Second, it does not require solving the sentence-level task perfectly, and also does not require precise sentence-level training labels. In other words, our goal is to learn to identify the informative (subjective) sentences that best explain the training labels to the extent required for good document classification performance. OP 1 is non-convex because of the constraints (7). To solve OP 1, we use the combination of the CCCP algorithm (Yuille and Rangarajan, 2003) with cutting plan where si is some fixed explanation (e.g., an initial guess of the best explanation). Then OP 1 reduces to a standard structural SVM, which can be solved efficiently (Joachims et al., 2009). Algorithm 2 describes our training procedure. Starting with an initial guess si for each training example, the training procedure alternates between solving an instance of the resulting structural SVM (called SSVMSolve in Algorithm 2) using the currently best known explanations si (Line 9), and making a new guess of the best explanations (Line 7). Yu and Joachims (2009) showed that this a</context>
</contexts>
<marker>Yuille, Rangarajan, 2003</marker>
<rawString>Alan L. Yuille and Anand Rangarajan. 2003. The concave-convex procedure. Neural Computation, 15(4):915–936, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
</authors>
<title>Modeling annotators: a generative approach to learning from annotator rationales.</title>
<date>2008</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6194" citStr="Zaidan and Eisner (2008)" startWordPosition="882" endWordPosition="885">label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document. Yessenalina et al. (2010) extended this approach to use automatically generated rationales. 1http://projects.yisongyue.com/svmsle/ A natural approach to avoid the pitfalls associated with cascaded methods is to use joint twolevel models that simultaneously solve the sentencelevel and document-level tasks (e.g., McDonald et al. (2007), Zaidan and Eisner (2008)). Since these models are trained jointly, the sentence-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural</context>
</contexts>
<marker>Zaidan, Eisner, 2008</marker>
<rawString>Omar F. Zaidan and Jason Eisner. 2008. Modeling annotators: a generative approach to learning from annotator rationales. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Jason Eisner</author>
<author>Christine Piatko</author>
</authors>
<title>Using “annotator rationales” to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="5452" citStr="Zaidan et al. (2007)" startWordPosition="773" endWordPosition="776">lobal min-cut inference. Afterward, the subjective extracts were converted into inputs for the document-level sentiment classifier. One advantage of their approach is that it avoids the need for explicit subjectivity annotations. However, like other cascaded approaches (e.g., Thomas et al. (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance, Zaidan et al. (2007) used human annotators to mark the “annotator rationales”, which are text spans that support the document’s sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document. Yessenalina et al. (2010) extended this approach to use automatically generated rationales. 1http://projects.yisongyue.com/svmsle/ A natural approach to avoid the pitfalls associated with cascaded methods is to use join</context>
<context position="7950" citStr="Zaidan et al., 2007" startWordPosition="1143" endWordPosition="1146"> particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to sentiment analysis, but they were focused on classifying either phrase-level (Choi and Cardie, 2008) or sentence-level polarity (Nakagawa et al., 2010). 3 Extracting Hidden Explanations In this paper, we take the view that each document has a subset of sentences that best explains its sentiment. Consider the “annotator rationales” generated by human judges for the movie reviews dataset (Zaidan et al., 2007). Each rationale is a text span that was identified to support (or explain) its parent document’s sentiment. Thus, these rationales can be interpreted as (something close to) a ground truth labeling of the explanatory segments. Using a dataset where each document contains only its rationales, 1047 Algorithm 1 Inference Algorithm for (2) 1: Input: x 2: Output: (y, s) 3: s+ &lt;-- argmaxsES(x) ~wTΨ(x, +1, s) 4: s− &lt;-- argmaxsES(x) ~wT Ψ(x, −1, s) 5: if ~wT Ψ(x, +1, s+) &gt; ~wT Ψ(x, −1, s−) then 6: Return (+1, s+) 7: else 8: Return (−1, s−) 9: end if cross validation experiments using an SVM classifie</context>
<context position="21918" citStr="Zaidan et al. (2007)" startWordPosition="3470" endWordPosition="3473">pol(x) jEs where the weight vector is now � w~ = � this model via OP 1 achieves that is (1) used to model the polarity of the entire document, and (2) used to compute a smoothed estimate of the polarity of the extracted sentences. This satisfies all three properties (A), (B), an Training ~wdoc d (C), although other approaches are also possible. 5 Experiments 5.1 Experimental Setup We evaluate our methods using the Movie Reviews and U.S. Congressional Floor Debates datasets, following the setup used in previous work for comparison purposes.3 Movie Reviews. We use the movie reviews dataset from Zaidan et al. (2007) that was originally released by Pang and Lee (2004). This version contains annotated rationales for each review, which we use to generate an additional initialization during training (described below). We follow exactly the experimental setup used in Zaidan et al. (2007).4 U.S. Congressional Floor Debates. We also use the U.S. Congressional floor debates transcripts from Thomas et al. (2006). The data was extracted from GovTrack (http://govtrack.us), which has all available transcripts of U.S. floor debates in the House of Representatives in 2005. As in previous work, only debates with discus</context>
<context position="27798" citStr="Zaidan et al. (2007)" startWordPosition="4432" endWordPosition="4435"> otherwise. 7We either used the same value of C to train both standard SVM model and SVM�&amp;quot; w/ Prior or used the best standard SVM model on the validation set to train SVM&amp;quot;&apos; w/ Prior. We chose the combination that works the best on the validation set. 8Using the random initialization on the U.S. Congressional Floor Debates dataset offers surprisingly good performance. 1052 Table 2: Comparison of SVMsle fs with previous work on the Movie Reviews dataset. We considered two settings: when human annotations are available (Annot. Labels), and when they are unavailable (No Annot. Labels). METHOD SVM Zaidan et al. (2007) SVM��� �� SVM��� �� + Prox.Feat. Baseline Annot. Labels ACC 88.56 92.20 92.28 93.22 No Annot. Yessenalina et al. (2010) 91.78 Labels SVM��� Figure 1: Overlap of extracted sentences from different �� 92.50 SVM��� 92.39 SV�ls models on the Movie Reviews training set. �� +Prox.Feat. Table 3: Comparison of SVMsle fs with previous work on the U.S. Congressional Floor Debates dataset for the speaker-based segment classification task. METHOD SVM Thomas et al. (2006) Bansal et al. (2008) SVM��� �� SVM��� �� + Prox.Feat. Tables 2 and 3 show a comparison of SVM��� �� with previous work on the Movie Rev</context>
<context position="34439" citStr="Zaidan et al. (2007)" startWordPosition="5527" endWordPosition="5530"> least subjective sentences according to SVM��� �� . Notice that most of these “objective” sentences can plausibly belong to speeches made in opposition to bills that limit stem cell research funding. That is, they do not clearly indicate the speaker’s stance towards the specific bill in question. We can thus see that our approach can indeed learn to infer sentences that are essential to understanding the document-level sentiment. 6 Discussion Making good structural assumptions simplifies the development process. Compared to methods that modify the training of flat document classifiers (e.g., Zaidan et al. (2007)), our approach uses fewer parameters, leading to a more compact and faster training stage. Compared to methods that use a cascaded approach (e.g., Pang and Lee (2004)), our approach is more robust to errors in the lower-level subtask due to being a joint model. Introducing latent variables makes the training procedure more flexible by not requiring lower-level labels, but does require a good initialization (i.e., a reasonable substitute for the lower-level labels). We believe that the widespread availability of off-theshelf sentiment lexicons and software, despite being developed for a differ</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>Omar F. Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve machine learning for text categorization. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>