<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005467">
<title confidence="0.982701">
Direct Word Sense Matching for Lexical Substitution
</title>
<author confidence="0.999023">
Ido Dagan&apos;, Oren Glickman&apos;, Alfio Gliozzo2, Efrat Marmorshtein&apos;, Carlo Strapparava2
</author>
<affiliation confidence="0.997316">
&apos;Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel
</affiliation>
<address confidence="0.436252">
2ITC-Irst, via Sommarive, I-38050, Trento, Italy
</address>
<sectionHeader confidence="0.957521" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999559">
This paper investigates conceptually and
empirically the novel sense matching task,
which requires to recognize whether the
senses of two synonymous words match in
context. We suggest direct approaches to
the problem, which avoid the intermediate
step of explicit word sense disambigua-
tion, and demonstrate their appealing ad-
vantages and stimulating potential for fu-
ture research.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907096774194">
In many language processing settings it is needed
to recognize that a given word or term may be sub-
stituted by a synonymous one. In a typical in-
formation seeking scenario, an information need
is specified by some given source words. When
looking for texts that match the specified need the
source words might be substituted with synony-
mous target words. For example, given the source
word ‘weapon’ a system may substitute it with the
target synonym ‘arm’.
This scenario, which is generally referred here
as lexical substitution, is a common technique
for increasing recall in Natural Language Process-
ing (NLP) applications. In Information Retrieval
(IR) and Question Answering (QA) it is typically
termed query/question expansion (Moldovan and
Mihalcea, 2000; Negri, 2004). Lexical Substi-
tution is also commonly applied to identify syn-
onyms in text summarization, for paraphrasing in
text generation, or is integrated into the features of
supervised tasks such as Text Categorization and
Information Extraction. Naturally, lexical substi-
tution is a very common first step in textual en-
tailment recognition, which models semantic in-
ference between a pair of texts in a generalized ap-
plication independent setting (Dagan et al., 2005).
To perform lexical substitution NLP applica-
tions typically utilize a knowledge source of syn-
onymous word pairs. The most commonly used
resource for lexical substitution is the manually
constructed WordNet (Fellbaum, 1998). Another
option is to use statistical word similarities, such
as in the database constructed by Dekang Lin (Lin,
1998). We generically refer to such resources as
substitution lexicons.
When using a substitution lexicon it is assumed
that there are some contexts in which the given
synonymous words share the same meaning. Yet,
due to polysemy, it is needed to verify that the
senses of the two words do indeed match in a given
context. For example, there are contexts in which
the source word ‘weapon’ may be substituted by
the target word ‘arm’; however one should recog-
nize that ‘arm’ has a different sense than ‘weapon’
in sentences such as “repetitive movements could
cause injuries to hands, wrists and arms.”
A commonly proposed approach to address
sense matching in lexical substitution is applying
Word Sense Disambiguation (WSD) to identify
the senses of the source and target words. Then,
substitution is applied only if the words have the
same sense (or synset, in WordNet terminology).
In settings in which the source is given as a sin-
gle term without context, sense disambiguation
is performed only for the target word; substitu-
tion is then applied only if the target word’s sense
matches at least one of the possible senses of the
source word.
One might observe that such application of WSD
addresses the task at hand in a somewhat indi-
rect manner. In fact, lexical substitution only re-
quires knowing that the source and target senses
</bodyText>
<page confidence="0.989804">
449
</page>
<note confidence="0.535655">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 449–456,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999834454545455">
do match, but it does not require that the match-
ing senses will be explicitly identified. Selecting
explicitly the right sense in context, which is then
followed by verifying the desired matching, might
be solving a harder intermediate problem than re-
quired. Instead, we can define the sense match-
ing problem directly as a binary classification task
for a pair of synonymous source and target words.
This task requires to decide whether the senses of
the two words do or do not match in a given con-
text (but it does not require to identify explicitly
the identity of the matching senses).
A highly related task was proposed in (Mc-
Carthy, 2002). McCarthy’s proposal was to ask
systems to suggest possible “semantically similar
replacements” of a target word in context, where
alternative replacements should be grouped to-
gether. While this task is somewhat more com-
plicated as an evaluation setting than our binary
recognition task, it was motivated by similar ob-
servations and applied goals. From another per-
spective, sense matching may be viewed as a lex-
ical sub-case of the general textual entailment
recognition setting, where we need to recognize
whether the meaning of the target word “entails”
the meaning of the source word in a given context.
This paper provides a first investigation of the
sense matching problem. To allow comparison
with the classical WSD setting we derived an
evaluation dataset for the new problem from the
Senseval-3 English lexical sample dataset (Mihal-
cea and Edmonds, 2004). We then evaluated alter-
native supervised and unsupervised methods that
perform sense matching either indirectly or di-
rectly (i.e. with or without the intermediate sense
identification step). Our findings suggest that in
the supervised setting the results of the direct and
indirect approaches are comparable. However, ad-
dressing directly the binary classification task has
practical advantages and can yield high precision
values, as desired in precision-oriented applica-
tions such as IR and QA.
More importantly, direct sense matching sets
the ground for implicit unsupervised approaches
that may utilize practically unlimited volumes
of unlabeled training data. Furthermore, such
approaches circumvent the sisyphean need for
specifying explicitly a set of stipulated senses.
We present an initial implementation of such an
approach using a one-class classifier, which is
trained on unlabeled occurrences of the source
word and applied to occurrences of the target
word. Our current results outperform the unsuper-
vised baseline and put forth a whole new direction
for future research.
</bodyText>
<sectionHeader confidence="0.892046" genericHeader="introduction">
2 WSD and Lexical Expansion
</sectionHeader>
<bodyText confidence="0.998901888888889">
Despite certain initial skepticism about the useful-
ness of WSD in practical tasks (Voorhees, 1993;
Sanderson, 1994), there is some evidence that
WSD can improve performance in typical NLP
tasks such as IR and QA. For example, (Sh¨utze
and Pederson, 1995) gives clear indication of the
potential for WSD to improve the precision of an IR
system. They tested the use of WSD on a standard
IR test collection (TREC-1B), improving precision
by more than 4%.
The use of WSD has produced successful exper-
iments for query expansion techniques. In partic-
ular, some attempts exploited WordNet to enrich
queries with semantically-related terms. For in-
stance, (Voorhees, 1994) manually expanded 50
queries over the TREC-1 collection using syn-
onymy and other WordNet relations. She found
that the expansion was useful with short and in-
complete queries, leaving the task of proper auto-
matic expansion as an open problem.
(Gonzalo et al., 1998) demonstrates an incre-
ment in performance over an IR test collection us-
ing the sense data contained in SemCor over a
purely term based model. In practice, they ex-
perimented searching SemCor with disambiguated
and expanded queries. Their work shows that
a WSD system, even if not performing perfectly,
combined with synonymy enrichment increases
retrieval performance.
(Moldovan and Mihalcea, 2000) introduces the
idea of using WordNet to extend Web searches
based on semantic similarity. Their results showed
that WSD-based query expansion actually im-
proves retrieval performance in a Web scenario.
Recently (Negri, 2004) proposed a sense-based
relevance feedback scheme for query enrichment
in a QA scenario (TREC-2003 and ACQUAINT),
demonstrating improvement in retrieval perfor-
mance.
While all these works clearly show the potential
usefulness of WSD in practical tasks, nonetheless
they do not necessarily justify the efforts for refin-
ing fine-grained sense repositories and for build-
ing large sense-tagged corpora. We suggest that
the sense matching task, as presented in the intro-
</bodyText>
<page confidence="0.994994">
450
</page>
<bodyText confidence="0.992273">
duction, may relieve major drawbacks of applying
WSD in practical scenarios.
</bodyText>
<sectionHeader confidence="0.699172" genericHeader="method">
3 Problem Setting and Dataset
</sectionHeader>
<bodyText confidence="0.999782602040816">
To investigate the direct sense matching problem
it is necessary to obtain an appropriate dataset of
examples for this binary classification task, along
with gold standard annotation. While there is
no such standard (application independent) dataset
available it is possible to derive it automatically
from existing WSD evaluation datasets, as de-
scribed below. This methodology also allows
comparing direct approaches for sense matching
with classical indirect approaches, which apply an
intermediate step of identifying the most likely
WordNet sense.
We derived our dataset from the Senseval-3 En-
glish lexical sample dataset (Mihalcea and Ed-
monds, 2004), taking all 25 nouns, adjectives and
adverbs in this sample. Verbs were excluded since
their sense annotation in Senseval-3 is not based
on WordNet senses. The Senseval dataset includes
a set of example occurrences in context for each
word, split to training and test sets, where each ex-
ample is manually annotated with the correspond-
ing WordNet synset.
For the sense matching setting we need exam-
ples of pairs of source-target synonymous words,
where at least one of these words should occur in
a given context. Following an applicative moti-
vation, we mimic an IR setting in which a sin-
gle source word query is expanded (substituted)
by a synonymous target word. Then, it is needed
to identify contexts in which the target word ap-
pears in a sense that matches the source word. Ac-
cordingly, we considered each of the 25 words in
the Senseval sample as a target word for the sense
matching task. Next, we had to pick for each target
word a corresponding synonym to play the role of
the source word. This was done by creating a list
of all WordNet synonyms of the target word, under
all its possible senses, and picking randomly one
of the synonyms as the source word. For example,
the word ‘disc’ is one of the words in the Sense-
val lexical sample. For this target word the syn-
onym ‘record’ was picked, which matches ‘disc’
in its musical sense. Overall, 59% of all possible
synsets of our target words included an additional
synonym, which could play the role of the source
word (that is, 41% of the synsets consisted of the
target word only). Similarly, 62% of the test exam-
ples of the target words were annotated by a synset
that included an additional synonym.
While creating source-target synonym pairs it
was evident that many WordNet synonyms corre-
spond to very infrequent senses or word usages,
such as the WordNet synonyms germ and source.
Such source synonyms are useless for evaluat-
ing sense matching with the target word since the
senses of the two words would rarely match in per-
ceivable contexts. In fact, considering our motiva-
tion for lexical substitution, it is usually desired to
exclude such obscure synonym pairs from substi-
tution lexicons in practical applications, since they
would mostly introduce noise to the system. To
avoid this problem the list of WordNet synonyms
for each target word was filtered by a lexicogra-
pher, who excluded manually obscure synonyms
that seemed worthless in practice. The source syn-
onym for each target word was then picked ran-
domly from the filtered list. Table 1 shows the 25
source-target pairs created for our experiments. In
future work it may be possible to apply automatic
methods for filtering infrequent sense correspon-
dences in the dataset, by adopting algorithms such
as in (McCarthy et al., 2004).
Having source-target synonym pairs, a classifi-
cation instance for the sense matching task is cre-
ated from each example occurrence of the target
word in the Senseval dataset. A classification in-
stance is thus defined by a pair of source and target
words and a given occurrence of the target word in
context. The instance should be classified as pos-
itive if the sense of the target word in the given
context matches one of the possible senses of the
source word, and as negative otherwise. Table 2
illustrates positive and negative example instances
for the source-target synonym pair ‘record-disc’,
where only occurrences of ‘disc’ in the musical
sense are considered positive.
The gold standard annotation for the binary
sense matching task can be derived automatically
from the Senseval annotations and the correspond-
ing WordNet synsets. An example occurrence of
the target word is considered positive if the an-
notated synset for that example includes also the
source word, and Negative otherwise. Notice that
different positive examples might correspond to
different senses of the target word. This happens
when the source and target share several senses,
and hence they appear together in several synsets.
Finally, since in Senseval an example may be an-
</bodyText>
<page confidence="0.988683">
451
</page>
<table confidence="0.999758">
source-target source-target source-target source-target source-target
statement-argument subdivision-arm atm-atmosphere hearing-audience camber-bank
level-degree deviation-difference dissimilar-different trouble-difficulty record-disc
raging-hot ikon-image crucial-important sake-interest bare-simple
opinion-judgment arrangement-organization newspaper-paper company-party substantial-solid
execution-performance design-plan protection-shelter variety-sort root-source
</table>
<tableCaption confidence="0.987304">
Table 1: Source and target pairs
</tableCaption>
<table confidence="0.958795333333333">
sentence annotation
This is anyway a stunning disc, thanks to the playing of the Moscow Virtuosi with Spivakov. positive
He said computer networks would not be affected and copies of information should be made on negative
floppy discs.
Before the dead soldier was placed in the ditch his personal possessions were removed, leaving negative
one disc on the body for identification purposes
</table>
<tableCaption confidence="0.99777">
Table 2: positive and negative examples for the source-target synonym pair ‘record-disc’
</tableCaption>
<bodyText confidence="0.998668583333333">
notated with more than one sense, it was consid-
ered positive if any of the annotated synsets for the
target word includes the source word.
Using this procedure we derived gold standard
annotations for all the examples in the Senseval-
3 training section for our 25 target words. For the
test set we took up to 40 test examples for each tar-
get word (some words had fewer test examples),
yielding 913 test examples in total, out of which
239 were positive. This test set was used to eval-
uate the sense matching methods described in the
next section.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="method">
4 Investigated Methods
</sectionHeader>
<bodyText confidence="0.999903315789474">
As explained in the introduction, the sense match-
ing task may be addressed by two general ap-
proaches. The traditional indirect approach would
first disambiguate the target word relative to a pre-
defined set of senses, using standard WSD meth-
ods, and would then verify that the selected sense
matches the source word. On the other hand, a
direct approach would address the binary sense
matching task directly, without selecting explicitly
a concrete sense for the target word. This section
describes the alternative methods we investigated
under supervised and unsupervised settings. The
supervised methods utilize manual sense annota-
tions for the given source and target words while
unsupervised methods do not require any anno-
tated sense examples. For the indirect approach
we assume the standard WordNet sense repository
and corresponding annotations of the target words
with WordNet synsets.
</bodyText>
<subsectionHeader confidence="0.998084">
4.1 Feature set and classifier
</subsectionHeader>
<bodyText confidence="0.999982096774194">
As a vehicle for investigating different classifica-
tion approaches we implemented a “vanilla” state
of the art architecture for WSD. Following com-
mon practice in feature extraction (e.g. (Yarowsky,
1994)), and using the mxpost1 part of speech tag-
ger and WordNet’s lemmatization, the following
feature set was used: bag of word lemmas for the
context words in the preceding, current and fol-
lowing sentence; unigrams of lemmas and parts
of speech in a window of +/- three words, where
each position provides a distinct feature; and bi-
grams of lemmas in the same window. The SVM-
Light (Joachims, 1999) classifier was used in the
supervised settings with its default parameters. To
obtain a multi-class classifier we used a standard
one-vs-all approach of training a binary SVM for
each possible sense and then selecting the highest
scoring sense for a test example.
To verify that our implementation provides a
reasonable replication of state of the art WSD we
applied it to the standard Senseval-3 Lexical Sam-
ple WSD task. The obtained accuracy2 was 66.7%,
which compares reasonably with the mid-range of
systems in the Senseval-3 benchmark (Mihalcea
and Edmonds, 2004). This figure is just a few
percent lower than the (quite complicated) best
Senseval-3 system, which achieved about 73% ac-
curacy, and it is much higher than the standard
Senseval baselines. We thus regard our classifier
as a fair vehicle for comparing the alternative ap-
proaches for sense matching on equal grounds.
</bodyText>
<footnote confidence="0.915231">
1ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
2The standard classification accuracy measure equals pre-
cision and recall as defined in the Senseval terminology when
the system classifies all examples, with no abstentions.
</footnote>
<page confidence="0.996835">
452
</page>
<subsectionHeader confidence="0.5802265">
4.2 Supervised Methods
4.2.1 Indirect approach
</subsectionHeader>
<bodyText confidence="0.999971909090909">
The indirect approach for sense matching fol-
lows the traditional scheme of performing WSD
for lexical substitution. First, the WSD classifier
described above was trained for the target words
of our dataset, using the Senseval-3 sense anno-
tated training data for these words. Then, the clas-
sifier was applied to the test examples of the target
words, selecting the most likely sense for each ex-
ample. Finally, an example was classified as pos-
itive if the selected synset for the target word in-
cludes the source word, and as negative otherwise.
</bodyText>
<subsectionHeader confidence="0.552317">
4.2.2 Direct approach
</subsectionHeader>
<bodyText confidence="0.999826933333333">
As explained above, the direct approach ad-
dresses the binary sense matching task directly,
without selecting explicitly a sense for the target
word. In the supervised setting it is easy to ob-
tain such a binary classifier using the annotation
scheme described in Section 3. Under this scheme
an example was annotated as positive (for the bi-
nary sense matching task) if the source word is
included in the Senseval gold standard synset of
the target word. We trained the classifier using the
set of Senseval-3 training examples for each tar-
get word, considering their derived binary anno-
tations. Finally, the trained classifier was applied
to the test examples of the target words, yielding
directly a binary positive-negative classification.
</bodyText>
<subsectionHeader confidence="0.930893">
4.3 Unsupervised Methods
</subsectionHeader>
<bodyText confidence="0.999935866666667">
It is well known that obtaining annotated training
examples for WSD tasks is very expensive, and
is often considered infeasible in unrestricted do-
mains. Therefore, many researchers investigated
unsupervised methods, which do not require an-
notated examples. Unsupervised approaches have
usually been investigated within Senseval using
the “All Words” dataset, which does not include
training examples. In this paper we preferred us-
ing the same test set which was used for the super-
vised setting (created from the Senseval-3 “Lexi-
cal Sample” dataset, as described above), in order
to enable comparison between the two settings.
Naturally, in the unsupervised setting the sense la-
bels in the training set were not utilized.
</bodyText>
<subsectionHeader confidence="0.868709">
4.3.1 Indirect approach
</subsectionHeader>
<bodyText confidence="0.999943842105263">
State-of-the-art unsupervised WSD systems are
quite complex and they are not easy to be repli-
cated. Thus, we implemented the unsupervised
version of the Lesk algorithm (Lesk, 1986) as a
reference system, since it is considered a standard
simple baseline for unsupervised approaches. The
Lesk algorithm is one of the first algorithms de-
veloped for semantic disambiguation of all-words
in unrestricted text. In its original unsupervised
version, the only resource required by the algo-
rithm is a machine readable dictionary with one
definition for each possible word sense. The algo-
rithm looks for words in the sense definitions that
overlap with context words in the given sentence,
and chooses the sense that yields maximal word
overlap. We implemented a version of this algo-
rithm using WordNet sense-definitions with con-
text length of ±10 words before and after the tar-
get word.
</bodyText>
<subsubsectionHeader confidence="0.69812">
4.3.2 The direct approach: one-class learning
</subsubsectionHeader>
<bodyText confidence="0.999765411764705">
The unsupervised settings for the direct method
are more problematic because most of unsuper-
vised WSD algorithms (such as the Lesk algo-
rithm) rely on dictionary definitions. For this rea-
son, standard unsupervised techniques cannot be
applied in a direct approach for sense matching, in
which the only external information is a substitu-
tion lexicon.
In this subsection we present a direct unsuper-
vised method for sense matching. It is based on
the assumption that typical contexts in which both
the source and target words appear correspond to
their matching senses. Unlabeled occurrences of
the source word can then be used to provide evi-
dence for lexical substitution because they allow
us to recognize whether the sense of the target
word matches that of the source. Our strategy is
to represent in a learning model the typical con-
texts of the source word in unlabeled training data.
Then, we exploit such model to match the contexts
of the target word, providing a decision criterion
for sense matching. In other words, we expect that
under a matching sense the target word would oc-
cur in prototypical contexts of the source word.
To implement such approach we need a learning
technique that does not rely on the availability of
negative evidence, that is, a one-class learning al-
gorithm. In general, the classification performance
of one-class approaches is usually quite poor, if
compared to supervised approaches for the same
tasks. However, in many practical settings one-
class learning is the only available solution.
For our experiments we adopted the one-class
SVM learning algorithm (Sch¨olkopf et al., 2001)
</bodyText>
<page confidence="0.998567">
453
</page>
<bodyText confidence="0.999814166666666">
implemented in the LIBSVM package,3 and repre-
sented the unlabeled training examples by adopt-
ing the feature set described in Subsection 4.1.
Roughly speaking, a one-class SVM estimates the
smallest hypersphere enclosing most of the train-
ing data. New test instances are then classified
positively if they lie inside the sphere, while out-
liers are regarded as negatives. The ratio between
the width of the enclosed region and the number
of misclassified training examples can be varied
by setting the parameter v E (0, 1). Smaller val-
ues of v will produce larger positive regions, with
the effect of increasing recall.
The appealing advantage of adopting one-class
learning for sense matching is that it allows us to
define a very elegant learning scenario, in which it
is possible to train “off-line” a different classifier
for each (source) word in the lexicon. Such a clas-
sifier can then be used to match the sense of any
possible target word for the source which is given
in the substitution lexicon. This is in contrast to
the direct supervised method proposed in Subsec-
tion 4.2, where a different classifier for each pair
of source - target words has to be defined.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="method">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999836">
5.1 Evaluation measures and baselines
</subsectionHeader>
<bodyText confidence="0.999478409090909">
In the lexical substitution (and expansion) set-
ting, the standard WSD metrics (Mihalcea and Ed-
monds, 2004) are not suitable, because we are in-
terested in the binary decision of whether the tar-
get word matches the sense of a given source word.
In analogy to IR, we are more interested in positive
assignments, while the opposite case (i.e. when the
two words cannot be substituted) is less interest-
ing. Accordingly, we utilize the standard defini-
tions of precision, recall and F1 typically used in
IR benchmarks. In the rest of this section we will
report micro averages for these measures on the
test set described in Section 3.
Following the Senseval methodology, we evalu-
ated two different baselines for unsupervised and
supervised methods. The random baseline, used
for the unsupervised algorithms, was obtained by
choosing either the positive or the negative class
at random resulting in P = 0.262, R = 0.5,
F1 = 0.344. The Most Frequent baseline has
been used for the supervised algorithms and is ob-
tained by assigning the positive class when the
</bodyText>
<footnote confidence="0.832406">
3Freely available from www.csie.ntu.edu.tw/
/—cjlin/libsvm.
</footnote>
<bodyText confidence="0.99873">
percentage of positive examples in the training set
is above 50%, resulting in P = 0.65, R = 0.41,
F1 = 0.51.
</bodyText>
<subsectionHeader confidence="0.972888">
5.2 Supervised Methods
</subsectionHeader>
<bodyText confidence="0.9999754">
Both the indirect and the direct supervised meth-
ods presented in Subsection 4.2 have been tested
and compared to the most frequent baseline.
Indirect. For the indirect methodology we
trained the supervised WSD system for each tar-
get word on the sense-tagged training sample. As
described in Subsection 4.2, we implemented a
simple SVM-based WSD system (see Section 4.2)
and applied it to the sense-matching task. Results
are reported in Table 3. The direct strategy sur-
passes the most frequent baseline F1 score, but the
achieved precision is still below it. We note that in
this multi-class setting it is less straightforward to
tradeoff recall for precision, as all senses compete
with each other.
Direct. In the direct supervised setting, sense
matching is performed by training a binary clas-
sifier, as described in Subsection 4.2.
The advantage of adopting a binary classifica-
tion strategy is that the precision/recall tradeoff
can be tuned in a meaningful way. In SVM learn-
ing, such tuning is achieved by varying the param-
eter J, that allows us to modify the cost function
of the SVM learning algorithm. If J = 1 (default),
the weight for the positive examples is equal to the
weight for the negatives. When J &gt; 1, negative
examples are penalized (increasing recall), while,
whenever 0 &lt; J &lt; 1, positive examples are penal-
ized (increasing precision). Results obtained by
varying this parameter are reported in Figure 1.
</bodyText>
<figureCaption confidence="0.995262">
Figure 1: Direct supervised results varying J
</figureCaption>
<page confidence="0.982439">
454
</page>
<table confidence="0.999773142857143">
Supervised P R Fl Unsupervised P R Fl
Most Frequent Baseline 0.65 0.41 0.51 Random Baseline 0.26 0.50 0.34
Multiclass SVM Indirect 0.59 0.63 0.61 Lesk Indirect 0.24 0.19 0.21
Binary SVM (J = 0.5) Direct 0.80 0.26 0.39 One-Class v = 0.3 Direct 0.26 0.72 0.39
Binary SVM (J = 1) Direct 0.76 0.46 0.57 One-Class v = 0.5 Direct 0.29 0.56 0.38
Binary SVM (J = 2) Direct 0.68 0.53 0.60 One-Class v = 0.7 Direct 0.28 0.36 0.32
Binary SVM (J = 3) Direct 0.69 0.55 0.61 One-Class v = 0.9 Direct 0.23 0.10 0.14
</table>
<tableCaption confidence="0.999712">
Table 3: Classification results on the sense matching task
</tableCaption>
<bodyText confidence="0.9997119">
Adopting the standard parameter settings (i.e.
J = 1, see Table 3), the F1 of the system
is slightly lower than for the indirect approach,
while it reaches the indirect figures when J in-
creases. More importantly, reducing J allows us
to boost precision towards 100%. This feature is
of great interest for lexical substitution, particu-
larly in precision oriented applications like IR and
QA, for filtering irrelevant candidate answers or
documents.
</bodyText>
<sectionHeader confidence="0.563123" genericHeader="method">
5.3 Unsupervised methods
</sectionHeader>
<bodyText confidence="0.999740133333333">
Indirect. To evaluate the indirect unsupervised
settings we implemented the Lesk algorithm, de-
scribed in Subsection 4.3.1, and evaluated it on
the sense matching task. The obtained figures,
reported in Table 3, are clearly below the base-
line, suggesting that simple unsupervised indirect
strategies cannot be used for this task. In fact, the
error of the first step, due to low WSD accuracy
of the unsupervised technique, is propagated in
the second step, producing poor sense matching.
Unfortunately, state-of-the-art unsupervised sys-
tems are actually not much better than Lesk on all-
words task (Mihalcea and Edmonds, 2004), dis-
couraging the use of unsupervised indirect meth-
ods for the sense matching task.
Direct. Conceptually, the most appealing solu-
tion for the sense matching task is the one-class
approach proposed for the direct method (Section
4.3.2). To perform our experiments, we trained a
different one-class SVM for each source word, us-
ing a sample of its unlabeled occurrences in the
BNC corpus as training set. To avoid huge train-
ing sets and to speed up the learning process, we
fixed the maximum number of training examples
to 10000 occurrences per word, collecting on av-
erage about 6500 occurrences per word.
For each target word in the test sample, we ap-
plied the classifier of the corresponding source
word. Results for different values of v are reported
in Figure 2 and summarized in Table 3.
</bodyText>
<figureCaption confidence="0.972967">
Figure 2: One-class evaluation varying v
</figureCaption>
<bodyText confidence="0.999927777777778">
While the results are somewhat above the base-
line, just small improvements in precision are re-
ported, and recall is higher than the baseline for
v &lt; 0.6. Such small improvements may suggest
that we are following a relevant direction, even
though they may not be useful yet for an applied
sense-matching setting.
Further analysis of the classification results for
each word revealed that optimal F1 values are ob-
tained by adopting different values of v for differ-
ent words. In the optimal (in retrospect) param-
eter settings for each word, performance for the
test set is noticeably boosted, achieving P = 0.40,
R = 0.85 and F1 = 0.54. Finding a principled un-
supervised way to automatically tune the v param-
eter is thus a promising direction for future work.
Investigating further the results per word, we
found that the correlation coefficient between the
optimal v values and the degree of polysemy of
the corresponding source words is 0.35. More in-
terestingly, we noticed a negative correlation (r
= -0.30) between the achieved F1 and the degree
of polysemy of the word, suggesting that polyse-
mous source words provide poor training models
for sense matching. This can be explained by ob-
serving that polysemous source words can be sub-
stituted with the target words only for a strict sub-
</bodyText>
<page confidence="0.997306">
455
</page>
<bodyText confidence="0.999911727272727">
set of their senses. On the other hand, our one-
class algorithm was trained on all the examples
of the source word, which include irrelevant ex-
amples that yield noisy training sets. A possible
solution may be obtained using clustering-based
word sense discrimination methods (Pedersen and
Bruce, 1997; Sch¨utze, 1998), in order to train dif-
ferent one-class models from different sense clus-
ters. Overall, the analysis suggests that future re-
search may obtain better binary classifiers based
just on unlabeled examples of the source word.
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989916666667">
This paper investigated the sense matching task,
which captures directly the polysemy problem in
lexical substitution. We proposed a direct ap-
proach for the task, suggesting the advantages of
natural control of precision/recall tradeoff, avoid-
ing the need in an explicitly defined sense reposi-
tory, and, most appealing, the potential for novel
completely unsupervised learning schemes. We
speculate that there is a great potential for such
approaches, and suggest that sense matching may
become an appealing problem and possible track
in lexical semantic evaluations.
</bodyText>
<sectionHeader confidence="0.998203" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997407">
This work was partly developed under the collab-
oration ITC-irst/University of Haifa.
</bodyText>
<sectionHeader confidence="0.9991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999934676470588">
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with wordnet synsets can improve
text retrieval. In ACL, Montreal, Canada.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 – 184. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
ACM-SIGDOC Conference, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768–774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Automatic identification of infre-
quent word senses. In Proceedings of COLING,
pages 1220–1226.
Diana McCarthy. 2002. Lexical substitution as a task
for wsd evaluation. In Proceedings of the ACL-
02 workshop on Word sense disambiguation, pages
109–115, Morristown, NJ, USA. Association for
Computational Linguistics.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Anal-
ysis of Text, Barcelona, Spain, July.
D. Moldovan and R. Mihalcea. 2000. Using wordnet
and lexical operators to improve internet searches.
IEEE Internet Computing, 4(1):34–43, January.
M. Negri. 2004. Sense-based blind relevance feedback
for question answering. In SIGIR-2004 Workshop
on Information Retrieval For Question Answering
(IR4QA), Sheffield, UK, July.
T. Pedersen and R. Bruce. 1997. Distinguishing word
sense in untagged text. In EMNLP, Providence, Au-
gust.
M. Sanderson. 1994. Word sense disambiguation and
information retrieval. In SIGIR, Dublin, Ireland,
June.
B. Sch¨olkopf, J. Platt, J. Shawe-Taylor, A. J. Smola,
and R. C. Williamson. 2001. Estimating the support
of a high-dimensional distribution. Neural Compu-
tation, 13:1443–1471.
H. Sch¨utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
H. Sh¨utze and J. Pederson. 1995. Information retrieval
based on word senses. In Proceedings of the 41h
Annual Symposium on Document Analysis and In-
formation Retrieval, Las Vegas.
E. Voorhees. 1993. Using WordNet to disambiguate
word sense for text retrieval. In SIGIR, Pittsburgh,
PA.
E. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 171h ACM
SIGIR Conference, Dublin, Ireland, June.
D. Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration
in spanish and french. In ACL, pages 88–95, Las
Cruces, New Mexico.
</reference>
<page confidence="0.999084">
456
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516918">
<title confidence="0.999758">Direct Word Sense Matching for Lexical Substitution</title>
<author confidence="0.990566">Oren Alfio Efrat Carlo</author>
<address confidence="0.7419365">of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel via Sommarive, I-38050, Trento, Italy</address>
<abstract confidence="0.992024">This paper investigates conceptually and the novel matching which requires to recognize whether the senses of two synonymous words match in context. We suggest direct approaches to the problem, which avoid the intermediate step of explicit word sense disambiguation, and demonstrate their appealing advantages and stimulating potential for future research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1901" citStr="Dagan et al., 2005" startWordPosition="282" endWordPosition="285">lications. In Information Retrieval (IR) and Question Answering (QA) it is typically termed query/question expansion (Moldovan and Mihalcea, 2000; Negri, 2004). Lexical Substitution is also commonly applied to identify synonyms in text summarization, for paraphrasing in text generation, or is integrated into the features of supervised tasks such as Text Categorization and Information Extraction. Naturally, lexical substitution is a very common first step in textual entailment recognition, which models semantic inference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most commonly used resource for lexical substitution is the manually constructed WordNet (Fellbaum, 1998). Another option is to use statistical word similarities, such as in the database constructed by Dekang Lin (Lin, 1998). We generically refer to such resources as substitution lexicons. When using a substitution lexicon it is assumed that there are some contexts in which the given synonymous words share the same meaning. Yet, due to polysemy, it is needed to verify that the </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2124" citStr="Fellbaum, 1998" startWordPosition="316" endWordPosition="317"> in text summarization, for paraphrasing in text generation, or is integrated into the features of supervised tasks such as Text Categorization and Information Extraction. Naturally, lexical substitution is a very common first step in textual entailment recognition, which models semantic inference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most commonly used resource for lexical substitution is the manually constructed WordNet (Fellbaum, 1998). Another option is to use statistical word similarities, such as in the database constructed by Dekang Lin (Lin, 1998). We generically refer to such resources as substitution lexicons. When using a substitution lexicon it is assumed that there are some contexts in which the given synonymous words share the same meaning. Yet, due to polysemy, it is needed to verify that the senses of the two words do indeed match in a given context. For example, there are contexts in which the source word ‘weapon’ may be substituted by the target word ‘arm’; however one should recognize that ‘arm’ has a differ</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet. An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gonzalo</author>
<author>F Verdejo</author>
<author>I Chugur</author>
<author>J Cigarran</author>
</authors>
<title>Indexing with wordnet synsets can improve text retrieval.</title>
<date>1998</date>
<booktitle>In ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7327" citStr="Gonzalo et al., 1998" startWordPosition="1149" endWordPosition="1152">prove the precision of an IR system. They tested the use of WSD on a standard IR test collection (TREC-1B), improving precision by more than 4%. The use of WSD has produced successful experiments for query expansion techniques. In particular, some attempts exploited WordNet to enrich queries with semantically-related terms. For instance, (Voorhees, 1994) manually expanded 50 queries over the TREC-1 collection using synonymy and other WordNet relations. She found that the expansion was useful with short and incomplete queries, leaving the task of proper automatic expansion as an open problem. (Gonzalo et al., 1998) demonstrates an increment in performance over an IR test collection using the sense data contained in SemCor over a purely term based model. In practice, they experimented searching SemCor with disambiguated and expanded queries. Their work shows that a WSD system, even if not performing perfectly, combined with synonymy enrichment increases retrieval performance. (Moldovan and Mihalcea, 2000) introduces the idea of using WordNet to extend Web searches based on semantic similarity. Their results showed that WSD-based query expansion actually improves retrieval performance in a Web scenario. R</context>
</contexts>
<marker>Gonzalo, Verdejo, Chugur, Cigarran, 1998</marker>
<rawString>J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarran. 1998. Indexing with wordnet synsets can improve text retrieval. In ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in kernel methods: support vector learning, chapter 11,</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16272" citStr="Joachims, 1999" startWordPosition="2559" endWordPosition="2560">et and classifier As a vehicle for investigating different classification approaches we implemented a “vanilla” state of the art architecture for WSD. Following common practice in feature extraction (e.g. (Yarowsky, 1994)), and using the mxpost1 part of speech tagger and WordNet’s lemmatization, the following feature set was used: bag of word lemmas for the context words in the preceding, current and following sentence; unigrams of lemmas and parts of speech in a window of +/- three words, where each position provides a distinct feature; and bigrams of lemmas in the same window. The SVMLight (Joachims, 1999) classifier was used in the supervised settings with its default parameters. To obtain a multi-class classifier we used a standard one-vs-all approach of training a binary SVM for each possible sense and then selecting the highest scoring sense for a test example. To verify that our implementation provides a reasonable replication of state of the art WSD we applied it to the standard Senseval-3 Lexical Sample WSD task. The obtained accuracy2 was 66.7%, which compares reasonably with the mid-range of systems in the Senseval-3 benchmark (Mihalcea and Edmonds, 2004). This figure is just a few per</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in kernel methods: support vector learning, chapter 11, pages 169 – 184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the ACM-SIGDOC Conference,</booktitle>
<location>Toronto, Canada.</location>
<contexts>
<context position="19689" citStr="Lesk, 1986" startWordPosition="3092" endWordPosition="3093">thin Senseval using the “All Words” dataset, which does not include training examples. In this paper we preferred using the same test set which was used for the supervised setting (created from the Senseval-3 “Lexical Sample” dataset, as described above), in order to enable comparison between the two settings. Naturally, in the unsupervised setting the sense labels in the training set were not utilized. 4.3.1 Indirect approach State-of-the-art unsupervised WSD systems are quite complex and they are not easy to be replicated. Thus, we implemented the unsupervised version of the Lesk algorithm (Lesk, 1986) as a reference system, since it is considered a standard simple baseline for unsupervised approaches. The Lesk algorithm is one of the first algorithms developed for semantic disambiguation of all-words in unrestricted text. In its original unsupervised version, the only resource required by the algorithm is a machine readable dictionary with one definition for each possible word sense. The algorithm looks for words in the sense definitions that overlap with context words in the given sentence, and chooses the sense that yields maximal word overlap. We implemented a version of this algorithm </context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the ACM-SIGDOC Conference, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2243" citStr="Lin, 1998" startWordPosition="335" endWordPosition="336">Text Categorization and Information Extraction. Naturally, lexical substitution is a very common first step in textual entailment recognition, which models semantic inference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most commonly used resource for lexical substitution is the manually constructed WordNet (Fellbaum, 1998). Another option is to use statistical word similarities, such as in the database constructed by Dekang Lin (Lin, 1998). We generically refer to such resources as substitution lexicons. When using a substitution lexicon it is assumed that there are some contexts in which the given synonymous words share the same meaning. Yet, due to polysemy, it is needed to verify that the senses of the two words do indeed match in a given context. For example, there are contexts in which the source word ‘weapon’ may be substituted by the target word ‘arm’; however one should recognize that ‘arm’ has a different sense than ‘weapon’ in sentences such as “repetitive movements could cause injuries to hands, wrists and arms.” A c</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguistics, pages 768–774, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Automatic identification of infrequent word senses.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1220--1226</pages>
<contexts>
<context position="11939" citStr="McCarthy et al., 2004" startWordPosition="1899" endWordPosition="1902">ubstitution lexicons in practical applications, since they would mostly introduce noise to the system. To avoid this problem the list of WordNet synonyms for each target word was filtered by a lexicographer, who excluded manually obscure synonyms that seemed worthless in practice. The source synonym for each target word was then picked randomly from the filtered list. Table 1 shows the 25 source-target pairs created for our experiments. In future work it may be possible to apply automatic methods for filtering infrequent sense correspondences in the dataset, by adopting algorithms such as in (McCarthy et al., 2004). Having source-target synonym pairs, a classification instance for the sense matching task is created from each example occurrence of the target word in the Senseval dataset. A classification instance is thus defined by a pair of source and target words and a given occurrence of the target word in context. The instance should be classified as positive if the sense of the target word in the given context matches one of the possible senses of the source word, and as negative otherwise. Table 2 illustrates positive and negative example instances for the source-target synonym pair ‘record-disc’, </context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Automatic identification of infrequent word senses. In Proceedings of COLING, pages 1220–1226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Lexical substitution as a task for wsd evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 workshop on Word sense disambiguation,</booktitle>
<pages>109--115</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4418" citStr="McCarthy, 2002" startWordPosition="698" endWordPosition="700">quire that the matching senses will be explicitly identified. Selecting explicitly the right sense in context, which is then followed by verifying the desired matching, might be solving a harder intermediate problem than required. Instead, we can define the sense matching problem directly as a binary classification task for a pair of synonymous source and target words. This task requires to decide whether the senses of the two words do or do not match in a given context (but it does not require to identify explicitly the identity of the matching senses). A highly related task was proposed in (McCarthy, 2002). McCarthy’s proposal was to ask systems to suggest possible “semantically similar replacements” of a target word in context, where alternative replacements should be grouped together. While this task is somewhat more complicated as an evaluation setting than our binary recognition task, it was motivated by similar observations and applied goals. From another perspective, sense matching may be viewed as a lexical sub-case of the general textual entailment recognition setting, where we need to recognize whether the meaning of the target word “entails” the meaning of the source word in a given c</context>
</contexts>
<marker>McCarthy, 2002</marker>
<rawString>Diana McCarthy. 2002. Lexical substitution as a task for wsd evaluation. In Proceedings of the ACL02 workshop on Word sense disambiguation, pages 109–115, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Edmonds</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<location>Barcelona, Spain,</location>
<marker>Mihalcea, Edmonds, editors, 2004</marker>
<rawString>R. Mihalcea and P. Edmonds, editors. 2004. Proceedings of SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>R Mihalcea</author>
</authors>
<title>Using wordnet and lexical operators to improve internet searches.</title>
<date>2000</date>
<journal>IEEE Internet Computing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1427" citStr="Moldovan and Mihalcea, 2000" startWordPosition="209" endWordPosition="212"> a typical information seeking scenario, an information need is specified by some given source words. When looking for texts that match the specified need the source words might be substituted with synonymous target words. For example, given the source word ‘weapon’ a system may substitute it with the target synonym ‘arm’. This scenario, which is generally referred here as lexical substitution, is a common technique for increasing recall in Natural Language Processing (NLP) applications. In Information Retrieval (IR) and Question Answering (QA) it is typically termed query/question expansion (Moldovan and Mihalcea, 2000; Negri, 2004). Lexical Substitution is also commonly applied to identify synonyms in text summarization, for paraphrasing in text generation, or is integrated into the features of supervised tasks such as Text Categorization and Information Extraction. Naturally, lexical substitution is a very common first step in textual entailment recognition, which models semantic inference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most com</context>
<context position="7724" citStr="Moldovan and Mihalcea, 2000" startWordPosition="1209" endWordPosition="1212"> the TREC-1 collection using synonymy and other WordNet relations. She found that the expansion was useful with short and incomplete queries, leaving the task of proper automatic expansion as an open problem. (Gonzalo et al., 1998) demonstrates an increment in performance over an IR test collection using the sense data contained in SemCor over a purely term based model. In practice, they experimented searching SemCor with disambiguated and expanded queries. Their work shows that a WSD system, even if not performing perfectly, combined with synonymy enrichment increases retrieval performance. (Moldovan and Mihalcea, 2000) introduces the idea of using WordNet to extend Web searches based on semantic similarity. Their results showed that WSD-based query expansion actually improves retrieval performance in a Web scenario. Recently (Negri, 2004) proposed a sense-based relevance feedback scheme for query enrichment in a QA scenario (TREC-2003 and ACQUAINT), demonstrating improvement in retrieval performance. While all these works clearly show the potential usefulness of WSD in practical tasks, nonetheless they do not necessarily justify the efforts for refining fine-grained sense repositories and for building large</context>
</contexts>
<marker>Moldovan, Mihalcea, 2000</marker>
<rawString>D. Moldovan and R. Mihalcea. 2000. Using wordnet and lexical operators to improve internet searches. IEEE Internet Computing, 4(1):34–43, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Negri</author>
</authors>
<title>Sense-based blind relevance feedback for question answering.</title>
<date>2004</date>
<booktitle>In SIGIR-2004 Workshop on Information Retrieval For Question Answering (IR4QA),</booktitle>
<location>Sheffield, UK,</location>
<contexts>
<context position="1441" citStr="Negri, 2004" startWordPosition="213" endWordPosition="214">g scenario, an information need is specified by some given source words. When looking for texts that match the specified need the source words might be substituted with synonymous target words. For example, given the source word ‘weapon’ a system may substitute it with the target synonym ‘arm’. This scenario, which is generally referred here as lexical substitution, is a common technique for increasing recall in Natural Language Processing (NLP) applications. In Information Retrieval (IR) and Question Answering (QA) it is typically termed query/question expansion (Moldovan and Mihalcea, 2000; Negri, 2004). Lexical Substitution is also commonly applied to identify synonyms in text summarization, for paraphrasing in text generation, or is integrated into the features of supervised tasks such as Text Categorization and Information Extraction. Naturally, lexical substitution is a very common first step in textual entailment recognition, which models semantic inference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most commonly used res</context>
<context position="7948" citStr="Negri, 2004" startWordPosition="1244" endWordPosition="1245">ates an increment in performance over an IR test collection using the sense data contained in SemCor over a purely term based model. In practice, they experimented searching SemCor with disambiguated and expanded queries. Their work shows that a WSD system, even if not performing perfectly, combined with synonymy enrichment increases retrieval performance. (Moldovan and Mihalcea, 2000) introduces the idea of using WordNet to extend Web searches based on semantic similarity. Their results showed that WSD-based query expansion actually improves retrieval performance in a Web scenario. Recently (Negri, 2004) proposed a sense-based relevance feedback scheme for query enrichment in a QA scenario (TREC-2003 and ACQUAINT), demonstrating improvement in retrieval performance. While all these works clearly show the potential usefulness of WSD in practical tasks, nonetheless they do not necessarily justify the efforts for refining fine-grained sense repositories and for building large sense-tagged corpora. We suggest that the sense matching task, as presented in the intro450 duction, may relieve major drawbacks of applying WSD in practical scenarios. 3 Problem Setting and Dataset To investigate the direc</context>
</contexts>
<marker>Negri, 2004</marker>
<rawString>M. Negri. 2004. Sense-based blind relevance feedback for question answering. In SIGIR-2004 Workshop on Information Retrieval For Question Answering (IR4QA), Sheffield, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>Distinguishing word sense in untagged text.</title>
<date>1997</date>
<booktitle>In EMNLP,</booktitle>
<location>Providence,</location>
<contexts>
<context position="30078" citStr="Pedersen and Bruce, 1997" startWordPosition="4821" endWordPosition="4824">oticed a negative correlation (r = -0.30) between the achieved F1 and the degree of polysemy of the word, suggesting that polysemous source words provide poor training models for sense matching. This can be explained by observing that polysemous source words can be substituted with the target words only for a strict sub455 set of their senses. On the other hand, our oneclass algorithm was trained on all the examples of the source word, which include irrelevant examples that yield noisy training sets. A possible solution may be obtained using clustering-based word sense discrimination methods (Pedersen and Bruce, 1997; Sch¨utze, 1998), in order to train different one-class models from different sense clusters. Overall, the analysis suggests that future research may obtain better binary classifiers based just on unlabeled examples of the source word. 6 Conclusion This paper investigated the sense matching task, which captures directly the polysemy problem in lexical substitution. We proposed a direct approach for the task, suggesting the advantages of natural control of precision/recall tradeoff, avoiding the need in an explicitly defined sense repository, and, most appealing, the potential for novel comple</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>T. Pedersen and R. Bruce. 1997. Distinguishing word sense in untagged text. In EMNLP, Providence, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
</authors>
<title>Word sense disambiguation and information retrieval.</title>
<date>1994</date>
<booktitle>In SIGIR,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="6513" citStr="Sanderson, 1994" startWordPosition="1017" endWordPosition="1018">ally unlimited volumes of unlabeled training data. Furthermore, such approaches circumvent the sisyphean need for specifying explicitly a set of stipulated senses. We present an initial implementation of such an approach using a one-class classifier, which is trained on unlabeled occurrences of the source word and applied to occurrences of the target word. Our current results outperform the unsupervised baseline and put forth a whole new direction for future research. 2 WSD and Lexical Expansion Despite certain initial skepticism about the usefulness of WSD in practical tasks (Voorhees, 1993; Sanderson, 1994), there is some evidence that WSD can improve performance in typical NLP tasks such as IR and QA. For example, (Sh¨utze and Pederson, 1995) gives clear indication of the potential for WSD to improve the precision of an IR system. They tested the use of WSD on a standard IR test collection (TREC-1B), improving precision by more than 4%. The use of WSD has produced successful experiments for query expansion techniques. In particular, some attempts exploited WordNet to enrich queries with semantically-related terms. For instance, (Voorhees, 1994) manually expanded 50 queries over the TREC-1 colle</context>
</contexts>
<marker>Sanderson, 1994</marker>
<rawString>M. Sanderson. 1994. Word sense disambiguation and information retrieval. In SIGIR, Dublin, Ireland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>J Platt</author>
<author>J Shawe-Taylor</author>
<author>A J Smola</author>
<author>R C Williamson</author>
</authors>
<title>Estimating the support of a high-dimensional distribution.</title>
<date>2001</date>
<journal>Neural Computation,</journal>
<pages>13--1443</pages>
<marker>Sch¨olkopf, Platt, Shawe-Taylor, Smola, Williamson, 2001</marker>
<rawString>B. Sch¨olkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. 2001. Estimating the support of a high-dimensional distribution. Neural Computation, 13:1443–1471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sh¨utze</author>
<author>J Pederson</author>
</authors>
<title>Information retrieval based on word senses.</title>
<date>1995</date>
<booktitle>In Proceedings of the 41h Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<location>Las Vegas.</location>
<marker>Sh¨utze, Pederson, 1995</marker>
<rawString>H. Sh¨utze and J. Pederson. 1995. Information retrieval based on word senses. In Proceedings of the 41h Annual Symposium on Document Analysis and Information Retrieval, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Using WordNet to disambiguate word sense for text retrieval.</title>
<date>1993</date>
<booktitle>In SIGIR,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="6495" citStr="Voorhees, 1993" startWordPosition="1015" endWordPosition="1016"> utilize practically unlimited volumes of unlabeled training data. Furthermore, such approaches circumvent the sisyphean need for specifying explicitly a set of stipulated senses. We present an initial implementation of such an approach using a one-class classifier, which is trained on unlabeled occurrences of the source word and applied to occurrences of the target word. Our current results outperform the unsupervised baseline and put forth a whole new direction for future research. 2 WSD and Lexical Expansion Despite certain initial skepticism about the usefulness of WSD in practical tasks (Voorhees, 1993; Sanderson, 1994), there is some evidence that WSD can improve performance in typical NLP tasks such as IR and QA. For example, (Sh¨utze and Pederson, 1995) gives clear indication of the potential for WSD to improve the precision of an IR system. They tested the use of WSD on a standard IR test collection (TREC-1B), improving precision by more than 4%. The use of WSD has produced successful experiments for query expansion techniques. In particular, some attempts exploited WordNet to enrich queries with semantically-related terms. For instance, (Voorhees, 1994) manually expanded 50 queries ove</context>
</contexts>
<marker>Voorhees, 1993</marker>
<rawString>E. Voorhees. 1993. Using WordNet to disambiguate word sense for text retrieval. In SIGIR, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Query expansion using lexicalsemantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of the 171h ACM SIGIR Conference,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="7062" citStr="Voorhees, 1994" startWordPosition="1107" endWordPosition="1108">ulness of WSD in practical tasks (Voorhees, 1993; Sanderson, 1994), there is some evidence that WSD can improve performance in typical NLP tasks such as IR and QA. For example, (Sh¨utze and Pederson, 1995) gives clear indication of the potential for WSD to improve the precision of an IR system. They tested the use of WSD on a standard IR test collection (TREC-1B), improving precision by more than 4%. The use of WSD has produced successful experiments for query expansion techniques. In particular, some attempts exploited WordNet to enrich queries with semantically-related terms. For instance, (Voorhees, 1994) manually expanded 50 queries over the TREC-1 collection using synonymy and other WordNet relations. She found that the expansion was useful with short and incomplete queries, leaving the task of proper automatic expansion as an open problem. (Gonzalo et al., 1998) demonstrates an increment in performance over an IR test collection using the sense data contained in SemCor over a purely term based model. In practice, they experimented searching SemCor with disambiguated and expanded queries. Their work shows that a WSD system, even if not performing perfectly, combined with synonymy enrichment </context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>E. Voorhees. 1994. Query expansion using lexicalsemantic relations. In Proceedings of the 171h ACM SIGIR Conference, Dublin, Ireland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in spanish and french.</title>
<date>1994</date>
<booktitle>In ACL,</booktitle>
<pages>88--95</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="15878" citStr="Yarowsky, 1994" startWordPosition="2490" endWordPosition="2491"> methods we investigated under supervised and unsupervised settings. The supervised methods utilize manual sense annotations for the given source and target words while unsupervised methods do not require any annotated sense examples. For the indirect approach we assume the standard WordNet sense repository and corresponding annotations of the target words with WordNet synsets. 4.1 Feature set and classifier As a vehicle for investigating different classification approaches we implemented a “vanilla” state of the art architecture for WSD. Following common practice in feature extraction (e.g. (Yarowsky, 1994)), and using the mxpost1 part of speech tagger and WordNet’s lemmatization, the following feature set was used: bag of word lemmas for the context words in the preceding, current and following sentence; unigrams of lemmas and parts of speech in a window of +/- three words, where each position provides a distinct feature; and bigrams of lemmas in the same window. The SVMLight (Joachims, 1999) classifier was used in the supervised settings with its default parameters. To obtain a multi-class classifier we used a standard one-vs-all approach of training a binary SVM for each possible sense and th</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in spanish and french. In ACL, pages 88–95, Las Cruces, New Mexico.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>