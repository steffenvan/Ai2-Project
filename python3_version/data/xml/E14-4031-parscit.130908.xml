<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.076280">
<title confidence="0.997338">
Improving Dependency Parsers using Combinatory Categorial Grammar
</title>
<author confidence="0.986079">
Bharat Ram Ambati Tejaswini Deoskar Mark Steedman
</author>
<affiliation confidence="0.9985905">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.993379">
bharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993803" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951571428571">
Subcategorization information is a useful
feature in dependency parsing. In this
paper, we explore a method of incorpo-
rating this information via Combinatory
Categorial Grammar (CCG) categories
from a supertagger. We experiment with
two popular dependency parsers (Malt
and MST) for two languages: English
and Hindi. For both languages, CCG
categories improve the overall accuracy
of both parsers by around 0.3-0.5% in
all experiments. For both parsers, we
see larger improvements specifically on
dependencies at which they are known to
be weak: long distance dependencies for
Malt, and verbal arguments for MST. The
result is particularly interesting in the case
of the fast greedy parser (Malt), since im-
proving its accuracy without significantly
compromising speed is relevant for large
scale applications such as parsing the web.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934923076923">
Dependency parsers can recover much of the
predicate-argument structure of a sentence, while
being relatively efficient to train and extremely
fast at parsing. Dependency parsers have been
gaining in popularity in recent times due to
the availability of large dependency treebanks
for several languages and parsing shared tasks
(Buchholz and Marsi, 2006; Nivre et al., 2007a;
Bharati et al., 2012).
Ambati et al. (2013) showed that the perfor-
mance of Malt (Nivre et al., 2007b) on the free
word order language, Hindi, is improved by using
lexical categories from Combinatory Categorial
Grammar (CCG) (Steedman, 2000). In this paper,
we extend this work and show that CCG categories
are useful even in the case of English, a typolog-
ically different language, where parsing accuracy
of dependency parsers is already extremely high.
In addition, we also demonstrate the utility of
CCG categories to MST (McDonald et al., 2005)
for both languages. CCG lexical categories
contain subcategorization information regarding
the dependencies of predicates, including long-
distance dependencies. We show that providing
this subcategorization information in the form of
CCG categories can help both Malt and MST on
precisely those dependencies for which they are
known to have weak rates of recovery. The result
is particularly interesting for Malt, the fast greedy
parser, as the improvement in Malt comes without
significantly compromising its speed, so that it
can be practically applied in web scale parsing.
Our results apply both to English, a fixed word
order and morphologically simple language, and
to Hindi, a free word order and morphologically
rich language, indicating that CCG categories
from a supertagger are an easy and robust way
of introducing lexicalized subcategorization
information into dependency parsers.
</bodyText>
<sectionHeader confidence="0.999695" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998931470588235">
Parsers using different grammar formalisms
have different strengths and weaknesses, and
prior work has shown that information from one
formalism can improve the performance of a
parser in another formalism. Sagae et al. (2007)
achieved a 1.4% improvement in accuracy over a
state-of-the-art HPSG parser by using dependen-
cies from a dependency parser for constraining
wide-coverage rules in the HPSG parser. Coppola
and Steedman (2013) incorporated higher-order
dependency features into a cube decoding phrase-
structure parser and obtained significant gains
on dependency recovery for both in-domain and
out-of-domain test sets.
Kim et al. (2012) improved a CCG parser using
dependency features. They extracted n-best parses
from a CCG parser and provided dependency
</bodyText>
<page confidence="0.985081">
159
</page>
<note confidence="0.892951">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 159–163,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.943702214285714">
Pierre Vinken will join the board as a nonexecutive director Nov. 29
N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N
&gt; &gt; &gt; &gt;
N NP N (S\NP)\(S\NP)
T &gt;
NP NP
&gt; &gt;
(S[b]\NP)/PP PP
&gt;
S[b]\NP
&lt;
S[b]\NP
&gt;
S[dcl]
</figure>
<figureCaption confidence="0.997623">
Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence.
</figureCaption>
<bodyText confidence="0.914251733333333">
S[dcl]\NP
&gt;
features from a dependency parser to a re-ranker
with an improvement of 0.35% in labelled F-score
of the CCGbank test set. Conversely, Ambati
et al. (2013) showed that a Hindi dependency
parser (Malt) could be improved by using CCG
categories. Using an algorithm similar to Cakici
(2005) and Uematsu et al. (2013), they first cre-
ated a Hindi CCGbank from a Hindi dependency
treebank and built a supertagger. They provided
CCG categories from a supertagger as features to
Malt and obtained overall improvements of 0.3%
and 0.4% in unlabelled and labelled attachment
scores respectively.
</bodyText>
<sectionHeader confidence="0.972779" genericHeader="method">
3 Data and Tools
</sectionHeader>
<bodyText confidence="0.999781166666667">
Figure 1 shows a CCG derivation with CCG
lexical categories for each word and Stanford
scheme dependencies (De Marneffe et al., 2006)
for an example English sentence. (Details of CCG
and dependency parsing are given by Steedman
(2000) and K¨ubler et al. (2009).)
</bodyText>
<subsectionHeader confidence="0.994133">
3.1 Treebanks
</subsectionHeader>
<bodyText confidence="0.9998393125">
In English dependency parsing literature, Stanford
and CoNLL dependency schemes are widely
popular. We used the Stanford parser’s built-in
converter (with the basic projective option) to
generate Stanford dependencies and Penn2Malt1
to generate CoNLL dependencies from Penn
Treebank (Marcus et al., 1993). We used standard
splits, training (sections 02-21), development
(section 22) and testing (section 23) for our
experiments. For Hindi, we worked with the
Hindi Dependency Treebank (HDT) released
as part of Coling 2012 Shared Task (Bharati et
al., 2012). HDT contains 12,041 training, 1,233
development and 1,828 testing sentences.
We used the English (Hockenmaier and Steed-
man, 2007) and Hindi CCGbanks (Ambati et al.,
</bodyText>
<footnote confidence="0.959875">
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
</footnote>
<bodyText confidence="0.9974075">
2013) for our experiments. For Hindi we used two
lexicons: a fine-grained one (with morphological
information) and a coarse-grained one (without
morphological information).
</bodyText>
<subsectionHeader confidence="0.998147">
3.2 Supertaggers
</subsectionHeader>
<bodyText confidence="0.999920384615385">
We used Clark and Curran (2004)’s supertagger
for English, and Ambati et al. (2013)’s supertag-
ger for Hindi. Both are Maximum Entropy based
CCG supertaggers. The Clark and Curran (2004)
supertagger uses different features like word, part-
of-speech, and contextual and complex bi-gram
features to obtain a 1-best accuracy of 91.5% on
the development set. In addition to the above
mentioned features, Ambati et al. (2013) em-
ployed morphological features useful for Hindi.
The 1-best accuracy of Hindi supertagger for fine-
grained and coarse-grained lexicon is 82.92% and
84.40% respectively.
</bodyText>
<subsectionHeader confidence="0.997221">
3.3 Dependency Parsers
</subsectionHeader>
<bodyText confidence="0.9999908">
There has been a significant amount of work on
parsing English and Hindi using the Malt and
MST parsers in the recent past (Nivre et al.,
2007a; Bharati et al., 2012). We first run these
parsers with previous best settings (McDonald et
al., 2005; Zhang and Nivre, 2012; Bharati et
al., 2012) and treat them as our baseline. In
the case of English, Malt uses arc-standard and
stack-projective parsing algorithms for CoNLL
and Stanford schemes respectively and LIBLIN-
EAR learner (Fan et al., 2008) for both the
schemes. MST uses 1st-order features, and a pro-
jective parsing algorithm with 5-best MIRA train-
ing for both the schemes. For Hindi, Malt uses
the arc-standard parsing algorithm with a LIBLIN-
EAR learner. MST uses 2nd-order features, non-
projective algorithm with 5-best MIRA training.
For English, we assigned POS-tags using a per-
ceptron tagger (Collins, 2002). For Hindi, we also
did all our experiments using automatic features
</bodyText>
<page confidence="0.988147">
160
</page>
<table confidence="0.999809636363637">
Language Experiment Malt MST
UAS LAS UAS LAS
English Stanford Baseline 90.32 87.87 90.36 87.18
Stanford + CCG 90.56** (2.5) 88.16** (2.5) 90.93** (5.9) 87.73** (4.3)
CoNLL Baseline 89.99 88.73 90.94 89.69
CoNLL + CCG 90.38** (4.0) 89.19** (4.1) 91.48** (5.9) 90.23** (5.3)
Hindi Baseline 88.67 (2.2) 83.04 (1.1) 90.52 80.67
88.93** (3.3) 83.23* (1.9)
89.04** 83.35*
Fine CCG 90.97** (4.8) 80.94* (1.4)
Coarse CCG 90.88** (3.8) 80.73* (0.4)
</table>
<tableCaption confidence="0.910469333333333">
Table 1: Impact of CCG categories from a supertagger on dependency parsing. Numbers in brackets
are percentage of errors reduced. McNemar’s test compared to baseline, * = p &lt; 0.05 ; ** = p &lt; 0.01
(Hindi Malt results (grey background) are from Ambati et al. (2013)).
</tableCaption>
<bodyText confidence="0.8431795">
(POS, chunk and morphological information)
extracted using a Hindi shallow parser2.
</bodyText>
<sectionHeader confidence="0.991366" genericHeader="method">
4 CCG Categories as Features
</sectionHeader>
<bodyText confidence="0.999940866666667">
Following Ambati et al. (2013), we used supertags
which occurred at least K times in the training
data, and backed off to coarse POS-tags otherwise.
For English K=1, i.e., when we use CCG cate-
gories for all words, gave the best results. K=15
gave the best results for Hindi due to sparsity is-
sues, as the data for Hindi is small. We provided
a supertag as an atomic symbol similar to a POS
tag and didn’t split it into a list of argument and
result categories. We explored both Stanford and
CoNLL schemes for English and fine and coarse-
grained CCG categories for Hindi. All feature and
parser tuning was done on the development data.
We assigned automatic POS-tags and supertags to
the training data.
</bodyText>
<subsectionHeader confidence="0.993416">
4.1 Experiments with Supertagger output
</subsectionHeader>
<bodyText confidence="0.999755294117647">
We first used gold CCG categories extracted from
each CCGbank as features to the Malt and MST,
to get an upper bound on the utility of CCG cate-
gories. As expected, gold CCG categories boosted
the Unlabelled Attachment Score (UAS) and La-
belled Attachment Score (LAS) by a large amount
(4-7% in all the cases).
We then experimented with using automatic
CCG categories from the English and Hindi su-
pertaggers as a feature to Malt and MST. With au-
tomatic categories from a supertagger, we got sta-
tistically significant improvements (McNemar’s
test, p &lt; 0.05 for Hindi LAS and p &lt; 0.01 for the
rest) over the baseline parsers, for all cases (Table
1). Since the CCGbanks used to train the supertag-
gers are automatically generated from the con-
stituency or dependency treebanks used to train
</bodyText>
<footnote confidence="0.843622">
2http://ltrc.iiit.ac.in/analyzer/hindi/
</footnote>
<bodyText confidence="0.9998795">
the dependency parsers, the improvements are
indeed due to reparameterization of the model to
include CCG categories and not due to additional
hand annotations in the CCGbanks. This shows
that the rich subcategorization information pro-
vided by automatically assigned CCG categories
can help Malt and MST in realistic applications.
For English, in case of Malt, we achieved
0.3% improvement in both UAS and LAS for
Stanford scheme. For CoNLL scheme, these
improvements were 0.4% and 0.5% in UAS and
LAS respectively. For MST, we got around 0.5%
improvements in all cases.
In case of Hindi, fine-grained supertags gave
larger improvements for MST. We got final
improvements of 0.5% and 0.3% in UAS and LAS
respectively. In contrast, for Malt, Ambati et al.
(2013) had shown that coarse-grained supertags
gave larger improvements of 0.3% and 0.4% in
UAS and LAS respectively. Due to better handling
of error propagation in MST, the richer informa-
tion in fine-grained categories may have surpassed
the slightly lower supertagger performance,
compared to coarse-grained categories.
</bodyText>
<subsectionHeader confidence="0.992386">
4.2 Analysis: English
</subsectionHeader>
<bodyText confidence="0.999980230769231">
We analyze the impact of CCG categories on
different labels (label-wise) and distance ranges
(distance-wise) for CoNLL scheme dependencies
(We observed a similar impact for the Stanford
scheme dependencies as well). Figure 2a shows
the F-score for three major dependency labels,
namely, ROOT (sentence root), SUBJ (subject),
OBJ (object). For Malt, providing CCG categories
gave an increment of 1.0%, 0.3% for ROOT and
SUBJ labels respectively. For MST, the improve-
ments for ROOT and SUBJ were 0.5% and 0.8%
respectively. There was no significant improve-
ment for OBJ label, especially in the case of Malt.
</bodyText>
<page confidence="0.98798">
161
</page>
<figure confidence="0.998196897959184">
(a) Label-wise impact
(b) Distance-wise impact
95 Malt
93.9
94 93.4 93.3
92.8
93
92
91
90
89
88
87
86
.
87.7
88.7
ROOT SUBJ
92.5
92.5
86.5 86.5
DOBJ
88.2
Malt + CCG
MST
MST + CCG
88.5
1-5 6-10 &gt;10
Malt
Malt + CCG
MST
MST + CCG
84.5
85.5
98.2
98.3
98.4
98.5
78.6
81.8
80.8
79.2
80.8
98
93
88
83
78
81.7
</figure>
<figureCaption confidence="0.999943">
Figure 2: Label-wise and Distance-wise impact of supertag features on Malt and MST for English
</figureCaption>
<bodyText confidence="0.991062333333333">
Figure 2b shows the F-score of dependencies
based on the distance ranges between words. The
percentage of dependencies in the 1−5, 6−10 and
&gt;10 distance ranges are 88.5%, 6.6% and 4.9% re-
spectively out of the total of around 50,000 depen-
dencies. For both Malt and MST, there was very
slight improvement for short distance dependen-
cies (1−5) but significant improvements for longer
distances (6−10 and &gt;10). For Malt, there was
an improvement of 0.6% and 0.9% for distances
6−10, and &gt;10 respectively. For MST, these
improvements were 1.0% and 1.0% respectively.
</bodyText>
<subsectionHeader confidence="0.999344">
4.3 Analysis: Hindi
</subsectionHeader>
<bodyText confidence="0.999992411764706">
In the case of Hindi, for MST, providing CCG
categories gave an increment of 0.5%, 0.4% and
0.3% for ROOT, SUBJ and OBJ labels respec-
tively in F-score over the baseline. Ambati et al.
(2013) showed that for Hindi, providing CCG
categories as features improved Malt in better
handling of long distance dependencies.
The percentage of dependencies in the 1−5,
6−10 and &gt;10 distance ranges are 82.2%,
8.6% and 9.2% respectively out of the total of
around 40,000 dependencies. Similar to English,
there was very slight improvement for short
distance dependencies (1−5). But for longer
distances, 6−10, and &gt;10, there was significant
improvement of 1.3% and 1.3% respectively
for MST. Ambati et al. (2013) reported similar
improvements for Malt as well.
</bodyText>
<subsectionHeader confidence="0.986375">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999946714285714">
Though valency is a useful feature in dependency
parsing (Zhang and Nivre, 2011), Zhang and Nivre
(2012) showed that providing valency information
dynamically, in the form of the number of depen-
dencies established in a particular state during
parsing, did not help Malt. However, as we have
shown above, providing this information as a static
lexical feature in the form of CCG categories does
help Malt. In addition to specifying the number of
arguments, CCG categories also contain syntactic
type and direction of those arguments. However,
providing CCG categories as features to zpar
(Zhang and Nivre, 2011) didn’t have significant
impact as it is already using similar information.
</bodyText>
<subsectionHeader confidence="0.99275">
4.5 Impact on Web Scale Parsing
</subsectionHeader>
<bodyText confidence="0.999718533333333">
Greedy parsers such as Malt are very fast and are
practically useful in large-scale applications such
as parsing the web. Table 2, shows the speed of
Malt, MST and zpar on parsing English test data
in CoNLL scheme (including POS-tagging and
supertagging time). Malt parses 310 sentences per
second, compared to 35 and 11 of zpar and MST
respectively. Clearly, Malt is orders of magnitude
faster than MST and zpar. After using CCG
categories from the supertagger, Malt parses 245
sentences per second, still much higher than other
parsers. Thus we have shown a way to improve
Malt without significantly compromising speed,
potentially enhancing its usefulness for web scale
parsing.
</bodyText>
<table confidence="0.9995378">
Parser Ave. Sents/Sec Total Time
MST 11 3m 36s
zpar 35 1m 11s
Malt 310 0m 7.7s
Malt + CCG 245 0m 10.2s
</table>
<tableCaption confidence="0.998689">
Table 2: Time taken to parse English test data.
</tableCaption>
<sectionHeader confidence="0.991297" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999898058823529">
We have shown that informative CCG categories,
which contain both local subcategorization infor-
mation and capture long distance dependencies
elegantly, improve the performance of two de-
pendency parsers, Malt and MST, by helping
in recovering long distance relations for Malt
and local verbal arguments for MST. This is
true both in the case of English (a fixed word
order language) and Hindi (free word order and
morphologically richer language), extending the
result of Ambati et al. (2013). The result is
particularly interesting in the case of Malt which
cannot directly use valency information, which
CCG categories provide indirectly. It leads to an
improvement in performance without significantly
compromising speed and hence promises to be
applicable to web scale processing.
</bodyText>
<page confidence="0.99622">
162
</page>
<sectionHeader confidence="0.990199" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985349623762376">
Bharat Ram Ambati, Tejaswini Deoskar, and Mark
Steedman. 2013. Using CCG categories to improve
Hindi dependency parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
604–609, Sofia, Bulgaria.
Akshar Bharati, Prashanth Mannem, and Dipti Misra
Sharma. 2012. Hindi Parsing Shared Task. In Pro-
ceedings of Coling Workshop on Machine Transla-
tion and Parsing in Indian Languages, Kharagpur,
India.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149–164,
New York City, New York.
Ruken Cakici. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of the ACL
Student Research Workshop, pages 73–78, Ann Ar-
bor, Michigan.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282–288.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical methods in natural
language processing, EMNLP ’02, pages 1–8.
Greg Coppola and Mark Steedman. 2013. The effect
of higher-order dependency features in discrimina-
tive phrase-structure parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
610–616, Sofia, Bulgaria.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871–1874, June.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355–396.
Sunghwan Mac Kim, Dominick Ng, Mark Johnson,
and James Curran. 2012. Improving combina-
tory categorial grammar parse reranking with depen-
dency grammar features. In Proceedings of COL-
ING 2012, pages 1441–1458, Mumbai, India.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan &amp; Clay-
pool Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91–98, Ann Arbor, Michigan.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 624–
631, Prague, Czech Republic.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,
Yusuke Miyao, and Hideki Mima. 2013. Inte-
grating multiple dependency corpora for inducing
wide-coverage Japanese CCG resources. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1042–1051, Sofia, Bulgaria.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA.
Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In Proceed-
ings of COLING 2012: Posters, pages 1391–1400,
Mumbai, India, December.
</reference>
<page confidence="0.999123">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974978">
<title confidence="0.999043">Improving Dependency Parsers using Combinatory Categorial Grammar</title>
<author confidence="0.999899">Bharat Ram Ambati Tejaswini Deoskar Mark Steedman</author>
<affiliation confidence="0.998316">Institute for Language, Cognition and School of Informatics, University of Edinburgh</affiliation>
<abstract confidence="0.998932090909091">Subcategorization information is a useful feature in dependency parsing. In this paper, we explore a method of incorporating this information via Combinatory Categorial Grammar (CCG) categories from a supertagger. We experiment with two popular dependency parsers (Malt and MST) for two languages: English and Hindi. For both languages, CCG categories improve the overall accuracy of both parsers by around 0.3-0.5% in all experiments. For both parsers, we see larger improvements specifically on dependencies at which they are known to be weak: long distance dependencies for Malt, and verbal arguments for MST. The result is particularly interesting in the case of the fast greedy parser (Malt), since improving its accuracy without significantly compromising speed is relevant for large scale applications such as parsing the web.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bharat Ram Ambati</author>
<author>Tejaswini Deoskar</author>
<author>Mark Steedman</author>
</authors>
<title>Using CCG categories to improve Hindi dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>604--609</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1547" citStr="Ambati et al. (2013)" startWordPosition="217" endWordPosition="220">ing in the case of the fast greedy parser (Malt), since improving its accuracy without significantly compromising speed is relevant for large scale applications such as parsing the web. 1 Introduction Dependency parsers can recover much of the predicate-argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing. Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012). Ambati et al. (2013) showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000). In this paper, we extend this work and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexical categories contain subcategorization information regarding the </context>
<context position="4443" citStr="Ambati et al. (2013)" startWordPosition="661" endWordPosition="664">l Linguistics, pages 159–163, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Pierre Vinken will join the board as a nonexecutive director Nov. 29 N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N &gt; &gt; &gt; &gt; N NP N (S\NP)\(S\NP) T &gt; NP NP &gt; &gt; (S[b]\NP)/PP PP &gt; S[b]\NP &lt; S[b]\NP &gt; S[dcl] Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence. S[dcl]\NP &gt; features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, Ambati et al. (2013) showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013), they first created a Hindi CCGbank from a Hindi dependency treebank and built a supertagger. They provided CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. 3 Data and Tools Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example Eng</context>
<context position="6218" citStr="Ambati et al. (2013)" startWordPosition="925" endWordPosition="928">or Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012). HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). 3.2 Supertaggers We used Clark and Curran (2004)’s supertagger for English, and Ambati et al. (2013)’s supertagger for Hindi. Both are Maximum Entropy based CCG supertaggers. The Clark and Curran (2004) supertagger uses different features like word, partof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, Ambati et al. (2013) employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. 3.3 Dependency Parsers There has been a significant amount of work on parsing English and Hindi</context>
<context position="8390" citStr="Ambati et al. (2013)" startWordPosition="1275" endWordPosition="1278">90.36 87.18 Stanford + CCG 90.56** (2.5) 88.16** (2.5) 90.93** (5.9) 87.73** (4.3) CoNLL Baseline 89.99 88.73 90.94 89.69 CoNLL + CCG 90.38** (4.0) 89.19** (4.1) 91.48** (5.9) 90.23** (5.3) Hindi Baseline 88.67 (2.2) 83.04 (1.1) 90.52 80.67 88.93** (3.3) 83.23* (1.9) 89.04** 83.35* Fine CCG 90.97** (4.8) 80.94* (1.4) Coarse CCG 90.88** (3.8) 80.73* (0.4) Table 1: Impact of CCG categories from a supertagger on dependency parsing. Numbers in brackets are percentage of errors reduced. McNemar’s test compared to baseline, * = p &lt; 0.05 ; ** = p &lt; 0.01 (Hindi Malt results (grey background) are from Ambati et al. (2013)). (POS, chunk and morphological information) extracted using a Hindi shallow parser2. 4 CCG Categories as Features Following Ambati et al. (2013), we used supertags which occurred at least K times in the training data, and backed off to coarse POS-tags otherwise. For English K=1, i.e., when we use CCG categories for all words, gave the best results. K=15 gave the best results for Hindi due to sparsity issues, as the data for Hindi is small. We provided a supertag as an atomic symbol similar to a POS tag and didn’t split it into a list of argument and result categories. We explored both Stanfo</context>
<context position="10833" citStr="Ambati et al. (2013)" startWordPosition="1681" endWordPosition="1684">l hand annotations in the CCGbanks. This shows that the rich subcategorization information provided by automatically assigned CCG categories can help Malt and MST in realistic applications. For English, in case of Malt, we achieved 0.3% improvement in both UAS and LAS for Stanford scheme. For CoNLL scheme, these improvements were 0.4% and 0.5% in UAS and LAS respectively. For MST, we got around 0.5% improvements in all cases. In case of Hindi, fine-grained supertags gave larger improvements for MST. We got final improvements of 0.5% and 0.3% in UAS and LAS respectively. In contrast, for Malt, Ambati et al. (2013) had shown that coarse-grained supertags gave larger improvements of 0.3% and 0.4% in UAS and LAS respectively. Due to better handling of error propagation in MST, the richer information in fine-grained categories may have surpassed the slightly lower supertagger performance, compared to coarse-grained categories. 4.2 Analysis: English We analyze the impact of CCG categories on different labels (label-wise) and distance ranges (distance-wise) for CoNLL scheme dependencies (We observed a similar impact for the Stanford scheme dependencies as well). Figure 2a shows the F-score for three major de</context>
<context position="12950" citStr="Ambati et al. (2013)" startWordPosition="2038" endWordPosition="2041"> ranges are 88.5%, 6.6% and 4.9% respectively out of the total of around 50,000 dependencies. For both Malt and MST, there was very slight improvement for short distance dependencies (1−5) but significant improvements for longer distances (6−10 and &gt;10). For Malt, there was an improvement of 0.6% and 0.9% for distances 6−10, and &gt;10 respectively. For MST, these improvements were 1.0% and 1.0% respectively. 4.3 Analysis: Hindi In the case of Hindi, for MST, providing CCG categories gave an increment of 0.5%, 0.4% and 0.3% for ROOT, SUBJ and OBJ labels respectively in F-score over the baseline. Ambati et al. (2013) showed that for Hindi, providing CCG categories as features improved Malt in better handling of long distance dependencies. The percentage of dependencies in the 1−5, 6−10 and &gt;10 distance ranges are 82.2%, 8.6% and 9.2% respectively out of the total of around 40,000 dependencies. Similar to English, there was very slight improvement for short distance dependencies (1−5). But for longer distances, 6−10, and &gt;10, there was significant improvement of 1.3% and 1.3% respectively for MST. Ambati et al. (2013) reported similar improvements for Malt as well. 4.4 Discussion Though valency is a useful</context>
</contexts>
<marker>Ambati, Deoskar, Steedman, 2013</marker>
<rawString>Bharat Ram Ambati, Tejaswini Deoskar, and Mark Steedman. 2013. Using CCG categories to improve Hindi dependency parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 604–609, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Prashanth Mannem</author>
<author>Dipti Misra Sharma</author>
</authors>
<title>Hindi Parsing Shared Task.</title>
<date>2012</date>
<booktitle>In Proceedings of Coling Workshop on Machine Translation and Parsing in Indian Languages,</booktitle>
<location>Kharagpur,</location>
<contexts>
<context position="1525" citStr="Bharati et al., 2012" startWordPosition="213" endWordPosition="216">s particularly interesting in the case of the fast greedy parser (Malt), since improving its accuracy without significantly compromising speed is relevant for large scale applications such as parsing the web. 1 Introduction Dependency parsers can recover much of the predicate-argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing. Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012). Ambati et al. (2013) showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000). In this paper, we extend this work and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexical categories contain subcategorization info</context>
<context position="5725" citStr="Bharati et al., 2012" startWordPosition="859" endWordPosition="862">en by Steedman (2000) and K¨ubler et al. (2009).) 3.1 Treebanks In English dependency parsing literature, Stanford and CoNLL dependency schemes are widely popular. We used the Stanford parser’s built-in converter (with the basic projective option) to generate Stanford dependencies and Penn2Malt1 to generate CoNLL dependencies from Penn Treebank (Marcus et al., 1993). We used standard splits, training (sections 02-21), development (section 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012). HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). 3.2 Supertaggers We used Clark and Curran (2004)’s supertagger for English, and Ambati et al. (2013)’s supertagger for Hindi. Both are Maximum Entropy based CCG supertaggers. The Clark and Curran (2004) supe</context>
<context position="7037" citStr="Bharati et al., 2012" startWordPosition="1056" endWordPosition="1059">m features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, Ambati et al. (2013) employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. 3.3 Dependency Parsers There has been a significant amount of work on parsing English and Hindi using the Malt and MST parsers in the recent past (Nivre et al., 2007a; Bharati et al., 2012). We first run these parsers with previous best settings (McDonald et al., 2005; Zhang and Nivre, 2012; Bharati et al., 2012) and treat them as our baseline. In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLINEAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002). For Hindi, we also did</context>
</contexts>
<marker>Bharati, Mannem, Sharma, 2012</marker>
<rawString>Akshar Bharati, Prashanth Mannem, and Dipti Misra Sharma. 2012. Hindi Parsing Shared Task. In Proceedings of Coling Workshop on Machine Translation and Parsing in Indian Languages, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>149--164</pages>
<location>New York City, New York.</location>
<contexts>
<context position="1481" citStr="Buchholz and Marsi, 2006" startWordPosition="205" endWordPosition="208">alt, and verbal arguments for MST. The result is particularly interesting in the case of the fast greedy parser (Malt), since improving its accuracy without significantly compromising speed is relevant for large scale applications such as parsing the web. 1 Introduction Dependency parsers can recover much of the predicate-argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing. Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012). Ambati et al. (2013) showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000). In this paper, we extend this work and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexic</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164, New York City, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruken Cakici</author>
</authors>
<title>Automatic induction of a CCG grammar for Turkish.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>73--78</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="4575" citStr="Cakici (2005)" startWordPosition="684" endWordPosition="685">n the board as a nonexecutive director Nov. 29 N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N &gt; &gt; &gt; &gt; N NP N (S\NP)\(S\NP) T &gt; NP NP &gt; &gt; (S[b]\NP)/PP PP &gt; S[b]\NP &lt; S[b]\NP &gt; S[dcl] Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence. S[dcl]\NP &gt; features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, Ambati et al. (2013) showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013), they first created a Hindi CCGbank from a Hindi dependency treebank and built a supertagger. They provided CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. 3 Data and Tools Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence. (Details of CCG and dependency parsing are given by Steedman (2000) and K¨ubler et al. (2009).) 3.1 Treebanks In Engl</context>
</contexts>
<marker>Cakici, 2005</marker>
<rawString>Ruken Cakici. 2005. Automatic induction of a CCG grammar for Turkish. In Proceedings of the ACL Student Research Workshop, pages 73–78, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<contexts>
<context position="6166" citStr="Clark and Curran (2004)" startWordPosition="917" endWordPosition="920">ion 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012). HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). 3.2 Supertaggers We used Clark and Curran (2004)’s supertagger for English, and Ambati et al. (2013)’s supertagger for Hindi. Both are Maximum Entropy based CCG supertaggers. The Clark and Curran (2004) supertagger uses different features like word, partof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, Ambati et al. (2013) employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. 3.3 Dependency Parsers There has been a sig</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING-04, pages 282–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the conference on Empirical methods in natural language processing, EMNLP ’02,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7613" citStr="Collins, 2002" startWordPosition="1149" endWordPosition="1150">and Nivre, 2012; Bharati et al., 2012) and treat them as our baseline. In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLINEAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002). For Hindi, we also did all our experiments using automatic features 160 Language Experiment Malt MST UAS LAS UAS LAS English Stanford Baseline 90.32 87.87 90.36 87.18 Stanford + CCG 90.56** (2.5) 88.16** (2.5) 90.93** (5.9) 87.73** (4.3) CoNLL Baseline 89.99 88.73 90.94 89.69 CoNLL + CCG 90.38** (4.0) 89.19** (4.1) 91.48** (5.9) 90.23** (5.3) Hindi Baseline 88.67 (2.2) 83.04 (1.1) 90.52 80.67 88.93** (3.3) 83.23* (1.9) 89.04** 83.35* Fine CCG 90.97** (4.8) 80.94* (1.4) Coarse CCG 90.88** (3.8) 80.73* (0.4) Table 1: Impact of CCG categories from a supertagger on dependency parsing. Numbers in</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the conference on Empirical methods in natural language processing, EMNLP ’02, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Coppola</author>
<author>Mark Steedman</author>
</authors>
<title>The effect of higher-order dependency features in discriminative phrase-structure parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>610--616</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3394" citStr="Coppola and Steedman (2013)" startWordPosition="498" endWordPosition="501">lly rich language, indicating that CCG categories from a supertagger are an easy and robust way of introducing lexicalized subcategorization information into dependency parsers. 2 Related Work Parsers using different grammar formalisms have different strengths and weaknesses, and prior work has shown that information from one formalism can improve the performance of a parser in another formalism. Sagae et al. (2007) achieved a 1.4% improvement in accuracy over a state-of-the-art HPSG parser by using dependencies from a dependency parser for constraining wide-coverage rules in the HPSG parser. Coppola and Steedman (2013) incorporated higher-order dependency features into a cube decoding phrasestructure parser and obtained significant gains on dependency recovery for both in-domain and out-of-domain test sets. Kim et al. (2012) improved a CCG parser using dependency features. They extracted n-best parses from a CCG parser and provided dependency 159 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 159–163, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Pierre Vinken will join the board as a nonexecutive di</context>
</contexts>
<marker>Coppola, Steedman, 2013</marker>
<rawString>Greg Coppola and Mark Steedman. 2013. The effect of higher-order dependency features in discriminative phrase-structure parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 610–616, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses. In</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>9--1871</pages>
<contexts>
<context position="7241" citStr="Fan et al., 2008" startWordPosition="1088" endWordPosition="1091">of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. 3.3 Dependency Parsers There has been a significant amount of work on parsing English and Hindi using the Malt and MST parsers in the recent past (Nivre et al., 2007a; Bharati et al., 2012). We first run these parsers with previous best settings (McDonald et al., 2005; Zhang and Nivre, 2012; Bharati et al., 2012) and treat them as our baseline. In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLINEAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002). For Hindi, we also did all our experiments using automatic features 160 Language Experiment Malt MST UAS LAS UAS LAS English Stanford Baseline 90.32 87.87 90.36 87.18 Stanford + CCG 90.56** (2.5) 88.16** (2.5) 90.93** (5.9) 87</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. J. Mach. Learn. Res., 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="5856" citStr="Hockenmaier and Steedman, 2007" startWordPosition="877" endWordPosition="881">NLL dependency schemes are widely popular. We used the Stanford parser’s built-in converter (with the basic projective option) to generate Stanford dependencies and Penn2Malt1 to generate CoNLL dependencies from Penn Treebank (Marcus et al., 1993). We used standard splits, training (sections 02-21), development (section 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012). HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-grained one (without morphological information). 3.2 Supertaggers We used Clark and Curran (2004)’s supertagger for English, and Ambati et al. (2013)’s supertagger for Hindi. Both are Maximum Entropy based CCG supertaggers. The Clark and Curran (2004) supertagger uses different features like word, partof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy o</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunghwan Mac Kim</author>
<author>Dominick Ng</author>
<author>Mark Johnson</author>
<author>James Curran</author>
</authors>
<title>Improving combinatory categorial grammar parse reranking with dependency grammar features.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1441--1458</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="3604" citStr="Kim et al. (2012)" startWordPosition="527" endWordPosition="530">mmar formalisms have different strengths and weaknesses, and prior work has shown that information from one formalism can improve the performance of a parser in another formalism. Sagae et al. (2007) achieved a 1.4% improvement in accuracy over a state-of-the-art HPSG parser by using dependencies from a dependency parser for constraining wide-coverage rules in the HPSG parser. Coppola and Steedman (2013) incorporated higher-order dependency features into a cube decoding phrasestructure parser and obtained significant gains on dependency recovery for both in-domain and out-of-domain test sets. Kim et al. (2012) improved a CCG parser using dependency features. They extracted n-best parses from a CCG parser and provided dependency 159 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 159–163, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Pierre Vinken will join the board as a nonexecutive director Nov. 29 N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N &gt; &gt; &gt; &gt; N NP N (S\NP)\(S\NP) T &gt; NP NP &gt; &gt; (S[b]\NP)/PP PP &gt; S[b]\NP &lt; S[b]\NP &gt; S[dcl] Figure 1: A CCG d</context>
</contexts>
<marker>Kim, Ng, Johnson, Curran, 2012</marker>
<rawString>Sunghwan Mac Kim, Dominick Ng, Mark Johnson, and James Curran. 2012. Improving combinatory categorial grammar parse reranking with dependency grammar features. In Proceedings of COLING 2012, pages 1441–1458, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing. Synthesis Lectures on Human Language Technologies.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5472" citStr="Marcus et al., 1993" startWordPosition="820" endWordPosition="823"> scores respectively. 3 Data and Tools Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence. (Details of CCG and dependency parsing are given by Steedman (2000) and K¨ubler et al. (2009).) 3.1 Treebanks In English dependency parsing literature, Stanford and CoNLL dependency schemes are widely popular. We used the Stanford parser’s built-in converter (with the basic projective option) to generate Stanford dependencies and Penn2Malt1 to generate CoNLL dependencies from Penn Treebank (Marcus et al., 1993). We used standard splits, training (sections 02-21), development (section 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012). HDT contains 12,041 training, 1,233 development and 1,828 testing sentences. We used the English (Hockenmaier and Steedman, 2007) and Hindi CCGbanks (Ambati et al., 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html 2013) for our experiments. For Hindi we used two lexicons: a fine-grained one (with morphological information) and a coarse-gra</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2051" citStr="McDonald et al., 2005" startWordPosition="299" endWordPosition="302">s and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012). Ambati et al. (2013) showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000). In this paper, we extend this work and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexical categories contain subcategorization information regarding the dependencies of predicates, including longdistance dependencies. We show that providing this subcategorization information in the form of CCG categories can help both Malt and MST on precisely those dependencies for which they are known to have weak rates of recovery. The result is particularly interesting for Malt, the fast greedy parser, as the improvement in Malt comes without significantly compromising its speed, so that it can be practically applied in web scale parsing. Our results apply both </context>
<context position="6991" citStr="McDonald et al., 2005" startWordPosition="1048" endWordPosition="1051">rtof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, Ambati et al. (2013) employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. 3.3 Dependency Parsers There has been a significant amount of work on parsing English and Hindi using the Malt and MST parsers in the recent past (Nivre et al., 2007a; Bharati et al., 2012). We first run these parsers with previous best settings (McDonald et al., 2005; Zhang and Nivre, 2012; Bharati et al., 2012) and treat them as our baseline. In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLINEAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 91–98, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007a. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1501" citStr="Nivre et al., 2007" startWordPosition="209" endWordPosition="212">for MST. The result is particularly interesting in the case of the fast greedy parser (Malt), since improving its accuracy without significantly compromising speed is relevant for large scale applications such as parsing the web. 1 Introduction Dependency parsers can recover much of the predicate-argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing. Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012). Ambati et al. (2013) showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000). In this paper, we extend this work and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexical categories contai</context>
<context position="6888" citStr="Nivre et al., 2007" startWordPosition="1031" endWordPosition="1034">ased CCG supertaggers. The Clark and Curran (2004) supertagger uses different features like word, partof-speech, and contextual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, Ambati et al. (2013) employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. 3.3 Dependency Parsers There has been a significant amount of work on parsing English and Hindi using the Malt and MST parsers in the recent past (Nivre et al., 2007a; Bharati et al., 2012). We first run these parsers with previous best settings (McDonald et al., 2005; Zhang and Nivre, 2012; Bharati et al., 2012) and treat them as our baseline. In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLINEAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, n</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007b. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>HPSG parsing with shallow dependency constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>624--631</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3186" citStr="Sagae et al. (2007)" startWordPosition="467" endWordPosition="470"> so that it can be practically applied in web scale parsing. Our results apply both to English, a fixed word order and morphologically simple language, and to Hindi, a free word order and morphologically rich language, indicating that CCG categories from a supertagger are an easy and robust way of introducing lexicalized subcategorization information into dependency parsers. 2 Related Work Parsers using different grammar formalisms have different strengths and weaknesses, and prior work has shown that information from one formalism can improve the performance of a parser in another formalism. Sagae et al. (2007) achieved a 1.4% improvement in accuracy over a state-of-the-art HPSG parser by using dependencies from a dependency parser for constraining wide-coverage rules in the HPSG parser. Coppola and Steedman (2013) incorporated higher-order dependency features into a cube decoding phrasestructure parser and obtained significant gains on dependency recovery for both in-domain and out-of-domain test sets. Kim et al. (2012) improved a CCG parser using dependency features. They extracted n-best parses from a CCG parser and provided dependency 159 Proceedings of the 14th Conference of the European Chapte</context>
</contexts>
<marker>Sagae, Miyao, Tsujii, 2007</marker>
<rawString>Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii. 2007. HPSG parsing with shallow dependency constraints. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624– 631, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1744" citStr="Steedman, 2000" startWordPosition="250" endWordPosition="251">ependency parsers can recover much of the predicate-argument structure of a sentence, while being relatively efficient to train and extremely fast at parsing. Dependency parsers have been gaining in popularity in recent times due to the availability of large dependency treebanks for several languages and parsing shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a; Bharati et al., 2012). Ambati et al. (2013) showed that the performance of Malt (Nivre et al., 2007b) on the free word order language, Hindi, is improved by using lexical categories from Combinatory Categorial Grammar (CCG) (Steedman, 2000). In this paper, we extend this work and show that CCG categories are useful even in the case of English, a typologically different language, where parsing accuracy of dependency parsers is already extremely high. In addition, we also demonstrate the utility of CCG categories to MST (McDonald et al., 2005) for both languages. CCG lexical categories contain subcategorization information regarding the dependencies of predicates, including longdistance dependencies. We show that providing this subcategorization information in the form of CCG categories can help both Malt and MST on precisely thos</context>
<context position="5125" citStr="Steedman (2000)" startWordPosition="773" endWordPosition="774">using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013), they first created a Hindi CCGbank from a Hindi dependency treebank and built a supertagger. They provided CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. 3 Data and Tools Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence. (Details of CCG and dependency parsing are given by Steedman (2000) and K¨ubler et al. (2009).) 3.1 Treebanks In English dependency parsing literature, Stanford and CoNLL dependency schemes are widely popular. We used the Stanford parser’s built-in converter (with the basic projective option) to generate Stanford dependencies and Penn2Malt1 to generate CoNLL dependencies from Penn Treebank (Marcus et al., 1993). We used standard splits, training (sections 02-21), development (section 22) and testing (section 23) for our experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et al., 2012)</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumire Uematsu</author>
<author>Takuya Matsuzaki</author>
<author>Hiroki Hanaoka</author>
<author>Yusuke Miyao</author>
<author>Hideki Mima</author>
</authors>
<title>Integrating multiple dependency corpora for inducing wide-coverage Japanese CCG resources.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1042--1051</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="4601" citStr="Uematsu et al. (2013)" startWordPosition="687" endWordPosition="690">onexecutive director Nov. 29 N/N N (S[dcl]\NP)/(S[b]\NP) ((S[b]\NP)/PP)/NP NP/N N PP/NP NP/N N/N N ((S\NP)\(S\NP))/N N &gt; &gt; &gt; &gt; N NP N (S\NP)\(S\NP) T &gt; NP NP &gt; &gt; (S[b]\NP)/PP PP &gt; S[b]\NP &lt; S[b]\NP &gt; S[dcl] Figure 1: A CCG derivation and the Stanford scheme dependencies for an example sentence. S[dcl]\NP &gt; features from a dependency parser to a re-ranker with an improvement of 0.35% in labelled F-score of the CCGbank test set. Conversely, Ambati et al. (2013) showed that a Hindi dependency parser (Malt) could be improved by using CCG categories. Using an algorithm similar to Cakici (2005) and Uematsu et al. (2013), they first created a Hindi CCGbank from a Hindi dependency treebank and built a supertagger. They provided CCG categories from a supertagger as features to Malt and obtained overall improvements of 0.3% and 0.4% in unlabelled and labelled attachment scores respectively. 3 Data and Tools Figure 1 shows a CCG derivation with CCG lexical categories for each word and Stanford scheme dependencies (De Marneffe et al., 2006) for an example English sentence. (Details of CCG and dependency parsing are given by Steedman (2000) and K¨ubler et al. (2009).) 3.1 Treebanks In English dependency parsing lit</context>
</contexts>
<marker>Uematsu, Matsuzaki, Hanaoka, Miyao, Mima, 2013</marker>
<rawString>Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka, Yusuke Miyao, and Hideki Mima. 2013. Integrating multiple dependency corpora for inducing wide-coverage Japanese CCG resources. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1042–1051, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="13604" citStr="Zhang and Nivre, 2011" startWordPosition="2139" endWordPosition="2142">g CCG categories as features improved Malt in better handling of long distance dependencies. The percentage of dependencies in the 1−5, 6−10 and &gt;10 distance ranges are 82.2%, 8.6% and 9.2% respectively out of the total of around 40,000 dependencies. Similar to English, there was very slight improvement for short distance dependencies (1−5). But for longer distances, 6−10, and &gt;10, there was significant improvement of 1.3% and 1.3% respectively for MST. Ambati et al. (2013) reported similar improvements for Malt as well. 4.4 Discussion Though valency is a useful feature in dependency parsing (Zhang and Nivre, 2011), Zhang and Nivre (2012) showed that providing valency information dynamically, in the form of the number of dependencies established in a particular state during parsing, did not help Malt. However, as we have shown above, providing this information as a static lexical feature in the form of CCG categories does help Malt. In addition to specifying the number of arguments, CCG categories also contain syntactic type and direction of those arguments. However, providing CCG categories as features to zpar (Zhang and Nivre, 2011) didn’t have significant impact as it is already using similar informa</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing the effect of global learning and beam-search on transition-based dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1391--1400</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="7014" citStr="Zhang and Nivre, 2012" startWordPosition="1052" endWordPosition="1055">tual and complex bi-gram features to obtain a 1-best accuracy of 91.5% on the development set. In addition to the above mentioned features, Ambati et al. (2013) employed morphological features useful for Hindi. The 1-best accuracy of Hindi supertagger for finegrained and coarse-grained lexicon is 82.92% and 84.40% respectively. 3.3 Dependency Parsers There has been a significant amount of work on parsing English and Hindi using the Malt and MST parsers in the recent past (Nivre et al., 2007a; Bharati et al., 2012). We first run these parsers with previous best settings (McDonald et al., 2005; Zhang and Nivre, 2012; Bharati et al., 2012) and treat them as our baseline. In the case of English, Malt uses arc-standard and stack-projective parsing algorithms for CoNLL and Stanford schemes respectively and LIBLINEAR learner (Fan et al., 2008) for both the schemes. MST uses 1st-order features, and a projective parsing algorithm with 5-best MIRA training for both the schemes. For Hindi, Malt uses the arc-standard parsing algorithm with a LIBLINEAR learner. MST uses 2nd-order features, nonprojective algorithm with 5-best MIRA training. For English, we assigned POS-tags using a perceptron tagger (Collins, 2002).</context>
<context position="13628" citStr="Zhang and Nivre (2012)" startWordPosition="2143" endWordPosition="2146">ures improved Malt in better handling of long distance dependencies. The percentage of dependencies in the 1−5, 6−10 and &gt;10 distance ranges are 82.2%, 8.6% and 9.2% respectively out of the total of around 40,000 dependencies. Similar to English, there was very slight improvement for short distance dependencies (1−5). But for longer distances, 6−10, and &gt;10, there was significant improvement of 1.3% and 1.3% respectively for MST. Ambati et al. (2013) reported similar improvements for Malt as well. 4.4 Discussion Though valency is a useful feature in dependency parsing (Zhang and Nivre, 2011), Zhang and Nivre (2012) showed that providing valency information dynamically, in the form of the number of dependencies established in a particular state during parsing, did not help Malt. However, as we have shown above, providing this information as a static lexical feature in the form of CCG categories does help Malt. In addition to specifying the number of arguments, CCG categories also contain syntactic type and direction of those arguments. However, providing CCG categories as features to zpar (Zhang and Nivre, 2011) didn’t have significant impact as it is already using similar information. 4.5 Impact on Web </context>
</contexts>
<marker>Zhang, Nivre, 2012</marker>
<rawString>Yue Zhang and Joakim Nivre. 2012. Analyzing the effect of global learning and beam-search on transition-based dependency parsing. In Proceedings of COLING 2012: Posters, pages 1391–1400, Mumbai, India, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>