<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.982893">
A Hybrid Convolution Tree Kernel for Semantic Role Labeling
</title>
<note confidence="0.612949333333333">
Wanxiang Che Min Zhang Ting Liu, Sheng Li
Harbin Inst. of Tech. Inst. for Infocomm Research Harbin Inst. of Tech.
Harbin, China, 150001 Singapore, 119613 Harbin, China, 150001
</note>
<email confidence="0.775784">
car@ir.hit.edu.cn mzhang@i2r.a-star.edu.sg {tliu, ls}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.973444" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979315789474">
A hybrid convolution tree kernel is pro-
posed in this paper to effectively model
syntactic structures for semantic role la-
beling (SRL). The hybrid kernel consists
of two individual convolution kernels: a
Path kernel, which captures predicate-
argument link features, and a Constituent
Structure kernel, which captures the syn-
tactic structure features of arguments.
Evaluation on the datasets of CoNLL-
2005 SRL shared task shows that the
novel hybrid convolution tree kernel out-
performs the previous tree kernels. We
also combine our new hybrid tree ker-
nel based method with the standard rich
flat feature based method. The experi-
mental results show that the combinational
method can get better performance than
each of them individually.
</bodyText>
<sectionHeader confidence="0.992544" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9988975625">
In the last few years there has been increasing in-
terest in Semantic Role Labeling (SRL). It is cur-
rently a well defined task with a substantial body
of work and comparative evaluation. Given a sen-
tence, the task consists of analyzing the proposi-
tions expressed by some target verbs and some
constituents of the sentence. In particular, for each
target verb (predicate) all the constituents in the
sentence which fill a semantic role (argument) of
the verb have to be recognized.
Figure 1 shows an example of a semantic role
labeling annotation in PropBank (Palmer et al.,
2005). The PropBank defines 6 main arguments,
Arg0 is the Agent, Arg1 is Patient, etc. ArgM-
may indicate adjunct arguments, such as Locative,
Temporal.
</bodyText>
<note confidence="0.5593995">
Many researchers (Gildea and Jurafsky, 2002;
Pradhan et al., 2005a) use feature-based methods
S
Arg1 ArgM-LOC
</note>
<figureCaption confidence="0.880562">
Figure 1: Semantic role labeling in a phrase struc-
ture syntactic tree representation
</figureCaption>
<bodyText confidence="0.999797884615385">
for argument identification and classification in
building SRL systems and participating in eval-
uations, such as Senseval-3 1, CoNLL-2004 and
2005 shared tasks: SRL (Carreras and M`arquez,
2004; Carreras and M`arquez, 2005), where a
flat feature vector is usually used to represent a
predicate-argument structure. However, it’s hard
for this kind of representation method to explicitly
describe syntactic structure information by a vec-
tor of flat features. As an alternative, convolution
tree kernel methods (Collins and Duffy, 2001)
provide an elegant kernel-based solution to im-
plicitly explore tree structure features by directly
computing the similarity between two trees. In
addition, some machine learning algorithms with
dual form, such as Perceptron and Support Vector
Machines (SVM) (Cristianini and Shawe-Taylor,
2000), which do not need know the exact presen-
tation of objects and only need compute their ker-
nel functions during the process of learning and
prediction. They can be well used as learning al-
gorithms in the kernel-based methods. They are
named kernel machines.
In this paper, we decompose the Moschitti
(2004)’s predicate-argument feature (PAF) kernel
into a Path kernel and a Constituent Structure ker-
</bodyText>
<figure confidence="0.953943733333333">
1http://www.cs.unt.edu/—rada/senseval/senseval3/
NP
PP
DT NN
IN NN
NP VP
PRP
VBD
She bought
silk
the
in
ArgO V
China
73
</figure>
<note confidence="0.8605575">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999966142857143">
nel, and then compose them into a hybrid con-
volution tree kernel. Our hybrid kernel method
using Voted Perceptron kernel machine outper-
forms the PAF kernel in the development sets of
CoNLL-2005 SRL shared task. In addition, the fi-
nal composing kernel between hybrid convolution
tree kernel and standard features’ polynomial ker-
nel outperforms each of them individually.
The remainder of the paper is organized as fol-
lows: In Section 2 we review the previous work.
In Section 3 we illustrate the state of the art
feature-based method for SRL. Section 4 discusses
our method. Section 5 shows the experimental re-
sults. We conclude our work in Section 6.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99994058490566">
Automatic semantic role labeling was first intro-
duced by Gildea and Jurafsky (2002). They used
a linear interpolation method and extract features
from a parse tree to identify and classify the con-
stituents in the FrameNet (Baker et al., 1998) with
syntactic parsing results. Here, the basic features
include Phrase Type, Parse Tree Path, Position.
Most of the following works focused on feature
engineering (Xue and Palmer, 2004; Jiang et al.,
2005) and machine learning models (Nielsen and
Pradhan, 2004; Pradhan et al., 2005a). Some
other works paid much attention to the robust SRL
(Pradhan et al., 2005b) and post inference (Pun-
yakanok et al., 2004).
These feature-based methods are considered as
the state of the art method for SRL and achieved
much success. However, as we know, the standard
flat features are less effective to model the syntac-
tic structured information. It is sensitive to small
changes of the syntactic structure features. This
can give rise to a data sparseness problem and pre-
vent the learning algorithms from generalizing un-
seen data well.
As an alternative to the standard feature-based
methods, kernel-based methods have been pro-
posed to implicitly explore features in a high-
dimension space by directly calculating the sim-
ilarity between two objects using kernel function.
In particular, the kernel methods could be effective
in reducing the burden of feature engineering for
structured objects in NLP problems. This is be-
cause a kernel can measure the similarity between
two discrete structured objects directly using the
original representation of the objects instead of ex-
plicitly enumerating their features.
Many kernel functions have been proposed in
machine learning community and have been ap-
plied to NLP study. In particular, Haussler (1999)
and Watkins (1999) proposed the best-known con-
volution kernels for a discrete structure. In the
context of convolution kernels, more and more
kernels for restricted syntaxes or specific do-
mains, such as string kernel for text categoriza-
tion (Lodhi et al., 2002), tree kernel for syntactic
parsing (Collins and Duffy, 2001), kernel for re-
lation extraction (Zelenko et al., 2003; Culotta
and Sorensen, 2004) are proposed and explored
in NLP domain. Of special interest here, Mos-
chitti (2004) proposed Predicate Argument Fea-
ture (PAF) kernel under the framework of convo-
lution tree kernel for SRL. In this paper, we fol-
low the same framework and design a novel hybrid
convolution kernel for SRL.
</bodyText>
<sectionHeader confidence="0.999655" genericHeader="method">
3 Feature-based methods for SRL
</sectionHeader>
<bodyText confidence="0.998881777777778">
Usually feature-based methods refer to the meth-
ods which use the flat features to represent in-
stances. At present, most of the successful SRL
systems use this method. Their features are usu-
ally extended from Gildea and Jurafsky (2002)’s
work, which uses flat information derived from
a parse tree. According to the literature, we
select the Constituent, Predicate, and Predicate-
Constituent related features shown in Table 1.
</bodyText>
<table confidence="0.9987775">
Feature Description
Constituent related features
Phrase Type syntactic category of the constituent
Head Word head word of the constituent
Last Word last word of the constituent
First Word first word of the constituent
Named Entity named entity type of the constituent’s head word
POS part of speech of the constituent
Previous Word sequence previous word of the constituent
Next Word sequence next word of the constituent
Predicate related features
Predicate predicate lemma
Voice grammatical voice of the predicate, either active or passive
SubCat Sub-category of the predicate’s parent node
Predicate POS part of speech of the predicate
Suffix suffix of the predicate
Predicate-Constituent related features
Path parse tree path from the predicate to the constituent
Position the relative position of the constituent and the predicate, before or after
Path Length the nodes number on the parse tree path
Partial Path some part on the parse tree path
Clause Layers the clause layers from the constituent to the predicate
</table>
<tableCaption confidence="0.999836">
Table 1: Standard flat features
</tableCaption>
<bodyText confidence="0.997130714285714">
However, to find relevant features is, as usual,
a complex task. In addition, according to the de-
scription of the standard features, we can see that
the syntactic features, such as Path, Path Length,
bulk large among all features. On the other hand,
the previous researches (Gildea and Palmer, 2002;
Punyakanok et al., 2005) have also recognized the
</bodyText>
<figure confidence="0.856763">
74
Arg1 ArgM-LOC
</figure>
<figureCaption confidence="0.999277">
Figure 2: Predicate Argument Feature space
</figureCaption>
<bodyText confidence="0.999959">
necessity of syntactic parsing for semantic role la-
beling. However, the standard flat features cannot
model the syntactic information well. A predicate-
argument pair has two different Path features even
if their paths differ only for a node in the parse
tree. This data sparseness problem prevents the
learning algorithms from generalizing unseen data
well. In order to address this problem, one method
is to list all sub-structures of the parse tree. How-
ever, both space complexity and time complexity
are too high for the algorithm to be realized.
</bodyText>
<sectionHeader confidence="0.9736065" genericHeader="method">
4 Hybrid Convolution Tree Kernels for
SRL
</sectionHeader>
<bodyText confidence="0.9999275">
In this section, we introduce the previous ker-
nel method for SRL in Subsection 4.1, discuss
our method in Subsection 4.2 and compare our
method with previous work in Subsection 4.3.
</bodyText>
<subsectionHeader confidence="0.999477">
4.1 Convolution Tree Kernels for SRL
</subsectionHeader>
<bodyText confidence="0.978303772727273">
Moschitti (2004) proposed to apply convolution
tree kernels (Collins and Duffy, 2001) to SRL.
He selected portions of syntactic parse trees,
which include salient sub-structures of predicate-
arguments, to define convolution kernels for the
task of predicate argument classification. This por-
tions selection method of syntactic parse trees is
named as predicate-arguments feature (PAF) ker-
nel. Figure 2 illustrates the PAF kernel feature
space of the predicate buy and the argument Arg1
in the circled sub-structure.
The kind of convolution tree kernel is similar to
Collins and Duffy (2001)’s tree kernel except the
sub-structure selection strategy. Moschitti (2004)
only selected the relative portion between a predi-
cate and an argument.
Given a tree portion instance defined above, we
design a convolution tree kernel in a way similar
to the parse tree kernel (Collins and Duffy, 2001).
Firstly, a parse tree T can be represented by a vec-
tor of integer counts of each sub-tree type (regard-
less of its ancestors):
</bodyText>
<listItem confidence="0.940227666666667">
4b(T) = (# of sub-trees of type 1, ... ,
# of sub-trees of type i, ... ,
# of sub-trees of type n)
</listItem>
<bodyText confidence="0.9998435">
This results in a very high dimension since the
number of different subtrees is exponential to the
tree’s size. Thus it is computationally infeasible
to use the feature vector 4b(T) directly. To solve
this problem, we introduce the tree kernel function
which is able to calculate the dot product between
the above high-dimension vectors efficiently. The
kernel function is defined as following:
</bodyText>
<equation confidence="0.992174">
K(T1, T2) _ (`D(T1), `D(T2)) _ PriOi(T1), Oi(T2)
_ Pn1111 Pn2112 /moi Ii(n1) * Ii(n2)
</equation>
<bodyText confidence="0.761166222222222">
where N1 and N2 are the sets of all nodes in
trees T1 and T2, respectively, and Ii(n) is the in-
dicator function whose value is 1 if and only if
there is a sub-tree of type i rooted at node n and
0 otherwise. Collins and Duffy (2001) show that
K(T1,T2) is an instance of convolution kernels
over tree structures, which can be computed in
O(|N1 |x |N2|) by the following recursive defi-
nitions (Let A(n1, n2) = Pi Ii(n1) * Ii(n2)):
</bodyText>
<listItem confidence="0.864448">
(1) if the children of n1 and n2 are different then
A(n1, n2) = 0;
(2) else if their children are the same and they are
leaves, then A(n1, n2) = µ;
</listItem>
<equation confidence="0.860405333333333">
µ Q��h�1)
(3) else A(n1, n2) = ��1 (1 +
A(ch(n1, j), ch(n2, j)))
</equation>
<bodyText confidence="0.9957578">
where nc(n1) is the number of the children of
n1, ch(n, j) is the jth child of node n and µ(0 &lt;
µ &lt; 1) is the decay factor in order to make the
kernel value less variable with respect to the tree
sizes.
</bodyText>
<subsectionHeader confidence="0.984156">
4.2 Hybrid Convolution Tree Kernels
</subsectionHeader>
<bodyText confidence="0.99994525">
In the PAF kernel, the feature spaces are consid-
ered as an integral portion which includes a pred-
icate and one of its arguments. We note that the
PAF feature consists of two kinds of features: one
is the so-called parse tree Path feature and another
one is the so-called Constituent Structure feature.
These two kinds of feature spaces represent dif-
ferent information. The Path feature describes the
</bodyText>
<figure confidence="0.999433234042553">
PRP
NP
VBD
PP
in
China
S
NP VP
She bought
ArgO V
DT NN
silk
IN NN
the
75
PRP
VBD
VBD
She bought
�
She bought
S
S
NP VP
NP
NP VP
+ PRP PRP VBD +
VBD
PRP
bought
She
bought She
PRP
NP
VBD
PP
in
China
S
NP VP
She bought
ArgO V
DT NN
silk
IN NN
the
Arg1 ArgM-LOC
</figure>
<figureCaption confidence="0.7993905">
Figure 3: Path and Constituent Structure feature
spaces
</figureCaption>
<figure confidence="0.950572666666667">
S
NP VP
S
NP VP
(a) PAF Kernel
�
</figure>
<bodyText confidence="0.999920736842105">
linking information between a predicate and its ar-
guments while the Constituent Structure feature
captures the syntactic structure information of an
argument. We believe that it is more reasonable
to capture the two different kinds of features sepa-
rately since they contribute to SRL in different fea-
ture spaces and it is better to give different weights
to fuse them. Therefore, we propose two convo-
lution kernels to capture the two features, respec-
tively and combine them into one hybrid convolu-
tion kernel for SRL. Figure 3 is an example to il-
lustrate the two feature spaces, where the Path fea-
ture space is circled by solid curves and the Con-
stituent Structure feature spaces is circled by dot-
ted curves. We name them Path kernel and Con-
stituent Structure kernel respectively.
Figure 4 illustrates an example of the distinc-
tion between the PAF kernel and our kernel. In
the PAF kernel, the tree structures are equal when
considering constitutes NP and PRP, as shown in
Figure 4(a). However, the two constituents play
different roles in the sentence and should not be
looked as equal. Figure 4(b) shows the comput-
ing example with our kernel. During computing
the hybrid convolution tree kernel, the NP–PRP
substructure is not computed. Therefore, the two
trees are distinguished correctly.
On the other hand, the constituent structure fea-
ture space reserves the most part in the traditional
PAF feature space usually. Then the Constituent
Structure kernel plays the main role in PAF kernel
computation, as shown in Figure 5. Here, believes
is a predicate and A1 is a long sub-sentence. Ac-
cording to our experimental results in Section 5.2,
we can see that the Constituent Structure kernel
does not perform well. Affected by this, the PAF
kernel cannot perform well, either. However, in
our hybrid method, we can adjust the compromise
</bodyText>
<figure confidence="0.892643">
(b) Hybrid Convolution Tree Kernel
</figure>
<figureCaption confidence="0.9784825">
Figure 4: Comparison between PAF and Hybrid
Convolution Tree Kernels
</figureCaption>
<bodyText confidence="0.999896909090909">
of the Path feature and the Constituent Structure
feature by tuning their weights to get an optimal
result.
Having defined two convolution tree kernels,
the Path kernel Kpath and the Constituent Struc-
ture kernel Kc,, we can define a new kernel to
compose and extend the individual kernels. Ac-
cording to Joachims et al. (2001), the kernel func-
tion set is closed under linear combination. It
means that the following Khybrid is a valid kernel
if Kpath and Kc� are both valid.
</bodyText>
<equation confidence="0.990534">
Khybrid = AKpath + (1 − A)Kc� (1)
</equation>
<bodyText confidence="0.99921175">
where 0 &lt; A &lt; 1.
According to the definitions of the Path and the
Constituent Structure kernels, each kernel is ex-
plicit. They can be viewed as a matching of fea-
</bodyText>
<figureCaption confidence="0.979414">
Figure 5: An example of Semantic Role Labeling
</figureCaption>
<bodyText confidence="0.934651875">
76
tures. Since the features are enumerable on the
given data, the kernels are all valid. Therefore, the
new kernel Khybrid is valid. We name the new ker-
nel hybrid convolution tree kernel, Khybrid.
Since the size of a parse tree is not con-
stant, we normalize K(T1, T2) by dividing it by
�IK(T1, T1) · K(T2, T2)
</bodyText>
<subsectionHeader confidence="0.999874">
4.3 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.999994292682927">
It would be interesting to investigate the differ-
ences between our method and the feature-based
methods. The basic difference between them lies
in the instance representation (parse tree vs. fea-
ture vector) and the similarity calculation mecha-
nism (kernel function vs. dot-product). The main
difference between them is that they belong to dif-
ferent feature spaces. In the kernel methods, we
implicitly represent a parse tree by a vector of in-
teger counts of each sub-tree type. That is to say,
we consider all the sub-tree types and their occur-
ring frequencies. In this way, on the one hand,
the predicate-argument related features, such as
Path, Position, in the flat feature set are embed-
ded in the Path feature space. Additionally, the
Predicate, Predicate POS features are embedded
in the Path feature space, too. The constituent re-
lated features, such as Phrase Type, Head Word,
Last Word, and POS, are embedded in the Con-
stituent Structure feature space. On the other hand,
the other features in the flat feature set, such as
Named Entity, Previous, and Next Word, Voice,
SubCat, Suffix, are not contained in our hybrid
convolution tree kernel. From the syntactic view-
point, the tree representation in our feature space
is more robust than the Parse Tree Path feature in
the flat feature set since the Path feature is sensi-
tive to small changes of the parse trees and it also
does not maintain the hierarchical information of
a parse tree.
It is also worth comparing our method with
the previous kernels. Our method is similar to
the Moschitti (2004)’s predicate-argument feature
(PAF) kernel. However, we differentiate the Path
feature and the Constituent Structure feature in our
hybrid kernel in order to more effectively capture
the syntactic structure information for SRL. In ad-
dition Moschitti (2004) only study the task of ar-
gument classification while in our experiment, we
report the experimental results on both identifica-
tion and classification.
</bodyText>
<sectionHeader confidence="0.988759" genericHeader="evaluation">
5 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999890666666667">
The aim of our experiments is to verify the effec-
tiveness of our hybrid convolution tree kernel and
and its combination with the standard flat features.
</bodyText>
<subsectionHeader confidence="0.9575315">
5.1 Experimental Setting
5.1.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9998098125">
We use the benchmark corpus provided by
CoNLL-2005 SRL shared task (Carreras and
M`arquez, 2005) provided corpus as our training,
development, and test sets. The data consist of
sections of the Wall Street Journal (WSJ) part of
the Penn TreeBank (Marcus et al., 1993), with
information on predicate-argument structures ex-
tracted from the PropBank corpus (Palmer et al.,
2005). We followed the standard partition used
in syntactic parsing: sections 02-21 for training,
section 24 for development, and section 23 for
test. In addition, the test set of the shared task
includes three sections of the Brown corpus. Ta-
ble 2 provides counts of sentences, tokens, anno-
tated propositions, and arguments in the four data
sets.
</bodyText>
<table confidence="0.9998592">
Train Devel tWSJ tBrown
Sentences 39,832 1,346 2,416 426
Tokens 950,028 32,853 56,684 7,159
Propositions 90,750 3,248 5,267 804
Arguments 239,858 8,346 14,077 2,177
</table>
<tableCaption confidence="0.999474">
Table 2: Counts on the data set
</tableCaption>
<bodyText confidence="0.9998734">
The preprocessing modules used in CONLL-
2005 include an SVM based POS tagger (Gim´enez
and M`arquez, 2003), Charniak (2000)’s full syn-
tactic parser, and Chieu and Ng (2003)’s Named
Entity recognizer.
</bodyText>
<subsubsectionHeader confidence="0.72691">
5.1.2 Evaluation
</subsubsectionHeader>
<bodyText confidence="0.999785083333333">
The system is evaluated with respect to
precision, recall, and F,3=1 of the predicted ar-
guments. Precision (p) is the proportion of ar-
guments predicted by a system which are cor-
rect. Recall (r) is the proportion of correct ar-
guments which are predicted by a system. F,3=1
computes the harmonic mean of precision and
recall, which is the final measure to evaluate the
performances of systems. It is formulated as:
F,3=1 = 2pr/(p + r). srl-eval.pl2 is the official
program of the CoNLL-2005 SRL shared task to
evaluate a system performance.
</bodyText>
<footnote confidence="0.664548">
2http://www.lsi.upc.edu/—srlconll/srl-eval.pl
</footnote>
<page confidence="0.732633">
77
</page>
<subsubsectionHeader confidence="0.616109">
5.1.3 SRL Strategies
</subsubsectionHeader>
<bodyText confidence="0.999511105263158">
We use constituents as the labeling units to form
the labeled arguments. In order to speed up the
learning process, we use a four-stage learning ar-
chitecture:
Stage 1: To save time, we use a pruning
stage (Xue and Palmer, 2004) to filter out the
constituents that are clearly not semantic ar-
guments to the predicate.
Stage 2: We then identify the candidates derived
from Stage 1 as either arguments or non-
arguments.
Stage 3: A multi-category classifier is used to
classify the constituents that are labeled as ar-
guments in Stage 2 into one of the argument
classes plus NULL.
Stage 4: A rule-based post-processing stage (Liu
et al., 2005) is used to handle some un-
matched arguments with constituents, such as
AM-MOD, AM-NEG.
</bodyText>
<subsubsectionHeader confidence="0.456985">
5.1.4 Classifier
</subsubsectionHeader>
<bodyText confidence="0.999982705882353">
We use the Voted Perceptron (Freund and
Schapire, 1998) algorithm as the kernel machine.
The performance of the Voted Perceptron is close
to, but not as good as, the performance of SVM on
the same problem, while saving computation time
and programming effort significantly. SVM is too
slow to finish our experiments for tuning parame-
ters.
The Voted Perceptron is a binary classifier. In
order to handle multi-classification problems, we
adopt the one vs. others strategy and select the
one with the largest margin as the final output. The
training parameters are chosen using development
data. After 5 iteration numbers, the best perfor-
mance is achieved. In addition, Moschitti (2004)’s
Tree Kernel Tool is used to compute the tree kernel
function.
</bodyText>
<subsectionHeader confidence="0.99955">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9859612">
In order to speed up the training process, in the
following experiments, we ONLY use WSJ sec-
tions 02-05 as training data. The same as Mos-
chitti (2004), we also set the p = 0.4 in the com-
putation of convolution tree kernels.
In order to study the impact of A in hybrid con-
volution tree kernel in Eq. 1, we only use the hy-
brid kernel between Kpath and Kms. The perfor-
mance curve on development set changing with A
is shown in Figure 6.
</bodyText>
<figure confidence="0.9518573">
67
65
63
61
59
57
55
53
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
lambda
</figure>
<figureCaption confidence="0.999982">
Figure 6: The performance curve changing with A
</figureCaption>
<bodyText confidence="0.999987194444445">
The performance curve shows that when A =
0.5, the hybrid convolution tree kernel gets the
best performance. Either the Path kernel (A = 1,
F,a_i = 61.26) or the Constituent Structure kernel
(A = 0, F,a—i = 54.91) cannot perform better than
the hybrid one. It suggests that the two individual
kernels are complementary to each other. In ad-
dition, the Path kernel performs much better than
the Constituent Structure kernel. It indicates that
the predicate-constituent related features are more
effective than the constituent features for SRL.
Table 3 compares the performance comparison
among our Hybrid convolution tree kernel, Mos-
chitti (2004)’s PAF kernel, standard flat features
with Linear kernels, and Poly kernel (d = 2). We
can see that our hybrid convolution tree kernel out-
performs the PAF kernel. It empirically demon-
strates that the weight linear combination in our
hybrid kernel is more effective than PAF kernel for
SRL.
However, our hybrid kernel still performs worse
than the standard feature based system. This is
simple because our kernel only use the syntac-
tic structure information while the feature-based
method use a large number of hand-craft diverse
features, from word, POS, syntax and semantics,
NER, etc. The standard features with polynomial
kernel gets the best performance. The reason is
that the arbitrary binary combination among fea-
tures implicated by the polynomial kernel is useful
to SRL. We believe that combining the two meth-
ods can perform better.
In order to make full use of the syntactic
information and the standard flat features, we
present a composite kernel between hybrid kernel
(Khybrid) and standard features with polynomial
</bodyText>
<page confidence="0.697971">
78
</page>
<table confidence="0.8088445">
Hybrid PAF Linear Poly
Devel 66.01 64.38 68.71 70.25
</table>
<tableCaption confidence="0.982815">
Table 3: Performance (F,a=1) comparison among
various kernels
</tableCaption>
<equation confidence="0.6908235">
kernel (Kpoly):
Kcomp = -yKhybrid + (1 − -y)Kpoly (2)
</equation>
<bodyText confidence="0.997293">
where 0 &lt; -y &lt; 1.
The performance curve changing with -y in Eq. 2
on development set is shown in Figure 7.
</bodyText>
<figure confidence="0.974757777777778">
72
71
70
69
68
67
66
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
gamma
</figure>
<figureCaption confidence="0.999993">
Figure 7: The performance curve changing with -y
</figureCaption>
<bodyText confidence="0.999992388888889">
We can see that when -y = 0.5, the system
achieves the best performance and F,3=1 = 70.78.
It’s statistically significant improvement (x2 test
with p = 0.1) than only using the standard features
with the polynomial kernel (-y = 0, F,3=1 = 70.25)
and much higher than only using the hybrid con-
volution tree kernel (-y = 1, F,3=1 = 66.01).
The main reason is that the convolution tree ker-
nel can represent more general syntactic features
than standard flat features, and the standard flat
features include the features that the convolution
tree kernel cannot represent, such as Voice, Sub-
Cat. The two kind features are complementary to
each other.
Finally, we train the composite method using
the above setting (Eq. 2 with when -y = 0.5) on the
entire training set. The final performance is shown
in Table 4.
</bodyText>
<sectionHeader confidence="0.994267" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9937175">
In this paper we proposed the hybrid convolu-
tion kernel to model syntactic structure informa-
tion for SRL. Different from the previous convo-
lution tree kernel based methods, our contribution
</bodyText>
<table confidence="0.999988125">
Precision Recall Fp=1
Development 80.71% 68.49% 74.10
Test WSJ 82.46% 70.65% 76.10
Test Brown 73.39% 57.01% 64.17
Test WSJ Precision Recall Fp=1
Overall 82.46% 70.65% 76.10
A0 87.97% 82.49% 85.14
A1 80.51% 71.69% 75.84
A2 75.79% 52.16% 61.79
A3 80.85% 43.93% 56.93
A4 83.56% 59.80% 69.71
A5 100.00% 20.00% 33.33
AM-ADV 66.27% 43.87% 52.79
AM-CAU 68.89% 42.47% 52.54
AM-DIR 56.82% 29.41% 38.76
AM-DIS 79.02% 75.31% 77.12
AM-EXT 73.68% 43.75% 54.90
AM-LOC 72.83% 50.96% 59.97
AM-MNR 68.54% 42.44% 52.42
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 49.32% 31.30% 38.30
AM-TMP 82.15% 68.17% 74.51
R-A0 86.28% 87.05% 86.67
R-A1 80.00% 74.36% 77.08
R-A2 100.00% 31.25% 47.62
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 50.00% 100.00% 66.67
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 20.00% 16.67% 18.18
R-AM-TMP 68.75% 63.46% 66.00
V 98.65% 98.65% 98.65
</table>
<tableCaption confidence="0.8274915">
Table 4: Overall results (top) and detailed results
on the WSJ test (bottom).
</tableCaption>
<bodyText confidence="0.999950157894737">
is that we distinguish between the Path and the
Constituent Structure feature spaces. Evaluation
on the datasets of CoNLL-2005 SRL shared task,
shows that our novel hybrid convolution tree ker-
nel outperforms the PAF kernel method. Although
the hybrid kernel base method is not as good as
the standard rich flat feature based methods, it can
improve the state of the art feature-based methods
by implicating the more generalizing syntactic in-
formation.
Kernel-based methods provide a good frame-
work to use some features which are difficult to
model in the standard flat feature based methods.
For example the semantic similarity of words can
be used in kernels well. We can use general pur-
pose corpus to create clusters of similar words or
use available resources like WordNet. We can also
use the hybrid kernel method into other tasks, such
as relation extraction in the future.
</bodyText>
<page confidence="0.867324">
79
</page>
<sectionHeader confidence="0.995713" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997275">
The authors would like to thank the reviewers for
their helpful comments and Shiqi Zhao, Yanyan
Zhao for their suggestions and useful discussions.
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60435020, 60575042, and 60503072.
</bodyText>
<sectionHeader confidence="0.994192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901827956989">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the ACL-Coling-1998, pages 86–90.
Xavier Carreras and Llu´ıs M`arquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proceedings of CoNLL-2004, pages 89–
97.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005, pages
152–164.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-2000.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proceedings of CoNLL-2003, pages 160–163.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proceedings
of NIPS-2001.
Nello Cristianini and John Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines. Cambridge
University Press, Cambirdge University.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of ACL-2004, pages 423–429.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In Computational Learning Theory, pages 209–217.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of ACL-2002, pages 239–246.
Jes´us Gim´enez and Llu´ıs M`arquez. 2003. Fast and
accurate part-of-speech tagging: The svm approach
revisited. In Proceedings of RANLP-2003.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, July.
Zheng Ping Jiang, Jia Li, and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument
interdependence. In Proceedings of IJCAI-2005.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cat-
egorisation. In Proceedings of ICML-2001, pages
250–257.
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, pages 189–192.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, 2:419–444.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
ofACL-2004, pages 335–342.
Rodney D. Nielsen and Sameer Pradhan. 2004. Mix-
ing weak learners in semantic parsing. In Proceed-
ings of EMNLP-2004.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005a. Support vector learning for semantic
argument classification. Machine Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005b. Semantic role
labeling using different syntactic views. In Proceed-
ings of ACL-2005, pages 581–588.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346–1352.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role
labeling. In Proceedings of IJCAI-2005, pages
1117–1123.
Chris Watkins. 1999. Dynamic alignment kernels.
Technical Report CSD-TR-98-11, Jan.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP 2004.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083–1106.
</reference>
<page confidence="0.865597">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.866807">
<title confidence="0.999876">A Hybrid Convolution Tree Kernel for Semantic Role Labeling</title>
<author confidence="0.979703">Wanxiang Che Min Zhang Ting Liu</author>
<author confidence="0.979703">Sheng Li</author>
<affiliation confidence="0.999631">Harbin Inst. of Tech. Inst. for Infocomm Research Harbin Inst. of Tech.</affiliation>
<address confidence="0.998659">Harbin, China, 150001 Singapore, 119613 Harbin, China, 150001</address>
<email confidence="0.896753">mzhang@i2r.a-star.edu.sg</email>
<abstract confidence="0.99924945">A hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structures for semantic role labeling (SRL). The hybrid kernel consists of two individual convolution kernels: a Path kernel, which captures predicateargument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments. Evaluation on the datasets of CoNLL- 2005 SRL shared task shows that the novel hybrid convolution tree kernel outperforms the previous tree kernels. We also combine our new hybrid tree kernel based method with the standard rich flat feature based method. The experimental results show that the combinational method can get better performance than each of them individually.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACL-Coling-1998,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="4384" citStr="Baker et al., 1998" startWordPosition="681" endWordPosition="684"> and standard features’ polynomial kernel outperforms each of them individually. The remainder of the paper is organized as follows: In Section 2 we review the previous work. In Section 3 we illustrate the state of the art feature-based method for SRL. Section 4 discusses our method. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model th</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the ACL-Coling-1998, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,</booktitle>
<pages>89--97</pages>
<marker>Carreras, M`arquez, 2004</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of CoNLL-2004, pages 89– 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<pages>152--164</pages>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-2005, pages 152–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000.</booktitle>
<contexts>
<context position="18789" citStr="Charniak (2000)" startWordPosition="3100" endWordPosition="3101">ntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. Train Devel tWSJ tBrown Sentences 39,832 1,346 2,416 426 Tokens 950,028 32,853 56,684 7,159 Propositions 90,750 3,248 5,267 804 Arguments 239,858 8,346 14,077 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gim´enez and M`arquez, 2003), Charniak (2000)’s full syntactic parser, and Chieu and Ng (2003)’s Named Entity recognizer. 5.1.2 Evaluation The system is evaluated with respect to precision, recall, and F,3=1 of the predicted arguments. Precision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. F,3=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems. It is formulated as: F,3=1 = 2pr/(p + r). srl-eval.pl2 is the official program of the CoNLL-2005 SRL shared task t</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of NAACL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>160--163</pages>
<contexts>
<context position="18838" citStr="Chieu and Ng (2003)" startWordPosition="3107" endWordPosition="3110"> section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. Train Devel tWSJ tBrown Sentences 39,832 1,346 2,416 426 Tokens 950,028 32,853 56,684 7,159 Propositions 90,750 3,248 5,267 804 Arguments 239,858 8,346 14,077 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gim´enez and M`arquez, 2003), Charniak (2000)’s full syntactic parser, and Chieu and Ng (2003)’s Named Entity recognizer. 5.1.2 Evaluation The system is evaluated with respect to precision, recall, and F,3=1 of the predicted arguments. Precision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. F,3=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems. It is formulated as: F,3=1 = 2pr/(p + r). srl-eval.pl2 is the official program of the CoNLL-2005 SRL shared task to evaluate a system performance. 2http://www.lsi.</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In Proceedings of CoNLL-2003, pages 160–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS-2001.</booktitle>
<contexts>
<context position="2514" citStr="Collins and Duffy, 2001" startWordPosition="384" endWordPosition="387">M-LOC Figure 1: Semantic role labeling in a phrase structure syntactic tree representation for argument identification and classification in building SRL systems and participating in evaluations, such as Senseval-3 1, CoNLL-2004 and 2005 shared tasks: SRL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), where a flat feature vector is usually used to represent a predicate-argument structure. However, it’s hard for this kind of representation method to explicitly describe syntactic structure information by a vector of flat features. As an alternative, convolution tree kernel methods (Collins and Duffy, 2001) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees. In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction. They can be well used as learning algorithms in the kernel-based methods. They are named kernel machines. In this paper, we decompose the Moschitti (2004)</context>
<context position="6244" citStr="Collins and Duffy, 2001" startWordPosition="974" endWordPosition="977"> the similarity between two discrete structured objects directly using the original representation of the objects instead of explicitly enumerating their features. Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. 3 Feature-based methods for SRL Usually feature-based methods refer to the methods which use the flat features to represent instances. At present, most of the successful SRL systems use this method. Their features are usually ext</context>
<context position="9430" citStr="Collins and Duffy, 2001" startWordPosition="1483" endWordPosition="1486">arseness problem prevents the learning algorithms from generalizing unseen data well. In order to address this problem, one method is to list all sub-structures of the parse tree. However, both space complexity and time complexity are too high for the algorithm to be realized. 4 Hybrid Convolution Tree Kernels for SRL In this section, we introduce the previous kernel method for SRL in Subsection 4.1, discuss our method in Subsection 4.2 and compare our method with previous work in Subsection 4.3. 4.1 Convolution Tree Kernels for SRL Moschitti (2004) proposed to apply convolution tree kernels (Collins and Duffy, 2001) to SRL. He selected portions of syntactic parse trees, which include salient sub-structures of predicatearguments, to define convolution kernels for the task of predicate argument classification. This portions selection method of syntactic parse trees is named as predicate-arguments feature (PAF) kernel. Figure 2 illustrates the PAF kernel feature space of the predicate buy and the argument Arg1 in the circled sub-structure. The kind of convolution tree kernel is similar to Collins and Duffy (2001)’s tree kernel except the sub-structure selection strategy. Moschitti (2004) only selected the r</context>
<context position="11171" citStr="Collins and Duffy (2001)" startWordPosition="1781" endWordPosition="1784">al to the tree’s size. Thus it is computationally infeasible to use the feature vector 4b(T) directly. To solve this problem, we introduce the tree kernel function which is able to calculate the dot product between the above high-dimension vectors efficiently. The kernel function is defined as following: K(T1, T2) _ (`D(T1), `D(T2)) _ PriOi(T1), Oi(T2) _ Pn1111 Pn2112 /moi Ii(n1) * Ii(n2) where N1 and N2 are the sets of all nodes in trees T1 and T2, respectively, and Ii(n) is the indicator function whose value is 1 if and only if there is a sub-tree of type i rooted at node n and 0 otherwise. Collins and Duffy (2001) show that K(T1,T2) is an instance of convolution kernels over tree structures, which can be computed in O(|N1 |x |N2|) by the following recursive definitions (Let A(n1, n2) = Pi Ii(n1) * Ii(n2)): (1) if the children of n1 and n2 are different then A(n1, n2) = 0; (2) else if their children are the same and they are leaves, then A(n1, n2) = µ; µ Q��h�1) (3) else A(n1, n2) = ��1 (1 + A(ch(n1, j), ch(n2, j))) where nc(n1) is the number of the children of n1, ch(n, j) is the jth child of node n and µ(0 &lt; µ &lt; 1) is the decay factor in order to make the kernel value less variable with respect to the</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of NIPS-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<institution>Cambirdge University.</institution>
<contexts>
<context position="2809" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="425" endWordPosition="428">; Carreras and M`arquez, 2005), where a flat feature vector is usually used to represent a predicate-argument structure. However, it’s hard for this kind of representation method to explicitly describe syntactic structure information by a vector of flat features. As an alternative, convolution tree kernel methods (Collins and Duffy, 2001) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees. In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction. They can be well used as learning algorithms in the kernel-based methods. They are named kernel machines. In this paper, we decompose the Moschitti (2004)’s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure ker1http://www.cs.unt.edu/—rada/senseval/senseval3/ NP PP DT NN IN NN NP VP PRP VBD She bought silk the in ArgO V China 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, S</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines. Cambridge University Press, Cambirdge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-2004,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="6327" citStr="Culotta and Sorensen, 2004" startWordPosition="987" endWordPosition="990">nal representation of the objects instead of explicitly enumerating their features. Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. 3 Feature-based methods for SRL Usually feature-based methods refer to the methods which use the flat features to represent instances. At present, most of the successful SRL systems use this method. Their features are usually extended from Gildea and Jurafsky (2002)’s work, which uses flat information derived f</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of ACL-2004, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1998</date>
<booktitle>In Computational Learning Theory,</booktitle>
<pages>209--217</pages>
<contexts>
<context position="20288" citStr="Freund and Schapire, 1998" startWordPosition="3345" endWordPosition="3348"> save time, we use a pruning stage (Xue and Palmer, 2004) to filter out the constituents that are clearly not semantic arguments to the predicate. Stage 2: We then identify the candidates derived from Stage 1 as either arguments or nonarguments. Stage 3: A multi-category classifier is used to classify the constituents that are labeled as arguments in Stage 2 into one of the argument classes plus NULL. Stage 4: A rule-based post-processing stage (Liu et al., 2005) is used to handle some unmatched arguments with constituents, such as AM-MOD, AM-NEG. 5.1.4 Classifier We use the Voted Perceptron (Freund and Schapire, 1998) algorithm as the kernel machine. The performance of the Voted Perceptron is close to, but not as good as, the performance of SVM on the same problem, while saving computation time and programming effort significantly. SVM is too slow to finish our experiments for tuning parameters. The Voted Perceptron is a binary classifier. In order to handle multi-classification problems, we adopt the one vs. others strategy and select the one with the largest margin as the final output. The training parameters are chosen using development data. After 5 iteration numbers, the best performance is achieved. </context>
</contexts>
<marker>Freund, Schapire, 1998</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1998. Large margin classification using the perceptron algorithm. In Computational Learning Theory, pages 209–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1829" citStr="Gildea and Jurafsky, 2002" startWordPosition="283" endWordPosition="286">al body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods S Arg1 ArgM-LOC Figure 1: Semantic role labeling in a phrase structure syntactic tree representation for argument identification and classification in building SRL systems and participating in evaluations, such as Senseval-3 1, CoNLL-2004 and 2005 shared tasks: SRL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), where a flat feature vector is usually used to represent a predicate-argument structure. However, it’s hard for this kind of representation method to explicitly describe syntactic structure information by a vector of flat f</context>
<context position="4225" citStr="Gildea and Jurafsky (2002)" startWordPosition="654" endWordPosition="657">chine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each of them individually. The remainder of the paper is organized as follows: In Section 2 we review the previous work. In Section 3 we illustrate the state of the art feature-based method for SRL. Section 4 discusses our method. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods </context>
<context position="6881" citStr="Gildea and Jurafsky (2002)" startWordPosition="1080" endWordPosition="1083">r relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. 3 Feature-based methods for SRL Usually feature-based methods refer to the methods which use the flat features to represent instances. At present, most of the successful SRL systems use this method. Their features are usually extended from Gildea and Jurafsky (2002)’s work, which uses flat information derived from a parse tree. According to the literature, we select the Constituent, Predicate, and PredicateConstituent related features shown in Table 1. Feature Description Constituent related features Phrase Type syntactic category of the constituent Head Word head word of the constituent Last Word last word of the constituent First Word first word of the constituent Named Entity named entity type of the constituent’s head word POS part of speech of the constituent Previous Word sequence previous word of the constituent Next Word sequence next word of the</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-2002,</booktitle>
<pages>239--246</pages>
<contexts>
<context position="8423" citStr="Gildea and Palmer, 2002" startWordPosition="1321" endWordPosition="1324">h from the predicate to the constituent Position the relative position of the constituent and the predicate, before or after Path Length the nodes number on the parse tree path Partial Path some part on the parse tree path Clause Layers the clause layers from the constituent to the predicate Table 1: Standard flat features However, to find relevant features is, as usual, a complex task. In addition, according to the description of the standard features, we can see that the syntactic features, such as Path, Path Length, bulk large among all features. On the other hand, the previous researches (Gildea and Palmer, 2002; Punyakanok et al., 2005) have also recognized the 74 Arg1 ArgM-LOC Figure 2: Predicate Argument Feature space necessity of syntactic parsing for semantic role labeling. However, the standard flat features cannot model the syntactic information well. A predicateargument pair has two different Path features even if their paths differ only for a node in the parse tree. This data sparseness problem prevents the learning algorithms from generalizing unseen data well. In order to address this problem, one method is to list all sub-structures of the parse tree. However, both space complexity and ti</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of ACL-2002, pages 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Fast and accurate part-of-speech tagging: The svm approach revisited.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP-2003.</booktitle>
<marker>Gim´enez, M`arquez, 2003</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2003. Fast and accurate part-of-speech tagging: The svm approach revisited. In Proceedings of RANLP-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical Report UCSC-CRL-99-10,</tech>
<contexts>
<context position="5921" citStr="Haussler (1999)" startWordPosition="925" endWordPosition="926"> to implicitly explore features in a highdimension space by directly calculating the similarity between two objects using kernel function. In particular, the kernel methods could be effective in reducing the burden of feature engineering for structured objects in NLP problems. This is because a kernel can measure the similarity between two discrete structured objects directly using the original representation of the objects instead of explicitly enumerating their features. Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In t</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Ping Jiang</author>
<author>Jia Li</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Semantic argument classification exploiting argument interdependence.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-2005.</booktitle>
<contexts>
<context position="4591" citStr="Jiang et al., 2005" startWordPosition="713" endWordPosition="716">tate of the art feature-based method for SRL. Section 4 discusses our method. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing</context>
</contexts>
<marker>Jiang, Li, Ng, 2005</marker>
<rawString>Zheng Ping Jiang, Jia Li, and Hwee Tou Ng. 2005. Semantic argument classification exploiting argument interdependence. In Proceedings of IJCAI-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Nello Cristianini</author>
<author>John ShaweTaylor</author>
</authors>
<title>Composite kernels for hypertext categorisation.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-2001,</booktitle>
<pages>250--257</pages>
<contexts>
<context position="14833" citStr="Joachims et al. (2001)" startWordPosition="2439" endWordPosition="2442">e can see that the Constituent Structure kernel does not perform well. Affected by this, the PAF kernel cannot perform well, either. However, in our hybrid method, we can adjust the compromise (b) Hybrid Convolution Tree Kernel Figure 4: Comparison between PAF and Hybrid Convolution Tree Kernels of the Path feature and the Constituent Structure feature by tuning their weights to get an optimal result. Having defined two convolution tree kernels, the Path kernel Kpath and the Constituent Structure kernel Kc,, we can define a new kernel to compose and extend the individual kernels. According to Joachims et al. (2001), the kernel function set is closed under linear combination. It means that the following Khybrid is a valid kernel if Kpath and Kc� are both valid. Khybrid = AKpath + (1 − A)Kc� (1) where 0 &lt; A &lt; 1. According to the definitions of the Path and the Constituent Structure kernels, each kernel is explicit. They can be viewed as a matching of feaFigure 5: An example of Semantic Role Labeling 76 tures. Since the features are enumerable on the given data, the kernels are all valid. Therefore, the new kernel Khybrid is valid. We name the new kernel hybrid convolution tree kernel, Khybrid. Since the s</context>
</contexts>
<marker>Joachims, Cristianini, ShaweTaylor, 2001</marker>
<rawString>Thorsten Joachims, Nello Cristianini, and John ShaweTaylor. 2001. Composite kernels for hypertext categorisation. In Proceedings of ICML-2001, pages 250–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
<author>Sheng Li</author>
<author>Yuxuan Hu</author>
<author>Huaijun Liu</author>
</authors>
<title>Semantic role labeling system using maximum entropy classifier.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="20129" citStr="Liu et al., 2005" startWordPosition="3320" endWordPosition="3323">as the labeling units to form the labeled arguments. In order to speed up the learning process, we use a four-stage learning architecture: Stage 1: To save time, we use a pruning stage (Xue and Palmer, 2004) to filter out the constituents that are clearly not semantic arguments to the predicate. Stage 2: We then identify the candidates derived from Stage 1 as either arguments or nonarguments. Stage 3: A multi-category classifier is used to classify the constituents that are labeled as arguments in Stage 2 into one of the argument classes plus NULL. Stage 4: A rule-based post-processing stage (Liu et al., 2005) is used to handle some unmatched arguments with constituents, such as AM-MOD, AM-NEG. 5.1.4 Classifier We use the Voted Perceptron (Freund and Schapire, 1998) algorithm as the kernel machine. The performance of the Voted Perceptron is close to, but not as good as, the performance of SVM on the same problem, while saving computation time and programming effort significantly. SVM is too slow to finish our experiments for tuning parameters. The Voted Perceptron is a binary classifier. In order to handle multi-classification problems, we adopt the one vs. others strategy and select the one with t</context>
</contexts>
<marker>Liu, Che, Li, Hu, Liu, 2005</marker>
<rawString>Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun Liu. 2005. Semantic role labeling system using maximum entropy classifier. In Proceedings of CoNLL-2005, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="6183" citStr="Lodhi et al., 2002" startWordPosition="965" endWordPosition="968">ts in NLP problems. This is because a kernel can measure the similarity between two discrete structured objects directly using the original representation of the objects instead of explicitly enumerating their features. Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. 3 Feature-based methods for SRL Usually feature-based methods refer to the methods which use the flat features to represent instances. At present, most of the successfu</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="18019" citStr="Marcus et al., 1993" startWordPosition="2977" endWordPosition="2980">sk of argument classification while in our experiment, we report the experimental results on both identification and classification. 5 Experiments and Discussion The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features. 5.1 Experimental Setting 5.1.1 Corpus We use the benchmark corpus provided by CoNLL-2005 SRL shared task (Carreras and M`arquez, 2005) provided corpus as our training, development, and test sets. The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). We followed the standard partition used in syntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. Train Devel tWSJ tBrown Sentences 39,832 1,346 2,416 426 Tokens 950,028 32,853 56,684 7,159 Propositions 90,750 3,248 5,267 804 Arguments 239,858 8,3</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow statistic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL-2004,</booktitle>
<pages>335--342</pages>
<contexts>
<context position="3114" citStr="Moschitti (2004)" startWordPosition="479" endWordPosition="480">and Duffy, 2001) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees. In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction. They can be well used as learning algorithms in the kernel-based methods. They are named kernel machines. In this paper, we decompose the Moschitti (2004)’s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure ker1http://www.cs.unt.edu/—rada/senseval/senseval3/ NP PP DT NN IN NN NP VP PRP VBD She bought silk the in ArgO V China 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, Sydney, July 2006. c�2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final comp</context>
<context position="6411" citStr="Moschitti (2004)" startWordPosition="1002" endWordPosition="1004"> functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. 3 Feature-based methods for SRL Usually feature-based methods refer to the methods which use the flat features to represent instances. At present, most of the successful SRL systems use this method. Their features are usually extended from Gildea and Jurafsky (2002)’s work, which uses flat information derived from a parse tree. According to the literature, we select the Constituent, Predicate,</context>
<context position="9361" citStr="Moschitti (2004)" startWordPosition="1475" endWordPosition="1476"> paths differ only for a node in the parse tree. This data sparseness problem prevents the learning algorithms from generalizing unseen data well. In order to address this problem, one method is to list all sub-structures of the parse tree. However, both space complexity and time complexity are too high for the algorithm to be realized. 4 Hybrid Convolution Tree Kernels for SRL In this section, we introduce the previous kernel method for SRL in Subsection 4.1, discuss our method in Subsection 4.2 and compare our method with previous work in Subsection 4.3. 4.1 Convolution Tree Kernels for SRL Moschitti (2004) proposed to apply convolution tree kernels (Collins and Duffy, 2001) to SRL. He selected portions of syntactic parse trees, which include salient sub-structures of predicatearguments, to define convolution kernels for the task of predicate argument classification. This portions selection method of syntactic parse trees is named as predicate-arguments feature (PAF) kernel. Figure 2 illustrates the PAF kernel feature space of the predicate buy and the argument Arg1 in the circled sub-structure. The kind of convolution tree kernel is similar to Collins and Duffy (2001)’s tree kernel except the s</context>
<context position="17125" citStr="Moschitti (2004)" startWordPosition="2841" endWordPosition="2842">cture feature space. On the other hand, the other features in the flat feature set, such as Named Entity, Previous, and Next Word, Voice, SubCat, Suffix, are not contained in our hybrid convolution tree kernel. From the syntactic viewpoint, the tree representation in our feature space is more robust than the Parse Tree Path feature in the flat feature set since the Path feature is sensitive to small changes of the parse trees and it also does not maintain the hierarchical information of a parse tree. It is also worth comparing our method with the previous kernels. Our method is similar to the Moschitti (2004)’s predicate-argument feature (PAF) kernel. However, we differentiate the Path feature and the Constituent Structure feature in our hybrid kernel in order to more effectively capture the syntactic structure information for SRL. In addition Moschitti (2004) only study the task of argument classification while in our experiment, we report the experimental results on both identification and classification. 5 Experiments and Discussion The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features. 5.1 Experim</context>
<context position="20917" citStr="Moschitti (2004)" startWordPosition="3448" endWordPosition="3449"> the kernel machine. The performance of the Voted Perceptron is close to, but not as good as, the performance of SVM on the same problem, while saving computation time and programming effort significantly. SVM is too slow to finish our experiments for tuning parameters. The Voted Perceptron is a binary classifier. In order to handle multi-classification problems, we adopt the one vs. others strategy and select the one with the largest margin as the final output. The training parameters are chosen using development data. After 5 iteration numbers, the best performance is achieved. In addition, Moschitti (2004)’s Tree Kernel Tool is used to compute the tree kernel function. 5.2 Experimental Results In order to speed up the training process, in the following experiments, we ONLY use WSJ sections 02-05 as training data. The same as Moschitti (2004), we also set the p = 0.4 in the computation of convolution tree kernels. In order to study the impact of A in hybrid convolution tree kernel in Eq. 1, we only use the hybrid kernel between Kpath and Kms. The performance curve on development set changing with A is shown in Figure 6. 67 65 63 61 59 57 55 53 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 lambda Figur</context>
<context position="22204" citStr="Moschitti (2004)" startWordPosition="3678" endWordPosition="3680">that when A = 0.5, the hybrid convolution tree kernel gets the best performance. Either the Path kernel (A = 1, F,a_i = 61.26) or the Constituent Structure kernel (A = 0, F,a—i = 54.91) cannot perform better than the hybrid one. It suggests that the two individual kernels are complementary to each other. In addition, the Path kernel performs much better than the Constituent Structure kernel. It indicates that the predicate-constituent related features are more effective than the constituent features for SRL. Table 3 compares the performance comparison among our Hybrid convolution tree kernel, Moschitti (2004)’s PAF kernel, standard flat features with Linear kernels, and Poly kernel (d = 2). We can see that our hybrid convolution tree kernel outperforms the PAF kernel. It empirically demonstrates that the weight linear combination in our hybrid kernel is more effective than PAF kernel for SRL. However, our hybrid kernel still performs worse than the standard feature based system. This is simple because our kernel only use the syntactic structure information while the feature-based method use a large number of hand-craft diverse features, from word, POS, syntax and semantics, NER, etc. The standard </context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow statistic parsing. In Proceedings ofACL-2004, pages 335–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Sameer Pradhan</author>
</authors>
<title>Mixing weak learners in semantic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-2004.</booktitle>
<contexts>
<context position="4646" citStr="Nielsen and Pradhan, 2004" startWordPosition="721" endWordPosition="724">tion 4 discusses our method. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard fe</context>
</contexts>
<marker>Nielsen, Pradhan, 2004</marker>
<rawString>Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing weak learners in semantic parsing. In Proceedings of EMNLP-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1640" citStr="Palmer et al., 2005" startWordPosition="254" endWordPosition="257">each of them individually. 1 Introduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods S Arg1 ArgM-LOC Figure 1: Semantic role labeling in a phrase structure syntactic tree representation for argument identification and classification in building SRL systems and participating in evaluations, such as Senseval-3 1, CoNLL-2004 and 2005 shared tasks: SRL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), where a flat feature vector is usu</context>
<context position="18127" citStr="Palmer et al., 2005" startWordPosition="2992" endWordPosition="2995">tion and classification. 5 Experiments and Discussion The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features. 5.1 Experimental Setting 5.1.1 Corpus We use the benchmark corpus provided by CoNLL-2005 SRL shared task (Carreras and M`arquez, 2005) provided corpus as our training, development, and test sets. The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). We followed the standard partition used in syntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. Train Devel tWSJ tBrown Sentences 39,832 1,346 2,416 426 Tokens 950,028 32,853 56,684 7,159 Propositions 90,750 3,248 5,267 804 Arguments 239,858 8,346 14,077 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM b</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valeri Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<journal>Machine Learning Journal.</journal>
<contexts>
<context position="1851" citStr="Pradhan et al., 2005" startWordPosition="287" endWordPosition="290">tive evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods S Arg1 ArgM-LOC Figure 1: Semantic role labeling in a phrase structure syntactic tree representation for argument identification and classification in building SRL systems and participating in evaluations, such as Senseval-3 1, CoNLL-2004 and 2005 shared tasks: SRL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), where a flat feature vector is usually used to represent a predicate-argument structure. However, it’s hard for this kind of representation method to explicitly describe syntactic structure information by a vector of flat features. As an alterna</context>
<context position="4668" citStr="Pradhan et al., 2005" startWordPosition="725" endWordPosition="728">. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard feature-based methods, k</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005a. Support vector learning for semantic argument classification. Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role labeling using different syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<pages>581--588</pages>
<contexts>
<context position="1851" citStr="Pradhan et al., 2005" startWordPosition="287" endWordPosition="290">tive evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods S Arg1 ArgM-LOC Figure 1: Semantic role labeling in a phrase structure syntactic tree representation for argument identification and classification in building SRL systems and participating in evaluations, such as Senseval-3 1, CoNLL-2004 and 2005 shared tasks: SRL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), where a flat feature vector is usually used to represent a predicate-argument structure. However, it’s hard for this kind of representation method to explicitly describe syntactic structure information by a vector of flat features. As an alterna</context>
<context position="4668" citStr="Pradhan et al., 2005" startWordPosition="725" endWordPosition="728">. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard feature-based methods, k</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Daniel Jurafsky. 2005b. Semantic role labeling using different syntactic views. In Proceedings of ACL-2005, pages 581–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling-2004,</booktitle>
<pages>1346--1352</pages>
<contexts>
<context position="4795" citStr="Punyakanok et al., 2004" startWordPosition="746" endWordPosition="750">ing was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard feature-based methods, kernel-based methods have been proposed to implicitly explore features in a highdimension space by directly calculating the simi</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proceedings of Coling-2004, pages 1346–1352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-2005,</booktitle>
<pages>1117--1123</pages>
<contexts>
<context position="8449" citStr="Punyakanok et al., 2005" startWordPosition="1325" endWordPosition="1328">he constituent Position the relative position of the constituent and the predicate, before or after Path Length the nodes number on the parse tree path Partial Path some part on the parse tree path Clause Layers the clause layers from the constituent to the predicate Table 1: Standard flat features However, to find relevant features is, as usual, a complex task. In addition, according to the description of the standard features, we can see that the syntactic features, such as Path, Path Length, bulk large among all features. On the other hand, the previous researches (Gildea and Palmer, 2002; Punyakanok et al., 2005) have also recognized the 74 Arg1 ArgM-LOC Figure 2: Predicate Argument Feature space necessity of syntactic parsing for semantic role labeling. However, the standard flat features cannot model the syntactic information well. A predicateargument pair has two different Path features even if their paths differ only for a node in the parse tree. This data sparseness problem prevents the learning algorithms from generalizing unseen data well. In order to address this problem, one method is to list all sub-structures of the parse tree. However, both space complexity and time complexity are too high</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005. The necessity of syntactic parsing for semantic role labeling. In Proceedings of IJCAI-2005, pages 1117–1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Watkins</author>
</authors>
<title>Dynamic alignment kernels.</title>
<date>1999</date>
<tech>Technical Report CSD-TR-98-11,</tech>
<contexts>
<context position="5940" citStr="Watkins (1999)" startWordPosition="928" endWordPosition="929">re features in a highdimension space by directly calculating the similarity between two objects using kernel function. In particular, the kernel methods could be effective in reducing the burden of feature engineering for structured objects in NLP problems. This is because a kernel can measure the similarity between two discrete structured objects directly using the original representation of the objects instead of explicitly enumerating their features. Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follo</context>
</contexts>
<marker>Watkins, 1999</marker>
<rawString>Chris Watkins. 1999. Dynamic alignment kernels. Technical Report CSD-TR-98-11, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="4570" citStr="Xue and Palmer, 2004" startWordPosition="709" endWordPosition="712"> 3 we illustrate the state of the art feature-based method for SRL. Section 4 discusses our method. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorit</context>
<context position="19719" citStr="Xue and Palmer, 2004" startWordPosition="3250" endWordPosition="3253">correct arguments which are predicted by a system. F,3=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems. It is formulated as: F,3=1 = 2pr/(p + r). srl-eval.pl2 is the official program of the CoNLL-2005 SRL shared task to evaluate a system performance. 2http://www.lsi.upc.edu/—srlconll/srl-eval.pl 77 5.1.3 SRL Strategies We use constituents as the labeling units to form the labeled arguments. In order to speed up the learning process, we use a four-stage learning architecture: Stage 1: To save time, we use a pruning stage (Xue and Palmer, 2004) to filter out the constituents that are clearly not semantic arguments to the predicate. Stage 2: We then identify the candidates derived from Stage 1 as either arguments or nonarguments. Stage 3: A multi-category classifier is used to classify the constituents that are labeled as arguments in Stage 2 into one of the argument classes plus NULL. Stage 4: A rule-based post-processing stage (Liu et al., 2005) is used to handle some unmatched arguments with constituents, such as AM-MOD, AM-NEG. 5.1.4 Classifier We use the Voted Perceptron (Freund and Schapire, 1998) algorithm as the kernel machin</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="6298" citStr="Zelenko et al., 2003" startWordPosition="983" endWordPosition="986">rectly using the original representation of the objects instead of explicitly enumerating their features. Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. 3 Feature-based methods for SRL Usually feature-based methods refer to the methods which use the flat features to represent instances. At present, most of the successful SRL systems use this method. Their features are usually extended from Gildea and Jurafsky (2002)’s work, which us</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>