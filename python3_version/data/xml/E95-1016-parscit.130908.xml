<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.926047">
On Learning more Appropriate Selectional Restrictions
</title>
<author confidence="0.795375">
Francesc Ribas*
</author>
<affiliation confidence="0.634187">
Departament de Llenguatges i Sistemes Informatics
</affiliation>
<address confidence="0.7544355">
Universitat Politecnica de Catalunya
Pau Gargallo, 5
08028 Barcelona
Spain
</address>
<email confidence="0.886344">
ribaselsi.upc.es
</email>
<sectionHeader confidence="0.994304" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.958523769230769">
We present some variations affecting the
association measure and thresholding on
a technique for learning Selectional Re-
strictions from on-line corpora. It uses
a wide-coverage noun taxonomy and a
statistical measure to generalize the ap-
propriate semantic classes. Evaluation
measures for the Selectional Restrictions
learning task are discussed. Finally, an
experimental evaluation of these varia-
tions is reported.
Subject Areas: corpus-based language
modeling, computational lexicography
</bodyText>
<sectionHeader confidence="0.998582" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936727272727">
In recent years there has been a common agree-
ment in the NLP research community on the im-
portance of having an extensive coverage of selec-
tional restrictions (SRs) tuned to the domain to
work with. SRs can be seen as semantic type con-
straints that a word sense imposes on the words
with which it combines in the process of seman-
tic interpretation. SRs may have different ap-
plications in NLP, specifically, they may help a
parser with Word Sense Selection (WSS, as in
(Hirst, 1987)), with preferring certain structures
out of several grammatical ones (Whittemore et
al., 1990) and finally with deciding the semantic
role played by a syntactic complement (Basili et
al., 1992). Lexicography is also interested in the
acquisition of SRs (both defining in context ap-
proach and lexical semantics work (Levin, 1992)).
The aim of our work is to explore the feasibil-
ity of using an statistical method for extracting
SRs from on-line corpora. Resnik (1992) devel-
oped a method for automatically extracting class-
based SRs from on-line corpora. Ribas (1994a)
</bodyText>
<footnote confidence="0.98771225">
This research has been made in the framework of
the Acquilex-II Esprit Project (7315), and has been
supported by a grant of Departament d&apos;Ensenyament,
Generalitat de Catalunya, 91-DOGC-1491.
</footnote>
<bodyText confidence="0.987062352941177">
performed some experiments using this basic tech-
nique and drew up some limitations from the cor-
responding results.
In this paper we will describe some substantial
modifications to the basic technique and will re-
port the corresponding experimental evaluation.
The outline of the paper is as follows: in section
2 we summarize the basic methodology used in
(Ribas, 1994a), analyzing its limitations; in sec-
tion 3 we explore some alternative statistical mea-
sures for ranking the hypothesized SRs; in sec-
tion 4 we propose some evaluation measures on
the SRs-learning problem, and use them to test
the experimental results obtained by the different
techniques; finally, in section 5 we draw up the
final conclusions and establish future lines of re-
search.
</bodyText>
<sectionHeader confidence="0.482397" genericHeader="introduction">
2 The basic technique for learning
SRs
</sectionHeader>
<subsectionHeader confidence="0.772226">
2.1 Description
</subsectionHeader>
<bodyText confidence="0.989420833333333">
The technique functionality can be summarized
as:
Input The training set, i.e. a list of
complement co-occurrence triples, (verb-
lemma, syntactic-relationship, noun-lemma)
extracted from the corpus.
Previous knowledge used A semantic hierar-
chy (WordNetl) where words are clustered in
semantic classes, and semantic classes are or-
ganized hierarchically. Polysemous words are
represented as instances of different classes.
Output A set of syntac-
tic SRs, (verb-lemma, syntactic-relationship,
semantic-class, weight). The final SRs must
be mutually disjoint. SRs are weighted ac-
cording to the statistical evidence found in
the corpus.
Learning process 3 stages:
</bodyText>
<listItem confidence="0.4418375">
1. Creation of the space of candidate
classes.
</listItem>
<footnote confidence="0.961466">
iWordNet is a broad-coverage lexical database, see
(Miller et al., 1991))
</footnote>
<page confidence="0.991137">
112
</page>
<table confidence="0.9889795">
Acquired SR Type Assoc Examples of nouns in Treebank
&lt; suit, suing &gt; Senses 0.41 suit
&lt; suit_o f _clothes &gt; Senses 0.41 suit
&lt; suit &gt; Senses 0.40 suit
&lt; group&gt; frAbs 0.35 administration, agency, bank, ...
&lt;legal_action&gt; Ok 0.28 suit
&lt; person, individual&gt; Ok 0.23 advocate, buyer,carrier, client, ...
&lt; radical&gt; Senses 0.16 group
&lt; city&gt; Senses 0.15 proper_name
&lt; admin_district &gt; Senses 0.14 proper_name
&lt; social_control &gt; Senses 0.11 administration ,government
&lt; status&gt; Senses 0.087 government, leadership
&lt; activity&gt; Senses -0.01 administration, leadership, provision
&lt; cognition&gt; Senses -0.04 concern, leadership, provision, science
</table>
<tableCaption confidence="0.999899">
Table 1: SRs acquired for the subject of seek
</tableCaption>
<listItem confidence="0.9861598">
2. Evaluation of the appropriateness of the
candidates by means of a statistical mea-
sure.
3. Selection of the most appropriate subset
in the candidate space to convey the SRs.
</listItem>
<bodyText confidence="0.900818571428571">
The appropriateness of a class for expressing
SRs (stage 2) is quantified from the strength of
co-occurrence of verbs and classes of nouns in the.
corpus (Resnik, 1992). Given the verb v, the
syntactic-relationship s and the candidate class c,
the Association Score, Assoc, between v and c in
s is defined:
</bodyText>
<equation confidence="0.997767">
Assoc(v, s, c) = p(clv, s)I(v; els)
= p(clv s) log *iv&apos; s)
P(cs)
</equation>
<bodyText confidence="0.9926185">
The two terms of Assoc try to capture different
properties:
</bodyText>
<listItem confidence="0.9690005">
1. Mutual information ratio, /(v; e)s), measures
the strength of the statistical association be-
tween the given verb v and the candidate
class c in the given syntactic position s. It
compares the prior distribution, p(c1s), with
the posterior distribution, p(clv, s).
2. p(clv, , s) scales up the strength of the associ-
ation by the frequency of the relationship.
</listItem>
<bodyText confidence="0.99765225">
Probabilities are estimated by Maximum Likeli-
hood Estimation (MLE), i.e. counting the relative
frequency of the considered events in the corpus2.
However, it is not obvious how to calculate class
frequencies when the training corpus is not seman-
tically tagged as is the case. Nevertheless, we take
a simplistic approach and calculate them in the
following manner:
</bodyText>
<equation confidence="0.671553">
f req(v, s, c) = freq(v,s,n) x w (1)
riâ‚¬c
</equation>
<footnote confidence="0.827813">
2The utility of introducing smoothing techniques
on class-based distributions is dubious, see (Resnik,
1993).
</footnote>
<bodyText confidence="0.795653666666667">
Where w is a constant factor used to normalize
the probabilities&apos;
= Evev EsEs EnEA, f req(v, s, n)
</bodyText>
<equation confidence="0.745106">
w
</equation>
<bodyText confidence="0.979784083333333">
EsEs ErlE.Ar f req(v, s, n)Isenses(n)I
(2)
When creating the space of candidate classes
(learning process, stage 1), we use a threshold-
jug technique to ignore as much as possible the
noise introduced in the training set. Specifically,
we consider only those classes that have a higher
number of occurrences than the threshold. The
selection of the most appropriate classes (stage 3)
is based on a global search through the candidates,
in such a way that the final classes are mutually
disjoint (not related by hyperonymy).
</bodyText>
<subsectionHeader confidence="0.992207">
2.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999372666666667">
Ribas (1994a) reported experimental results ob-
tained from the application of the above technique
to learn SRs. He performed an evaluation of the
Sits obtained from a training set of 870,000 words
of the Wall Street Journal. In this section we sum-
marize the results and conclusions reached in that
paper.
For instance, table 1 shows the SRs acquired
for the subject position of the verb seek. Type indi-
cates a manual diagnosis about the class appropri-
ateness (Ok: correct; itAbs: over-generalization;
Senses: due to erroneous senses). Assoc cor-
responds to the association score (higher values
appear first). Most of the induced classes are
due to incorrect senses. Thus, although suit was
used in the WSJ articles only in the sense of
&lt; legal_action &gt; , the algorithm not only consid-
ered the other senses as well (&lt; suit, suing &gt;,&lt;
</bodyText>
<footnote confidence="0.9082095">
3Resnik (1992) and Ribas (1994a) used equation
1 without introducing normalization. Therefore, the
</footnote>
<bodyText confidence="0.629375">
estimated function didn&apos;t accomplish probability ax-
ioms. Nevertheless, their results should be equivalent
(for our purposes) to those introducing normalization
because it shouldn&apos;t affect the relative ordering of As-
soc among rival candidate classes for the same (v, s).
</bodyText>
<page confidence="0.99848">
113
</page>
<bodyText confidence="0.992470777777778">
suii_of _clothes&gt;, &lt; suit&gt;) , but the Assoc score
ranked them higher than the appropriate sense.
We can also notice that the frAbs class, &lt;group&gt;,
seems too general for the example nouns, while
one of its daughters, &lt; people &gt; seems to fit the
data much better.
Analyzing the results obtained from different
experimental evaluation methods, Ribas (1994a)
drew up some conclusions:
</bodyText>
<listItem confidence="0.916356142857143">
a. The technique achieves a good coverage.
b. Most of the classes acquired result from the
accumulation of incorrect senses.
c. No clear co-relation between Assoc and the
manual diagnosis is found.
d. A slight tendency to over-generalization exists
due to incorrect senses.
</listItem>
<bodyText confidence="0.999683166666667">
Although the performance of the presented
technique seems to be quite good, we think that
some of the detected flaws could possibly be ad-
dressed. Noise due to polysemy of the nouns in-
volved seems to be the main obstacle for the prac-
ticality of the technique. It makes the association
score prefer incorrect classes and jump on over-
generalizations. In this paper we are interested
in exploring various ways to make the technique
more robust to noise, namely, (a) to experiment
with variations of the association score, (b) to ex-
periment with thresholding.
</bodyText>
<sectionHeader confidence="0.493261" genericHeader="method">
3 Variations on the association
statistical measure
</sectionHeader>
<bodyText confidence="0.9999565">
In this section we consider different variations on
the association score in order to make it more ro-
bust. The different techniques are experimentally
evaluated in section 4.2.
</bodyText>
<subsectionHeader confidence="0.981594">
3.1 Variations on the prior probability
</subsectionHeader>
<bodyText confidence="0.9990186">
When considering the prior probability, the more
independent of the context it is the better to mea-
sure actual associations. A sensible modification
of the measure would be to consider p(c) as the
prior distribution:
</bodyText>
<equation confidence="0.628713">
p(c)
</equation>
<bodyText confidence="0.999812333333333">
Using the chain rule on mutual information
(Cover and Thomas, 1991, p. 22) we can mathe-
matically relate the different versions of Assoc,
</bodyText>
<equation confidence="0.713759">
Assoc&apos; (v, s, = p(clv, s) log p(cls) +Assoc(v, s, c)
p(c)
</equation>
<bodyText confidence="0.994884027777778">
The first advantage of Assoc&apos; would come from
this (information theoretical) relationship. Specif-
ically, the Assoc&apos; takes into account the prefer-
ence (selection) of syntactic positions for partic-
ular classes. In intuitive terms, typical subjects
(e.g. &lt;person, individual, ...&gt;) would be preferred
(to atypical subjects as &lt;suit_of_clothes&gt;) as SRs
on the subject in contrast to Assoc. The second
advantage is that as long as the prior probabili-
ties, p(c), involve simpler events than those used
in Assoc, p(c1s), the estimation is easier and more
accurate (ameliorating data sparseness).
A subsequent modification would be to estimate
the prior, p(c), from the counts of all the nouns ap-
pearing in the corpus independently of their syn-
tactic positions (not restricted to be heads of ver-
bal complements). In this way, the estimation of
p(c) would be easier and more accurate.
3.2 Estimating class probabilities from
noun frequencies
In the global weighting technique presented in
equation 2 very polysemous nouns provide the
same amount of evidence to every sense as non-
ambiguous nouns do â€”while less ambiguous nouns
could be more informative about the correct
classes as long as they do not carry ambiguity.
The weight introduced in (1) could alternatively
be found in a local manner, in such a way that
more polysemous nouns would give less evidence
to each one of their senses than less ambiguous
ones. Local weight could be obtained using p(cln).
Nevertheless, a good estimation of this probabil-
ity seems quite problematic because of the lack of
tagged training material. In absence of a better
estimator we use a rather poor one as the uniform
distribution,
</bodyText>
<equation confidence="0.997476333333333">
isenses(n) E cl
w(n, c) =
Isenses(n)
</equation>
<bodyText confidence="0.970657307692308">
Resnik (1993) also uses a local normalization
technique but he normalizes by the total number
of classes in the hierarchy. This scheme seems
to present two problematic features (see (Ribas,
1994b) for more details). First, it doesn&apos;t take
dependency relationships introduced by hyper-
onymy into account. Second, nouns categorized in
lower levels in the taxonomy provide less weight
to each class than higher nouns.
3.3 Other statistical measures to score
SRs
In this section we propose the application of other
measures apart from Assoc for learning SRs: log-
likelihood ratio (Dunning, 1993), relative entropy
(Cover and Thomas, 1991), mutual information
ratio (Church and Hanks, 1990), 02 (Gale and
Church, 1991). In section (4) their experimental
evaluation is presented.
The statistical measures used to detect associ-
ations on the distribution defined by two random
variables X and Y work by measuring the devia-
tion of the conditional distribution, P(XIY), from
the expected distribution if both variables were
considered independent, i.e. the marginal distri-
bution, P (X). If P (X) is a good approximation
Assoc&apos; (v, s, c) = p(clv, s) log kciv, s)
</bodyText>
<page confidence="0.759608">
114
</page>
<table confidence="0.998107">
C
v_s p(c v_s) p(-&apos;c v_s)
-&apos;v_s p(c -w_s) p(-.c -w_s)
p(c) p(-,c)
</table>
<tableCaption confidence="0.997886">
Table 2: Conditional and marginal distributions
</tableCaption>
<bodyText confidence="0.99801225">
of P(XIY), association measures should be low
(near zero), otherwise deviating significantly from
zero.
Table 2 shows the cross-table formed by the con-
ditional and marginal distributions in the case of
X = {c, -,c} and Y = {v_s, -,v_s). Different asso-
ciation measures use the information provided in
the cross-table to different extents. Thus, Assoc
and mutual information ratio consider only the
deviation of the conditional probability p(civ, s)
from the corresponding marginal, p(c).
On the other hand, log-likelihood ratio and 02
measure the association between v_s and c con-
sidering the deviation of the four conditional cells
in table 2 from the corresponding marginals. It is
plausible that the deviation of the cells not taken
into account by Assoc can help on extracting use-
ful SRs.
Finally, it would be interesting to only use the
information related to the selectional behavior of
v_s, i.e. comparing the conditional probabilities
of c and given v_s with the corresponding
marginals. Relative entropy, D(P(Xlv_s)11P(X)),
could do this job.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.758779">
4.1 Evaluation methods of SRs
</subsectionHeader>
<bodyText confidence="0.999985909090909">
Evaluation on NLP has been crucial to fostering
research in particular areas. Evaluation of the SR
learning task would provide grounds to compare
different techniques that try to abstract SRs from
corpus using WordNet (e.g, section 4.2). It would
also permit measuring the utility of the SRs ob-
tained using WordNet in comparison with other
frameworks using other kinds of knowledge. Fi-
nally it would be a powerful tool for detecting
flaws of a particular technique (e.g, (Ribas, 1994a)
analysis).
However, a related and crucial issue is which
linguistic tasks are used as a reference. SRs are
useful for both lexicography and NLP. On the one
hand, from the point of view of lexicography, the
goal of evaluation would be to measure the quality
of the SRs induced, (i.e., how well the resulting
classes correspond to the nouns as they were used
in the corpus). On the other hand, from the point
of view of NLP, SRs should be evaluated on their
utility (i.e., how much they help on performing
the reference task).
</bodyText>
<subsubsectionHeader confidence="0.958749">
4.1.1 Lexicography-oriented evaluation
</subsubsectionHeader>
<bodyText confidence="0.999852833333333">
As far as lexicography (quality) is concerned,
we think the main criteria SRs acquired from cor-
pora should meet are: (a) correct categorization
-inferred classes should correspond to the correct
senses of the words that are being generalized-,
(b) appropriate generalization level and (c) good
coverage -the majority of the noun occurrences in
the corpus should be successfully generalized by
the induced SRs.
Some of the methods we could use for assessing
experimentally the accomplishment of these crite-
ria would be:
</bodyText>
<listItem confidence="0.944332666666667">
â€¢ Introspection A lexicographer checks if the
SRs accomplish the criteria (a) and (b) above
(e.g., the manual diagnosis in table 1). Be-
sides the intrinsic difficulties of this approach,
it does not seem appropriate when comparing
across different techniques for learning SRs,
because of its qualitative flavor.
â€¢ Quantification of generalization level
appropriateness A possible measure would
be the percentage of sense occurrences in-
cluded in the induced SRs which are effec-
tively correct (from now on called Abstraction
Ratio). Hopefully, a technique with a higher
abstraction ratio learns classes that fit the set
of examples better. A manual assessment of
the ratio confirmed this behavior, as testing
sets with a lower ratio seemed to be inducing
less 1lAbs cases.
â€¢ Quantification of coverage It could be
measured as the proportion of triples whose
correct sense belongs to one of the SRs.
</listItem>
<subsubsectionHeader confidence="0.93972">
4.1.2 NLP evaluation tasks
</subsubsectionHeader>
<bodyText confidence="0.999966652173913">
The NLP tasks where SRs utility could be eval-
uated are diverse. Some of them have already
been introduced in section 1. In the recent lit-
erature aGrishman and Sterling, 1992), (Resnik,
1993), ...) several task oriented schemes to test
Selectional Restrictions (mainly on syntactic am-
biguity resolution) have been proposed. However,
we have tested SRs on a WSS task, using the
following scheme. For every triple in the testing
set the algorithm selects as most appropriate that
noun-sense that has as hyperonym the SR class
with highest association score. When more than
one sense belongs to the highest SR, a random
selection is performed. When no SR has been ac-
quired, the algorithm remains undecided. The re-
sults of this WSS procedure are checked against a
testing-sample manually analyzed, and precision
and recall ratios are calculated. Precision is cal-
culated as the ratio of manual-automatic matches
/ number of noun occurrences disambiguated by
the procedure. Recall is computed as the ratio
of manual-automatic matches / total number of
noun occurrences.
</bodyText>
<page confidence="0.997363">
115
</page>
<table confidence="0.999482555555556">
Technique Coverage (%)
Assoc &amp; All nouns 95.7
Assoc Si Acis) 95.5
Assoc &amp; Head-nouns 95.3
D 93.7
log - likelihood 92.9
Assoc &amp; Normalizing 92.7
02 88.2
/ 74.1
</table>
<tableCaption confidence="0.999093">
Table 3: Coverage Ratio
</tableCaption>
<subsectionHeader confidence="0.989238">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999895578947368">
In order to evaluate the different variants on the
association score and the impact of thresholding
we performed several experiments. In this section
we analyze the results. As training set we used
the 870,000 words of WSJ material provided in
the ACL/DCI version of the Penn Treebank. The
testing set consisted of 2,658 triples corresponding
to four average common verbs in the Treebank:
rise, report, seek and present. We only considered
those triples that had been correctly extracted
from the Treebank and whose noun had the cor-
rect sense included in WordNet (2,165 triples out
of the 2,658, from now on, called the testing-
sample).
As evaluation measures we used coverage, ab-
straction ratio, and recall and precision ratios on
the WSS task (section 4.1). In addition we per-
formed some evaluation by hand comparing the
SRs acquired by the different techniques.
</bodyText>
<subsectionHeader confidence="0.526127">
4.2.1 Comparing different techniques
</subsectionHeader>
<bodyText confidence="0.999982038461538">
Coverage for the different techniques is shown
in table 3. The higher the coverage, the better the
technique succeeds in correctly generalizing more
of the input examples. The labels used for re-
ferring to the different techniques are as follows:
&amp;quot;Assoc &amp; p(c1s)&amp;quot; corresponds to the basic associ-
ation measure (section 2), &amp;quot;Assoc Si Head-nouns&amp;quot;
and &amp;quot;Assoc &amp; All nouns&amp;quot; to the techniques intro-
duced in section 3.1, &amp;quot;Assoc &amp; Normalizing&amp;quot; to
the local normalization (section 3.2), and finally,
log-likelihood, D (relative entropy) and I (mutual
information ratio) to the techniques discussed in
section 3.3.
The abstraction ratio for the different tech-
niques is shown in table 4. In principle, the higher
abstraction ratio, the better the technique suc-
ceeds in filtering out incorrect senses (less -11.Abs).
The precision and recall ratios on the noun WSS
task for the different techniques are shown in ta-
ble 5. In principle, the higher the precision and
recall ratios the better the technique succeeds in
inducing appropriate SRs for the disambiguation
task.
As far as the evaluation measures try to account
for different phenomena the goodness of a partic-
ular technique should be quantified as a trade-off.
</bodyText>
<table confidence="0.999615333333333">
I Technique Abs Ratio (%)
/ 66.6
log - likelihood 64.6
02 64.4
Assoc &amp; All nouns 64.3
Assoc &amp; Head-nouns 63.9
Assoc &amp; p(c1s) 63
D 62.3
Assoc &amp; Normalizing 58.5
</table>
<tableCaption confidence="0.984296">
Table 4: Abstraction Ratio
</tableCaption>
<table confidence="0.9997733">
Technique Prec. (%) Rec. (%)
Assoc &amp; All nouns 80.3 78.5
Assoc Si p(cls) 79.9 77.9
Assoc ik Head-nouns 78.5 76.7
log - likelihood 77.2 74.4
D 75.9 74.1
Assoc &amp; Normalizing 75.9 73.3
02 67.8 63
/ 50.4 45.7
Guessing Heuristic 62.7 62.7
</table>
<tableCaption confidence="0.999727">
Table 5: Precision and Recall on the WSS task
</tableCaption>
<bodyText confidence="0.99947475">
Most of the results are very similar (differences
are not statistically significative). Therefore we
should be cautious when extrapolating the results.
Some of the conclusions from the tables above are:
</bodyText>
<listItem confidence="0.751706166666667">
1. 02 and I get sensibly worse results than
other measures (although abstraction is quite
good).
2. The local normalizing technique using the
uniform distribution does not help. It seems
that by using the local weighting we mis-
</listItem>
<bodyText confidence="0.743959">
inform the algorithm. The problem is the
reduced weight that polysemous nouns get,
while they seem to be the most informative4.
However, a better informed kind of local
weight (section 5) should improve the tech-
nique significantly.
3. All versions of Assoc (except the local nor-
malization) get good results. Specially the
two techniques that exploit a simpler prior
distribution, which seem to improve the ba-
sic technique.
4. log-likelihood and D seem to get slightly worse
results than Assoc techniques, although the
results are very similar.
</bodyText>
<subsectionHeader confidence="0.909271">
4.2.2 Thresholding
</subsectionHeader>
<bodyText confidence="0.9992626">
We were also interested in measuring the impact
of thresholding on the SRs acquired. In figure 1
we can see the different evaluation measures of
the basic technique when varying the threshold.
Precision and recall coincide when no candidate
</bodyText>
<footnote confidence="0.9983255">
4In some way, it conforms to Zipf-law (Zipf, 1945):
noun frequency and polysemy are correlated.
</footnote>
<page confidence="0.997468">
116
</page>
<figure confidence="0.972928">
Precision 4â€”
Recall +
Coverage -g.â€”
Abstraction Ratio .x
0 5 10 15 20
Threshold
</figure>
<figureCaption confidence="0.999811">
Figure 1: Assoc: Evaluation ratios vs. Threshold
</figureCaption>
<bodyText confidence="0.964100055555556">
classes are refused (threshold = 1). However, as
it might be expected, as the threshold increases
(i.e. some cases are not classified) the two ratios
slightly diverge (precision increases and recall di-
minishes).
Figure 1 also shows the impact of thresholding
on coverage and abstraction ratios. Both decreaseâ€¢
when threshold increases, probably because when
the rejecting threshold is low, small classes that
fit the data well can be induced, learning over-
general or incomplete SRs otherwise.
Finally, it seems that precision and abstrac-
tion ratios are in inverse co-relation (as precision
grows, abstraction decreases). In terms of WSS,
general classes may be performing better than
classes that fit the data better. Nevertheless, this
relationship should be further explored in future
work.
</bodyText>
<sectionHeader confidence="0.966966" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9979406875">
In this paper we have presented some variations
affecting the association measure and thresholding
on the basic technique for learning SRs from on-
line corpora. We proposed some evaluation mea-
sures for the SRs learning task. Finally, experi-
mental results on these variations were reported.
We can conclude that some of these variations
seem to improve the results obtained using the
basic technique. However, although the technique
still seems far from practical application to NLP
tasks, it may be most useful for providing exper-
imental insight to lexicographers. Future lines of
research will mainly concentrate on improving the
local normalization technique by solving the noun
sense ambiguity. We have foreseen the application
of the following techniques:
</bodyText>
<listItem confidence="0.979611">
â€¢ Simple techniques to decide the best sense
c given the target noun n using estimates
of the n-grams: P(c), P(eln), P(elv,$) and
P(civ, s,n), obtained from supervised and
un-supervised corpora.
â€¢ Combining the different n-grams by means of
smoothing techniques.
â€¢ Calculating P(clv, s, n) combining P(nic)
and P(clv, s), and applying the EM Algo-
rithm (Dempster et al., 1977) to improve the
model.
â€¢ Using the WordNet hierarchy as a source of
backing-off knowledge, in such a way that if
n-grams composed by c aren&apos;t enough to de-
cide the best sense (are equal to zero), the
tri-grams of ancestor classes could be used
instead.
</listItem>
<sectionHeader confidence="0.991578" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994373230769231">
R. Basili, M.T. Pazienza, and P. Velardi. 1992.
Computational lexicons: the neat examples and
the odd exemplars. In Procs 3rd A NLP, Trento,
Italy, April.
K.W. Church and P. Hanks. 1990. Word associa-
tion norms, mutual information and lexicogra-
phy. Computational Linguistics, 16(1).
T.M. Cover and J.A. Thomas, editors. 1991. El-
ements of Information Theory. John Wiley.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the em algorithm. Journal of the
Royal Statistical Society, 39(B):1-38.
T. Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Compu-
tational Linguistics, 19(1):61-74.
W. Gale and K. W. Church. 1991. Identify-
ing word correspondences in parallel texts. In
DARPA Speech and Natural Language Work-
shop, Pacific Grove, California, February.
R. Grishman and J. Sterling. 1992. Acquisition
of selectional patterns. In COLING, Nantes,
France, march.
G. Hirst. 1987. Semantic interpretation and the
resolution of ambiguity. Cambridge University
Press.
. Levin. 1992. Towards a lexical organization of
English verbs. University of Chicago Press.
. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller. 1991. Five papers on wordnet.
International Journal of Lexicography.
P. S. Resnik. 1992. Wordnet and distributional
analysis: A class-based approach to lexical dis-
covery. In AAAI Symposium on Probabilistic
Approaches to NL, San Jose, CA.
P. S. Resnik. 1993. Selection and Information: A
Class-Based Approach to lexical relationships.
Ph.D. thesis, Computer and Information Sci-
ence Department, University of Pennsylvania.
</reference>
<figure confidence="0.995481181818182">
100
% 95
90
85
80
75
70 -
65 -
60 -
++++++++
x.Xâ€¢X Xâ€¢X X&apos;X .X1( Kâ€¢X X.x
</figure>
<reference confidence="0.9142269375">
F. Ribas. 1994a. An experiment on learning ap-
propriate selectional restrictions from a parsed
corpus. In COLING, Kyoto, Japan, August.
F. Ribas. 1994b. Learning more appropriate
selectional restrictions. Technical report, ES-
PRIT BRA-7315 ACQUILEX-II WP.
G. Whittemore, K. Ferrara, and H. Brunner.
1990. Empirical study of predictive powers
of simple attachment schemes for post-modifier
prepositional phrases. In Procs. ACL, Pennsyl-
vania.
G. K. Zipf. 1945. The meaning-frequency rela-
tionship of words. The Journal of General Psy-
chology, 33:251-256.
(Acquilex-II Working Papers can be obtained
by sending a request to cide@cup . cam . uk)
</reference>
<page confidence="0.956197">
110
</page>
<figure confidence="0.984982875">
Precision
Recall + -
Coverage
Abstraction Ratio â€¢x â€¢ -
++++++++ H
â€”x x Xâ€¢X Xâ€¢X x.)( x.x x.v
0 5 10 15 20
Threshold
</figure>
<figureCaption confidence="0.999929">
Figure 1: Assoc: Evaluation ratios vs. Threshold
</figureCaption>
<bodyText confidence="0.963576166666667">
classes are refused (threshold = 1). However, as
it might be expected, as the threshold increases
(i.e. some cases are not classified) the two ratios
slightly diverge (precision increases and recall di-
minishes).
Figure 1 also shows the impact of thresholding
on coverage and abstraction ratios. Both decrease.
when threshold increases, probably because when
the rejecting threshold is low, small classes that
fit the data well can be induced, learning over-
general or incomplete SRs otherwise.
Finally, it seems that precision and abstrac-
tion ratios are in inverse co-relation (as precision
grows, abstraction decreases). In terms of WSS,
general classes may be performing better than
classes that fit the data better. Nevertheless, this
relationship should be further explored in future
work.
</bodyText>
<sectionHeader confidence="0.992395" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9996194375">
In this paper we have presented some variations
affecting the association measure and thresholding
on the basic technique for learning SRs from on-
line corpora. We proposed some evaluation mea-
sures for the SRs learning task. Finally, experi-
mental results on these variations were reported.
We can conclude that some of these variations
seem to improve the results obtained using the
basic technique. However, although the technique
still seems far from practical application to NLP
tasks, it may be most useful for providing exper-
imental insight to lexicographers. Future lines of
research will mainly concentrate on improving the
local normalization technique by solving the noun
sense ambiguity. We have foreseen the application
of the following techniques:
</bodyText>
<listItem confidence="0.955988461538462">
â€¢ Simple techniques to decide the best sense
c given the target noun n using estimates
of the n-grams: P(c), P(cln), P(clv, s) and
P(clv, s, n), obtained from supervised and
un-supervised corpora.
â€¢ Combining the different n-grams by means of
smoothing techniques.
â€¢ Calculating P(clv, s, n) combining P(rile)
and P(clv, s), and applying the EM Algo-
rithm (Dempster et al., 1977) to improve the
model.
â€¢ Using the WordNet hierarchy as a source of
backing-off knowledge, in such a way that if
</listItem>
<bodyText confidence="0.83519275">
n-grams composed by c aren&apos;t enough to de-
cide the best sense (are equal to zero), the
tri-grams of ancestor classes could be used
instead.
</bodyText>
<sectionHeader confidence="0.997458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999303025641026">
R. Basili, M.T. Pazienza, and P. Velardi. 1992.
Computational lexicons: the neat examples and
the odd exemplars. In Procs 3rd A NLP, Trento,
Italy, April.
K.W. Church and P. Hanks. 1990. Word associa-
tion norms, mutual information and lexicogra-
phy. Computational Linguistics, 16(1).
T.M. Cover and J.A. Thomas, editors. 1991. El-
ements of Information Theory. John Wiley.
A. P. Dempster, N. M. Laird, and D. B. Ru-
bin. 1977. Maximum likelihood from incom-
plete data via the em algorithm. Journal of the
Royal Statistical Society, 39(B):1-38.
T. Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Compu-
tational Linguistics, 19(1):61-74.
W. Gale and K. W. Church. 1991. Identify-
ing word correspondences in parallel texts. In
DARPA Speech and Natural Language Work-
shop, Pacific Grove, California, February.
R. Grishman and J. Sterling. 1992. Acquisition
of selectional patterns. In COLING, Nantes,
France, march.
G. Hirst. 1987. Semantic interpretation and the
resolution of ambiguity. Cambridge University
Press.
. Levin. 1992. Towards a lexical organization of
English verbs. University of Chicago Press.
. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller. 1991. Five papers on wordnet.
International Journal of Lexicography.
P. S. Resnik. 1992. Wordnet and distributional
analysis: A class-based approach to lexical dis-
covery. In AAAI Symposium on Probabilistic
Approaches to NL, San Jose, CA.
P. S. Resnik. 1993. Selection and Information: A
Class-Based Approach to lexical relationships.
Ph.D. thesis, Computer and Information Sci-
ence Department, University of Pennsylvania.
</reference>
<figure confidence="0.548157777777778">
100
95
90
85
80
75
70
65
60
</figure>
<page confidence="0.977304">
117
</page>
<reference confidence="0.979346">
F. Ribas. 1994a. An experiment on learning ap-
propriate selectional restrictions from a parsed
corpus. In COLING, Kyoto, Japan, August.
F. Ribas. 1994b. Learning more appropriate
selectional restrictions. Technical report, ES-
PRIT BRA-7315 ACQUILEX-II WP.
G. Whittemore, K. Ferrara, and H. Brunner.
1990. Empirical study of predictive powers
of simple attachment schemes for post-modifier
prepositional phrases. In Procs. ACL, Pennsyl-
vania.
G. K. Zipf. 1945. The meaning-frequency rela-
tionship of words. The Journal of General Psy-
chology, 33:251-256.
(Acquilex-II Working Papers can be obtained
by sending a request to cide@cup. cam . uk)
</reference>
<page confidence="0.996001">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.363738">
<title confidence="0.999916">On Learning more Appropriate Selectional Restrictions</title>
<author confidence="0.996913">Francesc Ribas</author>
<affiliation confidence="0.9998125">Departament de Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya</affiliation>
<address confidence="0.913650333333333">Pau Gargallo, 5 08028 Barcelona Spain</address>
<email confidence="0.98178">ribaselsi.upc.es</email>
<abstract confidence="0.996494166666667">We present some variations affecting the association measure and thresholding on a technique for learning Selectional Restrictions from on-line corpora. It uses a wide-coverage noun taxonomy and a statistical measure to generalize the appropriate semantic classes. Evaluation measures for the Selectional Restrictions learning task are discussed. Finally, an experimental evaluation of these variations is reported.</abstract>
<note confidence="0.765959">Subject Areas: corpus-based language</note>
<intro confidence="0.667181">modeling, computational lexicography</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>Computational lexicons: the neat examples and the odd exemplars.</title>
<date>1992</date>
<booktitle>In Procs 3rd A NLP,</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="1400" citStr="Basili et al., 1992" startWordPosition="205" endWordPosition="208">he NLP research community on the importance of having an extensive coverage of selectional restrictions (SRs) tuned to the domain to work with. SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using thi</context>
</contexts>
<marker>Basili, Pazienza, Velardi, 1992</marker>
<rawString>R. Basili, M.T. Pazienza, and P. Velardi. 1992. Computational lexicons: the neat examples and the odd exemplars. In Procs 3rd A NLP, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="11843" citStr="Church and Hanks, 1990" startWordPosition="1879" endWordPosition="1882">he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution, P(XIY), from the expected distribution if both variables were considered independent, i.e. the marginal distribution, P (X). If P (X) is a good approximation Assoc&apos; (v, s, c) = p(clv, s) log kciv, s) 114 C v_s p(c v_s) p(-&apos;c v_s) -&apos;v_s p(c -w_s) p(-.c -w_s) p(c) p(-,c) Table 2: Conditional and marginal distributions of P(XIY), asso</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<title>Elements of Information Theory.</title>
<date>1991</date>
<editor>T.M. Cover and J.A. Thomas, editors.</editor>
<publisher>John Wiley.</publisher>
<marker>1991</marker>
<rawString>T.M. Cover and J.A. Thomas, editors. 1991. Elements of Information Theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="11749" citStr="Dunning, 1993" startWordPosition="1868" endWordPosition="1869"> cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution, P(XIY), from the expected distribution if both variables were considered independent, i.e. the marginal distribution, P (X). If P (X) is a good approximation Assoc&apos; (v, s, c) = p(clv, s) log kciv, s) 114 C v_s p(c v_s) p(-&apos;c v_s) -&apos;v_s p(c</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In DARPA Speech and Natural Language Workshop,</booktitle>
<location>Pacific Grove, California,</location>
<contexts>
<context position="11871" citStr="Gale and Church, 1991" startWordPosition="1884" endWordPosition="1887">mber of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution, P(XIY), from the expected distribution if both variables were considered independent, i.e. the marginal distribution, P (X). If P (X) is a good approximation Assoc&apos; (v, s, c) = p(clv, s) log kciv, s) 114 C v_s p(c v_s) p(-&apos;c v_s) -&apos;v_s p(c -w_s) p(-.c -w_s) p(c) p(-,c) Table 2: Conditional and marginal distributions of P(XIY), association measures should be l</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W. Gale and K. W. Church. 1991. Identifying word correspondences in parallel texts. In DARPA Speech and Natural Language Workshop, Pacific Grove, California, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>J Sterling</author>
</authors>
<title>Acquisition of selectional patterns.</title>
<date>1992</date>
<booktitle>In COLING,</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="16188" citStr="Grishman and Sterling, 1992" startWordPosition="2584" endWordPosition="2587"> effectively correct (from now on called Abstraction Ratio). Hopefully, a technique with a higher abstraction ratio learns classes that fit the set of examples better. A manual assessment of the ratio confirmed this behavior, as testing sets with a lower ratio seemed to be inducing less 1lAbs cases. â€¢ Quantification of coverage It could be measured as the proportion of triples whose correct sense belongs to one of the SRs. 4.1.2 NLP evaluation tasks The NLP tasks where SRs utility could be evaluated are diverse. Some of them have already been introduced in section 1. In the recent literature aGrishman and Sterling, 1992), (Resnik, 1993), ...) several task oriented schemes to test Selectional Restrictions (mainly on syntactic ambiguity resolution) have been proposed. However, we have tested SRs on a WSS task, using the following scheme. For every triple in the testing set the algorithm selects as most appropriate that noun-sense that has as hyperonym the SR class with highest association score. When more than one sense belongs to the highest SR, a random selection is performed. When no SR has been acquired, the algorithm remains undecided. The results of this WSS procedure are checked against a testing-sample </context>
</contexts>
<marker>Grishman, Sterling, 1992</marker>
<rawString>R. Grishman and J. Sterling. 1992. Acquisition of selectional patterns. In COLING, Nantes, France, march.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Semantic interpretation and the resolution of ambiguity.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1206" citStr="Hirst, 1987" startWordPosition="178" endWordPosition="179">valuation of these variations is reported. Subject Areas: corpus-based language modeling, computational lexicography 1 Introduction In recent years there has been a common agreement in the NLP research community on the importance of having an extensive coverage of selectional restrictions (SRs) tuned to the domain to work with. SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in th</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>G. Hirst. 1987. Semantic interpretation and the resolution of ambiguity. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levin</author>
</authors>
<title>Towards a lexical organization of English verbs.</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1535" citStr="Levin, 1992" startWordPosition="228" endWordPosition="229"> SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifica</context>
</contexts>
<marker>Levin, 1992</marker>
<rawString>. Levin. 1992. Towards a lexical organization of English verbs. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Beckwith Miller</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1991</date>
<journal>International Journal of Lexicography.</journal>
<contexts>
<context position="3546" citStr="Miller et al., 1991" startWordPosition="534" endWordPosition="537">p, noun-lemma) extracted from the corpus. Previous knowledge used A semantic hierarchy (WordNetl) where words are clustered in semantic classes, and semantic classes are organized hierarchically. Polysemous words are represented as instances of different classes. Output A set of syntactic SRs, (verb-lemma, syntactic-relationship, semantic-class, weight). The final SRs must be mutually disjoint. SRs are weighted according to the statistical evidence found in the corpus. Learning process 3 stages: 1. Creation of the space of candidate classes. iWordNet is a broad-coverage lexical database, see (Miller et al., 1991)) 112 Acquired SR Type Assoc Examples of nouns in Treebank &lt; suit, suing &gt; Senses 0.41 suit &lt; suit_o f _clothes &gt; Senses 0.41 suit &lt; suit &gt; Senses 0.40 suit &lt; group&gt; frAbs 0.35 administration, agency, bank, ... &lt;legal_action&gt; Ok 0.28 suit &lt; person, individual&gt; Ok 0.23 advocate, buyer,carrier, client, ... &lt; radical&gt; Senses 0.16 group &lt; city&gt; Senses 0.15 proper_name &lt; admin_district &gt; Senses 0.14 proper_name &lt; social_control &gt; Senses 0.11 administration ,government &lt; status&gt; Senses 0.087 government, leadership &lt; activity&gt; Senses -0.01 administration, leadership, provision &lt; cognition&gt; Senses -0.</context>
</contexts>
<marker>Miller, Fellbaum, Gross, Miller, 1991</marker>
<rawString>. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1991. Five papers on wordnet. International Journal of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Resnik</author>
</authors>
<title>Wordnet and distributional analysis: A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>In AAAI Symposium on Probabilistic Approaches to NL,</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="1673" citStr="Resnik (1992)" startWordPosition="252" endWordPosition="253">terpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifications to the basic technique and will report the corresponding experimental evaluation. The outline of the paper is as follows: in section</context>
<context position="4579" citStr="Resnik, 1992" startWordPosition="699" endWordPosition="700">ontrol &gt; Senses 0.11 administration ,government &lt; status&gt; Senses 0.087 government, leadership &lt; activity&gt; Senses -0.01 administration, leadership, provision &lt; cognition&gt; Senses -0.04 concern, leadership, provision, science Table 1: SRs acquired for the subject of seek 2. Evaluation of the appropriateness of the candidates by means of a statistical measure. 3. Selection of the most appropriate subset in the candidate space to convey the SRs. The appropriateness of a class for expressing SRs (stage 2) is quantified from the strength of co-occurrence of verbs and classes of nouns in the. corpus (Resnik, 1992). Given the verb v, the syntactic-relationship s and the candidate class c, the Association Score, Assoc, between v and c in s is defined: Assoc(v, s, c) = p(clv, s)I(v; els) = p(clv s) log *iv&apos; s) P(cs) The two terms of Assoc try to capture different properties: 1. Mutual information ratio, /(v; e)s), measures the strength of the statistical association between the given verb v and the candidate class c in the given syntactic position s. It compares the prior distribution, p(c1s), with the posterior distribution, p(clv, s). 2. p(clv, , s) scales up the strength of the association by the frequ</context>
<context position="7205" citStr="Resnik (1992)" startWordPosition="1139" endWordPosition="1140">section we summarize the results and conclusions reached in that paper. For instance, table 1 shows the SRs acquired for the subject position of the verb seek. Type indicates a manual diagnosis about the class appropriateness (Ok: correct; itAbs: over-generalization; Senses: due to erroneous senses). Assoc corresponds to the association score (higher values appear first). Most of the induced classes are due to incorrect senses. Thus, although suit was used in the WSJ articles only in the sense of &lt; legal_action &gt; , the algorithm not only considered the other senses as well (&lt; suit, suing &gt;,&lt; 3Resnik (1992) and Ribas (1994a) used equation 1 without introducing normalization. Therefore, the estimated function didn&apos;t accomplish probability axioms. Nevertheless, their results should be equivalent (for our purposes) to those introducing normalization because it shouldn&apos;t affect the relative ordering of Assoc among rival candidate classes for the same (v, s). 113 suii_of _clothes&gt;, &lt; suit&gt;) , but the Assoc score ranked them higher than the appropriate sense. We can also notice that the frAbs class, &lt;group&gt;, seems too general for the example nouns, while one of its daughters, &lt; people &gt; seems to fit t</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>P. S. Resnik. 1992. Wordnet and distributional analysis: A class-based approach to lexical discovery. In AAAI Symposium on Probabilistic Approaches to NL, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to lexical relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer and Information Science Department, University of Pennsylvania.</institution>
<contexts>
<context position="5719" citStr="Resnik, 1993" startWordPosition="888" endWordPosition="889">s). 2. p(clv, , s) scales up the strength of the association by the frequency of the relationship. Probabilities are estimated by Maximum Likelihood Estimation (MLE), i.e. counting the relative frequency of the considered events in the corpus2. However, it is not obvious how to calculate class frequencies when the training corpus is not semantically tagged as is the case. Nevertheless, we take a simplistic approach and calculate them in the following manner: f req(v, s, c) = freq(v,s,n) x w (1) riâ‚¬c 2The utility of introducing smoothing techniques on class-based distributions is dubious, see (Resnik, 1993). Where w is a constant factor used to normalize the probabilities&apos; = Evev EsEs EnEA, f req(v, s, n) w EsEs ErlE.Ar f req(v, s, n)Isenses(n)I (2) When creating the space of candidate classes (learning process, stage 1), we use a thresholdjug technique to ignore as much as possible the noise introduced in the training set. Specifically, we consider only those classes that have a higher number of occurrences than the threshold. The selection of the most appropriate classes (stage 3) is based on a global search through the candidates, in such a way that the final classes are mutually disjoint (no</context>
<context position="11173" citStr="Resnik (1993)" startWordPosition="1778" endWordPosition="1779"> ambiguous nouns could be more informative about the correct classes as long as they do not carry ambiguity. The weight introduced in (1) could alternatively be found in a local manner, in such a way that more polysemous nouns would give less evidence to each one of their senses than less ambiguous ones. Local weight could be obtained using p(cln). Nevertheless, a good estimation of this probability seems quite problematic because of the lack of tagged training material. In absence of a better estimator we use a rather poor one as the uniform distribution, isenses(n) E cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cove</context>
<context position="16204" citStr="Resnik, 1993" startWordPosition="2588" endWordPosition="2589">on called Abstraction Ratio). Hopefully, a technique with a higher abstraction ratio learns classes that fit the set of examples better. A manual assessment of the ratio confirmed this behavior, as testing sets with a lower ratio seemed to be inducing less 1lAbs cases. â€¢ Quantification of coverage It could be measured as the proportion of triples whose correct sense belongs to one of the SRs. 4.1.2 NLP evaluation tasks The NLP tasks where SRs utility could be evaluated are diverse. Some of them have already been introduced in section 1. In the recent literature aGrishman and Sterling, 1992), (Resnik, 1993), ...) several task oriented schemes to test Selectional Restrictions (mainly on syntactic ambiguity resolution) have been proposed. However, we have tested SRs on a WSS task, using the following scheme. For every triple in the testing set the algorithm selects as most appropriate that noun-sense that has as hyperonym the SR class with highest association score. When more than one sense belongs to the highest SR, a random selection is performed. When no SR has been acquired, the algorithm remains undecided. The results of this WSS procedure are checked against a testing-sample manually analyze</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. S. Resnik. 1993. Selection and Information: A Class-Based Approach to lexical relationships. Ph.D. thesis, Computer and Information Science Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ribas</author>
</authors>
<title>An experiment on learning appropriate selectional restrictions from a parsed corpus.</title>
<date>1994</date>
<booktitle>In COLING,</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="1770" citStr="Ribas (1994" startWordPosition="267" endWordPosition="268"> Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifications to the basic technique and will report the corresponding experimental evaluation. The outline of the paper is as follows: in section 2 we summarize the basic methodology used in (Ribas, 1994a), analyzing its limitations; in secti</context>
<context position="6371" citStr="Ribas (1994" startWordPosition="998" endWordPosition="999">rmalize the probabilities&apos; = Evev EsEs EnEA, f req(v, s, n) w EsEs ErlE.Ar f req(v, s, n)Isenses(n)I (2) When creating the space of candidate classes (learning process, stage 1), we use a thresholdjug technique to ignore as much as possible the noise introduced in the training set. Specifically, we consider only those classes that have a higher number of occurrences than the threshold. The selection of the most appropriate classes (stage 3) is based on a global search through the candidates, in such a way that the final classes are mutually disjoint (not related by hyperonymy). 2.2 Evaluation Ribas (1994a) reported experimental results obtained from the application of the above technique to learn SRs. He performed an evaluation of the Sits obtained from a training set of 870,000 words of the Wall Street Journal. In this section we summarize the results and conclusions reached in that paper. For instance, table 1 shows the SRs acquired for the subject position of the verb seek. Type indicates a manual diagnosis about the class appropriateness (Ok: correct; itAbs: over-generalization; Senses: due to erroneous senses). Assoc corresponds to the association score (higher values appear first). Most</context>
<context position="7916" citStr="Ribas (1994" startWordPosition="1248" endWordPosition="1249">ion didn&apos;t accomplish probability axioms. Nevertheless, their results should be equivalent (for our purposes) to those introducing normalization because it shouldn&apos;t affect the relative ordering of Assoc among rival candidate classes for the same (v, s). 113 suii_of _clothes&gt;, &lt; suit&gt;) , but the Assoc score ranked them higher than the appropriate sense. We can also notice that the frAbs class, &lt;group&gt;, seems too general for the example nouns, while one of its daughters, &lt; people &gt; seems to fit the data much better. Analyzing the results obtained from different experimental evaluation methods, Ribas (1994a) drew up some conclusions: a. The technique achieves a good coverage. b. Most of the classes acquired result from the accumulation of incorrect senses. c. No clear co-relation between Assoc and the manual diagnosis is found. d. A slight tendency to over-generalization exists due to incorrect senses. Although the performance of the presented technique seems to be quite good, we think that some of the detected flaws could possibly be addressed. Noise due to polysemy of the nouns involved seems to be the main obstacle for the practicality of the technique. It makes the association score prefer </context>
<context position="11354" citStr="Ribas, 1994" startWordPosition="1807" endWordPosition="1808">r, in such a way that more polysemous nouns would give less evidence to each one of their senses than less ambiguous ones. Local weight could be obtained using p(cln). Nevertheless, a good estimation of this probability seems quite problematic because of the lack of tagged training material. In absence of a better estimator we use a rather poor one as the uniform distribution, isenses(n) E cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measur</context>
<context position="14005" citStr="Ribas, 1994" startWordPosition="2230" endWordPosition="2231">the corresponding marginals. Relative entropy, D(P(Xlv_s)11P(X)), could do this job. 4 Evaluation 4.1 Evaluation methods of SRs Evaluation on NLP has been crucial to fostering research in particular areas. Evaluation of the SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet (e.g, section 4.2). It would also permit measuring the utility of the SRs obtained using WordNet in comparison with other frameworks using other kinds of knowledge. Finally it would be a powerful tool for detecting flaws of a particular technique (e.g, (Ribas, 1994a) analysis). However, a related and crucial issue is which linguistic tasks are used as a reference. SRs are useful for both lexicography and NLP. On the one hand, from the point of view of lexicography, the goal of evaluation would be to measure the quality of the SRs induced, (i.e., how well the resulting classes correspond to the nouns as they were used in the corpus). On the other hand, from the point of view of NLP, SRs should be evaluated on their utility (i.e., how much they help on performing the reference task). 4.1.1 Lexicography-oriented evaluation As far as lexicography (quality) </context>
</contexts>
<marker>Ribas, 1994</marker>
<rawString>F. Ribas. 1994a. An experiment on learning appropriate selectional restrictions from a parsed corpus. In COLING, Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ribas</author>
</authors>
<title>Learning more appropriate selectional restrictions.</title>
<date>1994</date>
<tech>Technical report, ESPRIT BRA-7315 ACQUILEX-II WP.</tech>
<contexts>
<context position="1770" citStr="Ribas (1994" startWordPosition="267" endWordPosition="268"> Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifications to the basic technique and will report the corresponding experimental evaluation. The outline of the paper is as follows: in section 2 we summarize the basic methodology used in (Ribas, 1994a), analyzing its limitations; in secti</context>
<context position="6371" citStr="Ribas (1994" startWordPosition="998" endWordPosition="999">rmalize the probabilities&apos; = Evev EsEs EnEA, f req(v, s, n) w EsEs ErlE.Ar f req(v, s, n)Isenses(n)I (2) When creating the space of candidate classes (learning process, stage 1), we use a thresholdjug technique to ignore as much as possible the noise introduced in the training set. Specifically, we consider only those classes that have a higher number of occurrences than the threshold. The selection of the most appropriate classes (stage 3) is based on a global search through the candidates, in such a way that the final classes are mutually disjoint (not related by hyperonymy). 2.2 Evaluation Ribas (1994a) reported experimental results obtained from the application of the above technique to learn SRs. He performed an evaluation of the Sits obtained from a training set of 870,000 words of the Wall Street Journal. In this section we summarize the results and conclusions reached in that paper. For instance, table 1 shows the SRs acquired for the subject position of the verb seek. Type indicates a manual diagnosis about the class appropriateness (Ok: correct; itAbs: over-generalization; Senses: due to erroneous senses). Assoc corresponds to the association score (higher values appear first). Most</context>
<context position="7916" citStr="Ribas (1994" startWordPosition="1248" endWordPosition="1249">ion didn&apos;t accomplish probability axioms. Nevertheless, their results should be equivalent (for our purposes) to those introducing normalization because it shouldn&apos;t affect the relative ordering of Assoc among rival candidate classes for the same (v, s). 113 suii_of _clothes&gt;, &lt; suit&gt;) , but the Assoc score ranked them higher than the appropriate sense. We can also notice that the frAbs class, &lt;group&gt;, seems too general for the example nouns, while one of its daughters, &lt; people &gt; seems to fit the data much better. Analyzing the results obtained from different experimental evaluation methods, Ribas (1994a) drew up some conclusions: a. The technique achieves a good coverage. b. Most of the classes acquired result from the accumulation of incorrect senses. c. No clear co-relation between Assoc and the manual diagnosis is found. d. A slight tendency to over-generalization exists due to incorrect senses. Although the performance of the presented technique seems to be quite good, we think that some of the detected flaws could possibly be addressed. Noise due to polysemy of the nouns involved seems to be the main obstacle for the practicality of the technique. It makes the association score prefer </context>
<context position="11354" citStr="Ribas, 1994" startWordPosition="1807" endWordPosition="1808">r, in such a way that more polysemous nouns would give less evidence to each one of their senses than less ambiguous ones. Local weight could be obtained using p(cln). Nevertheless, a good estimation of this probability seems quite problematic because of the lack of tagged training material. In absence of a better estimator we use a rather poor one as the uniform distribution, isenses(n) E cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measur</context>
<context position="14005" citStr="Ribas, 1994" startWordPosition="2230" endWordPosition="2231">the corresponding marginals. Relative entropy, D(P(Xlv_s)11P(X)), could do this job. 4 Evaluation 4.1 Evaluation methods of SRs Evaluation on NLP has been crucial to fostering research in particular areas. Evaluation of the SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet (e.g, section 4.2). It would also permit measuring the utility of the SRs obtained using WordNet in comparison with other frameworks using other kinds of knowledge. Finally it would be a powerful tool for detecting flaws of a particular technique (e.g, (Ribas, 1994a) analysis). However, a related and crucial issue is which linguistic tasks are used as a reference. SRs are useful for both lexicography and NLP. On the one hand, from the point of view of lexicography, the goal of evaluation would be to measure the quality of the SRs induced, (i.e., how well the resulting classes correspond to the nouns as they were used in the corpus). On the other hand, from the point of view of NLP, SRs should be evaluated on their utility (i.e., how much they help on performing the reference task). 4.1.1 Lexicography-oriented evaluation As far as lexicography (quality) </context>
</contexts>
<marker>Ribas, 1994</marker>
<rawString>F. Ribas. 1994b. Learning more appropriate selectional restrictions. Technical report, ESPRIT BRA-7315 ACQUILEX-II WP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Whittemore</author>
<author>K Ferrara</author>
<author>H Brunner</author>
</authors>
<title>Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.</title>
<date>1990</date>
<booktitle>In Procs. ACL,</booktitle>
<location>Pennsylvania.</location>
<contexts>
<context position="1301" citStr="Whittemore et al., 1990" startWordPosition="189" endWordPosition="192">eling, computational lexicography 1 Introduction In recent years there has been a common agreement in the NLP research community on the importance of having an extensive coverage of selectional restrictions (SRs) tuned to the domain to work with. SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Depa</context>
</contexts>
<marker>Whittemore, Ferrara, Brunner, 1990</marker>
<rawString>G. Whittemore, K. Ferrara, and H. Brunner. 1990. Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases. In Procs. ACL, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>The meaning-frequency relationship of words.</title>
<date>1945</date>
<journal>The Journal of General Psychology,</journal>
<pages>33--251</pages>
<contexts>
<context position="21145" citStr="Zipf, 1945" startWordPosition="3398" endWordPosition="3399">ions of Assoc (except the local normalization) get good results. Specially the two techniques that exploit a simpler prior distribution, which seem to improve the basic technique. 4. log-likelihood and D seem to get slightly worse results than Assoc techniques, although the results are very similar. 4.2.2 Thresholding We were also interested in measuring the impact of thresholding on the SRs acquired. In figure 1 we can see the different evaluation measures of the basic technique when varying the threshold. Precision and recall coincide when no candidate 4In some way, it conforms to Zipf-law (Zipf, 1945): noun frequency and polysemy are correlated. 116 Precision 4â€” Recall + Coverage -g.â€” Abstraction Ratio .x 0 5 10 15 20 Threshold Figure 1: Assoc: Evaluation ratios vs. Threshold classes are refused (threshold = 1). However, as it might be expected, as the threshold increases (i.e. some cases are not classified) the two ratios slightly diverge (precision increases and recall diminishes). Figure 1 also shows the impact of thresholding on coverage and abstraction ratios. Both decreaseâ€¢ when threshold increases, probably because when the rejecting threshold is low, small classes that fit the data</context>
</contexts>
<marker>Zipf, 1945</marker>
<rawString>G. K. Zipf. 1945. The meaning-frequency relationship of words. The Journal of General Psychology, 33:251-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>Acquilex-II Working Papers can be obtained by sending a request to cide@cup</title>
<date>1992</date>
<booktitle>In Procs 3rd A NLP,</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="1400" citStr="Basili et al., 1992" startWordPosition="205" endWordPosition="208">he NLP research community on the importance of having an extensive coverage of selectional restrictions (SRs) tuned to the domain to work with. SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using thi</context>
</contexts>
<marker>Basili, Pazienza, Velardi, 1992</marker>
<rawString>(Acquilex-II Working Papers can be obtained by sending a request to cide@cup . cam . uk) R. Basili, M.T. Pazienza, and P. Velardi. 1992. Computational lexicons: the neat examples and the odd exemplars. In Procs 3rd A NLP, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="11843" citStr="Church and Hanks, 1990" startWordPosition="1879" endWordPosition="1882">he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution, P(XIY), from the expected distribution if both variables were considered independent, i.e. the marginal distribution, P (X). If P (X) is a good approximation Assoc&apos; (v, s, c) = p(clv, s) log kciv, s) 114 C v_s p(c v_s) p(-&apos;c v_s) -&apos;v_s p(c -w_s) p(-.c -w_s) p(c) p(-,c) Table 2: Conditional and marginal distributions of P(XIY), asso</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K.W. Church and P. Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<title>Elements of Information Theory.</title>
<date>1991</date>
<editor>T.M. Cover and J.A. Thomas, editors.</editor>
<publisher>John Wiley.</publisher>
<marker>1991</marker>
<rawString>T.M. Cover and J.A. Thomas, editors. 1991. Elements of Information Theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="11749" citStr="Dunning, 1993" startWordPosition="1868" endWordPosition="1869"> cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution, P(XIY), from the expected distribution if both variables were considered independent, i.e. the marginal distribution, P (X). If P (X) is a good approximation Assoc&apos; (v, s, c) = p(clv, s) log kciv, s) 114 C v_s p(c v_s) p(-&apos;c v_s) -&apos;v_s p(c</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In DARPA Speech and Natural Language Workshop,</booktitle>
<location>Pacific Grove, California,</location>
<contexts>
<context position="11871" citStr="Gale and Church, 1991" startWordPosition="1884" endWordPosition="1887">mber of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measures used to detect associations on the distribution defined by two random variables X and Y work by measuring the deviation of the conditional distribution, P(XIY), from the expected distribution if both variables were considered independent, i.e. the marginal distribution, P (X). If P (X) is a good approximation Assoc&apos; (v, s, c) = p(clv, s) log kciv, s) 114 C v_s p(c v_s) p(-&apos;c v_s) -&apos;v_s p(c -w_s) p(-.c -w_s) p(c) p(-,c) Table 2: Conditional and marginal distributions of P(XIY), association measures should be l</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W. Gale and K. W. Church. 1991. Identifying word correspondences in parallel texts. In DARPA Speech and Natural Language Workshop, Pacific Grove, California, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>J Sterling</author>
</authors>
<title>Acquisition of selectional patterns.</title>
<date>1992</date>
<booktitle>In COLING,</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="16188" citStr="Grishman and Sterling, 1992" startWordPosition="2584" endWordPosition="2587"> effectively correct (from now on called Abstraction Ratio). Hopefully, a technique with a higher abstraction ratio learns classes that fit the set of examples better. A manual assessment of the ratio confirmed this behavior, as testing sets with a lower ratio seemed to be inducing less 1lAbs cases. â€¢ Quantification of coverage It could be measured as the proportion of triples whose correct sense belongs to one of the SRs. 4.1.2 NLP evaluation tasks The NLP tasks where SRs utility could be evaluated are diverse. Some of them have already been introduced in section 1. In the recent literature aGrishman and Sterling, 1992), (Resnik, 1993), ...) several task oriented schemes to test Selectional Restrictions (mainly on syntactic ambiguity resolution) have been proposed. However, we have tested SRs on a WSS task, using the following scheme. For every triple in the testing set the algorithm selects as most appropriate that noun-sense that has as hyperonym the SR class with highest association score. When more than one sense belongs to the highest SR, a random selection is performed. When no SR has been acquired, the algorithm remains undecided. The results of this WSS procedure are checked against a testing-sample </context>
</contexts>
<marker>Grishman, Sterling, 1992</marker>
<rawString>R. Grishman and J. Sterling. 1992. Acquisition of selectional patterns. In COLING, Nantes, France, march.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Semantic interpretation and the resolution of ambiguity.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1206" citStr="Hirst, 1987" startWordPosition="178" endWordPosition="179">valuation of these variations is reported. Subject Areas: corpus-based language modeling, computational lexicography 1 Introduction In recent years there has been a common agreement in the NLP research community on the importance of having an extensive coverage of selectional restrictions (SRs) tuned to the domain to work with. SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in th</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>G. Hirst. 1987. Semantic interpretation and the resolution of ambiguity. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levin</author>
</authors>
<title>Towards a lexical organization of English verbs.</title>
<date>1992</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="1535" citStr="Levin, 1992" startWordPosition="228" endWordPosition="229"> SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifica</context>
</contexts>
<marker>Levin, 1992</marker>
<rawString>. Levin. 1992. Towards a lexical organization of English verbs. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Beckwith Miller</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1991</date>
<journal>International Journal of Lexicography.</journal>
<contexts>
<context position="3546" citStr="Miller et al., 1991" startWordPosition="534" endWordPosition="537">p, noun-lemma) extracted from the corpus. Previous knowledge used A semantic hierarchy (WordNetl) where words are clustered in semantic classes, and semantic classes are organized hierarchically. Polysemous words are represented as instances of different classes. Output A set of syntactic SRs, (verb-lemma, syntactic-relationship, semantic-class, weight). The final SRs must be mutually disjoint. SRs are weighted according to the statistical evidence found in the corpus. Learning process 3 stages: 1. Creation of the space of candidate classes. iWordNet is a broad-coverage lexical database, see (Miller et al., 1991)) 112 Acquired SR Type Assoc Examples of nouns in Treebank &lt; suit, suing &gt; Senses 0.41 suit &lt; suit_o f _clothes &gt; Senses 0.41 suit &lt; suit &gt; Senses 0.40 suit &lt; group&gt; frAbs 0.35 administration, agency, bank, ... &lt;legal_action&gt; Ok 0.28 suit &lt; person, individual&gt; Ok 0.23 advocate, buyer,carrier, client, ... &lt; radical&gt; Senses 0.16 group &lt; city&gt; Senses 0.15 proper_name &lt; admin_district &gt; Senses 0.14 proper_name &lt; social_control &gt; Senses 0.11 administration ,government &lt; status&gt; Senses 0.087 government, leadership &lt; activity&gt; Senses -0.01 administration, leadership, provision &lt; cognition&gt; Senses -0.</context>
</contexts>
<marker>Miller, Fellbaum, Gross, Miller, 1991</marker>
<rawString>. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1991. Five papers on wordnet. International Journal of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Resnik</author>
</authors>
<title>Wordnet and distributional analysis: A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>In AAAI Symposium on Probabilistic Approaches to NL,</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="1673" citStr="Resnik (1992)" startWordPosition="252" endWordPosition="253">terpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifications to the basic technique and will report the corresponding experimental evaluation. The outline of the paper is as follows: in section</context>
<context position="4579" citStr="Resnik, 1992" startWordPosition="699" endWordPosition="700">ontrol &gt; Senses 0.11 administration ,government &lt; status&gt; Senses 0.087 government, leadership &lt; activity&gt; Senses -0.01 administration, leadership, provision &lt; cognition&gt; Senses -0.04 concern, leadership, provision, science Table 1: SRs acquired for the subject of seek 2. Evaluation of the appropriateness of the candidates by means of a statistical measure. 3. Selection of the most appropriate subset in the candidate space to convey the SRs. The appropriateness of a class for expressing SRs (stage 2) is quantified from the strength of co-occurrence of verbs and classes of nouns in the. corpus (Resnik, 1992). Given the verb v, the syntactic-relationship s and the candidate class c, the Association Score, Assoc, between v and c in s is defined: Assoc(v, s, c) = p(clv, s)I(v; els) = p(clv s) log *iv&apos; s) P(cs) The two terms of Assoc try to capture different properties: 1. Mutual information ratio, /(v; e)s), measures the strength of the statistical association between the given verb v and the candidate class c in the given syntactic position s. It compares the prior distribution, p(c1s), with the posterior distribution, p(clv, s). 2. p(clv, , s) scales up the strength of the association by the frequ</context>
<context position="7205" citStr="Resnik (1992)" startWordPosition="1139" endWordPosition="1140">section we summarize the results and conclusions reached in that paper. For instance, table 1 shows the SRs acquired for the subject position of the verb seek. Type indicates a manual diagnosis about the class appropriateness (Ok: correct; itAbs: over-generalization; Senses: due to erroneous senses). Assoc corresponds to the association score (higher values appear first). Most of the induced classes are due to incorrect senses. Thus, although suit was used in the WSJ articles only in the sense of &lt; legal_action &gt; , the algorithm not only considered the other senses as well (&lt; suit, suing &gt;,&lt; 3Resnik (1992) and Ribas (1994a) used equation 1 without introducing normalization. Therefore, the estimated function didn&apos;t accomplish probability axioms. Nevertheless, their results should be equivalent (for our purposes) to those introducing normalization because it shouldn&apos;t affect the relative ordering of Assoc among rival candidate classes for the same (v, s). 113 suii_of _clothes&gt;, &lt; suit&gt;) , but the Assoc score ranked them higher than the appropriate sense. We can also notice that the frAbs class, &lt;group&gt;, seems too general for the example nouns, while one of its daughters, &lt; people &gt; seems to fit t</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>P. S. Resnik. 1992. Wordnet and distributional analysis: A class-based approach to lexical discovery. In AAAI Symposium on Probabilistic Approaches to NL, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to lexical relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer and Information Science Department, University of Pennsylvania.</institution>
<contexts>
<context position="5719" citStr="Resnik, 1993" startWordPosition="888" endWordPosition="889">s). 2. p(clv, , s) scales up the strength of the association by the frequency of the relationship. Probabilities are estimated by Maximum Likelihood Estimation (MLE), i.e. counting the relative frequency of the considered events in the corpus2. However, it is not obvious how to calculate class frequencies when the training corpus is not semantically tagged as is the case. Nevertheless, we take a simplistic approach and calculate them in the following manner: f req(v, s, c) = freq(v,s,n) x w (1) riâ‚¬c 2The utility of introducing smoothing techniques on class-based distributions is dubious, see (Resnik, 1993). Where w is a constant factor used to normalize the probabilities&apos; = Evev EsEs EnEA, f req(v, s, n) w EsEs ErlE.Ar f req(v, s, n)Isenses(n)I (2) When creating the space of candidate classes (learning process, stage 1), we use a thresholdjug technique to ignore as much as possible the noise introduced in the training set. Specifically, we consider only those classes that have a higher number of occurrences than the threshold. The selection of the most appropriate classes (stage 3) is based on a global search through the candidates, in such a way that the final classes are mutually disjoint (no</context>
<context position="11173" citStr="Resnik (1993)" startWordPosition="1778" endWordPosition="1779"> ambiguous nouns could be more informative about the correct classes as long as they do not carry ambiguity. The weight introduced in (1) could alternatively be found in a local manner, in such a way that more polysemous nouns would give less evidence to each one of their senses than less ambiguous ones. Local weight could be obtained using p(cln). Nevertheless, a good estimation of this probability seems quite problematic because of the lack of tagged training material. In absence of a better estimator we use a rather poor one as the uniform distribution, isenses(n) E cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cove</context>
<context position="16204" citStr="Resnik, 1993" startWordPosition="2588" endWordPosition="2589">on called Abstraction Ratio). Hopefully, a technique with a higher abstraction ratio learns classes that fit the set of examples better. A manual assessment of the ratio confirmed this behavior, as testing sets with a lower ratio seemed to be inducing less 1lAbs cases. â€¢ Quantification of coverage It could be measured as the proportion of triples whose correct sense belongs to one of the SRs. 4.1.2 NLP evaluation tasks The NLP tasks where SRs utility could be evaluated are diverse. Some of them have already been introduced in section 1. In the recent literature aGrishman and Sterling, 1992), (Resnik, 1993), ...) several task oriented schemes to test Selectional Restrictions (mainly on syntactic ambiguity resolution) have been proposed. However, we have tested SRs on a WSS task, using the following scheme. For every triple in the testing set the algorithm selects as most appropriate that noun-sense that has as hyperonym the SR class with highest association score. When more than one sense belongs to the highest SR, a random selection is performed. When no SR has been acquired, the algorithm remains undecided. The results of this WSS procedure are checked against a testing-sample manually analyze</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. S. Resnik. 1993. Selection and Information: A Class-Based Approach to lexical relationships. Ph.D. thesis, Computer and Information Science Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ribas</author>
</authors>
<title>An experiment on learning appropriate selectional restrictions from a parsed corpus.</title>
<date>1994</date>
<booktitle>In COLING,</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="1770" citStr="Ribas (1994" startWordPosition="267" endWordPosition="268"> Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifications to the basic technique and will report the corresponding experimental evaluation. The outline of the paper is as follows: in section 2 we summarize the basic methodology used in (Ribas, 1994a), analyzing its limitations; in secti</context>
<context position="6371" citStr="Ribas (1994" startWordPosition="998" endWordPosition="999">rmalize the probabilities&apos; = Evev EsEs EnEA, f req(v, s, n) w EsEs ErlE.Ar f req(v, s, n)Isenses(n)I (2) When creating the space of candidate classes (learning process, stage 1), we use a thresholdjug technique to ignore as much as possible the noise introduced in the training set. Specifically, we consider only those classes that have a higher number of occurrences than the threshold. The selection of the most appropriate classes (stage 3) is based on a global search through the candidates, in such a way that the final classes are mutually disjoint (not related by hyperonymy). 2.2 Evaluation Ribas (1994a) reported experimental results obtained from the application of the above technique to learn SRs. He performed an evaluation of the Sits obtained from a training set of 870,000 words of the Wall Street Journal. In this section we summarize the results and conclusions reached in that paper. For instance, table 1 shows the SRs acquired for the subject position of the verb seek. Type indicates a manual diagnosis about the class appropriateness (Ok: correct; itAbs: over-generalization; Senses: due to erroneous senses). Assoc corresponds to the association score (higher values appear first). Most</context>
<context position="7916" citStr="Ribas (1994" startWordPosition="1248" endWordPosition="1249">ion didn&apos;t accomplish probability axioms. Nevertheless, their results should be equivalent (for our purposes) to those introducing normalization because it shouldn&apos;t affect the relative ordering of Assoc among rival candidate classes for the same (v, s). 113 suii_of _clothes&gt;, &lt; suit&gt;) , but the Assoc score ranked them higher than the appropriate sense. We can also notice that the frAbs class, &lt;group&gt;, seems too general for the example nouns, while one of its daughters, &lt; people &gt; seems to fit the data much better. Analyzing the results obtained from different experimental evaluation methods, Ribas (1994a) drew up some conclusions: a. The technique achieves a good coverage. b. Most of the classes acquired result from the accumulation of incorrect senses. c. No clear co-relation between Assoc and the manual diagnosis is found. d. A slight tendency to over-generalization exists due to incorrect senses. Although the performance of the presented technique seems to be quite good, we think that some of the detected flaws could possibly be addressed. Noise due to polysemy of the nouns involved seems to be the main obstacle for the practicality of the technique. It makes the association score prefer </context>
<context position="11354" citStr="Ribas, 1994" startWordPosition="1807" endWordPosition="1808">r, in such a way that more polysemous nouns would give less evidence to each one of their senses than less ambiguous ones. Local weight could be obtained using p(cln). Nevertheless, a good estimation of this probability seems quite problematic because of the lack of tagged training material. In absence of a better estimator we use a rather poor one as the uniform distribution, isenses(n) E cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measur</context>
<context position="14005" citStr="Ribas, 1994" startWordPosition="2230" endWordPosition="2231">the corresponding marginals. Relative entropy, D(P(Xlv_s)11P(X)), could do this job. 4 Evaluation 4.1 Evaluation methods of SRs Evaluation on NLP has been crucial to fostering research in particular areas. Evaluation of the SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet (e.g, section 4.2). It would also permit measuring the utility of the SRs obtained using WordNet in comparison with other frameworks using other kinds of knowledge. Finally it would be a powerful tool for detecting flaws of a particular technique (e.g, (Ribas, 1994a) analysis). However, a related and crucial issue is which linguistic tasks are used as a reference. SRs are useful for both lexicography and NLP. On the one hand, from the point of view of lexicography, the goal of evaluation would be to measure the quality of the SRs induced, (i.e., how well the resulting classes correspond to the nouns as they were used in the corpus). On the other hand, from the point of view of NLP, SRs should be evaluated on their utility (i.e., how much they help on performing the reference task). 4.1.1 Lexicography-oriented evaluation As far as lexicography (quality) </context>
</contexts>
<marker>Ribas, 1994</marker>
<rawString>F. Ribas. 1994a. An experiment on learning appropriate selectional restrictions from a parsed corpus. In COLING, Kyoto, Japan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ribas</author>
</authors>
<title>Learning more appropriate selectional restrictions.</title>
<date>1994</date>
<tech>Technical report, ESPRIT BRA-7315 ACQUILEX-II WP.</tech>
<contexts>
<context position="1770" citStr="Ribas (1994" startWordPosition="267" endWordPosition="268"> Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Departament d&apos;Ensenyament, Generalitat de Catalunya, 91-DOGC-1491. performed some experiments using this basic technique and drew up some limitations from the corresponding results. In this paper we will describe some substantial modifications to the basic technique and will report the corresponding experimental evaluation. The outline of the paper is as follows: in section 2 we summarize the basic methodology used in (Ribas, 1994a), analyzing its limitations; in secti</context>
<context position="6371" citStr="Ribas (1994" startWordPosition="998" endWordPosition="999">rmalize the probabilities&apos; = Evev EsEs EnEA, f req(v, s, n) w EsEs ErlE.Ar f req(v, s, n)Isenses(n)I (2) When creating the space of candidate classes (learning process, stage 1), we use a thresholdjug technique to ignore as much as possible the noise introduced in the training set. Specifically, we consider only those classes that have a higher number of occurrences than the threshold. The selection of the most appropriate classes (stage 3) is based on a global search through the candidates, in such a way that the final classes are mutually disjoint (not related by hyperonymy). 2.2 Evaluation Ribas (1994a) reported experimental results obtained from the application of the above technique to learn SRs. He performed an evaluation of the Sits obtained from a training set of 870,000 words of the Wall Street Journal. In this section we summarize the results and conclusions reached in that paper. For instance, table 1 shows the SRs acquired for the subject position of the verb seek. Type indicates a manual diagnosis about the class appropriateness (Ok: correct; itAbs: over-generalization; Senses: due to erroneous senses). Assoc corresponds to the association score (higher values appear first). Most</context>
<context position="7916" citStr="Ribas (1994" startWordPosition="1248" endWordPosition="1249">ion didn&apos;t accomplish probability axioms. Nevertheless, their results should be equivalent (for our purposes) to those introducing normalization because it shouldn&apos;t affect the relative ordering of Assoc among rival candidate classes for the same (v, s). 113 suii_of _clothes&gt;, &lt; suit&gt;) , but the Assoc score ranked them higher than the appropriate sense. We can also notice that the frAbs class, &lt;group&gt;, seems too general for the example nouns, while one of its daughters, &lt; people &gt; seems to fit the data much better. Analyzing the results obtained from different experimental evaluation methods, Ribas (1994a) drew up some conclusions: a. The technique achieves a good coverage. b. Most of the classes acquired result from the accumulation of incorrect senses. c. No clear co-relation between Assoc and the manual diagnosis is found. d. A slight tendency to over-generalization exists due to incorrect senses. Although the performance of the presented technique seems to be quite good, we think that some of the detected flaws could possibly be addressed. Noise due to polysemy of the nouns involved seems to be the main obstacle for the practicality of the technique. It makes the association score prefer </context>
<context position="11354" citStr="Ribas, 1994" startWordPosition="1807" endWordPosition="1808">r, in such a way that more polysemous nouns would give less evidence to each one of their senses than less ambiguous ones. Local weight could be obtained using p(cln). Nevertheless, a good estimation of this probability seems quite problematic because of the lack of tagged training material. In absence of a better estimator we use a rather poor one as the uniform distribution, isenses(n) E cl w(n, c) = Isenses(n) Resnik (1993) also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn&apos;t take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns. 3.3 Other statistical measures to score SRs In this section we propose the application of other measures apart from Assoc for learning SRs: loglikelihood ratio (Dunning, 1993), relative entropy (Cover and Thomas, 1991), mutual information ratio (Church and Hanks, 1990), 02 (Gale and Church, 1991). In section (4) their experimental evaluation is presented. The statistical measur</context>
<context position="14005" citStr="Ribas, 1994" startWordPosition="2230" endWordPosition="2231">the corresponding marginals. Relative entropy, D(P(Xlv_s)11P(X)), could do this job. 4 Evaluation 4.1 Evaluation methods of SRs Evaluation on NLP has been crucial to fostering research in particular areas. Evaluation of the SR learning task would provide grounds to compare different techniques that try to abstract SRs from corpus using WordNet (e.g, section 4.2). It would also permit measuring the utility of the SRs obtained using WordNet in comparison with other frameworks using other kinds of knowledge. Finally it would be a powerful tool for detecting flaws of a particular technique (e.g, (Ribas, 1994a) analysis). However, a related and crucial issue is which linguistic tasks are used as a reference. SRs are useful for both lexicography and NLP. On the one hand, from the point of view of lexicography, the goal of evaluation would be to measure the quality of the SRs induced, (i.e., how well the resulting classes correspond to the nouns as they were used in the corpus). On the other hand, from the point of view of NLP, SRs should be evaluated on their utility (i.e., how much they help on performing the reference task). 4.1.1 Lexicography-oriented evaluation As far as lexicography (quality) </context>
</contexts>
<marker>Ribas, 1994</marker>
<rawString>F. Ribas. 1994b. Learning more appropriate selectional restrictions. Technical report, ESPRIT BRA-7315 ACQUILEX-II WP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Whittemore</author>
<author>K Ferrara</author>
<author>H Brunner</author>
</authors>
<title>Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.</title>
<date>1990</date>
<booktitle>In Procs. ACL,</booktitle>
<location>Pennsylvania.</location>
<contexts>
<context position="1301" citStr="Whittemore et al., 1990" startWordPosition="189" endWordPosition="192">eling, computational lexicography 1 Introduction In recent years there has been a common agreement in the NLP research community on the importance of having an extensive coverage of selectional restrictions (SRs) tuned to the domain to work with. SRs can be seen as semantic type constraints that a word sense imposes on the words with which it combines in the process of semantic interpretation. SRs may have different applications in NLP, specifically, they may help a parser with Word Sense Selection (WSS, as in (Hirst, 1987)), with preferring certain structures out of several grammatical ones (Whittemore et al., 1990) and finally with deciding the semantic role played by a syntactic complement (Basili et al., 1992). Lexicography is also interested in the acquisition of SRs (both defining in context approach and lexical semantics work (Levin, 1992)). The aim of our work is to explore the feasibility of using an statistical method for extracting SRs from on-line corpora. Resnik (1992) developed a method for automatically extracting classbased SRs from on-line corpora. Ribas (1994a) This research has been made in the framework of the Acquilex-II Esprit Project (7315), and has been supported by a grant of Depa</context>
</contexts>
<marker>Whittemore, Ferrara, Brunner, 1990</marker>
<rawString>G. Whittemore, K. Ferrara, and H. Brunner. 1990. Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases. In Procs. ACL, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>The meaning-frequency relationship of words.</title>
<date>1945</date>
<journal>The Journal of General Psychology,</journal>
<pages>33--251</pages>
<contexts>
<context position="21145" citStr="Zipf, 1945" startWordPosition="3398" endWordPosition="3399">ions of Assoc (except the local normalization) get good results. Specially the two techniques that exploit a simpler prior distribution, which seem to improve the basic technique. 4. log-likelihood and D seem to get slightly worse results than Assoc techniques, although the results are very similar. 4.2.2 Thresholding We were also interested in measuring the impact of thresholding on the SRs acquired. In figure 1 we can see the different evaluation measures of the basic technique when varying the threshold. Precision and recall coincide when no candidate 4In some way, it conforms to Zipf-law (Zipf, 1945): noun frequency and polysemy are correlated. 116 Precision 4â€” Recall + Coverage -g.â€” Abstraction Ratio .x 0 5 10 15 20 Threshold Figure 1: Assoc: Evaluation ratios vs. Threshold classes are refused (threshold = 1). However, as it might be expected, as the threshold increases (i.e. some cases are not classified) the two ratios slightly diverge (precision increases and recall diminishes). Figure 1 also shows the impact of thresholding on coverage and abstraction ratios. Both decreaseâ€¢ when threshold increases, probably because when the rejecting threshold is low, small classes that fit the data</context>
</contexts>
<marker>Zipf, 1945</marker>
<rawString>G. K. Zipf. 1945. The meaning-frequency relationship of words. The Journal of General Psychology, 33:251-256.</rawString>
</citation>
<citation valid="false">
<title>Acquilex-II Working Papers can be obtained by sending a request to cide@cup. cam .</title>
<publisher>uk</publisher>
<marker></marker>
<rawString>(Acquilex-II Working Papers can be obtained by sending a request to cide@cup. cam . uk)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>