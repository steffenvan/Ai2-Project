<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000535">
<title confidence="0.963508">
A Correction Model for Word Alignments
</title>
<note confidence="0.6454625">
J. Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, Jian-ming Xu
IBM T.J. Watson Research Center
</note>
<address confidence="0.974665">
1101 Kitchawan Road, Rt. 134
Yorktown Heights, NY 10598
</address>
<email confidence="0.999194">
{jsmc,abei,roukos,bxiang,jianxu}@us.ibm.com
</email>
<sectionHeader confidence="0.996662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998327642857143">
Models of word alignment built as sequences
of links have limited expressive power, but are
easy to decode. Word aligners that model the
alignment matrix can express arbitrary align-
ments, but are difficult to decode. We pro-
pose an alignment matrix model as a cor-
rection algorithm to an underlying sequence-
based aligner. Then a greedy decoding al-
gorithm enables the full expressive power of
the alignment matrix formulation. Improved
alignment performance is shown for all nine
language pairs tested. The improved align-
ments also improved translation quality from
Chinese to English and English to Italian.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999385921568627">
Word-level alignments of parallel text are crucial for
enabling machine learning algorithms to fully uti-
lize parallel corpora as training data. Word align-
ments appear as hidden variables in IBM Models 1-
5 (Brown et al., 1993) in order to bridge a gap be-
tween the sentence-level granularity that is explicit
in the training data, and the implicit word-level cor-
respondence that is needed to statistically model lex-
ical ambiguity and word order rearrangements that
are inherent in the translation process. Other no-
table applications of word alignments include cross-
language projection of linguistic analyzers (such as
POS taggers and named entity detectors,) a subject
which continues to be of interest. (Yarowsky et al.,
2001), (Benajiba and Zitouni, 2010)
The structure of the alignment model is tightly
linked to the task of finding the optimal alignment.
Many alignment models are factorized in order to
use dynamic programming and beam search for ef-
ficient marginalization and search. Such a factoriza-
tion encourages - but does not require - a sequential
(often left-to-right) decoding order. If left-to-right
decoding is adopted (and exact dynamic program-
ming is intractable) important right context may ex-
ist beyond the search window. For example, the link-
age of an English determiner may be considered be-
fore the linkage of a distant head noun.
An alignment model that jointly models all of the
links in the entire sentence does not motivate a par-
ticular decoding order. It simply assigns comparable
scores to the alignment of the entire sentence, and
may be used to rescore the top-N hypotheses of an-
other aligner, or to decide whether heuristic pertur-
bations to the output of an existing aligner constitute
an improvement. Both the training and decoding of
full-sentence models have presented difficulties in
the past, and approximations are necessary.
In this paper, we will show that by using an ex-
isting alignment as a starting point, we can make a
significant improvement to the alignment by propos-
ing a series of heuristic perturbations. In effect, we
train a model to fix the errors of the existing aligner.
From any initial alignment configuration, these per-
turbations define a multitude of paths to the refer-
ence (gold) alignment. Our model learns alignment
moves that modify an initial alignment into the ref-
erence alignment. Furthermore, the resulting model
assigns a score to the alignment and thus could be
used in numerous rescoring algorithms, such as top-
N rescorers.
In particular, we use the maximum entropy frame-
</bodyText>
<page confidence="0.981915">
889
</page>
<note confidence="0.9587265">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 889–898,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.993612666666667">
work to choose alignment moves. The model is sym-
metric: source and target languages are interchange-
able. The alignment moves are sufficiently rich to
reach arbitrary phrase to phrase alignments. Since
most of the features in the model are not language-
specific, we are able to test the correction model
easily on nine language pairs; our corrections im-
proved the alignment quality compared to the input
alignments in all nine. We also tested the impact on
translation and found a 0.48 BLEU improvement on
Chinese to English and a 1.26 BLEU improvement
on English to Italian translation.
</bodyText>
<sectionHeader confidence="0.94367" genericHeader="method">
2 Alignment sequence models
</sectionHeader>
<bodyText confidence="0.999908475">
Sequence models are the traditional workhorse for
word alignment, appearing, for instance, in IBM
Models 1-5. This type of alignment model is not
symmetric; interchanging source and target lan-
guages results in a different aligner. This parameter-
ization does not allow a target word to be linked to
more than one source word, so some phrasal align-
ments are simply not considered. Often the choice of
directionality is motivated by this restriction, and the
choice of tokenization style may be designed (Lee,
2004) to reduce this problem. Nevertheless, aligners
that use this parameterization internally often incor-
porate various heuristics in order to augment their
output with the disallowed alignments - for example,
swapping source and target languages to obtain a
second alignment (Koehn et al., 2007) with different
limitations. Training both directions jointly (Liang
et al., 2006) and using posterior probabilities dur-
ing alignment prediction even allows the model to
see limited right context. Another alignment combi-
nation strategy (Deng and Zhou, 2009) directly op-
timizes the size of the phrase table of a target MT
system.
Generative models (such as Models 1-5, and the
HMM model (Vogel et al., 1996)) motivate a narra-
tive where alignments are selected left-to-right and
target words are then generated conditioned upon
the alignment and the source words. Generative
models are typically trained unsupervised, from par-
allel corpora without manually annotated word-level
alignments.
Discriminative models of alignment incorporate
source and target words, as well as more linguisti-
cally motivated features into the prediction of align-
ment. These models are trained from annotated
word alignments. Examples include the maximum
entropy model of (Ittycheriah and Roukos, 2005) or
the conditional random field jointly normalized over
the entire sequence of alignments of (Blunsom and
Cohn, 2006).
</bodyText>
<sectionHeader confidence="0.992857" genericHeader="method">
3 Joint Models
</sectionHeader>
<bodyText confidence="0.99261812">
An alternate parameterization of alignment is the
alignment matrix (Niehues and Vogel, 2008). For a
source sentence F consisting of words f1...fm, and
a target sentence E = e1...el, the alignment matrix
A = {Qij} is an l x m matrix of binary variables.
If Qij = 1, then ei is said to be linked to fj. If ei
is unlinked then Qij = 0 for all j. There is no con-
straint limiting the number of source tokens to which
a target word is linked either; thus the binary ma-
trix allows some alignments that cannot be modeled
by the sequence parameterization. All 2lm binary
matrices are potentially allowed in alignment matrix
models. For typical l and m, 2lm » (m + 1)l, the
number of alignments described by a comparable se-
quence model. This parameterization is symmetric -
if source and target are interchanged, then the align-
ment matrix is transposed.
A straightforward approach to the alignment ma-
trix is to build a log linear model (Liu et al., 2005)
for the probability of the alignment A. (We continue
to refer to “source” and “target” words only for con-
sistency of notation - alignment models such as this
are indifferent to the actual direction of translation.)
The log linear model for the alignment (Liu et al.,
2005) is
</bodyText>
<equation confidence="0.891735">
exp (Ei λiOi(A, E, F))
p(A|E, F) = (1)
Z(E, F)
</equation>
<bodyText confidence="0.945677857142857">
where the partition function (normalization) is given
by
�λiOi(A, E, F) . (2)
Here the Oi(A, E, F) are feature functions. The
model is parameterized by a set of weights λi, one
for each feature function. Feature functions are often
binary, but are not required to be. Feature functions
</bodyText>
<equation confidence="0.7741185">
Z(E, F) = � exp
A i
</equation>
<page confidence="0.979723">
890
</page>
<bodyText confidence="0.999475727272727">
may depend upon any number of components Qij of
the alignment matrix A.
The sum over all alignments of a sentence pair
(2lm terms) in the partition function is computa-
tionally impractical except for very short sentences,
and is rarely amenable to dynamic programming.
Thus the partition function is replaced by an ap-
proximation. For example, the sum over all align-
ments may be restricted to a sum over the n-best
list from other aligners (Liu et al., 2005). This ap-
proximation was found to be inconsistent for small
n unless the merged results of several aligners were
used. Alternately, loopy belief propagation tech-
niques were used in (Niehues and Vogel, 2008).
Loopy belief propagation is not guaranteed to con-
verge, and feature design is influenced by consider-
ation of the loops created by the features. Outside
of the maximum entropy framework, similar models
have been trained using maximum weighted bipar-
tite graph matching (Taskar et al., 2005), averaged
perceptron (Moore, 2005), (Moore et al., 2006), and
transformation-based learning (Ayan et al., 2005).
</bodyText>
<sectionHeader confidence="0.995868" genericHeader="method">
4 Alignment Correction Model
</sectionHeader>
<bodyText confidence="0.995533869565217">
In this section we describe a novel approach to word
alignment, in which we train a log linear (maximum
entropy) model of alignment by viewing it as correc-
tion model that fixes the errors of an existing aligner.
We assume a priori that the aligner will start from
an existing alignment of reasonable quality, and will
attempt to apply a series of small changes to that
alignment in order to correct it. The aligner naturally
consists of a move generator and a move selector.
The move generator perturbs an existing align-
ment A in order to create a set of candidate align-
ments Mt(A), all of which are nearby to A in the
space of alignments. We index the set of moves by
the decoding step t to indicate that we generate en-
tirely different (even non-overlapping) sets of moves
at different steps t of the alignment prediction. Typ-
ically the moves affect linkages local to a particular
word, e.g. the t’th source word.
The move selector then chooses one of the align-
ments At+1 E Mt(At), and proceeds iteratively:
At+2 E Mt+1(At+1), etc. until suitable termina-
tion criteria are reached. Pseudocode is depicted in
Fig. (1.) In practice, one move for each source and
</bodyText>
<figure confidence="0.974548888888889">
Input: sentence pair E1 .. El, F1 .. Fm
Input: alignment A
Output: improved alignment Afinal
for t = 1 → l do
generate moves: Mt(At)
select move:
At+1 ← argmaxA∈Mt(At)p(AjAt, E, F)
Afinal ← Al+1
{repeat for source words}
</figure>
<figureCaption confidence="0.999956">
Figure 1: pseudocode for alignment correction
</figureCaption>
<bodyText confidence="0.89025">
target word is sufficient.
</bodyText>
<subsectionHeader confidence="0.993522">
4.1 Move generation
</subsectionHeader>
<bodyText confidence="0.999910612903226">
Many different types of alignment perturbations are
possible. Here we restrict ourselves to a very sim-
ple move generator that changes the linkage of ex-
actly one source word at a time, or exactly one target
word at a time. Many of our corrections are simi-
lar to those of (Setiawan et al., 2010), although our
motivation is perhaps closer to (Brown et al., 1993),
who used similar perturbations to approximate in-
tractable sums that arise when estimating the param-
eters of the generative models Models 3-5, and ap-
proach refined in (Och and Ney, 2003). We note that
our corrections are designed to improve even a high-
quality starting alignment; in contrast the model of
(Fossum et al., 2008) considers deletion of links
from an initial alignment (union of aligners) that is
likely to overproduce links.
From the point of view of the alignment ma-
trix, we consider changes to one row or one col-
umn (generically, one slice) of the alignment matrix.
At each step t, the move set Mt(At) is formed by
choosing a slice of the current alignment matrix At,
and generating all possible alignments from a few
families of moves. Then the move generator picks
another slice and repeats. The m + l slices are cy-
cled in a fixed order: the first m slices correspond to
source words (ordered according to a heuristic top-
down traversal of the dependency parse tree if avail-
able), and the remaining l slices correspond to target
words, similarly parse-ordered. For each slice we
consider the following families of moves, illustrated
by rows.
</bodyText>
<listItem confidence="0.907855">
• add link to row i - for one j such that Qij = 0,
</listItem>
<page confidence="0.963854">
891
</page>
<bodyText confidence="0.705046">
where the partition function is now given by
</bodyText>
<equation confidence="0.415024666666667">
1: P
Z(E, F, M) = e i λiφi(A,E,F) (4)
AEM
</equation>
<bodyText confidence="0.992473294117647">
and At+1 ∈ Mt(At) is required for correct normal-
ization. This equation is notationally very similar
to equation (1), except that the predictions of the
model are restricted to a small set of nearby align-
ments. For the move generator considered in this pa-
per, the summation in Eq.(4) is similarly restricted,
and hence training the model is tractable. The set
of candidate alignments Mt(At) typically does not
contain the reference (gold) alignment; we model
the best alignment among a finite set of alternatives,
rather than the correct alignment from among all
possible alignments. This is a key difference be-
tween our model and (Liu et al., 2005).
Note that if we extended our definition of pertur-
bation to the limiting case that the alignment set in-
cluded all possible alignments then we would clearly
recover the standard log linear model of alignment.
</bodyText>
<subsectionHeader confidence="0.966244">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.999946388888889">
Since the model is designed to predict perturbation
to an alignment, it is trained from a collection of
errorful alignments and corresponding reference se-
quences of aligner moves that reach the reference
(gold) alignment. We construct a training set from
a collection of sentence pairs and reference align-
ments for training (A*n, En, Fn)Nn=1, as well as col-
lections of corresponding “first pass” alignments An 1
produced by another aligner. For each n, we form a
number of candidate alignment sets Mt(Ant ), one
for each source and target word. For training pur-
poses, the true alignment from the set is taken to be
the one identical with A*n in the slice targeted by the
move generator at the current step. (A small number
of move sets do not have an exact match and are dis-
carded.) Then we form an objective function from
the log likelihood of reference alignment, smoothed
with a gaussian prior
</bodyText>
<equation confidence="0.8701085">
L =1: Ln +1: (λi/&apos;Y)2 (5)
n i
</equation>
<bodyText confidence="0.368056">
make σij = 1 (shown here for row i = 1.)
</bodyText>
<figure confidence="0.941313">
α 0 &apos;Y α 0 &apos;Y
a ◦ ◦ ◦ =⇒ a • ◦ ◦
b ◦ • ◦ b ◦ • ◦
c ◦ ◦ ◦ c ◦ ◦ ◦
• remove one or more links from row i - for some
j such that σij = 1, make σij = 0 (shown here
for i = 3.)
α 0 &apos;Y α 0 &apos;Y
a • ◦ ◦ =⇒ a • ◦ ◦
b ◦ • ◦ b ◦ • ◦
c ◦ ◦ • c ◦ ◦ ◦
• move a link in row i - for one j and one j&apos; such
that σij = 1 and σij, = 0, make σij = 0 and
σij, = 1 (shown here for i = 1.)
α 0 &apos;Y α 0 &apos;Y
a ◦ • ◦ =⇒ a • ◦ ◦
b ◦ • ◦ b ◦ • ◦
c ◦ ◦ ◦ c ◦ ◦ ◦
• leave row i unchanged
</figure>
<bodyText confidence="0.999332090909091">
Similar families of moves apply to column slices
(source words.) In practice, perturbations are re-
stricted by a window (typically ±5 from existing
links.) If the given source word is unlinked, we
consider adding a link to each target word in a win-
dow (±5 from nearby links.) The window size re-
strictions mean that some reference alignments are
not reachable from the starting point. However, this
is unlikely to limit performance - an oracle aligner
achieves 97.6%F-measure on the Arabic-English
training set.
</bodyText>
<subsectionHeader confidence="0.993479">
4.2 Move selection
</subsectionHeader>
<bodyText confidence="0.999697">
A log linear model for the selection of the candidate
alignment at t+1 from the set of alignments Mt(At)
generated by the move generator at step t takes the
form:
</bodyText>
<equation confidence="0.987337">
P
e i λiφi(At+1,E,F )
p(At+1|E, F, Mt(At)) = Z(E, F, Mt(At)) (3)
</equation>
<page confidence="0.967252">
892
</page>
<bodyText confidence="0.936184">
where the likelihood of each training sample is
�Ln = log p1(A0n|E, Fn; M(fα, A0n, E, Fn))
α
determiner links to the same word as its head noun.
As an example, if ei is the headword of ei,, and fj is
the headword of fj,, then
</bodyText>
<table confidence="0.546496">
� log p1(A0n|E,Fn;M(eβ,A0n,E,Fn)) (6) � σijσi/j/ (8)
+ φ(A, E, F) =
β ij
</table>
<bodyText confidence="0.999634714285714">
The likelihood has a term for each sentence pair
and for each decoder step. The model is trained
by gradient ascent using the l-BFGS method (Liu
and Nocedal, 1989), which has been successfully
used for training log linear models (Blunsom and
Cohn, 2006) in many natural language tasks, includ-
ing alignment.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999801037037037">
A wide variety of features were used in the model.
We group the features in three broad categories:
link-based, geometrical, and parse-based.
Link-based features are those which decompose
into a (linear) sum of alignment matrix elements σij.
An example link-based feature is one that fires if a
source language noun is linked to a target language
determiner. Note that this feature may fire more than
once in a given sentence pair: as with most fea-
tures in our model, it is an integer-valued feature
that counts the number of times a structure appears
in a sentence pair. These features do not capture any
correlation between different σij. Among the link-
based features are those based on Model 1 transla-
tion matrix parameters τ(ei|fj) and τ(fj|ei). We
bin the model 1 parameters, and form integer-valued
features for each bin that count the number of links
with τ0 &lt; τ(ei|fj) &lt; τ1.
Geometrical features are those which capture cor-
relation between different σij based on adjacency or
nearness. They capture the idea that nearby words
in one language link to nearby words in the other
language - the motivation of HMM-based models
of alignment. An example is a feature that counts
the number of times that the next word in the source
language is linked to the next word in the target lan-
guage:
</bodyText>
<equation confidence="0.897201">
φ(A, E, F) = � σijσi+1,j+1 (7)
ij
</equation>
<bodyText confidence="0.999882375">
Parse-based features are those which capture cor-
relation between different σij, but use parsing to de-
termine links which are correlated - for example, if a
counts the number of times that a dependency rela-
tion in one language is preserved by alignment in the
other language. This feature can also be decorated,
either lexically, or with part-of-speech tags (as many
features in all three categories are.)
</bodyText>
<subsectionHeader confidence="0.983522">
5.1 Unsupervised Adaptation
</subsectionHeader>
<bodyText confidence="0.999927384615385">
We constructed a heuristic phrase dictionary for un-
supervised adapatation. After aligning a large unan-
notated parallel corpus with our aligner, we enumer-
ate fully lexicalized geometrical features that can be
extracted from the resulting alignments - these are
entries in a phrase dictionary. These features are
tied, and treated as a single real-valued feature that
fires during training and decoding phases if a set of
hypothesized links matches the geometrical feature
extracted from the unannotated data. The value of
this real-valued feature is the log of the number of
occurrences of the identical (lexicalized) geometri-
cal feature in the aligned unannotated corpus.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999975">
We design our experiments to validate that a cor-
rection model using simple features, mostly non-
language-specific, can improve the alignment accu-
racy of a variety of existing aligners for a variety of
language pairs; we do not attempt to exactly match
features between comparison aligners - this is un-
likely to lead to a robust correction model.
</bodyText>
<subsectionHeader confidence="0.999558">
6.1 Arabic-English alignment results
</subsectionHeader>
<bodyText confidence="0.9999773">
We trained the Arabic-English alignment system
on 5125 sentences from Arabic-English treebanks
(LDC2008E61, LDC2008E22) that had been an-
notated for word alignment. Reference parses
were used during the training. Results are mea-
sured on a 500 sentence test set, sampled from
a wide variety of parallel corpora, including vari-
ous genres. During alignment, only automatically-
generated parses (based on the parser of (Rat-
naparkhi, 1999)) were available. Alignments on
</bodyText>
<page confidence="0.998192">
893
</page>
<table confidence="0.997387636363636">
initial align correction model R (%) P (%) F(%) AF
GIZA++ 76 76 76
corr(GIZA++) 86 94 90 14*
corr(ME-seq) 88 92 90 14*
HMM 73 73 73
corr(HMM) 87 92 89 16*
corr(ME-seq) 87 93 90 17*
ME-seq 82 84 83
corr(HMM) 88 92 90 7*
corr(GIZA++) 87 94 91 8*
corr(ME-seq) 89 94 91 8*
</table>
<tableCaption confidence="0.9954405">
Table 1: Alignment accuracy for Arabic-English systems in percentage recall (R), precision(P), and F-measure. *
denotes statistical significance (see text.)
</tableCaption>
<table confidence="0.9983705">
lang method R (%) P(%) F(%) AF
ZH—+EN GIZA++ 55 67 61
ME-seq 66 72 69
corr(ME-seq) 74 76 75 6*
</table>
<tableCaption confidence="0.960772">
Table 2: Alignment accuracy for Chinese(ZH)-English(EN) systems. * denotes statistical significance
</tableCaption>
<table confidence="0.999866473684211">
lang aligner R(%) P(%) F(%) AF
IT—+ EN ME-seq 74 87 80
corr(ME-seq) 84 92 88 8*
EN—+IT ME-seq 75 86 80
corr(ME-seq) 84 92 88 8*
PT—+EN ME-seq 77 83 80
corr(ME-seq) 87 91 89 9†
EN—+PT ME-seq 79 87 83
corr(ME-seq) 88 90 89 6†
JA—+EN ME-seq 72 78 75
corr(ME-seq) 77 83 80 5*
RU—+EN ME-seq 81 85 83
corr(ME-seq) 82 92 87 4*
DE—+EN ME-seq 77 82 79
corr(ME-seq) 78 87 82 3*
ES—+EN ME-seq 93 86 90
corr(ME-seq) 92 88 90 0.6
FR—+EN ME-seq 89 91 90
corr(ME-seq) 88 92 90 0.1
</table>
<tableCaption confidence="0.995959">
Table 3: Alignment accuracy for additional languages. * denotes statistical significance; † statistical significance not
available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French
</tableCaption>
<page confidence="0.998731">
894
</page>
<bodyText confidence="0.999985542857143">
the training and test sets were decoded with three
other aligners, so that the robustness of the cor-
rection model to different input alignments could
be validated. The three aligners were GIZA++
(Och and Ney, 2003) (with the MOSES (Koehn
et al., 2007) postprocessing option -alignment
grow-diag-final-and) the posterior HMM
aligner of (Ge, 2004), a maximum entropy sequen-
tial model (ME-seq) (Ittycheriah and Roukos, 2005).
ME-seq is our primary point of comparison: it is
discriminatively trained (on the same training data,)
uses a rich set of features, and provides the best
alignments of the three. Three correction models
were trained: corr(GIZA++) is trained to correct
the alignments produced by GIZA++, corr(HMM)
is trained to correct the alignments produced by the
HMM aligner, and corr(ME-seq) is trained to correct
the alignments produced by the ME-seq model.
In Table (1) we show results for our system cor-
recting each of the aligners as measured in the usual
recall, precision, and F-measure.1 The resulting
improvements in F-measure of the alignments pro-
duced by our models over their corresponding base-
lines is statistically significant (p &lt; 10−4, indicated
by a ∗.) Statistical significance is tested by a Monte
Carlo bootstrap (Efron and Tibshirani, 1986) - sam-
pling with replacement the difference in F-measure
of the two system’s alignments of the same sentence
pair. Both recall and precision are improved, but the
improvement in precision is somewhat larger. We
also show cross-condition results in which a correc-
tion model trained to correct HMM alignments is ap-
plied to correct ME-seq alignments. These results
show that our correction model is robust to different
starting aligners.
</bodyText>
<subsectionHeader confidence="0.999757">
6.2 Chinese-English alignment results
</subsectionHeader>
<bodyText confidence="0.999867375">
Table (2) presents results for Chinese-English word
alignments. The training set for the corr(ME-
seq) model consisted of approximately 8000 hand-
aligned sentences sampled from LDC2006E93 and
LDC2008E57. The model was trained to correct
the output of the ME-seq aligner, and tested on
the same condition. For this language pair, refer-
ence parses were not available in our training set, so
</bodyText>
<footnote confidence="0.856011333333333">
1We do not distinguish sure and possible links in our anno-
tations - under this circumstance, alignment error rate(Och and
Ney, 2003) is 1 − F.
</footnote>
<bodyText confidence="0.999717777777778">
automatically-generated parses were used for both
training and test sets. Results are measured on a 512
sentence test set, sampled from a wide variety of par-
allel corpora of various genres. We compare perfor-
mance with GIZA++, and with the ME-seq aligner.
Again the resulting improvement over the ME-seq
aligner is statistically significant. However, here the
improvement in recall is somewhat larger than the
improvement in precision.
</bodyText>
<subsectionHeader confidence="0.99963">
6.3 Additional language pairs
</subsectionHeader>
<bodyText confidence="0.999446333333333">
Table (3) presents alignment results for seven other
language pairs. Separate alignment corrector mod-
els were trained for both directions of Italian ↔
English and Portuguese ↔ English. The training
and test data vary by language, and are sampled
uniformly from a diverse set of corpora of various
genres, including newswire, and technical manuals.
Manual alignments for training and test data were
annotated. We compare performance with the ME-
seq aligner trained on the same training data. As
with the Chinese results above, customization and
feature development for the language pairs was min-
imal. In general, machine parses were always avail-
able for the English half of the pair. Machine parses
were also available for French and Spanish. Ma-
chine part of speech tags were available for all lan-
guage (although character-based heuristic was sub-
stituted for Japanese.) Large amounts (up to 10 mil-
lion sentence pairs) of unaligned parallel text was
available for model 1 type features. Our model ob-
tained improved alignment F-measure in all lan-
guage pairs, although the improvements were small
for ES→EN and FR→EN, the language pairs for
which the baseline accuracy was the highest.
</bodyText>
<subsectionHeader confidence="0.998391">
6.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999914272727273">
Some of the improvement can be attributed to “look-
ahead” during the decoding. For example, the
English word “the”, which (during Arabic-English
alignment) should often be aligned to the same Ara-
bic words to which its headword is linked. The num-
ber of errors associated with “the” dropped from 383
(186 false alarms, 197 misses) in the ME-seq model
to 137 (60 false alarms and 77 misses) in the current
model.
In table 5, we show contributions to performance
resulting from various classes of features. The
</bodyText>
<page confidence="0.996542">
895
</page>
<table confidence="0.999528333333333">
Zh-En Ar-En
method correct miss fa correct miss fa
hmm 147 256 300
GIZA++ 139 677 396 132 271 370
ME-seq 71 745 133 127 276 191
corr(ME-seq) 358 458 231 264 139 114
</table>
<tableCaption confidence="0.99974">
Table 4: Analysis of 2−1 alignments errors (misses and false alarms) for Zh-En and Ar-En aligners
</tableCaption>
<bodyText confidence="0.999839518518518">
largest contribution is noted by removing features
based on the Model 1 translation matrices. These
features contain a wealth of lexical information
learned from approximately 7 x 106 parallel sen-
tences - information that cannot be learned from
a relatively small amount of word-aligned train-
ing data. Geometrical features contribute more
than parse-based features, but the contribution from
parse-based features is important, and these are
more difficult to incorporate into sequential mod-
els. We note that all of the comparison aligners had
equivalent lexical information.
We show a small improvement from the unsuper-
vised adaptation - learning phrases from the parallel
corpus that are not captured by the lexical features
based on model 1. The final row in the table shows
the result of running the correction model on its own
output. The improvement is not statistically signif-
icant, but it is important to note the performance is
stable - a further indication that the model is robust
to a wide variety of input alignments, and that our
decoding scheme is a reasonable approach to find-
ing the best alignment.
In table 4, we characterize the errors based on the
fertility of the source and target words. We focus
on the case that exactly one target word is linked to
exactly two source words. These are the links that
</bodyText>
<table confidence="0.927448714285714">
feature R(%) P(%) F(%) Nexact
base 89 94 91 136
base-M1 82 88 85 89
base-geometric 83 90 86 92
base-parse 87 93 90 116
base+un.adapt 89 94 92 141
+iter2 90 94 92 141
</table>
<tableCaption confidence="0.974671">
Table 5: Importance of feature classes - ablation experi-
ments
</tableCaption>
<table confidence="0.9988232">
corpus-level p90
alignment TER BLEU TER BLEU
ME-seq 56.06 32.65 64.20 21.31
corr(Me-seq) 56.25 33.10 63.47 22.02
both 56.07 33.13 63.41 22.14
</table>
<tableCaption confidence="0.987568">
Table 6: Translation results, Zh to En. BLEU=BLEUr4n4
</tableCaption>
<table confidence="0.991267">
alignment TER BLEUr1n4
ME-seq 35.02 69.94
corr(Me-seq) 33.10 71.20
</table>
<tableCaption confidence="0.999658">
Table 7: Translation results, En to It
</tableCaption>
<bodyText confidence="0.999957947368421">
are poorly suited for the HMM and ME-seq mod-
els used in this comparison because of the chosen
directionality: the source (Arabic, Chinese) words
are the states and the target (English) words are the
observation. The HMM is able to produce these
links only by the use of posterior probabilities, rather
than viterbi decoding. The ME-seq model only pro-
duces these links because of language-specific post-
processing. GIZA++ has an underlying sequential
model, but uses both directionalities. The correc-
tion model improved performance across all three of
these links structures. The single exception is that
the number of 2−1 false alarms increased (Zh-En
alignments) but in this case, the first pass ME-seq
alignment produced few false alarms because it sim-
ply proposed few links of this form. It is also notable
that 1−2 links are more numerous than 2−1 links,
in both language pairs. This is consequence of the
choice of directionality and tokenization style.
</bodyText>
<subsectionHeader confidence="0.96492">
6.5 Translation Impact
</subsectionHeader>
<bodyText confidence="0.999729666666667">
We tested the impact of improved alignments on
the performance of a phrase-based translation sys-
tem (Ittycheriah and Roukos, 2007) for three lan-
</bodyText>
<page confidence="0.995943">
896
</page>
<bodyText confidence="0.999989684210527">
guage pairs. Our alignment did not improve the
performance of a mature Arabic to English trans-
lation system, but two notable successes were ob-
tained: Chinese to English, and English to Italian.
It is well known that improved alignment perfor-
mance does not always improve translation perfor-
mance (Fraser and Marcu, 2007). A mature machine
translation system may incorporate alignments ob-
tained from multiple aligners, or from both direc-
tions of an asymmetric aligner. Furthermore, with
large amounts of training data (the Gale Phase 4
Arabic English corpus consisting of 8 × 106 sen-
tences,) a machine translation system is subject to
a saturation effect: correcting an alignment may
not yield a significant improvement because the the
phrases learned from the correct alignment have al-
ready been acquired in other contexts.
For the Chinese to English translation system (ta-
ble 6) the training corpus consisted of 11 × 106 sen-
tence pairs, subsampled to 106. The test set was
NIST MT08 Newswire, consisting of 691 sentences
and 4 reference translations. Corpus-level perfor-
mance (columns 2 and 3) improved when measured
by BLEU, but not by TER. Performance on the
most difficult sentences (near the 90th percentile,
columns 4 and 5) improved on both BLEU and TER
(Snover et al., 2006), and the improvement in BLEU
was larger for the more difficult sentences than it
was overall. Translation performance further im-
proved, by a smaller amount, using both ME-seq and
corr(ME-seq) alignments during the training.
The improved alignments impacted the transla-
tion performance of the English to Italian transla-
tion system (table 7) even more strongly. Here the
training corpus consisted of 9.4×106 sentence pairs,
subsampled to 387000 pairs. The test set consisted
of 7899 sentences. Overall performance improved
as measured by both TER and BLEU (1.26 points.)
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999988789473684">
A log linear model for the alignment matrix is used
to guide systematic improvements to an existing
aligner. Our system models arbitrary alignment ma-
trices and allows features that incorporate such in-
formation as correlations based on parse trees in
both languages. We train models to correct the er-
rors of several existing aligners; we find the resulting
models are robust to using different aligners as start-
ing points. Improvements in alignment F-measure,
often significant improvements, show that our model
successfully corrects input alignments from existing
models in all nine language pairs tested. The result-
ing Chinese-English and English-Italian word align-
ments also improved translation performance, espe-
cially on the English-Italian test, and notably on the
particularly difficult subset of the Chinese sentences.
Future work will assess its impact on translation for
the other language pairs, as well as its impact on
other tasks, such as named entity projection.
</bodyText>
<sectionHeader confidence="0.996906" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999873111111111">
We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-
ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.
</bodyText>
<sectionHeader confidence="0.997136" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99793852173913">
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Alignment link projection using transformation-
based learning. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages 185–
192, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yassine Benajiba and Imed Zitouni. 2010. Enhanc-
ing mention detection using projection via aligned
corpora. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’10, pages 993–1001. Association for Com-
putational Linguistics.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In In
Proc. of ACL-2006, pages 65–72.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the ACL-IJCNLP 2009 Conference
</reference>
<page confidence="0.981055">
897
</page>
<reference confidence="0.999535896907217">
Short Papers, ACLShort ’09, pages 229–232, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
B. Efron and R. Tibshirani. 1986. Bootstrap meth-
ods for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1(1):pp. 54–75.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment precision for
syntax-based machine translation. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, StatMT ’08, pages 44–52. Association for Com-
putational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33(3):293–303.
Niyu Ge. 2004. Improvement in word alignments. In
DARPA/TIDES MT workshop.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT-EMNLP, pages 89–96.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Human Language Technolo-
gies 2007: The Conference of the NA-ACL, pages 57–
64, Rochester, New York, April. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004: Short Papers on XX, HLT-NAACL ’04,
pages 57–60. Association for Computational Linguis-
tics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 104–111. Associa-
tion for Computational Linguistics.
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimization.
Mathematical Programming, 45:503–528.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In ACL ’05: Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 459–466. Association for
Computational Linguistics.
Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved discriminative bilingual word alignment. In
ACL-44: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 513–520. Association for Computa-
tional Linguistics.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In In Proceedings of HLT-
EMNLP, pages 81–88.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18–25, Columbus, Ohio,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Adwait Ratnaparkhi. 1999. Learning to parse natu-
ral language with maximum entropy models. Mach.
Learn., 34:151–175, February.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’10, pages 534–544. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings ofAssociation for Machine Translation in
the Americas.
Ben Taskar, Simon Lacoste-julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In In Proceedings of HLT-EMNLP, pages 73–
80.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics, pages 836–841.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of the first international conference on Human lan-
guage technology research, HLT ’01, pages 1–8. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.997909">
898
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.255928">
<title confidence="0.999963">A Correction Model for Word Alignments</title>
<author confidence="0.9986">J Scott McCarley</author>
<author confidence="0.9986">Abraham Ittycheriah</author>
<author confidence="0.9986">Salim Roukos</author>
<author confidence="0.9986">Bing Xiang</author>
<author confidence="0.9986">Jian-ming</author>
<affiliation confidence="0.996713">IBM T.J. Watson Research</affiliation>
<address confidence="0.870724">1101 Kitchawan Road, Rt. Yorktown Heights, NY</address>
<abstract confidence="0.978124">Models of word alignment built as sequences of links have limited expressive power, but are easy to decode. Word aligners that model the alignment matrix can express arbitrary alignments, but are difficult to decode. We propose an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from</abstract>
<note confidence="0.636394">Chinese to English and English to Italian.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
<author>Christof Monz</author>
</authors>
<title>Alignment link projection using transformationbased learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>185--192</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8789" citStr="Ayan et al., 2005" startWordPosition="1419" endWordPosition="1422">al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generator perturbs an existing alignment A in order to create a set of candidate alig</context>
</contexts>
<marker>Ayan, Dorr, Monz, 2005</marker>
<rawString>Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz. 2005. Alignment link projection using transformationbased learning. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 185– 192, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yassine Benajiba</author>
<author>Imed Zitouni</author>
</authors>
<title>Enhancing mention detection using projection via aligned corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>993--1001</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1636" citStr="Benajiba and Zitouni, 2010" startWordPosition="246" endWordPosition="249">training data. Word alignments appear as hidden variables in IBM Models 1- 5 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of finding the optimal alignment. Many alignment models are factorized in order to use dynamic programming and beam search for efficient marginalization and search. Such a factorization encourages - but does not require - a sequential (often left-to-right) decoding order. If left-to-right decoding is adopted (and exact dynamic programming is intractable) important right context may exist beyond the search window. For example, the linkage of an English determiner may be considered before the linkage of a distant head noun. An a</context>
</contexts>
<marker>Benajiba, Zitouni, 2010</marker>
<rawString>Yassine Benajiba and Imed Zitouni. 2010. Enhancing mention detection using projection via aligned corpora. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 993–1001. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative word alignment with conditional random fields. In</title>
<date>2006</date>
<booktitle>In Proc. of ACL-2006,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="6130" citStr="Blunsom and Cohn, 2006" startWordPosition="955" endWordPosition="958"> and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguistically motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1...fm, and a target sentence E = e1...el, the alignment matrix A = {Qij} is an l x m matrix of binary variables. If Qij = 1, then ei is said to be linked to fj. If ei is unlinked then Qij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are potentia</context>
<context position="15560" citStr="Blunsom and Cohn, 2006" startWordPosition="2695" endWordPosition="2698">: P e i λiφi(At+1,E,F ) p(At+1|E, F, Mt(At)) = Z(E, F, Mt(At)) (3) 892 where the likelihood of each training sample is �Ln = log p1(A0n|E, Fn; M(fα, A0n, E, Fn)) α determiner links to the same word as its head noun. As an example, if ei is the headword of ei,, and fj is the headword of fj,, then � log p1(A0n|E,Fn;M(eβ,A0n,E,Fn)) (6) � σijσi/j/ (8) + φ(A, E, F) = β ij The likelihood has a term for each sentence pair and for each decoder step. The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. 5 Features A wide variety of features were used in the model. We group the features in three broad categories: link-based, geometrical, and parse-based. Link-based features are those which decompose into a (linear) sum of alignment matrix elements σij. An example link-based feature is one that fires if a source language noun is linked to a target language determiner. Note that this feature may fire more than once in a given sentence pair: as with most features in our model, it is an integer-valued feature that counts the number of times a s</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative word alignment with conditional random fields. In In Proc. of ACL-2006, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematic of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1106" citStr="Brown et al., 1993" startWordPosition="164" endWordPosition="167">an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian. 1 Introduction Word-level alignments of parallel text are crucial for enabling machine learning algorithms to fully utilize parallel corpora as training data. Word alignments appear as hidden variables in IBM Models 1- 5 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of</context>
<context position="10654" citStr="Brown et al., 1993" startWordPosition="1746" endWordPosition="1749">t A Output: improved alignment Afinal for t = 1 → l do generate moves: Mt(At) select move: At+1 ← argmaxA∈Mt(At)p(AjAt, E, F) Afinal ← Al+1 {repeat for source words} Figure 1: pseudocode for alignment correction target word is sufficient. 4.1 Move generation Many different types of alignment perturbations are possible. Here we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (generically, one slice) of the alignment matrix. At each step t, th</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematic of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>Bowen Zhou</author>
</authors>
<title>Optimizing word alignment combination for phrase table training.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference</booktitle>
<contexts>
<context position="5292" citStr="Deng and Zhou, 2009" startWordPosition="829" endWordPosition="832">ction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguistically motivated features into the prediction of alignment. These models </context>
</contexts>
<marker>Deng, Zhou, 2009</marker>
<rawString>Yonggang Deng and Bowen Zhou. 2009. Optimizing word alignment combination for phrase table training. In Proceedings of the ACL-IJCNLP 2009 Conference</rawString>
</citation>
<citation valid="false">
<authors>
<author>Short Papers</author>
</authors>
<title>ACLShort ’09,</title>
<pages>229--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Papers, </marker>
<rawString>Short Papers, ACLShort ’09, pages 229–232, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy.</title>
<date>1986</date>
<journal>Statistical Science,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>54--75</pages>
<contexts>
<context position="21496" citStr="Efron and Tibshirani, 1986" startWordPosition="3672" endWordPosition="3675">(GIZA++) is trained to correct the alignments produced by GIZA++, corr(HMM) is trained to correct the alignments produced by the HMM aligner, and corr(ME-seq) is trained to correct the alignments produced by the ME-seq model. In Table (1) we show results for our system correcting each of the aligners as measured in the usual recall, precision, and F-measure.1 The resulting improvements in F-measure of the alignments produced by our models over their corresponding baselines is statistically significant (p &lt; 10−4, indicated by a ∗.) Statistical significance is tested by a Monte Carlo bootstrap (Efron and Tibshirani, 1986) - sampling with replacement the difference in F-measure of the two system’s alignments of the same sentence pair. Both recall and precision are improved, but the improvement in precision is somewhat larger. We also show cross-condition results in which a correction model trained to correct HMM alignments is applied to correct ME-seq alignments. These results show that our correction model is robust to different starting aligners. 6.2 Chinese-English alignment results Table (2) presents results for Chinese-English word alignments. The training set for the corr(MEseq) model consisted of approxi</context>
</contexts>
<marker>Efron, Tibshirani, 1986</marker>
<rawString>B. Efron and R. Tibshirani. 1986. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical Science, 1(1):pp. 54–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment precision for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08,</booktitle>
<pages>44--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10981" citStr="Fossum et al., 2008" startWordPosition="1801" endWordPosition="1804">we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (generically, one slice) of the alignment matrix. At each step t, the move set Mt(At) is formed by choosing a slice of the current alignment matrix At, and generating all possible alignments from a few families of moves. Then the move generator picks another slice and repeats. The m + l slices are cycled in a fixed order: the first m slices correspond to source words (ordered according to a h</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment precision for syntax-based machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08, pages 44–52. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="28231" citStr="Fraser and Marcu, 2007" startWordPosition="4776" endWordPosition="4779">re more numerous than 2−1 links, in both language pairs. This is consequence of the choice of directionality and tokenization style. 6.5 Translation Impact We tested the impact of improved alignments on the performance of a phrase-based translation system (Ittycheriah and Roukos, 2007) for three lan896 guage pairs. Our alignment did not improve the performance of a mature Arabic to English translation system, but two notable successes were obtained: Chinese to English, and English to Italian. It is well known that improved alignment performance does not always improve translation performance (Fraser and Marcu, 2007). A mature machine translation system may incorporate alignments obtained from multiple aligners, or from both directions of an asymmetric aligner. Furthermore, with large amounts of training data (the Gale Phase 4 Arabic English corpus consisting of 8 × 106 sentences,) a machine translation system is subject to a saturation effect: correcting an alignment may not yield a significant improvement because the the phrases learned from the correct alignment have already been acquired in other contexts. For the Chinese to English translation system (table 6) the training corpus consisted of 11 × 10</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Comput. Linguist., 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>Improvement in word alignments.</title>
<date>2004</date>
<booktitle>In DARPA/TIDES MT workshop.</booktitle>
<contexts>
<context position="20570" citStr="Ge, 2004" startWordPosition="3528" endWordPosition="3529"> ME-seq 89 91 90 corr(ME-seq) 88 92 90 0.1 Table 3: Alignment accuracy for additional languages. * denotes statistical significance; † statistical significance not available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French 894 the training and test sets were decoded with three other aligners, so that the robustness of the correction model to different input alignments could be validated. The three aligners were GIZA++ (Och and Ney, 2003) (with the MOSES (Koehn et al., 2007) postprocessing option -alignment grow-diag-final-and) the posterior HMM aligner of (Ge, 2004), a maximum entropy sequential model (ME-seq) (Ittycheriah and Roukos, 2005). ME-seq is our primary point of comparison: it is discriminatively trained (on the same training data,) uses a rich set of features, and provides the best alignments of the three. Three correction models were trained: corr(GIZA++) is trained to correct the alignments produced by GIZA++, corr(HMM) is trained to correct the alignments produced by the HMM aligner, and corr(ME-seq) is trained to correct the alignments produced by the ME-seq model. In Table (1) we show results for our system correcting each of the aligners</context>
</contexts>
<marker>Ge, 2004</marker>
<rawString>Niyu Ge. 2004. Improvement in word alignments. In DARPA/TIDES MT workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="6012" citStr="Ittycheriah and Roukos, 2005" startWordPosition="937" endWordPosition="940">such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguistically motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1...fm, and a target sentence E = e1...el, the alignment matrix A = {Qij} is an l x m matrix of binary variables. If Qij = 1, then ei is said to be linked to fj. If ei is unlinked then Qij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matri</context>
<context position="20646" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3537" endWordPosition="3540">nment accuracy for additional languages. * denotes statistical significance; † statistical significance not available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French 894 the training and test sets were decoded with three other aligners, so that the robustness of the correction model to different input alignments could be validated. The three aligners were GIZA++ (Och and Ney, 2003) (with the MOSES (Koehn et al., 2007) postprocessing option -alignment grow-diag-final-and) the posterior HMM aligner of (Ge, 2004), a maximum entropy sequential model (ME-seq) (Ittycheriah and Roukos, 2005). ME-seq is our primary point of comparison: it is discriminatively trained (on the same training data,) uses a rich set of features, and provides the best alignments of the three. Three correction models were trained: corr(GIZA++) is trained to correct the alignments produced by GIZA++, corr(HMM) is trained to correct the alignments produced by the HMM aligner, and corr(ME-seq) is trained to correct the alignments produced by the ME-seq model. In Table (1) we show results for our system correcting each of the aligners as measured in the usual recall, precision, and F-measure.1 The resulting i</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In HLT-EMNLP, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the NA-ACL,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="27894" citStr="Ittycheriah and Roukos, 2007" startWordPosition="4720" endWordPosition="4723">ties. The correction model improved performance across all three of these links structures. The single exception is that the number of 2−1 false alarms increased (Zh-En alignments) but in this case, the first pass ME-seq alignment produced few false alarms because it simply proposed few links of this form. It is also notable that 1−2 links are more numerous than 2−1 links, in both language pairs. This is consequence of the choice of directionality and tokenization style. 6.5 Translation Impact We tested the impact of improved alignments on the performance of a phrase-based translation system (Ittycheriah and Roukos, 2007) for three lan896 guage pairs. Our alignment did not improve the performance of a mature Arabic to English translation system, but two notable successes were obtained: Chinese to English, and English to Italian. It is well known that improved alignment performance does not always improve translation performance (Fraser and Marcu, 2007). A mature machine translation system may incorporate alignments obtained from multiple aligners, or from both directions of an asymmetric aligner. Furthermore, with large amounts of training data (the Gale Phase 4 Arabic English corpus consisting of 8 × 106 sent</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2007. Direct translation model 2. In Human Language Technologies 2007: The Conference of the NA-ACL, pages 57– 64, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="5035" citStr="Koehn et al., 2007" startWordPosition="792" endWordPosition="795">get languages results in a different aligner. This parameterization does not allow a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically traine</context>
<context position="20476" citStr="Koehn et al., 2007" startWordPosition="3515" endWordPosition="3518"> DE—+EN ME-seq 77 82 79 corr(ME-seq) 78 87 82 3* ES—+EN ME-seq 93 86 90 corr(ME-seq) 92 88 90 0.6 FR—+EN ME-seq 89 91 90 corr(ME-seq) 88 92 90 0.1 Table 3: Alignment accuracy for additional languages. * denotes statistical significance; † statistical significance not available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French 894 the training and test sets were decoded with three other aligners, so that the robustness of the correction model to different input alignments could be validated. The three aligners were GIZA++ (Och and Ney, 2003) (with the MOSES (Koehn et al., 2007) postprocessing option -alignment grow-diag-final-and) the posterior HMM aligner of (Ge, 2004), a maximum entropy sequential model (ME-seq) (Ittycheriah and Roukos, 2005). ME-seq is our primary point of comparison: it is discriminatively trained (on the same training data,) uses a rich set of features, and provides the best alignments of the three. Three correction models were trained: corr(GIZA++) is trained to correct the alignments produced by GIZA++, corr(HMM) is trained to correct the alignments produced by the HMM aligner, and corr(ME-seq) is trained to correct the alignments produced by</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL 2004: Short Papers on XX, HLT-NAACL ’04,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4743" citStr="Lee, 2004" startWordPosition="751" endWordPosition="752">nglish and a 1.26 BLEU improvement on English to Italian translation. 2 Alignment sequence models Sequence models are the traditional workhorse for word alignment, appearing, for instance, in IBM Models 1-5. This type of alignment model is not symmetric; interchanging source and target languages results in a different aligner. This parameterization does not allow a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In Proceedings of HLTNAACL 2004: Short Papers on XX, HLT-NAACL ’04, pages 57–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5117" citStr="Liang et al., 2006" startWordPosition="803" endWordPosition="806"> a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignm</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 104–111. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="15470" citStr="Liu and Nocedal, 1989" startWordPosition="2681" endWordPosition="2684">rom the set of alignments Mt(At) generated by the move generator at step t takes the form: P e i λiφi(At+1,E,F ) p(At+1|E, F, Mt(At)) = Z(E, F, Mt(At)) (3) 892 where the likelihood of each training sample is �Ln = log p1(A0n|E, Fn; M(fα, A0n, E, Fn)) α determiner links to the same word as its head noun. As an example, if ei is the headword of ei,, and fj is the headword of fj,, then � log p1(A0n|E,Fn;M(eβ,A0n,E,Fn)) (6) � σijσi/j/ (8) + φ(A, E, F) = β ij The likelihood has a term for each sentence pair and for each decoder step. The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. 5 Features A wide variety of features were used in the model. We group the features in three broad categories: link-based, geometrical, and parse-based. Link-based features are those which decompose into a (linear) sum of alignment matrix elements σij. An example link-based feature is one that fires if a source language noun is linked to a target language determiner. Note that this feature may fire more than once in a given sentence pair: as with most </context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Log-linear models for word alignment.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>459--466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7091" citStr="Liu et al., 2005" startWordPosition="1130" endWordPosition="1133"> = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are potentially allowed in alignment matrix models. For typical l and m, 2lm » (m + 1)l, the number of alignments described by a comparable sequence model. This parameterization is symmetric - if source and target are interchanged, then the alignment matrix is transposed. A straightforward approach to the alignment matrix is to build a log linear model (Liu et al., 2005) for the probability of the alignment A. (We continue to refer to “source” and “target” words only for consistency of notation - alignment models such as this are indifferent to the actual direction of translation.) The log linear model for the alignment (Liu et al., 2005) is exp (Ei λiOi(A, E, F)) p(A|E, F) = (1) Z(E, F) where the partition function (normalization) is given by �λiOi(A, E, F) . (2) Here the Oi(A, E, F) are feature functions. The model is parameterized by a set of weights λi, one for each feature function. Feature functions are often binary, but are not required to be. Feature </context>
<context position="12603" citStr="Liu et al., 2005" startWordPosition="2088" endWordPosition="2091">or correct normalization. This equation is notationally very similar to equation (1), except that the predictions of the model are restricted to a small set of nearby alignments. For the move generator considered in this paper, the summation in Eq.(4) is similarly restricted, and hence training the model is tractable. The set of candidate alignments Mt(At) typically does not contain the reference (gold) alignment; we model the best alignment among a finite set of alternatives, rather than the correct alignment from among all possible alignments. This is a key difference between our model and (Liu et al., 2005). Note that if we extended our definition of perturbation to the limiting case that the alignment set included all possible alignments then we would clearly recover the standard log linear model of alignment. 4.3 Training Since the model is designed to predict perturbation to an alignment, it is trained from a collection of errorful alignments and corresponding reference sequences of aligner moves that reach the reference (gold) alignment. We construct a training set from a collection of sentence pairs and reference alignments for training (A*n, En, Fn)Nn=1, as well as collections of correspon</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear models for word alignment. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 459–466. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Wen-tau Yih</author>
<author>Andreas Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>513--520</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8734" citStr="Moore et al., 2006" startWordPosition="1412" endWordPosition="1415"> a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generator perturbs an existing</context>
</contexts>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>Robert C. Moore, Wen-tau Yih, and Andreas Bode. 2006. Improved discriminative bilingual word alignment. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 513–520. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment. In</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="8712" citStr="Moore, 2005" startWordPosition="1410" endWordPosition="1411">e restricted to a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generato</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In In Proceedings of HLTEMNLP, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative word alignment via alignment matrix modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>18--25</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6239" citStr="Niehues and Vogel, 2008" startWordPosition="971" endWordPosition="974">are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguistically motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1...fm, and a target sentence E = e1...el, the alignment matrix A = {Qij} is an l x m matrix of binary variables. If Qij = 1, then ei is said to be linked to fj. If ei is unlinked then Qij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are potentially allowed in alignment matrix models. For typical l and m, 2lm » (m + 1)l, the number of alignments describ</context>
<context position="8387" citStr="Niehues and Vogel, 2008" startWordPosition="1358" endWordPosition="1361">ts Qij of the alignment matrix A. The sum over all alignments of a sentence pair (2lm terms) in the partition function is computationally impractical except for very short sentences, and is rarely amenable to dynamic programming. Thus the partition function is replaced by an approximation. For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model t</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Jan Niehues and Stephan Vogel. 2008. Discriminative word alignment via alignment matrix modeling. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 18–25, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10841" citStr="Och and Ney, 2003" startWordPosition="1777" endWordPosition="1780">or alignment correction target word is sufficient. 4.1 Move generation Many different types of alignment perturbations are possible. Here we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (generically, one slice) of the alignment matrix. At each step t, the move set Mt(At) is formed by choosing a slice of the current alignment matrix At, and generating all possible alignments from a few families of moves. Then the move generator picks anot</context>
<context position="20439" citStr="Och and Ney, 2003" startWordPosition="3508" endWordPosition="3511">eq 81 85 83 corr(ME-seq) 82 92 87 4* DE—+EN ME-seq 77 82 79 corr(ME-seq) 78 87 82 3* ES—+EN ME-seq 93 86 90 corr(ME-seq) 92 88 90 0.6 FR—+EN ME-seq 89 91 90 corr(ME-seq) 88 92 90 0.1 Table 3: Alignment accuracy for additional languages. * denotes statistical significance; † statistical significance not available. IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French 894 the training and test sets were decoded with three other aligners, so that the robustness of the correction model to different input alignments could be validated. The three aligners were GIZA++ (Och and Ney, 2003) (with the MOSES (Koehn et al., 2007) postprocessing option -alignment grow-diag-final-and) the posterior HMM aligner of (Ge, 2004), a maximum entropy sequential model (ME-seq) (Ittycheriah and Roukos, 2005). ME-seq is our primary point of comparison: it is discriminatively trained (on the same training data,) uses a rich set of features, and provides the best alignments of the three. Three correction models were trained: corr(GIZA++) is trained to correct the alignments produced by GIZA++, corr(HMM) is trained to correct the alignments produced by the HMM aligner, and corr(ME-seq) is trained </context>
<context position="22487" citStr="Och and Ney, 2003" startWordPosition="3828" endWordPosition="3831">ur correction model is robust to different starting aligners. 6.2 Chinese-English alignment results Table (2) presents results for Chinese-English word alignments. The training set for the corr(MEseq) model consisted of approximately 8000 handaligned sentences sampled from LDC2006E93 and LDC2008E57. The model was trained to correct the output of the ME-seq aligner, and tested on the same condition. For this language pair, reference parses were not available in our training set, so 1We do not distinguish sure and possible links in our annotations - under this circumstance, alignment error rate(Och and Ney, 2003) is 1 − F. automatically-generated parses were used for both training and test sets. Results are measured on a 512 sentence test set, sampled from a wide variety of parallel corpora of various genres. We compare performance with GIZA++, and with the ME-seq aligner. Again the resulting improvement over the ME-seq aligner is statistically significant. However, here the improvement in recall is somewhat larger than the improvement in precision. 6.3 Additional language pairs Table (3) presents alignment results for seven other language pairs. Separate alignment corrector models were trained for bo</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<location>Mach. Learn., 34:151–175,</location>
<contexts>
<context position="18881" citStr="Ratnaparkhi, 1999" startWordPosition="3238" endWordPosition="3240"> of language pairs; we do not attempt to exactly match features between comparison aligners - this is unlikely to lead to a robust correction model. 6.1 Arabic-English alignment results We trained the Arabic-English alignment system on 5125 sentences from Arabic-English treebanks (LDC2008E61, LDC2008E22) that had been annotated for word alignment. Reference parses were used during the training. Results are measured on a 500 sentence test set, sampled from a wide variety of parallel corpora, including various genres. During alignment, only automaticallygenerated parses (based on the parser of (Ratnaparkhi, 1999)) were available. Alignments on 893 initial align correction model R (%) P (%) F(%) AF GIZA++ 76 76 76 corr(GIZA++) 86 94 90 14* corr(ME-seq) 88 92 90 14* HMM 73 73 73 corr(HMM) 87 92 89 16* corr(ME-seq) 87 93 90 17* ME-seq 82 84 83 corr(HMM) 88 92 90 7* corr(GIZA++) 87 94 91 8* corr(ME-seq) 89 94 91 8* Table 1: Alignment accuracy for Arabic-English systems in percentage recall (R), precision(P), and F-measure. * denotes statistical significance (see text.) lang method R (%) P(%) F(%) AF ZH—+EN GIZA++ 55 67 61 ME-seq 66 72 69 corr(ME-seq) 74 76 75 6* Table 2: Alignment accuracy for Chinese(ZH)</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Mach. Learn., 34:151–175, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Discriminative word alignment with a function word reordering model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>534--544</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10587" citStr="Setiawan et al., 2010" startWordPosition="1735" endWordPosition="1738">ach source and Input: sentence pair E1 .. El, F1 .. Fm Input: alignment A Output: improved alignment Afinal for t = 1 → l do generate moves: Mt(At) select move: At+1 ← argmaxA∈Mt(At)p(AjAt, E, F) Afinal ← Al+1 {repeat for source words} Figure 1: pseudocode for alignment correction target word is sufficient. 4.1 Move generation Many different types of alignment perturbations are possible. Here we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (</context>
</contexts>
<marker>Setiawan, Dyer, Resnik, 2010</marker>
<rawString>Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010. Discriminative word alignment with a function word reordering model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 534–544. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="29193" citStr="Snover et al., 2006" startWordPosition="4934" endWordPosition="4937">ing an alignment may not yield a significant improvement because the the phrases learned from the correct alignment have already been acquired in other contexts. For the Chinese to English translation system (table 6) the training corpus consisted of 11 × 106 sentence pairs, subsampled to 106. The test set was NIST MT08 Newswire, consisting of 691 sentences and 4 reference translations. Corpus-level performance (columns 2 and 3) improved when measured by BLEU, but not by TER. Performance on the most difficult sentences (near the 90th percentile, columns 4 and 5) improved on both BLEU and TER (Snover et al., 2006), and the improvement in BLEU was larger for the more difficult sentences than it was overall. Translation performance further improved, by a smaller amount, using both ME-seq and corr(ME-seq) alignments during the training. The improved alignments impacted the translation performance of the English to Italian translation system (table 7) even more strongly. Here the training corpus consisted of 9.4×106 sentence pairs, subsampled to 387000 pairs. The test set consisted of 7899 sentences. Overall performance improved as measured by both TER and BLEU (1.26 points.) 7 Conclusions A log linear mod</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment. In</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="8677" citStr="Taskar et al., 2005" startWordPosition="1404" endWordPosition="1407"> example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and</context>
</contexts>
<marker>Taskar, Lacoste-julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In In Proceedings of HLT-EMNLP, pages 73– 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="5441" citStr="Vogel et al., 1996" startWordPosition="857" endWordPosition="860">internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguistically motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random fi</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research, HLT ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1606" citStr="Yarowsky et al., 2001" startWordPosition="242" endWordPosition="245">lize parallel corpora as training data. Word alignments appear as hidden variables in IBM Models 1- 5 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of finding the optimal alignment. Many alignment models are factorized in order to use dynamic programming and beam search for efficient marginalization and search. Such a factorization encourages - but does not require - a sequential (often left-to-right) decoding order. If left-to-right decoding is adopted (and exact dynamic programming is intractable) important right context may exist beyond the search window. For example, the linkage of an English determiner may be considered before the linkag</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the first international conference on Human language technology research, HLT ’01, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>