<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009644">
<title confidence="0.998754">
An Exploration of Forest-to-String Translation:
Does Translation Help or Hurt Parsing?
</title>
<author confidence="0.998925">
Hui Zhang
</author>
<affiliation confidence="0.9997565">
University of Southern California
Department of Computer Science
</affiliation>
<email confidence="0.998958">
hzhang@isi.edu
</email>
<author confidence="0.997262">
David Chiang
</author>
<affiliation confidence="0.9842255">
University of Southern California
Information Sciences Institute
</affiliation>
<email confidence="0.998834">
chiang@isi.edu
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995880090909091">
Syntax-based translation models that operate
on the output of a source-language parser have
been shown to perform better if allowed to
choose from a set of possible parses. In this
paper, we investigate whether this is because it
allows the translation stage to overcome parser
errors or to override the syntactic structure it-
self. We find that it is primarily the latter, but
that under the right conditions, the transla-
tion stage does correct parser errors, improv-
ing parsing accuracy on the Chinese Treebank.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.970672913043478">
Tree-to-string translation systems (Liu et al., 2006;
Huang et al., 2006) typically employ a pipeline of
two stages: a syntactic parser for the source lan-
guage, and a decoder that translates source-language
trees into target-language strings. Originally, the
output of the parser stage was a single parse tree, and
this type of system has been shown to outperform
phrase-based translation on, for instance, Chinese-
to-English translation (Liu et al., 2006). More recent
work has shown that translation quality is improved
further if the parser outputs a weighted parse forest,
that is, a representation of a whole distribution over
possible parse trees (Mi et al., 2008). In this paper,
we investigate two hypotheses to explain why.
One hypothesis is that forest-to-string translation
selects worse parses. Although syntax often helps
translation, there may be situations where syntax, or
at least syntax in the way that our models use it, can
impose constraints that are too rigid for good-quality
translation (Liu et al., 2007; Zhang et al., 2008).
For example, suppose that a tree-to-string system
encounters the following correct tree (only partial
bracketing shown):
</bodyText>
<equation confidence="0.345542">
(1) [NP jingjieconomy
‘economic growth rate’
</equation>
<bodyText confidence="0.947323">
Suppose further that the model has never seen this
phrase before, although it has seen the subphrase
z¯engzhˇang de s`ud`u ‘growth rate’. Because this sub-
phrase is not a syntactic unit in sentence (1), the sys-
tem will be unable to translate it. But a forest-to-
string system would be free to choose another (in-
correct but plausible) bracketing:
(2) jingjieconomy
and successfully translate it using rules learned from
observed data.
The other hypothesis is that forest-to-string trans-
lation selects better parses. For example, if a Chi-
nese parser is given the input c¯anji¯a biˇaojiˇe de h¯unli,
it might consider two structures:
h¯unliwedding
‘wedding that attends a cousin’
</bodyText>
<listItem confidence="0.921514666666667">
(4) c¯anji¯a [NP biˇaojiˇe de h¯unli]
attend cousin DE wedding
‘attend a cousin’s wedding’
</listItem>
<bodyText confidence="0.9987704">
The two structures have two different translations
into English, shown above. While the parser prefers
structure (3), an n-gram language model would eas-
ily prefer translation (4) and, therefore, its corre-
sponding Chinese parse.
</bodyText>
<figure confidence="0.9978325">
z¯engzhˇang] de s`ud`u
growth DE rate
[NP z¯engzhˇang
growth
de
DE
s`ud`u]
rate
(3) [VP c¯anji¯a biˇaojiˇe] de
attend cousin DE
</figure>
<page confidence="0.964111">
317
</page>
<note confidence="0.796662">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 317–321,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.994161125">
f f f parser decoder e e e e
(a) f f f
source source target
string tree string
f f f parser decoder e e e e
(b)f f f
source source target
string forest string
</figure>
<figureCaption confidence="0.9961458">
Figure 1: (a) In tree-to-string translation, the parser gen-
erates a single tree which the decoder must use to gen-
erate a translation. (b) In forest-to-string translation, the
parser generates a forest of possible trees, any of which
the decoder can use to generate a translation.
</figureCaption>
<bodyText confidence="0.999954272727273">
Previous work has shown that an observed target-
language translation can improve parsing of source-
language text (Burkett and Klein, 2008; Huang et al.,
2009), but to our knowledge, only Chen et al. (2011)
have explored the case where the target-language
translation is unobserved.
Below, we carry out experiments to test these
two hypotheses. We measure the accuracy (using
labeled-bracket F1) of the parses that the translation
model selects, and find that they are worse than the
parses selected by the parser. Our basic conclusion,
then, is that the parses that help translation (accord-
ing to B ) are, on average, worse parses. That is,
forest-to-string translation hurts parsing.
But there is a twist. Neither labeled-bracket F1
nor B is a perfect metric of the phenomena it is
meant to measure, and our translation system is op-
timized to maximize B . If we optimize our sys-
tem to maximize labeled-bracket F1 instead, we find
that our translation system selects parses that score
higher than the baseline parser’s. That is, forest-to-
string translation can help parsing.
</bodyText>
<sectionHeader confidence="0.983584" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.950577533333333">
We provide here only a cursory overview of tree-
to-string and forest-to-string translation. For more
details, the reader is referred to the original papers
describing them (Liu et al., 2006; Mi et al., 2008).
Figure 1a illustrates the tree-to-string transla-
tion pipeline. The parser stage can be any phrase-
structure parser; it computes a parse for each source-
language string. The decoder stage translates the
source-language tree into a target-language string,
using a synchronous tree-substitution grammar.
In forest-to-string translation (Figure 1b), the
parser outputs a forest of possible parses of each
source-language string. The decoder uses the same
rules as in tree-to-string translation, but is free to se-
lect any of the trees contained in the parse forest.
</bodyText>
<sectionHeader confidence="0.981646" genericHeader="method">
3 Translation hurts parsing
</sectionHeader>
<bodyText confidence="0.999984375">
The simplest experiment to carry out is to exam-
ine the parses actually selected by the decoder, and
see whether they are better or worse than the parses
selected by the parser. If they are worse, this sup-
ports the hypothesis that syntax can hurt translation.
If they are better, we can conclude that translation
can help parsing. In this initial experiment, we find
that the former is the case.
</bodyText>
<subsectionHeader confidence="0.997294">
3.1 Setup
</subsectionHeader>
<bodyText confidence="0.999951230769231">
The baseline parser is the Charniak parser (Char-
niak, 2000). We trained it on the Chinese Treebank
(CTB) 5.1, split as shown in Table 1, following
Duan et al. (2007).1 The parser outputs a parse forest
annotated with head words and other information.
Since the decoder does not use these annotations,
we use the max-rule algorithm (Petrov et al., 2006)
to (approximately) sum them out. As a side bene-
fit, this improves parsing accuracy from 77.76% to
78.42% F1. The weight of a hyperedge in this for-
est is its posterior probability, given the input string.
We retain these weights as a feature in the translation
model.
The decoder stage is a forest-to-string system (Liu
et al., 2006; Mi et al., 2008) for Chinese-to-English
translation. The datasets used are listed in Ta-
ble 1. We generated word alignments with GIZA++
and symmetrized them using the grow-diag-final-
and heuristic. We parsed the Chinese side using
the Charniak parser as described above, and per-
formed forest-based rule extraction (Mi and Huang,
2008) with a maximum height of 3 nodes. We used
the same features as Mi and Huang (2008). The
language model was a trigram model with modi-
fied Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998), trained on the target
</bodyText>
<footnote confidence="0.978517">
1The more common split, used by Bikel and Chiang (2000),
has flaws that are described by Levy and Manning (2003).
</footnote>
<page confidence="0.975899">
318
</page>
<table confidence="0.983765142857143">
Parsing Translation
CTB 1–815 FBIS
CTB 1101–1136
CTB 900–931 NIST 2002
CTB 1148–1151
CTB 816–885 NIST 2003
CTB 1137–1147
</table>
<tableCaption confidence="0.997855666666667">
Table 1: Data used for training and testing the parsing and
translation models.
Table 2: Forest-to-string translation outperforms tree-to-
</tableCaption>
<bodyText confidence="0.994527428571429">
string translation according to B , but the decreases
parsing accuracy according to labeled-bracket F1. How-
ever, when we train to maximize labeled-bracket F1,
forest-to-string translation yields better parses than both
tree-to-string translation and the original parser.
side of the training data. We used minimum-error-
rate (MER) training to optimize the feature weights
(Och, 2003) to maximize B .
At decoding time, we select the best derivation
and extract its source tree. In principle, we ought
to sum over all derivations for each source tree; but
the approximations that we tried (n-best list crunch-
ing, max-rule decoding, minimum Bayes risk) did
not appear to help.
</bodyText>
<subsectionHeader confidence="0.90242">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999610727272727">
Table 2 shows the main results of our experiments.
In the second and third line, we see that the forest-
to-string system outperforms the tree-to-string sys-
tem by 1.53 B , consistent with previously pub-
lished results (Mi et al., 2008; Zhang et al., 2009).
However, we also find that the trees selected by the
forest-to-string system score much lower according
to labeled-bracket F1. This suggests that the reason
the forest-to-string system is able to generate better
translations is that it can soften the constraints im-
posed by the syntax of the source language.
</bodyText>
<sectionHeader confidence="0.977287" genericHeader="method">
4 Translation helps parsing
</sectionHeader>
<bodyText confidence="0.9999194">
We have found that better translations can be ob-
tained by settling for worse parses. However, trans-
lation accuracy is measured using B and pars-
ing accuracy is measured using labeled-bracket F1,
and neither of these is a perfect metric of the phe-
nomenon it is meant to measure. Moreover, we op-
timized the translation model in order to maximize
B . It is known that when MER training is used
to optimize one translation metric, other translation
metrics suffer (Och, 2003); much more, then, can
we expect that optimizing B will cause labeled-
bracket F1 to suffer. In this section, we try optimiz-
ing labeled-bracket F1, and find that, in this case, the
translation model does indeed select parses that are
better on average.
</bodyText>
<subsectionHeader confidence="0.98133">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.986600037037037">
MER training with labeled-bracket F1 as an objec-
tive function is straightforward. At each iteration of
MER training, we run the parser and decoder over
the CTB dev set to generate an n-best list of possible
translation derivations (Huang and Chiang, 2005).
For each derivation, we extract its Chinese parse tree
and compute the number of brackets guessed and
the number matched against the gold-standard parse
tree. A trivial modification of the MER trainer then
optimizes the feature weights to maximize labeled-
bracket F1.
A technical challenge that arises is ensuring di-
versity in the n-best lists. The MER trainer re-
quires that each list contain enough unique transla-
tions (when maximizing B ) or source trees (when
maximizing labeled-bracket F1). However, because
one source tree may lead to many translation deriva-
tions, the n-best list may contain only a few unique
source trees, or in the extreme case, the derivations
may all have the same source tree. We use a variant
of the n-best algorithm that allows efficient genera-
tion of equivalence classes of derivations (Huang et
al., 2006). The standard algorithm works by gener-
ating, at each node of the forest, a list of the best
subderivations at that node; the variant drops a sub-
derivation if it has the same source tree as a higher-
scoring subderivation.
</bodyText>
<figure confidence="0.998049571428571">
Train
Dev
Test
System Objective
Parsing Translation
F1% B %
Charniak n/a
78.42 n/a
tree-to-string max-B
forest-to-string max-B
78.42 23.07
77.75 24.60
forest-to-string max-F1
78.81 19.18
</figure>
<page confidence="0.866984">
319
</page>
<table confidence="0.72287675">
Maximum F1% LM data F1% Features F1% Parallel data F1%
rule height (lines) (lines)
3 78.81 none 78.78 monolingual 78.89 60k 78.00
4 78.93 100 78.79 + bilingual 79.24 120k 78.16
5 79.14 30k 78.67 300k 79.24
300k 79.14
13M 79.24
(a) (b) (c) (d)
</table>
<tableCaption confidence="0.9941315">
Table 3: Effect of variations on parsing performance. (a) Increasing the maximum translation rule height increases
parsing accuracy further. (b) Increasing/decreasing the language model size increases/decreases parsing accuracy.
(c) Decreasing the parallel text size decreases parsing accuracy. (d) Removing all bilingual features decreases parsing
accuracy, but only slightly.
</tableCaption>
<subsectionHeader confidence="0.66094">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999771113636363">
The last line of Table 2 shows the results of this
second experiment. The system trained to opti-
mize labeled-bracket F1 (max-F1) obtains a much
lower B score than the one trained to maximize
B (max-B )—unsurprisingly, because a single
source-side parse can yield many different transla-
tions, but the objective function scores them equally.
What is more interesting is that the max-F1 system
obtains a higher F1 score, not only compared with
the max-B system but also the original parser.
We then tried various settings to investigate what
factors affect parsing performance. First, we found
that increasing the maximum rule height increases
F1 further (Table 3a).
One of the motivations of our method is that bilin-
gual information (especially the language model)
can help disambiguate the source side structures. To
test this, we varied the size of the corpus used to train
the language model (keeping a maximum rule height
of 5 from the previous experiment). The 13M-line
language model adds the Xinhua portion of Giga-
word 3. In Table 3b we see that the parsing perfor-
mance does increase with the language model size,
with the largest language model yielding a net im-
provement of 0.82 over the baseline parser.
To test further the importance of bilingual infor-
mation, we compared against a system built only
from the Chinese side of the parallel text (with each
word aligned to itself). We removed all features that
use bilingual information, retaining only the parser
probability and the phrase penalty. In their place
we added a new feature, the probability of a rule’s
source side tree given its root label, which is essen-
tially the same model used in Data-Oriented Parsing
(Bod, 1992). Table 3c shows that this system still
outperforms the original parser. In other words, part
of the gain is not attributable to translation, but ad-
ditional source-side context and data that the trans-
lation model happens to capture.
Finally, we varied the size of the parallel text
(keeping a maximum rule height of 5 and the largest
language model) and found that, as expected, pars-
ing performance correlates with parallel data size
(Table 3d).
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998975">
We set out to investigate why forest-to-string trans-
lation outperforms tree-to-string translation. By
comparing their performance as Chinese parsers, we
found that forest-to-string translation sacrifices pars-
ing accuracy, suggesting that forest-to-string trans-
lation works by overriding constraints imposed by
syntax. But when we optimized the system to max-
imize labeled-bracket F1, we found that, in fact,
forest-to-string translation is able to achieve higher
accuracy, by 0.82 F1%, than the baseline Chinese
parser, demonstrating that, to a certain extent, forest-
to-string translation is able to correct parsing errors.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99909825">
We are grateful to the anonymous reviewers for
their helpful comments. This research was sup-
ported in part by DARPA under contract DOI-NBC
D11AP00244.
</bodyText>
<page confidence="0.996068">
320
</page>
<sectionHeader confidence="0.995852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998389939393939">
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the Chinese Treebank.
In Proc. Second Chinese Language Processing Work-
shop, pages 1–6.
Rens Bod. 1992. A computational model of language
performance: Data Oriented Parsing. In Proc. COL-
ING 1992, pages 855–859.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
EMNLP 2008, pages 877–886.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL, pages 132–139.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Wenliang Chen, Jun’ichi Kazama, Min Zhang, Yoshi-
masa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro
Torisawa, and Haizhou Li. 2011. SMT helps bitext
dependency parsing. In Proc. EMNLP 2011, pages
73–83.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic models for action-based Chinese dependency pars-
ing. In Proc. ECML 2007, pages 559–566.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. IWPT 2005, pages 53–64.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. AMTA 2006, pages 65–
73.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. EMNLP 2009, pages 1222–1231.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Proc.
ICASSP 1995, pages 181–184.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese Treebank? In
Proc. ACL 2003, pages 439–446.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. COLING-ACL 2006, pages 609–616.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Proc.
ACL 2007, pages 704–711.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. EMNLP 2008, pages
206–214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ACL-08: HLT, pages 192–
199.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. ACL 2003,
pages 160–167.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING-ACL 2006,
pages 433–440.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL-08: HLT, pages 559–567.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In Proc. ACL-IJCNLP 2009,
pages 172–180.
</reference>
<page confidence="0.998765">
321
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.611955">
<title confidence="0.9951865">An Exploration of Forest-to-String Does Translation Help or Hurt Parsing?</title>
<author confidence="0.985913">Hui</author>
<affiliation confidence="0.9999225">University of Southern Department of Computer</affiliation>
<email confidence="0.996688">hzhang@isi.edu</email>
<author confidence="0.761408">David</author>
<affiliation confidence="0.998382">University of Southern Information Sciences</affiliation>
<email confidence="0.998263">chiang@isi.edu</email>
<abstract confidence="0.985536833333334">Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two statistical parsing models applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>In Proc. Second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="7369" citStr="Bikel and Chiang (2000)" startWordPosition="1182" endWordPosition="1185"> 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target 1The more common split, used by Bikel and Chiang (2000), has flaws that are described by Levy and Manning (2003). 318 Parsing Translation CTB 1–815 FBIS CTB 1101–1136 CTB 900–931 NIST 2002 CTB 1148–1151 CTB 816–885 NIST 2003 CTB 1137–1147 Table 1: Data used for training and testing the parsing and translation models. Table 2: Forest-to-string translation outperforms tree-tostring translation according to B , but the decreases parsing accuracy according to labeled-bracket F1. However, when we train to maximize labeled-bracket F1, forest-to-string translation yields better parses than both tree-to-string translation and the original parser. side of </context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied to the Chinese Treebank. In Proc. Second Chinese Language Processing Workshop, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>A computational model of language performance: Data Oriented Parsing.</title>
<date>1992</date>
<booktitle>In Proc. COLING</booktitle>
<pages>855--859</pages>
<contexts>
<context position="13524" citStr="Bod, 1992" startWordPosition="2189" endWordPosition="2190"> increase with the language model size, with the largest language model yielding a net improvement of 0.82 over the baseline parser. To test further the importance of bilingual information, we compared against a system built only from the Chinese side of the parallel text (with each word aligned to itself). We removed all features that use bilingual information, retaining only the parser probability and the phrase penalty. In their place we added a new feature, the probability of a rule’s source side tree given its root label, which is essentially the same model used in Data-Oriented Parsing (Bod, 1992). Table 3c shows that this system still outperforms the original parser. In other words, part of the gain is not attributable to translation, but additional source-side context and data that the translation model happens to capture. Finally, we varied the size of the parallel text (keeping a maximum rule height of 5 and the largest language model) and found that, as expected, parsing performance correlates with parallel data size (Table 3d). 5 Conclusion We set out to investigate why forest-to-string translation outperforms tree-to-string translation. By comparing their performance as Chinese </context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>Rens Bod. 1992. A computational model of language performance: Data Oriented Parsing. In Proc. COLING 1992, pages 855–859.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>877--886</pages>
<contexts>
<context position="3916" citStr="Burkett and Klein, 2008" startWordPosition="610" endWordPosition="613">c�2012 Association for Computational Linguistics f f f parser decoder e e e e (a) f f f source source target string tree string f f f parser decoder e e e e (b)f f f source source target string forest string Figure 1: (a) In tree-to-string translation, the parser generates a single tree which the decoder must use to generate a translation. (b) In forest-to-string translation, the parser generates a forest of possible trees, any of which the decoder can use to generate a translation. Previous work has shown that an observed targetlanguage translation can improve parsing of sourcelanguage text (Burkett and Klein, 2008; Huang et al., 2009), but to our knowledge, only Chen et al. (2011) have explored the case where the target-language translation is unobserved. Below, we carry out experiments to test these two hypotheses. We measure the accuracy (using labeled-bracket F1) of the parses that the translation model selects, and find that they are worse than the parses selected by the parser. Our basic conclusion, then, is that the parses that help translation (according to B ) are, on average, worse parses. That is, forest-to-string translation hurts parsing. But there is a twist. Neither labeled-bracket F1 nor</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proc. EMNLP 2008, pages 877–886.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="6126" citStr="Charniak, 2000" startWordPosition="972" endWordPosition="974">ses the same rules as in tree-to-string translation, but is free to select any of the trees contained in the parse forest. 3 Translation hurts parsing The simplest experiment to carry out is to examine the parses actually selected by the decoder, and see whether they are better or worse than the parses selected by the parser. If they are worse, this supports the hypothesis that syntax can hurt translation. If they are better, we can conclude that translation can help parsing. In this initial experiment, we find that the former is the case. 3.1 Setup The baseline parser is the Charniak parser (Charniak, 2000). We trained it on the Chinese Treebank (CTB) 5.1, split as shown in Table 1, following Duan et al. (2007).1 The parser outputs a parse forest annotated with head words and other information. Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al., 2006) to (approximately) sum them out. As a side benefit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this forest is its posterior probability, given the input string. We retain these weights as a feature in the translation model. The decoder stage is a forest-to-string</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. NAACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="7290" citStr="Chen and Goodman, 1998" startWordPosition="1168" endWordPosition="1171"> translation model. The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target 1The more common split, used by Bikel and Chiang (2000), has flaws that are described by Levy and Manning (2003). 318 Parsing Translation CTB 1–815 FBIS CTB 1101–1136 CTB 900–931 NIST 2002 CTB 1148–1151 CTB 816–885 NIST 2003 CTB 1137–1147 Table 1: Data used for training and testing the parsing and translation models. Table 2: Forest-to-string translation outperforms tree-tostring translation according to B , but the decreases parsing accuracy according to labeled-bracket F1. However, when we train to maximize labeled-bracket F1, forest-to-string translation yields bette</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Min Zhang</author>
</authors>
<title>Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro Torisawa, and Haizhou Li.</title>
<date>2011</date>
<booktitle>In Proc. EMNLP 2011,</booktitle>
<pages>73--83</pages>
<contexts>
<context position="3984" citStr="Chen et al. (2011)" startWordPosition="623" endWordPosition="626"> e e (a) f f f source source target string tree string f f f parser decoder e e e e (b)f f f source source target string forest string Figure 1: (a) In tree-to-string translation, the parser generates a single tree which the decoder must use to generate a translation. (b) In forest-to-string translation, the parser generates a forest of possible trees, any of which the decoder can use to generate a translation. Previous work has shown that an observed targetlanguage translation can improve parsing of sourcelanguage text (Burkett and Klein, 2008; Huang et al., 2009), but to our knowledge, only Chen et al. (2011) have explored the case where the target-language translation is unobserved. Below, we carry out experiments to test these two hypotheses. We measure the accuracy (using labeled-bracket F1) of the parses that the translation model selects, and find that they are worse than the parses selected by the parser. Our basic conclusion, then, is that the parses that help translation (according to B ) are, on average, worse parses. That is, forest-to-string translation hurts parsing. But there is a twist. Neither labeled-bracket F1 nor B is a perfect metric of the phenomena it is meant to measure, and </context>
</contexts>
<marker>Chen, Kazama, Zhang, 2011</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Min Zhang, Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro Torisawa, and Haizhou Li. 2011. SMT helps bitext dependency parsing. In Proc. EMNLP 2011, pages 73–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic models for action-based Chinese dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc. ECML</booktitle>
<pages>559--566</pages>
<contexts>
<context position="6232" citStr="Duan et al. (2007)" startWordPosition="991" endWordPosition="994">n the parse forest. 3 Translation hurts parsing The simplest experiment to carry out is to examine the parses actually selected by the decoder, and see whether they are better or worse than the parses selected by the parser. If they are worse, this supports the hypothesis that syntax can hurt translation. If they are better, we can conclude that translation can help parsing. In this initial experiment, we find that the former is the case. 3.1 Setup The baseline parser is the Charniak parser (Charniak, 2000). We trained it on the Chinese Treebank (CTB) 5.1, split as shown in Table 1, following Duan et al. (2007).1 The parser outputs a parse forest annotated with head words and other information. Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al., 2006) to (approximately) sum them out. As a side benefit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this forest is its posterior probability, given the input string. We retain these weights as a feature in the translation model. The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are list</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic models for action-based Chinese dependency parsing. In Proc. ECML 2007, pages 559–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. IWPT</booktitle>
<pages>53--64</pages>
<contexts>
<context position="9952" citStr="Huang and Chiang, 2005" startWordPosition="1605" endWordPosition="1608"> when MER training is used to optimize one translation metric, other translation metrics suffer (Och, 2003); much more, then, can we expect that optimizing B will cause labeledbracket F1 to suffer. In this section, we try optimizing labeled-bracket F1, and find that, in this case, the translation model does indeed select parses that are better on average. 4.1 Setup MER training with labeled-bracket F1 as an objective function is straightforward. At each iteration of MER training, we run the parser and decoder over the CTB dev set to generate an n-best list of possible translation derivations (Huang and Chiang, 2005). For each derivation, we extract its Chinese parse tree and compute the number of brackets guessed and the number matched against the gold-standard parse tree. A trivial modification of the MER trainer then optimizes the feature weights to maximize labeledbracket F1. A technical challenge that arises is ensuring diversity in the n-best lists. The MER trainer requires that each list contain enough unique translations (when maximizing B ) or source trees (when maximizing labeled-bracket F1). However, because one source tree may lead to many translation derivations, the n-best list may contain o</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. IWPT 2005, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. AMTA</booktitle>
<pages>65--73</pages>
<contexts>
<context position="879" citStr="Huang et al., 2006" startWordPosition="126" endWordPosition="129">g@isi.edu Abstract Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank. 1 Introduction Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) typically employ a pipeline of two stages: a syntactic parser for the source language, and a decoder that translates source-language trees into target-language strings. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chineseto-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In</context>
<context position="10790" citStr="Huang et al., 2006" startWordPosition="1743" endWordPosition="1746">he feature weights to maximize labeledbracket F1. A technical challenge that arises is ensuring diversity in the n-best lists. The MER trainer requires that each list contain enough unique translations (when maximizing B ) or source trees (when maximizing labeled-bracket F1). However, because one source tree may lead to many translation derivations, the n-best list may contain only a few unique source trees, or in the extreme case, the derivations may all have the same source tree. We use a variant of the n-best algorithm that allows efficient generation of equivalence classes of derivations (Huang et al., 2006). The standard algorithm works by generating, at each node of the forest, a list of the best subderivations at that node; the variant drops a subderivation if it has the same source tree as a higherscoring subderivation. Train Dev Test System Objective Parsing Translation F1% B % Charniak n/a 78.42 n/a tree-to-string max-B forest-to-string max-B 78.42 23.07 77.75 24.60 forest-to-string max-F1 78.81 19.18 319 Maximum F1% LM data F1% Features F1% Parallel data F1% rule height (lines) (lines) 3 78.81 none 78.78 monolingual 78.89 60k 78.00 4 78.93 100 78.79 + bilingual 79.24 120k 78.16 5 79.14 30k</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. AMTA 2006, pages 65– 73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-constrained (monolingual) shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>1222--1231</pages>
<contexts>
<context position="3937" citStr="Huang et al., 2009" startWordPosition="614" endWordPosition="617">mputational Linguistics f f f parser decoder e e e e (a) f f f source source target string tree string f f f parser decoder e e e e (b)f f f source source target string forest string Figure 1: (a) In tree-to-string translation, the parser generates a single tree which the decoder must use to generate a translation. (b) In forest-to-string translation, the parser generates a forest of possible trees, any of which the decoder can use to generate a translation. Previous work has shown that an observed targetlanguage translation can improve parsing of sourcelanguage text (Burkett and Klein, 2008; Huang et al., 2009), but to our knowledge, only Chen et al. (2011) have explored the case where the target-language translation is unobserved. Below, we carry out experiments to test these two hypotheses. We measure the accuracy (using labeled-bracket F1) of the parses that the translation model selects, and find that they are worse than the parses selected by the parser. Our basic conclusion, then, is that the parses that help translation (according to B ) are, on average, worse parses. That is, forest-to-string translation hurts parsing. But there is a twist. Neither labeled-bracket F1 nor B is a perfect metri</context>
</contexts>
<marker>Huang, Jiang, Liu, 2009</marker>
<rawString>Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In Proc. EMNLP 2009, pages 1222–1231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for M-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. ICASSP</booktitle>
<pages>181--184</pages>
<contexts>
<context position="7265" citStr="Kneser and Ney, 1995" startWordPosition="1164" endWordPosition="1167">ts as a feature in the translation model. The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target 1The more common split, used by Bikel and Chiang (2000), has flaws that are described by Levy and Manning (2003). 318 Parsing Translation CTB 1–815 FBIS CTB 1101–1136 CTB 900–931 NIST 2002 CTB 1148–1151 CTB 816–885 NIST 2003 CTB 1137–1147 Table 1: Data used for training and testing the parsing and translation models. Table 2: Forest-to-string translation outperforms tree-tostring translation according to B , but the decreases parsing accuracy according to labeled-bracket F1. However, when we train to maximize labeled-bracket F1, forest-to-string</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. In Proc. ICASSP 1995, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese Treebank? In</title>
<date>2003</date>
<booktitle>Proc. ACL</booktitle>
<pages>439--446</pages>
<contexts>
<context position="7426" citStr="Levy and Manning (2003)" startWordPosition="1192" endWordPosition="1195">n. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target 1The more common split, used by Bikel and Chiang (2000), has flaws that are described by Levy and Manning (2003). 318 Parsing Translation CTB 1–815 FBIS CTB 1101–1136 CTB 900–931 NIST 2002 CTB 1148–1151 CTB 816–885 NIST 2003 CTB 1137–1147 Table 1: Data used for training and testing the parsing and translation models. Table 2: Forest-to-string translation outperforms tree-tostring translation according to B , but the decreases parsing accuracy according to labeled-bracket F1. However, when we train to maximize labeled-bracket F1, forest-to-string translation yields better parses than both tree-to-string translation and the original parser. side of the training data. We used minimum-errorrate (MER) traini</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher D. Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In Proc. ACL 2003, pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>609--616</pages>
<contexts>
<context position="858" citStr="Liu et al., 2006" startWordPosition="122" endWordPosition="125">es Institute chiang@isi.edu Abstract Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank. 1 Introduction Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) typically employ a pipeline of two stages: a syntactic parser for the source language, and a decoder that translates source-language trees into target-language strings. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chineseto-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees </context>
<context position="5055" citStr="Liu et al., 2006" startWordPosition="797" endWordPosition="800">anslation hurts parsing. But there is a twist. Neither labeled-bracket F1 nor B is a perfect metric of the phenomena it is meant to measure, and our translation system is optimized to maximize B . If we optimize our system to maximize labeled-bracket F1 instead, we find that our translation system selects parses that score higher than the baseline parser’s. That is, forest-tostring translation can help parsing. 2 Background We provide here only a cursory overview of treeto-string and forest-to-string translation. For more details, the reader is referred to the original papers describing them (Liu et al., 2006; Mi et al., 2008). Figure 1a illustrates the tree-to-string translation pipeline. The parser stage can be any phrasestructure parser; it computes a parse for each sourcelanguage string. The decoder stage translates the source-language tree into a target-language string, using a synchronous tree-substitution grammar. In forest-to-string translation (Figure 1b), the parser outputs a forest of possible parses of each source-language string. The decoder uses the same rules as in tree-to-string translation, but is free to select any of the trees contained in the parse forest. 3 Translation hurts p</context>
<context position="6751" citStr="Liu et al., 2006" startWordPosition="1079" endWordPosition="1082">ned it on the Chinese Treebank (CTB) 5.1, split as shown in Table 1, following Duan et al. (2007).1 The parser outputs a parse forest annotated with head words and other information. Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al., 2006) to (approximately) sum them out. As a side benefit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this forest is its posterior probability, given the input string. We retain these weights as a feature in the translation model. The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target 1The more common split, used by Bikel</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proc. COLING-ACL 2006, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proc. ACL</booktitle>
<pages>704--711</pages>
<contexts>
<context position="1833" citStr="Liu et al., 2007" startWordPosition="276" endWordPosition="279">eseto-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why. One hypothesis is that forest-to-string translation selects worse parses. Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system encounters the following correct tree (only partial bracketing shown): (1) [NP jingjieconomy ‘economic growth rate’ Suppose further that the model has never seen this phrase before, although it has seen the subphrase z¯engzhˇang de s`ud`u ‘growth rate’. Because this subphrase is not a syntactic unit in sentence (1), the system will be unable to translate it. But a forest-tostring system would be free to choose another (incorrect but plausible) bracketing: (2) jingjieconomy and successfully translate it using rules learned</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proc. ACL 2007, pages 704–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP</booktitle>
<pages>206--214</pages>
<contexts>
<context position="7085" citStr="Mi and Huang, 2008" startWordPosition="1131" endWordPosition="1134">efit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this forest is its posterior probability, given the input string. We retain these weights as a feature in the translation model. The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target 1The more common split, used by Bikel and Chiang (2000), has flaws that are described by Levy and Manning (2003). 318 Parsing Translation CTB 1–815 FBIS CTB 1101–1136 CTB 900–931 NIST 2002 CTB 1148–1151 CTB 816–885 NIST 2003 CTB 1137–1147 Table 1: Data used for training and testing the parsing and translation models. Table 2: Forest-to-string translation outperforms tr</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proc. EMNLP 2008, pages 206–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1475" citStr="Mi et al., 2008" startWordPosition="220" endWordPosition="223"> Huang et al., 2006) typically employ a pipeline of two stages: a syntactic parser for the source language, and a decoder that translates source-language trees into target-language strings. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chineseto-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why. One hypothesis is that forest-to-string translation selects worse parses. Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system encounters the following correct tree (only partial bracketing shown): (1) [NP jingjieconomy ‘economic growth rate’ Suppose further that the model has never seen this ph</context>
<context position="5073" citStr="Mi et al., 2008" startWordPosition="801" endWordPosition="804">rsing. But there is a twist. Neither labeled-bracket F1 nor B is a perfect metric of the phenomena it is meant to measure, and our translation system is optimized to maximize B . If we optimize our system to maximize labeled-bracket F1 instead, we find that our translation system selects parses that score higher than the baseline parser’s. That is, forest-tostring translation can help parsing. 2 Background We provide here only a cursory overview of treeto-string and forest-to-string translation. For more details, the reader is referred to the original papers describing them (Liu et al., 2006; Mi et al., 2008). Figure 1a illustrates the tree-to-string translation pipeline. The parser stage can be any phrasestructure parser; it computes a parse for each sourcelanguage string. The decoder stage translates the source-language tree into a target-language string, using a synchronous tree-substitution grammar. In forest-to-string translation (Figure 1b), the parser outputs a forest of possible parses of each source-language string. The decoder uses the same rules as in tree-to-string translation, but is free to select any of the trees contained in the parse forest. 3 Translation hurts parsing The simples</context>
<context position="6769" citStr="Mi et al., 2008" startWordPosition="1083" endWordPosition="1086">ese Treebank (CTB) 5.1, split as shown in Table 1, following Duan et al. (2007).1 The parser outputs a parse forest annotated with head words and other information. Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al., 2006) to (approximately) sum them out. As a side benefit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this forest is its posterior probability, given the input string. We retain these weights as a feature in the translation model. The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. We used the same features as Mi and Huang (2008). The language model was a trigram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), trained on the target 1The more common split, used by Bikel and Chiang (2000)</context>
<context position="8606" citStr="Mi et al., 2008" startWordPosition="1379" endWordPosition="1382">used minimum-errorrate (MER) training to optimize the feature weights (Och, 2003) to maximize B . At decoding time, we select the best derivation and extract its source tree. In principle, we ought to sum over all derivations for each source tree; but the approximations that we tried (n-best list crunching, max-rule decoding, minimum Bayes risk) did not appear to help. 3.2 Results Table 2 shows the main results of our experiments. In the second and third line, we see that the forestto-string system outperforms the tree-to-string system by 1.53 B , consistent with previously published results (Mi et al., 2008; Zhang et al., 2009). However, we also find that the trees selected by the forest-to-string system score much lower according to labeled-bracket F1. This suggests that the reason the forest-to-string system is able to generate better translations is that it can soften the constraints imposed by the syntax of the source language. 4 Translation helps parsing We have found that better translations can be obtained by settling for worse parses. However, translation accuracy is measured using B and parsing accuracy is measured using labeled-bracket F1, and neither of these is a perfect metric of th</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. ACL-08: HLT, pages 192– 199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="8072" citStr="Och, 2003" startWordPosition="1288" endWordPosition="1289">FBIS CTB 1101–1136 CTB 900–931 NIST 2002 CTB 1148–1151 CTB 816–885 NIST 2003 CTB 1137–1147 Table 1: Data used for training and testing the parsing and translation models. Table 2: Forest-to-string translation outperforms tree-tostring translation according to B , but the decreases parsing accuracy according to labeled-bracket F1. However, when we train to maximize labeled-bracket F1, forest-to-string translation yields better parses than both tree-to-string translation and the original parser. side of the training data. We used minimum-errorrate (MER) training to optimize the feature weights (Och, 2003) to maximize B . At decoding time, we select the best derivation and extract its source tree. In principle, we ought to sum over all derivations for each source tree; but the approximations that we tried (n-best list crunching, max-rule decoding, minimum Bayes risk) did not appear to help. 3.2 Results Table 2 shows the main results of our experiments. In the second and third line, we see that the forestto-string system outperforms the tree-to-string system by 1.53 B , consistent with previously published results (Mi et al., 2008; Zhang et al., 2009). However, we also find that the trees select</context>
<context position="9436" citStr="Och, 2003" startWordPosition="1520" endWordPosition="1521">enerate better translations is that it can soften the constraints imposed by the syntax of the source language. 4 Translation helps parsing We have found that better translations can be obtained by settling for worse parses. However, translation accuracy is measured using B and parsing accuracy is measured using labeled-bracket F1, and neither of these is a perfect metric of the phenomenon it is meant to measure. Moreover, we optimized the translation model in order to maximize B . It is known that when MER training is used to optimize one translation metric, other translation metrics suffer (Och, 2003); much more, then, can we expect that optimizing B will cause labeledbracket F1 to suffer. In this section, we try optimizing labeled-bracket F1, and find that, in this case, the translation model does indeed select parses that are better on average. 4.1 Setup MER training with labeled-bracket F1 as an objective function is straightforward. At each iteration of MER training, we run the parser and decoder over the CTB dev set to generate an n-best list of possible translation derivations (Huang and Chiang, 2005). For each derivation, we extract its Chinese parse tree and compute the number of b</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>433--440</pages>
<contexts>
<context position="6419" citStr="Petrov et al., 2006" startWordPosition="1021" endWordPosition="1024">than the parses selected by the parser. If they are worse, this supports the hypothesis that syntax can hurt translation. If they are better, we can conclude that translation can help parsing. In this initial experiment, we find that the former is the case. 3.1 Setup The baseline parser is the Charniak parser (Charniak, 2000). We trained it on the Chinese Treebank (CTB) 5.1, split as shown in Table 1, following Duan et al. (2007).1 The parser outputs a parse forest annotated with head words and other information. Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al., 2006) to (approximately) sum them out. As a side benefit, this improves parsing accuracy from 77.76% to 78.42% F1. The weight of a hyperedge in this forest is its posterior probability, given the input string. We retain these weights as a feature in the translation model. The decoder stage is a forest-to-string system (Liu et al., 2006; Mi et al., 2008) for Chinese-to-English translation. The datasets used are listed in Table 1. We generated word alignments with GIZA++ and symmetrized them using the grow-diag-finaland heuristic. We parsed the Chinese side using the Charniak parser as described abov</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. COLING-ACL 2006, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. ACL-08: HLT,</booktitle>
<pages>559--567</pages>
<contexts>
<context position="1854" citStr="Zhang et al., 2008" startWordPosition="280" endWordPosition="283">slation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why. One hypothesis is that forest-to-string translation selects worse parses. Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system encounters the following correct tree (only partial bracketing shown): (1) [NP jingjieconomy ‘economic growth rate’ Suppose further that the model has never seen this phrase before, although it has seen the subphrase z¯engzhˇang de s`ud`u ‘growth rate’. Because this subphrase is not a syntactic unit in sentence (1), the system will be unable to translate it. But a forest-tostring system would be free to choose another (incorrect but plausible) bracketing: (2) jingjieconomy and successfully translate it using rules learned from observed data. </context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. ACL-08: HLT, pages 559–567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
</authors>
<title>Forest-based tree sequence to string translation model.</title>
<date>2009</date>
<booktitle>In Proc. ACL-IJCNLP</booktitle>
<pages>172--180</pages>
<contexts>
<context position="8627" citStr="Zhang et al., 2009" startWordPosition="1383" endWordPosition="1386">rrate (MER) training to optimize the feature weights (Och, 2003) to maximize B . At decoding time, we select the best derivation and extract its source tree. In principle, we ought to sum over all derivations for each source tree; but the approximations that we tried (n-best list crunching, max-rule decoding, minimum Bayes risk) did not appear to help. 3.2 Results Table 2 shows the main results of our experiments. In the second and third line, we see that the forestto-string system outperforms the tree-to-string system by 1.53 B , consistent with previously published results (Mi et al., 2008; Zhang et al., 2009). However, we also find that the trees selected by the forest-to-string system score much lower according to labeled-bracket F1. This suggests that the reason the forest-to-string system is able to generate better translations is that it can soften the constraints imposed by the syntax of the source language. 4 Translation helps parsing We have found that better translations can be obtained by settling for worse parses. However, translation accuracy is measured using B and parsing accuracy is measured using labeled-bracket F1, and neither of these is a perfect metric of the phenomenon it is me</context>
</contexts>
<marker>Zhang, Zhang, Li, Aw, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proc. ACL-IJCNLP 2009, pages 172–180.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>