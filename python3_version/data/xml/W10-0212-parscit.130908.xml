<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.063578">
<title confidence="0.9969585">
Emotional Perception of Fairy Tales:
Achieving Agreement in Emotion Annotation of Text
</title>
<author confidence="0.943334">
Ekaterina P. Volkova1,2, Betty J. Mohler2, Detmar Meurers1, Dale Gerdemann1, Heinrich H. B¨ulthoff2
</author>
<affiliation confidence="0.793543">
1 Universit¨at T¨ubingen, Seminar f¨ur Sprachwissenschaft
</affiliation>
<address confidence="0.975783666666667">
19 Wilchelmstr., T¨ubingen, 72074, Germany
2 Max Planck Institute for Biological Cybernetics
38 Spemannstr., T¨ubingen, 72076, Germany
</address>
<sectionHeader confidence="0.968003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994880952381">
Emotion analysis (EA) is a rapidly developing
area in computational linguistics. An EA
system can be extremely useful in fields such
as information retrieval and emotion-driven
computer animation. For most EA systems,
the number of emotion classes is very limited
and the text units the classes are assigned
to are discrete and predefined. The question
we address in this paper is whether the set
of emotion categories can be enriched and
whether the units to which the categories
are assigned can be more flexibly defined.
We present an experiment showing how an
annotation task can be set up so that untrained
participants can perform emotion analysis
with high agreement even when not restricted
to a predetermined annotation unit and using
a rich set of emotion categories. As such it
sets the stage for the development of more
complex EA systems which are closer to the
actual human emotional perception of text.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999513">
As a first step towards developing an emotion
analysis (EA) system simulating human emotional
perception of text, it is important to research the
nature of the emotion analysis performed by humans
and examine whether they can reliably perform
the task. To investigate these issues, we conducted
an experiment to find out the strategies people
use to annotate selected folk fairy tale texts for
emotions. The participants had to choose from a set
of fifteen emotion categories, a significantly larger
</bodyText>
<page confidence="0.947397">
98
</page>
<bodyText confidence="0.998419695652174">
set than typically used in EA, and assign them to an
unrestricted range of text.
To explore whether human annotators can reliably
perform a task, inter-annotator agreement (IAA)
(Artstein and Poesio, 2008) is the relevant measure.
This measure can be calculated between every two
individual annotations in order to find pairs or even
teams of annotators whose strategies seem to be
consistent and coherent enough so that they can be
used further as the gold-standard annotation suited
to train a machine learning approach for automatic
EA analysis. A resulting EA system, capable of
simulating human emotional perception of text,
would be useful for information retrieval and many
other fields.
There are two main aspects of the resulting anno-
tations to be researched. First, how consistently can
people perceive and locate the emotional aspect of
fairy tale texts? Second, how do they express their
perception of text by means of annotation strategies?
In the next sections, we address these questions and
provide details of an experiment we conducted to
empirically advance our understanding of the issues.
</bodyText>
<sectionHeader confidence="0.92172" genericHeader="introduction">
2 Motivation and Aimed Application
</sectionHeader>
<bodyText confidence="0.994309444444445">
Most existing EA systems are implemented for and
used in specific predefined areas. The application
field could be anything from extracting appraisal
expressions (Whitelaw et al., 2005) to opinion
mining of customer feedback (Lee et al., 2008).
In our case, the intended application of the EA
system predominantly is emotion enhancement of
human-computer interaction, especially in virtual
or augmented reality. Emotion enhancement of
</bodyText>
<note confidence="0.9846845">
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98–106,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999480230769231">
computer animation, especially when it deals with
spoken or written text, is primarily done through
manual annotation of text, even if a rich database
of perceptually guided animations for behavioral
scripts compilation is available (Cunningham and
Wallraven, 2009). The resulting system of our
project is meant to be a bridge between unprocessed
input text (generated or provided) and visual and
auditory information, coming from the virtual
character, like generated speech, facial expressions
and body language. In this way a virtual character
would be able to simulate emotional perception and
production of text in story telling scenarios.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999946974358974">
Although EA is often referred to as a developing
field, the amount of work carried out during the last
decades is phenomenal. This section is not meant as
a full overview of the related research as that scope
is too great for the length of this paper. To contextu-
alize the research presented in this paper we focus on
the projects that inspired us and fostered the ideas.
The work done by Alm (Alm and Sproat, 2005;
Alm et al., 2005; Alm, 2008) is close to our
project in its sprit and goals. Alm, (2008) aims at
implementing affective text-to-speech system for
storytelling scenarios. An EA system, detecting
sentences with emotions expressed in written text
is a crucial element for achieving this goal. The
annotated corpus was composed of three sets of
children’s stories written by Beatrix Potter, H. C.
Andersen, and the Brothers Grimm.
Like Liu et al. (2003), Alm (2008) uses sev-
eral emotional categories, while most research in
automatic EA works with pure polarities. The set
of emotion categories used is essentially the list of
basic emotions (Ekman, 1993), which has a justified
preference for negative emotion categories. Ek-
mann’s list of basic emotions was extended by Alm,
since the emotion of surprise is validly taken as am-
bivalent and was thus split into positive surprise and
negative surprise. The EA system described in Alm
et al. (2005) is machine learning based, where the
EA problem is defined as multi-class classification
problem, with sentences as classification units.
Liu et al. (2003) have combined an emotion
lexicon and handcrafted rules, which allowed them
to create affect models and thus form a representa-
tion of the emotional affinity of a sentence. Their
annotation scheme is also sentence-based. The
EA system was tested on short user-composed text
emails describing emotionally colored events.
In the research on recognizing contextual polarity
done by Wilson et al. (2009) a rich prior-polarity
lexicon and dependency parsing technique were
employed to detect and analyze subjectivity on
phrasal level, taking into account all the power of
context, captured through such features as negation,
polarity modification and polarity shifters. The
work presents auspicious results of high accuracy
scores for classification between neutrality and
polarized private states and between negative and
positive subjective phrases. A detailed account
of several ML algorithms performance tests is
discussed in thought-provoking manner. This work
encouraged us to build a lexicon of subjective clues
and use sentence structure information for future
feature extraction and ML architecture training.
Another thought-provoking work by Polanyj
(2006) shows the influence of the context on subjec-
tive clues. This is relevant to our project since we
are collecting lexicons of subjective clues and the
mechanisms of contextual influence may prove to
be of value for future automatic EA system training.
Bethard et at. (2004) provide valuable informa-
tion about corpus annotation for EA means and give
accounts on the performance of various existing ML
algorithms. They provide excellent analysis of au-
tomatic extraction of opinion proposition and their
holders. For feature extraction, the authors employ
such well-known resources as WordNet (Miller et
al., 1990), PropBank (Kingsbury et al., 2002) and
FrameNet (Baker et al., 1998). Several types of
classification tasks involve evaluation on the level
of documents. For example, detecting subjective
sentences, expressions, and other opinionated items
in documents representing certain press categories
(Wiebe et al., 2004) and measuring strength of
subjective clauses (Wilson et al., 2004). All these
and many more helped us to decide upon our own
strategies, provided many examples of corpus col-
lection and annotation, feature extraction and ML
techniques usage in ways specific for the EA task.
</bodyText>
<page confidence="0.999176">
99
</page>
<sectionHeader confidence="0.998833" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999984611111111">
Having established the research context, we now
turn to the questions we investigate in this paper:
the use of an enriched category set and the flexible
annotation units, and their influence on annotation
quality. We describe the experiment we conducted
and its main results. Each participant performed
several tasks for each session. The first task always
was a cognitive task on emotion categories taken
outside the fairy tales context. The results are dis-
cussed in Sections 4.1 and 4.2. The next assignment
discussed in Section 4.3 was to annotate a list of
words for their inherent polarities. The third task
was to read the text out loud to the experimenter.
This allowed the participant to feel immersed into
the story telling scenario and also get used to the
text of the story they were about to annotate for
the full set of emotion categories. The annotation
process is described in Section 4.4. The last exercise
was to read the full fairy tale text out loud again,
with the difference that this time their voice and
face were recorded by means of a microphone and a
camera. The potential importance of the extra data
sources like speech melody and facial expressions
are further discussed in Section 8 as future work.
Ten German native speakers voluntarily partic-
ipated in the experiment. The participants were
divided into two groups and each participant worked
on five of the eight texts. The fairy tale sets for each
group overlapped in two texts, which allowed us to
achieve a high number of individual annotations in a
short amount of time and compare the performance
of people working on different sets of texts (see
Table 1). Each participant annotated their texts in
five sessions, dealing with only one text per session.
The fatigue effect was avoided as no annotator had
more than one session a day.
</bodyText>
<subsectionHeader confidence="0.998539">
4.1 Determining Emotion Categories
</subsectionHeader>
<bodyText confidence="0.999846125">
First, we needed to define the set of emotions to
be used in the experiment. Based on the current
emotion theories from comparative literature and
cognitive psychology (Ekman, 1993; Auracher,
2007; Fontaine et al., 2007), we compiled a set of
fifteen emotion categories: seven positive, seven
negative, and neutral (see Table 2). We chose an
equal number of negative and positive emotions,
</bodyText>
<table confidence="0.3548495">
User Fairy Tale ID
JG D R BR FH DS BM SJ
</table>
<equation confidence="0.9669541">
A1 • • • • •
A2 • • • • •
A3 • • • • •
A4 • • • • •
A5 • • • • •
As • • • • •
A7 • • • • •
As • • • • •
A9 • • • • •
A10 • • • • •
</equation>
<tableCaption confidence="0.998134">
Table 1: Annotation Sets
</tableCaption>
<table confidence="0.999796625">
Positive Negative
Entspannung (relief) Unruhe (disturbance)
Freude (joy) Trauer (sadness)
Hoffnung (hope) Verzweiflung ( despair)
Interesse (interest) Ekel (disgust)
Mitgef¨uhl (compassion) Hass (hatred)
¨Uberraschung (surprise) Angst (fear)
Zustimmung (approval) ¨Arger (anger)
</table>
<tableCaption confidence="0.999374">
Table 2: Emotion Categories Used in the Experiment
</tableCaption>
<bodyText confidence="0.9999621">
since in our experiment the main focus is on the
freedom and equality of choice of emotion cate-
gories. We aimed at the set to be comprehensive and
we also expected the participants to be able to detect
each of the emotions in the text as well as express
them through speech melody and facial expressions.
The polarity of each category was determined
experimentally. Participants were asked to decide
on the underlying polarity of each emotion category
and then to evaluate each emotion on an intensity
scale [1:5], ‘5’ marking extreme polarization, ‘1’
being close to neutral. All participants were in full
agreement concerning the underlying polarity of
the emotions in the set, while the numerical values
varied. It is important to note, that the category
¨Uberraschung (surprise) was stably estimated as
positive. In English the word surprise is reported
to be ambivalent (Alm and Sproat, 2005), but we
found that in German its most common translation
is clearly positive.
</bodyText>
<subsectionHeader confidence="0.965492">
4.2 Emotion Categories Clustering
</subsectionHeader>
<bodyText confidence="0.999709666666667">
In the second part of the experiment we asked partic-
ipants to organize the fifteen emotions into clusters.
Each cluster was to represent a situation in which
</bodyText>
<page confidence="0.74558">
100
</page>
<table confidence="0.851419222222222">
Cluster Polarity
{relief, hope, joy} positive
{joy, surprise} positive
{joy, approval} positive
{approval, interest} positive
{disgust, anger, hatred} negative
{fear, despair, disturbance} negative
{fear, disturbance, sadness} negative
{sadness, compassion} mixed
</table>
<tableCaption confidence="0.989337">
Table 3: Emotion Clusters
</tableCaption>
<table confidence="0.999890666666667">
German Title English Title Abbr.
Arme Junge im Grab Poor Boy in Grave JG
Bremer Stadtmusikanten Bremen Musicians BM
Dornr¨oschen Little Briar-Rose BR
Eselein Donkey D
Frau Holle Mother Hulda FH
Heilige Joseph im Walde St. Joseph in Forest SJ
Hund und Sperling Dog and Sparrow DS
R¨atsel Riddle R
</table>
<tableCaption confidence="0.999845">
Table 4: Stories Used (the titles are shortened)
</tableCaption>
<bodyText confidence="0.999589045454545">
several emotions were equally likely to co-occur,
e.g. a situation formulated by a participant as “When
a friend gives me a nicely wrapped birthday present
and I am about to open it.” was reported to involve
such emotions as joy, interest and surprise. On
average, each participant has formed 5 clusters with
3–4 items per cluster. The clusters were encoded as
sets on unordered pairs of items. Pairs were filtered
out if they were indicated by fewer than seven par-
ticipants. As the result, the following eight clusters
were obtained (see Table 3). For most clusters, the
categories composing them share one polarity. The
{sadness, compassion} cluster is the only exception.
It is important to note that the clusters were
determined through this cognitive task, indepen-
dently of the annotations. Since the annotators
agree well on clustering the emotions, employing
this information captures conceptual agreement
between individual annotations even if the specific
emotion categories for the same stretch of text do
not coincide. However, we intend to keep the full
set of emotions for the future corpus expansions.
</bodyText>
<subsectionHeader confidence="0.999193">
4.3 Word list Annotation
</subsectionHeader>
<bodyText confidence="0.999963151515152">
For each text, we compiled its word list by taking the
set of words contained in the text, normalizing each
word to its lemma and filtering the set for most com-
mon German stop words (function words, pronouns,
auxiliaries). Like full story texts, word lists were
divided into two annotation sets. At each session,
before seeing the full text of the fairy tale, the partic-
ipant was to annotate each item of the corresponding
word list for its inherent polarity. All the words were
taken out their contexts and were neutral by default.
The annotator’s task was to label only those words
that had the potential to change the polarity of the
context in which they could occur. We purposefully
did not limit the task to the words occurring in all
texts in order to be able to investigate the stability
of participants’ decisions. Every annotator worked
with five word lists, one for each fairy tale text. The
total number of unique items for the first annotation
set was 893 words and 823 words long for the
second set; 267 and 236 words correspondingly
occurred in more than one word list. These words
could potentially be marked with different polarity
categories, but in fact only about 15% of those
words (4% from the total number of items on each
of the word lists) were “unstable”, namely, labeled
with different polarities by the same annotator. The
labels received in these cases were either {positive,
neutral} or {negative, neutral}. These words were
further “stabilized” by either choosing the most
frequent label or the neutral label if the unstable
word had received only two label instances. The
results show that such annotation tasks could be
used further for subjective clues lexicon collection.
</bodyText>
<subsectionHeader confidence="0.993602">
4.4 Text Annotation
</subsectionHeader>
<bodyText confidence="0.999857">
For the third and main part of the experiment, we
selected eight Grimm’s fairy tales, each 1200 – 1400
words long and written in Standard German (see
Table 4). The texts were chosen based on their
genre, for in spite of the depth of all the hidden
and open references to human psyche and national
traditions that were shown in works of (von Franz,
1996; Propp and Dundes, 1977), folk fairy tales
are relatively uncomplicated in the plot-line and
the characters’ personalities. Due to this relative
simplicity of the content, we expect the participants’
emotional reactions to folk fairy tale texts to be more
coherent than to other texts of fiction literature.
The task for the participants was to locate and
mark stretches of text where an emotion was to be
</bodyText>
<page confidence="0.995375">
101
</page>
<bodyText confidence="0.999978411764706">
conveyed through the speech melody and/or facial
expressions if the participant was to read the text
out loud. To make the annotation process and its
further analysis time-efficient and convenient for
both, annotators and experimenters, a simple tool
was developed. We created the Manual Emotion
Annotation Tool (MEAT) which allows the user
to annotate text for emotion by selecting stretches
of text and labeling it with one of fifteen emotion
categories. The application also has a special mode
for word list annotation, where only the three
polarity categories are available: positive, negative
and neutral. The user can always undo their labels
or change them until they are satisfied with the
annotation and can submit the results. The main
part of the experiment resulted in fifty individual
annotations which produced 150 annotation pairs.
</bodyText>
<sectionHeader confidence="0.98781" genericHeader="method">
5 Analyzing Inter-annotator Agreement
</sectionHeader>
<bodyText confidence="0.9998771">
For each of the 150 pairs (two texts annotated
by ten annotators, six texts annotated by five
annotators), the IAA rate was calculated. However,
the calculation of IAA is not as straightforward
in this situation as it might seem. In many types
of corpus annotation, e.g., in POS tagging, there
are previously identified discrete elements. In this
experiment we intentionally have no predefined
units, even if this makes the IAA calculation more
difficult. Consider the following examples:
</bodyText>
<listItem confidence="0.99941575">
(1) A1: “... [the evil wolf]X ate the girl”
A2: “... the [evil wolf ate the girl]X”
(2) A1: “... [the evil wolf]X ate the girl”
A2: “... [the evil wolf]Y ate the girl”
(3) A1: “... [the evil wolf]X ate the girl”
A2: “...the evil wolf ate [the girl]X”
(4) A1: “... [the evil wolf]X ate [the girl]Z”
A2: “... [the evil wolf ate the girl]X”
</listItem>
<bodyText confidence="0.999807142857143">
In example (1) both annotators marked certain
stretches of text with the same category X, but the
annotations do not completely coincide, there is
only an overlap. This situation is similar to that in
syntactic annotation, where one needs to distinguish
between bracketing and labeling of the constituent
and measures such as Parseval (Carroll et al., 2002)
have been much debated.
Both annotators in example (1) recognize evil
wolf as marked for X and thus this example should
be counted towards agreement, while examples (2)
and (3) should not. A second type of evaluation
arises if the emotion clusters are taken into account.
According to this evaluation type, example (2) is
counted towards agreement if the categories X and
Y belong to the same cluster.
Example (4) provides an illustration of how IAA
is accounted for in a more complex case. Annotator
A1 has marked two stretches of text with two
different emotion categories, while annotator A2
has united both stretches under the same emotion
category. Both annotators agree that the evil wolf is
marked for X, but disagree on the emotion category
for the girl. In order to avoid the crossing brackets
problem (Carroll et al., 2002), we treat the evil
wolf ate as agreement, and the girl as disagree-
ment. Although ate was left unmarked by one of
the annotators, it is counted towards agreement
because it is next to a stretch of text on which both
annotators agree. Stretches of text the annotators
agree or disagree upon also receive weight values:
the higher the number of words that belong to open
word classes in a stretch, the higher its weight.
The general calculation formulae for the IAA
measure are taken from (Artstein and Poesio, 2008):
</bodyText>
<equation confidence="0.992870571428571">
Ao − Ae
1 − Ae
1 � Ao = i
iEI
1
Ae = I2
kEK
</equation>
<bodyText confidence="0.999938">
Ao is the observed agreement, Ae is the expected
agreement, I is the number of annotation items, K
is the set of all categories used by both annotators,
nck is the number of items assigned by annotator c
to category k.
</bodyText>
<sectionHeader confidence="0.959459" genericHeader="method">
6 Analyzing Annotation Strategies
</sectionHeader>
<bodyText confidence="0.999732555555556">
Analysis of IAA, presented in Section 5 can answer
the first question we aim to investigate: How consis-
tently do people perceive and locate the emotional
aspect of fairy tale texts? The second issue nec-
essary for investigation is the annotation strategies
people use to express their emotional perception
of text. In our experiment conditions, the resulting
strategies can be investigated via three aspects:
a) length of user-defined flexible units b) emotional
</bodyText>
<equation confidence="0.979902666666667">
K =
argi
nc1knc2k
</equation>
<page confidence="0.995556">
102
</page>
<figureCaption confidence="0.999792">
Figure 1: Annotator Defined Unit Length Rating
</figureCaption>
<bodyText confidence="0.999580088235294">
composition of fairy tales c) emotional flow of the
fairy tales. In this section we give a brief account of
our findings concerning the given aspects.
The participants were always free to select text
stretches of the length they considered to be appro-
priate for a specific emotional category label. The
only guideline they received was to mark the entire
stretch of text which, according to their judgement,
was marked by the chosen emotion category and,
if read without the surrounding context, would
still allow one to clearly perceive the applied
emotion category label. As Figure 1 shows, the
most frequent unit length consists of four to seven
word tokens, which corresponds to short phrases,
e.g., a verb phrase with a noun phrase argument.
We consider the findings to be encouraging, since
this observation could be used favorably for the
automatic EA system training.
Emotional composition of a fairy tale helps to re-
veal the overall character of the text and establish
if the story is abundant with various emotions or is
overloaded with only a few. For our overall research
goal, we would prefer the former kind of stories,
since they would build a rich training corpus. Fig-
ures 2 and 3 give an overview on the average shares
various emotion categories hold over the eight texts.
It is important to note that 65%– 75% of the text was
left neutral. The results show that most stories are
rich in positive rather than negative emotions, with
two exceptions we would like to elaborate upon. The
stories The Poor Boy in the Grave and The Dog
and the Sparrow belonged to different annotation
sets and thus no annotator dealt with both stories.
These texts were selected partially for their potential
</bodyText>
<figureCaption confidence="0.999984">
Figure 2: Distribution of Positive Emotion Categories in Texts
Figure 3: Distribution of Negative Emotion Categories in Texts
</figureCaption>
<bodyText confidence="0.999905954545455">
overcharge with negative emotions. The hypothesis
proved to be true, since the annotators have labeled
on average 20% of text with negative emotions, like
hatred and sadness. The only positive emotion cate-
gory salient for the The Poor Boy in the Grave story
is compassion, which is also mostly triggered by sad
events happening to a positive character.
The emotional flow in the fairy tales is illustrated
by the graph presented in Figure 4. In order to build
it, we used the numerical evaluations obtained in
the first part of the experiment and described in
section 4.1. For each fairy tale text, each word token
was mapped to the absolute value of the average
numerical evaluation of its emotional categories
assigned by all participants. The word tokens also
received its relative position in the text, where the
first word was at position 0.0 and the last at 1.0.
Thus, the emotional trajectories of all texts were
correlated despite the fact that their actual lengths
differed. The polynomial fit graph, taken over thus
acquired emotional flow common for all fairy tale
texts has a wave-shaped form and is similar to the
</bodyText>
<figure confidence="0.999623285714286">
Unit length Frequency (%)
10%
4%
9%
8%
7%
6%
5%
3%
2%
0%
1%
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Unit Length (in word tokens)
25%
20%
15%
10%
5%
0%
JG DS BR BT SJ D FH R
approval
compassion
hope
interest
joy
relief
surprise
25%
20%
15%
10%
5%
0%
anger
despair
disgust
disturbance
fear
hatered
sadness
JG DS BR BT SJ D FH R
</figure>
<page confidence="0.931287">
103
</page>
<figureCaption confidence="0.999686">
Figure 4: Emotional Trajectory over all Stories
</figureCaption>
<bodyText confidence="0.999981318181818">
emotional trajectory reported by Alm and Sproat
(2005). The emotional charge increases and falls
steeply in the beginning of the fairy tale, then cycles
though rise and fall phases (which do not exceed
in their intensity the average rate of 0.6) and then
ascents steeply at the end of the story. We agree with
the explanation of such a trajectory, given by Propp
and Dundes (1977) and also elaborated by Alm and
Sproat (2005) — the first emotional intensity peak
in the story line corresponds to the rising action,
after the main characters have been introduced and
the plot develops through a usually unexpected
event. At the end of the story the intensity is high-
est, regardless whether the denouement is a happy
ending or a tragedy. The fact that the fairy tale texts
we chose for the experiment are relatively short is
probably responsible for the steep peak of intensity
in the very beginning of the story — the stories are
too short to include a proper exposition. However,
we need to investigate further how much of this is a
property of texts themselves and how much — the
perception (and thus annotation) of emotions.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999987296296297">
The IAA scores were calculated using the emotion
clusters information, for according to the results,
participants would often stably use different emo-
tions from same clusters at the same stretch of text.
Four out of ten participants, two from each
group (marked gray in Table 1), had very low IAA
scores (n &lt; 0.40 average per participant), a high
proportion of unmarked text, and they used few
emotion categories ( &lt; 7 categories average per
participant), so for the evaluation part their data was
discarded. The final IAA evaluation was calculated
on all the annotation pairs obtained from the six
remaining participants (marked black in table 1),
whose average agreement score in the original set
of participants was originally higher than 0.50. The
total number of annotation pairs amounted to 48:
two texts annotated by all the six annotators, six
texts annotated by three annotators for each of the
two annotation sets.
According to the interpretation of n by (Landis
and Koch, 1977), the annotator agreement was mod-
erate on average (0.53), and some pairs approached
the almost perfect IAA rate (0.83). The IAA rates,
calculated on the full set of fifteen emotions, with-
out taking the emotion clusters into consideration,
gave a moderate IAA rate on average (0.34) and
reached substantial level (0.62) at maximum. The
n rates are considerably high for the hard task and
are comparable with the results presented in (Alm
and Sproat, 2005). The word lists have a somewhat
lower n IAA (0.45 on average, 0.72 at maximum),
which is due to the low number of categories and
the heavy bias towards the neutral category. The
observed agreement on word lists is considerably
high: 0.81 on average, reaching 0.91 at maximum.
While our approach may seem very similar to
the one of Alm (2005), there are some important
differences. We gave the participants the freedom of
using flexible annotation units, which allowed the
annotators to define the source of emotion more pre-
cisely and mark several emotions in one sentence. In
fact, in 39% of all annotated sentences represented a
mixture of the neutral category and “polarized” cat-
egories, 20% of which included more than one “po-
larized” categories. Another difference is the rich set
of emotion categories, with equal number of positive
and negative items. The results show that people can
successfully use the large set to express their emo-
tional perception of text (e.g., see Figures 3 and 2).
Other important findings include the fact that
short phrases are the naturally preferred annotation
unit among our participants and that the emotional
trajectory of a general story line corresponds to the
one proposed by Propp and Dundes (1977).
</bodyText>
<figure confidence="0.99703675">
Emotional response [ ru]
0.8
0.6
0.4
0.2
0.0
1 .6
1 .4
1 .2
1 .0
0.0 0.2 0.4 0.6 0.8 1 .0
Story progress [ru]
</figure>
<page confidence="0.993008">
104
</page>
<sectionHeader confidence="0.998958" genericHeader="discussions">
8 Future Work
</sectionHeader>
<subsectionHeader confidence="0.998403">
8.1 Corpus Expansion
</subsectionHeader>
<bodyText confidence="0.999966216216216">
In the near future, we will expand the collections
of annotated text in order to compile a substantially
large training corpus. We plan to work further
with three annotators that have formed a natural
team, since their group has always attained the
highest annotation scores for their annotation set,
exceeding the highest scores in the other annotation
set. The task defined for the three annotators is
similar to the experiment described in the paper,
with several differences. For the corpus expansion
we chose 85 stories by the Grimm Brothers 1400
– 4500 tokens long. We expect that longer texts
have more potential space for an emotionally rich
plot. Each text will be annotated by two people,
the third annotator will tie-break disagreements by
choosing the most appropriate of the conflicting
categories, similar to the method described by (Alm
and Sproat, 2005). It is also probable that a basic
annotation unit will be defined and imposed on the
annotators, for, as the studies discussed in Section 6
show, short phrases are a language unit most often
naturally chosen by annotators.
Each of the annotators will also work with a sin-
gle word list, compiled from all texts and filtered for
the most common stop-words. Each of the words on
the word list should be annotated with its inherent
polarity (positive, negative or neutral). Since each
word on the list is free of its context, the lists
provide valuable information about the word and its
context interaction in full texts, which can be further
used for machine learning architecture training.
We also plan to keep the fifteen emotion cat-
egories and their clustering, since it gives the
annotator more freedom of expression and simulta-
neously allows the researches to find the common
cognitive ground behind the labels if they vary
within one cluster
</bodyText>
<subsectionHeader confidence="0.9905345">
8.2 Feature Extraction and Machine Learning
Architecture Training
</subsectionHeader>
<bodyText confidence="0.999966909090909">
When the corpus is large enough, the relevant
features will be extracted automatically by means
of existing NLP tools, followed by training a ma-
chine learning architecture, most probably TiMBL
(Daelemans et al., 2004), to map textual units to
the emotion categories. It is yet to be determined
which features to use, one compulsory parameter
is that all the features should be available through
automatic processing tools. This is crucial, since
the resulting EA system has to be fully automated
with no manual work involved.
</bodyText>
<subsectionHeader confidence="0.9944735">
8.3 Extra Information Sources and their
Potential Contribution
</subsectionHeader>
<bodyText confidence="0.999976047619048">
We also plan to collect data from other information
sources, like video and audio recordings, by inviting
amateur actors for story-telling sessions. This will
allow emotion retrieval from the speech melody,
facial expressions and body language. The manual
annotation and the extra data sources can be aligned
by means of Text and Speech Aligner (Rapp, 1995),
which allows to track correspondences between
them. This alignment would most certainly ben-
efit the facial and body animation of the virtual
characters, since there is no clear understanding
of time correlation between emotions labeled in
written text and the ones expressed through speech
and facial clues in a story telling scenario. An EA
system could also be perfected through a careful
analysis of recorded speech and video of story
telling sessions — regular recurrence of subjectivity
of certain contexts will be even more significant
if the transmission of the emotions from the story
teller to the listener via mentioned information
sources is successful.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999968722222222">
In this paper, we reported on an experiment inves-
tigating the inter-annotator agreement levels which
can be achieved by untrained human annotators per-
forming emotion analysis of variable units of text.
While EA is a very difficult task, our experiment
shows that even untrained annotators can have high
agreement rates, even given considerable freedom
in expressing their emotional perception of text. To
the best of our knowledge, this is the first attempt at
emotion analysis that operates on flexible, annotator
defined units and uses a relatively rich inventory of
emotion categories. We consider the resulting IAA
rates to be high enough to accept the annotations
as suitable for gold-standard corpus compilation in
the frame of this research. As such, we view this
work as the first step towards the development of a
more complex EA system, which aims to simulate
the actual human emotional perception of text.
</bodyText>
<page confidence="0.998977">
105
</page>
<sectionHeader confidence="0.995892" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923648351649">
C.O. Alm and R. Sproat. 2005. Emotional sequencing
and development in fairy tales. In Proceedings of the
First International Conference on Affective Computing
and Intelligent Interaction (ACII05). Springer.
C.O. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion predic-
tion. In Proceedings of HLT/EMNLP, volume 2005.
C.O. Alm. 2008. Affect in Text and Speech.
lrc.cornell.edu.
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555–596.
Jan Auracher. 2007.... wie auf den allm¨achtigen Schlag
einer magischen Rute. Psychophysiologische Messun-
gen zur Textwirkung. Ars poetica ; 3. Dt. Wiss.-Verl.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86–90. Association for
Computational Linguistics Morristown, NJ, USA.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic
extraction of opinion propositions and their holders. In
2004 AAAI Spring Symposium on Exploring Attitude
and Affect in Text, page 2224.
J. Carroll, A. Frank, D. Lin, D. Prescher, and H. Uszkor-
eit. 2002. Beyond Parseval-Towards improved evalua-
tion measures for parsing systems. In Workshop at the
3rd International Conference on Language Resources
and Evaluation LREC-02., Las Palmas.
D. W. Cunningham and C. Wallraven. 2009. Dynamic
information for the recognition of conversational ex-
pressions. Journal of Vision, 9(13:7):1–17, 12.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. ilk techni-
cal report 04-02. Technical report.
P. Ekman. 1993. Facial Expression and Emotion. Amer-
ican Psychologist, 48(4):384–392.
JR Fontaine, KR Scherer, EB Roesch, and PC Ellsworth.
2007. The world of emotions is not two-dimensional.
Psychological science: a journal of the American Psy-
chological Society/APS, 18(12):1050.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
semantic annotation to the Penn Treebank. In Pro-
ceedings of the Human Language Technology Confer-
ence, pages 252–256. Citeseer.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159–174.
D. Lee, O.R. Jeong, and S. Lee. 2008. Opinion min-
ing of customer feedback data on the web. In Pro-
ceedings of the 2nd international conference on Ubiq-
uitous information management and communication,
page 230235, New York, New York, USA. ACM.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge. In Proceedings of the 8th international
conference on Intelligent user interfaces - IUI ’03,
page 125, New York, New York, USA. ACM Press.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Introduction to Wordnet: An on-
line lexical database*. International Journal of lexi-
cography, 3(4):235.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing Attitude and Affect in Text: The-
ory and Applications, page 110.
V.I.A. Propp and A. Dundes. 1977. Morphology of the
Folktale. University of Texas Press.
S. Rapp. 1995. Automatic phonemic transcription and
linguistic annotation from known text with Hidden
Markov Models. In Proceedings of ELSNET Goes
East and IMACS Workshop. Citeseer.
M.L. von Franz. 1996. The interpretation offairy tales.
Shambhala Publications.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proceedings
of the 14th ACM international conference on Informa-
tion and knowledge management, page 631. ACM.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.
2004. Learning subjective language. Computational
linguistics, 30(3):277–308.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the National Conference on Artificial
Intelligence, pages 761–769. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399433, September.
</reference>
<page confidence="0.997324">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.379174">
<title confidence="0.9722825">Emotional Perception of Fairy Achieving Agreement in Emotion Annotation of Text</title>
<author confidence="0.870134">P Betty J Detmar Dale Heinrich H</author>
<note confidence="0.6478185">1Universit¨at T¨ubingen, Seminar f¨ur 19 Wilchelmstr., T¨ubingen, 72074, 2Max Planck Institute for Biological 38 Spemannstr., T¨ubingen, 72076, Germany</note>
<abstract confidence="0.999548818181818">Emotion analysis (EA) is a rapidly developing area in computational linguistics. An EA system can be extremely useful in fields such as information retrieval and emotion-driven computer animation. For most EA systems, the number of emotion classes is very limited and the text units the classes are assigned to are discrete and predefined. The question we address in this paper is whether the set of emotion categories can be enriched and whether the units to which the categories are assigned can be more flexibly defined. We present an experiment showing how an annotation task can be set up so that untrained participants can perform emotion analysis with high agreement even when not restricted to a predetermined annotation unit and using a rich set of emotion categories. As such it sets the stage for the development of more complex EA systems which are closer to the actual human emotional perception of text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C O Alm</author>
<author>R Sproat</author>
</authors>
<title>Emotional sequencing and development in fairy tales.</title>
<date>2005</date>
<booktitle>In Proceedings of the First International Conference on Affective Computing and Intelligent Interaction (ACII05).</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="4695" citStr="Alm and Sproat, 2005" startWordPosition="727" endWordPosition="730">like generated speech, facial expressions and body language. In this way a virtual character would be able to simulate emotional perception and production of text in story telling scenarios. 3 Related Work Although EA is often referred to as a developing field, the amount of work carried out during the last decades is phenomenal. This section is not meant as a full overview of the related research as that scope is too great for the length of this paper. To contextualize the research presented in this paper we focus on the projects that inspired us and fostered the ideas. The work done by Alm (Alm and Sproat, 2005; Alm et al., 2005; Alm, 2008) is close to our project in its sprit and goals. Alm, (2008) aims at implementing affective text-to-speech system for storytelling scenarios. An EA system, detecting sentences with emotions expressed in written text is a crucial element for achieving this goal. The annotated corpus was composed of three sets of children’s stories written by Beatrix Potter, H. C. Andersen, and the Brothers Grimm. Like Liu et al. (2003), Alm (2008) uses several emotional categories, while most research in automatic EA works with pure polarities. The set of emotion categories used is</context>
<context position="11846" citStr="Alm and Sproat, 2005" startWordPosition="1895" endWordPosition="1898">lody and facial expressions. The polarity of each category was determined experimentally. Participants were asked to decide on the underlying polarity of each emotion category and then to evaluate each emotion on an intensity scale [1:5], ‘5’ marking extreme polarization, ‘1’ being close to neutral. All participants were in full agreement concerning the underlying polarity of the emotions in the set, while the numerical values varied. It is important to note, that the category ¨Uberraschung (surprise) was stably estimated as positive. In English the word surprise is reported to be ambivalent (Alm and Sproat, 2005), but we found that in German its most common translation is clearly positive. 4.2 Emotion Categories Clustering In the second part of the experiment we asked participants to organize the fifteen emotions into clusters. Each cluster was to represent a situation in which 100 Cluster Polarity {relief, hope, joy} positive {joy, surprise} positive {joy, approval} positive {approval, interest} positive {disgust, anger, hatred} negative {fear, despair, disturbance} negative {fear, disturbance, sadness} negative {sadness, compassion} mixed Table 3: Emotion Clusters German Title English Title Abbr. Ar</context>
<context position="24024" citStr="Alm and Sproat (2005)" startWordPosition="3943" endWordPosition="3946"> the fact that their actual lengths differed. The polynomial fit graph, taken over thus acquired emotional flow common for all fairy tale texts has a wave-shaped form and is similar to the Unit length Frequency (%) 10% 4% 9% 8% 7% 6% 5% 3% 2% 0% 1% 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 Unit Length (in word tokens) 25% 20% 15% 10% 5% 0% JG DS BR BT SJ D FH R approval compassion hope interest joy relief surprise 25% 20% 15% 10% 5% 0% anger despair disgust disturbance fear hatered sadness JG DS BR BT SJ D FH R 103 Figure 4: Emotional Trajectory over all Stories emotional trajectory reported by Alm and Sproat (2005). The emotional charge increases and falls steeply in the beginning of the fairy tale, then cycles though rise and fall phases (which do not exceed in their intensity the average rate of 0.6) and then ascents steeply at the end of the story. We agree with the explanation of such a trajectory, given by Propp and Dundes (1977) and also elaborated by Alm and Sproat (2005) — the first emotional intensity peak in the story line corresponds to the rising action, after the main characters have been introduced and the plot develops through a usually unexpected event. At the end of the story the intens</context>
<context position="26549" citStr="Alm and Sproat, 2005" startWordPosition="4371" endWordPosition="4374">by all the six annotators, six texts annotated by three annotators for each of the two annotation sets. According to the interpretation of n by (Landis and Koch, 1977), the annotator agreement was moderate on average (0.53), and some pairs approached the almost perfect IAA rate (0.83). The IAA rates, calculated on the full set of fifteen emotions, without taking the emotion clusters into consideration, gave a moderate IAA rate on average (0.34) and reached substantial level (0.62) at maximum. The n rates are considerably high for the hard task and are comparable with the results presented in (Alm and Sproat, 2005). The word lists have a somewhat lower n IAA (0.45 on average, 0.72 at maximum), which is due to the low number of categories and the heavy bias towards the neutral category. The observed agreement on word lists is considerably high: 0.81 on average, reaching 0.91 at maximum. While our approach may seem very similar to the one of Alm (2005), there are some important differences. We gave the participants the freedom of using flexible annotation units, which allowed the annotators to define the source of emotion more precisely and mark several emotions in one sentence. In fact, in 39% of all ann</context>
<context position="28811" citStr="Alm and Sproat, 2005" startWordPosition="4752" endWordPosition="4755"> highest annotation scores for their annotation set, exceeding the highest scores in the other annotation set. The task defined for the three annotators is similar to the experiment described in the paper, with several differences. For the corpus expansion we chose 85 stories by the Grimm Brothers 1400 – 4500 tokens long. We expect that longer texts have more potential space for an emotionally rich plot. Each text will be annotated by two people, the third annotator will tie-break disagreements by choosing the most appropriate of the conflicting categories, similar to the method described by (Alm and Sproat, 2005). It is also probable that a basic annotation unit will be defined and imposed on the annotators, for, as the studies discussed in Section 6 show, short phrases are a language unit most often naturally chosen by annotators. Each of the annotators will also work with a single word list, compiled from all texts and filtered for the most common stop-words. Each of the words on the word list should be annotated with its inherent polarity (positive, negative or neutral). Since each word on the list is free of its context, the lists provide valuable information about the word and its context interac</context>
</contexts>
<marker>Alm, Sproat, 2005</marker>
<rawString>C.O. Alm and R. Sproat. 2005. Emotional sequencing and development in fairy tales. In Proceedings of the First International Conference on Affective Computing and Intelligent Interaction (ACII05). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C O Alm</author>
<author>D Roth</author>
<author>R Sproat</author>
</authors>
<title>Emotions from text: Machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<volume>volume</volume>
<contexts>
<context position="4713" citStr="Alm et al., 2005" startWordPosition="731" endWordPosition="734"> facial expressions and body language. In this way a virtual character would be able to simulate emotional perception and production of text in story telling scenarios. 3 Related Work Although EA is often referred to as a developing field, the amount of work carried out during the last decades is phenomenal. This section is not meant as a full overview of the related research as that scope is too great for the length of this paper. To contextualize the research presented in this paper we focus on the projects that inspired us and fostered the ideas. The work done by Alm (Alm and Sproat, 2005; Alm et al., 2005; Alm, 2008) is close to our project in its sprit and goals. Alm, (2008) aims at implementing affective text-to-speech system for storytelling scenarios. An EA system, detecting sentences with emotions expressed in written text is a crucial element for achieving this goal. The annotated corpus was composed of three sets of children’s stories written by Beatrix Potter, H. C. Andersen, and the Brothers Grimm. Like Liu et al. (2003), Alm (2008) uses several emotional categories, while most research in automatic EA works with pure polarities. The set of emotion categories used is essentially the l</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>C.O. Alm, D. Roth, and R. Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. In Proceedings of HLT/EMNLP, volume 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C O Alm</author>
</authors>
<date>2008</date>
<note>Affect in Text and Speech. lrc.cornell.edu.</note>
<contexts>
<context position="4725" citStr="Alm, 2008" startWordPosition="735" endWordPosition="736">s and body language. In this way a virtual character would be able to simulate emotional perception and production of text in story telling scenarios. 3 Related Work Although EA is often referred to as a developing field, the amount of work carried out during the last decades is phenomenal. This section is not meant as a full overview of the related research as that scope is too great for the length of this paper. To contextualize the research presented in this paper we focus on the projects that inspired us and fostered the ideas. The work done by Alm (Alm and Sproat, 2005; Alm et al., 2005; Alm, 2008) is close to our project in its sprit and goals. Alm, (2008) aims at implementing affective text-to-speech system for storytelling scenarios. An EA system, detecting sentences with emotions expressed in written text is a crucial element for achieving this goal. The annotated corpus was composed of three sets of children’s stories written by Beatrix Potter, H. C. Andersen, and the Brothers Grimm. Like Liu et al. (2003), Alm (2008) uses several emotional categories, while most research in automatic EA works with pure polarities. The set of emotion categories used is essentially the list of basic</context>
</contexts>
<marker>Alm, 2008</marker>
<rawString>C.O. Alm. 2008. Affect in Text and Speech. lrc.cornell.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="2030" citStr="Artstein and Poesio, 2008" startWordPosition="310" endWordPosition="313">n emotional perception of text, it is important to research the nature of the emotion analysis performed by humans and examine whether they can reliably perform the task. To investigate these issues, we conducted an experiment to find out the strategies people use to annotate selected folk fairy tale texts for emotions. The participants had to choose from a set of fifteen emotion categories, a significantly larger 98 set than typically used in EA, and assign them to an unrestricted range of text. To explore whether human annotators can reliably perform a task, inter-annotator agreement (IAA) (Artstein and Poesio, 2008) is the relevant measure. This measure can be calculated between every two individual annotations in order to find pairs or even teams of annotators whose strategies seem to be consistent and coherent enough so that they can be used further as the gold-standard annotation suited to train a machine learning approach for automatic EA analysis. A resulting EA system, capable of simulating human emotional perception of text, would be useful for information retrieval and many other fields. There are two main aspects of the resulting annotations to be researched. First, how consistently can people p</context>
<context position="19804" citStr="Artstein and Poesio, 2008" startWordPosition="3206" endWordPosition="3209">ree on the emotion category for the girl. In order to avoid the crossing brackets problem (Carroll et al., 2002), we treat the evil wolf ate as agreement, and the girl as disagreement. Although ate was left unmarked by one of the annotators, it is counted towards agreement because it is next to a stretch of text on which both annotators agree. Stretches of text the annotators agree or disagree upon also receive weight values: the higher the number of words that belong to open word classes in a stretch, the higher its weight. The general calculation formulae for the IAA measure are taken from (Artstein and Poesio, 2008): Ao − Ae 1 − Ae 1 � Ao = i iEI 1 Ae = I2 kEK Ao is the observed agreement, Ae is the expected agreement, I is the number of annotation items, K is the set of all categories used by both annotators, nck is the number of items assigned by annotator c to category k. 6 Analyzing Annotation Strategies Analysis of IAA, presented in Section 5 can answer the first question we aim to investigate: How consistently do people perceive and locate the emotional aspect of fairy tale texts? The second issue necessary for investigation is the annotation strategies people use to express their emotional percept</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>R. Artstein and M. Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Auracher</author>
</authors>
<title>wie auf den allm¨achtigen Schlag einer magischen Rute. Psychophysiologische Messungen zur Textwirkung.</title>
<date>2007</date>
<journal>Ars poetica ;</journal>
<volume>3</volume>
<contexts>
<context position="10228" citStr="Auracher, 2007" startWordPosition="1604" endWordPosition="1605">verlapped in two texts, which allowed us to achieve a high number of individual annotations in a short amount of time and compare the performance of people working on different sets of texts (see Table 1). Each participant annotated their texts in five sessions, dealing with only one text per session. The fatigue effect was avoided as no annotator had more than one session a day. 4.1 Determining Emotion Categories First, we needed to define the set of emotions to be used in the experiment. Based on the current emotion theories from comparative literature and cognitive psychology (Ekman, 1993; Auracher, 2007; Fontaine et al., 2007), we compiled a set of fifteen emotion categories: seven positive, seven negative, and neutral (see Table 2). We chose an equal number of negative and positive emotions, User Fairy Tale ID JG D R BR FH DS BM SJ A1 • • • • • A2 • • • • • A3 • • • • • A4 • • • • • A5 • • • • • As • • • • • A7 • • • • • As • • • • • A9 • • • • • A10 • • • • • Table 1: Annotation Sets Positive Negative Entspannung (relief) Unruhe (disturbance) Freude (joy) Trauer (sadness) Hoffnung (hope) Verzweiflung ( despair) Interesse (interest) Ekel (disgust) Mitgef¨uhl (compassion) Hass (hatred) ¨Uber</context>
</contexts>
<marker>Auracher, 2007</marker>
<rawString>Jan Auracher. 2007.... wie auf den allm¨achtigen Schlag einer magischen Rute. Psychophysiologische Messungen zur Textwirkung. Ars poetica ; 3. Dt. Wiss.-Verl. C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998.</rawString>
</citation>
<citation valid="false">
<title>The berkeley framenet project.</title>
<booktitle>In Proceedings of the 17th international conference on Computational</booktitle>
<volume>1</volume>
<pages>86--90</pages>
<location>Morristown, NJ, USA.</location>
<marker></marker>
<rawString>The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pages 86–90. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Hong Yu</author>
<author>Ashley Thornton</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Dan Jurafsky</author>
</authors>
<title>Automatic extraction of opinion propositions and their holders.</title>
<date>2004</date>
<booktitle>In 2004 AAAI Spring Symposium on Exploring Attitude and Affect in Text,</booktitle>
<pages>2224</pages>
<marker>Bethard, Yu, Thornton, Hatzivassiloglou, Jurafsky, 2004</marker>
<rawString>Steven Bethard, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic extraction of opinion propositions and their holders. In 2004 AAAI Spring Symposium on Exploring Attitude and Affect in Text, page 2224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>A Frank</author>
<author>D Lin</author>
<author>D Prescher</author>
<author>H Uszkoreit</author>
</authors>
<title>Beyond Parseval-Towards improved evaluation measures for parsing systems.</title>
<date>2002</date>
<booktitle>In Workshop at the 3rd International Conference on Language Resources and Evaluation LREC-02.,</booktitle>
<location>Las Palmas.</location>
<contexts>
<context position="18456" citStr="Carroll et al., 2002" startWordPosition="2975" endWordPosition="2978">e girl]X” (2) A1: “... [the evil wolf]X ate the girl” A2: “... [the evil wolf]Y ate the girl” (3) A1: “... [the evil wolf]X ate the girl” A2: “...the evil wolf ate [the girl]X” (4) A1: “... [the evil wolf]X ate [the girl]Z” A2: “... [the evil wolf ate the girl]X” In example (1) both annotators marked certain stretches of text with the same category X, but the annotations do not completely coincide, there is only an overlap. This situation is similar to that in syntactic annotation, where one needs to distinguish between bracketing and labeling of the constituent and measures such as Parseval (Carroll et al., 2002) have been much debated. Both annotators in example (1) recognize evil wolf as marked for X and thus this example should be counted towards agreement, while examples (2) and (3) should not. A second type of evaluation arises if the emotion clusters are taken into account. According to this evaluation type, example (2) is counted towards agreement if the categories X and Y belong to the same cluster. Example (4) provides an illustration of how IAA is accounted for in a more complex case. Annotator A1 has marked two stretches of text with two different emotion categories, while annotator A2 has </context>
</contexts>
<marker>Carroll, Frank, Lin, Prescher, Uszkoreit, 2002</marker>
<rawString>J. Carroll, A. Frank, D. Lin, D. Prescher, and H. Uszkoreit. 2002. Beyond Parseval-Towards improved evaluation measures for parsing systems. In Workshop at the 3rd International Conference on Language Resources and Evaluation LREC-02., Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Cunningham</author>
<author>C Wallraven</author>
</authors>
<title>Dynamic information for the recognition of conversational expressions.</title>
<date>2009</date>
<journal>Journal of Vision,</journal>
<volume>9</volume>
<issue>13</issue>
<pages>12</pages>
<contexts>
<context position="3886" citStr="Cunningham and Wallraven, 2009" startWordPosition="589" endWordPosition="592">the EA system predominantly is emotion enhancement of human-computer interaction, especially in virtual or augmented reality. Emotion enhancement of Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98–106, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics computer animation, especially when it deals with spoken or written text, is primarily done through manual annotation of text, even if a rich database of perceptually guided animations for behavioral scripts compilation is available (Cunningham and Wallraven, 2009). The resulting system of our project is meant to be a bridge between unprocessed input text (generated or provided) and visual and auditory information, coming from the virtual character, like generated speech, facial expressions and body language. In this way a virtual character would be able to simulate emotional perception and production of text in story telling scenarios. 3 Related Work Although EA is often referred to as a developing field, the amount of work carried out during the last decades is phenomenal. This section is not meant as a full overview of the related research as that sc</context>
</contexts>
<marker>Cunningham, Wallraven, 2009</marker>
<rawString>D. W. Cunningham and C. Wallraven. 2009. Dynamic information for the recognition of conversational expressions. Journal of Vision, 9(13:7):1–17, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 5.1, reference guide. ilk technical report 04-02.</title>
<date>2004</date>
<tech>Technical report.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2004. Timbl: Tilburg memory based learner, version 5.1, reference guide. ilk technical report 04-02. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<title>Facial Expression and Emotion.</title>
<date>1993</date>
<journal>American Psychologist,</journal>
<volume>48</volume>
<issue>4</issue>
<contexts>
<context position="5348" citStr="Ekman, 1993" startWordPosition="834" endWordPosition="835">o our project in its sprit and goals. Alm, (2008) aims at implementing affective text-to-speech system for storytelling scenarios. An EA system, detecting sentences with emotions expressed in written text is a crucial element for achieving this goal. The annotated corpus was composed of three sets of children’s stories written by Beatrix Potter, H. C. Andersen, and the Brothers Grimm. Like Liu et al. (2003), Alm (2008) uses several emotional categories, while most research in automatic EA works with pure polarities. The set of emotion categories used is essentially the list of basic emotions (Ekman, 1993), which has a justified preference for negative emotion categories. Ekmann’s list of basic emotions was extended by Alm, since the emotion of surprise is validly taken as ambivalent and was thus split into positive surprise and negative surprise. The EA system described in Alm et al. (2005) is machine learning based, where the EA problem is defined as multi-class classification problem, with sentences as classification units. Liu et al. (2003) have combined an emotion lexicon and handcrafted rules, which allowed them to create affect models and thus form a representation of the emotional affin</context>
<context position="10212" citStr="Ekman, 1993" startWordPosition="1602" endWordPosition="1603"> each group overlapped in two texts, which allowed us to achieve a high number of individual annotations in a short amount of time and compare the performance of people working on different sets of texts (see Table 1). Each participant annotated their texts in five sessions, dealing with only one text per session. The fatigue effect was avoided as no annotator had more than one session a day. 4.1 Determining Emotion Categories First, we needed to define the set of emotions to be used in the experiment. Based on the current emotion theories from comparative literature and cognitive psychology (Ekman, 1993; Auracher, 2007; Fontaine et al., 2007), we compiled a set of fifteen emotion categories: seven positive, seven negative, and neutral (see Table 2). We chose an equal number of negative and positive emotions, User Fairy Tale ID JG D R BR FH DS BM SJ A1 • • • • • A2 • • • • • A3 • • • • • A4 • • • • • A5 • • • • • As • • • • • A7 • • • • • As • • • • • A9 • • • • • A10 • • • • • Table 1: Annotation Sets Positive Negative Entspannung (relief) Unruhe (disturbance) Freude (joy) Trauer (sadness) Hoffnung (hope) Verzweiflung ( despair) Interesse (interest) Ekel (disgust) Mitgef¨uhl (compassion) Has</context>
</contexts>
<marker>Ekman, 1993</marker>
<rawString>P. Ekman. 1993. Facial Expression and Emotion. American Psychologist, 48(4):384–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JR Fontaine</author>
<author>KR Scherer</author>
<author>EB Roesch</author>
<author>PC Ellsworth</author>
</authors>
<title>The world of emotions is not two-dimensional. Psychological science: a journal of the American Psychological Society/APS,</title>
<date>2007</date>
<contexts>
<context position="10252" citStr="Fontaine et al., 2007" startWordPosition="1606" endWordPosition="1609"> texts, which allowed us to achieve a high number of individual annotations in a short amount of time and compare the performance of people working on different sets of texts (see Table 1). Each participant annotated their texts in five sessions, dealing with only one text per session. The fatigue effect was avoided as no annotator had more than one session a day. 4.1 Determining Emotion Categories First, we needed to define the set of emotions to be used in the experiment. Based on the current emotion theories from comparative literature and cognitive psychology (Ekman, 1993; Auracher, 2007; Fontaine et al., 2007), we compiled a set of fifteen emotion categories: seven positive, seven negative, and neutral (see Table 2). We chose an equal number of negative and positive emotions, User Fairy Tale ID JG D R BR FH DS BM SJ A1 • • • • • A2 • • • • • A3 • • • • • A4 • • • • • A5 • • • • • As • • • • • A7 • • • • • As • • • • • A9 • • • • • A10 • • • • • Table 1: Annotation Sets Positive Negative Entspannung (relief) Unruhe (disturbance) Freude (joy) Trauer (sadness) Hoffnung (hope) Verzweiflung ( despair) Interesse (interest) Ekel (disgust) Mitgef¨uhl (compassion) Hass (hatred) ¨Uberraschung (surprise) Angs</context>
</contexts>
<marker>Fontaine, Scherer, Roesch, Ellsworth, 2007</marker>
<rawString>JR Fontaine, KR Scherer, EB Roesch, and PC Ellsworth. 2007. The world of emotions is not two-dimensional. Psychological science: a journal of the American Psychological Society/APS, 18(12):1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
<author>M Marcus</author>
</authors>
<title>Adding semantic annotation to the Penn Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>252--256</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="7600" citStr="Kingsbury et al., 2002" startWordPosition="1171" endWordPosition="1174">f the context on subjective clues. This is relevant to our project since we are collecting lexicons of subjective clues and the mechanisms of contextual influence may prove to be of value for future automatic EA system training. Bethard et at. (2004) provide valuable information about corpus annotation for EA means and give accounts on the performance of various existing ML algorithms. They provide excellent analysis of automatic extraction of opinion proposition and their holders. For feature extraction, the authors employ such well-known resources as WordNet (Miller et al., 1990), PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). Several types of classification tasks involve evaluation on the level of documents. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al., 2004) and measuring strength of subjective clauses (Wilson et al., 2004). All these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ML techniques usage in ways specific for the EA task. 99 4 Experimental Setup Having established the r</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding semantic annotation to the Penn Treebank. In Proceedings of the Human Language Technology Conference, pages 252–256. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="26095" citStr="Landis and Koch, 1977" startWordPosition="4296" endWordPosition="4299">ked text, and they used few emotion categories ( &lt; 7 categories average per participant), so for the evaluation part their data was discarded. The final IAA evaluation was calculated on all the annotation pairs obtained from the six remaining participants (marked black in table 1), whose average agreement score in the original set of participants was originally higher than 0.50. The total number of annotation pairs amounted to 48: two texts annotated by all the six annotators, six texts annotated by three annotators for each of the two annotation sets. According to the interpretation of n by (Landis and Koch, 1977), the annotator agreement was moderate on average (0.53), and some pairs approached the almost perfect IAA rate (0.83). The IAA rates, calculated on the full set of fifteen emotions, without taking the emotion clusters into consideration, gave a moderate IAA rate on average (0.34) and reached substantial level (0.62) at maximum. The n rates are considerably high for the hard task and are comparable with the results presented in (Alm and Sproat, 2005). The word lists have a somewhat lower n IAA (0.45 on average, 0.72 at maximum), which is due to the low number of categories and the heavy bias t</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J.R. Landis and G.G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lee</author>
<author>O R Jeong</author>
<author>S Lee</author>
</authors>
<title>Opinion mining of customer feedback data on the web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd international conference on Ubiquitous information management and communication,</booktitle>
<pages>230235</pages>
<publisher>ACM.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="3212" citStr="Lee et al., 2008" startWordPosition="495" endWordPosition="498">irst, how consistently can people perceive and locate the emotional aspect of fairy tale texts? Second, how do they express their perception of text by means of annotation strategies? In the next sections, we address these questions and provide details of an experiment we conducted to empirically advance our understanding of the issues. 2 Motivation and Aimed Application Most existing EA systems are implemented for and used in specific predefined areas. The application field could be anything from extracting appraisal expressions (Whitelaw et al., 2005) to opinion mining of customer feedback (Lee et al., 2008). In our case, the intended application of the EA system predominantly is emotion enhancement of human-computer interaction, especially in virtual or augmented reality. Emotion enhancement of Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98–106, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics computer animation, especially when it deals with spoken or written text, is primarily done through manual annotation of text, even if a rich database of perceptually guided animations for be</context>
</contexts>
<marker>Lee, Jeong, Lee, 2008</marker>
<rawString>D. Lee, O.R. Jeong, and S. Lee. 2008. Opinion mining of customer feedback data on the web. In Proceedings of the 2nd international conference on Ubiquitous information management and communication, page 230235, New York, New York, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
<author>Henry Lieberman</author>
<author>Ted Selker</author>
</authors>
<title>A model of textual affect sensing using real-world knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th international conference on Intelligent user interfaces - IUI ’03,</booktitle>
<pages>125</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="5146" citStr="Liu et al. (2003)" startWordPosition="800" endWordPosition="803">er. To contextualize the research presented in this paper we focus on the projects that inspired us and fostered the ideas. The work done by Alm (Alm and Sproat, 2005; Alm et al., 2005; Alm, 2008) is close to our project in its sprit and goals. Alm, (2008) aims at implementing affective text-to-speech system for storytelling scenarios. An EA system, detecting sentences with emotions expressed in written text is a crucial element for achieving this goal. The annotated corpus was composed of three sets of children’s stories written by Beatrix Potter, H. C. Andersen, and the Brothers Grimm. Like Liu et al. (2003), Alm (2008) uses several emotional categories, while most research in automatic EA works with pure polarities. The set of emotion categories used is essentially the list of basic emotions (Ekman, 1993), which has a justified preference for negative emotion categories. Ekmann’s list of basic emotions was extended by Alm, since the emotion of surprise is validly taken as ambivalent and was thus split into positive surprise and negative surprise. The EA system described in Alm et al. (2005) is machine learning based, where the EA problem is defined as multi-class classification problem, with sen</context>
</contexts>
<marker>Liu, Lieberman, Selker, 2003</marker>
<rawString>Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A model of textual affect sensing using real-world knowledge. In Proceedings of the 8th international conference on Intelligent user interfaces - IUI ’03, page 125, New York, New York, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to Wordnet: An online lexical database*.</title>
<date>1990</date>
<journal>International Journal of lexicography,</journal>
<pages>3--4</pages>
<contexts>
<context position="7565" citStr="Miller et al., 1990" startWordPosition="1166" endWordPosition="1169">nyj (2006) shows the influence of the context on subjective clues. This is relevant to our project since we are collecting lexicons of subjective clues and the mechanisms of contextual influence may prove to be of value for future automatic EA system training. Bethard et at. (2004) provide valuable information about corpus annotation for EA means and give accounts on the performance of various existing ML algorithms. They provide excellent analysis of automatic extraction of opinion proposition and their holders. For feature extraction, the authors employ such well-known resources as WordNet (Miller et al., 1990), PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). Several types of classification tasks involve evaluation on the level of documents. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al., 2004) and measuring strength of subjective clauses (Wilson et al., 2004). All these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ML techniques usage in ways specific for the EA task. 99 4 Experime</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K.J. Miller. 1990. Introduction to Wordnet: An online lexical database*. International Journal of lexicography, 3(4):235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Polanyi</author>
<author>A Zaenen</author>
</authors>
<title>Contextual valence shifters. Computing Attitude and Affect in Text: Theory and Applications,</title>
<date>2006</date>
<pages>110</pages>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>L. Polanyi and A. Zaenen. 2006. Contextual valence shifters. Computing Attitude and Affect in Text: Theory and Applications, page 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I A Propp</author>
<author>A Dundes</author>
</authors>
<title>Morphology of the Folktale.</title>
<date>1977</date>
<publisher>University of Texas Press.</publisher>
<contexts>
<context position="16001" citStr="Propp and Dundes, 1977" startWordPosition="2575" endWordPosition="2578">ther choosing the most frequent label or the neutral label if the unstable word had received only two label instances. The results show that such annotation tasks could be used further for subjective clues lexicon collection. 4.4 Text Annotation For the third and main part of the experiment, we selected eight Grimm’s fairy tales, each 1200 – 1400 words long and written in Standard German (see Table 4). The texts were chosen based on their genre, for in spite of the depth of all the hidden and open references to human psyche and national traditions that were shown in works of (von Franz, 1996; Propp and Dundes, 1977), folk fairy tales are relatively uncomplicated in the plot-line and the characters’ personalities. Due to this relative simplicity of the content, we expect the participants’ emotional reactions to folk fairy tale texts to be more coherent than to other texts of fiction literature. The task for the participants was to locate and mark stretches of text where an emotion was to be 101 conveyed through the speech melody and/or facial expressions if the participant was to read the text out loud. To make the annotation process and its further analysis time-efficient and convenient for both, annotat</context>
<context position="24350" citStr="Propp and Dundes (1977)" startWordPosition="4001" endWordPosition="4004">% 15% 10% 5% 0% JG DS BR BT SJ D FH R approval compassion hope interest joy relief surprise 25% 20% 15% 10% 5% 0% anger despair disgust disturbance fear hatered sadness JG DS BR BT SJ D FH R 103 Figure 4: Emotional Trajectory over all Stories emotional trajectory reported by Alm and Sproat (2005). The emotional charge increases and falls steeply in the beginning of the fairy tale, then cycles though rise and fall phases (which do not exceed in their intensity the average rate of 0.6) and then ascents steeply at the end of the story. We agree with the explanation of such a trajectory, given by Propp and Dundes (1977) and also elaborated by Alm and Sproat (2005) — the first emotional intensity peak in the story line corresponds to the rising action, after the main characters have been introduced and the plot develops through a usually unexpected event. At the end of the story the intensity is highest, regardless whether the denouement is a happy ending or a tragedy. The fact that the fairy tale texts we chose for the experiment are relatively short is probably responsible for the steep peak of intensity in the very beginning of the story — the stories are too short to include a proper exposition. However, </context>
<context position="27791" citStr="Propp and Dundes (1977)" startWordPosition="4577" endWordPosition="4580">presented a mixture of the neutral category and “polarized” categories, 20% of which included more than one “polarized” categories. Another difference is the rich set of emotion categories, with equal number of positive and negative items. The results show that people can successfully use the large set to express their emotional perception of text (e.g., see Figures 3 and 2). Other important findings include the fact that short phrases are the naturally preferred annotation unit among our participants and that the emotional trajectory of a general story line corresponds to the one proposed by Propp and Dundes (1977). Emotional response [ ru] 0.8 0.6 0.4 0.2 0.0 1 .6 1 .4 1 .2 1 .0 0.0 0.2 0.4 0.6 0.8 1 .0 Story progress [ru] 104 8 Future Work 8.1 Corpus Expansion In the near future, we will expand the collections of annotated text in order to compile a substantially large training corpus. We plan to work further with three annotators that have formed a natural team, since their group has always attained the highest annotation scores for their annotation set, exceeding the highest scores in the other annotation set. The task defined for the three annotators is similar to the experiment described in the pa</context>
</contexts>
<marker>Propp, Dundes, 1977</marker>
<rawString>V.I.A. Propp and A. Dundes. 1977. Morphology of the Folktale. University of Texas Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rapp</author>
</authors>
<title>Automatic phonemic transcription and linguistic annotation from known text with Hidden Markov Models.</title>
<date>1995</date>
<booktitle>In Proceedings of ELSNET Goes East and IMACS Workshop. Citeseer.</booktitle>
<contexts>
<context position="30770" citStr="Rapp, 1995" startWordPosition="5071" endWordPosition="5072">ameter is that all the features should be available through automatic processing tools. This is crucial, since the resulting EA system has to be fully automated with no manual work involved. 8.3 Extra Information Sources and their Potential Contribution We also plan to collect data from other information sources, like video and audio recordings, by inviting amateur actors for story-telling sessions. This will allow emotion retrieval from the speech melody, facial expressions and body language. The manual annotation and the extra data sources can be aligned by means of Text and Speech Aligner (Rapp, 1995), which allows to track correspondences between them. This alignment would most certainly benefit the facial and body animation of the virtual characters, since there is no clear understanding of time correlation between emotions labeled in written text and the ones expressed through speech and facial clues in a story telling scenario. An EA system could also be perfected through a careful analysis of recorded speech and video of story telling sessions — regular recurrence of subjectivity of certain contexts will be even more significant if the transmission of the emotions from the story telle</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>S. Rapp. 1995. Automatic phonemic transcription and linguistic annotation from known text with Hidden Markov Models. In Proceedings of ELSNET Goes East and IMACS Workshop. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L von Franz</author>
</authors>
<title>The interpretation offairy tales.</title>
<date>1996</date>
<publisher>Shambhala Publications.</publisher>
<marker>von Franz, 1996</marker>
<rawString>M.L. von Franz. 1996. The interpretation offairy tales. Shambhala Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>N Garg</author>
<author>S Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>631</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3154" citStr="Whitelaw et al., 2005" startWordPosition="485" endWordPosition="488">o main aspects of the resulting annotations to be researched. First, how consistently can people perceive and locate the emotional aspect of fairy tale texts? Second, how do they express their perception of text by means of annotation strategies? In the next sections, we address these questions and provide details of an experiment we conducted to empirically advance our understanding of the issues. 2 Motivation and Aimed Application Most existing EA systems are implemented for and used in specific predefined areas. The application field could be anything from extracting appraisal expressions (Whitelaw et al., 2005) to opinion mining of customer feedback (Lee et al., 2008). In our case, the intended application of the EA system predominantly is emotion enhancement of human-computer interaction, especially in virtual or augmented reality. Emotion enhancement of Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98–106, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics computer animation, especially when it deals with spoken or written text, is primarily done through manual annotation of text, even i</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>C. Whitelaw, N. Garg, and S. Argamon. 2005. Using appraisal groups for sentiment analysis. In Proceedings of the 14th ACM international conference on Information and knowledge management, page 631. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>R Bruce</author>
<author>M Bell</author>
<author>M Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="7877" citStr="Wiebe et al., 2004" startWordPosition="1209" endWordPosition="1212">bout corpus annotation for EA means and give accounts on the performance of various existing ML algorithms. They provide excellent analysis of automatic extraction of opinion proposition and their holders. For feature extraction, the authors employ such well-known resources as WordNet (Miller et al., 1990), PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). Several types of classification tasks involve evaluation on the level of documents. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al., 2004) and measuring strength of subjective clauses (Wilson et al., 2004). All these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ML techniques usage in ways specific for the EA task. 99 4 Experimental Setup Having established the research context, we now turn to the questions we investigate in this paper: the use of an enriched category set and the flexible annotation units, and their influence on annotation quality. We describe the experiment we conducted and its main results. Each participant performe</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin. 2004. Learning subjective language. Computational linguistics, 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>761--769</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="7944" citStr="Wilson et al., 2004" startWordPosition="1219" endWordPosition="1222">rmance of various existing ML algorithms. They provide excellent analysis of automatic extraction of opinion proposition and their holders. For feature extraction, the authors employ such well-known resources as WordNet (Miller et al., 1990), PropBank (Kingsbury et al., 2002) and FrameNet (Baker et al., 1998). Several types of classification tasks involve evaluation on the level of documents. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al., 2004) and measuring strength of subjective clauses (Wilson et al., 2004). All these and many more helped us to decide upon our own strategies, provided many examples of corpus collection and annotation, feature extraction and ML techniques usage in ways specific for the EA task. 99 4 Experimental Setup Having established the research context, we now turn to the questions we investigate in this paper: the use of an enriched category set and the flexible annotation units, and their influence on annotation quality. We describe the experiment we conducted and its main results. Each participant performed several tasks for each session. The first task always was a cogni</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings of the National Conference on Artificial Intelligence, pages 761–769. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="6193" citStr="Wilson et al. (2009)" startWordPosition="965" endWordPosition="968">nd negative surprise. The EA system described in Alm et al. (2005) is machine learning based, where the EA problem is defined as multi-class classification problem, with sentences as classification units. Liu et al. (2003) have combined an emotion lexicon and handcrafted rules, which allowed them to create affect models and thus form a representation of the emotional affinity of a sentence. Their annotation scheme is also sentence-based. The EA system was tested on short user-composed text emails describing emotionally colored events. In the research on recognizing contextual polarity done by Wilson et al. (2009) a rich prior-polarity lexicon and dependency parsing technique were employed to detect and analyze subjectivity on phrasal level, taking into account all the power of context, captured through such features as negation, polarity modification and polarity shifters. The work presents auspicious results of high accuracy scores for classification between neutrality and polarized private states and between negative and positive subjective phrases. A detailed account of several ML algorithms performance tests is discussed in thought-provoking manner. This work encouraged us to build a lexicon of su</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399433, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>