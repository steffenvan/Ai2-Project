<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000102">
<title confidence="0.9642475">
Improving the Lexical Function Composition Model
with Pathwise Optimized Elastic-Net Regression
</title>
<author confidence="0.996537">
Jiming Li and Marco Baroni and Georgiana Dinu
</author>
<affiliation confidence="0.998283">
Center for Mind/Brain Sciences
University of Trento, Italy
</affiliation>
<email confidence="0.9913">
(jiming.li|marco.baroni|georgiana.dinu)@unitn.it
</email>
<sectionHeader confidence="0.99731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841555555556">
In this paper, we show that the lexical
function model for composition of dis-
tributional semantic vectors can be im-
proved by adopting a more advanced re-
gression technique. We use the pathwise
coordinate-descent optimized elastic-net
regression method to estimate the compo-
sition parameters, and compare the result-
ing model with several recent alternative
approaches in the task of composing sim-
ple intransitive sentences, adjective-noun
phrases and determiner phrases. Experi-
mental results demonstrate that the lexical
function model estimated by elastic-net re-
gression achieves better performance, and
it provides good qualitative interpretabil-
ity through sparsity constraints on model
parameters.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998612">
Vector-based distributional semantic models of
word meaning have gained increased attention in
recent years (Turney and Pantel, 2010). Differ-
ent from formal semantics, distributional seman-
tics represents word meanings as vectors in a high-
dimensional semantic space, where the dimen-
sions are given by co-occurring contextual fea-
tures. The intuition behind these models lies in
the fact that words which are similar in meaning
often occur in similar contexts, e.g., moon and
star might both occur with sky, night and bright.
This leads to convenient ways to measure similar-
ity between different words using geometric meth-
ods (e.g., the cosine of the angle between two
vectors that summarize their contextual distribu-
tion). Distributional semantic models have been
successfully applied to many tasks in linguistics
and cognitive science (Griffiths et al., 2007; Foltz
et al., 1998; Laham, 1997; McDonald and Brew,
2004). However, most of these tasks only deal
with isolated words, and there is a strong need
to construct representations for longer linguistic
structures such as phrases and sentences. In or-
der to achieve this goal, the principle of com-
positionality of linguistic structures, which states
that complex linguistic structures can be formed
through composition of simple elements, is ap-
plied to distributional vectors. Therefore, in recent
years, the problem of composition within distribu-
tional models has caught many researchers’ atten-
tion (Clark, 2013; Erk, 2012).
A number of compositional frameworks have
been proposed and tested. Mitchell and Lapata
(2008) propose a set of simple component-wise
operations, such as multiplication and addition.
Later, Guevara (2010) and Baroni and Zampar-
elli (2010) proposed more elaborate methods, in
which composition is modeled as matrix-vector
multiplication operations. Particularly new to their
approach is the proposal to estimate model param-
eters by minimizing the distance of the composed
vectors to corpus-observed phrase vectors. For ex-
ample, Baroni and Zamparelli (2010) consider the
case of Adjective-Noun composition and model it
as matrix-vector multiplication: adjective matrices
are parameters to be estimated and nouns are co-
occurrence vectors. The model parameter estima-
tion procedure becomes a multiple response mul-
tivariate regression problem. This method, that,
following Dinu et al. (2013) and others, we term
the lexical function composition model, can also
be generalized to more complex structures such
as 3rd order tensors for modeling transitive verbs
(Grefenstette et al., 2013).
Socher et al. (2012) proposed a more complex
and flexible framework based on matrix-vector
representations. Each word or lexical node in a
parsing tree is assigned a vector (representing in-
herent meaning of the constituent) and a matrix
(controlling the behavior to modify the meaning of
</bodyText>
<page confidence="0.986712">
434
</page>
<note confidence="0.9985315">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 434–442,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.6949845">
Model Composition function Parameters
��⃗u��2 2⃗v + �λ � ��⟨⃗u,⃗v⟩⃗u λ
Fulladd W1⃗u + W2⃗v W1, W2 E Rmxm
Lexfunc Au⃗v Au E Rmxm
Fulllex tanhQW1, W2] [A,,a W1, W2,
v Au, Av E Rmxm
</table>
<tableCaption confidence="0.999771">
Table 1: Composition functions of inputs (u, v).
</tableCaption>
<bodyText confidence="0.999986387096774">
neighbor words or phrases) simultaneously. They
use recursive neural networks to learn and con-
struct the entire model and show that it reaches
state-of-the-art performance in various evaluation
experiments.
In this paper, we focus on the simpler, linear
lexical function model proposed by Baroni and
Zamparelli (2010) (see also Coecke et al. (2010))
and show that its performance can be further im-
proved through more advanced regression tech-
niques. We use the recently introduced elastic-
net regularized linear regression method, which
is solved by the pathwise coordinate descent opti-
mization algorithm along a regularization parame-
ter path. This new regression method can rapidly
generate a sequence of solutions along the regular-
ization path. Performing cross-validation on this
parameter path should yield a much more accurate
model for prediction. Besides better prediction ac-
curacy, the elastic-net method also brings inter-
pretability to the composition procedure through
sparsity constraints on the model.
The rest of this paper is organized as follows: In
Section 2, we give details on the above-mentioned
composition models, which will be used for com-
parison in our experiments. In Section 3, we de-
scribe the pathwise optimized elastic-net regres-
sion algorithm. Experimental evaluation on three
composition tasks is provided in Section 4. In Sec-
tion 5 we conclude and suggest directions for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.982984" genericHeader="method">
2 Composition Models
</sectionHeader>
<bodyText confidence="0.999675421052632">
Mitchell and Lapata (2008; 2010) present a set of
simple but effective models in which each compo-
nent of the output vector is a function of the cor-
responding components of the inputs. Given in-
put vectors u⃗ and ⃗v, the weighted additive model
(Add) returns their weighted sum: p⃗ = w1⃗u +
w2⃗v. In the dilation model (Dil), the output vector
is obtained by decomposing one of the input vec-
tors, say ⃗v, into a vector parallel to u⃗ and an or-
thogonal vector, and then dilating only the parallel
vector by a factor λ before re-combining (formula
in Table 1). Mitchell and Lapata also propose a
simple multiplicative model in which the output
components are obtained by component-wise mul-
tiplication of the corresponding input components.
We use its natural weighted extension (Mult), in-
troduced by Dinu et al. (2013), that takes w1 and
w2 powers of the components before multiplying,
such that each phrase component pi is given by:
</bodyText>
<equation confidence="0.919559333333333">
pi = uw1
i vw2
i .
</equation>
<bodyText confidence="0.9982691">
Guevara (2010) and Zanzotto et al. (2010) ex-
plore a full form of the additive model (Fulladd),
where the two vectors entering a composition pro-
cess are pre-multiplied by weight matrices before
being added, so that each output component is
a weighted sum of all input components: p⃗ =
W1⃗u + W2⃗v.
Baroni and Zamparelli (2010) and Coecke et
al. (2010), taking inspiration from formal seman-
tics, characterize composition as function applica-
tion. For example, Baroni and Zamparelli model
adjective-noun phrases by treating the adjective
as a regression function from nouns onto (mod-
ified) nouns. Given that linear functions can be
expressed by matrices and their application by
matrix-by-vector multiplication, a functor (such
as the adjective) is represented by a matrix Au
to be composed with the argument vector v⃗ (e.g.,
the noun) by multiplication, returning the lexical
function (Lexfunc) representation of the phrase:
p⃗= Au⃗v.
The method proposed by Socher et al. (2012)
can be seen as a combination and non-linear ex-
tension of Fulladd and Lexfunc (that Dinu and col-
leagues thus called Fulllex) in which both phrase
elements act as functors (matrices) and arguments
(vectors). Given input terms u and v represented
by (⃗u, Au) and (⃗v, Av), respectively, their com-
position vector is obtained by applying first a lin-
ear transformation and then the hyperbolic tangent
function to the concatenation of the products Au⃗v
and Av⃗u (see Table 1 for the equation). Socher
and colleagues also present a way to construct ma-
trix representations for specific phrases, needed
to scale this composition method to larger con-
stituents. We ignore it here since we focus on the
two-word case.
Parameter estimation of the above composition
models follows Dinu et al. (2013) by minimizing
the distance to corpus-extracted phrase vectors. In
</bodyText>
<equation confidence="0.474312666666667">
Add w1⃗u + w2⃗v w1, w2
Mult ⃗uw1 ⊙ ⃗vw2 w1, w2
Dil
</equation>
<page confidence="0.988071">
435
</page>
<figureCaption confidence="0.977332">
Figure 1: A sketch of the composition model train-
ing and composing procedure.
</figureCaption>
<bodyText confidence="0.999662147058824">
the case of the Fulladd and Lexfunc models this
amounts to solving a multiple response multivari-
ate regression problem.
The whole composition model training and
phrase composition procedure is described with a
sketch in Figure 1. To illustrate with an example,
given an intransitive verb boom, we want to train
a model for this intransitive verb so that we can
use it for composition with a noun subject (e.g.,
export) to form an intransitive sentence (e.g., ex-
port boom(s)). We treat these steps as a composi-
tion model learning and predicting procedure. The
training dataset is formed with pairs of input (e.g.,
activity) and output (e.g., activity boom) vectors.
All composition models except Lexfunc also use
the functor vector (boom) in the training data. Lex-
func does not use this functor vector, but it would
rather like to encode the learning target’s vector
meaning in a different way (see experimental anal-
ysis in Section 4.3). Then, this dataset is used for
parameter estimation of models. When a model
(boom) is trained and given a new input seman-
tic vector (e.g., export), it will output another vec-
tor representing the concept for export boom. And
the concept export boom should be close to simi-
lar concepts (e.g., export prosper) in meaning un-
der some distance metric in semantic vector space.
The same training and composition scheme is ap-
plied for other types of functors (e.g., adjectives
and determiners). All the above mentioned com-
position models are evaluated within this scheme,
but note that in the case of Add, Dil, Mult and Ful-
ladd, a single set of parameters is obtained across
all functors of a certain syntactic category.
</bodyText>
<sectionHeader confidence="0.984309" genericHeader="method">
3 Pathwise Optimized Elastic-net
Algorithm
</sectionHeader>
<bodyText confidence="0.9999286">
The elastic-net regression method (Zou and
Hastie, 2005) is proposed as a compromise be-
tween lasso (Tibshirani, 1996) and ridge regres-
sion (Hastie et al., 2009). Suppose there are N
observation pairs (xi, yi), here xi ∈ Rp is the ith
training sample and yi ∈ R is the corresponding
response variable in the typical regression setting.
For simplicity, assume the xij are standardized:
ENi=1 x2 ij= 1, for j = 1, ... , p. The elastic-net
solves the following problem:
</bodyText>
<equation confidence="0.9972414">
� �N
1
min
(β0,β)ERP+1 N i=1
(1)
</equation>
<bodyText confidence="0.719419">
where
</bodyText>
<equation confidence="0.9970145">
Pα(β) = λ((1 − α)12 ∥ β ∥2ℓ2 +αβℓ1)
[12(1 − α)β2j + α|βj|�.
</equation>
<bodyText confidence="0.999795">
P is the elastic-net penalty, and it is a compro-
mise between the ridge regression penalty and the
lasso penalty. The merit of the elastic-net penalty
depends on two facts: the first is that elastic-net in-
herits lasso’s characteristic to shrink many of the
regression coefficients to zero, a property called
sparsity, which results in better interpretability of
model; the second is that elastic-net inherits ridge
regression’s property of a grouping effect, which
means important correlated features can be con-
tained in the model simultaneously, and not be
omitted as in lasso.
For these linear-type regression problem (ridge,
lasso and elastic-net), the determination of the λ
value is very important for prediction accuracy.
Efron et al. (2004) developed an efficient algo-
rithm to compute the entire regularization path
for the lasso problem in 2004. Later, Friedman
et al. (Friedman et al., 2007; Friedman et al.,
2010) proposed a coordinate descent optimization
</bodyText>
<equation confidence="0.960291">
�(yi − β0 − xTi β)2 + λPα(β)
� p
j=1
</equation>
<page confidence="0.989627">
436
</page>
<bodyText confidence="0.969538333333333">
method for the regularization parameter path, and
they also provided a solution for elastic-net. The
main idea of pathwise coordinate descent is to
solve the penalized regression problem along an
entire path of values for the regularization param-
eters λ, using the current estimates as warm starts.
The idea turns out to be quite efficient for elastic-
net regression. The procedure can be described as
below: firstly establish an 100 λ value sequence
in log scale, and for each of the 100 regulariza-
tion parameters, use the following coordinate-wise
updating rule to cycle around the features for es-
timating the corresponding regression coefficients
until convergence.
Model selection procedure for ’EnetLex’
</bodyText>
<equation confidence="0.706665333333333">
50 50 50 50 50 50 50 50 50 48 29 21 12 7 4 2
−2 0 2 4
log(Lambda)
</equation>
<figureCaption confidence="0.819716666666667">
Figure 2: Example of model selection procedure
for elastic-net regression (“the” model for deter-
miner phrase experiment, SVD, 50 dimensions).
</figureCaption>
<equation confidence="0.771325428571429">
200 400 600 800
Mean−Squared Error
�βj ←
S ( (j
N ( 1 Ei 1 xi9lyi − yi ), λα
(2)
1 + λ(1 − α)
</equation>
<bodyText confidence="0.778603">
where
</bodyText>
<equation confidence="0.7582255">
• y(j)
i = �β0 +Eℓ̸�j xiℓ�βℓ is the fitted value ex-
cluding the contribution from xij, and hence
�y(j)
</equation>
<bodyText confidence="0.717351">
i the partial residual for fitting βj.
</bodyText>
<listItem confidence="0.9384815">
• S(z, γ) is the soft-thresholding operator with
value
</listItem>
<equation confidence="0.998432">
S(z, γ) = sign(z)(|z |− γ)+
{ z − γ if z &gt; 0 and γ &lt; |z|
z + γ if z &lt; 0 and γ &lt; |z|
0 if γ ≥ |z|
</equation>
<bodyText confidence="0.999951941176471">
Then solutions for a decreasing sequence of val-
ues for λ are computed in this way, starting at the
smallest value λmax for which the entire coeffi-
cient vector β� = 0. Then, 10-fold cross valida-
tion on this regularization path is used to deter-
mine the best model for prediction accuracy. The
α parameter controls the model sparsity (the num-
ber of coefficients equal to zero) and grouping ef-
fect (shrinking highly correlated features simulta-
neously).
In what follows, we call the elastic-net regres-
sion lexical function model EnetLex. In Sec-
tion 4, we will report the experiment results by
EnetLex with α = 1. It equals to pathwise co-
ordinate descent optimized lasso, which favours
sparser solutions and is often a better estimator
when the number of training samples is far greater
than the number of feature dimensions, as in our
case. We also experimented with intermediate α
values (e.g., α = 0.5), that were, consistently, in-
ferior or equal to the lasso setting.
Figure 2 is an example of the model selection
procedure between different regularization param-
eter λ values for determiner “the” (experimental
details are described in section 4). When α is
fixed, EnetLex first generates a λ sequence from
λmax to λmin (λmax is set to the smallest value
which will shrink all the regression coefficients
to zero, λmin = 0.0001) in log scale (rightmost
point in the plot). The red points corresponding
to each λ value in the plot represent mean cross-
validated errors and their standard errors. To esti-
mate a model corresponding to some λ value ex-
cept λmax, we use the solution from previous λ
value as the initial coefficients (the warm starts
mentioned before) for iteration with coordinate
descent. This will often generate a stable solu-
tion path for the whole λ sequence very fast. And
we can choose the model with minimum cross-
validation error on this path and use it for more
accurate prediction. In Figure 2, the labels on the
top are numbers of corresponding selected vari-
ables (features), the right vertical dotted line is the
largest value of lambda such that error is within 1
standard error of the minimum, and the left verti-
cal dotted line corresponds to the λ value which
gives minimum cross-validated error. In this case,
the λ value of minimum cross-validated error is
0.106, and its log is -2.244316. In all of our ex-
periments, we will select models corresponding to
minimum training-data cross-validated error.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.791437">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9985825">
We evaluate on the three data sets described below,
that were also used by Dinu et al. (2013), our most
</bodyText>
<equation confidence="0.736192">
yi −
=
</equation>
<page confidence="0.983669">
437
</page>
<bodyText confidence="0.992988488888889">
direct point of comparison.
Intransitive sentences The first dataset, intro-
duced by Mitchell and Lapata (2010), focuses
on the composition of intransitive verbs and their
noun subjects. It contains a total of 120 sentence
pairs together with human similarity judgments on
a 7-point scale. For example, value slumps/value
declines is scored 7, skin glows/skin burns is
scored 1. On average, each pair is rated by 30
participants. Rather than evaluating against mean
scores, we use each rating as a separate data point,
as done by Mitchell and Lapata. We report Spear-
man correlations between human-assigned scores
and cosines of model-generated vector pairs.
Adjective-noun phrases Turney (2012) intro-
duced a dataset including both noun-noun com-
pounds and adjective-noun phrases (ANs). We fo-
cus on the latter, and we frame the task as in Dinu
et al. (2013). The dataset contains 620 ANs, each
paired with a single-noun paraphrase. Examples
include: upper side/upside, false belief/fallacy and
electric refrigerator/fridge. We evaluate a model
by computing the cosine of all 20K nouns in our
semantic space with the target AN, and looking at
the rank of the correct paraphrase in this list. The
lower the rank, the better the model. We report
median rank across the test items.
Determiner phrases The third dataset, intro-
duced in Bernardi et al. (2013), focuses on a
class of determiner words. It is a multiple-
choice test where target nouns (e.g., omniscience)
must be matched with the most closely related
determiner(-noun) phrases (DPs) (e.g., all knowl-
edge). There are 173 target nouns in total, each
paired with one correct DP response, as well as
5 foils, namely the determiner (all) and noun
(knowledge) from the correct response and three
more DPs, two of which contain the same noun as
the correct phrase (much knowledge, some knowl-
edge), the third the same determiner (all prelimi-
naries). Other examples of targets/related-phrases
are quatrain/four lines and apathy/no emotion.
The models compute cosines between target noun
and responses and are scored based on their accu-
racy at ranking the correct phrase first.
</bodyText>
<subsectionHeader confidence="0.887489">
4.2 Setup
</subsectionHeader>
<bodyText confidence="0.980397">
We use a concatenation of ukWaC, Wikipedia
(2009 dump) and BNC as source corpus, total-
</bodyText>
<table confidence="0.999922625">
Model Reduction Dim Correlation
Add NMF 150 0.1349
Dil NMF 300 0.1288
Mult NMF 250 0.2246
Fulladd SVD 300 0.0461
Lexfunc SVD 250 0.2673
Fulllex NMF 300 0.2682
EnetLex SVD 250 0.3239
</table>
<tableCaption confidence="0.9841895">
Table 2: Best performance comparison for intran-
sitive verb sentence composition.
</tableCaption>
<bodyText confidence="0.996694538461538">
ing 2.8 billion tokens.1 Word co-occurrences are
collected within sentence boundaries (with a max-
imum of a 50-words window around the target
word). Following Dinu et al. (2013), we use the
top 10K most frequent content lemmas as context
features, Pointwise Mutual Information as weight-
ing method and we reduce the dimensionality of
the data by both Non-negative Matrix Factoriza-
tion (NMF, Lee and Seung (2000)) and Singular
Value Decomposition (SVD). For both data di-
mensionality reduction techniques, we experiment
with different numbers of dimension varying from
50 to 300 with a step of 50. Since the Mult model
works very poorly when the input vectors contain
negative values, as is the case with SVD, for this
model we report result distributions across the 6
NMF variations only.
We use the DIStributional SEmantics Compo-
sition Toolkit (DISSECT)2 which provides imple-
mentations for all models we use for comparison.
Following Dinu and colleagues, we used ordinary
least-squares to estimate Fulladd and ridge for
Lexfunc. The EnetLex model is implemented in R
with support from the glmnet package,3 which im-
plements pathwise coordinate descent elastic-net
regression.
</bodyText>
<subsectionHeader confidence="0.999797">
4.3 Experimental Results and Analysis
</subsectionHeader>
<bodyText confidence="0.9999755">
The experimental results are shown in Ta-
bles 2, 3, 4 and Figures 3, 4, 5. The best per-
formances from each model on the three compo-
sition tasks are shown in the tables. The over-
all result distributions across reduction techniques
and dimensionalities are displayed in the figure
</bodyText>
<footnote confidence="0.997798833333333">
1http://wacky.sslmit.unibo.it;
http://www.natcorp.ox.ac.uk
2http://clic.cimec.unitn.it/composes/
toolkit/
3http://cran.r-project.org/web/
packages/glmnet/
</footnote>
<page confidence="0.993839">
438
</page>
<table confidence="0.9996545">
Model Reduction Dim Rank
Add NMF 300 113
Dil NMF 300 354.5
Mult NMF 300 146.5
Fulladd SVD 300 123
Lexfunc SVD 150 117.5
Fulllex SVD 50 394
EnetLex SVD 300 108.5
</table>
<tableCaption confidence="0.971966">
Table 3: Best performance comparison for adjec-
tive noun composition (lower ranks mean better
performance).
</tableCaption>
<table confidence="0.999883625">
Model Reduction Dim Rank
Add NMF 100 0.3237
Dil NMF 100 0.3584
Mult NMF 300 0.2023
Fulladd NMF 200 0.3642
Lexfunc SVD 200 0.3699
Fulllex SVD 100 0.3699
EnetLex SVD 250 0.4046
</table>
<tableCaption confidence="0.793818">
Table 4: Best performance comparison for deter-
miner phrase composition.
</tableCaption>
<bodyText confidence="0.999268615384615">
boxplots (NMF and SVD results are shown sep-
arately). From Tables 2, 3, 4, we can see that
EnetLex consistently achieves the best composi-
tion performance overall, also outperforming the
standard lexical function model. In the boxplot
display, we can see that SVD is in general more
stable across dimensionalities, yielding smaller
variance in the results than NMF. We also observe,
more specifically, larger variance in EnetLex per-
formance on NMF than in Lexfunc, especially for
determiner phrase composition. The large vari-
ance with EnetLex comes from the NMF low-
dimensionality results, especially the 50 dimen-
sions condition. The main reason for this lies
in the fast-computing tricks of the coordinate de-
scent algorithm when cycling around many fea-
tures with zero values (as resulting from NMF),
which cause fast convergence at the beginning of
the regularization path, generating an inaccurate
model. A subordinate reason might lie in the un-
standardized larger values of the NMF features
(causing large gaps between adjacent parameter
values in the regularization path). Although data
standardization or other feature scaling techniques
are often adopted in statistical analysis, they are
seldom used in semantic composition tasks due to
</bodyText>
<figure confidence="0.992730375">
Intransitive sentences
0.30
0.25
0.20
0.15
0.10
0.05
0.00
</figure>
<figureCaption confidence="0.9822395">
Figure 3: Intransitive verb sentence composition
results.
Figure 4: Adjective noun phrase composition re-
sults.
</figureCaption>
<bodyText confidence="0.999908391304348">
the fact that they might negatively affect the se-
mantic vector space. A reasonable way out of this
problem would be to save the mean and standard
deviation parameters used for data standardization
and use them to project the composed phrase vec-
tor outputs back to the original vector space.
On the other hand, EnetLex obtained a stable
good performance in SVD space, with the best re-
sults achieved with dimensions between 200 and
300. A set of Tukey’s Honestly Significant Tests
show that EnetLex significantly outperforms the
other models across SVD settings for determiner
phrases and intransitive sentences. The difference
is not significant for most comparisons in the ad-
jective phrases task.
For the simpler models for which it was com-
putationally feasible, we repeated the experiments
without dimensionality reduction. The results ob-
tained with (unweighted) Add and Mult using full-
space representations are reported in Table 5. Due
to computational limitations, we tuned full-space
weights for Add model only, obtaining similar re-
sults to those reported in the table. The full-space
</bodyText>
<figure confidence="0.999055911764706">
Add−nmf
Dil−nmf
Mult−nmf
Fulladd−nmf
Lexfunc−nmf
Fulllex−nmf
EnetLex−nmf
Add−svd
Dil−svd
Mult−svd
Fulladd−svd
Lexfunc−svd
Fulllex−svd
EnetLex−svd
Adjective−noun phrases
0
200
400
600
800
Add−nmf
Dil−nmf
Mult−nmf
Fulladd−nmf
Lexfunc−nmf
Fulllex−nmf
EnetLex−nmf
Add−svd
Dil−svd
Mult−svd
Fulladd−svd
Lexfunc−svd
Fulllex−svd
EnetLex−svd
</figure>
<page confidence="0.758141">
439
</page>
<figure confidence="0.995551571428572">
Determiner phrases
0.40
0.35
0.30
0.25
0.20
0.15
</figure>
<figureCaption confidence="0.99976">
Figure 5: Determiner phrase composition results.
</figureCaption>
<table confidence="0.998316">
model verb adjective determiner
Add 0.0259 957 0.2832
Mult 0.1796 298.5 0.0405
</table>
<tableCaption confidence="0.941067">
Table 5: Performance of Add and Mult models
without dimensionality reduction.
</tableCaption>
<bodyText confidence="0.999095333333333">
results confirm that dimensionality reduction is
not only a computational necessity when work-
ing with more complex models, but it is actually
improving the quality of the underlying semantic
space.
Another benefit that elastic-net has brought to
us is the sparsity in coefficient matrices. Sparsity
here means that many entries in the coefficient ma-
trix are shrunk to 0. For the above three exper-
iments, the mean adjective, verb and determiner
models’ sparsity ratios are 0.66, 0.55 and 0.18 re-
spectively. Sparsity can greatly reduce the space
needed to store the lexical function model, espe-
cially when we want to use higher orders of repre-
sentation. Moreover, sparsity in the model is help-
ful to interpret the concept a specific functor word
is conveying. For example, we show how to an-
alyze the coefficient matrices for functor content
words (verbs and adjectives). The verb burst and
adjective poisonous, when estimated in the space
projected to 100 dimensions with NMF, have per-
centages of sparsity 47% and 39% respectively,
which means 47% of the entries in the burst ma-
trix and 39% of the entries in the poisonous ma-
trix are zeros.4 Most of the (hopefully) irrelevant
dimensions were discarded during model training.
For visualization, we list the 6 most significant
</bodyText>
<footnote confidence="0.9863468">
4We analyze NMF rather than the better-performing SVD
features because the presence of negative values in the latter
makes their interpretation very difficult. And NMF achieves
comparably good performance for interpretation when di-
mension exceeds 100.
</footnote>
<bodyText confidence="0.999960215686274">
columns and rows from verb burst and adjective
poisonous in Table 6. Each reduced NMF di-
mension is represented by the 3 largest original-
context entries in the corresponding row of the
NMF basis matrix. The top columns and rows
are selected by ordering sums of row entries and
sums of column entries (the 10 most common fea-
tures across trained matrices are omitted). In the
matrix-vector multiplication scenario, a larger col-
umn contributes more to all the features of the
composed output phrase vector, while one large
row corresponds to a large composition output fea-
ture. From these tables, we can see that the se-
lected top columns and rows are mostly semanti-
cally relevant to the corresponding functor words
(burst and poisonous, in the displayed examples).
A very interesting aspect of these experiments
is the role of the intercept in our regression model.
The path-wise optimization algorithm starts with
a lambda value (Amax), which sets all the coef-
ficients exactly to 0, and at that time the inter-
cept is just the expected mean value of the train-
ing phrase vectors, which in turn is of course quite
similar to the co-occurrence vector of the cor-
responding functor word (by averaging the poi-
sonous N context distributions, we obtain a vec-
tor that approximates the poisonous distribution).
And, although the intercept also changes with dif-
ferent lambda values, it still highly correlates with
the co-occurrence vectors of the functor words
in vector space. For adjectives and verbs, we
compared the initial model’s (Amax) intercept and
the minimum cross-validation error model inter-
cept with corpus-extracted vectors for the corre-
sponding words. That is, we used the word co-
occurrence vector for a verb or an adjective ex-
tracted from the corpus and projected onto the
reduced feature space (e.g., NMF, 100 dimen-
sions), then computed cosine similarity between
this word meaning representation and its corre-
sponding EnetLex matrix initial and minimum-
error intercepts, respectively. Most of the simi-
larities are still quite high after estimation: The
mean cosine values for adjectives are 0.82 for the
initial intercept and 0.72 for the minimum-error
one. For verbs, the corresponding values are 0.75
and 0.69, respectively. Apparently, the sparsity
constraint helps the intercept retaining information
from training phrases.
Qualitatively, often the intercept encodes the
representation of the original word meaning in
</bodyText>
<figure confidence="0.997920428571429">
Add−nmf
Dil−nmf
Mult−nmf
Fulladd−nmf
Lexfunc−nmf
Fulllex−nmf
EnetLex−nmf
Add−svd
Dil−svd
Mult−svd
Fulladd−svd
Lexfunc−svd
Fulllex−svd
EnetLex−svd
</figure>
<page confidence="0.988853">
440
</page>
<bodyText confidence="0.998858785714286">
burst significant columns significant rows
policeman, mob, guard hurricane, earthquake, disaster
Iraqi, Lebanese, Kurdish conquer, Byzantine, conquest
jealousy, anger, guilt policeman, mob, guard
hurricane, earthquake, disaster terminus, traffic, interchange
defender, keeper, striker convict, sentence, imprisonment
volcanic, sediment, geological boost, unveil, campaigner
poisonous significant columns significant rows
bathroom, wc, shower ventilation, fluid, bacterium
ignite, emit, reactor ignite, emit, reactor
reptile, mammal, predator infectious, infect, infected
ventilation, fluid, bacterium slay, pharaoh, tribe
flowering, shrub, perennial park, lorry, pavement
sauce, onion, garlic knife, pierce, brass
</bodyText>
<tableCaption confidence="0.602018">
Table 6: Interpretability for verbs and adjectives (exemplified by burst and poisonous).
</tableCaption>
<bodyText confidence="0.998141461538462">
vector space. For example, if we check the inter-
cept for poisonous, the cosine between the origi-
nal vector space representation (from corpus) and
the minimum-error solution intercept (from train-
ing phrases) is at 0.7. The NMF dimensions cor-
responding with the largest intercept entries are
rather intuitive for poisonous: (ventilation, fluid,
bacterium), (racist, racism, outrage), (reptile,
mammal, predator), (flowering, shrub, perennial),
(sceptical, accusation, credibility), (infectious, in-
fect, infected).
The mathematical reason for the above facts lies
in the updating rule of the elastic-net’s intercept:
</bodyText>
<equation confidence="0.9809">
Q0 = Y�− �p �Qjkj (3)
j=1
</equation>
<bodyText confidence="0.9999869">
Sparsity in the regression coefficients (Qj) encour-
ages intercept Q0 to stay as close to the mean
value of response y as possible. So the elastic-
net lexical function composition model is de facto
also capturing the inherent meaning of the func-
tor word, learning it from the training word-phrase
pairs. In future research, we would like to test if
these lexical meaning representations are as good
or even better than standard co-occurrence vectors
for single-word similarity tasks.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99992809375">
In this paper, we have shown that the lexical func-
tion composition model can be improved by ad-
vanced regression techniques. We use pathwise
coordinate descent optimized elastic-net, testing
it on composing intransitive sentences, adjective-
noun phrases and determiner phrases in compari-
son with other composition models, including lex-
ical function estimated with ridge regression. The
elastic-net method leads to performance gains on
all three tasks. Through sparsity constraints on the
model, elastic-net also introduces interpretability
in the lexical function composition model. The
regression coefficient matrices can often be eas-
ily interpreted by looking at large row and column
sums, as many matrix entries are shrunk to zero.
The intercept of elastic-net regression also plays
an interesting role in the model. With the sparsity
constraints, the intercept of the model tends to re-
tain the inherent meaning of the word by averaging
training phrase vectors.
Our approach naturally generalizes to similar
composition tasks, in particular those involving
higher-order tensors (Grefenstette et al., 2013),
where sparseness might be crucial in producing
compact representations of very large objects. Our
results also suggest that the performance of the
lexical function composition model might be fur-
ther improved with even more advanced methods,
such as nonlinear regression. In the future, we
would also like to explore interpretability more in
depth, by looking at grouping and interaction ef-
fects between features.
</bodyText>
<sectionHeader confidence="0.999329" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999503666666667">
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES), and we
thank the reviewers for helpful feedback.
</bodyText>
<page confidence="0.998475">
441
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999674865979382">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings ofACL (Short
Papers), pages 53–57, Sofia, Bulgaria.
Stephen Clark. 2013. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345–384.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50–
58, Sofia, Bulgaria.
Bradley Efron, Trevor Hastie, Iain Johnstone, and
Robert Tibshirani. 2004. Least angle regression.
The Annals of statistics, 32(2):407–499.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635–653.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
Latent Semantic Analysis. Discourse Processes,
25:285–307.
Jerome Friedman, Trevor Hastie, Holger H¨ofling, and
Robert Tibshirani. 2007. Pathwise coordinate
optimization. The Annals of Applied Statistics,
1(2):302–332.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2010. Regularization paths for generalized linear
models via coordinate descent. Journal of statisti-
cal software, 33(1):1.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131–142, Potsdam, Germany.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211–244.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37,
Uppsala, Sweden.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning,
2nd ed. Springer, New York.
Darrell Laham. 1997. Latent Semantic Analysis
approaches to categorization. In Proceedings of
CogSci, page 979.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556–562.
Scott McDonald and Chris Brew. 2004. A distribu-
tional model of semantic context effects in lexical
processing. In Proceedings of ACL, pages 17–24,
Barcelona, Spain.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236–244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Ko-
rea.
Rob Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267–288.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
J. Artif. Intell. Res.(JAIR), 44:533–585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263–
1271, Beijing, China.
Hui Zou and Trevor Hastie. 2005. Regularization
and variable selection via the elastic net. Journal
of the Royal Statistical Society: Series B (Statistical
Methodology), 67(2):301–320.
</reference>
<page confidence="0.998436">
442
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939061">
<title confidence="0.9990545">Improving the Lexical Function Composition with Pathwise Optimized Elastic-Net Regression</title>
<author confidence="0.999965">Li Baroni</author>
<affiliation confidence="0.9995595">Center for Mind/Brain University of Trento,</affiliation>
<email confidence="0.998415">(jiming.li|marco.baroni|georgiana.dinu)@unitn.it</email>
<abstract confidence="0.996959315789474">In this paper, we show that the lexical function model for composition of distributional semantic vectors can be improved by adopting a more advanced regression technique. We use the pathwise coordinate-descent optimized elastic-net regression method to estimate the composition parameters, and compare the resulting model with several recent alternative approaches in the task of composing simple intransitive sentences, adjective-noun phrases and determiner phrases. Experimental results demonstrate that the lexical function model estimated by elastic-net regression achieves better performance, and it provides good qualitative interpretability through sparsity constraints on model parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2688" citStr="Baroni and Zamparelli (2010)" startWordPosition="386" endWordPosition="390">sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem. </context>
<context position="4580" citStr="Baroni and Zamparelli (2010)" startWordPosition="673" endWordPosition="676">enburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Model Composition function Parameters ��⃗u��2 2⃗v + �λ � ��⟨⃗u,⃗v⟩⃗u λ Fulladd W1⃗u + W2⃗v W1, W2 E Rmxm Lexfunc Au⃗v Au E Rmxm Fulllex tanhQW1, W2] [A,,a W1, W2, v Au, Av E Rmxm Table 1: Composition functions of inputs (u, v). neighbor words or phrases) simultaneously. They use recursive neural networks to learn and construct the entire model and show that it reaches state-of-the-art performance in various evaluation experiments. In this paper, we focus on the simpler, linear lexical function model proposed by Baroni and Zamparelli (2010) (see also Coecke et al. (2010)) and show that its performance can be further improved through more advanced regression techniques. We use the recently introduced elasticnet regularized linear regression method, which is solved by the pathwise coordinate descent optimization algorithm along a regularization parameter path. This new regression method can rapidly generate a sequence of solutions along the regularization path. Performing cross-validation on this parameter path should yield a much more accurate model for prediction. Besides better prediction accuracy, the elastic-net method also b</context>
<context position="6970" citStr="Baroni and Zamparelli (2010)" startWordPosition="1070" endWordPosition="1073">ents are obtained by component-wise multiplication of the corresponding input components. We use its natural weighted extension (Mult), introduced by Dinu et al. (2013), that takes w1 and w2 powers of the components before multiplying, such that each phrase component pi is given by: pi = uw1 i vw2 i . Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p⃗ = W1⃗u + W2⃗v. Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as function application. For example, Baroni and Zamparelli model adjective-noun phrases by treating the adjective as a regression function from nouns onto (modified) nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a functor (such as the adjective) is represented by a matrix Au to be composed with the argument vector v⃗ (e.g., the noun) by multiplication, returning the lexical function (Lexfunc) representation of the phrase: p⃗=</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raffaella Bernardi</author>
<author>Georgiana Dinu</author>
<author>Marco Marelli</author>
<author>Marco Baroni</author>
</authors>
<title>A relatedness benchmark to test the role of determiners in compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL (Short Papers),</booktitle>
<pages>53--57</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="17096" citStr="Bernardi et al. (2013)" startWordPosition="2790" endWordPosition="2793">th noun-noun compounds and adjective-noun phrases (ANs). We focus on the latter, and we frame the task as in Dinu et al. (2013). The dataset contains 620 ANs, each paired with a single-noun paraphrase. Examples include: upper side/upside, false belief/fallacy and electric refrigerator/fridge. We evaluate a model by computing the cosine of all 20K nouns in our semantic space with the target AN, and looking at the rank of the correct paraphrase in this list. The lower the rank, the better the model. We report median rank across the test items. Determiner phrases The third dataset, introduced in Bernardi et al. (2013), focuses on a class of determiner words. It is a multiplechoice test where target nouns (e.g., omniscience) must be matched with the most closely related determiner(-noun) phrases (DPs) (e.g., all knowledge). There are 173 target nouns in total, each paired with one correct DP response, as well as 5 foils, namely the determiner (all) and noun (knowledge) from the correct response and three more DPs, two of which contain the same noun as the correct phrase (much knowledge, some knowledge), the third the same determiner (all preliminaries). Other examples of targets/related-phrases are quatrain</context>
</contexts>
<marker>Bernardi, Dinu, Marelli, Baroni, 2013</marker>
<rawString>Raffaella Bernardi, Georgiana Dinu, Marco Marelli, and Marco Baroni. 2013. A relatedness benchmark to test the role of determiners in compositional distributional semantics. In Proceedings ofACL (Short Papers), pages 53–57, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector space models of lexical meaning.</title>
<date>2013</date>
<booktitle>Handbook of Contemporary Semantics, 2nd edition.</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<publisher>In press.</publisher>
<location>Blackwell, Malden, MA.</location>
<contexts>
<context position="2437" citStr="Clark, 2013" startWordPosition="352" endWordPosition="353">2007; Foltz et al., 1998; Laham, 1997; McDonald and Brew, 2004). However, most of these tasks only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-</context>
</contexts>
<marker>Clark, 2013</marker>
<rawString>Stephen Clark. 2013. Vector space models of lexical meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, 2nd edition. Blackwell, Malden, MA. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="4611" citStr="Coecke et al. (2010)" startWordPosition="679" endWordPosition="682">4 Association for Computational Linguistics Model Composition function Parameters ��⃗u��2 2⃗v + �λ � ��⟨⃗u,⃗v⟩⃗u λ Fulladd W1⃗u + W2⃗v W1, W2 E Rmxm Lexfunc Au⃗v Au E Rmxm Fulllex tanhQW1, W2] [A,,a W1, W2, v Au, Av E Rmxm Table 1: Composition functions of inputs (u, v). neighbor words or phrases) simultaneously. They use recursive neural networks to learn and construct the entire model and show that it reaches state-of-the-art performance in various evaluation experiments. In this paper, we focus on the simpler, linear lexical function model proposed by Baroni and Zamparelli (2010) (see also Coecke et al. (2010)) and show that its performance can be further improved through more advanced regression techniques. We use the recently introduced elasticnet regularized linear regression method, which is solved by the pathwise coordinate descent optimization algorithm along a regularization parameter path. This new regression method can rapidly generate a sequence of solutions along the regularization path. Performing cross-validation on this parameter path should yield a much more accurate model for prediction. Besides better prediction accuracy, the elastic-net method also brings interpretability to the c</context>
<context position="6995" citStr="Coecke et al. (2010)" startWordPosition="1075" endWordPosition="1078">se multiplication of the corresponding input components. We use its natural weighted extension (Mult), introduced by Dinu et al. (2013), that takes w1 and w2 powers of the components before multiplying, such that each phrase component pi is given by: pi = uw1 i vw2 i . Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p⃗ = W1⃗u + W2⃗v. Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as function application. For example, Baroni and Zamparelli model adjective-noun phrases by treating the adjective as a regression function from nouns onto (modified) nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a functor (such as the adjective) is represented by a matrix Au to be composed with the argument vector v⃗ (e.g., the noun) by multiplication, returning the lexical function (Lexfunc) representation of the phrase: p⃗= Au⃗v. The method propose</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>50--58</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3335" citStr="Dinu et al. (2013)" startWordPosition="479" endWordPosition="482">ods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem. This method, that, following Dinu et al. (2013) and others, we term the lexical function composition model, can also be generalized to more complex structures such as 3rd order tensors for modeling transitive verbs (Grefenstette et al., 2013). Socher et al. (2012) proposed a more complex and flexible framework based on matrix-vector representations. Each word or lexical node in a parsing tree is assigned a vector (representing inherent meaning of the constituent) and a matrix (controlling the behavior to modify the meaning of 434 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pa</context>
<context position="6510" citStr="Dinu et al. (2013)" startWordPosition="987" endWordPosition="990">vectors u⃗ and ⃗v, the weighted additive model (Add) returns their weighted sum: p⃗ = w1⃗u + w2⃗v. In the dilation model (Dil), the output vector is obtained by decomposing one of the input vectors, say ⃗v, into a vector parallel to u⃗ and an orthogonal vector, and then dilating only the parallel vector by a factor λ before re-combining (formula in Table 1). Mitchell and Lapata also propose a simple multiplicative model in which the output components are obtained by component-wise multiplication of the corresponding input components. We use its natural weighted extension (Mult), introduced by Dinu et al. (2013), that takes w1 and w2 powers of the components before multiplying, such that each phrase component pi is given by: pi = uw1 i vw2 i . Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p⃗ = W1⃗u + W2⃗v. Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as function application. For example, Baroni a</context>
<context position="8405" citStr="Dinu et al. (2013)" startWordPosition="1298" endWordPosition="1301">ors (matrices) and arguments (vectors). Given input terms u and v represented by (⃗u, Au) and (⃗v, Av), respectively, their composition vector is obtained by applying first a linear transformation and then the hyperbolic tangent function to the concatenation of the products Au⃗v and Av⃗u (see Table 1 for the equation). Socher and colleagues also present a way to construct matrix representations for specific phrases, needed to scale this composition method to larger constituents. We ignore it here since we focus on the two-word case. Parameter estimation of the above composition models follows Dinu et al. (2013) by minimizing the distance to corpus-extracted phrase vectors. In Add w1⃗u + w2⃗v w1, w2 Mult ⃗uw1 ⊙ ⃗vw2 w1, w2 Dil 435 Figure 1: A sketch of the composition model training and composing procedure. the case of the Fulladd and Lexfunc models this amounts to solving a multiple response multivariate regression problem. The whole composition model training and phrase composition procedure is described with a sketch in Figure 1. To illustrate with an example, given an intransitive verb boom, we want to train a model for this intransitive verb so that we can use it for composition with a noun subj</context>
<context position="15725" citStr="Dinu et al. (2013)" startWordPosition="2568" endWordPosition="2571">he top are numbers of corresponding selected variables (features), the right vertical dotted line is the largest value of lambda such that error is within 1 standard error of the minimum, and the left vertical dotted line corresponds to the λ value which gives minimum cross-validated error. In this case, the λ value of minimum cross-validated error is 0.106, and its log is -2.244316. In all of our experiments, we will select models corresponding to minimum training-data cross-validated error. 4 Experiments 4.1 Datasets We evaluate on the three data sets described below, that were also used by Dinu et al. (2013), our most yi − = 437 direct point of comparison. Intransitive sentences The first dataset, introduced by Mitchell and Lapata (2010), focuses on the composition of intransitive verbs and their noun subjects. It contains a total of 120 sentence pairs together with human similarity judgments on a 7-point scale. For example, value slumps/value declines is scored 7, skin glows/skin burns is scored 1. On average, each pair is rated by 30 participants. Rather than evaluating against mean scores, we use each rating as a separate data point, as done by Mitchell and Lapata. We report Spearman correlati</context>
<context position="18403" citStr="Dinu et al. (2013)" startWordPosition="3002" endWordPosition="3005">ponses and are scored based on their accuracy at ranking the correct phrase first. 4.2 Setup We use a concatenation of ukWaC, Wikipedia (2009 dump) and BNC as source corpus, totalModel Reduction Dim Correlation Add NMF 150 0.1349 Dil NMF 300 0.1288 Mult NMF 250 0.2246 Fulladd SVD 300 0.0461 Lexfunc SVD 250 0.2673 Fulllex NMF 300 0.2682 EnetLex SVD 250 0.3239 Table 2: Best performance comparison for intransitive verb sentence composition. ing 2.8 billion tokens.1 Word co-occurrences are collected within sentence boundaries (with a maximum of a 50-words window around the target word). Following Dinu et al. (2013), we use the top 10K most frequent content lemmas as context features, Pointwise Mutual Information as weighting method and we reduce the dimensionality of the data by both Non-negative Matrix Factorization (NMF, Lee and Seung (2000)) and Singular Value Decomposition (SVD). For both data dimensionality reduction techniques, we experiment with different numbers of dimension varying from 50 to 300 with a step of 50. Since the Mult model works very poorly when the input vectors contain negative values, as is the case with SVD, for this model we report result distributions across the 6 NMF variati</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. General estimation and evaluation of compositional distributional semantic models. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, pages 50– 58, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Trevor Hastie</author>
<author>Iain Johnstone</author>
<author>Robert Tibshirani</author>
</authors>
<title>Least angle regression. The Annals of statistics,</title>
<date>2004</date>
<pages>32--2</pages>
<contexts>
<context position="11609" citStr="Efron et al. (2004)" startWordPosition="1837" endWordPosition="1840">he elastic-net penalty depends on two facts: the first is that elastic-net inherits lasso’s characteristic to shrink many of the regression coefficients to zero, a property called sparsity, which results in better interpretability of model; the second is that elastic-net inherits ridge regression’s property of a grouping effect, which means important correlated features can be contained in the model simultaneously, and not be omitted as in lasso. For these linear-type regression problem (ridge, lasso and elastic-net), the determination of the λ value is very important for prediction accuracy. Efron et al. (2004) developed an efficient algorithm to compute the entire regularization path for the lasso problem in 2004. Later, Friedman et al. (Friedman et al., 2007; Friedman et al., 2010) proposed a coordinate descent optimization �(yi − β0 − xTi β)2 + λPα(β) � p j=1 436 method for the regularization parameter path, and they also provided a solution for elastic-net. The main idea of pathwise coordinate descent is to solve the penalized regression problem along an entire path of values for the regularization parameters λ, using the current estimates as warm starts. The idea turns out to be quite efficient</context>
</contexts>
<marker>Efron, Hastie, Johnstone, Tibshirani, 2004</marker>
<rawString>Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. Least angle regression. The Annals of statistics, 32(2):407–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<contexts>
<context position="2449" citStr="Erk, 2012" startWordPosition="354" endWordPosition="355">t al., 1998; Laham, 1997; McDonald and Brew, 2004). However, most of these tasks only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composi</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landauer</author>
</authors>
<title>The measurement of textual coherence with Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--285</pages>
<contexts>
<context position="1850" citStr="Foltz et al., 1998" startWordPosition="261" endWordPosition="264">space, where the dimensions are given by co-occurring contextual features. The intuition behind these models lies in the fact that words which are similar in meaning often occur in similar contexts, e.g., moon and star might both occur with sky, night and bright. This leads to convenient ways to measure similarity between different words using geometric methods (e.g., the cosine of the angle between two vectors that summarize their contextual distribution). Distributional semantic models have been successfully applied to many tasks in linguistics and cognitive science (Griffiths et al., 2007; Foltz et al., 1998; Laham, 1997; McDonald and Brew, 2004). However, most of these tasks only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012).</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with Latent Semantic Analysis. Discourse Processes, 25:285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Holger H¨ofling</author>
<author>Robert Tibshirani</author>
</authors>
<title>Pathwise coordinate optimization.</title>
<date>2007</date>
<journal>The Annals of Applied Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<marker>Friedman, Hastie, H¨ofling, Tibshirani, 2007</marker>
<rawString>Jerome Friedman, Trevor Hastie, Holger H¨ofling, and Robert Tibshirani. 2007. Pathwise coordinate optimization. The Annals of Applied Statistics, 1(2):302–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
<author>Rob Tibshirani</author>
</authors>
<title>Regularization paths for generalized linear models via coordinate descent.</title>
<date>2010</date>
<journal>Journal of statistical software,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="11785" citStr="Friedman et al., 2010" startWordPosition="1866" endWordPosition="1869">called sparsity, which results in better interpretability of model; the second is that elastic-net inherits ridge regression’s property of a grouping effect, which means important correlated features can be contained in the model simultaneously, and not be omitted as in lasso. For these linear-type regression problem (ridge, lasso and elastic-net), the determination of the λ value is very important for prediction accuracy. Efron et al. (2004) developed an efficient algorithm to compute the entire regularization path for the lasso problem in 2004. Later, Friedman et al. (Friedman et al., 2007; Friedman et al., 2010) proposed a coordinate descent optimization �(yi − β0 − xTi β)2 + λPα(β) � p j=1 436 method for the regularization parameter path, and they also provided a solution for elastic-net. The main idea of pathwise coordinate descent is to solve the penalized regression problem along an entire path of values for the regularization parameters λ, using the current estimates as warm starts. The idea turns out to be quite efficient for elasticnet regression. The procedure can be described as below: firstly establish an 100 λ value sequence in log scale, and for each of the 100 regularization parameters, </context>
</contexts>
<marker>Friedman, Hastie, Tibshirani, 2010</marker>
<rawString>Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2010. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1):1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>131--142</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="3530" citStr="Grefenstette et al., 2013" startWordPosition="509" endWordPosition="512"> of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem. This method, that, following Dinu et al. (2013) and others, we term the lexical function composition model, can also be generalized to more complex structures such as 3rd order tensors for modeling transitive verbs (Grefenstette et al., 2013). Socher et al. (2012) proposed a more complex and flexible framework based on matrix-vector representations. Each word or lexical node in a parsing tree is assigned a vector (representing inherent meaning of the constituent) and a matrix (controlling the behavior to modify the meaning of 434 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 434–442, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Model Composition function Parameters ��⃗u��2 2⃗v + �λ � ��⟨⃗u,⃗v⟩⃗u λ Fulladd W1⃗u + W2⃗v W1, </context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. In Proceedings of IWCS, pages 131–142, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
<author>Mark Steyvers</author>
<author>Josh Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<pages>114--211</pages>
<contexts>
<context position="1830" citStr="Griffiths et al., 2007" startWordPosition="257" endWordPosition="260">ighdimensional semantic space, where the dimensions are given by co-occurring contextual features. The intuition behind these models lies in the fact that words which are similar in meaning often occur in similar contexts, e.g., moon and star might both occur with sky, night and bright. This leads to convenient ways to measure similarity between different words using geometric methods (e.g., the cosine of the angle between two vectors that summarize their contextual distribution). Distributional semantic models have been successfully applied to many tasks in linguistics and cognitive science (Griffiths et al., 2007; Foltz et al., 1998; Laham, 1997; McDonald and Brew, 2004). However, most of these tasks only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clar</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Tom Griffiths, Mark Steyvers, and Josh Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114:211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="2655" citStr="Guevara (2010)" startWordPosition="383" endWordPosition="384">uch as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response </context>
<context position="6659" citStr="Guevara (2010)" startWordPosition="1018" endWordPosition="1019">ined by decomposing one of the input vectors, say ⃗v, into a vector parallel to u⃗ and an orthogonal vector, and then dilating only the parallel vector by a factor λ before re-combining (formula in Table 1). Mitchell and Lapata also propose a simple multiplicative model in which the output components are obtained by component-wise multiplication of the corresponding input components. We use its natural weighted extension (Mult), introduced by Dinu et al. (2013), that takes w1 and w2 powers of the components before multiplying, such that each phrase component pi is given by: pi = uw1 i vw2 i . Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p⃗ = W1⃗u + W2⃗v. Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as function application. For example, Baroni and Zamparelli model adjective-noun phrases by treating the adjective as a regression function from nouns onto (modified) nouns. Given that linear fun</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome Friedman</author>
</authors>
<date>2009</date>
<booktitle>The Elements of Statistical Learning, 2nd ed.</booktitle>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="10458" citStr="Hastie et al., 2009" startWordPosition="1642" endWordPosition="1645">t prosper) in meaning under some distance metric in semantic vector space. The same training and composition scheme is applied for other types of functors (e.g., adjectives and determiners). All the above mentioned composition models are evaluated within this scheme, but note that in the case of Add, Dil, Mult and Fulladd, a single set of parameters is obtained across all functors of a certain syntactic category. 3 Pathwise Optimized Elastic-net Algorithm The elastic-net regression method (Zou and Hastie, 2005) is proposed as a compromise between lasso (Tibshirani, 1996) and ridge regression (Hastie et al., 2009). Suppose there are N observation pairs (xi, yi), here xi ∈ Rp is the ith training sample and yi ∈ R is the corresponding response variable in the typical regression setting. For simplicity, assume the xij are standardized: ENi=1 x2 ij= 1, for j = 1, ... , p. The elastic-net solves the following problem: � �N 1 min (β0,β)ERP+1 N i=1 (1) where Pα(β) = λ((1 − α)12 ∥ β ∥2ℓ2 +αβℓ1) [12(1 − α)β2j + α|βj|�. P is the elastic-net penalty, and it is a compromise between the ridge regression penalty and the lasso penalty. The merit of the elastic-net penalty depends on two facts: the first is that elast</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning, 2nd ed. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darrell Laham</author>
</authors>
<title>Latent Semantic Analysis approaches to categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>979</pages>
<contexts>
<context position="1863" citStr="Laham, 1997" startWordPosition="265" endWordPosition="266">ensions are given by co-occurring contextual features. The intuition behind these models lies in the fact that words which are similar in meaning often occur in similar contexts, e.g., moon and star might both occur with sky, night and bright. This leads to convenient ways to measure similarity between different words using geometric methods (e.g., the cosine of the angle between two vectors that summarize their contextual distribution). Distributional semantic models have been successfully applied to many tasks in linguistics and cognitive science (Griffiths et al., 2007; Foltz et al., 1998; Laham, 1997; McDonald and Brew, 2004). However, most of these tasks only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of </context>
</contexts>
<marker>Laham, 1997</marker>
<rawString>Darrell Laham. 1997. Latent Semantic Analysis approaches to categorization. In Proceedings of CogSci, page 979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lee</author>
<author>Sebastian Seung</author>
</authors>
<title>Algorithms for Non-negative Matrix Factorization.</title>
<date>2000</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="18636" citStr="Lee and Seung (2000)" startWordPosition="3040" endWordPosition="3043">il NMF 300 0.1288 Mult NMF 250 0.2246 Fulladd SVD 300 0.0461 Lexfunc SVD 250 0.2673 Fulllex NMF 300 0.2682 EnetLex SVD 250 0.3239 Table 2: Best performance comparison for intransitive verb sentence composition. ing 2.8 billion tokens.1 Word co-occurrences are collected within sentence boundaries (with a maximum of a 50-words window around the target word). Following Dinu et al. (2013), we use the top 10K most frequent content lemmas as context features, Pointwise Mutual Information as weighting method and we reduce the dimensionality of the data by both Non-negative Matrix Factorization (NMF, Lee and Seung (2000)) and Singular Value Decomposition (SVD). For both data dimensionality reduction techniques, we experiment with different numbers of dimension varying from 50 to 300 with a step of 50. Since the Mult model works very poorly when the input vectors contain negative values, as is the case with SVD, for this model we report result distributions across the 6 NMF variations only. We use the DIStributional SEmantics Composition Toolkit (DISSECT)2 which provides implementations for all models we use for comparison. Following Dinu and colleagues, we used ordinary least-squares to estimate Fulladd and r</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel Lee and Sebastian Seung. 2000. Algorithms for Non-negative Matrix Factorization. In Proceedings of NIPS, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
<author>Chris Brew</author>
</authors>
<title>A distributional model of semantic context effects in lexical processing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>17--24</pages>
<location>Barcelona,</location>
<contexts>
<context position="1889" citStr="McDonald and Brew, 2004" startWordPosition="267" endWordPosition="270">iven by co-occurring contextual features. The intuition behind these models lies in the fact that words which are similar in meaning often occur in similar contexts, e.g., moon and star might both occur with sky, night and bright. This leads to convenient ways to measure similarity between different words using geometric methods (e.g., the cosine of the angle between two vectors that summarize their contextual distribution). Distributional semantic models have been successfully applied to many tasks in linguistics and cognitive science (Griffiths et al., 2007; Foltz et al., 1998; Laham, 1997; McDonald and Brew, 2004). However, most of these tasks only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks h</context>
</contexts>
<marker>McDonald, Brew, 2004</marker>
<rawString>Scott McDonald and Chris Brew. 2004. A distributional model of semantic context effects in lexical processing. In Proceedings of ACL, pages 17–24, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2545" citStr="Mitchell and Lapata (2008)" startWordPosition="366" endWordPosition="369"> only deal with isolated words, and there is a strong need to construct representations for longer linguistic structures such as phrases and sentences. In order to achieve this goal, the principle of compositionality of linguistic structures, which states that complex linguistic structures can be formed through composition of simple elements, is applied to distributional vectors. Therefore, in recent years, the problem of composition within distributional models has caught many researchers’ attention (Clark, 2013; Erk, 2012). A number of compositional frameworks have been proposed and tested. Mitchell and Lapata (2008) propose a set of simple component-wise operations, such as multiplication and addition. Later, Guevara (2010) and Baroni and Zamparelli (2010) proposed more elaborate methods, in which composition is modeled as matrix-vector multiplication operations. Particularly new to their approach is the proposal to estimate model parameters by minimizing the distance of the composed vectors to corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estim</context>
<context position="5721" citStr="Mitchell and Lapata (2008" startWordPosition="850" endWordPosition="853">el for prediction. Besides better prediction accuracy, the elastic-net method also brings interpretability to the composition procedure through sparsity constraints on the model. The rest of this paper is organized as follows: In Section 2, we give details on the above-mentioned composition models, which will be used for comparison in our experiments. In Section 3, we describe the pathwise optimized elastic-net regression algorithm. Experimental evaluation on three composition tasks is provided in Section 4. In Section 5 we conclude and suggest directions for future work. 2 Composition Models Mitchell and Lapata (2008; 2010) present a set of simple but effective models in which each component of the output vector is a function of the corresponding components of the inputs. Given input vectors u⃗ and ⃗v, the weighted additive model (Add) returns their weighted sum: p⃗ = w1⃗u + w2⃗v. In the dilation model (Dil), the output vector is obtained by decomposing one of the input vectors, say ⃗v, into a vector parallel to u⃗ and an orthogonal vector, and then dilating only the parallel vector by a factor λ before re-combining (formula in Table 1). Mitchell and Lapata also propose a simple multiplicative model in wh</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="15857" citStr="Mitchell and Lapata (2010)" startWordPosition="2590" endWordPosition="2593">mbda such that error is within 1 standard error of the minimum, and the left vertical dotted line corresponds to the λ value which gives minimum cross-validated error. In this case, the λ value of minimum cross-validated error is 0.106, and its log is -2.244316. In all of our experiments, we will select models corresponding to minimum training-data cross-validated error. 4 Experiments 4.1 Datasets We evaluate on the three data sets described below, that were also used by Dinu et al. (2013), our most yi − = 437 direct point of comparison. Intransitive sentences The first dataset, introduced by Mitchell and Lapata (2010), focuses on the composition of intransitive verbs and their noun subjects. It contains a total of 120 sentence pairs together with human similarity judgments on a 7-point scale. For example, value slumps/value declines is scored 7, skin glows/skin burns is scored 1. On average, each pair is rated by 30 participants. Rather than evaluating against mean scores, we use each rating as a separate data point, as done by Mitchell and Lapata. We report Spearman correlations between human-assigned scores and cosines of model-generated vector pairs. Adjective-noun phrases Turney (2012) introduced a dat</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="3552" citStr="Socher et al. (2012)" startWordPosition="513" endWordPosition="516">corpus-observed phrase vectors. For example, Baroni and Zamparelli (2010) consider the case of Adjective-Noun composition and model it as matrix-vector multiplication: adjective matrices are parameters to be estimated and nouns are cooccurrence vectors. The model parameter estimation procedure becomes a multiple response multivariate regression problem. This method, that, following Dinu et al. (2013) and others, we term the lexical function composition model, can also be generalized to more complex structures such as 3rd order tensors for modeling transitive verbs (Grefenstette et al., 2013). Socher et al. (2012) proposed a more complex and flexible framework based on matrix-vector representations. Each word or lexical node in a parsing tree is assigned a vector (representing inherent meaning of the constituent) and a matrix (controlling the behavior to modify the meaning of 434 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 434–442, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Model Composition function Parameters ��⃗u��2 2⃗v + �λ � ��⟨⃗u,⃗v⟩⃗u λ Fulladd W1⃗u + W2⃗v W1, W2 E Rmxm Lexfunc Au⃗v</context>
<context position="7620" citStr="Socher et al. (2012)" startWordPosition="1168" endWordPosition="1171">ing inspiration from formal semantics, characterize composition as function application. For example, Baroni and Zamparelli model adjective-noun phrases by treating the adjective as a regression function from nouns onto (modified) nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a functor (such as the adjective) is represented by a matrix Au to be composed with the argument vector v⃗ (e.g., the noun) by multiplication, returning the lexical function (Lexfunc) representation of the phrase: p⃗= Au⃗v. The method proposed by Socher et al. (2012) can be seen as a combination and non-linear extension of Fulladd and Lexfunc (that Dinu and colleagues thus called Fulllex) in which both phrase elements act as functors (matrices) and arguments (vectors). Given input terms u and v represented by (⃗u, Au) and (⃗v, Av), respectively, their composition vector is obtained by applying first a linear transformation and then the hyperbolic tangent function to the concatenation of the products Au⃗v and Av⃗u (see Table 1 for the equation). Socher and colleagues also present a way to construct matrix representations for specific phrases, needed to sca</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>58</volume>
<issue>1</issue>
<contexts>
<context position="10415" citStr="Tibshirani, 1996" startWordPosition="1636" endWordPosition="1637">e close to similar concepts (e.g., export prosper) in meaning under some distance metric in semantic vector space. The same training and composition scheme is applied for other types of functors (e.g., adjectives and determiners). All the above mentioned composition models are evaluated within this scheme, but note that in the case of Add, Dil, Mult and Fulladd, a single set of parameters is obtained across all functors of a certain syntactic category. 3 Pathwise Optimized Elastic-net Algorithm The elastic-net regression method (Zou and Hastie, 2005) is proposed as a compromise between lasso (Tibshirani, 1996) and ridge regression (Hastie et al., 2009). Suppose there are N observation pairs (xi, yi), here xi ∈ Rp is the ith training sample and yi ∈ R is the corresponding response variable in the typical regression setting. For simplicity, assume the xij are standardized: ENi=1 x2 ij= 1, for j = 1, ... , p. The elastic-net solves the following problem: � �N 1 min (β0,β)ERP+1 N i=1 (1) where Pα(β) = λ((1 − α)12 ∥ β ∥2ℓ2 +αβℓ1) [12(1 − α)β2j + α|βj|�. P is the elastic-net penalty, and it is a compromise between the ridge regression penalty and the lasso penalty. The merit of the elastic-net penalty de</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Rob Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1106" citStr="Turney and Pantel, 2010" startWordPosition="144" endWordPosition="147">astic-net regression method to estimate the composition parameters, and compare the resulting model with several recent alternative approaches in the task of composing simple intransitive sentences, adjective-noun phrases and determiner phrases. Experimental results demonstrate that the lexical function model estimated by elastic-net regression achieves better performance, and it provides good qualitative interpretability through sparsity constraints on model parameters. 1 Introduction Vector-based distributional semantic models of word meaning have gained increased attention in recent years (Turney and Pantel, 2010). Different from formal semantics, distributional semantics represents word meanings as vectors in a highdimensional semantic space, where the dimensions are given by co-occurring contextual features. The intuition behind these models lies in the fact that words which are similar in meaning often occur in similar contexts, e.g., moon and star might both occur with sky, night and bright. This leads to convenient ways to measure similarity between different words using geometric methods (e.g., the cosine of the angle between two vectors that summarize their contextual distribution). Distribution</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<pages>44--533</pages>
<contexts>
<context position="16440" citStr="Turney (2012)" startWordPosition="2682" endWordPosition="2683"> by Mitchell and Lapata (2010), focuses on the composition of intransitive verbs and their noun subjects. It contains a total of 120 sentence pairs together with human similarity judgments on a 7-point scale. For example, value slumps/value declines is scored 7, skin glows/skin burns is scored 1. On average, each pair is rated by 30 participants. Rather than evaluating against mean scores, we use each rating as a separate data point, as done by Mitchell and Lapata. We report Spearman correlations between human-assigned scores and cosines of model-generated vector pairs. Adjective-noun phrases Turney (2012) introduced a dataset including both noun-noun compounds and adjective-noun phrases (ANs). We focus on the latter, and we frame the task as in Dinu et al. (2013). The dataset contains 620 ANs, each paired with a single-noun paraphrase. Examples include: upper side/upside, false belief/fallacy and electric refrigerator/fridge. We evaluate a model by computing the cosine of all 20K nouns in our semantic space with the target AN, and looking at the rank of the correct paraphrase in this list. The lower the rank, the better the model. We report median rank across the test items. Determiner phrases</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. J. Artif. Intell. Res.(JAIR), 44:533–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Falucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1263--1271</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6686" citStr="Zanzotto et al. (2010)" startWordPosition="1021" endWordPosition="1024"> one of the input vectors, say ⃗v, into a vector parallel to u⃗ and an orthogonal vector, and then dilating only the parallel vector by a factor λ before re-combining (formula in Table 1). Mitchell and Lapata also propose a simple multiplicative model in which the output components are obtained by component-wise multiplication of the corresponding input components. We use its natural weighted extension (Mult), introduced by Dinu et al. (2013), that takes w1 and w2 powers of the components before multiplying, such that each phrase component pi is given by: pi = uw1 i vw2 i . Guevara (2010) and Zanzotto et al. (2010) explore a full form of the additive model (Fulladd), where the two vectors entering a composition process are pre-multiplied by weight matrices before being added, so that each output component is a weighted sum of all input components: p⃗ = W1⃗u + W2⃗v. Baroni and Zamparelli (2010) and Coecke et al. (2010), taking inspiration from formal semantics, characterize composition as function application. For example, Baroni and Zamparelli model adjective-noun phrases by treating the adjective as a regression function from nouns onto (modified) nouns. Given that linear functions can be expressed by </context>
</contexts>
<marker>Zanzotto, Korkontzelos, Falucchi, Manandhar, 2010</marker>
<rawString>Fabio Zanzotto, Ioannis Korkontzelos, Francesca Falucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of COLING, pages 1263– 1271, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zou</author>
<author>Trevor Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<volume>67</volume>
<issue>2</issue>
<contexts>
<context position="10354" citStr="Zou and Hastie, 2005" startWordPosition="1624" endWordPosition="1627">the concept for export boom. And the concept export boom should be close to similar concepts (e.g., export prosper) in meaning under some distance metric in semantic vector space. The same training and composition scheme is applied for other types of functors (e.g., adjectives and determiners). All the above mentioned composition models are evaluated within this scheme, but note that in the case of Add, Dil, Mult and Fulladd, a single set of parameters is obtained across all functors of a certain syntactic category. 3 Pathwise Optimized Elastic-net Algorithm The elastic-net regression method (Zou and Hastie, 2005) is proposed as a compromise between lasso (Tibshirani, 1996) and ridge regression (Hastie et al., 2009). Suppose there are N observation pairs (xi, yi), here xi ∈ Rp is the ith training sample and yi ∈ R is the corresponding response variable in the typical regression setting. For simplicity, assume the xij are standardized: ENi=1 x2 ij= 1, for j = 1, ... , p. The elastic-net solves the following problem: � �N 1 min (β0,β)ERP+1 N i=1 (1) where Pα(β) = λ((1 − α)12 ∥ β ∥2ℓ2 +αβℓ1) [12(1 − α)β2j + α|βj|�. P is the elastic-net penalty, and it is a compromise between the ridge regression penalty a</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>