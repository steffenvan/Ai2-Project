<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.91782">
The role of memory in superiority violation gradience
</title>
<author confidence="0.95471">
Marisa Ferrara Boston
</author>
<affiliation confidence="0.968283">
Cornell University
</affiliation>
<address confidence="0.686391">
Ithaca, NY, USA
</address>
<email confidence="0.995972">
mfb74@cornell.edu
</email>
<sectionHeader confidence="0.993824" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999138">
This paper examines how grammatical and
memory constraints explain gradience in
superiority violation acceptability. A com-
putational model encoding both categories
of constraints is compared to experimental
evidence. By formalizing memory capac-
ity as beam-search in the parser, the model
predicts gradience evident in human data.
To predict attachment behavior, the parser
must be sensitive to the types of nominal
intervenors that occur between a wh-filler
and its head. The results suggest memory
is more informative for modeling violation
gradience patterns than grammatical con-
straints.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998626333333333">
Sentences that include two wh-words, as in Exam-
ple (1), are often considered difficult by English
speakers.
</bodyText>
<listItem confidence="0.817227">
(1) *Diego asked what, who2 read?
</listItem>
<bodyText confidence="0.9997855">
This superiority effect holds when a second wh-
word, who in this example, acts as a barrier to at-
tachment of the first wh-word and its verb (Chom-
sky, 1973).
The difficulty is ameliorated when the wh-
words are switched to which-N, or which-Noun,
form as in Examples (2) and (3) (Karttunen, 1977;
Pesetsky, 1987). This is confirmed by experimen-
tal evidence (Arnon et al., To appear; Hofmeister,
2007).
</bodyText>
<listItem confidence="0.99916">
(2) ?Diego asked which book who read?
(3) ?Diego asked what which girl read?
</listItem>
<bodyText confidence="0.998713727272727">
Memory is often implicated as the source of this
gradience, though it is unclear which aspects of
memory best model experimental results. This
computational model encodes grammatical and
memory-based constraints proposed in the liter-
ature to account for the phenomenon. The re-
sults demonstrate that as memory resources are in-
creased, the parser can model the human pattern if
it is sensitive to the types of nominal intervenors.
This supports memory-based accounts of superi-
ority violation (SUV) gradience.
</bodyText>
<sectionHeader confidence="0.997246" genericHeader="method">
2 Explanations for SUV gradience
</sectionHeader>
<bodyText confidence="0.99951575">
This section details grammatical and reductionist
explanations for SUV gradience, motivating the
encoding of various constraints in the computa-
tional model.
</bodyText>
<subsectionHeader confidence="0.972965">
2.1 Grammatical explanations
</subsectionHeader>
<bodyText confidence="0.945093916666667">
Grammatical accounts of gradience rely on intrin-
sic discourse differences between phrases that al-
low for SUVs and those that do not. In this work,
which-N phrases are examples of the former, and
so-called bare wh-phrases (including who and
what) the latter1. Rizzi (1990) incorporates ideas
from Pesetsky’s D-Linking, or discourse-linking,
hypothesis (1987) into a grammatical account of
SUV gradience, Relativized Minimality. He ar-
gues that referential phrases like which-N refer
to a pre-established set in the discourse and are
not subject to the same constraints on attachment
as non-referential phrases, like what. Which book
delimits a set of possible discourse entities, books,
and is more restrictive than what, which could in-
stead delimit sets of books, cats, or abstract en-
tities. The Relativized Minimality hypothesis ac-
counts for SUV gradience on the basis of this cate-
gorical separation on wh-phrases in the discourse.
1Both bare phrases and which-N phrases could have the
appropriate discourse conditions to allow for superiority vio-
lations, and vice versa. However, to relate the theory’s pre-
dictions to the experiment modeled here, I use a categorical
split between which-N and bare wh-phrases.
</bodyText>
<page confidence="0.980252">
36
</page>
<note confidence="0.987642">
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 36–44,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.992769">
2.2 Reductionist explanations
</subsectionHeader>
<bodyText confidence="0.999849125">
Many grammatical accounts, particularly those
that are grounded in cognitive factors, incorporate
some element of processing or memory in their ex-
planations (Phillips, Submitted). Reductionist ac-
counts are different; their proponents do not be-
lieve that superiority requires a grammatical ex-
planation. Rather, SUVs that appear ungrammat-
ical, such as Example (1), are the result of severe
processing difficulty alone.
These accounts attribute processing difficulty to
memory: severe memory resource limitations ac-
count for ungrammatical sentences in SUVs, and
increased memory resources allow for more ac-
ceptable sentences. This is the central idea be-
hind Hofmeister’s Memory Facilitation Hypothe-
sis (2007):
</bodyText>
<subsectionHeader confidence="0.829276">
Memory Facilitation Hypothesis
</subsectionHeader>
<bodyText confidence="0.992943722222222">
Linguistic elements that encode more informa-
tion (lexical, semantic, syntactic, etc.) facili-
tate their own subsequent retrieval from memory
(Hofmeister, 2007, p.4)2.
This memory explanation is central to activation-
based memory hypotheses previously proposed
in the psycholinguistic literature, such as CC-
READER (Just and Carpenter, 1992), ACT-R
(Lewis and Vasishth, 2005), and 4CAPS (Just and
Varma, 2007). This work considers activation,
and manipulates memory resources by varying the
number of analyses the parser considers at each
parse step.
Table 1 lists memory factors that may contribute
to SUV gradience. They are sensitive to the mem-
ory resources available during syntactic parsing,
but account for memory differently. Below I de-
scribe these variations.
</bodyText>
<subsectionHeader confidence="0.681806">
2.2.1 Distance and the DLT
</subsectionHeader>
<bodyText confidence="0.9992947">
Distance, as measured by the number of words
between, for example, a wh-word and its verb,
has been argued to affect sentence comprehen-
sion (Wanner and Maratsos, 1978; Rambow and
Joshi, 1994; Gibson, 1998). Experimental evi-
dence supports this claim, but there exist a num-
ber of anomalous results that resist explanation in
terms of distance alone (Gibson, 1998; Hawkins,
1999; Gibson, 2000). For example, it is not the
case that processing difficulty increases solely as
</bodyText>
<footnote confidence="0.921186">
2Recent work by Hofmeister and colleagues attributes the
advantage to a decrease in memory interference rather than
retrieval facilitation (Submitted), but the spirit of the work
remains the same.
</footnote>
<bodyText confidence="0.99996375">
a function of the number of words in a sentence.
However, it is possible that SUV gradience could
be affected by this simple metric.
The Dependency Locality Theory (DLT) (Gib-
son, 2000) is a more linguistically-informed mea-
sure of distance. The DLT argues that an accurate
model of sentence processing difficulty is sensitive
to the number and discourse-status (given or new)
of nominal intervenors that occur across a particu-
lar distance. The DLT’s sensitivity to discourse-
newness integrates aspects of D-linking: which
book, for example, requires that books already be a
part of the discourse, though what does not (Gun-
del et al., 1993; Warren and Gibson, 2002). The
DLT has been demonstrated to model difficulty in
ways that simple distance alone can not (Grodner
and Gibson, 2005).
This study also considers a stronger version of
the DLT, Intervenors. Intervenors considers both
the number and part-of-speech (POS) of nominal
intervenors between a wh-word and its head. This
feature is sensitive to nuanced differences between
nominal intervenors, providing a more accurate
model of the Memory Facilitation Hypothesis.
</bodyText>
<subsectionHeader confidence="0.954584">
2.2.2 Stack memory
</subsectionHeader>
<bodyText confidence="0.999842571428571">
Distance can also be measured in terms of the
parser’s internal resources. The computational
model described here incorporates a stack mem-
ory. Although stacks are not accurate models of
human memory (McElree, 2000), this architec-
tural property may provide insight into how mem-
ory affects SUV gradience.
</bodyText>
<subsectionHeader confidence="0.944095">
2.2.3 Activation and interference
</subsectionHeader>
<bodyText confidence="0.999978176470588">
Sentence processing difficulty has been attributed
to the amount of time it takes to retrieve a word
from memory. Lewis &amp; Vasishth (2005) find sup-
port for this argument by applying equations from
a general cognitive model, ACT-R (Adaptive Con-
trol of Thought-Rational) (Anderson, 2005), to a
sentence processsing model. Their calculation of
retrieval time, henceforth retrieval, is sensitive to a
word’s activation and its similarity-based interfer-
ence with other words in memory (Gordon et al.,
2002; Van Dyke and McElree, 2006). Activation,
Interference, and the conjunction of the two in the
form of Retrieval, are considered in this work.
The grammatical and memory-based accounts
described above offer several explanations for
SUV gradience. They can be represented along a
continuum, where the type of information consid-
</bodyText>
<page confidence="0.998857">
37
</page>
<subsectionHeader confidence="0.603578">
Hypothesis Sensitive to
</subsectionHeader>
<table confidence="0.893633571428571">
Distance String distance between words.
DLT Number of nominal intervenors.
Intervenors POS of nominal intervenors.
Stack Memory Elements currently in parser memory.
Baseline Activation Amount structure is activated in memory.
Interference Amount of competition from similar words in memory.
Retrieval Retrieval time of word from memory.
</table>
<tableCaption confidence="0.999147">
Table 1: Memory-based sentence processing theories.
</tableCaption>
<listItem confidence="0.91379675">
ered in memory varies from the simple (Distance)
to complex (Retrieval), as in (4).
(4) Distance &lt; DLT &lt; Intervenors &lt; Stack Memory
&lt; Activation &lt; Interference &lt; Retrieval
</listItem>
<bodyText confidence="0.99956675">
Despite this representation, in this work I con-
sider each as an independent theory. Together,
they form the hypothesis set in the model, se-
lected because they represent the major explana-
tions posited for gradience in SUVs and related
phenomena, like islands.
The computational model not only formalizes
the memory accounts, but also provides a frame-
work for memory-based factors that require a
computational model, such as retrieval. The re-
sults determine memory factors that best account
for SUV gradience patterns.
</bodyText>
<sectionHeader confidence="0.998348" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9997035">
The test set for SUV gradience is the experimental
results from Arnon et al. (To appear). The experi-
ment tests gradience across four conditions, shown
in Examples (5)-(8).
</bodyText>
<listItem confidence="0.998790166666667">
(5) Pat wondered what who read. (bare.bare)
(6) Pat wondered what which student read.
(bare.which)
(7) Pat wondered which book who read. (which.bare)
(8) Pat wondered which book which student read.
(which.which)
</listItem>
<bodyText confidence="0.959677827586207">
The conditions substitute the wh-type of both wh-
fillers and wh-intervenors in the island context.
In Example (5) both the filler and intervenor are
bare (the bare.bare condition), whereas in Exam-
ple (8), both the filler and intervenor are which-Ns
(which.which). Examples (6) and (7) provide the
other possible configurations.
Arnon and colleagues find which.which to be
the fastest condition. Figure 1 depicts these re-
sults. The other conditions are more difficult,
Figure 1: Reading time is fastest in the
which.which condition (Arnon et al., To appear,
p.5).
at varying levels: the which.bare condition is
less difficult than the bare.which condition, and
both are less difficult than the bare.bare condi-
tion. These results roughly pattern with accept-
ability judgments discussed in syntactic literature
(Pesetsky, 1987).
Corpora for superiority processing results do
not exist. Further, few studies on SUVs incorpo-
rate the same structures, techniques, and experi-
mental conditions. Although Arnon et al. consid-
ered 20 lexical variations, the unlexicalized parser
can not distinguish these variations. Therefore, the
parser is only evaluated on these four sentences;
however, they are taken to represent classes of
structures that generalize to all SUV gradience in
English.
</bodyText>
<subsectionHeader confidence="0.99881">
3.1 The parsing model
</subsectionHeader>
<bodyText confidence="0.999950142857143">
The computational model is based on Nivre’s
(2004) dependency parsing algorithm. The al-
gorithm builds directed, word-to-word analyses
of test input following the Dependency Gram-
mar syntactic formalism (Tesni`ere, 1959; Hays,
1964). Figure 2 depicts the full dependency anal-
ysis of the which.which condition from Example
</bodyText>
<page confidence="0.998366">
38
</page>
<figureCaption confidence="0.9785605">
Figure 2: A dependency analysis of the
which.which condition.
</figureCaption>
<bodyText confidence="0.995962375">
(8), where heads point to their dependents via arcs.
The Nivre parser assembles dependency struc-
ture incrementally by passing through parser states
that aggregate four data structures, shown in Table
2. The stack σ holds parsed words that require fur-
ther analysis, and the list τ holds words yet to be
parsed. h and d encode the current list of depen-
dency relations.
</bodyText>
<listItem confidence="0.78111925">
σ A stack of already-parsed unreduced words.
τ An ordered input list of words.
h A function from dependent words to heads.
d A function from dependent words to arc types.
</listItem>
<tableCaption confidence="0.991072">
Table 2: Parser configuration.
</tableCaption>
<bodyText confidence="0.999859">
The parser transitions from state to state via four
possible actions. Shift and Reduce manipulate
σ. LeftArc and RightArc build dependencies
between σ1 (the element at the top of the stack)
and τ1 (the next input word); LeftArc makes
σ1 the dependent, and RightArc makes σ1 the
head.
The parser determines actions by consulting a
probability model derived from the Brown Corpus
(Francis and Kucera, 1979). The corpus is con-
verted to dependencies via the Pennconverter tool
(Johansson and Nugues, 2007). The parser is then
simulated on these dependencies, providing a cor-
pus of parser states and subsequent actions that
form the basis of the training data. Because the
parser is POS-based, this corpus is manipulated in
two ways to sensitize it to the differences in the
experimental conditions. First, the corpus is given
finer-grained POS tags for each of the wh-words,
described in Table 3.
Secondly, which-N dependencies are encoded
as DPs (determiner phrases) and are headed by
the wh-phrase (Abney, 1987). This ensures the
parser differentiates a wh-word retrieval from a
simple noun retrieval, which is necessary for sev-
eral of the memory-based constraints. Other noun
phrases are headed by their nouns. The corpus is
</bodyText>
<table confidence="0.953551285714286">
Original POS Wh Example
WP WP-WHAT what
WP WP-WHO who
WDT WDT-WHICH which book
WDT WDT-WHAT what book
IN IN-WHETHER whether
WRB WRB how/why/when
</table>
<tableCaption confidence="0.992917">
Table 3: POS for wh fillers and intervenors.
</tableCaption>
<figureCaption confidence="0.9556375">
Figure 3: The relevant attachment is between
which and read.
</figureCaption>
<bodyText confidence="0.999861">
not switched to a fully DP analysis to preserve as
many of the original relationships as possible.
I extend the Nivre algorithm to allow for beam
search within the parser state space. This allows
the parser to consider different degrees of paral-
lelism k, and manipulate the amount of memory
allotted to incremental parse states. This manipu-
lation serves as a model of variation in an individ-
ual’s memory as a sentence is parsed.
</bodyText>
<subsectionHeader confidence="0.991364">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999926888888889">
To determine how well the accounts model the ex-
perimental data, I consider the likelihood of the
parser resolving the island-violating dependency
between wh-fillers and their verbs in the Arnon et
al. data. In terms of the dependency parser, the test
determines whether the parser creates a LeftArc
attachment in a state where which or what is σ1
and read is τ1. The dependency structure associ-
ated with this parser state is depicted in Figure 3
for the which.which condition.
This evaluation is categorical rather than statis-
tical: SUV-processing is based on the decision to
form an attachment in a superiority-violating con-
text, given four experimental sentences. While fu-
ture work will incorporate more experiments for
robust statistical analysis, this work focuses on a
small subset that generalizes to the greater phe-
nomenon.
</bodyText>
<subsectionHeader confidence="0.998486">
3.3 Encoding constraints
</subsectionHeader>
<bodyText confidence="0.999833">
The parser determines actions on the basis of prob-
abilistic models, or features. In this work, I en-
</bodyText>
<page confidence="0.997155">
39
</page>
<bodyText confidence="0.929770375">
code each of the grammatical and memory-based
explanations as its own feature. I normalize the
weights from the LIBLINEAR (Lin et al., 2008)
SVM classification tool to determine probabilities
for each parser action (LeftArc, RightArc,
Shift, Reduce) . The features are sensitive to
specific aspects of the current parser state, allow-
ing an examination of whether the features sug-
gest the superiority violating LeftArc action in
the context depicted in Figure 3. The prediction is
that attachment will be easiest in the which.which
condition and impossible in the other conditions
when memory resources are limited (k=1), as in
Table 4.
to include nominal wh-words to more accurately
model the experimental conditions.
</bodyText>
<table confidence="0.739945769230769">
Intervenor POS Example
NN book
NNS books
PRP they
NNP Pat
NNPS Americans
WP-WHAT what
WP-WHO who
WDT-WHICH which book
WDT-WHAT what book
Table 6: POS for nominal intervenors.
Condition b.b b.w w.b w.w
Attachment N N N Y
</table>
<tableCaption confidence="0.9543378">
Table 4: LeftArc attachments given Arnon et al.
(To appear) results. Y = Yes, N=No.
Table 5 depicts the full list of grammatical and
memory-based features considered in this study,
which are detailed below.
</tableCaption>
<subsectionHeader confidence="0.900384">
3.3.1 Grammatical constraint
</subsectionHeader>
<bodyText confidence="0.9999232">
In Relativized Minimality, referential noun
phrases override superiority violations, whereas
non-referential noun phrases do not. This con-
straint is included as a probabilistic feature of
the parser, RELMIN, specified in Table 5. The
condition holds if a non-referential NP (what) is
in σ1 (RELMIN=Yes). But the violation condition
does not hold (RELMIN=No) if a non-referential
NP (which) is in σ1. The feature categorically
separates which-N and bare wh-phrases to capture
the Relativized Minimality predictions for these
experimental sentences. The probabilistic feature
also adds a grammatical gradience component to
the model, which is not proposed by the original
hypothesis.
</bodyText>
<subsectionHeader confidence="0.962473">
3.3.2 Memory constraints
</subsectionHeader>
<bodyText confidence="0.998993230769231">
The parser encodes each of the memory accounts
provided in Table 1 as probabilistic features. DIS-
TANCE, the simplest feature, determines parser ac-
tions on the basis of how far apart σ1 and τ1 are
in the string.
DLT and INTERVENORS require parser sen-
sitivity to the nominal intervenors between σ1
and τ1 according to Gibson’s DLT specification
(2000). Table 6 provides a list of the nominal inter-
venors considered. Gibson’s hierarchy is extended
The sequence of STACKNEXT features are sen-
sitive to the parser’s memory, in the form of the
POS of elements at varying depths of the stack.
These features are found to have high overall ac-
curacy in the Nivre parser (Nivre, 2004) and in hu-
man sentence processing modeling (Boston et al.,
2008).
ACTIVATION, INTERFERENCE, and RE-
TRIEVAL predictions are based on the sequence
of Lewis &amp; Vasisth (2005) calculations provided
in Equations 1-4. These equations require some
notion of duration, which is calculated as a func-
tion of parser actions and word retrieval times.
Table 7 describes this calculation, motivated by
the production rule time in Lewis &amp; Vasisth’s
ACT-R model.
</bodyText>
<subsectionHeader confidence="0.434028">
Transition Time
</subsectionHeader>
<equation confidence="0.623074">
LEFT 50 ms + 50 ms + Retrieval Time
RIGHT 50 ms + 50 ms + Retrieval Time
SHIFT 50ms
REDUCE 0ms
</equation>
<tableCaption confidence="0.867121">
Table 7: How time is determined in the parser.
</tableCaption>
<bodyText confidence="0.99982025">
Because only words at the top of the stack can
be retrieved, the following will be described for
σ1. Retrieval time for σ1 is based on its activation
A, calculated as in Equation 1.
</bodyText>
<equation confidence="0.9175145">
Ai = Bi +� Wj5ji (1)
9
</equation>
<bodyText confidence="0.9989432">
Total activation is the sum of two quantities, the
word’s baseline activation Bi and similarity-based
interference for that word, calculated in the sec-
ond addend of the equation. The baseline activa-
tion, provided in Equation 2, increases with more
</bodyText>
<page confidence="0.995014">
40
</page>
<table confidence="0.914614615384615">
Feature Feature Type Includes
Grammar
RELMIN Yes/No σ1 wh−word :: intervenorswh−word(σ1 ...τ1)
Memory
DISTANCE String Position τ1 − σ1
DLT Count intervenorsnom(σ1 ...τ1)
INTERVENORS POS intervenorsnom(σ1 ...τ1)
STACK1NEXT POS σ1 :: τ1
STACK2NEXT POS σ1 :: σ2 :: τ1
STACK3NEXT POS σ1 :: σ2 :: σ3 :: τ1
ACTIVATION Value baselineActivation(σ1)
INTERFERENCE Value interference(σ1)
RETRIEVAL Time (ms.) retrievalTime(σ1)
</table>
<tableCaption confidence="0.999621">
Table 5: Feature specification.:: indicates concatenation.
</tableCaption>
<bodyText confidence="0.99746875">
recent retrievals at time tj. This implementation
follows standard ACT-R practice in setting the de-
cay rate d to 0.5 (Lewis and Vasishth, 2005; An-
derson, 2005).
</bodyText>
<equation confidence="0.9427915">
�
tj −d �(2)
</equation>
<bodyText confidence="0.999028636363636">
σ1’s activation can decrease if competitors, or
other words with similar grammatical categories,
have already been parsed. In Equation (1), Wj de-
notes weights associated with the retrieval cues j
that are shared with these competitors, and Sji
symbolizes the strengths of association between
cues j and the retrieved item i (σ1). For this
model, weights are set to 1 because there is only
one retrieval cue j in operation: the POS. The
strength of association Sji is computed as in Equa-
tion 3.
</bodyText>
<equation confidence="0.790367">
Sji = Smax − ln(fanj) (3)
</equation>
<bodyText confidence="0.999740777777778">
The fan, fanj, is the number of words that have
the same grammatical category as cue j, the POS.
The maximum degree of association between sim-
ilar items in memory is Smax which is set to 1.5
following Lewis &amp; Vasishth.
To get the retrieval time, in milliseconds, of σ1,
the activation value calculated in Equation 1 is in-
serted in Equation 4. The implementation follows
Lewis &amp; Vasishth in setting F to 0.14.
</bodyText>
<equation confidence="0.9873">
Ti = Fe−Ai (4)
</equation>
<bodyText confidence="0.9906278">
The time Ti is the quantity the parser is sensi-
tive to in determining attachments based on the
RETRIEVAL feature. Because it is possible that
SUVs are better modeled by only part of the re-
trieval equation, such as baseline activation or in-
terference, the implementation also considers AC-
TIVATION and INTERFERENCE features. The fea-
tures are sensitive to the quantities in the addends
in Equation 1, Bi and � W j Sji respectively.
j
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.99927692">
The results focus on whether the parser chooses
a LeftArc attachment when it is in the config-
uration depicted in Figure 3 given the grammati-
cal and memory constraints listed in Table 5. Ta-
ble 8 depicts the outcome, where Y signifies a
LeftArc attachment is preferred and N that it is
not.
Only one feature correctly patterns with the ex-
perimental evidence: INTERVENORS. It allows a
LeftArc in the which.which condition, and dis-
allows the arc in other conditions. The INTER-
VENORS feature also patterns with the experimen-
tal evidence as more memory is added. Table 9 de-
picts the LeftArc attachment for increasing lev-
els of k with this feature. At k=1, the parser only
chooses the attachment for the which.which con-
dition. At k=2, the parser chooses the attachment
for both which.which and which.bare. At k=3, it
chooses the attachment for all conditions. This
mimics the decreases in difficulty evident in Fig-
ure 1, and provides support for reductionist theo-
ries: if memory is restricted (k=1), only the easi-
est attachment is allowed. As memory increases,
more attachments are possible.
INTERVENORS is sensitive to the nominal in-
</bodyText>
<equation confidence="0.960915">
�
Bi = ln �
n
j=1
</equation>
<page confidence="0.997757">
41
</page>
<table confidence="0.9998298">
Condition b.b b.w w.b w.w
Experiment N N N Y
Grammar
RELMIN=YES N N N N
RELMIN=NO N N N N
Memory
DISTANCE N N N N
DLT N N N N
INTERVENORS N N N Y
STACK1NEXT N N N N
STACK2NEXT Y N Y N
STACK3NEXT Y Y Y Y
ACTIVATION N N N N
INTERFERENCE Y N N N
RETRIEVAL Y N N N
</table>
<tableCaption confidence="0.968897">
Table 8: LeftArc attachments for the experi-
mental data.
</tableCaption>
<table confidence="0.99944975">
Condition b.b b.w w.b w.w
INTERVENORS K=1 N N N Y
INTERVENORS K=2 N N Y Y
INTERVENORS K=3 Y Y Y Y
</table>
<tableCaption confidence="0.830986">
Table 9: INTERVENORS allows more attachments
as k increases.
</tableCaption>
<bodyText confidence="0.999074933333333">
tervenors between which and read. RETRIEVAL,
INTERFERENCE, and particularly DLT, should
also be sensitive to these intervenors. Despite their
similarity, none of these features are able to model
the attachment behavior in the experimental data.
The STACK3NEXT feature differs from the
other features in that it allows the LeftArc at-
tachment to occur in any of the conditions. Al-
though this does not match the interpretation of
the experimental results followed in this paper, it
leaves open the possibility that the feature could
model the data according to a different measure of
parsing difficulty, such as surprisal (Hale, 2001).
The RELMIN constraint is not able to model the
experimental results for gradience.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999978948275862">
The results demonstrate that modeling the exper-
imental data for SUV gradience requires a parser
that can vary memory resources as well as be sen-
sitive to the types of the nominal intervenors cur-
rently in memory. The gradience is modeled by
increasing memory resources, in the form of in-
creases in the beam-width. This demonstrates the
usefulness of varying both the types and amounts
of memory resources available in a computational
model of human sentence processing.
The positive results from the INTERVENORS
feature confirms the discourse accessibility hierar-
chy encoded in the DLT (Gundel et al., 1993; War-
ren and Gibson, 2002), but only when wh-words
are included as nominal intervenors. The results
also suggest that it is the type, and not just the
number of intervenors as suggested by the DLT,
that is important.
Further, the INTERVENORS feature does not
pattern with the DLT hypothesis. DLT assumes
that increasing the number of nominal inter-
venors causes sentence processing difficulty (Gib-
son, 2000; Warren and Gibson, 2002). Here, the
number of intervenors is increased, but sentence
processing is relatively easier. This effect is ex-
plained by the intrinsic difference between the
DLT and INTERVENORS features: INTERVENORS
provides more information to the parser, in the
form of the POS of all intervenors. This indicates
that certain intervenors help, rather than hinder,
the retrieval process.
The negative results demonstrate that other rep-
resentations of memory do not model SUV gra-
dience. If we consider this along the continuum
from (4), those features that take into account less
information than INTERVENORS (DISTANCE and
DLT) are too restrictive. Of those features that
are more complex than INTERVENORS, many are
too permissive, or permit the wrong attachments.
This pattern is also visible in the STACKNEXT
features: STACK1NEXT is too restrictive, while
STACK3NEXT too permissive. STACK2NEXT un-
fortunately permits the wrong attachments. This
pattern in the continuum indicates that an interme-
diate amount of memory information is required
to adequately model these results.
INTERFERENCE, which also considers competi-
tors in the intervening string, would seem likely
to pattern with the INTERVENORS results. In
fact, similarity-based interference and retrieval
have previously been argued to account for these
gradience patterns (Hofmeister et al., Submitted).
However, the only words considered as competi-
tors with which for both features in this model
are other wh-words. For the which.which con-
dition, for example, INTERFERENCE would only
consider the second which a competitor. IN-
TERVENORS, on the other hand, considers book,
</bodyText>
<page confidence="0.996994">
42
</page>
<bodyText confidence="0.999981896551724">
which, and student as possible intervenors. This
suggests that the INTERFERENCE measure in re-
trieval would be more accurate if it considered
more competitors, a consideration for future work.
Hofmeister (2007) suggests that it is not a sin-
gle memory factor, but a number of factors, that
contribute to SUV gradience. Some features, such
as INTERFERENCE or DLT, may be more accurate
when they are considered in addition to other fea-
tures. It is also likely that probabilistic models that
include many features will be more robust than
single-feature models, particularly when tested on
similar phenomena, like islands. I leave these pos-
sibilities to future work.
Although the variable beam-width INTER-
VENORS feature patterns well with the Arnon et
al. results, it does not capture the reading time dif-
ference between the bare.bare and the bare.which
conditions; both are unavailable at k=2 and avail-
able at k=3. Although this may indicate a prob-
lem with the feature itself, it is also possible that a
more gradient evaluation technique is needed. As
suggested in Section 4, determining accuracy on
the basis of attachment alone may be insufficient
to correctly model the full experimental evidence
in terms of reading times. This is an empirical
question that can be tested with this computational
model. In future work, I consider the role of parser
difficulty, via linking hypotheses such as surprisal,
in modeling the experimental data.
The interpretation of Relativized Minimality
used here as a grammatical constraint could not
derive the experimental results. LeftArc is not
preferred when the parser is in a SUV context
(RELMIN=Yes)–an expected result as attachments
should not occur in SUV contexts. However, the
which.which, which.bare, and the bare.which con-
ditions are not violations because they include
non-referential NPs. Even with the RELMIN=NO
feature, the parser does not select LeftArc at-
tachments, suggesting grammatical gradience is
not useful in modeling the SUV gradience results.
This model does not attempt to capture exper-
imental evidence that SUVs and similar phenom-
ena, like islands, are better modeled by grammati-
cal constraints (Phillips, 2006; Sprouse et al., Sub-
mitted). Not only does this work only focus on one
kind of grammatical constraint for SUV gradience,
but the results reported here do not reveal whether
the intervention effect itself is better modeled by
grammatical or reductionist factors. Rather, the
results demonstrate that the gradience in the inter-
vention effect is better modeled by memory than
by the gradient grammatical feature. Future work
with this computational model will allow for an
examination of those memory factors and gram-
matical factors most useful in exploring the source
of the intervention effect itself.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999949">
This study considers grammatical and memory-
based explanations for SUV gradience in a hu-
man sentence processing model. The results sug-
gest that gradience is best modeled by a parser
that can vary memory resources while being sen-
sitive to the types of nominal intervenors that have
been parsed. Grammatical and other memory con-
straints do not determine correct attachments in
the SUV environment. The results argue for a the-
ory of language that accounts for SUV gradience
in terms of specific memory factors.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999864">
I am grateful to Sam Epstein, John T. Hale, Philip
Hofmeister, Rick Lewis, Colin Phillips, students
of the University of Michigan Rational Behavior
and Minimalist Inquiry course, and two anony-
mous reviewers for helpful comments and sugges-
tions on this work.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998771619047619">
S. Abney. 1987. The English noun phrase in its sen-
tential aspect. Ph.D. thesis, MIT, Cambridge, MA.
J. R. Anderson. 2005. Human symbol manipulation
within an integrated cognitive architecture. Cogni-
tive Science, 29:313–341.
I. Arnon, N. Snider, P. Hofmeister, T. F. Jaeger, and
I. Sag. To appear. Cross-linguistic variation in
a processing account: The case of multiple wh-
questions. In Proceedings of Berkeley Linguistics
Society, volume 32.
M. F. Boston, J. T. Hale, R. Kliegl, and S. Vasishth.
2008. Surprising parser actions and reading diffi-
culty. In Proceedings ofACL-08: HLT Short Papers,
pages 5–8.
N. Chomsky. 1973. Conditions on transformations.
In Stephen Anderson and Paul Kiparsky, editors, A
Festschrift for Morris Halle, pages 232–286. Holt,
Reinhart and Winston, New York.
W. N. Francis and H. Kucera. 1979. Brown corpus
manual. Technical report, Department of Linguis-
tics, Brown University, Providence, RI.
</reference>
<page confidence="0.995566">
43
</page>
<reference confidence="0.999859744680851">
E. Gibson. 1998. Linguistic complexity: locality of
syntactic dependencies. Cognition, 68:1–76.
E. Gibson. 2000. Dependency locality theory: A
distance-based theory of linguistic complexity. In
A. Marantz, Y. Miyashita, and W. O’Neil, editors,
Image, language, brain: Papers from the First Mind
Articulation Symposium. MIT Press, Cambridge,
MA.
P. C. Gordon, R. Hendrick, and W. H. Levine. 2002.
Memory-load interference in syntactic processing.
Psychological Science, 13(5):425–430.
D. J. Grodner and E. A. F. Gibson. 2005. Conse-
quences of the serial nature of linguistic input for
sentential complexity. Cognitive Science, 29:261–
91.
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993.
Cognitive status and the form of referring expres-
sions in discourse. Language, 69:274–307.
J. T. Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of NAACL
2001, pages 1–8.
J. A. Hawkins. 1999. Processing complexity and filler-
gap dependencies across grammars. Language,
75(2):244–285.
D. G. Hays. 1964. Dependency Theory: A formalism
and some observations. Language, 40:511–525.
P. Hofmeister, I. Arnon, T. F. Jaeger, I. A. Sag, and
N. Snider. Submitted. The source ambiguity prob-
lem: distinguishing the effects of grammar and pro-
cessing on acceptability judgments. Language and
Cognitive Processes.
P. Hofmeister. 2007. Retrievability and gradience in
filler-gap dependencies. In Proceedings of the 43rd
Regional Meeting of the Chicago Linguistics Soci-
ety, Chicago. University of Chicago Press.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Proceedings of NODALIDA 2007, Tartu, Estonia.
M. A. Just and P.A. Carpenter. 1992. A capacity theory
of comprehension: Individual differences in work-
ing memory. Psychological Review, 98:122–149.
M. A. Just and S. Varma. 2007. The organization
of thinking: What functional brain imaging reveals
about the neuroarchitecture of complex cognition.
Cognitive, Affective, and Behavioral Neuroscience,
7(3):153–191.
L. Karttunen. 1977. Syntax and semantics of ques-
tions. Linguistics and Philosophy, 1:3–44.
R. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory re-
trieval. Cognitive Science, 29:1–45.
C.-J. Lin, R. C. Weng, and S. S. Keerthi. 2008. Trust
region newton method for large-scale regularized lo-
gistic regression. Journal of Machine Learning Re-
search, 9.
B. McElree. 2000. Sentence comprehension is me-
diated by content-addressable memory structures.
Journal of Psycholinguistic Research, 29(2):111–
123.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proceedings of the Workshop on
Incremental Parsing (ACL), pages 50–57.
D. Pesetsky. 1987. Wh-in-situ: movement and unse-
lective binding. In Eric Reuland and A. ter Meulen,
editors, The representation of (In)Definiteness,
pages 98–129. MIT Press, Cambridge, MA.
C. Phillips. 2006. The real-time status of island phe-
nomena. Language, 82:795–823.
C. Phillips. Submitted. Some arguments and non-
arguments for reductionist accounts of syntactic
phenomena. Language and Cognitive Processes.
O. Rambow and A. K. Joshi. 1994. A processing
model for free word-order languages. In Charles
Clifton, Jr., Lyn Frazier, and Keith Rayner, editors,
Perspectives on sentence processing, pages 267–
301. Erlbaum, Hillsdale, NJ.
L. Rizzi. 1990. Relativized Minimality. MIT Press.
J. Sprouse, M. Wagers, and C. Phillips. Sub-
mitted. A test of the relation between work-
ing memory capacity and syntactic island effects.
http://ling.auf.net/lingBuzz/001042.
L. Tesni`ere. 1959. ´El´ements de syntaxe structurale.
Editions Klincksiek.
J. A. Van Dyke and B. McElree. 2006. Retrieval in-
terference in sentence comprehension. Journal of
Memory and Language, 55:157–166.
E. Wanner and M. Maratsos. 1978. An ATN approach
in comprehension. In Morris Halle, Joan Bresnan,
and George Miller, editors, Linguistic theory and
psychological reality, pages 119–161. MIT Press,
Cambridge, MA.
T. Warren and Edward Gibson. 2002. The influence of
referential processing on sentence complexity. Cog-
nition, 85:79–112.
</reference>
<page confidence="0.999293">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.566360">
<title confidence="0.979458">The role of memory in superiority violation gradience</title>
<author confidence="0.999436">Marisa Ferrara</author>
<affiliation confidence="0.866308">Cornell Ithaca, NY,</affiliation>
<email confidence="0.99713">mfb74@cornell.edu</email>
<abstract confidence="0.985224875">This paper examines how grammatical and memory constraints explain gradience in superiority violation acceptability. A computational model encoding both categories of constraints is compared to experimental evidence. By formalizing memory capacity as beam-search in the parser, the model predicts gradience evident in human data. To predict attachment behavior, the parser must be sensitive to the types of nominal that occur between a and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>The English noun phrase in its sentential aspect.</title>
<date>1987</date>
<booktitle>Ph.D. thesis, MIT,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="12784" citStr="Abney, 1987" startWordPosition="1955" endWordPosition="1956">, 1979). The corpus is converted to dependencies via the Pennconverter tool (Johansson and Nugues, 2007). The parser is then simulated on these dependencies, providing a corpus of parser states and subsequent actions that form the basis of the training data. Because the parser is POS-based, this corpus is manipulated in two ways to sensitize it to the differences in the experimental conditions. First, the corpus is given finer-grained POS tags for each of the wh-words, described in Table 3. Secondly, which-N dependencies are encoded as DPs (determiner phrases) and are headed by the wh-phrase (Abney, 1987). This ensures the parser differentiates a wh-word retrieval from a simple noun retrieval, which is necessary for several of the memory-based constraints. Other noun phrases are headed by their nouns. The corpus is Original POS Wh Example WP WP-WHAT what WP WP-WHO who WDT WDT-WHICH which book WDT WDT-WHAT what book IN IN-WHETHER whether WRB WRB how/why/when Table 3: POS for wh fillers and intervenors. Figure 3: The relevant attachment is between which and read. not switched to a fully DP analysis to preserve as many of the original relationships as possible. I extend the Nivre algorithm to all</context>
</contexts>
<marker>Abney, 1987</marker>
<rawString>S. Abney. 1987. The English noun phrase in its sentential aspect. Ph.D. thesis, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>Human symbol manipulation within an integrated cognitive architecture.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<pages>29--313</pages>
<contexts>
<context position="7479" citStr="Anderson, 2005" startWordPosition="1133" endWordPosition="1134">ce can also be measured in terms of the parser’s internal resources. The computational model described here incorporates a stack memory. Although stacks are not accurate models of human memory (McElree, 2000), this architectural property may provide insight into how memory affects SUV gradience. 2.2.3 Activation and interference Sentence processing difficulty has been attributed to the amount of time it takes to retrieve a word from memory. Lewis &amp; Vasishth (2005) find support for this argument by applying equations from a general cognitive model, ACT-R (Adaptive Control of Thought-Rational) (Anderson, 2005), to a sentence processsing model. Their calculation of retrieval time, henceforth retrieval, is sensitive to a word’s activation and its similarity-based interference with other words in memory (Gordon et al., 2002; Van Dyke and McElree, 2006). Activation, Interference, and the conjunction of the two in the form of Retrieval, are considered in this work. The grammatical and memory-based accounts described above offer several explanations for SUV gradience. They can be represented along a continuum, where the type of information consid37 Hypothesis Sensitive to Distance String distance between</context>
<context position="18899" citStr="Anderson, 2005" startWordPosition="2940" endWordPosition="2942">e Includes Grammar RELMIN Yes/No σ1 wh−word :: intervenorswh−word(σ1 ...τ1) Memory DISTANCE String Position τ1 − σ1 DLT Count intervenorsnom(σ1 ...τ1) INTERVENORS POS intervenorsnom(σ1 ...τ1) STACK1NEXT POS σ1 :: τ1 STACK2NEXT POS σ1 :: σ2 :: τ1 STACK3NEXT POS σ1 :: σ2 :: σ3 :: τ1 ACTIVATION Value baselineActivation(σ1) INTERFERENCE Value interference(σ1) RETRIEVAL Time (ms.) retrievalTime(σ1) Table 5: Feature specification.:: indicates concatenation. recent retrievals at time tj. This implementation follows standard ACT-R practice in setting the decay rate d to 0.5 (Lewis and Vasishth, 2005; Anderson, 2005). � tj −d �(2) σ1’s activation can decrease if competitors, or other words with similar grammatical categories, have already been parsed. In Equation (1), Wj denotes weights associated with the retrieval cues j that are shared with these competitors, and Sji symbolizes the strengths of association between cues j and the retrieved item i (σ1). For this model, weights are set to 1 because there is only one retrieval cue j in operation: the POS. The strength of association Sji is computed as in Equation 3. Sji = Smax − ln(fanj) (3) The fan, fanj, is the number of words that have the same grammati</context>
</contexts>
<marker>Anderson, 2005</marker>
<rawString>J. R. Anderson. 2005. Human symbol manipulation within an integrated cognitive architecture. Cognitive Science, 29:313–341.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Arnon</author>
<author>N Snider</author>
<author>P Hofmeister</author>
<author>T F Jaeger</author>
<author>I Sag</author>
</authors>
<title>To appear. Cross-linguistic variation in a processing account: The case of multiple whquestions.</title>
<booktitle>In Proceedings of Berkeley Linguistics Society,</booktitle>
<volume>32</volume>
<marker>Arnon, Snider, Hofmeister, Jaeger, Sag, </marker>
<rawString>I. Arnon, N. Snider, P. Hofmeister, T. F. Jaeger, and I. Sag. To appear. Cross-linguistic variation in a processing account: The case of multiple whquestions. In Proceedings of Berkeley Linguistics Society, volume 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Boston</author>
<author>J T Hale</author>
<author>R Kliegl</author>
<author>S Vasishth</author>
</authors>
<title>Surprising parser actions and reading difficulty.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT Short Papers,</booktitle>
<pages>5--8</pages>
<contexts>
<context position="17270" citStr="Boston et al., 2008" startWordPosition="2675" endWordPosition="2678">feature, determines parser actions on the basis of how far apart σ1 and τ1 are in the string. DLT and INTERVENORS require parser sensitivity to the nominal intervenors between σ1 and τ1 according to Gibson’s DLT specification (2000). Table 6 provides a list of the nominal intervenors considered. Gibson’s hierarchy is extended The sequence of STACKNEXT features are sensitive to the parser’s memory, in the form of the POS of elements at varying depths of the stack. These features are found to have high overall accuracy in the Nivre parser (Nivre, 2004) and in human sentence processing modeling (Boston et al., 2008). ACTIVATION, INTERFERENCE, and RETRIEVAL predictions are based on the sequence of Lewis &amp; Vasisth (2005) calculations provided in Equations 1-4. These equations require some notion of duration, which is calculated as a function of parser actions and word retrieval times. Table 7 describes this calculation, motivated by the production rule time in Lewis &amp; Vasisth’s ACT-R model. Transition Time LEFT 50 ms + 50 ms + Retrieval Time RIGHT 50 ms + 50 ms + Retrieval Time SHIFT 50ms REDUCE 0ms Table 7: How time is determined in the parser. Because only words at the top of the stack can be retrieved, </context>
</contexts>
<marker>Boston, Hale, Kliegl, Vasishth, 2008</marker>
<rawString>M. F. Boston, J. T. Hale, R. Kliegl, and S. Vasishth. 2008. Surprising parser actions and reading difficulty. In Proceedings ofACL-08: HLT Short Papers, pages 5–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Conditions on transformations.</title>
<date>1973</date>
<pages>232--286</pages>
<editor>In Stephen Anderson and Paul Kiparsky, editors,</editor>
<location>Holt, Reinhart and Winston, New York.</location>
<contexts>
<context position="1040" citStr="Chomsky, 1973" startWordPosition="155" endWordPosition="157">predicts gradience evident in human data. To predict attachment behavior, the parser must be sensitive to the types of nominal intervenors that occur between a wh-filler and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints. 1 Introduction Sentences that include two wh-words, as in Example (1), are often considered difficult by English speakers. (1) *Diego asked what, who2 read? This superiority effect holds when a second whword, who in this example, acts as a barrier to attachment of the first wh-word and its verb (Chomsky, 1973). The difficulty is ameliorated when the whwords are switched to which-N, or which-Noun, form as in Examples (2) and (3) (Karttunen, 1977; Pesetsky, 1987). This is confirmed by experimental evidence (Arnon et al., To appear; Hofmeister, 2007). (2) ?Diego asked which book who read? (3) ?Diego asked what which girl read? Memory is often implicated as the source of this gradience, though it is unclear which aspects of memory best model experimental results. This computational model encodes grammatical and memory-based constraints proposed in the literature to account for the phenomenon. The resul</context>
</contexts>
<marker>Chomsky, 1973</marker>
<rawString>N. Chomsky. 1973. Conditions on transformations. In Stephen Anderson and Paul Kiparsky, editors, A Festschrift for Morris Halle, pages 232–286. Holt, Reinhart and Winston, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Brown corpus manual.</title>
<date>1979</date>
<tech>Technical report,</tech>
<institution>Department of Linguistics, Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="12179" citStr="Francis and Kucera, 1979" startWordPosition="1856" endWordPosition="1859">endency relations. σ A stack of already-parsed unreduced words. τ An ordered input list of words. h A function from dependent words to heads. d A function from dependent words to arc types. Table 2: Parser configuration. The parser transitions from state to state via four possible actions. Shift and Reduce manipulate σ. LeftArc and RightArc build dependencies between σ1 (the element at the top of the stack) and τ1 (the next input word); LeftArc makes σ1 the dependent, and RightArc makes σ1 the head. The parser determines actions by consulting a probability model derived from the Brown Corpus (Francis and Kucera, 1979). The corpus is converted to dependencies via the Pennconverter tool (Johansson and Nugues, 2007). The parser is then simulated on these dependencies, providing a corpus of parser states and subsequent actions that form the basis of the training data. Because the parser is POS-based, this corpus is manipulated in two ways to sensitize it to the differences in the experimental conditions. First, the corpus is given finer-grained POS tags for each of the wh-words, described in Table 3. Secondly, which-N dependencies are encoded as DPs (determiner phrases) and are headed by the wh-phrase (Abney, </context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. N. Francis and H. Kucera. 1979. Brown corpus manual. Technical report, Department of Linguistics, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>Linguistic complexity: locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<pages>68--1</pages>
<contexts>
<context position="5254" citStr="Gibson, 1998" startWordPosition="787" endWordPosition="788">(Just and Varma, 2007). This work considers activation, and manipulates memory resources by varying the number of analyses the parser considers at each parse step. Table 1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanner and Maratsos, 1978; Rambow and Joshi, 1994; Gibson, 1998). Experimental evidence supports this claim, but there exist a number of anomalous results that resist explanation in terms of distance alone (Gibson, 1998; Hawkins, 1999; Gibson, 2000). For example, it is not the case that processing difficulty increases solely as 2Recent work by Hofmeister and colleagues attributes the advantage to a decrease in memory interference rather than retrieval facilitation (Submitted), but the spirit of the work remains the same. a function of the number of words in a sentence. However, it is possible that SUV gradience could be affected by this simple metric. The </context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>E. Gibson. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition, 68:1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>Dependency locality theory: A distance-based theory of linguistic complexity.</title>
<date>2000</date>
<booktitle>Papers from the First Mind Articulation Symposium.</booktitle>
<editor>In A. Marantz, Y. Miyashita, and W. O’Neil, editors, Image, language, brain:</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5439" citStr="Gibson, 2000" startWordPosition="816" endWordPosition="817">factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanner and Maratsos, 1978; Rambow and Joshi, 1994; Gibson, 1998). Experimental evidence supports this claim, but there exist a number of anomalous results that resist explanation in terms of distance alone (Gibson, 1998; Hawkins, 1999; Gibson, 2000). For example, it is not the case that processing difficulty increases solely as 2Recent work by Hofmeister and colleagues attributes the advantage to a decrease in memory interference rather than retrieval facilitation (Submitted), but the spirit of the work remains the same. a function of the number of words in a sentence. However, it is possible that SUV gradience could be affected by this simple metric. The Dependency Locality Theory (DLT) (Gibson, 2000) is a more linguistically-informed measure of distance. The DLT argues that an accurate model of sentence processing difficulty is sensiti</context>
<context position="23662" citStr="Gibson, 2000" startWordPosition="3773" endWordPosition="3775">urces available in a computational model of human sentence processing. The positive results from the INTERVENORS feature confirms the discourse accessibility hierarchy encoded in the DLT (Gundel et al., 1993; Warren and Gibson, 2002), but only when wh-words are included as nominal intervenors. The results also suggest that it is the type, and not just the number of intervenors as suggested by the DLT, that is important. Further, the INTERVENORS feature does not pattern with the DLT hypothesis. DLT assumes that increasing the number of nominal intervenors causes sentence processing difficulty (Gibson, 2000; Warren and Gibson, 2002). Here, the number of intervenors is increased, but sentence processing is relatively easier. This effect is explained by the intrinsic difference between the DLT and INTERVENORS features: INTERVENORS provides more information to the parser, in the form of the POS of all intervenors. This indicates that certain intervenors help, rather than hinder, the retrieval process. The negative results demonstrate that other representations of memory do not model SUV gradience. If we consider this along the continuum from (4), those features that take into account less informati</context>
</contexts>
<marker>Gibson, 2000</marker>
<rawString>E. Gibson. 2000. Dependency locality theory: A distance-based theory of linguistic complexity. In A. Marantz, Y. Miyashita, and W. O’Neil, editors, Image, language, brain: Papers from the First Mind Articulation Symposium. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Gordon</author>
<author>R Hendrick</author>
<author>W H Levine</author>
</authors>
<title>Memory-load interference in syntactic processing.</title>
<date>2002</date>
<journal>Psychological Science,</journal>
<volume>13</volume>
<issue>5</issue>
<contexts>
<context position="7694" citStr="Gordon et al., 2002" startWordPosition="1163" endWordPosition="1166">his architectural property may provide insight into how memory affects SUV gradience. 2.2.3 Activation and interference Sentence processing difficulty has been attributed to the amount of time it takes to retrieve a word from memory. Lewis &amp; Vasishth (2005) find support for this argument by applying equations from a general cognitive model, ACT-R (Adaptive Control of Thought-Rational) (Anderson, 2005), to a sentence processsing model. Their calculation of retrieval time, henceforth retrieval, is sensitive to a word’s activation and its similarity-based interference with other words in memory (Gordon et al., 2002; Van Dyke and McElree, 2006). Activation, Interference, and the conjunction of the two in the form of Retrieval, are considered in this work. The grammatical and memory-based accounts described above offer several explanations for SUV gradience. They can be represented along a continuum, where the type of information consid37 Hypothesis Sensitive to Distance String distance between words. DLT Number of nominal intervenors. Intervenors POS of nominal intervenors. Stack Memory Elements currently in parser memory. Baseline Activation Amount structure is activated in memory. Interference Amount o</context>
</contexts>
<marker>Gordon, Hendrick, Levine, 2002</marker>
<rawString>P. C. Gordon, R. Hendrick, and W. H. Levine. 2002. Memory-load interference in syntactic processing. Psychological Science, 13(5):425–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Grodner</author>
<author>E A F Gibson</author>
</authors>
<title>Consequences of the serial nature of linguistic input for sentential complexity.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<pages>91</pages>
<contexts>
<context position="6500" citStr="Grodner and Gibson, 2005" startWordPosition="984" endWordPosition="987">heory (DLT) (Gibson, 2000) is a more linguistically-informed measure of distance. The DLT argues that an accurate model of sentence processing difficulty is sensitive to the number and discourse-status (given or new) of nominal intervenors that occur across a particular distance. The DLT’s sensitivity to discoursenewness integrates aspects of D-linking: which book, for example, requires that books already be a part of the discourse, though what does not (Gundel et al., 1993; Warren and Gibson, 2002). The DLT has been demonstrated to model difficulty in ways that simple distance alone can not (Grodner and Gibson, 2005). This study also considers a stronger version of the DLT, Intervenors. Intervenors considers both the number and part-of-speech (POS) of nominal intervenors between a wh-word and its head. This feature is sensitive to nuanced differences between nominal intervenors, providing a more accurate model of the Memory Facilitation Hypothesis. 2.2.2 Stack memory Distance can also be measured in terms of the parser’s internal resources. The computational model described here incorporates a stack memory. Although stacks are not accurate models of human memory (McElree, 2000), this architectural propert</context>
</contexts>
<marker>Grodner, Gibson, 2005</marker>
<rawString>D. J. Grodner and E. A. F. Gibson. 2005. Consequences of the serial nature of linguistic input for sentential complexity. Cognitive Science, 29:261– 91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Gundel</author>
<author>N Hedberg</author>
<author>R Zacharski</author>
</authors>
<title>Cognitive status and the form of referring expressions in discourse.</title>
<date>1993</date>
<journal>Language,</journal>
<pages>69--274</pages>
<contexts>
<context position="6353" citStr="Gundel et al., 1993" startWordPosition="959" endWordPosition="963"> number of words in a sentence. However, it is possible that SUV gradience could be affected by this simple metric. The Dependency Locality Theory (DLT) (Gibson, 2000) is a more linguistically-informed measure of distance. The DLT argues that an accurate model of sentence processing difficulty is sensitive to the number and discourse-status (given or new) of nominal intervenors that occur across a particular distance. The DLT’s sensitivity to discoursenewness integrates aspects of D-linking: which book, for example, requires that books already be a part of the discourse, though what does not (Gundel et al., 1993; Warren and Gibson, 2002). The DLT has been demonstrated to model difficulty in ways that simple distance alone can not (Grodner and Gibson, 2005). This study also considers a stronger version of the DLT, Intervenors. Intervenors considers both the number and part-of-speech (POS) of nominal intervenors between a wh-word and its head. This feature is sensitive to nuanced differences between nominal intervenors, providing a more accurate model of the Memory Facilitation Hypothesis. 2.2.2 Stack memory Distance can also be measured in terms of the parser’s internal resources. The computational mo</context>
<context position="23257" citStr="Gundel et al., 1993" startWordPosition="3706" endWordPosition="3709"> 5 Discussion The results demonstrate that modeling the experimental data for SUV gradience requires a parser that can vary memory resources as well as be sensitive to the types of the nominal intervenors currently in memory. The gradience is modeled by increasing memory resources, in the form of increases in the beam-width. This demonstrates the usefulness of varying both the types and amounts of memory resources available in a computational model of human sentence processing. The positive results from the INTERVENORS feature confirms the discourse accessibility hierarchy encoded in the DLT (Gundel et al., 1993; Warren and Gibson, 2002), but only when wh-words are included as nominal intervenors. The results also suggest that it is the type, and not just the number of intervenors as suggested by the DLT, that is important. Further, the INTERVENORS feature does not pattern with the DLT hypothesis. DLT assumes that increasing the number of nominal intervenors causes sentence processing difficulty (Gibson, 2000; Warren and Gibson, 2002). Here, the number of intervenors is increased, but sentence processing is relatively easier. This effect is explained by the intrinsic difference between the DLT and IN</context>
</contexts>
<marker>Gundel, Hedberg, Zacharski, 1993</marker>
<rawString>J. K. Gundel, N. Hedberg, and R. Zacharski. 1993. Cognitive status and the form of referring expressions in discourse. Language, 69:274–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>1--8</pages>
<contexts>
<context position="22554" citStr="Hale, 2001" startWordPosition="3595" endWordPosition="3596">ich and read. RETRIEVAL, INTERFERENCE, and particularly DLT, should also be sensitive to these intervenors. Despite their similarity, none of these features are able to model the attachment behavior in the experimental data. The STACK3NEXT feature differs from the other features in that it allows the LeftArc attachment to occur in any of the conditions. Although this does not match the interpretation of the experimental results followed in this paper, it leaves open the possibility that the feature could model the data according to a different measure of parsing difficulty, such as surprisal (Hale, 2001). The RELMIN constraint is not able to model the experimental results for gradience. 5 Discussion The results demonstrate that modeling the experimental data for SUV gradience requires a parser that can vary memory resources as well as be sensitive to the types of the nominal intervenors currently in memory. The gradience is modeled by increasing memory resources, in the form of increases in the beam-width. This demonstrates the usefulness of varying both the types and amounts of memory resources available in a computational model of human sentence processing. The positive results from the INT</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>J. T. Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceedings of NAACL 2001, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hawkins</author>
</authors>
<title>Processing complexity and fillergap dependencies across grammars.</title>
<date>1999</date>
<journal>Language,</journal>
<volume>75</volume>
<issue>2</issue>
<contexts>
<context position="5424" citStr="Hawkins, 1999" startWordPosition="814" endWordPosition="815">1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanner and Maratsos, 1978; Rambow and Joshi, 1994; Gibson, 1998). Experimental evidence supports this claim, but there exist a number of anomalous results that resist explanation in terms of distance alone (Gibson, 1998; Hawkins, 1999; Gibson, 2000). For example, it is not the case that processing difficulty increases solely as 2Recent work by Hofmeister and colleagues attributes the advantage to a decrease in memory interference rather than retrieval facilitation (Submitted), but the spirit of the work remains the same. a function of the number of words in a sentence. However, it is possible that SUV gradience could be affected by this simple metric. The Dependency Locality Theory (DLT) (Gibson, 2000) is a more linguistically-informed measure of distance. The DLT argues that an accurate model of sentence processing diffic</context>
</contexts>
<marker>Hawkins, 1999</marker>
<rawString>J. A. Hawkins. 1999. Processing complexity and fillergap dependencies across grammars. Language, 75(2):244–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Dependency Theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--511</pages>
<contexts>
<context position="11051" citStr="Hays, 1964" startWordPosition="1670" endWordPosition="1671">rate the same structures, techniques, and experimental conditions. Although Arnon et al. considered 20 lexical variations, the unlexicalized parser can not distinguish these variations. Therefore, the parser is only evaluated on these four sentences; however, they are taken to represent classes of structures that generalize to all SUV gradience in English. 3.1 The parsing model The computational model is based on Nivre’s (2004) dependency parsing algorithm. The algorithm builds directed, word-to-word analyses of test input following the Dependency Grammar syntactic formalism (Tesni`ere, 1959; Hays, 1964). Figure 2 depicts the full dependency analysis of the which.which condition from Example 38 Figure 2: A dependency analysis of the which.which condition. (8), where heads point to their dependents via arcs. The Nivre parser assembles dependency structure incrementally by passing through parser states that aggregate four data structures, shown in Table 2. The stack σ holds parsed words that require further analysis, and the list τ holds words yet to be parsed. h and d encode the current list of dependency relations. σ A stack of already-parsed unreduced words. τ An ordered input list of words.</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>D. G. Hays. 1964. Dependency Theory: A formalism and some observations. Language, 40:511–525.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Submitted</author>
</authors>
<title>The source ambiguity problem: distinguishing the effects of grammar and processing on acceptability judgments. Language and Cognitive Processes.</title>
<marker>Submitted, </marker>
<rawString>P. Hofmeister, I. Arnon, T. F. Jaeger, I. A. Sag, and N. Snider. Submitted. The source ambiguity problem: distinguishing the effects of grammar and processing on acceptability judgments. Language and Cognitive Processes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hofmeister</author>
</authors>
<title>Retrievability and gradience in filler-gap dependencies.</title>
<date>2007</date>
<booktitle>In Proceedings of the 43rd Regional Meeting of the Chicago Linguistics Society,</booktitle>
<publisher>University of Chicago Press.</publisher>
<location>Chicago.</location>
<contexts>
<context position="1282" citStr="Hofmeister, 2007" startWordPosition="195" endWordPosition="196">ling violation gradience patterns than grammatical constraints. 1 Introduction Sentences that include two wh-words, as in Example (1), are often considered difficult by English speakers. (1) *Diego asked what, who2 read? This superiority effect holds when a second whword, who in this example, acts as a barrier to attachment of the first wh-word and its verb (Chomsky, 1973). The difficulty is ameliorated when the whwords are switched to which-N, or which-Noun, form as in Examples (2) and (3) (Karttunen, 1977; Pesetsky, 1987). This is confirmed by experimental evidence (Arnon et al., To appear; Hofmeister, 2007). (2) ?Diego asked which book who read? (3) ?Diego asked what which girl read? Memory is often implicated as the source of this gradience, though it is unclear which aspects of memory best model experimental results. This computational model encodes grammatical and memory-based constraints proposed in the literature to account for the phenomenon. The results demonstrate that as memory resources are increased, the parser can model the human pattern if it is sensitive to the types of nominal intervenors. This supports memory-based accounts of superiority violation (SUV) gradience. 2 Explanations</context>
<context position="4415" citStr="Hofmeister, 2007" startWordPosition="657" endWordPosition="658">ammatical explanation. Rather, SUVs that appear ungrammatical, such as Example (1), are the result of severe processing difficulty alone. These accounts attribute processing difficulty to memory: severe memory resource limitations account for ungrammatical sentences in SUVs, and increased memory resources allow for more acceptable sentences. This is the central idea behind Hofmeister’s Memory Facilitation Hypothesis (2007): Memory Facilitation Hypothesis Linguistic elements that encode more information (lexical, semantic, syntactic, etc.) facilitate their own subsequent retrieval from memory (Hofmeister, 2007, p.4)2. This memory explanation is central to activationbased memory hypotheses previously proposed in the psycholinguistic literature, such as CCREADER (Just and Carpenter, 1992), ACT-R (Lewis and Vasishth, 2005), and 4CAPS (Just and Varma, 2007). This work considers activation, and manipulates memory resources by varying the number of analyses the parser considers at each parse step. Table 1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these varia</context>
<context position="25518" citStr="Hofmeister (2007)" startWordPosition="4052" endWordPosition="4053">ty-based interference and retrieval have previously been argued to account for these gradience patterns (Hofmeister et al., Submitted). However, the only words considered as competitors with which for both features in this model are other wh-words. For the which.which condition, for example, INTERFERENCE would only consider the second which a competitor. INTERVENORS, on the other hand, considers book, 42 which, and student as possible intervenors. This suggests that the INTERFERENCE measure in retrieval would be more accurate if it considered more competitors, a consideration for future work. Hofmeister (2007) suggests that it is not a single memory factor, but a number of factors, that contribute to SUV gradience. Some features, such as INTERFERENCE or DLT, may be more accurate when they are considered in addition to other features. It is also likely that probabilistic models that include many features will be more robust than single-feature models, particularly when tested on similar phenomena, like islands. I leave these possibilities to future work. Although the variable beam-width INTERVENORS feature patterns well with the Arnon et al. results, it does not capture the reading time difference b</context>
</contexts>
<marker>Hofmeister, 2007</marker>
<rawString>P. Hofmeister. 2007. Retrievability and gradience in filler-gap dependencies. In Proceedings of the 43rd Regional Meeting of the Chicago Linguistics Society, Chicago. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA 2007,</booktitle>
<location>Tartu, Estonia.</location>
<contexts>
<context position="12276" citStr="Johansson and Nugues, 2007" startWordPosition="1871" endWordPosition="1874">s. h A function from dependent words to heads. d A function from dependent words to arc types. Table 2: Parser configuration. The parser transitions from state to state via four possible actions. Shift and Reduce manipulate σ. LeftArc and RightArc build dependencies between σ1 (the element at the top of the stack) and τ1 (the next input word); LeftArc makes σ1 the dependent, and RightArc makes σ1 the head. The parser determines actions by consulting a probability model derived from the Brown Corpus (Francis and Kucera, 1979). The corpus is converted to dependencies via the Pennconverter tool (Johansson and Nugues, 2007). The parser is then simulated on these dependencies, providing a corpus of parser states and subsequent actions that form the basis of the training data. Because the parser is POS-based, this corpus is manipulated in two ways to sensitize it to the differences in the experimental conditions. First, the corpus is given finer-grained POS tags for each of the wh-words, described in Table 3. Secondly, which-N dependencies are encoded as DPs (determiner phrases) and are headed by the wh-phrase (Abney, 1987). This ensures the parser differentiates a wh-word retrieval from a simple noun retrieval, w</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA 2007, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Just</author>
<author>P A Carpenter</author>
</authors>
<title>A capacity theory of comprehension: Individual differences in working memory.</title>
<date>1992</date>
<journal>Psychological Review,</journal>
<pages>98--122</pages>
<contexts>
<context position="4595" citStr="Just and Carpenter, 1992" startWordPosition="680" endWordPosition="683">ng difficulty to memory: severe memory resource limitations account for ungrammatical sentences in SUVs, and increased memory resources allow for more acceptable sentences. This is the central idea behind Hofmeister’s Memory Facilitation Hypothesis (2007): Memory Facilitation Hypothesis Linguistic elements that encode more information (lexical, semantic, syntactic, etc.) facilitate their own subsequent retrieval from memory (Hofmeister, 2007, p.4)2. This memory explanation is central to activationbased memory hypotheses previously proposed in the psycholinguistic literature, such as CCREADER (Just and Carpenter, 1992), ACT-R (Lewis and Vasishth, 2005), and 4CAPS (Just and Varma, 2007). This work considers activation, and manipulates memory resources by varying the number of analyses the parser considers at each parse step. Table 1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanne</context>
</contexts>
<marker>Just, Carpenter, 1992</marker>
<rawString>M. A. Just and P.A. Carpenter. 1992. A capacity theory of comprehension: Individual differences in working memory. Psychological Review, 98:122–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Just</author>
<author>S Varma</author>
</authors>
<title>The organization of thinking: What functional brain imaging reveals about the neuroarchitecture of complex cognition.</title>
<date>2007</date>
<journal>Cognitive, Affective, and Behavioral Neuroscience,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="4663" citStr="Just and Varma, 2007" startWordPosition="691" endWordPosition="694">ungrammatical sentences in SUVs, and increased memory resources allow for more acceptable sentences. This is the central idea behind Hofmeister’s Memory Facilitation Hypothesis (2007): Memory Facilitation Hypothesis Linguistic elements that encode more information (lexical, semantic, syntactic, etc.) facilitate their own subsequent retrieval from memory (Hofmeister, 2007, p.4)2. This memory explanation is central to activationbased memory hypotheses previously proposed in the psycholinguistic literature, such as CCREADER (Just and Carpenter, 1992), ACT-R (Lewis and Vasishth, 2005), and 4CAPS (Just and Varma, 2007). This work considers activation, and manipulates memory resources by varying the number of analyses the parser considers at each parse step. Table 1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanner and Maratsos, 1978; Rambow and Joshi, 1994; Gibson, 1998). Experim</context>
</contexts>
<marker>Just, Varma, 2007</marker>
<rawString>M. A. Just and S. Varma. 2007. The organization of thinking: What functional brain imaging reveals about the neuroarchitecture of complex cognition. Cognitive, Affective, and Behavioral Neuroscience, 7(3):153–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>Syntax and semantics of questions.</title>
<date>1977</date>
<journal>Linguistics and Philosophy,</journal>
<pages>1--3</pages>
<contexts>
<context position="1177" citStr="Karttunen, 1977" startWordPosition="179" endWordPosition="180">rs that occur between a wh-filler and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints. 1 Introduction Sentences that include two wh-words, as in Example (1), are often considered difficult by English speakers. (1) *Diego asked what, who2 read? This superiority effect holds when a second whword, who in this example, acts as a barrier to attachment of the first wh-word and its verb (Chomsky, 1973). The difficulty is ameliorated when the whwords are switched to which-N, or which-Noun, form as in Examples (2) and (3) (Karttunen, 1977; Pesetsky, 1987). This is confirmed by experimental evidence (Arnon et al., To appear; Hofmeister, 2007). (2) ?Diego asked which book who read? (3) ?Diego asked what which girl read? Memory is often implicated as the source of this gradience, though it is unclear which aspects of memory best model experimental results. This computational model encodes grammatical and memory-based constraints proposed in the literature to account for the phenomenon. The results demonstrate that as memory resources are increased, the parser can model the human pattern if it is sensitive to the types of nominal </context>
</contexts>
<marker>Karttunen, 1977</marker>
<rawString>L. Karttunen. 1977. Syntax and semantics of questions. Linguistics and Philosophy, 1:3–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lewis</author>
<author>S Vasishth</author>
</authors>
<title>An activation-based model of sentence processing as skilled memory retrieval.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<pages>29--1</pages>
<contexts>
<context position="4629" citStr="Lewis and Vasishth, 2005" startWordPosition="685" endWordPosition="688">mory resource limitations account for ungrammatical sentences in SUVs, and increased memory resources allow for more acceptable sentences. This is the central idea behind Hofmeister’s Memory Facilitation Hypothesis (2007): Memory Facilitation Hypothesis Linguistic elements that encode more information (lexical, semantic, syntactic, etc.) facilitate their own subsequent retrieval from memory (Hofmeister, 2007, p.4)2. This memory explanation is central to activationbased memory hypotheses previously proposed in the psycholinguistic literature, such as CCREADER (Just and Carpenter, 1992), ACT-R (Lewis and Vasishth, 2005), and 4CAPS (Just and Varma, 2007). This work considers activation, and manipulates memory resources by varying the number of analyses the parser considers at each parse step. Table 1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanner and Maratsos, 1978; Rambow and J</context>
<context position="18882" citStr="Lewis and Vasishth, 2005" startWordPosition="2936" endWordPosition="2939">ore 40 Feature Feature Type Includes Grammar RELMIN Yes/No σ1 wh−word :: intervenorswh−word(σ1 ...τ1) Memory DISTANCE String Position τ1 − σ1 DLT Count intervenorsnom(σ1 ...τ1) INTERVENORS POS intervenorsnom(σ1 ...τ1) STACK1NEXT POS σ1 :: τ1 STACK2NEXT POS σ1 :: σ2 :: τ1 STACK3NEXT POS σ1 :: σ2 :: σ3 :: τ1 ACTIVATION Value baselineActivation(σ1) INTERFERENCE Value interference(σ1) RETRIEVAL Time (ms.) retrievalTime(σ1) Table 5: Feature specification.:: indicates concatenation. recent retrievals at time tj. This implementation follows standard ACT-R practice in setting the decay rate d to 0.5 (Lewis and Vasishth, 2005; Anderson, 2005). � tj −d �(2) σ1’s activation can decrease if competitors, or other words with similar grammatical categories, have already been parsed. In Equation (1), Wj denotes weights associated with the retrieval cues j that are shared with these competitors, and Sji symbolizes the strengths of association between cues j and the retrieved item i (σ1). For this model, weights are set to 1 because there is only one retrieval cue j in operation: the POS. The strength of association Sji is computed as in Equation 3. Sji = Smax − ln(fanj) (3) The fan, fanj, is the number of words that have </context>
<context position="7332" citStr="Lewis &amp; Vasishth (2005)" startWordPosition="1109" endWordPosition="1112">sitive to nuanced differences between nominal intervenors, providing a more accurate model of the Memory Facilitation Hypothesis. 2.2.2 Stack memory Distance can also be measured in terms of the parser’s internal resources. The computational model described here incorporates a stack memory. Although stacks are not accurate models of human memory (McElree, 2000), this architectural property may provide insight into how memory affects SUV gradience. 2.2.3 Activation and interference Sentence processing difficulty has been attributed to the amount of time it takes to retrieve a word from memory. Lewis &amp; Vasishth (2005) find support for this argument by applying equations from a general cognitive model, ACT-R (Adaptive Control of Thought-Rational) (Anderson, 2005), to a sentence processsing model. Their calculation of retrieval time, henceforth retrieval, is sensitive to a word’s activation and its similarity-based interference with other words in memory (Gordon et al., 2002; Van Dyke and McElree, 2006). Activation, Interference, and the conjunction of the two in the form of Retrieval, are considered in this work. The grammatical and memory-based accounts described above offer several explanations for SUV gr</context>
</contexts>
<marker>Lewis, Vasishth, 2005</marker>
<rawString>R. Lewis and S. Vasishth. 2005. An activation-based model of sentence processing as skilled memory retrieval. Cognitive Science, 29:1–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-J Lin</author>
<author>R C Weng</author>
<author>S S Keerthi</author>
</authors>
<title>Trust region newton method for large-scale regularized logistic regression.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="14792" citStr="Lin et al., 2008" startWordPosition="2283" endWordPosition="2286"> evaluation is categorical rather than statistical: SUV-processing is based on the decision to form an attachment in a superiority-violating context, given four experimental sentences. While future work will incorporate more experiments for robust statistical analysis, this work focuses on a small subset that generalizes to the greater phenomenon. 3.3 Encoding constraints The parser determines actions on the basis of probabilistic models, or features. In this work, I en39 code each of the grammatical and memory-based explanations as its own feature. I normalize the weights from the LIBLINEAR (Lin et al., 2008) SVM classification tool to determine probabilities for each parser action (LeftArc, RightArc, Shift, Reduce) . The features are sensitive to specific aspects of the current parser state, allowing an examination of whether the features suggest the superiority violating LeftArc action in the context depicted in Figure 3. The prediction is that attachment will be easiest in the which.which condition and impossible in the other conditions when memory resources are limited (k=1), as in Table 4. to include nominal wh-words to more accurately model the experimental conditions. Intervenor POS Example</context>
</contexts>
<marker>Lin, Weng, Keerthi, 2008</marker>
<rawString>C.-J. Lin, R. C. Weng, and S. S. Keerthi. 2008. Trust region newton method for large-scale regularized logistic regression. Journal of Machine Learning Research, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B McElree</author>
</authors>
<title>Sentence comprehension is mediated by content-addressable memory structures.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>29</volume>
<issue>2</issue>
<pages>123</pages>
<contexts>
<context position="7072" citStr="McElree, 2000" startWordPosition="1070" endWordPosition="1071">e alone can not (Grodner and Gibson, 2005). This study also considers a stronger version of the DLT, Intervenors. Intervenors considers both the number and part-of-speech (POS) of nominal intervenors between a wh-word and its head. This feature is sensitive to nuanced differences between nominal intervenors, providing a more accurate model of the Memory Facilitation Hypothesis. 2.2.2 Stack memory Distance can also be measured in terms of the parser’s internal resources. The computational model described here incorporates a stack memory. Although stacks are not accurate models of human memory (McElree, 2000), this architectural property may provide insight into how memory affects SUV gradience. 2.2.3 Activation and interference Sentence processing difficulty has been attributed to the amount of time it takes to retrieve a word from memory. Lewis &amp; Vasishth (2005) find support for this argument by applying equations from a general cognitive model, ACT-R (Adaptive Control of Thought-Rational) (Anderson, 2005), to a sentence processsing model. Their calculation of retrieval time, henceforth retrieval, is sensitive to a word’s activation and its similarity-based interference with other words in memor</context>
</contexts>
<marker>McElree, 2000</marker>
<rawString>B. McElree. 2000. Sentence comprehension is mediated by content-addressable memory structures. Journal of Psycholinguistic Research, 29(2):111– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing (ACL),</booktitle>
<pages>50--57</pages>
<contexts>
<context position="17206" citStr="Nivre, 2004" startWordPosition="2666" endWordPosition="2667">ble 1 as probabilistic features. DISTANCE, the simplest feature, determines parser actions on the basis of how far apart σ1 and τ1 are in the string. DLT and INTERVENORS require parser sensitivity to the nominal intervenors between σ1 and τ1 according to Gibson’s DLT specification (2000). Table 6 provides a list of the nominal intervenors considered. Gibson’s hierarchy is extended The sequence of STACKNEXT features are sensitive to the parser’s memory, in the form of the POS of elements at varying depths of the stack. These features are found to have high overall accuracy in the Nivre parser (Nivre, 2004) and in human sentence processing modeling (Boston et al., 2008). ACTIVATION, INTERFERENCE, and RETRIEVAL predictions are based on the sequence of Lewis &amp; Vasisth (2005) calculations provided in Equations 1-4. These equations require some notion of duration, which is calculated as a function of parser actions and word retrieval times. Table 7 describes this calculation, motivated by the production rule time in Lewis &amp; Vasisth’s ACT-R model. Transition Time LEFT 50 ms + 50 ms + Retrieval Time RIGHT 50 ms + 50 ms + Retrieval Time SHIFT 50ms REDUCE 0ms Table 7: How time is determined in the parse</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>J. Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing (ACL), pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pesetsky</author>
</authors>
<title>Wh-in-situ: movement and unselective binding.</title>
<date>1987</date>
<booktitle>The representation of (In)Definiteness,</booktitle>
<pages>98--129</pages>
<editor>In Eric Reuland and A. ter Meulen, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1194" citStr="Pesetsky, 1987" startWordPosition="181" endWordPosition="182">ween a wh-filler and its head. The results suggest memory is more informative for modeling violation gradience patterns than grammatical constraints. 1 Introduction Sentences that include two wh-words, as in Example (1), are often considered difficult by English speakers. (1) *Diego asked what, who2 read? This superiority effect holds when a second whword, who in this example, acts as a barrier to attachment of the first wh-word and its verb (Chomsky, 1973). The difficulty is ameliorated when the whwords are switched to which-N, or which-Noun, form as in Examples (2) and (3) (Karttunen, 1977; Pesetsky, 1987). This is confirmed by experimental evidence (Arnon et al., To appear; Hofmeister, 2007). (2) ?Diego asked which book who read? (3) ?Diego asked what which girl read? Memory is often implicated as the source of this gradience, though it is unclear which aspects of memory best model experimental results. This computational model encodes grammatical and memory-based constraints proposed in the literature to account for the phenomenon. The results demonstrate that as memory resources are increased, the parser can model the human pattern if it is sensitive to the types of nominal intervenors. This</context>
<context position="10345" citStr="Pesetsky, 1987" startWordPosition="1567" endWordPosition="1568">h the filler and intervenor are which-Ns (which.which). Examples (6) and (7) provide the other possible configurations. Arnon and colleagues find which.which to be the fastest condition. Figure 1 depicts these results. The other conditions are more difficult, Figure 1: Reading time is fastest in the which.which condition (Arnon et al., To appear, p.5). at varying levels: the which.bare condition is less difficult than the bare.which condition, and both are less difficult than the bare.bare condition. These results roughly pattern with acceptability judgments discussed in syntactic literature (Pesetsky, 1987). Corpora for superiority processing results do not exist. Further, few studies on SUVs incorporate the same structures, techniques, and experimental conditions. Although Arnon et al. considered 20 lexical variations, the unlexicalized parser can not distinguish these variations. Therefore, the parser is only evaluated on these four sentences; however, they are taken to represent classes of structures that generalize to all SUV gradience in English. 3.1 The parsing model The computational model is based on Nivre’s (2004) dependency parsing algorithm. The algorithm builds directed, word-to-word</context>
</contexts>
<marker>Pesetsky, 1987</marker>
<rawString>D. Pesetsky. 1987. Wh-in-situ: movement and unselective binding. In Eric Reuland and A. ter Meulen, editors, The representation of (In)Definiteness, pages 98–129. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Phillips</author>
</authors>
<title>The real-time status of island phenomena.</title>
<date>2006</date>
<tech>Language, 82:795–823.</tech>
<contexts>
<context position="27481" citStr="Phillips, 2006" startWordPosition="4362" endWordPosition="4363">sults. LeftArc is not preferred when the parser is in a SUV context (RELMIN=Yes)–an expected result as attachments should not occur in SUV contexts. However, the which.which, which.bare, and the bare.which conditions are not violations because they include non-referential NPs. Even with the RELMIN=NO feature, the parser does not select LeftArc attachments, suggesting grammatical gradience is not useful in modeling the SUV gradience results. This model does not attempt to capture experimental evidence that SUVs and similar phenomena, like islands, are better modeled by grammatical constraints (Phillips, 2006; Sprouse et al., Submitted). Not only does this work only focus on one kind of grammatical constraint for SUV gradience, but the results reported here do not reveal whether the intervention effect itself is better modeled by grammatical or reductionist factors. Rather, the results demonstrate that the gradience in the intervention effect is better modeled by memory than by the gradient grammatical feature. Future work with this computational model will allow for an examination of those memory factors and grammatical factors most useful in exploring the source of the intervention effect itself</context>
</contexts>
<marker>Phillips, 2006</marker>
<rawString>C. Phillips. 2006. The real-time status of island phenomena. Language, 82:795–823.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Submitted</author>
</authors>
<title>Some arguments and nonarguments for reductionist accounts of syntactic phenomena. Language and Cognitive Processes.</title>
<marker>Submitted, </marker>
<rawString>C. Phillips. Submitted. Some arguments and nonarguments for reductionist accounts of syntactic phenomena. Language and Cognitive Processes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>A K Joshi</author>
</authors>
<title>A processing model for free word-order languages.</title>
<date>1994</date>
<booktitle>Perspectives on sentence processing,</booktitle>
<pages>267--301</pages>
<editor>In Charles Clifton, Jr., Lyn Frazier, and Keith Rayner, editors,</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="5239" citStr="Rambow and Joshi, 1994" startWordPosition="783" endWordPosition="786">ishth, 2005), and 4CAPS (Just and Varma, 2007). This work considers activation, and manipulates memory resources by varying the number of analyses the parser considers at each parse step. Table 1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanner and Maratsos, 1978; Rambow and Joshi, 1994; Gibson, 1998). Experimental evidence supports this claim, but there exist a number of anomalous results that resist explanation in terms of distance alone (Gibson, 1998; Hawkins, 1999; Gibson, 2000). For example, it is not the case that processing difficulty increases solely as 2Recent work by Hofmeister and colleagues attributes the advantage to a decrease in memory interference rather than retrieval facilitation (Submitted), but the spirit of the work remains the same. a function of the number of words in a sentence. However, it is possible that SUV gradience could be affected by this simp</context>
</contexts>
<marker>Rambow, Joshi, 1994</marker>
<rawString>O. Rambow and A. K. Joshi. 1994. A processing model for free word-order languages. In Charles Clifton, Jr., Lyn Frazier, and Keith Rayner, editors, Perspectives on sentence processing, pages 267– 301. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rizzi</author>
</authors>
<title>Relativized Minimality.</title>
<date>1990</date>
<publisher>MIT</publisher>
<note>http://ling.auf.net/lingBuzz/001042.</note>
<contexts>
<context position="2358" citStr="Rizzi (1990)" startWordPosition="360" endWordPosition="361">sitive to the types of nominal intervenors. This supports memory-based accounts of superiority violation (SUV) gradience. 2 Explanations for SUV gradience This section details grammatical and reductionist explanations for SUV gradience, motivating the encoding of various constraints in the computational model. 2.1 Grammatical explanations Grammatical accounts of gradience rely on intrinsic discourse differences between phrases that allow for SUVs and those that do not. In this work, which-N phrases are examples of the former, and so-called bare wh-phrases (including who and what) the latter1. Rizzi (1990) incorporates ideas from Pesetsky’s D-Linking, or discourse-linking, hypothesis (1987) into a grammatical account of SUV gradience, Relativized Minimality. He argues that referential phrases like which-N refer to a pre-established set in the discourse and are not subject to the same constraints on attachment as non-referential phrases, like what. Which book delimits a set of possible discourse entities, books, and is more restrictive than what, which could instead delimit sets of books, cats, or abstract entities. The Relativized Minimality hypothesis accounts for SUV gradience on the basis of</context>
</contexts>
<marker>Rizzi, 1990</marker>
<rawString>L. Rizzi. 1990. Relativized Minimality. MIT Press. J. Sprouse, M. Wagers, and C. Phillips. Submitted. A test of the relation between working memory capacity and syntactic island effects. http://ling.auf.net/lingBuzz/001042.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale.</title>
<date>1959</date>
<journal>Editions Klincksiek.</journal>
<marker>Tesni`ere, 1959</marker>
<rawString>L. Tesni`ere. 1959. ´El´ements de syntaxe structurale. Editions Klincksiek.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Van Dyke</author>
<author>B McElree</author>
</authors>
<title>Retrieval interference in sentence comprehension.</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<pages>55--157</pages>
<marker>Van Dyke, McElree, 2006</marker>
<rawString>J. A. Van Dyke and B. McElree. 2006. Retrieval interference in sentence comprehension. Journal of Memory and Language, 55:157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wanner</author>
<author>M Maratsos</author>
</authors>
<title>An ATN approach in comprehension. In</title>
<date>1978</date>
<booktitle>Linguistic theory and psychological reality,</booktitle>
<pages>119--161</pages>
<editor>Morris Halle, Joan Bresnan, and George Miller, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5215" citStr="Wanner and Maratsos, 1978" startWordPosition="779" endWordPosition="782">1992), ACT-R (Lewis and Vasishth, 2005), and 4CAPS (Just and Varma, 2007). This work considers activation, and manipulates memory resources by varying the number of analyses the parser considers at each parse step. Table 1 lists memory factors that may contribute to SUV gradience. They are sensitive to the memory resources available during syntactic parsing, but account for memory differently. Below I describe these variations. 2.2.1 Distance and the DLT Distance, as measured by the number of words between, for example, a wh-word and its verb, has been argued to affect sentence comprehension (Wanner and Maratsos, 1978; Rambow and Joshi, 1994; Gibson, 1998). Experimental evidence supports this claim, but there exist a number of anomalous results that resist explanation in terms of distance alone (Gibson, 1998; Hawkins, 1999; Gibson, 2000). For example, it is not the case that processing difficulty increases solely as 2Recent work by Hofmeister and colleagues attributes the advantage to a decrease in memory interference rather than retrieval facilitation (Submitted), but the spirit of the work remains the same. a function of the number of words in a sentence. However, it is possible that SUV gradience could </context>
</contexts>
<marker>Wanner, Maratsos, 1978</marker>
<rawString>E. Wanner and M. Maratsos. 1978. An ATN approach in comprehension. In Morris Halle, Joan Bresnan, and George Miller, editors, Linguistic theory and psychological reality, pages 119–161. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Warren</author>
<author>Edward Gibson</author>
</authors>
<title>The influence of referential processing on sentence complexity.</title>
<date>2002</date>
<journal>Cognition,</journal>
<pages>85--79</pages>
<contexts>
<context position="6379" citStr="Warren and Gibson, 2002" startWordPosition="964" endWordPosition="967"> sentence. However, it is possible that SUV gradience could be affected by this simple metric. The Dependency Locality Theory (DLT) (Gibson, 2000) is a more linguistically-informed measure of distance. The DLT argues that an accurate model of sentence processing difficulty is sensitive to the number and discourse-status (given or new) of nominal intervenors that occur across a particular distance. The DLT’s sensitivity to discoursenewness integrates aspects of D-linking: which book, for example, requires that books already be a part of the discourse, though what does not (Gundel et al., 1993; Warren and Gibson, 2002). The DLT has been demonstrated to model difficulty in ways that simple distance alone can not (Grodner and Gibson, 2005). This study also considers a stronger version of the DLT, Intervenors. Intervenors considers both the number and part-of-speech (POS) of nominal intervenors between a wh-word and its head. This feature is sensitive to nuanced differences between nominal intervenors, providing a more accurate model of the Memory Facilitation Hypothesis. 2.2.2 Stack memory Distance can also be measured in terms of the parser’s internal resources. The computational model described here incorpo</context>
<context position="23283" citStr="Warren and Gibson, 2002" startWordPosition="3710" endWordPosition="3714">ults demonstrate that modeling the experimental data for SUV gradience requires a parser that can vary memory resources as well as be sensitive to the types of the nominal intervenors currently in memory. The gradience is modeled by increasing memory resources, in the form of increases in the beam-width. This demonstrates the usefulness of varying both the types and amounts of memory resources available in a computational model of human sentence processing. The positive results from the INTERVENORS feature confirms the discourse accessibility hierarchy encoded in the DLT (Gundel et al., 1993; Warren and Gibson, 2002), but only when wh-words are included as nominal intervenors. The results also suggest that it is the type, and not just the number of intervenors as suggested by the DLT, that is important. Further, the INTERVENORS feature does not pattern with the DLT hypothesis. DLT assumes that increasing the number of nominal intervenors causes sentence processing difficulty (Gibson, 2000; Warren and Gibson, 2002). Here, the number of intervenors is increased, but sentence processing is relatively easier. This effect is explained by the intrinsic difference between the DLT and INTERVENORS features: INTERV</context>
</contexts>
<marker>Warren, Gibson, 2002</marker>
<rawString>T. Warren and Edward Gibson. 2002. The influence of referential processing on sentence complexity. Cognition, 85:79–112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>