<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.37527">
PLAN REVISION IN PERSON-MACHINE
DIALOGUE
</title>
<author confidence="0.365923">
Cleo JULLIEN
Jean-Charles MARTY
</author>
<affiliation confidence="0.60566">
Grenoble Research Center of CAP SESA INNOVATION
</affiliation>
<address confidence="0.832919">
33, Chemin du Vieux Cherie
38240 Meylan France
</address>
<email confidence="0.999778">
e-mail: jullien@capsogetilr, marty@capsogeti.fr
</email>
<sectionHeader confidence="0.995657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948">
Dialogue is a cooperative process in which
each speech act of the participants con-
tributes to the overall purpose of the dia-
logue. Participating in a full dialogue im-
plies understanding at each point of the
dialogue session the role of each speech
act with respect to the rest of the dia-
logue. We concentrate in this paper on
speech acts that diverge from the straight-
forward unfolding of the dialogue. Such
speech acts represent dialogue deviations.
We analyze the representation of differ-
ent types and degrees of deviations and
present a plan revision mechanism for
dialogue management that permits their
treatment in the context of advice giving
dialogues between a novice user and an ex-
pert problem solver.
</bodyText>
<sectionHeader confidence="0.983534" genericHeader="method">
1 A Sample Dialogue
</sectionHeader>
<bodyText confidence="0.849934461538461">
In ESPRIT project ESTEAM-3161, we
are designing a dialoguer that manages
the interaction between a novice user and
1ESTEAM-316: &amp;quot;An Architecture for Interac-
tive Problem Solving by Cooperating Data and
Knowledge Bases&amp;quot; supported in part by the CEC.
advice—giving expert systems. The test-
bed application is personal financial in-
vestment [51. As an introduction to our
paper, we present a dialogue in which the
system obtains from the user the informa-
tion needed to activate the financial expert
system.
</bodyText>
<note confidence="0.63777575">
01,S: Hello. Tell me about your investment
problem.
02,U: I&apos;d like to buy a car in two years.
03,3: Ok. Normally an investment plan
</note>
<bodyText confidence="0.980956454545455">
comprises four subparts: an emergency
fund, for unforeseen expenses; a cash
need when you know you&apos;ll need a cer-
tain sum at a certain date, as is the case
with your car; a fixed-term investment
to earn the maximum in a specified time
period; and a long-term investment for
an indefinite amount of time which can
be designed either for capital growth or
for a regular income. How much do you
want to invest?
</bodyText>
<note confidence="0.719687363636364">
04,U: $20000.
05,3: How much -do you want to put into an
emergency fund?
WU: Let&apos;s talk about my car first.
07,5: Ok, how much do you think your car
will coat?
08,U: Oh, let&apos;s say $9000, I guess.
- 153 -
09,S: And you plan to buy it in two years?
10;U: Yes.
11,S: Oh, if you put $7,500 in a money-
</note>
<tableCaption confidence="0.436376">
market fund earning 9.9% p.a., in two
years you&apos;ll have $9,000. Let&apos;s return
to the emergency fund, now. How much
do you want to put in it?
12,U: $5000.
13,S: Tell me also the delay.
14,U: I don&apos;t understand what is the delay of
an emergency fund.
15,S: The delay is how soon you can have
your money back after asking for it. So,
for your emergency fund, what is this
delay?
</tableCaption>
<footnote confidence="0.439583666666667">
16,U: 2 days.
17,S: Do you want a fixed term investment?
18,U: No.
</footnote>
<construct confidence="0.950675315789474">
19,S: So, it leaves $7,500 for your long-term
investment. Is it oh?
20,U: I wanted $10000.
21,5: If you want an investment plan for
$20,000 with an emergency fund of
$5,000 and a cash-need of $7,500, then
it only leaves $7,500 for a long-term in-
vestment. If you want $10,000 for a
long-term investment, then you should
modify something else in your plan.
22,U: Could I put $2500 into the emergency
fund?
23,5: Ok. So, to sum up, you want to invest
$20,000, with $2,500 in an emergency
fund with a delay of 2 days, $7,500
in a cash need to buy a $9000 car in
2 years, no fixed-term investment, and
$10,000 in a long-term investment. Is
that right?
</construct>
<sectionHeader confidence="0.455279" genericHeader="method">
24,U: That&apos;s fine.
</sectionHeader>
<bodyText confidence="0.999963266666667">
The above sample dialogue has a quite
simple structure: presentation of the main
sub-investment types and collection of in-
formation from the user concerning each
sub-investment. We notice, however, that
at some points the user does not react di-
rectly as the system would expect. For
instance, in sentence 6, the user is asked
about saving money in an emergency fund,
but she wants to talk about buying her car
first. Later, in sentence 14, the user is un-
able to answer a question about the delay
of the emergency fund because she does
not understand what such a delay is.
Such reactions illustrate dialogue devia-
tions. Dialogue deviations are sentences
that diverge form the straightforward un-
folding of the dialogue, while having a co-
herent interpretation with respect to the
whole dialogue. These unexpected reac-
tions are inevitable in a dialogue where the
participants are independent agents with
their own goals and differing degrees of
knowledge about each other and the sub-
ject under discussion.
Before describing our dialogue manager
and its mechanism for handling such devi-
ations, we present in the next section the
framework we adopted to model flexible
dialogue management.
</bodyText>
<sectionHeader confidence="0.962769" genericHeader="method">
2 Deviations in Dialogue
</sectionHeader>
<bodyText confidence="0.997239614457832">
Dialogue is considered to be a coopera-
tive activity where the goals and actions
of each participant contribute to the over-
all purpose of the dialogue [3]. In task-
oriented dialogues, we distinguish between
task level goals and plans (e.g., investing,
traveling), and communicative level inten-
tions and speech acts (e.g., explaining, re-
questing information) [2,6,11]. We call
- 154 -
these aspects the intentional dimension.
Each step in the dialogue concerns a par-
ticular topic. Intuitively the notion of
topic might be described by a subset of ob-
jects of the problem under discussion. In
fact, the boundary of this &amp;quot;subset&amp;quot; is not
strict: one can only say that some objects
are more salient than others. The atten-
tional state is hence better represented by
different levels in the focus of attention,
corresponding to embedded subsets of ob-
jects [9].
It is important to note that in the con-
text of dialogue, the term deviation is not
used with its strict boolean meaning: it is
a complex and a relative notion.
Deviation are complex because they in-
volve both intentional and attentional di-
mensions. Deviations in this coopera-
tive process arise from inconsistencies be-
tween the observed speech act and the di-
alogue considered as a coherent plan [4].
They can be classified according to the
type of interactions among of interactions
among intentions of the participants and
and changes on the focus of attention. The
sample session above illustrates several
types of such deviations: at the commu-
nicative level, the novice user requests for
explanations before giving a requested in-
formation; at the task level, the user gives
an inconsistent value or does not want a
given action in the task plan. Deviations
are generally combined with changes of
subjets.
Within each dimension there exist differ-
ent degrees of deviations: speech acts may
have indirect effects, changes in the fo-
cus of attention may be be more or less
abrupt, deviations depend on the expecta-
tions each participant has concerning the
reactions of the other.
Therefore, we adopted models of the dia-
logue structure where the relationship be-
tween the intentions and the evolution of
the focus of attention are made explicit
[10].
The detection and analysis of deviation af-
ter a user speech act relies on expectations
and predictions in both the intentional and
attentional dimensions. The system, hav-
ing produced a speech act and waiting for
a reaction of the user, expects in the first
place a reaction corresponding exactly to
the effect it intended to produce. If the
user reacts differently, the system will use
knowledge about possible types of devia-
tion and the state of dialogue to analyze
the nature of the deviation.
Once a deviation has been identified, the
system may need to modify more or less
deeply the planned course of the dialogue:
from local adaptation like embedding a
small clarification subdialogue (sentences
14-15), to more global revision like re-
ordering entire sub-topics (sentence 6).
Hence to interpret the influence of an un-
expected speech act at a certain point in
the dialogue the representation of the state
of dialogue should reflect the structure of
the whole dialogue: keeping track of past
exchanges and anticipating the remainder
of the dialogue.
</bodyText>
<sectionHeader confidence="0.9233255" genericHeader="method">
3 Overview of the Dia-
logue Manager
</sectionHeader>
<bodyText confidence="0.9991645">
The general organization of the Esteam-
316 Dialogue System [1] is depicted in fig-
ure 1.
A Natural Language Front-End (NLF)
transforms natural language utterances
into literal meaning and vice-versa. The
literal meaning corresponds to an iso-
lated surface speech act. The Recognizer
takes a literal meaning from the Front-End
and determines whether the corresponding
</bodyText>
<figure confidence="0.89901875">
- 155 -
Planner
I Front End
UbR
</figure>
<figureCaption confidence="0.9664555">
Figure 1: Overview of the Dialogue Man-
ager
</figureCaption>
<bodyText confidence="0.999935470588235">
surface act of the user could be an exam-
ple of, or a part of, one of the commu-
nicative actions that the system expects
from the user in the context of the cur-
rent dialogue. The expectations are con-
trolled by the Planner which conducts the
dialogue and maintains a structure of the
system&apos;s intentions (SSI), while reacting to
user&apos;s intentions detected by the Recog-
nizer. The Planner interacts with the Ex-
pression Specialists for constructing from
the communicative acts it intends to per-
form (what to communicate) an appropri-
ate literal meaning (how to say it).
An advice giving dialogue is a particular
case of task-oriented dialogue. The task-
level plan reflects the problem of the user.
The advice giving system has only commu-
nicative intentions for constructing and re-
fining the task-level plan. A complete ad-
vice giving session contains three phases:
problem formulation, resolution and pre-
sentation of the solution. In this paper, we
concentrate on first phase. During prob-
lem formulation, the system helps the user
to refine, select and instantiate appropri-
ate subplans according to the user&apos;s goals.
The task plan is initialized by a stereo-
typical pattern of actions which could be
recommanded as part of a &amp;quot;good&amp;quot; solu-
tion. The result of the problem formu-
lation phase is a coherent task-level plan
which can be passed to an expert problem
solver.
</bodyText>
<sectionHeader confidence="0.654679" genericHeader="method">
4 Representation of the
</sectionHeader>
<subsectionHeader confidence="0.584984">
State of Dialogue
</subsectionHeader>
<bodyText confidence="0.9994786">
The communicative intentions of the sys-
tem are stored in the Structure of System
Intentions: the SSI reflects the state of the
plan of dialogue from the system&apos;s point of
view.
</bodyText>
<subsectionHeader confidence="0.981044">
4.1 Communicative Level Plans
</subsectionHeader>
<bodyText confidence="0.986413615384616">
The Planner uses a hierarchical set of
plans for constructing the SSI. Plans
are associated with the various commu-
nicative level intentions of the system.
We have designed two types of plans:
dialogue-plans and communicative-plans.
Dialogue-Plans are the most abstract
plans of the Planner. They express the
strategy of the overall advice-giving ses-
sion [7]. The purpose of these plans is
to capture procedural knowledge for an
&amp;quot;ideal&amp;quot; advice-giving session. They are
used to initiate the SSI, but also include
means for adaptation at execution time
[8].
Basically the models of dialogue-plans ex-
press dominance and sequencing relations
among the sub-parts of the dialogue ses-
sion (decomposition), and the part of the
task level plan that is in focus at a given
step of the communicative level plan (pa-
rameter).
Communicative-Plans models
contain the effects an elementary commu-
nicative intention of the system in terms
of the immediate expectations about the
</bodyText>
<figure confidence="0.8628946">
- 156 -
help user fo ulate problem
collect on EF
ask parame er amount
SS/
</figure>
<figureCaption confidence="0.989533">
Figure 2: Structure of System&apos;s Intentions
and Attentional State
</figureCaption>
<table confidence="0.906726">
give parm (delay, Ell
request expl (delay, EF
give parm (amount EFi
request expl (amount E)
request ex )
refuse U.E;
Acts concerning Cash Need
and other subplans
give parm plan FI)
request expi plan
refuse plan (P1
Others
Plan
Em. fund
Amount
Focus
</table>
<tableCaption confidence="0.68765175">
communicative acts of the user. For the
Planner a communicative plan is seen as
a primitive action to be be passed to the
Expression Specialists for execution.
</tableCaption>
<subsectionHeader confidence="0.959828">
4.2 Expectation Stack and
Structure of System Inten-
tions
</subsectionHeader>
<bodyText confidence="0.999424769230769">
The SSI is a tree enhanced by orderings
relations, in which each node represents a
communicative level plan of the system,
and links decomposition relations of plans
into subplans. In addition each node in
the SSI is related to a given subpart of the
task-plan. At a given point in the dialogue
the attentional state can be represented
by a stack in which the bottom contains
the focus associated with the most gen-
eral plan and the top contains the focus
of the plan currently executed. For exam-
ple, when the system asks for the amount
of the emergency fund, there will be three
focus levels in the stack: the investment
plan at the bottom, the emergency fund
in the middle and the amount on top (see
figure 2).
Using this attentional state the Recognizer
could only analyze changes of focus. The
problem is still to provide the Recognizer
with expectations concerning the inten-
tions of the user. The method of rep-
resenting expectations is to attach a set
user&apos;s communicative acts to each type of
system&apos;s intentions in the SSI and to or-
</bodyText>
<figureCaption confidence="0.9579525">
Figure 3: A sample State of the Expecta-
tion Stack
</figureCaption>
<bodyText confidence="0.94929796">
ganize these expectations according to the
attentional state. We obtain the Expecta-
tion Stack.
Let us consider an example to understand
better the significance of the different ob-
jects included in the Expectation Stack.
Figure 3 represents the state of the Ex-
pectation Stack when the system asks for
the delay of the emergency fund (sentence
13 or 15).
The most expected answer is the delay of
the emergency fund, but the system also
foresees the possibility of a request for ex-
planation from the user. At the next level,
the system expects the user to speak about
another parameter of the same plan (the
amount). At the level still further be-
low, he/she can speak about the emer-
gency fund in general, he/she can for ex-
ample refuse the emergency fund, or ask
for explanation on it. And so on, until the
system reaches the most unexpected reac-
tions of the user, i.e., even things that are
not related to the investment problem. (in
the level &amp;quot;others&amp;quot;)
</bodyText>
<sectionHeader confidence="0.9776535" genericHeader="method">
5 Dialogue Management:
Execution and Revision
</sectionHeader>
<bodyText confidence="0.99534977631579">
In the previous sections, we have presented
the different structures needed to reflect
the state of dialogue. The aim of this part
- 157 -
is to show how these structures are used
for handling revisions during the execution
of the dialogue.
The SSI is generated by a depth-first ex-
pansion of the abstract communicative
goals of the system (use of dialogue-plan
models). This process stops as soon
as the Planner reaches an atomic action
(communicative-plan)that produces a re-
quest toward the user. At this point,
the Expectation Stack is derived from the
state of the SSI and reflects the precise
topic of the question on top.
The input of the user is analyzed by the
NLF and the Recognizer [1] is called to
derive the user&apos;s intentions encoded in
the answer. The Recognizer returns the
speech act in the Expectation Stack it was
able to match. In most cases, the returned
speech act corresponds to the direct ex-
pected answer and the dialogue continues
without revision.
A deviation occurs, however, when the
Recognizer does not return the level cor-
responding to the most expected answer,
or when the user tries to put inconsis-
tent value in the task plan (an example of
such an inconsistent value is given in sen-
tence 20). In this case, the Planner must
use consistency constraints attached to the
task plan. Thus, the pointer returned by
the Recognizer in the Expectation Stack
indicates all the changes of subject or of
intention by the user while the constraints
attached to parameters of the task plan
reveal inconsistent values.
There is a revision associated with each
type of deviation. A revision is a struc-
tural transformation pattern for correct-
ing the SSI after the user&apos;s answer in order
to continue a coherent dialogue.
We illustrate the result of a revision on
sentences 5 and 6 (see figure 4.
We can see that the transformation takes
into account a subsequent return to
the emergency fund and an explicit re-
introduction of this subject.
The same types of transformation are used
to treat the deviations caused by the re-
quests for explanation (sentence 14). In
the case of inconsistent values (sentence
20), a subplan is inserted for explaining
why the value is inconsistent and asking
the user to change something in order to
correct the violation (sentences 21).
It is interesting to note here that the strat-
egy of the dialoguer can easily be modified
by changing either the models of the plans
or the transformations associated with the
cases of deviation. We presented above a
strategy in the dialogue that &amp;quot;follows the
user&amp;quot; (i.e., the dialoguer is very cooper-
ative and changes the subject each time
the user wants to). For instance, when
the user decides to speak about his/her
car, the system is cooperative and allows
him/her to do so. It would also have been
possible to adopt another strategy and to
say &amp;quot;ok, we will talk about your car later,
but now we need to discuss the emergency
fund because people tend to forget it oth-
erwise&amp;quot;.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999947">
The adaptive planning method presented
above takes into account both analyses of
intentions and changes of topic. Recogni-
tion of intentions is organized around the
attentional state in order to delimit the
scope of the revision. The revision mech-
anism is currently implemented in an ex-
perimental prototype of the ESTEANI-316
Dialoguer (written in PROLOG and run-
ning on SUN Workstations).
This approach seems appropriate for the
type of dialogues where the &amp;quot;expert&amp;quot; ad-
vice giving system has a quite directive
</bodyText>
<table confidence="0.995540285714286">
- 158 -
give parm (amount, EF)
request expl (amount, EF)
give parm (delay, EF)
request expl (delay, EF)
request expl (EF)
refuse (EF)
Acts concerning Cash Need
and other subplans
give parm plan (P1)
request expl plan (P1)
refuse plan (P1)
Others
request expl (ON)
refuse (ON)
Acts concerning Emergency Fund
and other subplans
give parm plan (P1)
request expl plan (P1)
refuse plan (P1)
Others
</table>
<subsectionHeader confidence="0.450255">
Transformation of the Expectation Stack
</subsectionHeader>
<bodyText confidence="0.822250333333333">
help user formulate problem
help user formulate problem
collect in on ON collect o on EF
return to EF
ask parameter amount ask parameter amount
Transformation of the SSI
</bodyText>
<figureCaption confidence="0.997378">
Figure 4: A Example of revision: Changing Subject
</figureCaption>
<bodyText confidence="0.974722">
- 159 -
control of the dialogue. Revisions give [8]
some degrees of flexibility to the &amp;quot;novice&amp;quot;
user who is unfamiliar with the domain
and the progression in the consultation.
Future work will extend the set of revision
strategies to take into account deviations
that might arise the final phase of an ad-
vice giving session, presentation and nego-
tiation of the solution.
</bodyText>
<sectionHeader confidence="0.453013" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.87474066">
[1] ESTEA.M 316. An Architecture for
Interactive Problem Solving by Coop-
erating Data and Knowledge Bases.
Technical Report Deliverable 3, ES- [11]
PRIT Program, 1987.
[2] J. F. Allen and C. R. Perrault.
Analyzing intentions in utterences.
Artificial Intelligence, 3(15):143-178,
1980.
[3] James F. Allen. Plans, goals and nat-
ural language. Research Review of
Computer Science Department, Uni-
versity of Rochester, 4-12, 1986.
[4] Carol A. Broverman and W. Bruce
Croft. Reasoning about exceptions
during plan execution monitoring.
Proc. of AAAI, 190-195, 1987.
[5] A. Bruffaerts, E. Henin, and V. Mar-
lair. An Expert System Prototype
for Financial Counseling. Technical
Report Research Report 507, Philips
Research Laboratory, 1986.
[6] Philip R. Cohen and C. Raymond
Perrault. Elements of a plan-based
theory of speech acts. Cognitive Sci-
ence, (3):177-212, 1979.
[7] P. Decitre, T. Grossi, C. Jullien, and
JP. Solvay. Planning for problem
formulation in advice-giving dialogue.
Proc. of ACL, 1987.
M. P. Georgeff and A. L. Lansky.
Procedural Knowledge. Technical Re-
port Technical Note 411, SRI Inter-
national, January 1987.
Barbara J. Grosz. The Represen-
tation and Use of Focus in Dia-
logue Understanding. Technical Re-
port TR 151, Artificial Intelligence
Center, SRI International, 1977.
Barbara J. Grosz and Candace L.
Sidner. Attention, intentions and
the structure of discourse. Com-
putational Linguistics, 12(3):175-205,
1986.
Diane J. Litman. Discourse and
Problem Solving. Technical Re-
port TR 130, Computer Science
Dpt., University of Rochester, Octo-
ber 1983.
- 160 -
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473070">
<title confidence="0.983252">PLAN REVISION IN PERSON-MACHINE DIALOGUE</title>
<author confidence="0.8800805">Cleo JULLIEN Jean-Charles MARTY</author>
<affiliation confidence="0.997715">Grenoble Research Center of CAP SESA INNOVATION</affiliation>
<address confidence="0.994083">33, Chemin du Vieux Cherie 38240 Meylan France</address>
<email confidence="0.995221">marty@capsogeti.fr</email>
<abstract confidence="0.986751464285714">Dialogue is a cooperative process in which each speech act of the participants contributes to the overall purpose of the dialogue. Participating in a full dialogue implies understanding at each point of the dialogue session the role of each speech act with respect to the rest of the dialogue. We concentrate in this paper on speech acts that diverge from the straightforward unfolding of the dialogue. Such speech acts represent dialogue deviations. We analyze the representation of different types and degrees of deviations and present a plan revision mechanism for dialogue management that permits their treatment in the context of advice giving dialogues between a novice user and an expert problem solver. 1 A Sample Dialogue ESPRIT project we are designing a dialoguer that manages the interaction between a novice user and &amp;quot;An Architecture for Interactive Problem Solving by Cooperating Data and Knowledge Bases&amp;quot; supported in part by the CEC. advice—giving expert systems. The testbed application is personal financial in-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M ESTEA</author>
</authors>
<title>An Architecture for Interactive Problem Solving by Cooperating Data and Knowledge Bases.</title>
<date>1987</date>
<tech>Technical Report Deliverable 3, ES- [11] PRIT Program,</tech>
<marker>ESTEA, 1987</marker>
<rawString>[1] ESTEA.M 316. An Architecture for Interactive Problem Solving by Cooperating Data and Knowledge Bases. Technical Report Deliverable 3, ES- [11] PRIT Program, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing intentions in utterences.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>3--15</pages>
<marker>Allen, Perrault, 1980</marker>
<rawString>[2] J. F. Allen and C. R. Perrault. Analyzing intentions in utterences. Artificial Intelligence, 3(15):143-178, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Plans, goals and natural language.</title>
<date>1986</date>
<journal>Research Review</journal>
<pages>4--12</pages>
<institution>of Computer Science Department, University of Rochester,</institution>
<marker>Allen, 1986</marker>
<rawString>[3] James F. Allen. Plans, goals and natural language. Research Review of Computer Science Department, University of Rochester, 4-12, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol A Broverman</author>
<author>W Bruce Croft</author>
</authors>
<title>Reasoning about exceptions during plan execution monitoring.</title>
<date>1987</date>
<booktitle>Proc. of AAAI,</booktitle>
<pages>190--195</pages>
<marker>Broverman, Croft, 1987</marker>
<rawString>[4] Carol A. Broverman and W. Bruce Croft. Reasoning about exceptions during plan execution monitoring. Proc. of AAAI, 190-195, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bruffaerts</author>
<author>E Henin</author>
<author>V Marlair</author>
</authors>
<title>An Expert System Prototype for Financial Counseling.</title>
<date>1986</date>
<tech>Technical Report Research Report 507,</tech>
<institution>Philips Research Laboratory,</institution>
<marker>Bruffaerts, Henin, Marlair, 1986</marker>
<rawString>[5] A. Bruffaerts, E. Henin, and V. Marlair. An Expert System Prototype for Financial Counseling. Technical Report Research Report 507, Philips Research Laboratory, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>C Raymond Perrault</author>
</authors>
<title>Elements of a plan-based theory of speech acts.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--177</pages>
<marker>Cohen, Perrault, 1979</marker>
<rawString>[6] Philip R. Cohen and C. Raymond Perrault. Elements of a plan-based theory of speech acts. Cognitive Science, (3):177-212, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solvay</author>
</authors>
<title>Planning for problem formulation in advice-giving dialogue.</title>
<date>1987</date>
<booktitle>Proc. of ACL,</booktitle>
<tech>Technical Report Technical Note 411,</tech>
<institution>SRI International,</institution>
<marker>Solvay, 1987</marker>
<rawString>[7] P. Decitre, T. Grossi, C. Jullien, and JP. Solvay. Planning for problem formulation in advice-giving dialogue. Proc. of ACL, 1987. M. P. Georgeff and A. L. Lansky. Procedural Knowledge. Technical Report Technical Note 411, SRI International, January 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
</authors>
<title>The Representation and Use of Focus in Dialogue Understanding.</title>
<date>1977</date>
<tech>Technical Report TR 151,</tech>
<institution>Artificial Intelligence Center, SRI International,</institution>
<marker>Grosz, 1977</marker>
<rawString>Barbara J. Grosz. The Representation and Use of Focus in Dialogue Understanding. Technical Report TR 151, Artificial Intelligence Center, SRI International, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. Attention, intentions and the structure of discourse. Computational Linguistics, 12(3):175-205, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
</authors>
<title>Discourse and Problem Solving.</title>
<date>1983</date>
<tech>Technical Report TR 130,</tech>
<institution>Computer Science Dpt., University of Rochester,</institution>
<marker>Litman, 1983</marker>
<rawString>Diane J. Litman. Discourse and Problem Solving. Technical Report TR 130, Computer Science Dpt., University of Rochester, October 1983.</rawString>
</citation>
<citation valid="false">
<pages>160</pages>
<marker></marker>
<rawString>- 160 -</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>