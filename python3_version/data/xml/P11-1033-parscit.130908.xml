<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.985201">
Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora
</title>
<author confidence="0.999026">
Bin Lu1,3*, Chenhao Tan2, Claire Cardie2 and Benjamin K. Tsou3,1
</author>
<affiliation confidence="0.98295725">
1 Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong Kong
2 Department of Computer Science, Cornell University, Ithaca, NY, USA
3 Research Centre on Linguistics and Language Information Sciences,
Hong Kong Institute of Education, Hong Kong
</affiliation>
<email confidence="0.963865">
lubin2010@gmail.com, {chenhao, cardie}@cs.cornell.edu, btsou99@gmail.com
</email>
<sectionHeader confidence="0.995195" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999197142857143">
Most previous work on multilingual sentiment
analysis has focused on methods to adapt
sentiment resources from resource-rich
languages to resource-poor languages. We
present a novel approach for joint bilingual
sentiment classification at the sentence level
that augments available labeled data in each
language with unlabeled parallel data. We rely
on the intuition that the sentiment labels for
parallel sentences should be similar and present
a model that jointly learns improved mono-
lingual sentiment classifiers for each language.
Experiments on multiple data sets show that the
proposed approach (1) outperforms the mono-
lingual baselines, significantly improving the
accuracy for both languages by 3.44%-8.12%;
(2) outperforms two standard approaches for
leveraging unlabeled data; and (3) produces
(albeit smaller) performance gains when
employing pseudo-parallel data from machine
translation engines.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982572869565217">
The field of sentiment analysis has quickly
attracted the attention of researchers and
practitioners alike (e.g. Pang et al., 2002; Turney,
2002; Hu and Liu, 2004; Wiebe et al., 2005; Breck
et al., 2007; Pang and Lee, 2008). Indeed,
sentiment analysis systems, which mine opinions
from textual sources (e.g. news, blogs, and
reviews), can be used in a wide variety of
*The work was conducted when the first author was visiting
Cornell University.
applications, including interpreting product
reviews, opinion retrieval and political polling.
Not surprisingly, most methods for sentiment
classification are supervised learning techniques,
which require training data annotated with the
appropriate sentiment labels (e.g. document-level
or sentence-level positive vs. negative polarity).
This data is difficult and costly to obtain, and must
be acquired separately for each language under
consideration.
Previous work in multilingual sentiment analysis
has therefore focused on methods to adapt
sentiment resources (e.g. lexicons) from resource-
rich languages (typically English) to other
languages, with the goal of transferring sentiment
or subjectivity analysis capabilities from English to
other languages (e.g. Mihalcea et al. (2007); Banea
et al. (2008; 2010); Wan (2008; 2009);
Prettenhofer and Stein (2010)). In recent years,
however, sentiment-labeled data is gradually
becoming available for languages other than
English (e.g. Seki et al. (2007; 2008); Nakagawa et
al. (2010); Schulz et al. (2010)). In addition, there
is still much room for improvement in existing
monolingual (including English) sentiment
classifiers, especially at the sentence level (Pang
and Lee, 2008).
This paper tackles the task of bilingual
sentiment analysis. In contrast to previous work,
we (1) assume that some amount of sentiment-
labeled data is available for the language pair
under study, and (2) investigate methods to
simultaneously improve sentiment classification
for both languages. Given the labeled data in each
language, we propose an approach that exploits an
unlabeled parallel corpus with the following
</bodyText>
<page confidence="0.96902">
320
</page>
<note confidence="0.9794585">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 320–330,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999827846153846">
intuition: two sentences or documents that are
parallel (i.e. translations of one another) should
exhibit the same sentiment — their sentiment
labels (e.g. polarity, subjectivity, intensity) should
be similar. The proposed maximum entropy-based
EM approach jointly learns two monolingual
sentiment classifiers by treating the sentiment
labels in the unlabeled parallel text as unobserved
latent variables, and maximizes the regularized
joint likelihood of the language-specific labeled
data together with the inferred sentiment labels of
the parallel text. Although our approach should be
applicable at the document-level and for additional
sentiment tasks, we focus on sentence-level
polarity classification in this work.
We evaluate our approach for English and
Chinese on two dataset combinations (see Section
4) and find that the proposed approach outperforms
the monolingual baselines (i.e. maximum entropy
and SVM classifiers) as well as two alternative
methods for leveraging unlabeled data
(transductive SVMs (Joachims, 1999b) and co-
training (Blum and Mitchell, 1998)). Accuracy is
significantly improved for both languages, by
3.44%-8.12%. We furthermore find that
improvements, albeit smaller, are obtained when
the parallel data is replaced with a pseudo-parallel
(i.e. automatically translated) corpus. To our
knowledge, this is the first multilingual sentiment
analysis study to focus on methods for
simultaneously improving sentiment classification
for a pair of languages based on unlabeled data
rather than resource adaptation from one language
to another.
The rest of the paper is organized as follows.
Section 2 introduces related work. In Section 3, the
proposed joint model is described. Sections 4 and
5, respectively, provide the experimental setup and
results; the conclusion (Section 6) follows.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.994136237288135">
Multilingual Sentiment Analysis. There is a
growing body of work on multilingual sentiment
analysis. Most approaches focus on resource
adaptation from one language (usually English) to
other languages with few sentiment resources.
Mihalcea et al. (2007), for example, generate
subjectivity analysis resources in a new language
from English sentiment resources by leveraging a
bilingual dictionary or a parallel corpus. Banea et
al. (2008; 2010) instead automatically translate the
English resources using automatic machine
translation engines for subjectivity classification.
Prettenhofer and Stein (2010) investigate cross-
lingual sentiment classification from the
perspective of domain adaptation based on
structural correspondence learning (Blitzer et al.,
2006).
Approaches that do not explicitly involve
resource adaptation include Wan (2009), which
uses co-training (Blum and Mitchell, 1998) with
English vs. Chinese features comprising the two
independent ―views‖ to exploit unlabeled Chinese
data and a labeled English corpus and thereby
improves Chinese sentiment classification.
Another notable approach is the work of Boyd-
Graber and Resnik (2010), which presents a
generative model --- supervised multilingual latent
Dirichlet allocation --- that jointly models topics
that are consistent across languages, and employs
them to better predict sentiment ratings.
Unlike the methods described above, we focus
on simultaneously improving the performance of
sentiment classification in a pair of languages by
developing a model that relies on sentiment-
labeled data in each language as well as unlabeled
parallel text for the language pair.
Semi-supervised Learning. Another line of
related work is semi-supervised learning, which
combines labeled and unlabeled data to improve
the performance of the task of interest (Zhu and
Goldberg, 2009). Among the popular semi-
supervised methods (e.g. EM on Naïve Bayes
(Nigam et al., 2000), co-training (Blum and
Mitchell, 1998), transductive SVMs (Joachims,
1999b), and co-regularization (Sindhwani et al.,
2005; Amini et al., 2010)), our approach employs
the EM algorithm, extending it to the bilingual
case based on maximum entropy. We compare to
co-training and transductive SVMs in Section 5.
Multilingual NLP for Other Tasks. Finally,
there exists related work using bilingual resources
to help other NLP tasks, such as word sense
disambiguation (e.g. Ido and Itai (1994)), parsing
(e.g. Burkett and Klein (2008); Zhao et al. (2009);
Burkett et al. (2010)), information retrieval (Gao et
al., 2009), named entity detection (Burkett et al.,
2010); topic extraction (e.g. Zhang et al., 2010),
text classification (e.g. Amini et al., 2010), and
hyponym-relation acquisition (e.g. Oh et al., 2009).
</bodyText>
<page confidence="0.997771">
321
</page>
<bodyText confidence="0.9998134">
In these cases, multilingual models increase
performance because different languages contain
different ambiguities and therefore present
complementary views on the shared underlying
labels. Our work shares a similar motivation.
</bodyText>
<sectionHeader confidence="0.9370195" genericHeader="method">
3 A Joint Model with Unlabeled Parallel
Text
</sectionHeader>
<bodyText confidence="0.99991025">
We propose a maximum entropy-based statistical
model. Maximum entropy (MaxEnt) models1 have
been widely used in many NLP tasks (Berger et al.,
1996; Ratnaparkhi, 1997; Smith, 2006). The
models assign the conditional probability of the
label given the observation as follows:
where is a real-valued vector of feature weights
and is a feature function that maps pairs to
a nonnegative real-valued feature vector. Each
feature has an associated parameter, , which is
called its weight; and is the corresponding
normalization factor.
Maximum likelihood parameter estimation
(training) for such a model, with a set of labeled
examples , amounts to solving the
following optimization problem:
</bodyText>
<subsectionHeader confidence="0.995748">
3.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.987699">
Given two languages and , suppose we have
two distinct (i.e. not parallel) sets of sentiment-
labeled data, and written in and
respectively. In addition, we have unlabeled (w.r.t.
sentiment) bilingual (in and ) parallel data
that are defined as follows.
where denotes the polarity of
the -th instance (positive or negative); and
are respectively the numbers of labeled instances
in and ; and are parallel instances in
and , respectively (i.e. they are supposed to be
</bodyText>
<footnote confidence="0.883453">
1They are sometimes referred to as log-linear models, but also
known as exponential models, generalized linear models, or
logistic regression.
</footnote>
<bodyText confidence="0.994041384615385">
translations of one another), whose labels and
are unobserved, but according to the intuition
outlined in Section 1, should be similar.
Given the input data and , our task is to
jointly learn two monolingual sentiment classifiers
— one for and one for . With MaxEnt, we
learn from the input data:
where and are the vectors of feature weights
for and , respectively (for brevity we denote
them as and in the remaining sections). In this
study, we focus on sentence-level sentiment
classification, i.e. each is a sentence, and and
are parallel sentences.
</bodyText>
<subsectionHeader confidence="0.99527">
3.2 The Joint Model
</subsectionHeader>
<bodyText confidence="0.99990136">
Given the problem definition above, we now
present a novel model to exploit the
correspondence of parallel sentences in unlabeled
bilingual text. The model maximizes the following
joint likelihood with respect to and :
where denotes or ; the first term on
the right-hand side is the likelihood of labeled data
for both and ; and the second term is the
likelihood of the unlabeled parallel data .
If we assume that parallel sentences are perfect
translations, the two sentences in each pair should
have the same polarity label, which gives us:
where is the unobserved class label for the -th
instance in the unlabeled data. This probability
directly models the sentiment label agreement
between and .
However, there could be considerable noise in
real-world parallel data, i.e. the sentence pairs may
be noisily parallel (or even comparable) instead of
fully parallel (Munteanu and Marcu, 2005). In such
noisy cases, the labels (positive or negative) could
be different for the two monolingual sentences in a
sentence pair. Although we do not know the exact
probability that a sentence pair exhibits the same
label, we can approximate it using their translation
</bodyText>
<page confidence="0.991963">
322
</page>
<bodyText confidence="0.999677464285714">
probabilities, which can be computed using word
alignment toolkits such as Giza++ (Och and Ney,
2003) or the Berkeley word aligner (Liang et al.,
2006). The intuition here is that if the translation
probability of two sentences is high, the probability
that they have the same sentiment label should be
high as well. Therefore, by considering the noise in
parallel data, we get:
where is the translation probability of the -th
sentence pair in ;2 is the opposite of ; the first
term models the probability that and have
the same label; and the second term models the
probability that they have different labels.
By further considering the weight to ascribe to
the unlabeled data vs. the labeled data (and the
weight for the L2-norm regularization), we get the
following regularized joint log likelihood to be
maximized:
where the first term on the right-hand side is the
log likelihood of the labeled data from both and
the second is the log likelihood of the
unlabeled parallel data , multiplied by , a
constant that controls the contribution of the
unlabeled data; and is a regularization
constant that penalizes model complexity or large
feature weights. When is 0, the algorithm
ignores the unlabeled data and degenerates to two
MaxEnt models trained on only the labeled data.
</bodyText>
<subsectionHeader confidence="0.999475">
3.3 The EM Algorithm on MaxEnt
</subsectionHeader>
<bodyText confidence="0.999992333333333">
To solve the optimization problem for the model,
we need to jointly estimate the optimal parameters
for the two monolingual classifiers by finding:
</bodyText>
<equation confidence="0.769321">
(Bl, BZ) = arg
</equation>
<bodyText confidence="0.999229588235294">
This can be done with an EM algorithm, whose
steps are summarized in Algorithm 1. First, the
MaxEnt parameters, and , are estimated from
2The probability should be rescaled within the range of [0, 1],
where 0.5 means that we are completely unsure if the
sentences are translations of each other or not, and only those
translation pairs with a probability larger than 0.5 are
meaningful for our purpose.
just the labeled data. Then, in the E-step, the
classifiers, based on current values of and ,
compute for each labeled example and
assign probabilistically-weighted class labels to
each unlabeled example. Next, in the M-step, the
parameters, and , are updated using both the
original labeled data ( and ) and the newly
labeled data . These last two steps are iterated
until convergence or a predefined iteration limit .
</bodyText>
<subsectionHeader confidence="0.9580575">
Algorithm 1. The MaxEnt-based EM Algorithm for
Multilingual Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.96673675">
Input: Labeled data and
Unlabeled parallel data
Two monolingual MaxEnt classifiers with
parameters and , respectively
</bodyText>
<listItem confidence="0.762817">
1. Train two initial monolingual models
Train and initialize and on the labeled data
2. Jointly optimize two monolingual models
</listItem>
<bodyText confidence="0.858872555555556">
for to do // T: number of iterations
E-Step:
Compute for each example in , and 1
based on and ;
Compute the expectation of the log likelihood with
respect to ;
M-Step:
Find and by maximizing the regularized
joint log likelihood;
</bodyText>
<subsectionHeader confidence="0.461219">
Convergence:
</subsectionHeader>
<bodyText confidence="0.975098">
If the increase of the joint log likelihood is
sufficiently small, break;
</bodyText>
<listItem confidence="0.906523">
end for
3. Output as s, and as
</listItem>
<bodyText confidence="0.882638470588235">
In the M-step, we can optimize the regularized
joint log likelihood using any gradient-based
optimization technique (Malouf, 2002). The
gradient for Equation 3 based on Equation 4 is
shown in Appendix A; those for Equations 5 and 6
can be derived similarly. In our experiments, we
use the L-BFGS algorithm (Liu et al., 1989) and
run EM until the change in regularized joint log
likelihood is less than 1e-5 or we reach 100
iterations.3
3Since the EM-based algorithm may find a local maximum of
the objective function, the initialization of the parameters is
important. Our experiments show that an effective maximum
can usually be found by initializing the parameters with those
learned from the labeled data; performance would be much
worse if we initialize all the parameters to 0 or 1.
Output:
</bodyText>
<page confidence="0.983148">
323
</page>
<subsectionHeader confidence="0.9966635">
3.4 Pseudo-Parallel Labeled and Unlabeled
Data
</subsectionHeader>
<bodyText confidence="0.999767071428572">
We also consider the case where a parallel corpus
is not available: to obtain a pseudo-parallel corpus
U (i.e. sentences in one language with their
corresponding automatic translations), we use an
automatic machine translation system (e.g. Google
machine translation 4 ) to translate unlabeled in-
domain data from Ll to L2 or vice versa.
Since previous work (Banea et al., 2008; 2010;
Wan, 2009) has shown that it could be useful to
automatically translate the labeled data from the
source language into the target language, we can
further incorporate such translated labeled data into
the joint model by adding the following component
into Equation 6:
</bodyText>
<equation confidence="0.88301">
2
A3 Zv=-_r Zi=-_r logp(Yi 1xi : ev) (8)
</equation>
<bodyText confidence="0.8773148">
where v is the alternative class of v, xl * is the
automatically translated example from xl ; and
A &gt;_ 0 is a constant that controls the weight of the
3
translated labeled data.
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.999954">
4.1 Data Sets and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999723">
The following labeled datasets are used in our
experiments.
MPQA (Labeled English Data): The Multi-
Perspective Question Answering (MPQA) corpus
(Wiebe et al., 2005) consists of newswire
documents manually annotated with phrase-level
subjectivity information. We extract all sentences
containing strong (i.e. intensity is medium or
higher), sentiment-bearing (i.e. polarity is positive
or negative) expressions following Choi and
Cardie (2008). Sentences with both positive and
negative strong expressions are then discarded, and
the polarity of each remaining sentence is set to
that of its sentiment-bearing expression(s).
</bodyText>
<sectionHeader confidence="0.9730665" genericHeader="method">
NTCIR-EN (Labeled English Data) and
NTCIR-CH (Labeled Chinese Data): The
</sectionHeader>
<bodyText confidence="0.949776166666667">
NTCIR Opinion Analysis task (Seki et al., 2007;
2008) provides sentiment-labeled news data in
Chinese, Japanese and English. Only those
sentences with a polarity label (positive or
negative) agreed to by at least two annotators are
extracted. We use the Chinese data from NTCIR-6
</bodyText>
<footnote confidence="0.864011">
4http://translate.google.com/
</footnote>
<bodyText confidence="0.99725875">
as our Chinese labeled data. Since far fewer
sentences in the English data pass the annotator
agreement filter, we combine the English data from
NTCIR-6 and NTCIR-7. The Chinese sentences
are segmented using the Stanford Chinese word
segmenter (Tseng et al., 2005).
The number of sentences in each of these
datasets is shown in Table 1. In our experiments,
we evaluate two settings of the data: (1)
MPQA+NTCIR-CH, and (2) NTCIR-EN+NTCIR-
CH. In each setting, the English labeled data
constitutes Dl and the Chinese labeled data, D2.
</bodyText>
<table confidence="0.97699075">
MPQA NTCIR-EN NTCIR-CH
Positive 1,471 (30%) 528 (30%) 2,378 (55%)
Negative 3,487 (70%) 1,209 (70%) 1,916 (45%)
Total 4,958 1,737 4,294
</table>
<tableCaption confidence="0.999619">
Table 1: Sentence Counts for the Labeled Data
</tableCaption>
<bodyText confidence="0.985925515151515">
Unlabeled Parallel Text and its Preprocessing.
For the unlabeled parallel text, we use the ISI
Chinese-English parallel corpus (Munteanu and
Marcu, 2005), which was extracted automatically
from news articles published by Xinhua News
Agency in the Chinese Gigaword (2nd Edition) and
English Gigaword (2nd Edition) collections.
Because sentence pairs in the ISI corpus are quite
noisy, we rely on Giza++ (Och and Ney, 2003) to
obtain a new translation probability for each
sentence pair, and select the 100,000 pairs with the
highest translation probabilities.5
We also try to remove neutral sentences from
the parallel data since they can introduce noise into
our model, which deals only with positive and
negative examples. To do this, we train a single
classifier from the combined Chinese and English
labeled data for each data setting above by
concatenating the original English and Chinese
feature sets. We then classify each unlabeled
sentence pair by combining the two sentences in
each pair into one. We choose the most confidently
predicted 10,000 positive and 10,000 negative
pairs to constitute the unlabeled parallel corpus U
for each data setting.
5We removed sentence pairs with an original confidence score
(given in the corpus) smaller than 0.98, and also removed the
pairs that are too long (more than 60 characters in one
sentence) to facilitate Giza++. We first obtain translation
probabilities for both directions (i.e. Chinese to English and
English to Chinese) with Giza++, take the log of the product
of those two probabilities, and then divide it by the sum of
lengths of the two sentences in each pair.
</bodyText>
<page confidence="0.997071">
324
</page>
<sectionHeader confidence="0.826444" genericHeader="method">
4.2 Baseline Methods
</sectionHeader>
<bodyText confidence="0.999424921052632">
In our experiments, the proposed joint model is
compared with the following baseline methods.
MaxEnt: This method learns a MaxEnt
classifier for each language given the monolingual
labeled data; the unlabeled data is not used.
SVM: This method learns an SVM classifier for
each language given the monolingual labeled data;
the unlabeled data is not used. SVM-light
(Joachims, 1999a) is used for all the SVM-related
experiments.
Monolingual TSVM (TSVM-M): This method
learns two transductive SVM (TSVM) classifiers
given the monolingual labeled data and the
monolingual unlabeled data for each language.
Bilingual TSVM (TSVM-B): This method
learns one TSVM classifier given the labeled
training data in two languages together with the
unlabeled sentences by combining the two
sentences in each unlabeled pair into one. We
expect this method to perform better than TSVM-
M since the combined (bilingual) unlabeled
sentences could be more helpful than the unlabeled
monolingual sentences.
Co-Training with SVMs (Co-SVM): This
method applies SVM-based co-training given both
the labeled training data and the unlabeled parallel
data following Wan (2009). First, two monolingual
SVM classifiers are built based on only the
corresponding labeled data, and then they are
bootstrapped by adding the most confident
predicted examples from the unlabeled data into
the training set. We run bootstrapping for 100
iterations. In each iteration, we select the most
confidently predicted 50 positive and 50 negative
sentences from each of the two classifiers, and take
the union of the resulting 200 sentence pairs as the
newly labeled training data. (Examples with
conflicting labels within the pair are not included.)
</bodyText>
<sectionHeader confidence="0.999396" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.984377210526316">
In our experiments, the methods are tested in the
two data settings with the corresponding unlabeled
parallel corpus as mentioned in Section 4.6 We use
6 The results reported in this section employ Equation 4.
Preliminary experiments showed that Equation 5 does not
significantly improve the performance in our case, which is
reasonable since we choose only sentence pairs with the
highest translation probabilities to be our unlabeled data (see
Section 4.1).
5-fold cross-validation and report average accuracy
(also MicroF1 in this case) and MacroF1 scores.
Unigrams are used as binary features for all
models, as Pang et al. (2002) showed that binary
features perform better than frequency features for
sentiment classification. The weights for unlabeled
data and regularization, and , are set to 1
unless otherwise stated. Later, we will show that
the proposed approach performs well with a wide
range of parameter values.7
</bodyText>
<subsectionHeader confidence="0.998417">
5.1 Method Comparison
</subsectionHeader>
<bodyText confidence="0.999908212121212">
We first compare the proposed joint model (Joint)
with the baselines in Table 2. As seen from the
table, the proposed approach outperforms all five
baseline methods in terms of both accuracy and
MacroF1 for both English and Chinese and in both
of the data settings. 8 By making use of the
unlabeled parallel data, our proposed approach
improves the accuracy, compared to MaxEnt, by
8.12% (or 33.27% error reduction) on English and
3.44% (or 16.92% error reduction) on Chinese in
the first setting, and by 5.07% (or 19.67% error
reduction) on English and 3.87% (or 19.4% error
reduction) on Chinese in the second setting.
Among the baselines, the best is Co-SVM;
TSVMs do not always improve performance using
the unlabeled data compared to the standalone
SVM; and TSVM-B outperforms TSVM-M except
for Chinese in the second setting. The MPQA data
is more difficult in general compared to the NTCIR
data. Without unlabeled parallel data, the
performance on the Chinese data is better than on
the English data, which is consistent with results
reported in NTCIR-6 (Seki et al., 2007).
Overall, the unlabeled parallel data improves
classification accuracy for both languages when
using our proposed joint model and Co-SVM. The
joint model makes better use of the unlabeled
parallel data than Co-SVM or TSVMs presumably
because of its attempt to jointly optimize the two
monolingual models via soft (probabilistic)
assignments of the unlabeled instances to classes in
each iteration, instead of the hard assignments in
Co-SVM and TSVMs. Although English sentiment
</bodyText>
<footnote confidence="0.994352666666667">
7The code is at http://sites.google.com/site/lubin2010.
8 Significance is tested using paired t-tests with &lt;0.05: €
denotes statistical significance compared to the corresponding
performance of MaxEnt; * denotes statistical significance
compared to SVM; and Γ denotes statistical significance
compared to Co-SVM.
</footnote>
<page confidence="0.994944">
325
</page>
<table confidence="0.999754666666667">
Setting 1: NTCIR-EN+NTCIR-CH Setting 2: MPQA+NTCIR-CH
Accuracy MacroF1 Accuracy MacroF1
English Chinese English Chinese English Chinese English Chinese
MaxEnt 75.59 79.67 66.61* 79.34 74.22 79.67 65.09* 79.34
SVM 76.34 81.02 61.12 80.75€ 76.74€ 81.02 61.35 80.75€
TSVM-M 73.46 80.21 55.33 79.99 72.89 81.14 52.82 79.99
TSVM-B 78.36 81.60€ 65.53 81.42 76.42€ 78.51 61.66 78.32
Co-SVM 82.44€* 82.79€ 72.61€* 82.67€* 78.18€* 82.63€* 68.03€* 82.51€*
Joint 83.71€* 83.11€* 75.89€*Γ 82.97€* 79.29€*Γ 83.54€* 72.58€*Γ 83.37€*
</table>
<tableCaption confidence="0.999578">
Table 2: Comparison of Results
</tableCaption>
<figure confidence="0.998057392857143">
Weight of Unlabeled Data
Size of Unlabeled Data
0 0.2 0.4 0.6 0.8 1
86
84
82
80
78
76
74
72
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
0 0.5 1 1.5 2
86
84
82
80
78
76
74
72
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
</figure>
<figureCaption confidence="0.999997">
Figure 1. Accuracy vs. Weight of Unlabeled Data Figure 2. Accuracy vs. Amount of Unlabeled Data
</figureCaption>
<bodyText confidence="0.99995425">
classification alone is more difficult than Chinese
for our datasets, we obtain greater performance
gains for English by exploiting unlabeled parallel
data as well as the Chinese labeled data.
</bodyText>
<subsectionHeader confidence="0.9980975">
5.2 Varying the Weight and Amount of
Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999885375">
Figure 1 shows the accuracy curve of the proposed
approach for the two data settings when varying
the weight for the unlabeled data, , from 0 to 1.
When is set to 0, the joint model degenerates to
two MaxEnt models trained with only the labeled
data.
We can see that the performance gains for the
proposed approach are quite remarkable even when
is set to 0.1; performance is largely stable after
reaches 0.4. Although MPQA is more difficult
in general compared to the NTCIR data, we still
see steady improvements in performance with
unlabeled parallel data. Overall, the proposed
approach performs quite well for a wide range of
parameter values of .
Figure 2 shows the accuracy curve of the
proposed approach for the two data settings when
varying the amount of unlabeled data from 0 to
20,000 instances. We see that the performance of
the proposed approach improves steadily by adding
more and more unlabeled data. However, even
with only 2,000 unlabeled sentence pairs, the
proposed approach still produces large
performance gains.
</bodyText>
<subsectionHeader confidence="0.837589">
5.3 Results on Pseudo-Parallel Unlabeled
Data
</subsectionHeader>
<bodyText confidence="0.979835047619048">
As discussed in Section 3.4, we generate pseudo-
parallel data by translating the monolingual
sentences in each setting using Google’s machine
translation system. Figures 3 and 4 show the
performance of our model using the pseudo-
parallel data versus the real parallel data, in the two
settings, respectively. The EN-&gt;CH pseudo-
parallel data consists of the English unlabeled data
and its automatic Chinese translation, and vice
versa.
Although not as significant as those with parallel
data, we can still obtain improvements using the
pseudo-parallel data, especially in the first setting.
The difference between using parallel versus
pseudo-parallel data is around 2-4% in Figures 3
and 4, which is reasonable since the quality of the
pseudo-parallel data is not as good as that of the
parallel data. Therefore, the performance using
pseudo-parallel data is better with a small weight
(e.g. = 0.1) in some cases.
irAccacy
</bodyText>
<page confidence="0.957763">
326
</page>
<figure confidence="0.996726964285714">
Weight of Unlabeled Data
Weight of Unlabeled Data
0 0.2 0.4 0.6 0.8 1
86
84
82
80
78
76
74
English on Parallel Data
Chinese on Parallel Data
English on EN-&gt;CH Pseudo-Parallel Data
Chinese on EN-&gt;CH Pseudo-Parallel Data
English on CH-&gt;EN Pseudo-Parallel Data
Chinese on CH-&gt;EN Pseudo-Parallel Data
0 0.2 0.4 0.6 0.8 1
65
85
80
75
70
English on Parallel Data
Chinese on Parallel Data
English on EN-&gt;CH Pseudo-Parallel Data
Chinese on EN-&gt;CH Pseudo-Parallel Data
English on CH-&gt;EN Pseudo-Parallel Data
Chinese on CH-&gt;EN Pseudo-Parallel Data
</figure>
<figureCaption confidence="0.988759666666667">
Figure 3. Accuracy with Pseudo-Parallel Unlabeled Figure 4. Accuracy with Pseudo-Parallel Unlabeled
Data in Setting 1 Data in Setting 2
Figure 5. Accuracy with Pseudo-Parallel Labeled Figure 6. Accuracy with Pseudo-Parallel Labeled
</figureCaption>
<figure confidence="0.99320934375">
Data in Setting 1 Data in Setting 2
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
0 0.2 0.4 0.6 0.8 1
Weight of Translated Labeled Data
0 0.2 0.4 0.6 0.8 1
Weight of Translated Labeled Data
86
84
82
80
78
76
74
72
70
86
84
82
80
78
76
74
72
70
68
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
</figure>
<subsectionHeader confidence="0.997565">
5.4 Adding Pseudo-Parallel Labeled Data
</subsectionHeader>
<bodyText confidence="0.987108705882353">
In this section, we investigate how adding
automatically translated labeled data might
influence the performance as mentioned in Section
3.4. We use only the translated labeled data to train
classifiers, and then directly classify the test data.
The average accuracies in setting 1 are 66.61% and
63.11% on English and Chinese, respectively;
while the accuracies in setting 2 are 58.43% and
54.07% on English and Chinese, respectively. This
result is reasonable because of the language gap
between the original language and the translated
language. In addition, the class distributions of the
English labeled data and the Chinese are quite
different (30% vs. 55% for positive as shown in
Table 1).
Figures 5 and 6 show the accuracies when
varying the weight of the translated labeled data vs.
the labeled data, with and without the unlabeled
parallel data. From Figure 5 for setting 1, we can
%
r
A
see that the translated data can be helpful given the
labeled data and even the unlabeled data, as long as
3 is small; while in Figure 6, the translated data
decreases the performance in most cases for setting
2. One possible reason is that in the first data
setting, the NTCIR English data covers the same
topics as the NTCIR Chinese data and thus direct
translation is helpful, while the English and
Chinese topics are quite different in the second
data setting, and thus direct translation hurts the
performance given the existing labeled data in each
language.
</bodyText>
<subsectionHeader confidence="0.89354">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999966571428572">
To further understand what contributions our
proposed approach makes to the performance gain,
we look inside the parameters in the MaxEnt
models learned before and after adding the parallel
unlabeled data. Table 3 shows the features in the
model learned from the labeled data that have the
largest weight change after adding the parallel data;
</bodyText>
<page confidence="0.993957">
327
</page>
<table confidence="0.999686166666667">
Word Weight
Before After Change
Positive important 0.452 1.659 1.207
cooperation 0.325 1.492 1.167
support 0.533 1.483 0.950
importance 0.450 1.193 0.742
agreed 0.347 1.061 0.714
Negative difficulties 0.018 0.663 0.645
not 0.202 0.844 0.641
never 0.245 0.879 0.634
germany 0.035 0.664 0.629
taiwan 0.590 1.216 0.626
</table>
<tableCaption confidence="0.955037">
Table 3. Original Features with Largest Weight Change
</tableCaption>
<table confidence="0.999870285714286">
Positive Negative
Word Weight Word Weight
friendly 0.701 german 0.783
principles 0.684 arduous 0.531
hopes 0.630 oppose 0.511
hoped 0.553 administrations 0.431
cooperative 0.552 oau9 0.408
</table>
<tableCaption confidence="0.998441">
Table 4. New Features Learned from Unlabeled Data
</tableCaption>
<bodyText confidence="0.999977307692308">
and Table 4 shows the newly learned features from
the unlabeled data with the largest weights.
From Table 3 10 we can see that the weight
changes of the original features are quite
reasonable, e.g. the top words in the positive class
are obviously positive and the proposed approach
gives them higher weights. The new features also
seem reasonable given the knowledge that the
labeled and unlabeled data includes negative news
about for specific topics (e.g. Germany, Taiwan),.
We also examine the process of joint training by
checking the performance on test data and the
agreement of the two monolingual models on the
unlabeled parallel data in both settings. The
average agreement across 5 folds is 85.06% and
73.87% in settings 1 and 2, respectively, before the
joint training, and increases to 100% and 99.89%,
respectively, after 100 iterations of joint training.
Although the average agreement has already
increased to 99.50% and 99.02% in settings 1 and
2, respectively, after 30 iterations, the performance
on the test set steadily improves in both settings
until around 50-60 iterations, and then becomes
relatively stable after that.
Examination of those sentence pairs in setting 2
for which the two monolingual models still
</bodyText>
<footnote confidence="0.992246666666667">
9This is an abbreviation for the Organization of African Unity.
10The features and weights in Tables 3 and 4 are extracted
from the English model in the first fold of setting 1.
</footnote>
<bodyText confidence="0.994762">
disagree after 100 iterations of joint training often
produces sentences that are not quite parallel, e.g.:
English: The two sides attach great importance to
international cooperation on protection and promotion of
human rights.
Chinese: ( %1. 7, I�XP7��Tf1 :�X― ( �f ‖,fzXf�
�Ir�cs&apos;#&apos;J�I�XP7��Lo(Both sides agree that double
standards on the issue of human rights are to be avoided, and
are opposed to using pressure on human rights issues in
international relations.)
Since the two sentences discuss human rights
from very different perspectives, it is reasonable
that the two monolingual models will classify them
with different polarities (i.e. positive for the
English sentence and negative for the Chinese
sentence) even after joint training.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997655">
In this paper, we study bilingual sentiment
classification and propose a joint model to
simultaneously learn better monolingual sentiment
classifiers for each language by exploiting an
unlabeled parallel corpus together with the labeled
data available for each language. Our experiments
show that the proposed approach can significantly
improve sentiment classification for both
languages. Moreover, the proposed approach
continues to produce (albeit smaller) performance
gains when employing pseudo-parallel data from
machine translation engines.
In future work, we would like to apply the joint
learning idea to other learning frameworks (e.g.
SVMs), and to extend the proposed model to
handle word-level parallel information, e.g.
bilingual dictionaries or word alignment
information. Another issue is to investigate how to
improve multilingual sentiment analysis by
exploiting comparable corpora.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997298">
We thank Shuo Chen, Long Jiang, Thorsten
Joachims, Lillian Lee, Myle Ott, Yan Song,
Xiaojun Wan, Ainur Yessenalina, Jingbo Zhu and
the anonymous reviewers for many useful
comments and discussion. This work was
supported in part by National Science Foundation
Grants BCS-0904822, BCS-0624277, IIS-
0968450; and by a gift from Google. Chenhao Tan
is supported by NSF (DMS-0808864), ONR (YIP-
N000140910911), and a grant from Microsoft.
</bodyText>
<page confidence="0.998075">
328
</page>
<sectionHeader confidence="0.99012" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999829989583334">
Massih-Reza Amini, Cyril Goutte, and Nicolas Usunier.
2010. Combining coregularization and consensus-
based self-training for multilingual text
categorization. In Proceeding of SIGIR’10.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more languages
better? In Proceedings of COLING’10.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings of
EMNLP’08.
Adam L. Berger, Stephen A. Della Pietra and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational
Linguistics, 22(1).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspond-
dence learning. In Proceedings of EMNLP’06.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of COLT’98.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual
supervised Latent Dirichlet Allocation. In
Proceedings of EMNLP’10.
Eric Breck, Yejin Choi, and Claire Cardie. 2007.
Identifying expressions of opinion in context. In
Proceedings of IJCAI’07.
David Burkett, Slav Petrov, John Blitzer, and Dan
Klein. 2010. Learning better monolingual models
with unannotated bilingual text. In Proceedings of
CoNLL’10.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In
Proceedings of EMNLP’08.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
EMNLP’08.
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.
2009. Exploiting bilingual information to improve
web search. In Proceedings of ACL/IJCNLP‘09.
Minqing Hu and Bing Liu. 2004. Mining opinion
features in customer reviews. In Proceedings of
AAAI’04.
Ido Dagan, and Alon Itai. 1994. Word sense
disambiguation using a second language monolingual
corpus, Computational Linguistics, 20(4): 563-596.
Thorsten Joachims. 1999a. Making Large-Scale SVM
Learning Practical. In: Advances in Kernel Methods -
Support Vector Learning, B. Schölkopf, C. Burges,
and A. Smola (ed.), MIT Press.
Thorsten Joachims. 1999b. Transductive inference for
text classification using support vector machines. In
Proceedings of ICML’99.
Percy Liang, Ben Taskar, and Dan Klein. 2006.
Alignment by agreement. In Proceedings of
NAACL’06.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, (45): 503–528.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In
Proceedings of CoNLL’02.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of ACL’07.
Dragos S. Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by
exploiting non-parallel corpora. Computational
Linguistics, 31(4): 477–504.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Proceedings of
NAACL/HLT ‘10.
Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun,
and Tom Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Machine
Learning, 39(2): 103–134.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1): 19-51.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis, Foundations and Trends in
Information Retrieval, Now Publishers.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of
EMNLP’02.
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural
correspondence learning. In Proceedings of ACL’10.
Adwait Ratnaparkhi. 1997. A simple introduction to
maximum entropy models for natural language
processing. Technical Report 97-08, University of
Pennsylvania.
</reference>
<page confidence="0.988872">
329
</page>
<reference confidence="0.999472869565217">
Julia M. Schulz, Christa Womser-Hacker, and Thomas
Mandl. 2010. Multilingual corpus development for
opinion mining. In Proceedings of LREC’10.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-His Chen, and Noriko Kando. 2008. Overview
of multilingual opinion analysis task at NTCIR-7. In
Proceedings of the NTCIR-7 Workshop.
Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun,
Hsin-His Chen, Noriko Kando, and Chin-Yew Lin.
2007. Overview of opinion analysis pilot task at
NTCIR-6. In Proceedings of the NTCIR-6 Workshop.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. A co-regularization approach to semi-
supervised learning with multiple views. In
Proceedings of ICML’05.
Noah A. Smith. 2006. Novel estimation methods for
unsupervised discovery of latent structure in natural
language text. Ph.D. thesis, Department of Computer
Science, Johns Hopkins University.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky and Christopher Manning. 2005. A
conditional random field word segmenter. In
Proeedings of the 4th SIGHAN Workshop.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised
classification of reviews, In Proceedings of ACL’02.
Xiaojun Wan. 2008. Using Bilingual Knowledge and
Ensemble Techniques for Unsupervised Chinese
Sentiment Analysis. In Proceedings of EMNLP’08.
Xiaojun Wan. 2009. Co-training for cross-lingual
sentiment classification. In Proceedings of
ACL/AFNLP’09.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2- 3): 165-210.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction, In Proceedings
of ACL’10.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of
ACL/IJCNLP’09.
Xiaojin Zhu and Andrew B. Goldberg. 2009.
Introduction to Semi-Supervised Learning. Morgan
&amp; Claypool Publishers.
</reference>
<sectionHeader confidence="0.917076" genericHeader="references">
Appendix A. Equation Deduction
</sectionHeader>
<bodyText confidence="0.963497461538462">
In this appendix, we derive the gradient for the objective
function in Equation 3, which is used in parameter
estimation. As mentioned in Section 3.3, the parameters
can be learned by finding:
Since the first term on the right-hand side is just the
expression for the standard MaxEnt problem, we will
focus on the gradient for the second term, and denote
as ( ).
Let denote or , and be the th weight
in the vector . For brevity, we drop the in the above
notation, and write to denote . Then the partial
derivative of (*) based on Equation 4 with respect to
is as follows:
</bodyText>
<equation confidence="0.4796025">
(1)
Further, we obtain:
(2)
Merge (2) into (1), we get:
</equation>
<page confidence="0.97839">
330
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.471794">
<title confidence="0.986001">Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</title>
<author confidence="0.981484">Chenhao Claire</author>
<author confidence="0.981484">K Benjamin</author>
<affiliation confidence="0.804321">1Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong 2Department of Computer Science, Cornell University, Ithaca, NY, USA</affiliation>
<title confidence="0.739231">3Research Centre on Linguistics and Language Information</title>
<author confidence="0.974554">Hong Kong Institute of Education</author>
<author confidence="0.974554">Hong Kong</author>
<email confidence="0.998673">lubin2010@gmail.com,{chenhao,cardie}@cs.cornell.edu,btsou99@gmail.com</email>
<abstract confidence="0.999550363636364">Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Massih-Reza Amini</author>
<author>Cyril Goutte</author>
<author>Nicolas Usunier</author>
</authors>
<title>Combining coregularization and consensusbased self-training for multilingual text categorization.</title>
<date>2010</date>
<booktitle>In Proceeding of SIGIR’10.</booktitle>
<contexts>
<context position="7629" citStr="Amini et al., 2010" startWordPosition="1073" endWordPosition="1076">e of sentiment classification in a pair of languages by developing a model that relies on sentimentlabeled data in each language as well as unlabeled parallel text for the language pair. Semi-supervised Learning. Another line of related work is semi-supervised learning, which combines labeled and unlabeled data to improve the performance of the task of interest (Zhu and Goldberg, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et a</context>
</contexts>
<marker>Amini, Goutte, Usunier, 2010</marker>
<rawString>Massih-Reza Amini, Cyril Goutte, and Nicolas Usunier. 2010. Combining coregularization and consensusbased self-training for multilingual text categorization. In Proceeding of SIGIR’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multilingual subjectivity: Are more languages better?</title>
<date>2010</date>
<booktitle>In Proceedings of COLING’10.</booktitle>
<marker>Banea, Mihalcea, Wiebe, 2010</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2010. Multilingual subjectivity: Are more languages better? In Proceedings of COLING’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
<author>Samer Hassan</author>
</authors>
<title>Multilingual subjectivity analysis using machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP’08.</booktitle>
<contexts>
<context position="2676" citStr="Banea et al. (2008" startWordPosition="367" endWordPosition="370"> which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the lang</context>
<context position="5989" citStr="Banea et al. (2008" startWordPosition="843" endWordPosition="846"> In Section 3, the proposed joint model is described. Sections 4 and 5, respectively, provide the experimental setup and results; the conclusion (Section 6) follows. 2 Related Work Multilingual Sentiment Analysis. There is a growing body of work on multilingual sentiment analysis. Most approaches focus on resource adaptation from one language (usually English) to other languages with few sentiment resources. Mihalcea et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and </context>
<context position="15830" citStr="Banea et al., 2008" startWordPosition="2403" endWordPosition="2406">e maximum can usually be found by initializing the parameters with those learned from the labeled data; performance would be much worse if we initialize all the parameters to 0 or 1. Output: 323 3.4 Pseudo-Parallel Labeled and Unlabeled Data We also consider the case where a parallel corpus is not available: to obtain a pseudo-parallel corpus U (i.e. sentences in one language with their corresponding automatic translations), we use an automatic machine translation system (e.g. Google machine translation 4 ) to translate unlabeled indomain data from Ll to L2 or vice versa. Since previous work (Banea et al., 2008; 2010; Wan, 2009) has shown that it could be useful to automatically translate the labeled data from the source language into the target language, we can further incorporate such translated labeled data into the joint model by adding the following component into Equation 6: 2 A3 Zv=-_r Zi=-_r logp(Yi 1xi : ev) (8) where v is the alternative class of v, xl * is the automatically translated example from xl ; and A &gt;_ 0 is a constant that controls the weight of the 3 translated labeled data. 4 Experimental Setup 4.1 Data Sets and Preprocessing The following labeled datasets are used in our exper</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using machine translation. In Proceedings of EMNLP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8721" citStr="Berger et al., 1996" startWordPosition="1237" endWordPosition="1240">named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Berger et al., 1996; Ratnaparkhi, 1997; Smith, 2006). The models assign the conditional probability of the label given the observation as follows: where is a real-valued vector of feature weights and is a feature function that maps pairs to a nonnegative real-valued feature vector. Each feature has an associated parameter, , which is called its weight; and is the corresponding normalization factor. Maximum likelihood parameter estimation (training) for such a model, with a set of labeled examples , amounts to solving the following optimization problem: 3.1 Problem Definition Given two languages and , suppose we </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural corresponddence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP’06.</booktitle>
<contexts>
<context position="6316" citStr="Blitzer et al., 2006" startWordPosition="882" endWordPosition="885">on from one language (usually English) to other languages with few sentiment resources. Mihalcea et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ra</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural corresponddence learning. In Proceedings of EMNLP’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT’98.</booktitle>
<contexts>
<context position="4792" citStr="Blum and Mitchell, 1998" startWordPosition="669" endWordPosition="672">d of the language-specific labeled data together with the inferred sentiment labels of the parallel text. Although our approach should be applicable at the document-level and for additional sentiment tasks, we focus on sentence-level polarity classification in this work. We evaluate our approach for English and Chinese on two dataset combinations (see Section 4) and find that the proposed approach outperforms the monolingual baselines (i.e. maximum entropy and SVM classifiers) as well as two alternative methods for leveraging unlabeled data (transductive SVMs (Joachims, 1999b) and cotraining (Blum and Mitchell, 1998)). Accuracy is significantly improved for both languages, by 3.44%-8.12%. We furthermore find that improvements, albeit smaller, are obtained when the parallel data is replaced with a pseudo-parallel (i.e. automatically translated) corpus. To our knowledge, this is the first multilingual sentiment analysis study to focus on methods for simultaneously improving sentiment classification for a pair of languages based on unlabeled data rather than resource adaptation from one language to another. The rest of the paper is organized as follows. Section 2 introduces related work. In Section 3, the pr</context>
<context position="6448" citStr="Blum and Mitchell, 1998" startWordPosition="900" endWordPosition="903">rate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ratings. Unlike the methods described above, we focus on simultaneously improving the performance of sentiment classification in a pai</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Holistic sentiment analysis across languages: Multilingual supervised Latent Dirichlet Allocation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP’10.</booktitle>
<marker>Boyd-Graber, Resnik, 2010</marker>
<rawString>Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised Latent Dirichlet Allocation. In Proceedings of EMNLP’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI’07.</booktitle>
<contexts>
<context position="1623" citStr="Breck et al., 2007" startWordPosition="221" endWordPosition="224">sifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1 Introduction The field of sentiment analysis has quickly attracted the attention of researchers and practitioners alike (e.g. Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Breck et al., 2007; Pang and Lee, 2008). Indeed, sentiment analysis systems, which mine opinions from textual sources (e.g. news, blogs, and reviews), can be used in a wide variety of *The work was conducted when the first author was visiting Cornell University. applications, including interpreting product reviews, opinion retrieval and political polling. Not surprisingly, most methods for sentiment classification are supervised learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is dif</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of IJCAI’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Slav Petrov</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Learning better monolingual models with unannotated bilingual text.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL’10.</booktitle>
<contexts>
<context position="8057" citStr="Burkett et al. (2010)" startWordPosition="1141" endWordPosition="1144">hods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) mode</context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010. Learning better monolingual models with unannotated bilingual text. In Proceedings of CoNLL’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Two languages are better than one (for syntactic parsing).</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP’08.</booktitle>
<contexts>
<context position="8014" citStr="Burkett and Klein (2008)" startWordPosition="1133" endWordPosition="1136">g, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based stati</context>
</contexts>
<marker>Burkett, Klein, 2008</marker>
<rawString>David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of EMNLP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP’08.</booktitle>
<contexts>
<context position="16819" citStr="Choi and Cardie (2008)" startWordPosition="2558" endWordPosition="2561"> automatically translated example from xl ; and A &gt;_ 0 is a constant that controls the weight of the 3 translated labeled data. 4 Experimental Setup 4.1 Data Sets and Preprocessing The following labeled datasets are used in our experiments. MPQA (Labeled English Data): The MultiPerspective Question Answering (MPQA) corpus (Wiebe et al., 2005) consists of newswire documents manually annotated with phrase-level subjectivity information. We extract all sentences containing strong (i.e. intensity is medium or higher), sentiment-bearing (i.e. polarity is positive or negative) expressions following Choi and Cardie (2008). Sentences with both positive and negative strong expressions are then discarded, and the polarity of each remaining sentence is set to that of its sentiment-bearing expression(s). NTCIR-EN (Labeled English Data) and NTCIR-CH (Labeled Chinese Data): The NTCIR Opinion Analysis task (Seki et al., 2007; 2008) provides sentiment-labeled news data in Chinese, Japanese and English. Only those sentences with a polarity label (positive or negative) agreed to by at least two annotators are extracted. We use the Chinese data from NTCIR-6 4http://translate.google.com/ as our Chinese labeled data. Since </context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of EMNLP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>John Blitzer</author>
<author>Ming Zhou</author>
<author>Kam-Fai Wong</author>
</authors>
<title>Exploiting bilingual information to improve web search.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP‘09.</booktitle>
<contexts>
<context position="8100" citStr="Gao et al., 2009" startWordPosition="1147" endWordPosition="1150">0), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks</context>
</contexts>
<marker>Gao, Blitzer, Zhou, Wong, 2009</marker>
<rawString>Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 2009. Exploiting bilingual information to improve web search. In Proceedings of ACL/IJCNLP‘09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI’04.</booktitle>
<contexts>
<context position="1583" citStr="Hu and Liu, 2004" startWordPosition="213" endWordPosition="216">ns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1 Introduction The field of sentiment analysis has quickly attracted the attention of researchers and practitioners alike (e.g. Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Breck et al., 2007; Pang and Lee, 2008). Indeed, sentiment analysis systems, which mine opinions from textual sources (e.g. news, blogs, and reviews), can be used in a wide variety of *The work was conducted when the first author was visiting Cornell University. applications, including interpreting product reviews, opinion retrieval and political polling. Not surprisingly, most methods for sentiment classification are supervised learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Word sense disambiguation using a second language monolingual corpus,</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>563--596</pages>
<marker>Dagan, Itai, 1994</marker>
<rawString>Ido Dagan, and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus, Computational Linguistics, 20(4): 563-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical. In:</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods -Support Vector Learning,</booktitle>
<editor>B. Schölkopf, C. Burges, and A. Smola (ed.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4749" citStr="Joachims, 1999" startWordPosition="664" endWordPosition="665">s the regularized joint likelihood of the language-specific labeled data together with the inferred sentiment labels of the parallel text. Although our approach should be applicable at the document-level and for additional sentiment tasks, we focus on sentence-level polarity classification in this work. We evaluate our approach for English and Chinese on two dataset combinations (see Section 4) and find that the proposed approach outperforms the monolingual baselines (i.e. maximum entropy and SVM classifiers) as well as two alternative methods for leveraging unlabeled data (transductive SVMs (Joachims, 1999b) and cotraining (Blum and Mitchell, 1998)). Accuracy is significantly improved for both languages, by 3.44%-8.12%. We furthermore find that improvements, albeit smaller, are obtained when the parallel data is replaced with a pseudo-parallel (i.e. automatically translated) corpus. To our knowledge, this is the first multilingual sentiment analysis study to focus on methods for simultaneously improving sentiment classification for a pair of languages based on unlabeled data rather than resource adaptation from one language to another. The rest of the paper is organized as follows. Section 2 in</context>
<context position="7559" citStr="Joachims, 1999" startWordPosition="1065" endWordPosition="1066">cribed above, we focus on simultaneously improving the performance of sentiment classification in a pair of languages by developing a model that relies on sentimentlabeled data in each language as well as unlabeled parallel text for the language pair. Semi-supervised Learning. Another line of related work is semi-supervised learning, which combines labeled and unlabeled data to improve the performance of the task of interest (Zhu and Goldberg, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extr</context>
<context position="20128" citStr="Joachims, 1999" startWordPosition="3080" endWordPosition="3081">oth directions (i.e. Chinese to English and English to Chinese) with Giza++, take the log of the product of those two probabilities, and then divide it by the sum of lengths of the two sentences in each pair. 324 4.2 Baseline Methods In our experiments, the proposed joint model is compared with the following baseline methods. MaxEnt: This method learns a MaxEnt classifier for each language given the monolingual labeled data; the unlabeled data is not used. SVM: This method learns an SVM classifier for each language given the monolingual labeled data; the unlabeled data is not used. SVM-light (Joachims, 1999a) is used for all the SVM-related experiments. Monolingual TSVM (TSVM-M): This method learns two transductive SVM (TSVM) classifiers given the monolingual labeled data and the monolingual unlabeled data for each language. Bilingual TSVM (TSVM-B): This method learns one TSVM classifier given the labeled training data in two languages together with the unlabeled sentences by combining the two sentences in each unlabeled pair into one. We expect this method to perform better than TSVMM since the combined (bilingual) unlabeled sentences could be more helpful than the unlabeled monolingual sentenc</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999a. Making Large-Scale SVM Learning Practical. In: Advances in Kernel Methods -Support Vector Learning, B. Schölkopf, C. Burges, and A. Smola (ed.), MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proceedings of ICML’99.</booktitle>
<contexts>
<context position="4749" citStr="Joachims, 1999" startWordPosition="664" endWordPosition="665">s the regularized joint likelihood of the language-specific labeled data together with the inferred sentiment labels of the parallel text. Although our approach should be applicable at the document-level and for additional sentiment tasks, we focus on sentence-level polarity classification in this work. We evaluate our approach for English and Chinese on two dataset combinations (see Section 4) and find that the proposed approach outperforms the monolingual baselines (i.e. maximum entropy and SVM classifiers) as well as two alternative methods for leveraging unlabeled data (transductive SVMs (Joachims, 1999b) and cotraining (Blum and Mitchell, 1998)). Accuracy is significantly improved for both languages, by 3.44%-8.12%. We furthermore find that improvements, albeit smaller, are obtained when the parallel data is replaced with a pseudo-parallel (i.e. automatically translated) corpus. To our knowledge, this is the first multilingual sentiment analysis study to focus on methods for simultaneously improving sentiment classification for a pair of languages based on unlabeled data rather than resource adaptation from one language to another. The rest of the paper is organized as follows. Section 2 in</context>
<context position="7559" citStr="Joachims, 1999" startWordPosition="1065" endWordPosition="1066">cribed above, we focus on simultaneously improving the performance of sentiment classification in a pair of languages by developing a model that relies on sentimentlabeled data in each language as well as unlabeled parallel text for the language pair. Semi-supervised Learning. Another line of related work is semi-supervised learning, which combines labeled and unlabeled data to improve the performance of the task of interest (Zhu and Goldberg, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extr</context>
<context position="20128" citStr="Joachims, 1999" startWordPosition="3080" endWordPosition="3081">oth directions (i.e. Chinese to English and English to Chinese) with Giza++, take the log of the product of those two probabilities, and then divide it by the sum of lengths of the two sentences in each pair. 324 4.2 Baseline Methods In our experiments, the proposed joint model is compared with the following baseline methods. MaxEnt: This method learns a MaxEnt classifier for each language given the monolingual labeled data; the unlabeled data is not used. SVM: This method learns an SVM classifier for each language given the monolingual labeled data; the unlabeled data is not used. SVM-light (Joachims, 1999a) is used for all the SVM-related experiments. Monolingual TSVM (TSVM-M): This method learns two transductive SVM (TSVM) classifiers given the monolingual labeled data and the monolingual unlabeled data for each language. Bilingual TSVM (TSVM-B): This method learns one TSVM classifier given the labeled training data in two languages together with the unlabeled sentences by combining the two sentences in each unlabeled pair into one. We expect this method to perform better than TSVMM since the combined (bilingual) unlabeled sentences could be more helpful than the unlabeled monolingual sentenc</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999b. Transductive inference for text classification using support vector machines. In Proceedings of ICML’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL’06.</booktitle>
<contexts>
<context position="11781" citStr="Liang et al., 2006" startWordPosition="1732" endWordPosition="1735"> . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation 322 probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by considering the noise in parallel data, we get: where is the translation probability of the -th sentence pair in ;2 is the opposite of ; the first term models the probability that and have the same label; and the second term models the probability that they have different labels. By further considering the weight to ascribe to the unlabeled data vs. the labeled data (and the weight for the L2-norm regularization), we</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of NAACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Mathematical Programming,</journal>
<volume>45</volume>
<pages>503--528</pages>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, (45): 503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL’02.</booktitle>
<contexts>
<context position="14736" citStr="Malouf, 2002" startWordPosition="2223" endWordPosition="2224">ctively 1. Train two initial monolingual models Train and initialize and on the labeled data 2. Jointly optimize two monolingual models for to do // T: number of iterations E-Step: Compute for each example in , and 1 based on and ; Compute the expectation of the log likelihood with respect to ; M-Step: Find and by maximizing the regularized joint log likelihood; Convergence: If the increase of the joint log likelihood is sufficiently small, break; end for 3. Output as s, and as In the M-step, we can optimize the regularized joint log likelihood using any gradient-based optimization technique (Malouf, 2002). The gradient for Equation 3 based on Equation 4 is shown in Appendix A; those for Equations 5 and 6 can be derived similarly. In our experiments, we use the L-BFGS algorithm (Liu et al., 1989) and run EM until the change in regularized joint log likelihood is less than 1e-5 or we reach 100 iterations.3 3Since the EM-based algorithm may find a local maximum of the objective function, the initialization of the parameters is important. Our experiments show that an effective maximum can usually be found by initializing the parameters with those learned from the labeled data; performance would be</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of CoNLL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carmen Banea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning multilingual subjective language via cross-lingual projections.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07.</booktitle>
<contexts>
<context position="2656" citStr="Mihalcea et al. (2007)" startWordPosition="363" endWordPosition="366">sed learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is av</context>
<context position="5805" citStr="Mihalcea et al. (2007)" startWordPosition="816" endWordPosition="819"> a pair of languages based on unlabeled data rather than resource adaptation from one language to another. The rest of the paper is organized as follows. Section 2 introduces related work. In Section 3, the proposed joint model is described. Sections 4 and 5, respectively, provide the experimental setup and results; the conclusion (Section 6) follows. 2 Related Work Multilingual Sentiment Analysis. There is a growing body of work on multilingual sentiment analysis. Most approaches focus on resource adaptation from one language (usually English) to other languages with few sentiment resources. Mihalcea et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which</context>
</contexts>
<marker>Mihalcea, Banea, Wiebe, 2007</marker>
<rawString>Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of ACL’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos S Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>477--504</pages>
<contexts>
<context position="11358" citStr="Munteanu and Marcu, 2005" startWordPosition="1662" endWordPosition="1665">nd side is the likelihood of labeled data for both and ; and the second term is the likelihood of the unlabeled parallel data . If we assume that parallel sentences are perfect translations, the two sentences in each pair should have the same polarity label, which gives us: where is the unobserved class label for the -th instance in the unlabeled data. This probability directly models the sentiment label agreement between and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation 322 probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, </context>
<context position="18249" citStr="Munteanu and Marcu, 2005" startWordPosition="2777" endWordPosition="2780">ter (Tseng et al., 2005). The number of sentences in each of these datasets is shown in Table 1. In our experiments, we evaluate two settings of the data: (1) MPQA+NTCIR-CH, and (2) NTCIR-EN+NTCIRCH. In each setting, the English labeled data constitutes Dl and the Chinese labeled data, D2. MPQA NTCIR-EN NTCIR-CH Positive 1,471 (30%) 528 (30%) 2,378 (55%) Negative 3,487 (70%) 1,209 (70%) 1,916 (45%) Total 4,958 1,737 4,294 Table 1: Sentence Counts for the Labeled Data Unlabeled Parallel Text and its Preprocessing. For the unlabeled parallel text, we use the ISI Chinese-English parallel corpus (Munteanu and Marcu, 2005), which was extracted automatically from news articles published by Xinhua News Agency in the Chinese Gigaword (2nd Edition) and English Gigaword (2nd Edition) collections. Because sentence pairs in the ISI corpus are quite noisy, we rely on Giza++ (Och and Ney, 2003) to obtain a new translation probability for each sentence pair, and select the 100,000 pairs with the highest translation probabilities.5 We also try to remove neutral sentences from the parallel data since they can introduce noise into our model, which deals only with positive and negative examples. To do this, we train a single</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos S. Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4): 477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL/HLT ‘10.</booktitle>
<contexts>
<context position="2903" citStr="Nakagawa et al. (2010)" startWordPosition="400" endWordPosition="403">ly for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to simultaneously improve sentiment classification for both languages. Given the labeled data in each language, we propose an approach that exploits an unlabeled parallel corpu</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In Proceedings of NAACL/HLT ‘10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew K Mccallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>39</volume>
<issue>2</issue>
<pages>103--134</pages>
<contexts>
<context position="7485" citStr="Nigam et al., 2000" startWordPosition="1054" endWordPosition="1057">s, and employs them to better predict sentiment ratings. Unlike the methods described above, we focus on simultaneously improving the performance of sentiment classification in a pair of languages by developing a model that relies on sentimentlabeled data in each language as well as unlabeled parallel text for the language pair. Semi-supervised Learning. Another line of related work is semi-supervised learning, which combines labeled and unlabeled data to improve the performance of the task of interest (Zhu and Goldberg, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Ga</context>
</contexts>
<marker>Nigam, Mccallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2): 103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11731" citStr="Och and Ney, 2003" startWordPosition="1723" endWordPosition="1726"> models the sentiment label agreement between and . However, there could be considerable noise in real-world parallel data, i.e. the sentence pairs may be noisily parallel (or even comparable) instead of fully parallel (Munteanu and Marcu, 2005). In such noisy cases, the labels (positive or negative) could be different for the two monolingual sentences in a sentence pair. Although we do not know the exact probability that a sentence pair exhibits the same label, we can approximate it using their translation 322 probabilities, which can be computed using word alignment toolkits such as Giza++ (Och and Ney, 2003) or the Berkeley word aligner (Liang et al., 2006). The intuition here is that if the translation probability of two sentences is high, the probability that they have the same sentiment label should be high as well. Therefore, by considering the noise in parallel data, we get: where is the translation probability of the -th sentence pair in ;2 is the opposite of ; the first term models the probability that and have the same label; and the second term models the probability that they have different labels. By further considering the weight to ascribe to the unlabeled data vs. the labeled data (</context>
<context position="18517" citStr="Och and Ney, 2003" startWordPosition="2819" endWordPosition="2822">beled data, D2. MPQA NTCIR-EN NTCIR-CH Positive 1,471 (30%) 528 (30%) 2,378 (55%) Negative 3,487 (70%) 1,209 (70%) 1,916 (45%) Total 4,958 1,737 4,294 Table 1: Sentence Counts for the Labeled Data Unlabeled Parallel Text and its Preprocessing. For the unlabeled parallel text, we use the ISI Chinese-English parallel corpus (Munteanu and Marcu, 2005), which was extracted automatically from news articles published by Xinhua News Agency in the Chinese Gigaword (2nd Edition) and English Gigaword (2nd Edition) collections. Because sentence pairs in the ISI corpus are quite noisy, we rely on Giza++ (Och and Ney, 2003) to obtain a new translation probability for each sentence pair, and select the 100,000 pairs with the highest translation probabilities.5 We also try to remove neutral sentences from the parallel data since they can introduce noise into our model, which deals only with positive and negative examples. To do this, we train a single classifier from the combined Chinese and English labeled data for each data setting above by concatenating the original English and Chinese feature sets. We then classify each unlabeled sentence pair by combining the two sentences in each pair into one. We choose the</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<publisher>Now Publishers.</publisher>
<contexts>
<context position="1644" citStr="Pang and Lee, 2008" startWordPosition="225" endWordPosition="228">guage. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1 Introduction The field of sentiment analysis has quickly attracted the attention of researchers and practitioners alike (e.g. Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Breck et al., 2007; Pang and Lee, 2008). Indeed, sentiment analysis systems, which mine opinions from textual sources (e.g. news, blogs, and reviews), can be used in a wide variety of *The work was conducted when the first author was visiting Cornell University. applications, including interpreting product reviews, opinion retrieval and political polling. Not surprisingly, most methods for sentiment classification are supervised learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to </context>
<context position="3102" citStr="Pang and Lee, 2008" startWordPosition="429" endWordPosition="432">cally English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to simultaneously improve sentiment classification for both languages. Given the labeled data in each language, we propose an approach that exploits an unlabeled parallel corpus with the following 320 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 320–330, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computation</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval, Now Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP’02.</booktitle>
<contexts>
<context position="1551" citStr="Pang et al., 2002" startWordPosition="207" endWordPosition="210">present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1 Introduction The field of sentiment analysis has quickly attracted the attention of researchers and practitioners alike (e.g. Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Breck et al., 2007; Pang and Lee, 2008). Indeed, sentiment analysis systems, which mine opinions from textual sources (e.g. news, blogs, and reviews), can be used in a wide variety of *The work was conducted when the first author was visiting Cornell University. applications, including interpreting product reviews, opinion retrieval and political polling. Not surprisingly, most methods for sentiment classification are supervised learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-l</context>
<context position="22108" citStr="Pang et al. (2002)" startWordPosition="3381" endWordPosition="3384">ts, the methods are tested in the two data settings with the corresponding unlabeled parallel corpus as mentioned in Section 4.6 We use 6 The results reported in this section employ Equation 4. Preliminary experiments showed that Equation 5 does not significantly improve the performance in our case, which is reasonable since we choose only sentence pairs with the highest translation probabilities to be our unlabeled data (see Section 4.1). 5-fold cross-validation and report average accuracy (also MicroF1 in this case) and MacroF1 scores. Unigrams are used as binary features for all models, as Pang et al. (2002) showed that binary features perform better than frequency features for sentiment classification. The weights for unlabeled data and regularization, and , are set to 1 unless otherwise stated. Later, we will show that the proposed approach performs well with a wide range of parameter values.7 5.1 Method Comparison We first compare the proposed joint model (Joint) with the baselines in Table 2. As seen from the table, the proposed approach outperforms all five baseline methods in terms of both accuracy and MacroF1 for both English and Chinese and in both of the data settings. 8 By making use of</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Prettenhofer</author>
<author>Benno Stein</author>
</authors>
<title>Crosslanguage text classification using structural correspondence learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL’10.</booktitle>
<contexts>
<context position="2732" citStr="Prettenhofer and Stein (2010)" startWordPosition="375" endWordPosition="378">he appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to si</context>
<context position="6157" citStr="Prettenhofer and Stein (2010)" startWordPosition="862" endWordPosition="865">follows. 2 Related Work Multilingual Sentiment Analysis. There is a growing body of work on multilingual sentiment analysis. Most approaches focus on resource adaptation from one language (usually English) to other languages with few sentiment resources. Mihalcea et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervise</context>
</contexts>
<marker>Prettenhofer, Stein, 2010</marker>
<rawString>Peter Prettenhofer and Benno Stein. 2010. Crosslanguage text classification using structural correspondence learning. In Proceedings of ACL’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A simple introduction to maximum entropy models for natural language processing.</title>
<date>1997</date>
<tech>Technical Report 97-08,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8740" citStr="Ratnaparkhi, 1997" startWordPosition="1241" endWordPosition="1242">n (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Berger et al., 1996; Ratnaparkhi, 1997; Smith, 2006). The models assign the conditional probability of the label given the observation as follows: where is a real-valued vector of feature weights and is a feature function that maps pairs to a nonnegative real-valued feature vector. Each feature has an associated parameter, , which is called its weight; and is the corresponding normalization factor. Maximum likelihood parameter estimation (training) for such a model, with a set of labeled examples , amounts to solving the following optimization problem: 3.1 Problem Definition Given two languages and , suppose we have two distinct (</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A simple introduction to maximum entropy models for natural language processing. Technical Report 97-08, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia M Schulz</author>
<author>Christa Womser-Hacker</author>
<author>Thomas Mandl</author>
</authors>
<title>Multilingual corpus development for opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC’10.</booktitle>
<contexts>
<context position="2925" citStr="Schulz et al. (2010)" startWordPosition="404" endWordPosition="407">er consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under study, and (2) investigate methods to simultaneously improve sentiment classification for both languages. Given the labeled data in each language, we propose an approach that exploits an unlabeled parallel corpus with the following 3</context>
</contexts>
<marker>Schulz, Womser-Hacker, Mandl, 2010</marker>
<rawString>Julia M. Schulz, Christa Womser-Hacker, and Thomas Mandl. 2010. Multilingual corpus development for opinion mining. In Proceedings of LREC’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-His Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of multilingual opinion analysis task at NTCIR-7.</title>
<date>2008</date>
<booktitle>In Proceedings of the NTCIR-7 Workshop.</booktitle>
<marker>Seki, Evans, Ku, Le Sun, Kando, 2008</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-His Chen, and Noriko Kando. 2008. Overview of multilingual opinion analysis task at NTCIR-7. In Proceedings of the NTCIR-7 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David K Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-His Chen Le Sun</author>
<author>Noriko Kando</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Overview of opinion analysis pilot task at NTCIR-6.</title>
<date>2007</date>
<booktitle>In Proceedings of the NTCIR-6 Workshop.</booktitle>
<marker>Seki, Evans, Ku, Le Sun, Kando, Lin, 2007</marker>
<rawString>Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun, Hsin-His Chen, Noriko Kando, and Chin-Yew Lin. 2007. Overview of opinion analysis pilot task at NTCIR-6. In Proceedings of the NTCIR-6 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Partha Niyogi</author>
<author>Mikhail Belkin</author>
</authors>
<title>A co-regularization approach to semisupervised learning with multiple views.</title>
<date>2005</date>
<booktitle>In Proceedings of ICML’05.</booktitle>
<contexts>
<context position="7608" citStr="Sindhwani et al., 2005" startWordPosition="1069" endWordPosition="1072">improving the performance of sentiment classification in a pair of languages by developing a model that relies on sentimentlabeled data in each language as well as unlabeled parallel text for the language pair. Semi-supervised Learning. Another line of related work is semi-supervised learning, which combines labeled and unlabeled data to improve the performance of the task of interest (Zhu and Goldberg, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classifica</context>
</contexts>
<marker>Sindhwani, Niyogi, Belkin, 2005</marker>
<rawString>Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 2005. A co-regularization approach to semisupervised learning with multiple views. In Proceedings of ICML’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Novel estimation methods for unsupervised discovery of latent structure in natural language text.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Johns Hopkins University.</institution>
<contexts>
<context position="8754" citStr="Smith, 2006" startWordPosition="1243" endWordPosition="1244">2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Berger et al., 1996; Ratnaparkhi, 1997; Smith, 2006). The models assign the conditional probability of the label given the observation as follows: where is a real-valued vector of feature weights and is a feature function that maps pairs to a nonnegative real-valued feature vector. Each feature has an associated parameter, , which is called its weight; and is the corresponding normalization factor. Maximum likelihood parameter estimation (training) for such a model, with a set of labeled examples , amounts to solving the following optimization problem: 3.1 Problem Definition Given two languages and , suppose we have two distinct (i.e. not paral</context>
</contexts>
<marker>Smith, 2006</marker>
<rawString>Noah A. Smith. 2006. Novel estimation methods for unsupervised discovery of latent structure in natural language text. Ph.D. thesis, Department of Computer Science, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In Proeedings of the 4th SIGHAN Workshop.</booktitle>
<contexts>
<context position="17648" citStr="Tseng et al., 2005" startWordPosition="2681" endWordPosition="2684">ta) and NTCIR-CH (Labeled Chinese Data): The NTCIR Opinion Analysis task (Seki et al., 2007; 2008) provides sentiment-labeled news data in Chinese, Japanese and English. Only those sentences with a polarity label (positive or negative) agreed to by at least two annotators are extracted. We use the Chinese data from NTCIR-6 4http://translate.google.com/ as our Chinese labeled data. Since far fewer sentences in the English data pass the annotator agreement filter, we combine the English data from NTCIR-6 and NTCIR-7. The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al., 2005). The number of sentences in each of these datasets is shown in Table 1. In our experiments, we evaluate two settings of the data: (1) MPQA+NTCIR-CH, and (2) NTCIR-EN+NTCIRCH. In each setting, the English labeled data constitutes Dl and the Chinese labeled data, D2. MPQA NTCIR-EN NTCIR-CH Positive 1,471 (30%) 528 (30%) 2,378 (55%) Negative 3,487 (70%) 1,209 (70%) 1,916 (45%) Total 4,958 1,737 4,294 Table 1: Sentence Counts for the Labeled Data Unlabeled Parallel Text and its Preprocessing. For the unlabeled parallel text, we use the ISI Chinese-English parallel corpus (Munteanu and Marcu, 2005</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky and Christopher Manning. 2005. A conditional random field word segmenter. In Proeedings of the 4th SIGHAN Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews,</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02.</booktitle>
<contexts>
<context position="1565" citStr="Turney, 2002" startWordPosition="211" endWordPosition="212">t jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1 Introduction The field of sentiment analysis has quickly attracted the attention of researchers and practitioners alike (e.g. Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Breck et al., 2007; Pang and Lee, 2008). Indeed, sentiment analysis systems, which mine opinions from textual sources (e.g. news, blogs, and reviews), can be used in a wide variety of *The work was conducted when the first author was visiting Cornell University. applications, including interpreting product reviews, opinion retrieval and political polling. Not surprisingly, most methods for sentiment classification are supervised learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or senten</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews, In Proceedings of ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP’08.</booktitle>
<contexts>
<context position="2694" citStr="Wan (2008" startWordPosition="372" endWordPosition="373">a annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarity). This data is difficult and costly to obtain, and must be acquired separately for each language under consideration. Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources (e.g. lexicons) from resourcerich languages (typically English) to other languages, with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages (e.g. Mihalcea et al. (2007); Banea et al. (2008; 2010); Wan (2008; 2009); Prettenhofer and Stein (2010)). In recent years, however, sentiment-labeled data is gradually becoming available for languages other than English (e.g. Seki et al. (2007; 2008); Nakagawa et al. (2010); Schulz et al. (2010)). In addition, there is still much room for improvement in existing monolingual (including English) sentiment classifiers, especially at the sentence level (Pang and Lee, 2008). This paper tackles the task of bilingual sentiment analysis. In contrast to previous work, we (1) assume that some amount of sentimentlabeled data is available for the language pair under st</context>
</contexts>
<marker>Wan, 2008</marker>
<rawString>Xiaojun Wan. 2008. Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis. In Proceedings of EMNLP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP’09.</booktitle>
<contexts>
<context position="6398" citStr="Wan (2009)" startWordPosition="895" endWordPosition="896">cea et al. (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. Banea et al. (2008; 2010) instead automatically translate the English resources using automatic machine translation engines for subjectivity classification. Prettenhofer and Stein (2010) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al., 2006). Approaches that do not explicitly involve resource adaptation include Wan (2009), which uses co-training (Blum and Mitchell, 1998) with English vs. Chinese features comprising the two independent ―views‖ to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification. Another notable approach is the work of BoydGraber and Resnik (2010), which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ratings. Unlike the methods described above, we focus on simultaneously improving th</context>
<context position="15848" citStr="Wan, 2009" startWordPosition="2408" endWordPosition="2409">ound by initializing the parameters with those learned from the labeled data; performance would be much worse if we initialize all the parameters to 0 or 1. Output: 323 3.4 Pseudo-Parallel Labeled and Unlabeled Data We also consider the case where a parallel corpus is not available: to obtain a pseudo-parallel corpus U (i.e. sentences in one language with their corresponding automatic translations), we use an automatic machine translation system (e.g. Google machine translation 4 ) to translate unlabeled indomain data from Ll to L2 or vice versa. Since previous work (Banea et al., 2008; 2010; Wan, 2009) has shown that it could be useful to automatically translate the labeled data from the source language into the target language, we can further incorporate such translated labeled data into the joint model by adding the following component into Equation 6: 2 A3 Zv=-_r Zi=-_r logp(Yi 1xi : ev) (8) where v is the alternative class of v, xl * is the automatically translated example from xl ; and A &gt;_ 0 is a constant that controls the weight of the 3 translated labeled data. 4 Experimental Setup 4.1 Data Sets and Preprocessing The following labeled datasets are used in our experiments. MPQA (Labe</context>
<context position="20895" citStr="Wan (2009)" startWordPosition="3192" endWordPosition="3193">abeled data and the monolingual unlabeled data for each language. Bilingual TSVM (TSVM-B): This method learns one TSVM classifier given the labeled training data in two languages together with the unlabeled sentences by combining the two sentences in each unlabeled pair into one. We expect this method to perform better than TSVMM since the combined (bilingual) unlabeled sentences could be more helpful than the unlabeled monolingual sentences. Co-Training with SVMs (Co-SVM): This method applies SVM-based co-training given both the labeled training data and the unlabeled parallel data following Wan (2009). First, two monolingual SVM classifiers are built based on only the corresponding labeled data, and then they are bootstrapped by adding the most confident predicted examples from the unlabeled data into the training set. We run bootstrapping for 100 iterations. In each iteration, we select the most confidently predicted 50 positive and 50 negative sentences from each of the two classifiers, and take the union of the resulting 200 sentence pairs as the newly labeled training data. (Examples with conflicting labels within the pair are not included.) 5 Results and Analysis In our experiments, t</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of ACL/AFNLP’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<pages>165--210</pages>
<contexts>
<context position="1603" citStr="Wiebe et al., 2005" startWordPosition="217" endWordPosition="220">ngual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8.12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1 Introduction The field of sentiment analysis has quickly attracted the attention of researchers and practitioners alike (e.g. Pang et al., 2002; Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Breck et al., 2007; Pang and Lee, 2008). Indeed, sentiment analysis systems, which mine opinions from textual sources (e.g. news, blogs, and reviews), can be used in a wide variety of *The work was conducted when the first author was visiting Cornell University. applications, including interpreting product reviews, opinion retrieval and political polling. Not surprisingly, most methods for sentiment classification are supervised learning techniques, which require training data annotated with the appropriate sentiment labels (e.g. document-level or sentence-level positive vs. negative polarit</context>
<context position="16541" citStr="Wiebe et al., 2005" startWordPosition="2523" endWordPosition="2526">data from the source language into the target language, we can further incorporate such translated labeled data into the joint model by adding the following component into Equation 6: 2 A3 Zv=-_r Zi=-_r logp(Yi 1xi : ev) (8) where v is the alternative class of v, xl * is the automatically translated example from xl ; and A &gt;_ 0 is a constant that controls the weight of the 3 translated labeled data. 4 Experimental Setup 4.1 Data Sets and Preprocessing The following labeled datasets are used in our experiments. MPQA (Labeled English Data): The MultiPerspective Question Answering (MPQA) corpus (Wiebe et al., 2005) consists of newswire documents manually annotated with phrase-level subjectivity information. We extract all sentences containing strong (i.e. intensity is medium or higher), sentiment-bearing (i.e. polarity is positive or negative) expressions following Choi and Cardie (2008). Sentences with both positive and negative strong expressions are then discarded, and the polarity of each remaining sentence is set to that of its sentiment-bearing expression(s). NTCIR-EN (Labeled English Data) and NTCIR-CH (Labeled Chinese Data): The NTCIR Opinion Analysis task (Seki et al., 2007; 2008) provides sent</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2- 3): 165-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duo Zhang</author>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Cross-lingual latent topic extraction,</title>
<date>2010</date>
<booktitle>In Proceedings of ACL’10.</booktitle>
<contexts>
<context position="8191" citStr="Zhang et al., 2010" startWordPosition="1161" endWordPosition="1164">gularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximum entropy (MaxEnt) models1 have been widely used in many NLP tasks (Berger et al., 1996; Ratnaparkhi, 1997; Smith, 2006). The models assign the conditional p</context>
</contexts>
<marker>Zhang, Mei, Zhai, 2010</marker>
<rawString>Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010. Cross-lingual latent topic extraction, In Proceedings of ACL’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Yan Song</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Cross language dependency parsing using a bilingual lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/IJCNLP’09.</booktitle>
<contexts>
<context position="8034" citStr="Zhao et al. (2009)" startWordPosition="1137" endWordPosition="1140">r semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett and Klein (2008); Zhao et al. (2009); Burkett et al. (2010)), information retrieval (Gao et al., 2009), named entity detection (Burkett et al., 2010); topic extraction (e.g. Zhang et al., 2010), text classification (e.g. Amini et al., 2010), and hyponym-relation acquisition (e.g. Oh et al., 2009). 321 In these cases, multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels. Our work shares a similar motivation. 3 A Joint Model with Unlabeled Parallel Text We propose a maximum entropy-based statistical model. Maximu</context>
</contexts>
<marker>Zhao, Song, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 2009. Cross language dependency parsing using a bilingual lexicon. In Proceedings of ACL/IJCNLP’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Andrew B Goldberg</author>
</authors>
<title>Introduction to Semi-Supervised Learning.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="7398" citStr="Zhu and Goldberg, 2009" startWordPosition="1039" endWordPosition="1042">ent Dirichlet allocation --- that jointly models topics that are consistent across languages, and employs them to better predict sentiment ratings. Unlike the methods described above, we focus on simultaneously improving the performance of sentiment classification in a pair of languages by developing a model that relies on sentimentlabeled data in each language as well as unlabeled parallel text for the language pair. Semi-supervised Learning. Another line of related work is semi-supervised learning, which combines labeled and unlabeled data to improve the performance of the task of interest (Zhu and Goldberg, 2009). Among the popular semisupervised methods (e.g. EM on Naïve Bayes (Nigam et al., 2000), co-training (Blum and Mitchell, 1998), transductive SVMs (Joachims, 1999b), and co-regularization (Sindhwani et al., 2005; Amini et al., 2010)), our approach employs the EM algorithm, extending it to the bilingual case based on maximum entropy. We compare to co-training and transductive SVMs in Section 5. Multilingual NLP for Other Tasks. Finally, there exists related work using bilingual resources to help other NLP tasks, such as word sense disambiguation (e.g. Ido and Itai (1994)), parsing (e.g. Burkett </context>
</contexts>
<marker>Zhu, Goldberg, 2009</marker>
<rawString>Xiaojin Zhu and Andrew B. Goldberg. 2009. Introduction to Semi-Supervised Learning. Morgan &amp; Claypool Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>