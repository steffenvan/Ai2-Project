<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001922">
<title confidence="0.999204">
Extracting Key Phrases to Disambiguate
Personal Name Queries in Web Search
</title>
<author confidence="0.986341">
Danushka Bollegala Yutaka Matsuo * Mitsuru Ishizuka
</author>
<affiliation confidence="0.993224">
Graduate School of Information Science and Technology
The University of Tokyo
</affiliation>
<address confidence="0.844507">
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
</address>
<email confidence="0.991268666666667">
danushka@mi.ci.i.u-tokyo.ac.jp
y.matsuo@aist.go.jp
ishizuka@i.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.993792" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890421052632">
Assume that you are looking for informa-
tion about a particular person. A search
engine returns many pages for that per-
son’s name. Some of these pages may
be on other people with the same name.
One method to reduce the ambiguity in the
query and filter out the irrelevant pages, is
by adding a phrase that uniquely identi-
fies the person we are interested in from
his/her namesakes. We propose an un-
supervised algorithm that extracts such
phrases from the Web. We represent each
document by a term-entity model and clus-
ter the documents using a contextual sim-
ilarity metric. We evaluate the algorithm
on a dataset of ambiguous names. Our
method outperforms baselines, achieving
over 80% accuracy and significantly re-
duces the ambiguity in a web search task.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9960815">
The Internet has grown into a collection of bil-
lions of web pages. Web search engines are im-
portant interfaces to this vast information. We
send simple text queries to search engines and re-
trieve web pages. However, due to the ambigu-
ities in the queries, a search engine may return
a lot of irrelevant pages. In the case of personal
name queries, we may receive web pages for other
people with the same name (namesakes). For ex-
ample, if we search Google 1 for Jim Clark, even
among the top 100 results we find at least eight
different Jim Clarks. The two popular namesakes;
</bodyText>
<note confidence="0.5163445">
National Institute of Advanced Industrial Science and
Technology
</note>
<footnote confidence="0.924592">
1www.google.com
</footnote>
<bodyText confidence="0.99895445">
Jim Clark the Formula one world champion (46
pages), and Jim Clark the founder of Netscape (26
pages), cover the majority of the pages. What if
we are interested only in the Formula one world
champion and want to filter out the pages for the
other Jim Clarks? One solution is to modify our
query by including a phrase such as Formula one
or racing driver with the name, Jim Clark.
This paper presents an automatic method to ex-
tract such phrases from the Web. We follow a
three-stage approach. In the first stage we rep-
resent each document containing the ambiguous
name by a term-entity model, as described in sec-
tion 5.2. We define a contextual similarity metric
based on snippets returned by a search engine, to
calculate the similarity between term-entity mod-
els. In the second stage, we cluster the documents
using the similarity metric. In the final stage, we
select key phrases from the clusters that uniquely
identify each namesake.
</bodyText>
<sectionHeader confidence="0.995475" genericHeader="introduction">
2 Applications
</sectionHeader>
<bodyText confidence="0.999351375">
Two tasks that can readily benefit from automat-
ically extracted key phrases to disambiguate per-
sonal names are query suggestion and social net-
work extraction. In query suggestion (Gauch and
Smith, 1991), the search engine returns a set of
phrases to the user alongside with the search re-
sults. The user can then modify the original query
using these phrases to narrow down the search.
Query suggestion helps the users to easily navigate
through the result set. For personal name queries,
the key phrases extracted by our algorithm can be
used as suggestions to reduce the ambiguity and
narrow down the search on a particular namesake.
Social networking services (SNSs) have been
given much attention on the Web recently. As
a kind of online applications, SNSs can be used
</bodyText>
<page confidence="0.990664">
17
</page>
<note confidence="0.6951395">
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17–24,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99964">
to register and share personal information among
friends and communities. There have been recent
attempts to extract social networks using the infor-
mation available on the Web 2(Mika, 2004; Mat-
suo et al., 2006). In both Matsuo’s (2006) and
Mika’s (2004) algorithms, each person is repre-
sented by a node in the social network and the
strength of the relationship between two people
is represented by the length of the edge between
the corresponding two nodes. As a measure of the
strength of the relationship between two people A
and B, these algorithms use the number of hits ob-
tained for the query A AND B. However, this ap-
proach fails when A or B has namesakes because
the number of hits in these cases includes the hits
for the namesakes. To overcome this problem, we
could include phrases in the query that uniquely
identify A and B from their namesakes.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999473379310345">
Person name disambiguation can be seen as
a special case of word sense disambiguation
(WSD) (Schutze, 1998; McCarthy et al., 2004)
problem which has been studied extensively in
Natural Language Understanding. However, there
are several fundamental differences between WSD
and person name disambiguation. WSD typically
concentrates on disambiguating between 2-4 pos-
sible meanings of the word, all of which are a
priori known. However, in person name disam-
biguation in Web, the number of different name-
sakes can be much larger and unknown. From a
resource point of view, WSD utilizes sense tagged
dictionaries such as WordNet, whereas no dictio-
nary can provide information regarding different
namesakes for a particular name.
The problem of person name disambiguation
has been addressed in the domain of research pa-
per citations (Han et al., 2005), with various super-
vised methods proposed for its solution. However,
citations have a fixed format compared to free text
on the Web. Fields such as co-authors, title, jour-
nal name, conference name, year of publication
can be easily extracted from a citation and provide
vital information to the disambiguation process.
Research on multi-document person name res-
olution (Bagga and Baldwin, 1998; Mann and
Yarowsky, 2003; Fleischman and Hovy, 2004) fo-
cuses on the related problem of determining if
</bodyText>
<footnote confidence="0.9785175">
2http://flink.sematicweb.org/. The system won the 1st
place at the Semantic Web Challenge in ISWC2004.
</footnote>
<bodyText confidence="0.999971333333334">
two instances with the same name and from dif-
ferent documents refer to the same individual.
Bagga and Baldwin (1998) first perform within-
document coreference resolution to form coref-
erence chains for each entity in each document.
They then use the text surrounding each reference
chain to create summaries about each entity in
each document. These summaries are then con-
verted to a bag of words feature vector and are
clustered using standard vector space model of-
ten employed in IR. The use of simplistic bag of
words clustering is an inherently limiting aspect of
their methodology. On the other hand, Mann and
Yarowsky (2003) proposes a richer document rep-
resentation involving automatically extracted fea-
tures. However, their clustering technique can be
basically used only for separating two people with
the same name. Fleischman and Hovy (2004) con-
structs a maximum entropy classifier to learn dis-
tances between documents that are then clustered.
Their method requires a large training set.
Pedersen et al. (2005) propose an unsupervised
approach to resolve name ambiguity by represent-
ing the context of an ambiguous name using sec-
ond order context vectors derived using singular
value decomposition (SVD) on a co-occurrence
matrix. They agglomeratively cluster the vec-
tors using cosine similarity. They evaluate their
method only on a conflated dataset of pseudo-
names, which begs the question of how well such
a technique would fair on a more real-world chal-
lenge. Li et al. (2005) propose two approaches to
disambiguate entities in a set of documents: a su-
pervisedly trained pairwise classifier and an unsu-
pervised generative model. However, they do not
evaluate the effectiveness of their method in Web
search.
Bekkerman and McCallum (2005) present two
unsupervised methods for finding web pages re-
ferring to a particular person: one based on
link structure and another using Agglomera-
tive/Conglomerative Double Clustering (A/CDC).
Their scenario focuses on simultaneously disam-
biguating an existing social network of people,
who are closely related. Therefore, their method
cannot be applied to disambiguate an individual
whose social network (for example, friends, col-
leagues) is not known. Guha and Grag (2004)
present a re-ranking algorithm to disambiguate
people. The algorithm requires a user to select one
of the returned pages as a starting point. Then,
</bodyText>
<page confidence="0.999767">
18
</page>
<tableCaption confidence="0.999782">
Table 1: Data set for experiments
</tableCaption>
<table confidence="0.9482904">
Collection No of namesakes
person-X 4
Michael Jackson 3
Jim Clark 8
William Cohen 10
</table>
<bodyText confidence="0.9998711">
through comparing the person descriptions, the al-
gorithm re-ranks the entire search results in such
a way that pages referring to the same person de-
scribed in the user-selected page are ranked higher.
A user needs to browse the documents in order to
find which matches the user’s intended referent,
which puts an extra burden on the user.
None of the above mentioned works attempt to
extract key phrases to disambiguate person name
queries, a contrasting feature in our work.
</bodyText>
<sectionHeader confidence="0.989467" genericHeader="method">
4 Data Set
</sectionHeader>
<bodyText confidence="0.999985736842105">
We select three ambiguous names (Micheal Jack-
son, William Cohen and Jim Clark) that appear in
previous work in name resolution. For each name
we query Google with the name and download top
100 pages. We manually classify each page ac-
cording to the namesakes discussed in the page.
We ignore pages which we could not decide the
namesake from the content. We also remove pages
with images that do not contain any text. No pages
were found where more than one namesakes of a
name appear. For automated pseudo-name evalua-
tion purposes, we select four names (Bill Clinton,
Bill Gates, Tom Cruise and Tiger Woods) for con-
flation, who we presumed had one vastly predom-
inant sense. We download 100 pages from Google
for each person. We replace the name of the per-
son by ”person-X” in the collection, thereby intro-
ducing ambiguity. The structure of our dataset is
shown in Table 1.
</bodyText>
<sectionHeader confidence="0.996223" genericHeader="method">
5 Method
</sectionHeader>
<subsectionHeader confidence="0.999748">
5.1 Problem Statement
</subsectionHeader>
<bodyText confidence="0.999970583333333">
Given a collection of documents relevant to an am-
biguous name, we assume that each document in
the collection contains exactly one namesake of
the ambiguous name. This is a fair assumption
considering the fact that although namesakes share
a common name, they specializes in different
fields and have different Web appearances. More-
over, the one-to-one association between docu-
ments and people formed by this assumption, let
us model the person name disambiguation prob-
lem as a one of hard-clustering of documents.
The outline of our method is as following;
Given a set of documents representing a group of
people with the same name, we represent each
document in the collection using a Term-Entity
model (section 5.2). We define a contextual sim-
ilarity metric (section 5.4) and then cluster (sec-
tion 5.5) the term-entity models using the contex-
tual similarity between them. Each cluster is con-
sidered to be representing a different namesake.
Finally, key phrases that uniquely identify each
namesake are selected from the clusters. We per-
form experiments at each step of our method to
evaluate its performance.
</bodyText>
<subsectionHeader confidence="0.84733">
5.2 Term-Entity Model
</subsectionHeader>
<bodyText confidence="0.9983588">
The first step toward disambiguating a personal
name is to identify the discriminating features of
one person from another. In this paper we propose
Term-Entity models to represent a person in a doc-
ument.
</bodyText>
<construct confidence="0.8830344">
Definition. A term-entity model T (A), represent-
ing a person A in a document D, is a boolean
expression of n literals a1, a2, ... , an. Here, a
boolean literal az is a multi-word term or a named
entity extracted from the document D.
</construct>
<bodyText confidence="0.999646615384615">
For simplicity, we only consider boolean ex-
pressions that combine the literals through AND
operator.
The reasons for using terms as well as named
entities in our model are two fold. Firstly, there are
multi-word phrases such as secretary of state, rac-
ing car driver which enable us to describe a person
uniquely but not recognized by named entity tag-
gers. Secondly, automatic term extraction (Frantzi
and Ananiadou, 1999) can be done using statistical
methods and does not require extensive linguistic
resources such as named entity dictionaries, which
may not be available for some domains.
</bodyText>
<subsectionHeader confidence="0.999893">
5.3 Creating Term-Entity Models
</subsectionHeader>
<bodyText confidence="0.99998725">
We extract terms and named entities from each
document to build the term-entity model for that
document. For automatic multi-word term ex-
traction, we use the C-value metric proposed by
Frantzi et al. (1999). Firstly, the text from which
we need to extract terms is tagged using a part
of speech tagger. Then a linguistic filter and a
stop words list constrain the word sequences that
</bodyText>
<page confidence="0.998006">
19
</page>
<figureCaption confidence="0.936635333333333">
Figure 1: Distribution of words in snippets for
”George Bush” and ”President of the United
States”
Figure 2: Distribution of words in snippets for
”Tiger Woods” and ”President of the United
States”
</figureCaption>
<figure confidence="0.999093557692308">
80
60
40
20
0
news
bush
bushs
father
vice
shall
united
library
states
executive
biography
president
games
george
presidential
presidents
government
President of the United States
George Bush
120
100
150
President of the United States
Tiger Woods
120
90
60
30
0
president
presidents
government
executive
reviews
united
golf
pga
woods
tour
bush
tiger
vice
states
golfer
shall
cheats
george
</figure>
<bodyText confidence="0.999077636363636">
are allowed as genuine multi-word terms. The
linguistic filter contains a predefined set of pat-
terns of nouns, adjectives and prepositions that are
likely to be terms. The sequences of words that re-
main after this initial filtering process (candidate
terms) are evaluated for their termhood (likeliness
of a candidate to be a term) using C-value. C-
value is built using statistical characteristics of the
candidate string, such as, total frequency of oc-
currence of the candidate string in the document,
the frequency of the candidate string as part of
other longer candidate strings, the number of these
longer candidate terms and the length of candidate
string (in number of words). We select the candi-
dates with higher C-values as terms (see (Frantzi
and Ananiadou, 1999) for more details on C-value
based term extraction).
To extract entities for the term-entity model, the
documents were annotated by a named entity tag-
ger 3. We select personal names, organization
names and location names to be included in the
term-entity model.
</bodyText>
<subsectionHeader confidence="0.992613">
5.4 Contextual Similarity
</subsectionHeader>
<bodyText confidence="0.999298166666667">
We need to calculate the similarity between term-
entity models derived from different documents,
in order to decide whether they belong to the
same namesake or not. WordNet 4 based similar-
ity metrics have been widely used to compute the
semantic similarity between words in sense dis-
</bodyText>
<footnote confidence="0.99924375">
3The named entity tagger was developed by the Cognitive
Computation Group at UIUC. http://L2R.cs.uiuc.edu/ cog-
comp/eoh/ne.html
4http://wordnet.princeton.edu/perl/webwn
</footnote>
<bodyText confidence="0.998897794117647">
ambiguation tasks (Banerjee and Pedersen, 2002;
McCarthy et al., 2004). However, most of the
terms and entities in our term-entity models are
proper names or multi-word expressions which are
not listed in WordNet.
Sahami et al. (2005) proposed the use of snip-
pets returned by a Web search engine to calculate
the semantic similarity between words. A snippet
is a brief text extracted from a document around
the query term. Many search engines provide snip-
pets alongside with the link to the original docu-
ment. Since snippets capture the immediate sur-
rounding of the query term in the document, we
can consider a snippet as the context of a query
term. Using snippets is also efficient because we
do not need to download the source documents.
To calculate the contextual similarity between two
terms (or entities), we first collect snippets for
each term (or entity) and pool the snippets into
a combined ”bag of words”. Each collection of
snippets is represented by a word vector, weighted
by the normalized frequency (i.e., frequency of a
word in the collection is divided by the total num-
ber of words in the collection). Then, the contex-
tual similarity between two phrases is defined as
the inner product of their snippet-word vectors.
Figures 1 and 2 show the distribution of most
frequent words in snippets for the queries ”George
Bush”, ”Tiger Woods” and ”President of the
United States”. In Figure 1 we observe the words
”george” and ”bush” appear in snippets for the
query ”President of the United States”, whereas
in Figure 2 none of the high frequent words ap-
pears in snippets for both queries. Contextual
</bodyText>
<page confidence="0.973725">
20
</page>
<bodyText confidence="0.99945725">
similarity calculated as the inner product between
word vectors is 0.2014 for ”George Bush” and
”President of the United States”, whereas the
same is 0.0691 for ”Tiger Woods” and ”Presi-
dent of the United States”. We define the simi-
larity sim(T(A), T(B)), between two term-entity
models T (A) = {a1, ... , an} and T (B) =
{b1, ... , bm} of documents A and B as follows,
</bodyText>
<equation confidence="0.971438333333333">
1
sim(T (A), T (B)) =
n
</equation>
<bodyText confidence="0.999107166666667">
Here, |ai |represents the vector that contains the
frequency of words that appear in the snippets
for term/entity ai. Contextual similarity between
terms/entities ai and bj, is defined as the inner
product |ai |· |bj|. Without a loss of generality we
assume n &lt; m in formula 1.
</bodyText>
<subsectionHeader confidence="0.963067">
5.5 Clustering
</subsectionHeader>
<bodyText confidence="0.997318">
We use Group-average agglomerative clustering
(GAAC) (Cutting et al., 1992), a hybrid of single-
link and complete-link clustering, to group the
documents that belong to a particular namesake.
Initially, we assign a separate cluster for each of
the documents in the collection. Then, GAAC in
each iteration executes the merger that gives rise
to the cluster Γ with the largest average correla-
tion QΓ) where,
</bodyText>
<equation confidence="0.891846">
_ 1 1
C(r) 2 |r|(|r |_ 1) uEr
</equation>
<bodyText confidence="0.99988975">
Here, |Γ |denotes the number of documents in
the merged cluster Γ; u and v are two documents
in Γ and sim(T (u), T (v)) is given by equation 1.
Determining the total number of clusters is an im-
portant issue that directly affects the accuracy of
disambiguation. We will discuss an automatic
method to determine the number of clusters in sec-
tion 6.3.
</bodyText>
<subsectionHeader confidence="0.994838">
5.6 Key phrases Selection
</subsectionHeader>
<bodyText confidence="0.999989333333333">
GAAC process yields a set of clusters representing
each of the different namesakes of the ambiguous
name. To select key phrases that uniquely iden-
tify each namesake, we first pool all the terms and
entities in all term-entity models in each cluster.
For each cluster we select the most discrimina-
tive terms/entities as the key phrases that uniquely
identify the namesake represented by that cluster
from the other namesakes. We achieve this in
two steps. In the first step, we reduce the num-
ber of terms/entities in each cluster by removing
terms/entities that also appear in other clusters.
In the second step, we select the terms/entities
in each cluster according to their relevance to
the ambiguous name. We compute the con-
textual similarity between the ambiguous name
and each term/entity and select the top ranking
terms/entities from each cluster.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.999569">
6.1 Evaluating Contextual Similarity
</subsectionHeader>
<bodyText confidence="0.999974826086957">
In section 5.4, we defined the similarity between
documents (i.e., term-entity models created from
the documents) using a web snippets based con-
textual similarity (Formula 1). However, how well
such a metric represents the similarity between
documents, remains unknown. Therefore, to eval-
uate the contextual similarity among documents,
we group the documents in ”person-X” dataset
into four classes (each class representing a differ-
ent person) and use Formula 1 to compute within-
class and cross-class similarity histograms, as il-
lustrated in Figure 3.
Ideally, within-class similarity distribution
should have a peak around 1 and cross-class sim-
ilarity distribution around 0, whereas both his-
tograms in Figure 3(a) and 3(b) have their peaks
around 0.2. However, within-class similarity dis-
tribution is heavily biased toward to the right of
this peak and cross-class similarity distribution to
the left. Moreover, there are no document pairs
with more than 0.5 cross-class similarity. The ex-
perimental results guarantees the validity of the
contextual similarity metric.
</bodyText>
<subsectionHeader confidence="0.997922">
6.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.99997">
We evaluate experimental results based on the
confusion matrix, where A[i.j] represents the
number of documents of ”person i” predicted as
”person j” in matrix A. A[i, i] represents the num-
ber of correctly predicted documents for ”person
i”. We define the disambiguation accuracy as the
sum of diagonal elements divided by the sum of
all elements in the matrix.
</bodyText>
<subsectionHeader confidence="0.998601">
6.3 Cluster Quality
</subsectionHeader>
<bodyText confidence="0.99993925">
Each cluster formed by the GAAC process is sup-
posed to be representing a different namesake.
Ideally, the number of clusters formed should be
equal to the number of different namesakes for
</bodyText>
<equation confidence="0.9542025">
max |ai |· |bj|. (1)
1�j�m
n
i=1
� S*T (u), T (v)) (2)
���
</equation>
<page confidence="0.986275">
21
</page>
<figure confidence="0.999431028571429">
5000
4000
3000
2000
1000
0
0.1
0.2
0.3
0.4
0.5 0.6 0.7
0.8
0.9 1.0
1.1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1500
1200
900
600
300
0
(a) Within-class similarity distribution in
”person-X” dataset
(b) Cross-class similarity distribution in
”person-X” dataset
</figure>
<figureCaption confidence="0.885040666666667">
Figure 3: The histogram of within-class and cross-class similarity distributions in ”person-X” dataset. X
axis represents the similarity value. Y axis represents the number of document pairs from the same class
(within-class) or from different classes (cross-class) that have the corresponding similarity value.
</figureCaption>
<bodyText confidence="0.99992064">
the ambiguous name. However, in reality it is
impossible to exactly know the number of name-
sakes that appear on the Web for a particular name.
Moreover, the distribution of pages among name-
sakes is not even. For example, in the ”Jim Clark”
dataset 78% of documents belong to the two fa-
mous namesakes (CEO Nestscape and Formula
one world champion). The rest of the documents
are distributed among the other six namesakes. If
these outliers get attached to the otherwise pure
clusters, both disambiguation accuracy and key
phrase selection deteriorate. Therefore, we moni-
tor the quality of clustering and terminate further
agglomeration when the cluster quality drops be-
low a pre-set threshold. Numerous metrics have
been proposed for evaluating quality of cluster-
ing (Kannan et al., 2000). We use normalized
cuts (Shi and Malik, 2000) as a measure of cluster-
quality.
Let, V denote the set of documents for a name.
Consider, A C_ V to be a cluster of documents
taken from V . For two documents x,y in V ,
sim(x, y) represents the contextual similarity be-
tween the documents (Formula 1). Then, the nor-
malized cut N,.t(A) of cluster A is defined as,
</bodyText>
<equation confidence="0.998342">
N,.t(A) = ExEA yE(V −A) sim(x, y) (3)
ExEA yEV sim(x, y)
</equation>
<bodyText confidence="0.9648735">
For a set, {A1, ... , An} of non-overlapping n
clusters Ai, we define the quality of clustering,
</bodyText>
<figure confidence="0.755674">
0.8 0.85 0.9 0.95 1 1.05
Quality
</figure>
<figureCaption confidence="0.9943225">
Figure 4: Accuracy Vs Cluster Quality for person-
X data set.
</figureCaption>
<bodyText confidence="0.970793666666666">
Quality({A1, ... , An}), as follows,
To explore the faithfulness of cluster quality
in approximating accuracy, we compare accuracy
(calculated using human-annotated data) and clus-
ter quality (automatically calculated using For-
mula 4) for person-X data set. Figure 4 shows
cluster quality in x-axis and accuracy in y-axis.
We observe a high correlation (Pearson coefficient
of 0.865) between these two measures, which en-
ables us to guide the clustering process through
cluster quality.
When cluster quality drops below a pre-defined
</bodyText>
<figure confidence="0.997006285714286">
1.2
1
0.8
0.6
0.4
0.2
0
</figure>
<equation confidence="0.6745846">
Quality({A1, ... , An}) = n
1
n
N,.t(Ai). (4)
i=1
</equation>
<page confidence="0.914571">
22
</page>
<figure confidence="0.970738285714286">
0.78
0.77
0.76
0.75
0.74
0.73
0.72
Table 2: Disambiguation accuracy for each collec-
tion.
Collection Majority Proposed Found
Sense Method Correct
person-X 0.3676 0.7794 4/4
Michael Jackson 0.6470 0.9706 2/3
Jim Clark 0.4407 0.7627 3/8
William Cohen 0.7614 0.8068 3/10
0.79
0.71
0.7
0.69
0.6 0.7 0.8 0.9 1
Threshold
</figure>
<figureCaption confidence="0.993786">
Figure 5: Accuracy Vs Threshold value for
person-X data set.
</figureCaption>
<bodyText confidence="0.9986878">
threshold, we terminate further clustering. We
assign the remaining documents to the already
formed clusters based on the correlation (For-
mula 2) between the document and the cluster. To
determine the threshold of cluster quality, we use
person-X collection as training data. Figure 5 il-
lustrates the variation of accuracy with threshold.
We select threshold at 0.935 where accuracy max-
imizes in Figure 5. Threshold was fixed at 0.935
for the rest of the experiments.
</bodyText>
<subsectionHeader confidence="0.991749">
6.4 Disambiguation Accuracy
</subsectionHeader>
<bodyText confidence="0.999939411764706">
Table 2 summarizes the experimental results. The
baseline, majority sense , assigns all the doc-
uments in a collection to the person that have
most documents in the collection. Proposed
method outperforms the baseline in all data sets.
Moreover, the accuracy values for the proposed
method in Table 2 are statistically significant (t-
test: P(TG_t)=0.0087, α = 0.05) compared to the
baseline. To identify each cluster with a name-
sake, we chose the person that has most num-
ber of documents in the cluster. ”Found” column
shows the number of correctly identified name-
sakes as a fraction of total namesakes. Although
the proposed method correctly identifies the pop-
ular namesakes, it fails to identify the namesakes
who have just one or two documents in the collec-
tion.
</bodyText>
<subsectionHeader confidence="0.957402">
6.5 Web Search Task
</subsectionHeader>
<bodyText confidence="0.99980225">
Key phrases extracted by the proposed method are
listed in Figure 6 (Due to space limitations, we
show only the top ranking key phrases for two col-
lections). To evaluate key phrases in disambiguat-
</bodyText>
<figureCaption confidence="0.63522">
Figure 6: Top ranking key phrases in clusters for
Michael Jackson and Jim Clark datasets.
</figureCaption>
<bodyText confidence="0.9998924">
ing namesakes, we set up a web search experiment
as follows. We search for the ambiguous name and
the key phrase (for example, ”Jim Clark” AND
”racing driver”) and classify the top 100 results
according to their relevance to each namesake. Re-
sults of our experiment on Jim Clark dataset for
the top ranking key phrases are shown in Table 3.
In Table 3 we classified Google search results
into three categories. ”person-1” is the formula
one racing world champion, ”person -2” is the
founder of Netscape and ”other” category contains
rest of the pages that we could not classify to pre-
vious two groups 5. We first searched Google
without adding any key phrases to the name. In-
cluding terms racing diver, rally and scotsman,
</bodyText>
<tableCaption confidence="0.980926">
Table 3: Effectiveness of key phrases in disam-
biguating namesakes.
</tableCaption>
<table confidence="0.99866175">
Phrase person-1 person-2 others Hits
NONE 41 26 33 1,080,000
racing driver 81 1 18 22,500
rally 42 0 58 82,200
scotsman 67 0 33 16,500
entrepreneur 1 74 25 28,000
story 17 53 30 186,000
silicon valley 0 81 19 46,800
</table>
<footnote confidence="0.7495015">
5some of these pages were on other namesakes and some
were not sufficiently detailed to properly classify
</footnote>
<figure confidence="0.999000344827586">
Michael Jackson
CLUSTER #1
CLUSTER #2
fan club
trial
world network
superstar
new charity song
neverland ranch
beer hunter
ultimate beer FAQ
christmas beer
great beer
pilsener beer
barvaria
Jim Clark
CLUSTER #1 CLUSTER #2
racing driver
rally
scotsman
driving genius
scottish automobile racer
british rally news
entrepreneur
story
silicon valley
CEO
silicon graphics
SGI/ Netscape
</figure>
<page confidence="0.994973">
23
</page>
<bodyText confidence="0.9999542">
which were the top ranking terms for Jim Clark
the formula one champion, yields no results for the
other popular namesake. Likewise, the key words
entrepreneur and silicon valley yield results fort
he founder of Netscape. However, the key word
story appears for both namesakes. A close investi-
gation revealed that, the keyword story is extracted
from the title of the book ”The New New Thing:
A Silicon Valley Story”, a book on the founder of
Netscape.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999988411764706">
We proposed and evaluated a key phrase extraction
algorithm to disambiguate people with the same
name on the Web. We represented each document
with a term-entity model and used a contextual
similarity metric to cluster the documents. We also
proposed a novel approach to determine the num-
ber of namesakes. Our experiments with pseudo
and naturally ambiguous names show a statisti-
cally significant improvement over the baseline
method. We evaluated the key phrases extracted
by the algorithm in a web search task. The web
search task reveals that including the key phrases
in the query considerably reduces ambiguity. In
future, we plan to extend the proposed method
to disambiguate other types of entities such as
location names, product names and organization
names.
</bodyText>
<sectionHeader confidence="0.998193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988130675675676">
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proceedings of COLING, pages 79–85.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using word net. In Proceedings of the third in-
ternational conference on computational linguistics
and intelligent text processing, pages 136–145.
Ron Bekkerman and Andrew McCallum. 2005. Dis-
ambiguating web appearances of people in a social
network. In Proceedings of the 14th international
conference on World Wide Web, pages 463–470.
Douglass R. Cutting, Jan O. Pedersen, David Karger,
and John W. Tukey. 1992. Scatter/gather: A cluster-
based approach to browsing large document collec-
tions. In Proceedings SIGIR ’92, pages 318–329.
M.B. Fleischman and E. Hovy. 2004. Multi-document
person name resolution. In Proceedings of 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Reference Resolution Workshop.
K.T. Frantzi and S. Ananiadou. 1999. The c-value/nc-
value domain independent method for multi-word
term extraction. Journal of Natural Language Pro-
cessing, 6(3):145–179.
S. Gauch and J. B. Smith. 1991. Search improvement
via automatic query reformulation. ACM Trans. on
Information Systems, 9(3):249–280.
R. Guha and A. Garg. 2004. Disambiguating people in
search. In Stanford University.
Hui Han, Hongyuan Zha, and C. Lee Giles. 2005.
Name disambiguation in author citations using a k-
way spectral clustering method. In Proceedings of
the International Conference on Digital Libraries.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2000. On clusterings: Good, bad, and spectral. In
Proceedings of the 41st Annual Symposium on the
Foundation of Computer Science, pages 367–380.
Xin Li, Paul Morie, and Dan Roth. 2005. Semantic
integration in text, from ambiguous names to identi-
fiable entities. AI Magazine, American Association
for Artificial Intelligence, Spring:45–58.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceed-
ings of CoNLL-2003, pages 33–40.
Y. Matsuo, J. Mori, and M. Hamasaki. 2006. Poly-
phonet: An advanced social network extraction sys-
tem. In to appear in World Wide Web Conference
(WWW).
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In Proceedings of the 42nd Meeting of the As-
sociation for Computational Linguistics (ACL’04),
pages 279–286.
P. Mika. 2004. Bootstrapping the foaf-web: and ex-
periment in social networking network minning. In
Proceedings of 1st Workshop on Friend of a Friend,
Social Networking and the Semantic Web.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering sim-
ilar contexts. In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Mehran Sahami and Tim Heilman. 2005. A web-based
kernel function for matching short text snippets. In
International Workshop located at the 22nd Inter-
national Conference on Machine Learning (ICML
2005).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888–
905.
</reference>
<page confidence="0.999163">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700756">
<title confidence="0.86642">Extracting Key Phrases to Personal Name Queries in Web Search</title>
<author confidence="0.950605">Bollegala Yutaka Matsuo Ishizuka</author>
<affiliation confidence="0.9968595">Graduate School of Information Science and The University of</affiliation>
<address confidence="0.991177">7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656,</address>
<email confidence="0.97924">ishizuka@i.u-tokyo.ac.jp</email>
<abstract confidence="0.9996464">Assume that you are looking for information about a particular person. A search engine returns many pages for that person’s name. Some of these pages may be on other people with the same name. One method to reduce the ambiguity in the query and filter out the irrelevant pages, is by adding a phrase that uniquely identifies the person we are interested in from his/her namesakes. We propose an unsupervised algorithm that extracts such phrases from the Web. We represent each by a and cluster the documents using a contextual similarity metric. We evaluate the algorithm on a dataset of ambiguous names. Our method outperforms baselines, achieving over 80% accuracy and significantly reduces the ambiguity in a web search task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based crossdocument coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="5784" citStr="Bagga and Baldwin, 1998" startWordPosition="945" endWordPosition="948">WordNet, whereas no dictionary can provide information regarding different namesakes for a particular name. The problem of person name disambiguation has been addressed in the domain of research paper citations (Han et al., 2005), with various supervised methods proposed for its solution. However, citations have a fixed format compared to free text on the Web. Fields such as co-authors, title, journal name, conference name, year of publication can be easily extracted from a citation and provide vital information to the disambiguation process. Research on multi-document person name resolution (Bagga and Baldwin, 1998; Mann and Yarowsky, 2003; Fleischman and Hovy, 2004) focuses on the related problem of determining if 2http://flink.sematicweb.org/. The system won the 1st place at the Semantic Web Challenge in ISWC2004. two instances with the same name and from different documents refer to the same individual. Bagga and Baldwin (1998) first perform withindocument coreference resolution to form coreference chains for each entity in each document. They then use the text surrounding each reference chain to create summaries about each entity in each document. These summaries are then converted to a bag of words</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based crossdocument coreferencing using the vector space model. In Proceedings of COLING, pages 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using word net.</title>
<date>2002</date>
<booktitle>In Proceedings of the third international conference on computational linguistics and intelligent text processing,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="14599" citStr="Banerjee and Pedersen, 2002" startWordPosition="2376" endWordPosition="2379">We select personal names, organization names and location names to be included in the term-entity model. 5.4 Contextual Similarity We need to calculate the similarity between termentity models derived from different documents, in order to decide whether they belong to the same namesake or not. WordNet 4 based similarity metrics have been widely used to compute the semantic similarity between words in sense dis3The named entity tagger was developed by the Cognitive Computation Group at UIUC. http://L2R.cs.uiuc.edu/ cogcomp/eoh/ne.html 4http://wordnet.princeton.edu/perl/webwn ambiguation tasks (Banerjee and Pedersen, 2002; McCarthy et al., 2004). However, most of the terms and entities in our term-entity models are proper names or multi-word expressions which are not listed in WordNet. Sahami et al. (2005) proposed the use of snippets returned by a Web search engine to calculate the semantic similarity between words. A snippet is a brief text extracted from a document around the query term. Many search engines provide snippets alongside with the link to the original document. Since snippets capture the immediate surrounding of the query term in the document, we can consider a snippet as the context of a query </context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using word net. In Proceedings of the third international conference on computational linguistics and intelligent text processing, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Bekkerman</author>
<author>Andrew McCallum</author>
</authors>
<title>Disambiguating web appearances of people in a social network.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>463--470</pages>
<contexts>
<context position="7739" citStr="Bekkerman and McCallum (2005)" startWordPosition="1256" endWordPosition="1259">us name using second order context vectors derived using singular value decomposition (SVD) on a co-occurrence matrix. They agglomeratively cluster the vectors using cosine similarity. They evaluate their method only on a conflated dataset of pseudonames, which begs the question of how well such a technique would fair on a more real-world challenge. Li et al. (2005) propose two approaches to disambiguate entities in a set of documents: a supervisedly trained pairwise classifier and an unsupervised generative model. However, they do not evaluate the effectiveness of their method in Web search. Bekkerman and McCallum (2005) present two unsupervised methods for finding web pages referring to a particular person: one based on link structure and another using Agglomerative/Conglomerative Double Clustering (A/CDC). Their scenario focuses on simultaneously disambiguating an existing social network of people, who are closely related. Therefore, their method cannot be applied to disambiguate an individual whose social network (for example, friends, colleagues) is not known. Guha and Grag (2004) present a re-ranking algorithm to disambiguate people. The algorithm requires a user to select one of the returned pages as a </context>
</contexts>
<marker>Bekkerman, McCallum, 2005</marker>
<rawString>Ron Bekkerman and Andrew McCallum. 2005. Disambiguating web appearances of people in a social network. In Proceedings of the 14th international conference on World Wide Web, pages 463–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglass R Cutting</author>
<author>Jan O Pedersen</author>
<author>David Karger</author>
<author>John W Tukey</author>
</authors>
<title>Scatter/gather: A clusterbased approach to browsing large document collections.</title>
<date>1992</date>
<booktitle>In Proceedings SIGIR ’92,</booktitle>
<pages>318--329</pages>
<contexts>
<context position="16932" citStr="Cutting et al., 1992" startWordPosition="2779" endWordPosition="2782">e same is 0.0691 for ”Tiger Woods” and ”President of the United States”. We define the similarity sim(T(A), T(B)), between two term-entity models T (A) = {a1, ... , an} and T (B) = {b1, ... , bm} of documents A and B as follows, 1 sim(T (A), T (B)) = n Here, |ai |represents the vector that contains the frequency of words that appear in the snippets for term/entity ai. Contextual similarity between terms/entities ai and bj, is defined as the inner product |ai |· |bj|. Without a loss of generality we assume n &lt; m in formula 1. 5.5 Clustering We use Group-average agglomerative clustering (GAAC) (Cutting et al., 1992), a hybrid of singlelink and complete-link clustering, to group the documents that belong to a particular namesake. Initially, we assign a separate cluster for each of the documents in the collection. Then, GAAC in each iteration executes the merger that gives rise to the cluster Γ with the largest average correlation QΓ) where, _ 1 1 C(r) 2 |r|(|r |_ 1) uEr Here, |Γ |denotes the number of documents in the merged cluster Γ; u and v are two documents in Γ and sim(T (u), T (v)) is given by equation 1. Determining the total number of clusters is an important issue that directly affects the accura</context>
</contexts>
<marker>Cutting, Pedersen, Karger, Tukey, 1992</marker>
<rawString>Douglass R. Cutting, Jan O. Pedersen, David Karger, and John W. Tukey. 1992. Scatter/gather: A clusterbased approach to browsing large document collections. In Proceedings SIGIR ’92, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Fleischman</author>
<author>E Hovy</author>
</authors>
<title>Multi-document person name resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.</booktitle>
<contexts>
<context position="5837" citStr="Fleischman and Hovy, 2004" startWordPosition="953" endWordPosition="956">tion regarding different namesakes for a particular name. The problem of person name disambiguation has been addressed in the domain of research paper citations (Han et al., 2005), with various supervised methods proposed for its solution. However, citations have a fixed format compared to free text on the Web. Fields such as co-authors, title, journal name, conference name, year of publication can be easily extracted from a citation and provide vital information to the disambiguation process. Research on multi-document person name resolution (Bagga and Baldwin, 1998; Mann and Yarowsky, 2003; Fleischman and Hovy, 2004) focuses on the related problem of determining if 2http://flink.sematicweb.org/. The system won the 1st place at the Semantic Web Challenge in ISWC2004. two instances with the same name and from different documents refer to the same individual. Bagga and Baldwin (1998) first perform withindocument coreference resolution to form coreference chains for each entity in each document. They then use the text surrounding each reference chain to create summaries about each entity in each document. These summaries are then converted to a bag of words feature vector and are clustered using standard vect</context>
</contexts>
<marker>Fleischman, Hovy, 2004</marker>
<rawString>M.B. Fleischman and E. Hovy. 2004. Multi-document person name resolution. In Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K T Frantzi</author>
<author>S Ananiadou</author>
</authors>
<title>The c-value/ncvalue domain independent method for multi-word term extraction.</title>
<date>1999</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="11867" citStr="Frantzi and Ananiadou, 1999" startWordPosition="1944" endWordPosition="1947">model T (A), representing a person A in a document D, is a boolean expression of n literals a1, a2, ... , an. Here, a boolean literal az is a multi-word term or a named entity extracted from the document D. For simplicity, we only consider boolean expressions that combine the literals through AND operator. The reasons for using terms as well as named entities in our model are two fold. Firstly, there are multi-word phrases such as secretary of state, racing car driver which enable us to describe a person uniquely but not recognized by named entity taggers. Secondly, automatic term extraction (Frantzi and Ananiadou, 1999) can be done using statistical methods and does not require extensive linguistic resources such as named entity dictionaries, which may not be available for some domains. 5.3 Creating Term-Entity Models We extract terms and named entities from each document to build the term-entity model for that document. For automatic multi-word term extraction, we use the C-value metric proposed by Frantzi et al. (1999). Firstly, the text from which we need to extract terms is tagged using a part of speech tagger. Then a linguistic filter and a stop words list constrain the word sequences that 19 Figure 1: </context>
<context position="13815" citStr="Frantzi and Ananiadou, 1999" startWordPosition="2261" endWordPosition="2264">are likely to be terms. The sequences of words that remain after this initial filtering process (candidate terms) are evaluated for their termhood (likeliness of a candidate to be a term) using C-value. Cvalue is built using statistical characteristics of the candidate string, such as, total frequency of occurrence of the candidate string in the document, the frequency of the candidate string as part of other longer candidate strings, the number of these longer candidate terms and the length of candidate string (in number of words). We select the candidates with higher C-values as terms (see (Frantzi and Ananiadou, 1999) for more details on C-value based term extraction). To extract entities for the term-entity model, the documents were annotated by a named entity tagger 3. We select personal names, organization names and location names to be included in the term-entity model. 5.4 Contextual Similarity We need to calculate the similarity between termentity models derived from different documents, in order to decide whether they belong to the same namesake or not. WordNet 4 based similarity metrics have been widely used to compute the semantic similarity between words in sense dis3The named entity tagger was d</context>
</contexts>
<marker>Frantzi, Ananiadou, 1999</marker>
<rawString>K.T. Frantzi and S. Ananiadou. 1999. The c-value/ncvalue domain independent method for multi-word term extraction. Journal of Natural Language Processing, 6(3):145–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gauch</author>
<author>J B Smith</author>
</authors>
<title>Search improvement via automatic query reformulation.</title>
<date>1991</date>
<journal>ACM Trans. on Information Systems,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="2920" citStr="Gauch and Smith, 1991" startWordPosition="478" endWordPosition="481">ment containing the ambiguous name by a term-entity model, as described in section 5.2. We define a contextual similarity metric based on snippets returned by a search engine, to calculate the similarity between term-entity models. In the second stage, we cluster the documents using the similarity metric. In the final stage, we select key phrases from the clusters that uniquely identify each namesake. 2 Applications Two tasks that can readily benefit from automatically extracted key phrases to disambiguate personal names are query suggestion and social network extraction. In query suggestion (Gauch and Smith, 1991), the search engine returns a set of phrases to the user alongside with the search results. The user can then modify the original query using these phrases to narrow down the search. Query suggestion helps the users to easily navigate through the result set. For personal name queries, the key phrases extracted by our algorithm can be used as suggestions to reduce the ambiguity and narrow down the search on a particular namesake. Social networking services (SNSs) have been given much attention on the Web recently. As a kind of online applications, SNSs can be used 17 Proceedings of the Workshop</context>
</contexts>
<marker>Gauch, Smith, 1991</marker>
<rawString>S. Gauch and J. B. Smith. 1991. Search improvement via automatic query reformulation. ACM Trans. on Information Systems, 9(3):249–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Guha</author>
<author>A Garg</author>
</authors>
<title>Disambiguating people in search. In</title>
<date>2004</date>
<institution>Stanford University.</institution>
<marker>Guha, Garg, 2004</marker>
<rawString>R. Guha and A. Garg. 2004. Disambiguating people in search. In Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Han</author>
<author>Hongyuan Zha</author>
<author>C Lee Giles</author>
</authors>
<title>Name disambiguation in author citations using a kway spectral clustering method.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Digital Libraries.</booktitle>
<contexts>
<context position="5390" citStr="Han et al., 2005" startWordPosition="884" endWordPosition="887">several fundamental differences between WSD and person name disambiguation. WSD typically concentrates on disambiguating between 2-4 possible meanings of the word, all of which are a priori known. However, in person name disambiguation in Web, the number of different namesakes can be much larger and unknown. From a resource point of view, WSD utilizes sense tagged dictionaries such as WordNet, whereas no dictionary can provide information regarding different namesakes for a particular name. The problem of person name disambiguation has been addressed in the domain of research paper citations (Han et al., 2005), with various supervised methods proposed for its solution. However, citations have a fixed format compared to free text on the Web. Fields such as co-authors, title, journal name, conference name, year of publication can be easily extracted from a citation and provide vital information to the disambiguation process. Research on multi-document person name resolution (Bagga and Baldwin, 1998; Mann and Yarowsky, 2003; Fleischman and Hovy, 2004) focuses on the related problem of determining if 2http://flink.sematicweb.org/. The system won the 1st place at the Semantic Web Challenge in ISWC2004. </context>
</contexts>
<marker>Han, Zha, Giles, 2005</marker>
<rawString>Hui Han, Hongyuan Zha, and C. Lee Giles. 2005. Name disambiguation in author citations using a kway spectral clustering method. In Proceedings of the International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kannan</author>
<author>Santosh Vempala</author>
<author>Adrian Vetta</author>
</authors>
<title>On clusterings: Good, bad, and spectral.</title>
<date>2000</date>
<booktitle>In Proceedings of the 41st Annual Symposium on the Foundation of Computer Science,</booktitle>
<pages>367--380</pages>
<contexts>
<context position="21674" citStr="Kannan et al., 2000" startWordPosition="3551" endWordPosition="3554">on of pages among namesakes is not even. For example, in the ”Jim Clark” dataset 78% of documents belong to the two famous namesakes (CEO Nestscape and Formula one world champion). The rest of the documents are distributed among the other six namesakes. If these outliers get attached to the otherwise pure clusters, both disambiguation accuracy and key phrase selection deteriorate. Therefore, we monitor the quality of clustering and terminate further agglomeration when the cluster quality drops below a pre-set threshold. Numerous metrics have been proposed for evaluating quality of clustering (Kannan et al., 2000). We use normalized cuts (Shi and Malik, 2000) as a measure of clusterquality. Let, V denote the set of documents for a name. Consider, A C_ V to be a cluster of documents taken from V . For two documents x,y in V , sim(x, y) represents the contextual similarity between the documents (Formula 1). Then, the normalized cut N,.t(A) of cluster A is defined as, N,.t(A) = ExEA yE(V −A) sim(x, y) (3) ExEA yEV sim(x, y) For a set, {A1, ... , An} of non-overlapping n clusters Ai, we define the quality of clustering, 0.8 0.85 0.9 0.95 1 1.05 Quality Figure 4: Accuracy Vs Cluster Quality for personX data</context>
</contexts>
<marker>Kannan, Vempala, Vetta, 2000</marker>
<rawString>Ravi Kannan, Santosh Vempala, and Adrian Vetta. 2000. On clusterings: Good, bad, and spectral. In Proceedings of the 41st Annual Symposium on the Foundation of Computer Science, pages 367–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Paul Morie</author>
<author>Dan Roth</author>
</authors>
<title>Semantic integration in text, from ambiguous names to identifiable entities.</title>
<date>2005</date>
<journal>AI Magazine, American Association for Artificial Intelligence, Spring:45–58.</journal>
<contexts>
<context position="7478" citStr="Li et al. (2005)" startWordPosition="1216" endWordPosition="1219">entropy classifier to learn distances between documents that are then clustered. Their method requires a large training set. Pedersen et al. (2005) propose an unsupervised approach to resolve name ambiguity by representing the context of an ambiguous name using second order context vectors derived using singular value decomposition (SVD) on a co-occurrence matrix. They agglomeratively cluster the vectors using cosine similarity. They evaluate their method only on a conflated dataset of pseudonames, which begs the question of how well such a technique would fair on a more real-world challenge. Li et al. (2005) propose two approaches to disambiguate entities in a set of documents: a supervisedly trained pairwise classifier and an unsupervised generative model. However, they do not evaluate the effectiveness of their method in Web search. Bekkerman and McCallum (2005) present two unsupervised methods for finding web pages referring to a particular person: one based on link structure and another using Agglomerative/Conglomerative Double Clustering (A/CDC). Their scenario focuses on simultaneously disambiguating an existing social network of people, who are closely related. Therefore, their method cann</context>
</contexts>
<marker>Li, Morie, Roth, 2005</marker>
<rawString>Xin Li, Paul Morie, and Dan Roth. 2005. Semantic integration in text, from ambiguous names to identifiable entities. AI Magazine, American Association for Artificial Intelligence, Spring:45–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="5809" citStr="Mann and Yarowsky, 2003" startWordPosition="949" endWordPosition="952">onary can provide information regarding different namesakes for a particular name. The problem of person name disambiguation has been addressed in the domain of research paper citations (Han et al., 2005), with various supervised methods proposed for its solution. However, citations have a fixed format compared to free text on the Web. Fields such as co-authors, title, journal name, conference name, year of publication can be easily extracted from a citation and provide vital information to the disambiguation process. Research on multi-document person name resolution (Bagga and Baldwin, 1998; Mann and Yarowsky, 2003; Fleischman and Hovy, 2004) focuses on the related problem of determining if 2http://flink.sematicweb.org/. The system won the 1st place at the Semantic Web Challenge in ISWC2004. two instances with the same name and from different documents refer to the same individual. Bagga and Baldwin (1998) first perform withindocument coreference resolution to form coreference chains for each entity in each document. They then use the text surrounding each reference chain to create summaries about each entity in each document. These summaries are then converted to a bag of words feature vector and are c</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2003. Unsupervised personal name disambiguation. In Proceedings of CoNLL-2003, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>J Mori</author>
<author>M Hamasaki</author>
</authors>
<title>Polyphonet: An advanced social network extraction system.</title>
<date>2006</date>
<note>In to appear in World Wide Web Conference (WWW).</note>
<contexts>
<context position="3881" citStr="Matsuo et al., 2006" startWordPosition="631" endWordPosition="635">as suggestions to reduce the ambiguity and narrow down the search on a particular namesake. Social networking services (SNSs) have been given much attention on the Web recently. As a kind of online applications, SNSs can be used 17 Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17–24, Sydney, July 2006. c�2006 Association for Computational Linguistics to register and share personal information among friends and communities. There have been recent attempts to extract social networks using the information available on the Web 2(Mika, 2004; Matsuo et al., 2006). In both Matsuo’s (2006) and Mika’s (2004) algorithms, each person is represented by a node in the social network and the strength of the relationship between two people is represented by the length of the edge between the corresponding two nodes. As a measure of the strength of the relationship between two people A and B, these algorithms use the number of hits obtained for the query A AND B. However, this approach fails when A or B has namesakes because the number of hits in these cases includes the hits for the namesakes. To overcome this problem, we could include phrases in the query that</context>
</contexts>
<marker>Matsuo, Mori, Hamasaki, 2006</marker>
<rawString>Y. Matsuo, J. Mori, and M. Hamasaki. 2006. Polyphonet: An advanced social network extraction system. In to appear in World Wide Web Conference (WWW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>279--286</pages>
<contexts>
<context position="4675" citStr="McCarthy et al., 2004" startWordPosition="773" endWordPosition="776">s represented by the length of the edge between the corresponding two nodes. As a measure of the strength of the relationship between two people A and B, these algorithms use the number of hits obtained for the query A AND B. However, this approach fails when A or B has namesakes because the number of hits in these cases includes the hits for the namesakes. To overcome this problem, we could include phrases in the query that uniquely identify A and B from their namesakes. 3 Related Work Person name disambiguation can be seen as a special case of word sense disambiguation (WSD) (Schutze, 1998; McCarthy et al., 2004) problem which has been studied extensively in Natural Language Understanding. However, there are several fundamental differences between WSD and person name disambiguation. WSD typically concentrates on disambiguating between 2-4 possible meanings of the word, all of which are a priori known. However, in person name disambiguation in Web, the number of different namesakes can be much larger and unknown. From a resource point of view, WSD utilizes sense tagged dictionaries such as WordNet, whereas no dictionary can provide information regarding different namesakes for a particular name. The pr</context>
<context position="14623" citStr="McCarthy et al., 2004" startWordPosition="2380" endWordPosition="2383">anization names and location names to be included in the term-entity model. 5.4 Contextual Similarity We need to calculate the similarity between termentity models derived from different documents, in order to decide whether they belong to the same namesake or not. WordNet 4 based similarity metrics have been widely used to compute the semantic similarity between words in sense dis3The named entity tagger was developed by the Cognitive Computation Group at UIUC. http://L2R.cs.uiuc.edu/ cogcomp/eoh/ne.html 4http://wordnet.princeton.edu/perl/webwn ambiguation tasks (Banerjee and Pedersen, 2002; McCarthy et al., 2004). However, most of the terms and entities in our term-entity models are proper names or multi-word expressions which are not listed in WordNet. Sahami et al. (2005) proposed the use of snippets returned by a Web search engine to calculate the semantic similarity between words. A snippet is a brief text extracted from a document around the query term. Many search engines provide snippets alongside with the link to the original document. Since snippets capture the immediate surrounding of the query term in the document, we can consider a snippet as the context of a query term. Using snippets is </context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), pages 279–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mika</author>
</authors>
<title>Bootstrapping the foaf-web: and experiment in social networking network minning.</title>
<date>2004</date>
<booktitle>In Proceedings of 1st Workshop on Friend of a Friend, Social Networking and the Semantic Web.</booktitle>
<contexts>
<context position="3859" citStr="Mika, 2004" startWordPosition="629" endWordPosition="630">can be used as suggestions to reduce the ambiguity and narrow down the search on a particular namesake. Social networking services (SNSs) have been given much attention on the Web recently. As a kind of online applications, SNSs can be used 17 Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 17–24, Sydney, July 2006. c�2006 Association for Computational Linguistics to register and share personal information among friends and communities. There have been recent attempts to extract social networks using the information available on the Web 2(Mika, 2004; Matsuo et al., 2006). In both Matsuo’s (2006) and Mika’s (2004) algorithms, each person is represented by a node in the social network and the strength of the relationship between two people is represented by the length of the edge between the corresponding two nodes. As a measure of the strength of the relationship between two people A and B, these algorithms use the number of hits obtained for the query A AND B. However, this approach fails when A or B has namesakes because the number of hits in these cases includes the hits for the namesakes. To overcome this problem, we could include phr</context>
</contexts>
<marker>Mika, 2004</marker>
<rawString>P. Mika. 2004. Bootstrapping the foaf-web: and experiment in social networking network minning. In Proceedings of 1st Workshop on Friend of a Friend, Social Networking and the Semantic Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Amruta Purandare</author>
<author>Anagha Kulkarni</author>
</authors>
<title>Name discrimination by clustering similar contexts.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="7009" citStr="Pedersen et al. (2005)" startWordPosition="1140" endWordPosition="1143">ature vector and are clustered using standard vector space model often employed in IR. The use of simplistic bag of words clustering is an inherently limiting aspect of their methodology. On the other hand, Mann and Yarowsky (2003) proposes a richer document representation involving automatically extracted features. However, their clustering technique can be basically used only for separating two people with the same name. Fleischman and Hovy (2004) constructs a maximum entropy classifier to learn distances between documents that are then clustered. Their method requires a large training set. Pedersen et al. (2005) propose an unsupervised approach to resolve name ambiguity by representing the context of an ambiguous name using second order context vectors derived using singular value decomposition (SVD) on a co-occurrence matrix. They agglomeratively cluster the vectors using cosine similarity. They evaluate their method only on a conflated dataset of pseudonames, which begs the question of how well such a technique would fair on a more real-world challenge. Li et al. (2005) propose two approaches to disambiguate entities in a set of documents: a supervisedly trained pairwise classifier and an unsupervi</context>
</contexts>
<marker>Pedersen, Purandare, Kulkarni, 2005</marker>
<rawString>Ted Pedersen, Amruta Purandare, and Anagha Kulkarni. 2005. Name discrimination by clustering similar contexts. In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Tim Heilman</author>
</authors>
<title>A web-based kernel function for matching short text snippets.</title>
<date>2005</date>
<booktitle>In International Workshop located at the 22nd International Conference on Machine Learning (ICML</booktitle>
<marker>Sahami, Heilman, 2005</marker>
<rawString>Mehran Sahami and Tim Heilman. 2005. A web-based kernel function for matching short text snippets. In International Workshop located at the 22nd International Conference on Machine Learning (ICML 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<contexts>
<context position="4651" citStr="Schutze, 1998" startWordPosition="771" endWordPosition="772">en two people is represented by the length of the edge between the corresponding two nodes. As a measure of the strength of the relationship between two people A and B, these algorithms use the number of hits obtained for the query A AND B. However, this approach fails when A or B has namesakes because the number of hits in these cases includes the hits for the namesakes. To overcome this problem, we could include phrases in the query that uniquely identify A and B from their namesakes. 3 Related Work Person name disambiguation can be seen as a special case of word sense disambiguation (WSD) (Schutze, 1998; McCarthy et al., 2004) problem which has been studied extensively in Natural Language Understanding. However, there are several fundamental differences between WSD and person name disambiguation. WSD typically concentrates on disambiguating between 2-4 possible meanings of the word, all of which are a priori known. However, in person name disambiguation in Web, the number of different namesakes can be much larger and unknown. From a resource point of view, WSD utilizes sense tagged dictionaries such as WordNet, whereas no dictionary can provide information regarding different namesakes for a</context>
</contexts>
<marker>Schutze, 1998</marker>
<rawString>Hinrich Schutze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianbo Shi</author>
<author>Jitendra Malik</author>
</authors>
<title>Normalized cuts and image segmentation.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>22</volume>
<issue>8</issue>
<pages>905</pages>
<contexts>
<context position="21720" citStr="Shi and Malik, 2000" startWordPosition="3559" endWordPosition="3562">xample, in the ”Jim Clark” dataset 78% of documents belong to the two famous namesakes (CEO Nestscape and Formula one world champion). The rest of the documents are distributed among the other six namesakes. If these outliers get attached to the otherwise pure clusters, both disambiguation accuracy and key phrase selection deteriorate. Therefore, we monitor the quality of clustering and terminate further agglomeration when the cluster quality drops below a pre-set threshold. Numerous metrics have been proposed for evaluating quality of clustering (Kannan et al., 2000). We use normalized cuts (Shi and Malik, 2000) as a measure of clusterquality. Let, V denote the set of documents for a name. Consider, A C_ V to be a cluster of documents taken from V . For two documents x,y in V , sim(x, y) represents the contextual similarity between the documents (Formula 1). Then, the normalized cut N,.t(A) of cluster A is defined as, N,.t(A) = ExEA yE(V −A) sim(x, y) (3) ExEA yEV sim(x, y) For a set, {A1, ... , An} of non-overlapping n clusters Ai, we define the quality of clustering, 0.8 0.85 0.9 0.95 1 1.05 Quality Figure 4: Accuracy Vs Cluster Quality for personX data set. Quality({A1, ... , An}), as follows, To </context>
</contexts>
<marker>Shi, Malik, 2000</marker>
<rawString>Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888– 905.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>