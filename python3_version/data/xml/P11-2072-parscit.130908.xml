<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000902">
<title confidence="0.993916">
Issues Concerning Decoding with Synchronous Context-free Grammar
</title>
<author confidence="0.999084">
Tagyoung Chung, Licheng Fang and Daniel Gildea
</author>
<affiliation confidence="0.997956">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.3161">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.960639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999804333333334">
We discuss some of the practical issues that
arise from decoding with general synchronous
context-free grammars. We examine problems
caused by unary rules and we also examine
how virtual nonterminals resulting from bina-
rization can best be handled. We also inves-
tigate adding more flexibility to synchronous
context-free grammars by adding glue rules
and phrases.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996079">
Synchronous context-free grammar (SCFG) is
widely used for machine translation. There are many
different ways to extract SCFGs from data. Hiero
(Chiang, 2005) represents a more restricted form of
SCFG, while GHKM (Galley et al., 2004) uses a gen-
eral form of SCFG.
In this paper, we discuss some of the practical is-
sues that arise from decoding general SCFGs that
are seldom discussed in the literature. We focus on
parsing grammars extracted using the method put
forth by Galley et al. (2004), but the solutions to
these issues are applicable to other general forms of
SCFG with many nonterminals.
The GHKM grammar extraction method produces
a large number of unary rules. Unary rules are the
rules that have exactly one nonterminal and no ter-
minals on the source side. They may be problematic
for decoders since they may create cycles, which are
unary production chains that contain duplicated dy-
namic programming states. In later sections, we dis-
cuss why unary rules are problematic and investigate
two possible solutions.
GHKM grammars often have rules with many
right-hand-side nonterminals and require binariza-
tion to ensure O(n3) time parsing. However, bina-
rization creates a large number of virtual nontermi-
nals. We discuss the challenges of, and possible so-
lutions to, issues arising from having a large num-
ber of virtual nonterminals. We also compare bina-
rizing the grammar with filtering rules according to
scope, a concept introduced by Hopkins and Lang-
mead (2010). By explicitly considering the effect
of anchoring terminals on input sentences, scope-
3 rules encompass a much larger set of rules than
Chomsky normal form but they can still be parsed in
O(n3) time.
Unlike phrase-based machine translation, GHKM
grammars are less flexible in how they can seg-
ment sentence pairs into phrases because they are
restricted not only by alignments between words in
sentence pairs, but also by target-side parse trees. In
general, GHKM grammars suffer more from data
sparsity than phrasal rules. To alleviate this issue,
we discuss adding glue rules and phrases extracted
using methods commonly used in phrase-based ma-
chine translation.
</bodyText>
<sectionHeader confidence="0.637775" genericHeader="method">
2 Handling unary rules
</sectionHeader>
<bodyText confidence="0.998340875">
Unary rules are common in GHKM grammars. We
observed that as many as 10% of the rules extracted
from a Chinese-English parallel corpus are unary.
Some unary rules are the result of alignment er-
rors, but other ones might be useful. For example,
Chinese lacks determiners, and English determiners
usually remain unaligned to any Chinese words. Ex-
tracted grammars include rules that reflect this fact:
</bodyText>
<equation confidence="0.7062175">
NP → NP, the NP
NP → NP, a NP
</equation>
<page confidence="0.965715">
413
</page>
<note confidence="0.780983">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 413–417,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.993639625">
NP
NPB
NNP
romance
PP
IN NP
with
NPB
</figure>
<bodyText confidence="0.683919">
However, unary rules can be problematic:
</bodyText>
<listItem confidence="0.911752538461538">
• Unary production cycles corrupt the translation
hypergraph generated by the decoder. A hyper-
graph containing a unary cycle cannot be topo-
logically sorted. Many algorithms for parame-
ter tuning and coarse-to-fine decoding, such as
the inside-outside algorithm and cube-pruning,
cannot be run in the presence of unary cycles.
DT
the
• The existence of many unary rules of the form
“NP → NP, the NP” quickly fills a pruning bin
with guesses of English words to insert without
any source-side lexical evidence.
</listItem>
<bodyText confidence="0.999956428571429">
The most obvious way of eliminating problem-
atic unary rules would be converting grammars into
Chomsky normal form. However, this may result
in bloated grammars. In this section, we present
two different ways to handle unary rules. The first
involves modifying the grammar extraction method,
and the second involves modifying the decoder.
</bodyText>
<subsectionHeader confidence="0.999163">
2.1 Modifying grammar extraction
</subsectionHeader>
<bodyText confidence="0.999902736842105">
We can modify the grammar extraction method such
that it does not extract any unary rules. Galley et al.
(2004) extracts rules by segmenting the target-side
parse parse tree based on frontier nodes. We modify
the definition of a frontier node in the following way.
We label frontier nodes in the English parse tree, and
examine the Chinese span each frontier node cov-
ers. If a frontier node covers the same span as the
frontier node that immediately dominates it, then the
dominated node is no longer considered a frontier.
This modification prevents unary rules from being
extracted.
Figure 1 shows an example of an English-Chinese
sentence pair with the English side automatically
parsed. Frontier nodes in the tree in the original
GHKM rule extraction method are marked with a
box. With the modification, only the top bold-
faced NP would be considered a frontier node. The
GHKM rule extraction results in the following rules:
</bodyText>
<construct confidence="0.60783725">
NPB → n鹭 鸶, the snowy egret
NP → NPB, NPB
PP → NP, with NP
NP → PP, romance PP
</construct>
<bodyText confidence="0.561575">
With the change, only the following rule is extracted:
</bodyText>
<figure confidence="0.69736">
JJ
</figure>
<figureCaption confidence="0.998379">
Figure 1: A sentence fragment pair with erroneous align-
ment and tokenization
</figureCaption>
<bodyText confidence="0.834016">
NP → n鹭 鸶, romance with the snowy egret
We examine the effect of this modification has on
translation performance in Section 5.
</bodyText>
<subsectionHeader confidence="0.999481">
2.2 Modifying the decoder
</subsectionHeader>
<bodyText confidence="0.9543257">
Modifying how grammars are extracted has an ob-
vious down side, i.e., the loss of generality. In the
previous example, the modification results in a bad
rule, which is the result of bad alignments. Before
the modification, the rule set includes a good rule:
NPB → n鹭 鸶, the snowy egret
which can be applied at test time. Because of this,
one may still want to decode with all available unary
rules. We handle unary rules inside the decoder in
the following ways:
</bodyText>
<listItem confidence="0.979385">
• Unary cycle detection
</listItem>
<bodyText confidence="0.999860875">
The naïve way to detect unary cycles is back-
tracking on a unary chain to see if a newly gen-
erated item has been generated before. The run-
ning time of this is constrained only by the num-
ber of possible items in a chart span. In prac-
tice, however, this is often not a problem: if all
unary derivations have positive costs and a pri-
ority queue is used to expand unary derivations,
</bodyText>
<equation confidence="0.868815666666667">
NN
snowy egret
n鹭 鸶 的 爱
</equation>
<page confidence="0.986918">
414
</page>
<bodyText confidence="0.998994">
only the best K unary items will be generated,
where K is the pruning constant.
</bodyText>
<listItem confidence="0.899397">
• Ban negative cost unary rules
</listItem>
<bodyText confidence="0.998826">
When tuning feature weights, an optimizer may
try feature weights that may give negative costs
to unary productions. This causes unary deriva-
tions to go on forever. The solution is to set
a maximum length for unary chains, or to ban
negative unary productions outright.
</bodyText>
<sectionHeader confidence="0.977971" genericHeader="method">
3 Issues with binarization
</sectionHeader>
<subsectionHeader confidence="0.998386">
3.1 Filtering and binarization
</subsectionHeader>
<bodyText confidence="0.999962">
Synchronous binarization (Zhang et al., 2006) is
an effective method to reduce SCFG parsing com-
plexity and allow early language model integration.
However, it creates virtual nonterminals which re-
quire special attention at parsing time. Alternatively,
we can filter rules that have more than scope-3 to
parse in O(n3) time with unbinarized rules. This
requires Earley (Earley, 1970) style parsing, which
does implicit binarization at decoding time. Scope-
filtering may filter out unnecessarily long rules that
may never be applied, but it may also throw out
rules with useful contextual information. In addi-
tion, scope-filtering does not accommodate early lan-
guage model state integration. We compare the two
with an experiment. For the rest of the section, we
discuss issues created by virtual nonterminals.
</bodyText>
<subsectionHeader confidence="0.999824">
3.2 Handling virtual nonterminals
</subsectionHeader>
<bodyText confidence="0.999951928571429">
One aspect of grammar binarization that is rarely
mentioned is how to assign probabilities to binarized
grammar rules. The naïve solution is to assign prob-
ability one to any rule whose left-hand side is a vir-
tual nonterminal. This maintains the original model.
However, it is generally not fair to put chart items of
virtual nonterminals and those of regular nontermi-
nals in the same bin, because virtual items have arti-
ficially low costs. One possible solution is adding a
heuristic to push up the cost of virtual items for fair
comparison.
For our experiments, we use an outside estimate
as a heuristic for a virtual item. Consider the follow-
ing rule binarization (only the source side shown):
</bodyText>
<equation confidence="0.988545666666667">
V → BC : 0
A → BCD : −log(p) �
A → VD : − log(p)
</equation>
<bodyText confidence="0.999305647058824">
A → BCD is the orginal rule and − log(p) is the cost
of the rule. In decoding time, when a chart item is
generated from the binarized rule V → BC, we add
− log(p) to its total cost as an optimistic estimate of
the cost to build the original unbinarized rule. The
heuristic is used only for pruning purposes, and it
does not change the real cost. The idea is similar
to A* parsing (Klein and Manning, 2003). One com-
plication is that a binarized rule can arise from multi-
ple different unbinarized rules. In this case, we pick
the lowest cost among the unbinarized rules as the
heuristic.
Another approach for handling virtual nontermi-
nals would be giving virtual items separate bins and
avoiding pruning them at all. This is usually not
practical for GHKM grammars, because of the large
number of nonterminals.
</bodyText>
<sectionHeader confidence="0.953992" genericHeader="method">
4 Adding flexibility
</sectionHeader>
<subsectionHeader confidence="0.987987">
4.1 Glue rules
</subsectionHeader>
<bodyText confidence="0.999983666666667">
Because of data sparsity, an SCFG extracted from
data may fail to parse sentences at test time. For
example, consider the following rules:
</bodyText>
<equation confidence="0.99783925">
NP → JJ NN, JJ NN
JJ → c1, e1
JJ → c2, e2
NN → c3, e3
</equation>
<bodyText confidence="0.999733428571429">
This set of rules is able to parse the word sequence
c1 c3 and c2 c3 but not c1 c2 c3, if we have not seen
“NP → JJ JJ NN” at training time. Because SCFGs
neither model adjunction, nor are they markovized,
with a small amount of data, such problems can oc-
cur. Therefore, we may opt to add glue rules as used
in Hiero (Chiang, 2005):
</bodyText>
<equation confidence="0.9490375">
S → C, C
S → S C, S C
</equation>
<bodyText confidence="0.9998949">
where S is the goal state and C is the glue nonter-
minal that can produce any nonterminals. We re-
fer to these glue rules as the monotonic glue rules.
We rely on GHKM rules for reordering when we use
the monotonic glue rules. However, we can also al-
low glue rules to reorder constituents. Wu (1997)
presents a better-constrained grammar designed to
only produce tail-recursive parses. See Table 1 for
the complete set of rules. We refer to these rules as
ABC glue rules. These rules always generate left-
</bodyText>
<page confidence="0.997917">
415
</page>
<table confidence="0.999172428571429">
BLEU
Baseline + monotonic glue rules 20.99
No-unary + monotonic glue rules 23.83
No-unary + ABC glue rules 23.94
No-unary (scope-filtered) + monotonic 23.99
No-unary (scope-filtered) + ABC glue rules 24.09
No-unary + ABC glue rules + phrases 23.43
</table>
<tableCaption confidence="0.949315">
Table 2: BLEU score results for Chinese-English with
different settings
</tableCaption>
<figure confidence="0.9981575">
S S A A S [A B] B S ( B A )
SSB A S [B B] B S ( A A )
SSC A S [C B] B S ( C A )
A S [A C] B S ( B C )
A S [B C] B S ( A C )
A S [C C] B S ( C C )
</figure>
<figureCaption confidence="0.887439">
Table 1: The ABC Grammar. We follow the convention
of Wu (1997) that square brackets stand for straight rules
and angle brackets stand for inverted rules.
</figureCaption>
<bodyText confidence="0.9999265">
heavy derivations, weeding out ambiguity and mak-
ing search more efficient. We learn probabilities of
ABC glue rules by using expectation maximization
(Dempster et al., 1977) to train a word-level Inver-
sion Transduction Grammar from data.
In our experiments, depending on the configura-
tion, the decoder failed to parse about 5% of sen-
tences without glue rules, which illustrates their ne-
cessity. Although it is reasonable to believe that re-
ordering should always have evidence in data, as
with GHKM rules, we may wish to reorder based
on evidence from the language model. In our ex-
periments, we compare the ABC glue rules with the
monotonic glue rules.
</bodyText>
<subsectionHeader confidence="0.999701">
4.2 Adding phrases
</subsectionHeader>
<bodyText confidence="0.999989176470588">
GHKM grammars are more restricted than the
phrase extraction methods used in phrase-based
models, since, in GHKM grammar extraction,
phrase segmentation is constrained by parse trees.
This may be a good thing, but it suffers from loss
of flexibility, and it also cannot use non-constituent
phrases. We use the method of Koehn et al. (2003)
to extract phrases, and, for each phrase, we add a
rule with the glue nonterminal as the left-hand side
and the phrase pair as the right-hand side. We exper-
iment to see whether adding phrases is beneficial.
There have been other efforts to extend GHKM
grammar to allow more flexible rule extraction. Gal-
ley et al. (2006) introduce composed rules where
minimal GHKM rules are fused to form larger rules.
Zollmann and Venugopal (2006) introduce a model
that allows more generalized rules to be extracted.
</bodyText>
<sectionHeader confidence="0.997703" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.983778">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999982333333333">
We extracted a GHKM grammar from a Chinese-
English parallel corpus with the English side parsed.
The corpus consists of 250K sentence pairs, which
is 6.3M words on the English side. Terminal-aware
synchronous binarization (Fang et al., 2011) was ap-
plied to all GHKM grammars that are not scope-
filtered. MERT (Och, 2003) was used to tune pa-
rameters. We used a 392-sentence development set
with four references for parameter tuning, and a 428-
sentence test set with four references for testing. Our
in-house decoder was used for experiments with a
trigram language model. The decoder is capable
of both CNF parsing and Earley-style parsing with
cube-pruning (Chiang, 2007).
For the experiment that incorporated phrases, the
phrase pairs were extracted from the same corpus
with the same set of alignments. We have limited
the maximum size of phrases to be four.
</bodyText>
<subsectionHeader confidence="0.925403">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999754285714286">
Our result is summarized in Table 2. The baseline
GHKM grammar with monotonic glue rules yielded
a worse result than the no-unary grammar with the
same glue rules. The difference is statistically signif-
icant at p &lt; 0.05 based on 1000 iterations of paired
bootstrap resampling (Koehn, 2004).
Compared to using monotonic glue rules, using
ABC glue rules brought slight improvements for
both the no-unary setting and the scope-filtered set-
ting, but the differences are not statistically signifi-
cant. In terms of decoding speed and memory usage,
using ABC glues and monotonic glue rules were vir-
tually identical. The fact that glue rules are seldom
used at decoding time may account for why there is
</bodyText>
<page confidence="0.997287">
416
</page>
<bodyText confidence="0.999953214285714">
little difference in using monotonic glue rules and us-
ing ABC glue rules. Out of all the rules that were ap-
plied to decoding our test set, less than one percent
were glue rules, and among the glue rules, straight
glue rules outnumbered inverted ones by three to
one.
Compared with binarized no-unary rules, scope-
3 filtered no-unary rules retained 87% of the rules
but still managed to have slightly better BLEU score.
However, the score difference is not statistically sig-
nificant. Because the size of the grammar is smaller,
compared to using no-unary grammar, it used less
memory at decoding time. However, decoding speed
was somewhat slower. This is because the decoder
employs Early-style dotted rules to handle unbina-
rized rules, and in order to decode with scope-3
rules, the decoder needs to build dotted items, which
are not pruned until a rule is completely matched,
thus leading to slower decoding.
Adding phrases made the translation result
slightly worse. The difference is not statistically
significant. There are two possible explanations for
this. Since there were more features to tune, MERT
may have not done a good job. We believe the
more important reason is that once a phrase is used,
only glue rules can be used to continue the deriva-
tion, thereby losing the richer information offered
by GHKM grammar.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999695266666667">
In this paper, we discussed several issues concerning
decoding with synchronous context-free grammars,
focusing on grammars resulting from the GHKM
extraction method. We discussed different ways to
handle cycles. We presented a modified grammar
extraction scheme that eliminates unary rules. We
also presented a way to decode with unary rules in
the grammar, and examined several different issues
resulting from binarizing SCFGs. We finally dis-
cussed adding flexibility to SCFGs by adding glue
rules and phrases.
Acknowledgments We would like to thank the
anonymous reviewers for their helpful comments.
This work was supported by NSF grants IIS-
0546554 and IIS-0910611.
</bodyText>
<sectionHeader confidence="0.996423" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777230769231">
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL-05, pages 263–270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1–21.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451–455.
Licheng Fang, Tagyoung Chung, and Daniel Gildea.
2011. Terminal-aware synchronous binarization. In
Proceedings of the ACL 2011 Conference Short Pa-
pers, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings ofNAACL-04, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-
rich syntactic translation models. In Proceedings of
COLING/ACL-06, pages 961–968, July.
Mark Hopkins and Greg Langmead. 2010. SCFG decod-
ing without binarization. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 646–655, Cambridge, MA,
October. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceedings
ofNAACL-03.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofNAACL-03, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388–395, Barcelona, Spain, July.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings ofACL-
03.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377–403.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of NAACL-06, pages 256–
263, New York, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. Workshop on Statistical Machine Translation,
pages 138–141.
</reference>
<page confidence="0.997413">
417
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950462">
<title confidence="0.998407">Issues Concerning Decoding with Synchronous Context-free Grammar</title>
<author confidence="0.96">Fang</author>
<affiliation confidence="0.9997755">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.9992487">We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-05,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="733" citStr="Chiang, 2005" startWordPosition="103" endWordPosition="104">nt of Computer Science University of Rochester Rochester, NY 14627 Abstract We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases. 1 Introduction Synchronous context-free grammar (SCFG) is widely used for machine translation. There are many different ways to extract SCFGs from data. Hiero (Chiang, 2005) represents a more restricted form of SCFG, while GHKM (Galley et al., 2004) uses a general form of SCFG. In this paper, we discuss some of the practical issues that arise from decoding general SCFGs that are seldom discussed in the literature. We focus on parsing grammars extracted using the method put forth by Galley et al. (2004), but the solutions to these issues are applicable to other general forms of SCFG with many nonterminals. The GHKM grammar extraction method produces a large number of unary rules. Unary rules are the rules that have exactly one nonterminal and no terminals on the s</context>
<context position="9913" citStr="Chiang, 2005" startWordPosition="1676" endWordPosition="1677">ammars, because of the large number of nonterminals. 4 Adding flexibility 4.1 Glue rules Because of data sparsity, an SCFG extracted from data may fail to parse sentences at test time. For example, consider the following rules: NP → JJ NN, JJ NN JJ → c1, e1 JJ → c2, e2 NN → c3, e3 This set of rules is able to parse the word sequence c1 c3 and c2 c3 but not c1 c2 c3, if we have not seen “NP → JJ JJ NN” at training time. Because SCFGs neither model adjunction, nor are they markovized, with a small amount of data, such problems can occur. Therefore, we may opt to add glue rules as used in Hiero (Chiang, 2005): S → C, C S → S C, S C where S is the goal state and C is the glue nonterminal that can produce any nonterminals. We refer to these glue rules as the monotonic glue rules. We rely on GHKM rules for reordering when we use the monotonic glue rules. However, we can also allow glue rules to reorder constituents. Wu (1997) presents a better-constrained grammar designed to only produce tail-recursive parses. See Table 1 for the complete set of rules. We refer to these rules as ABC glue rules. These rules always generate left415 BLEU Baseline + monotonic glue rules 20.99 No-unary + monotonic glue ru</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL-05, pages 263–270, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="13271" citStr="Chiang, 2007" startWordPosition="2281" endWordPosition="2282">l corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428- sentence test set with four references for testing. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of phrases to be four. 5.2 Results Our result is summarized in Table 2. The baseline GHKM grammar with monotonic glue rules yielded a worse result than the no-unary grammar with the same glue rules. The difference is statistically significant at p &lt; 0.05 based on 1000 iterations of paired bootstrap resampling (Koehn, 2004). Compared to using monotonic glue rules, using ABC glue rules brought slight improvements for both the no-un</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="11236" citStr="Dempster et al., 1977" startWordPosition="1941" endWordPosition="1944">-filtered) + ABC glue rules 24.09 No-unary + ABC glue rules + phrases 23.43 Table 2: BLEU score results for Chinese-English with different settings S S A A S [A B] B S ( B A ) SSB A S [B B] B S ( A A ) SSC A S [C B] B S ( C A ) A S [A C] B S ( B C ) A S [B C] B S ( A C ) A S [C C] B S ( C C ) Table 1: The ABC Grammar. We follow the convention of Wu (1997) that square brackets stand for straight rules and angle brackets stand for inverted rules. heavy derivations, weeding out ambiguity and making search more efficient. We learn probabilities of ABC glue rules by using expectation maximization (Dempster et al., 1977) to train a word-level Inversion Transduction Grammar from data. In our experiments, depending on the configuration, the decoder failed to parse about 5% of sentences without glue rules, which illustrates their necessity. Although it is reasonable to believe that reordering should always have evidence in data, as with GHKM rules, we may wish to reorder based on evidence from the language model. In our experiments, we compare the ABC glue rules with the monotonic glue rules. 4.2 Adding phrases GHKM grammars are more restricted than the phrase extraction methods used in phrase-based models, sinc</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>6</volume>
<issue>8</issue>
<contexts>
<context position="7339" citStr="Earley, 1970" startWordPosition="1210" endWordPosition="1211"> unary productions. This causes unary derivations to go on forever. The solution is to set a maximum length for unary chains, or to ban negative unary productions outright. 3 Issues with binarization 3.1 Filtering and binarization Synchronous binarization (Zhang et al., 2006) is an effective method to reduce SCFG parsing complexity and allow early language model integration. However, it creates virtual nonterminals which require special attention at parsing time. Alternatively, we can filter rules that have more than scope-3 to parse in O(n3) time with unbinarized rules. This requires Earley (Earley, 1970) style parsing, which does implicit binarization at decoding time. Scopefiltering may filter out unnecessarily long rules that may never be applied, but it may also throw out rules with useful contextual information. In addition, scope-filtering does not accommodate early language model state integration. We compare the two with an experiment. For the rest of the section, we discuss issues created by virtual nonterminals. 3.2 Handling virtual nonterminals One aspect of grammar binarization that is rarely mentioned is how to assign probabilities to binarized grammar rules. The naïve solution is</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 6(8):451–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Licheng Fang</author>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Terminal-aware synchronous binarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Conference Short Papers,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="12841" citStr="Fang et al., 2011" startWordPosition="2208" endWordPosition="2211">e experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428- sentence test set with four references for testing. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of </context>
</contexts>
<marker>Fang, Chung, Gildea, 2011</marker>
<rawString>Licheng Fang, Tagyoung Chung, and Daniel Gildea. 2011. Terminal-aware synchronous binarization. In Proceedings of the ACL 2011 Conference Short Papers, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofNAACL-04,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="809" citStr="Galley et al., 2004" startWordPosition="114" endWordPosition="117">stract We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases. 1 Introduction Synchronous context-free grammar (SCFG) is widely used for machine translation. There are many different ways to extract SCFGs from data. Hiero (Chiang, 2005) represents a more restricted form of SCFG, while GHKM (Galley et al., 2004) uses a general form of SCFG. In this paper, we discuss some of the practical issues that arise from decoding general SCFGs that are seldom discussed in the literature. We focus on parsing grammars extracted using the method put forth by Galley et al. (2004), but the solutions to these issues are applicable to other general forms of SCFG with many nonterminals. The GHKM grammar extraction method produces a large number of unary rules. Unary rules are the rules that have exactly one nonterminal and no terminals on the source side. They may be problematic for decoders since they may create cycle</context>
<context position="4440" citStr="Galley et al. (2004)" startWordPosition="702" endWordPosition="705"> the form “NP → NP, the NP” quickly fills a pruning bin with guesses of English words to insert without any source-side lexical evidence. The most obvious way of eliminating problematic unary rules would be converting grammars into Chomsky normal form. However, this may result in bloated grammars. In this section, we present two different ways to handle unary rules. The first involves modifying the grammar extraction method, and the second involves modifying the decoder. 2.1 Modifying grammar extraction We can modify the grammar extraction method such that it does not extract any unary rules. Galley et al. (2004) extracts rules by segmenting the target-side parse parse tree based on frontier nodes. We modify the definition of a frontier node in the following way. We label frontier nodes in the English parse tree, and examine the Chinese span each frontier node covers. If a frontier node covers the same span as the frontier node that immediately dominates it, then the dominated node is no longer considered a frontier. This modification prevents unary rules from being extracted. Figure 1 shows an example of an English-Chinese sentence pair with the English side automatically parsed. Frontier nodes in th</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings ofNAACL-04, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of contextrich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="12394" citStr="Galley et al. (2006)" startWordPosition="2137" endWordPosition="2141">he phrase extraction methods used in phrase-based models, since, in GHKM grammar extraction, phrase segmentation is constrained by parse trees. This may be a good thing, but it suffers from loss of flexibility, and it also cannot use non-constituent phrases. We use the method of Koehn et al. (2003) to extract phrases, and, for each phrase, we add a rule with the glue nonterminal as the left-hand side and the phrase pair as the right-hand side. We experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with f</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of contextrich syntactic translation models. In Proceedings of COLING/ACL-06, pages 961–968, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>SCFG decoding without binarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>646--655</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2050" citStr="Hopkins and Langmead (2010)" startWordPosition="318" endWordPosition="322">nary production chains that contain duplicated dynamic programming states. In later sections, we discuss why unary rules are problematic and investigate two possible solutions. GHKM grammars often have rules with many right-hand-side nonterminals and require binarization to ensure O(n3) time parsing. However, binarization creates a large number of virtual nonterminals. We discuss the challenges of, and possible solutions to, issues arising from having a large number of virtual nonterminals. We also compare binarizing the grammar with filtering rules according to scope, a concept introduced by Hopkins and Langmead (2010). By explicitly considering the effect of anchoring terminals on input sentences, scope3 rules encompass a much larger set of rules than Chomsky normal form but they can still be parsed in O(n3) time. Unlike phrase-based machine translation, GHKM grammars are less flexible in how they can segment sentence pairs into phrases because they are restricted not only by alignments between words in sentence pairs, but also by target-side parse trees. In general, GHKM grammars suffer more from data sparsity than phrasal rules. To alleviate this issue, we discuss adding glue rules and phrases extracted </context>
</contexts>
<marker>Hopkins, Langmead, 2010</marker>
<rawString>Mark Hopkins and Greg Langmead. 2010. SCFG decoding without binarization. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646–655, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL-03.</booktitle>
<contexts>
<context position="8949" citStr="Klein and Manning, 2003" startWordPosition="1493" endWordPosition="1496">air comparison. For our experiments, we use an outside estimate as a heuristic for a virtual item. Consider the following rule binarization (only the source side shown): V → BC : 0 A → BCD : −log(p) � A → VD : − log(p) A → BCD is the orginal rule and − log(p) is the cost of the rule. In decoding time, when a chart item is generated from the binarized rule V → BC, we add − log(p) to its total cost as an optimistic estimate of the cost to build the original unbinarized rule. The heuristic is used only for pruning purposes, and it does not change the real cost. The idea is similar to A* parsing (Klein and Manning, 2003). One complication is that a binarized rule can arise from multiple different unbinarized rules. In this case, we pick the lowest cost among the unbinarized rules as the heuristic. Another approach for handling virtual nonterminals would be giving virtual items separate bins and avoiding pruning them at all. This is usually not practical for GHKM grammars, because of the large number of nonterminals. 4 Adding flexibility 4.1 Glue rules Because of data sparsity, an SCFG extracted from data may fail to parse sentences at test time. For example, consider the following rules: NP → JJ NN, JJ NN JJ </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proceedings ofNAACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL-03,</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="12073" citStr="Koehn et al. (2003)" startWordPosition="2081" endWordPosition="2084">ty. Although it is reasonable to believe that reordering should always have evidence in data, as with GHKM rules, we may wish to reorder based on evidence from the language model. In our experiments, we compare the ABC glue rules with the monotonic glue rules. 4.2 Adding phrases GHKM grammars are more restricted than the phrase extraction methods used in phrase-based models, since, in GHKM grammar extraction, phrase segmentation is constrained by parse trees. This may be a good thing, but it suffers from loss of flexibility, and it also cannot use non-constituent phrases. We use the method of Koehn et al. (2003) to extract phrases, and, for each phrase, we add a rule with the glue nonterminal as the left-hand side and the phrase pair as the right-hand side. We experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofNAACL-03, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="13762" citStr="Koehn, 2004" startWordPosition="2363" endWordPosition="2364">gram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of phrases to be four. 5.2 Results Our result is summarized in Table 2. The baseline GHKM grammar with monotonic glue rules yielded a worse result than the no-unary grammar with the same glue rules. The difference is statistically significant at p &lt; 0.05 based on 1000 iterations of paired bootstrap resampling (Koehn, 2004). Compared to using monotonic glue rules, using ABC glue rules brought slight improvements for both the no-unary setting and the scope-filtered setting, but the differences are not statistically significant. In terms of decoding speed and memory usage, using ABC glues and monotonic glue rules were virtually identical. The fact that glue rules are seldom used at decoding time may account for why there is 416 little difference in using monotonic glue rules and using ABC glue rules. Out of all the rules that were applied to decoding our test set, less than one percent were glue rules, and among t</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL03.</booktitle>
<contexts>
<context position="12919" citStr="Och, 2003" startWordPosition="2225" endWordPosition="2226">s to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428- sentence test set with four references for testing. Our in-house decoder was used for experiments with a trigram language model. The decoder is capable of both CNF parsing and Earley-style parsing with cube-pruning (Chiang, 2007). For the experiment that incorporated phrases, the phrase pairs were extracted from the same corpus with the same set of alignments. We have limited the maximum size of phrases to be four. 5.2 Results Our result is summarized in Table 2. The basel</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings ofACL03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="10233" citStr="Wu (1997)" startWordPosition="1743" endWordPosition="1744">d sequence c1 c3 and c2 c3 but not c1 c2 c3, if we have not seen “NP → JJ JJ NN” at training time. Because SCFGs neither model adjunction, nor are they markovized, with a small amount of data, such problems can occur. Therefore, we may opt to add glue rules as used in Hiero (Chiang, 2005): S → C, C S → S C, S C where S is the goal state and C is the glue nonterminal that can produce any nonterminals. We refer to these glue rules as the monotonic glue rules. We rely on GHKM rules for reordering when we use the monotonic glue rules. However, we can also allow glue rules to reorder constituents. Wu (1997) presents a better-constrained grammar designed to only produce tail-recursive parses. See Table 1 for the complete set of rules. We refer to these rules as ABC glue rules. These rules always generate left415 BLEU Baseline + monotonic glue rules 20.99 No-unary + monotonic glue rules 23.83 No-unary + ABC glue rules 23.94 No-unary (scope-filtered) + monotonic 23.99 No-unary (scope-filtered) + ABC glue rules 24.09 No-unary + ABC glue rules + phrases 23.43 Table 2: BLEU score results for Chinese-English with different settings S S A A S [A B] B S ( B A ) SSB A S [B B] B S ( A A ) SSC A S [C B] B S</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-06,</booktitle>
<pages>256--263</pages>
<location>New York, NY.</location>
<contexts>
<context position="7002" citStr="Zhang et al., 2006" startWordPosition="1156" endWordPosition="1159">m: if all unary derivations have positive costs and a priority queue is used to expand unary derivations, NN snowy egret n鹭 鸶 的 爱 414 only the best K unary items will be generated, where K is the pruning constant. • Ban negative cost unary rules When tuning feature weights, an optimizer may try feature weights that may give negative costs to unary productions. This causes unary derivations to go on forever. The solution is to set a maximum length for unary chains, or to ban negative unary productions outright. 3 Issues with binarization 3.1 Filtering and binarization Synchronous binarization (Zhang et al., 2006) is an effective method to reduce SCFG parsing complexity and allow early language model integration. However, it creates virtual nonterminals which require special attention at parsing time. Alternatively, we can filter rules that have more than scope-3 to parse in O(n3) time with unbinarized rules. This requires Earley (Earley, 1970) style parsing, which does implicit binarization at decoding time. Scopefiltering may filter out unnecessarily long rules that may never be applied, but it may also throw out rules with useful contextual information. In addition, scope-filtering does not accommod</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of NAACL-06, pages 256– 263, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="12506" citStr="Zollmann and Venugopal (2006)" startWordPosition="2155" endWordPosition="2158">mentation is constrained by parse trees. This may be a good thing, but it suffers from loss of flexibility, and it also cannot use non-constituent phrases. We use the method of Koehn et al. (2003) to extract phrases, and, for each phrase, we add a rule with the glue nonterminal as the left-hand side and the phrase pair as the right-hand side. We experiment to see whether adding phrases is beneficial. There have been other efforts to extend GHKM grammar to allow more flexible rule extraction. Galley et al. (2006) introduce composed rules where minimal GHKM rules are fused to form larger rules. Zollmann and Venugopal (2006) introduce a model that allows more generalized rules to be extracted. 5 Experiments 5.1 Setup We extracted a GHKM grammar from a ChineseEnglish parallel corpus with the English side parsed. The corpus consists of 250K sentence pairs, which is 6.3M words on the English side. Terminal-aware synchronous binarization (Fang et al., 2011) was applied to all GHKM grammars that are not scopefiltered. MERT (Och, 2003) was used to tune parameters. We used a 392-sentence development set with four references for parameter tuning, and a 428- sentence test set with four references for testing. Our in-house</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. Workshop on Statistical Machine Translation, pages 138–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>