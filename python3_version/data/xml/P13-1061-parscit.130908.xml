<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.836336">
Non-Monotonic Sentence Alignment via Semisupervised Learning
</title>
<author confidence="0.97626">
Xiaojun Quan, Chunyu Kit and Yan Song
</author>
<affiliation confidence="0.9935625">
Department of Chinese, Translation and Linguistics
City University of Hong Kong, HKSAR, China
</affiliation>
<email confidence="0.753745">
{xiaoquan,ctckit,[yansong]}@[student.]cityu.edu.hk
</email>
<sectionHeader confidence="0.988917" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903619047619">
This paper studies the problem of non-
monotonic sentence alignment, motivated
by the observation that coupled sentences
in real bitexts do not necessarily occur
monotonically, and proposes a semisuper-
vised learning approach based on two as-
sumptions: (1) sentences with high affinity
in one language tend to have their counter-
parts with similar relatedness in the other;
and (2) initial alignment is readily avail-
able with existing alignment techniques.
They are incorporated as two constraints
into a semisupervised learning framework
for optimization to produce a globally op-
timal solution. The evaluation with real-
world legal data from a comprehensive
legislation corpus shows that while exist-
ing alignment algorithms suffer severely
from non-monotonicity, this approach can
work effectively on both monotonic and
non-monotonic data.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967155172415">
Bilingual sentence alignment is a fundamental
task to undertake for the purpose of facilitating
many important natural language processing ap-
plications such as statistical machine translation
(Brown et al., 1993), bilingual lexicography (Kla-
vans et al., 1990), and cross-language informa-
tion retrieval (Nie et al., 1999). Its objective is to
identify correspondences between bilingual sen-
tences in given bitexts. As summarized by Wu
(2010), existing sentence alignment techniques
rely mainly on sentence length and bilingual lex-
ical resource. Approaches based on the former
perform effectively on cognate languages but not
on the others. For instance, the statistical cor-
relation of sentence length between English and
Chinese is not as high as that between two Indo-
European languages (Wu, 1994). Lexicon-based
approaches resort to word correspondences in a
bilingual lexicon to match bilingual sentences. A
few sentence alignment methods and tools have
also been explored to combine the two. Moore
(2002) proposes a multi-pass search procedure us-
ing both sentence length and an automatically-
derived bilingual lexicon. Hunalign (Varga et al.,
2005) is another sentence aligner that combines
sentence length and a lexicon. Without a lexicon,
it backs off to a length-based algorithm and then
automatically derives a lexicon from the align-
ment result. Soon after, Ma (2006) develops the
lexicon-based aligner Champollion, assuming that
different words have different importance in align-
ing two sentences.
Nevertheless, most existing approaches to sen-
tence alignment follow the monotonicity assump-
tion that coupled sentences in bitexts appear in
a similar sequential order in two languages and
crossings are not entertained in general (Langlais
et al., 1998; Wu, 2010). Consequently the task of
sentence alignment becomes handily solvable by
means of such basic techniques as dynamic pro-
gramming. In many scenarios, however, this pre-
requisite monotonicity cannot be guaranteed. For
example, bilingual clauses in legal bitexts are of-
ten coordinated in a way not to keep the same
clause order, demanding fully or partially crossing
pairings. Figure 1 shows a real excerpt from a leg-
islation corpus. Such monotonicity seriously im-
pairs the existing alignment approaches founded
on the monotonicity assumption.
This paper is intended to explore the problem of
non-monotonic alignment within the framework
of semisupervised learning. Our approach is mo-
tivated by the above observation and based on
the following two assumptions. First, monolin-
gual sentences with high affinity are likely to have
their translations with similar relatedness. Follow-
ing this assumption, we propose the conception
of monolingual consistency which, to the best of
</bodyText>
<page confidence="0.966138">
622
</page>
<note confidence="0.8206365">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 622–630,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
&amp;quot;
1. Interpretation of words and expressions
</note>
<bodyText confidence="0.967561">
British citizen&amp;quot; (劙⚳℔ 㮹) means a person who has the status of a
British citizen under the British Nationality Act 1981 (1981 c. 61 U.K.)
&amp;quot;British Dependent Territories citizen&amp;quot; (������) means a person
who has or had the status of a British Dependent Territories citizen
under the British Nationality Act 1981 (1981 c. 61 U.K.)
&amp;quot;British enactment&amp;quot; and &amp;quot;imperial enactment&amp;quot; (������) Mean-
(a) any Act of Parliament; (b) any Order in Council; and (c) any rule,
regulation, proclamation, order, notice, rule of court, by-law or other
instrument made under or by virtue of any such Act or Order in Council
British Overseas citizen&amp;quot; (1WWF�) means a person who has the
status of a British Overseas citizen under the British Nationality Act
1981 (1981 c. 61 U.K.)
British protected person&amp;quot; (�������) means a person who has
the status of a British protected person under the British Nationality Act
1981 (1981 c. 61 U.K.)
</bodyText>
<equation confidence="0.747048928571428">
1.娆婆␴娆⎍䘬慳佑
&amp;quot; ⍿劙⚳ᾅ嬟Ṣ⢓ȿ(British protected person) ㊯㟡㒂˪1981⸜劙⚳⚳
䯵㱽Ẍ˫(1981 c. 61 �.�.)℟㚱⍿劙⚳ᾅ嬟Ṣ⢓幓↮䘬Ṣ
&amp;quot; 劙⚳℔㮹ȿ(British citizen) ㊯㟡㒂˪1981��������(1981
c. 61 U.K.)℟㚱劙⚳℔㮹幓↮䘬Ṣ
&amp;quot;劙⚳ㆸ㔯 &amp;quot; (British enactment, imperial enactment) fe
M —(a)gNr
⚳㚫忂忶䘬㱽Ẍ烊(b)ảỽ㧆⭮昊枺Ẍ烊⍲(c)㟡㒂ㆾㄹ啱ảỽ娚䫱
㱽Ẍㆾ㧆⭮昊枺Ẍ侴妪䩳䘬ảỽ夷⇯ˣ夷ἳˣ㔯⏲ˣ␥Ẍˣ℔
⏲ˣ㱽昊夷⇯ˣ旬ἳㆾ℞Ṿ㔯㚠
&amp;quot;2WWF$ �&amp;quot;(British Overseas citizen) M㟡㒂˪198144,�WWLI
&amp;quot;)(1981 c. 61 U.K.)℟㚱劙⚳㴟⢾℔㮹幓↮䘬Ṣ
&amp;quot; 劙⚳Ⱄ⛇℔㮹ȿ(British Dependent Territories citizen) ㊯ft˪1981
44,�WWR&amp;quot;)(1981 c. 61 U.K.)℟㚱ㆾ㚦℟㚱劙⚳Ⱄ⛇℔㮹幓
</equation>
<figure confidence="0.90185825">
↮nik
&amp;quot;
&amp;quot;
... ...
</figure>
<figureCaption confidence="0.999933">
Figure 1: A real example of non-monotonic sentence alignment from BLIS corpus.
</figureCaption>
<bodyText confidence="0.998852666666667">
our knowledge, has not been taken into account in
any previous work of alignment. Second, initial
alignment of certain quality can be obtained by
means of existing alignment techniques. Our ap-
proach attempts to incorporate both monolingual
consistency of sentences and bilingual consistency
of initial alignment into a semisupervised learning
framework to produce an optimal solution. Ex-
tensive evaluations are performed using real-world
legislation bitexts from BLIS, a comprehensive
legislation database maintained by the Depart-
ment of Justice, HKSAR. Our experimental results
show that the proposed method can work effec-
tively while two representatives of existing align-
ers suffer severely from the non-monotonicity.
</bodyText>
<sectionHeader confidence="0.991519" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.966021">
2.1 The Problem
</subsectionHeader>
<bodyText confidence="0.9997206">
An alignment algorithm accepts as input a bi-
text consisting of a set of source-language sen-
tences, S = {s1, s2, ... , sm}, and a set of target-
language sentences, T = {t1, t2, ... , tn}. Dif-
ferent from previous works relying on the mono-
tonicity assumption, our algorithm is generalized
to allow the pairings of sentences in S and T
to cross arbitrarily. Figure 2(a) illustrates mono-
tonic alignment with no crossing correspondences
in a bipartite graph and 2(b) non-monotonic align-
ment with scrambled pairings. Note that it is rela-
tively straightforward to identify the type of many-
to-many alignment in monotonic alignment using
techniques such as dynamic programming if there
is no scrambled pairing or the scrambled pairings
are local, limited to a short distance. However,
the situation of non-monotonic alignment is much
more complicated. Sentences to be merged into a
bundle for matching against another bundle in the
other language may occur consecutively or discon-
tinuously. For the sake of simplicity, we will not
consider non-monotonic alignment with many-to-
many pairings but rather assume that each sen-
tence may align to only one or zero sentence in
the other language.
Let F represent the correspondence relation be-
tween S and T, and therefore F C S x T. Let
matrix F denote a specific alignment solution of
F, where Fij is a real score to measure the likeli-
hood of matching the i-th sentence si in S against
the j-th sentence tj in T. We then define an align-
ment function A : F → A to produce the final
alignment, where A is the alignment matrix for S
and T, with Aij = 1 for a correspondence be-
tween si and tj and Aij = 0 otherwise.
</bodyText>
<subsectionHeader confidence="0.999573">
2.2 Semisupervised Learning
</subsectionHeader>
<bodyText confidence="0.999833">
A semisupervised learning framework is intro-
duced to incorporate the monolingual and bilin-
gual consistency into alignment scoring
</bodyText>
<equation confidence="0.99946">
Q(F) = Qm(F) + λQb(F), (1)
</equation>
<bodyText confidence="0.999961571428571">
where Qm(F) is the term for monolingual con-
straint to control the consistency of sentences with
high affinities, Qb(F) for the constraint of initial
alignment obtained with existing techniques, and
λ is the weight between them. Then, the optimal
alignment solution is to be derived by minimizing
the cost function Q(F), i.e.,
</bodyText>
<equation confidence="0.825657827586207">
Q(F). (2)
F∗ = arg min
F
623
s1
s2
s3
s4
s5
s6
s1
s2
s3
s4
s5
s6
t1
t2
t3
t4
t5
t6
t1
t2
t3
t4
t5
t6
(a) (b)
</equation>
<figureCaption confidence="0.972794">
Figure 2: Illustration of monotonic (a) and non-monotonic alignment (b), with a line representing the
correspondence of two bilingual sentences.
</figureCaption>
<bodyText confidence="0.949151">
In this paper, Qm(F) is defined as
</bodyText>
<equation confidence="0.981715">
2
V Fik − Fjl , (3)
kl √ DiiEkk V/DjjEll
</equation>
<bodyText confidence="0.982711884615385">
where W and V are the symmetric matrices to rep-
resent the monolingual sentence affinity matrices
in S and T, respectively, and D and E are the di-
agonal matrices with entries Dii = Ej Wij and
Eii = E j Vij. The idea behind (3) is that to min-
imize the cost function, the translations of those
monolingual sentences with close relatedness re-
flected in W and V should also keep similar close-
ness. The bilingual constraint term Qb(F) is de-
fined as
� �2
Fij − ˆAij , (4)
where Aˆ is the initial alignment matrix obtained
by A : Fˆ → ˆA. Note that Fˆ is the initial relation
matrix between S and T.
The monolingual constraint term Qm(F) de-
fined above corresponds to the smoothness con-
straint in the previous semisupervised learning
work by Zhou et al. (2004) that assigns higher
likelihood to objects with larger similarity to share
the same label. On the other hand, Qb(F) corre-
sponds to their fitting constraint, which requires
the final alignment to maintain the maximum con-
sistency with the initial alignment.
Taking the derivative of Q(F) with respect to
F, we have
</bodyText>
<equation confidence="0.985272">
∂Q(F)
∂F = 2F − 2SFT + 2λF − 2λ ˆA, (5)
</equation>
<bodyText confidence="0.8689365">
where S and T are the normalized matrices of W
and V , calculated by S = D−1/2WD−1/2 and
T = E−1/2V E−1/2. Then, the optimal F* is to
be found by solving the equation
</bodyText>
<equation confidence="0.99937">
(1 + λ) F* − SF*T = λ ˆA, (6)
</equation>
<bodyText confidence="0.9847672">
which is equivalent to αF* − F*β = γ with
α = (1 + λ) S−1, β = T and γ = λS−1 ˆA.
This is in fact a Sylvester equation (Barlow et al.,
1992), whose numerical solution can be found by
many classical algorithms. In this research, it is
solved using LAPACK,1 a software library for nu-
merical linear algebra. Non-positive entries in F*
indicate unrealistic correspondences of sentences
and are thus set to zero before applying the align-
ment function.
</bodyText>
<subsectionHeader confidence="0.999429">
2.3 Alignment Function
</subsectionHeader>
<bodyText confidence="0.999917947368421">
Once the optimal F* is acquired, the remaining
task is to design an alignment function A to con-
vert it into an alignment solution. An intuitive ap-
proach is to use a heuristic search for local op-
timization (Kit et al., 2004), which produces an
alignment with respect to the largest scores in
each row and each column. However, this does not
guarantee a globally optimal solution. Figure 3 il-
lustrates a mapping relation matrix onto an align-
ment matrix, which also shows that the optimal
alignment cannot be achieved by heuristic search.
Banding is another approach frequently used to
convert a relation matrix to alignment (Kay and
R¨oscheisen, 1993). It is founded on the observa-
tion that true monotonic alignment paths usually
lie close to the diagonal of a relation matrix. How-
ever, it is not applicable to our task due to the non-
monotonicity involved. We opt for converting a
relation matrix into specific alignment by solving
</bodyText>
<footnote confidence="0.554876">
1http://www.netlib.org/lapack/
</footnote>
<equation confidence="0.874134571428571">
1 �m Wij n
4 i,j=1 k,l=1
m
i=1
Qb(F) =
n
j=1
</equation>
<page confidence="0.93512">
624
</page>
<figure confidence="0.999524222222222">
1
2
1
0
0
2
0.3
0
3
0
4
0
0.4
0.5
alignment matrix
relation matrix
0
5
0
0.1
6
1 2 3 4 5 6 7
1 1 0 0 0 0 0 0
2 0 0 0 1 0 0 0
3 0 0 0 0 0 0 0
4 0 0 0 0 1 0 0
5 0 1 0 0 0 0 0
6 0 0 0 0 0 1 0
3 4 5 6 7
0 0.5 0 0 0
0 0.6 0 0 0
0 0 0 0 0
0 0 0.2 0 0
0 0 0 0.6 0
0 0 0 0.8 0
0.4
</figure>
<figureCaption confidence="0.97073">
Figure 3: Illustration of sentence alignment from relation matrix to alignment matrix. The scores marked
with arrows are the best in each row/column to be used by the heuristic search. The right matrix repre-
sents the corresponding alignment matrix by our algorithm.
</figureCaption>
<bodyText confidence="0.693541">
the following optimization
</bodyText>
<equation confidence="0.9985522">
A = arg max �m n XijFij (7)
X i=1 j=1
m n
s.t. Xij G 1, Xij G 1, Xij E 10, 11
i=1 j=1
</equation>
<bodyText confidence="0.9999084">
This turns sentence alignment into a problem to
be resolved by binary linear programming (BIP),
which has been successfully applied to word align-
ment (Taskar et al., 2005). Given a scoring matrix,
it guarantees an optimal solution.
</bodyText>
<subsectionHeader confidence="0.988364">
2.4 Alignment Initialization
</subsectionHeader>
<bodyText confidence="0.999995142857143">
Once the above alignment function is available,
the initial alignment matrix Aˆ can be derived from
an initial relation matrix Fˆ obtained by an avail-
able alignment method. This work resorts to an-
other approach to initializing the relation matrix.
In many genres of bitexts, such as government
transcripts or legal documents, there are a certain
number of common strings on the two sides of bi-
texts. In legal documents, for example, transla-
tions of many key terms are usually accompanied
with their source terms. Also, common number-
ings can be found in enumerated lists in bitexts.
These kinds of anchor strings provide quite reli-
able information to link bilingual sentences into
pairs, and thus can serve as useful cues for sen-
tence alignment. In fact, they can be treated as a
special type of highly reliable “bilexicon”.
The anchor strings used in this work are derived
by searching the bitexts using word-level inverted
indexing, a basic technique widely used in infor-
mation retrieval (Baeza-Yates and Ribeiro-Neto,
2011). For each index term, a list of postings is
created. Each posting includes a sentence identi-
fier, the in-sentence frequency and positions of this
term. The positions of terms are intersected to find
common anchor strings. The anchor strings, once
found, are used to calculate the initial affinity ˆFij
of two sentences using Dice’s coefficient
</bodyText>
<equation confidence="0.999819">
ˆFij = 2|C1i n C2j |(8)
|C1i |+ |C2j|
</equation>
<bodyText confidence="0.999943333333333">
where C1i and C2j are the anchor sets in si and tj,
respectively, and  |·  |is the cardinality of a set.
Apart from using anchor strings, other avenues
for the initialization are studied in the evaluation
section below, i.e., using another aligner and an
existing lexicon.
</bodyText>
<subsectionHeader confidence="0.98492">
2.5 Monolingual Affinity
</subsectionHeader>
<bodyText confidence="0.998164588235294">
Although various kinds of information from a
monolingual corpus have been exploited to boost
statistical machine translation models (Liu et al.,
2010; Su et al., 2012), we have not yet been
exposed to any attempt to leverage monolingual
sentence affinity for sentence alignment. In our
framework, an attempt to this can be made through
the computation of W and V . Let us take W as an
example, where the entry Wij represents the affin-
ity of sentence si and sentence sj, and it is set to
0 for i = j in order to avoid self-reinforcement
during optimization (Zhou et al., 2004).
When two sentences in S or T are not too short,
or their content is not divergent in meaning, their
semantic similarity can be estimated in terms of
common words. Motivated by this, we define Wij
(for i =� j) based on the Gaussian kernel as
</bodyText>
<equation confidence="0.9627365">
T j 2
Wij = exp −2σ2 C1 Ilvvl lvj I J (9)
</equation>
<page confidence="0.974952">
625
</page>
<bodyText confidence="0.9999598">
where Q is the standard deviation parameter, vi
and vj are vectors of si and sj with each com-
ponent corresponding to the tf-idf value of a par-
ticular term in S (or T), and II·II is the norm of
a vector. The underlying assumption here is that
words appearing frequently in a small number of
sentences but rarely in the others are more signifi-
cant in measuring sentence affinity.
Although semantic similarity estimation is a
straightforward approach to deriving the two affin-
ity matrices, other approaches are also feasible. An
alternative approach can be based on sentence
length under the assumption that two sentences
with close lengths in one language tend to have
their translations also with close lengths.
</bodyText>
<subsectionHeader confidence="0.800324">
2.6 Discussion
</subsectionHeader>
<bodyText confidence="0.9999941">
The proposed semisupervised framework for non-
monotonic alignment is in fact generalized be-
yond, and can also be applied to, monotonic align-
ment. Towards this, we need to make use of sen-
tence sequence information. One way to do it is
to incorporate sentence positions into Equation (1)
by introducing a position constraint Qp(F) to en-
force that bilingual sentences in closer positions
should have a higher chance to match one another.
For example, the new constraint can be defined as
</bodyText>
<equation confidence="0.989320333333333">
Qp(F) = �m n 2
i=1 j=1 |pi − qj|Fij
,
</equation>
<bodyText confidence="0.999089818181818">
where pi and qj are the absolute (or relative) posi-
tions of two bilingual sentences in their respective
sequences. Another way follows the banding as-
sumption that the actual couplings only appear in
a narrow band along the main diagonal of relation
matrix. Accordingly, all entries of F∗ outside this
band are set to zero before the alignment function
is applied. Kay and R¨oscheisen (1993) illustrate
that this can be done by modeling the maximum
deviation of true couplings from the diagonal as
O(√n).
</bodyText>
<sectionHeader confidence="0.998328" genericHeader="background">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99858">
3.1 Data Set
</subsectionHeader>
<bodyText confidence="0.9998535">
Our data set is acquired from the Bilingual
Laws Information System (BLIS),2 an electronic
database of Hong Kong legislation maintained
by the Department of Justice, HKSAR. BLIS
</bodyText>
<footnote confidence="0.731357">
2http://www.legislation.gov.hk
</footnote>
<bodyText confidence="0.99954734883721">
provides Chinese-English bilingual texts of ordi-
nances and subsidiary legislation in effect on or af-
ter 30 June 1997. It organizes the legal texts into a
hierarchy of chapters, sections, subsections, para-
graphs and subparagraphs, and displays the con-
tent of a such hierarchical construct (usually a sec-
tion) on a single web page.
By web crawling, we have collected in total
31,516 English and 31,405 Chinese web pages,
forming a bilingual corpus of 31,401 bitexts after
filtering out null pages. A text contains several to
two hundred sentences. Many bitexts exhibit par-
tially non-monotonic order of sentences. Among
them, 175 bitexts are randomly selected for man-
ual alignment. Sentences are identified based on
punctuations. OpenNLP Tokenizer3 is applied to
segment English sentences into tokens. For Chi-
nese, since there is no reliable segmenter for this
genre of text, we have to treat each Chinese char-
acter as a single token. In addition, to calculate the
monolingual sentence affinity, stemming of En-
glish words is performed with the Porter Stemmer
(Porter, 1980) after anchor string mining.
The manual alignment of the evaluation data set
is performed upon the initial alignment by Hu-
nalign (Varga et al., 2005), an effective sentence
aligner that uses both sentence length and a bilex-
icon (if available). For this work, Hunalign re-
lies solely on sentence length. Its output is then
double-checked and corrected by two experts in
bilingual studies, resulting in a data set of 1747
1-1 and 70 1-0 or 0-1 sentence pairs.
The standard deviation Q in (9) is an important
parameter for the Gaussian kernel that has to be
determined empirically (Zhu et al., 2003; Zhou et
al., 2004). In addition, the Q function also involves
another parameter A to adjust the weight of the
bilingual constraint. This work seeks an approach
to deriving the optimal parameters without any ex-
ternal training data beyond the initial alignment. A
three-fold cross-validation is thus performed on
the initial 1-1 alignment and the parameters that
give the best average performance are chosen.
</bodyText>
<subsectionHeader confidence="0.999809">
3.2 Monolingual Consistency
</subsectionHeader>
<bodyText confidence="0.861110666666667">
To demonstrate the validity of the monolingual
consistency, the semantic similarity defined by
vi vj is evaluated as follows. 500 pairs of En-
liviNIvA
glish sentences with the highest similarities are se-
lected, excluding null pairings (1-0 or 0-1 type).
</bodyText>
<footnote confidence="0.954061">
3http://opennlp.apache.org/
</footnote>
<page confidence="0.996562">
626
</page>
<figure confidence="0.997697636363636">
1
0.95
0.9
Similarity of Chinese sentence pair 0.85
0.8
0.75
0.7
0.65
0.6
0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
Similarity of English sentence pair
</figure>
<figureCaption confidence="0.79654125">
Figure 4: Demonstration of monolingual consis-
tency. The horizontal axis is the similarity of En-
glish sentence pairs and the vertical is the similar-
ity of the corresponding pairs in Chinese.
</figureCaption>
<table confidence="0.998955">
Type Total initAlign NonmoAlign
Pred Corr Pred Corr
1-0 70 662 66 70 50
1-1 1747 1451 1354 1747 1533
</table>
<tableCaption confidence="0.997081">
Table 1: Performance of the initial alignment and
</tableCaption>
<bodyText confidence="0.994386266666667">
our aligner, where the Pred and Corr columns are
the numbers of predicted and correct pairings.
All of these high-affinity pairs have a similarity
score higher than 0.72. A number of duplicate
sentences (e.g., date) with exceptionally high sim-
ilarity 1.0 are dropped. Also, the similarity of the
corresponding translations of each selected pair
is calculated. These two sets of similarity scores
are then plotted in a scatter plot, as in Figure 4.
If the monolingual consistency assumption holds,
the plotted points would appear nearby the diag-
onal. Figure 4 confirms this, indicating that sen-
tence pairs with high affinity in one language do
have their counterparts with similarly high affinity
in the other language.
</bodyText>
<subsectionHeader confidence="0.999763">
3.3 Impact of Initial Alignment
</subsectionHeader>
<bodyText confidence="0.9998547">
The 1-1 initial alignment plays the role of labeled
instances for the semisupervised learning. It is
of critical importance to the learning performance.
As shown in Table 1, our alignment function pre-
dicts 1451 1-1 pairings by virtue of anchor strings,
among which 1354 pairings are correct, yielding
a relatively high precision in the non-monotonic
circumstance. It also predicts null alignment for
many sentences that contain no anchor. This ex-
plains why it outputs 662 1-0 pairings when there
</bodyText>
<figure confidence="0.551453">
Percentage of initial 1−1 alignment
</figure>
<figureCaption confidence="0.8438815">
Figure 5: Performance of non-monotonic align-
ment along the percentage of initial 1-1 alignment.
</figureCaption>
<bodyText confidence="0.99832495">
are only 70 1-0 true ones. Starting from this initial
alignment, our aligner (let us call it NonmoAlign)
discovers 179 more 1-1 pairings.
A question here is concerned with how the scale
of initial alignment affects the final alignment. To
examine this, we randomly select 20%, 40%, 60%
and 80% of the 1451 1-1 detected pairings as the
initial alignments for a series of experiments. The
random selection for each proportion is performed
ten times and their average alignment performance
is taken as the final result and plotted in Figure 5.
An observation from this figure is that the aligner
consistently discovers significantly more 1-1 pair-
ings on top of an initial 1-1 alignment, which has
to be accounted for by the monolingual consis-
tency. Another observation is that the alignment
performance goes up along the increase of the
percentage of initial alignment while performance
gain slows down gradually. When the percentage
is very low, the aligner still works quite effectively.
</bodyText>
<subsectionHeader confidence="0.89361">
3.4 Non-Monotonic Alignment
</subsectionHeader>
<bodyText confidence="0.996754285714286">
To test our aligner with non-monotonic sequences
of sentences, we have them randomly scrambled
in our experimental data. This undoubtedly in-
creases the difficulty of sentence alignment, espe-
cially for the traditional approaches critically rely-
ing on monotonicity.
The baseline methods used for comparison are
Moore’s aligner (Moore, 2002) and Hunalign
(Varga et al., 2005). Hunalign is configured with
the option [-realign], which triggers a three-step
procedure: after an initial alignment, Hunalign
heuristically enriches its dictionary using word co-
occurrences in identified sentence pairs; then, it
re-runs the alignment process using the updated
</bodyText>
<figure confidence="0.993640769230769">
1600
1400
1200
1000
800
600
400
200
0
20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100%
NonmoAlign
initAlign
Correctly detected 1−1 pairings
</figure>
<page confidence="0.974951">
627
</page>
<table confidence="0.999679">
Type Moore Hunalign NonmoAlign
P R F1 P R F1 P R F1
1-1 0.104 0.104 0.104 0.407 0.229 0.293 0.878 0.878 0.878
1-0 0.288 0.243 0.264 0.033 0.671 0.062 0.714 0.714 0.714
Micro 0.110 0.110 0.110 0.184 0.246 0.210 0.871 0.871 0.871
</table>
<tableCaption confidence="0.99925">
Table 2: Performance comparison with the baseline methods.
</tableCaption>
<bodyText confidence="0.99978">
dictionary. According to Varga et al (2005), this
setting gives a higher alignment quality than oth-
erwise. In addition, Hunalign can use an external
bilexicon. For a fair comparison, the identified an-
chor set is fed to Hunalign as a special bilexicon.
The performance of alignment is measured by pre-
cision (P), recall (R) and F-measure (F1). Micro-
averaged performance scores of precision, recall
and F-measure are also computed to measure the
overall performance on 1-1 and 1-0 alignment.
The final results are presented in Table 2, show-
ing that both Moore’s aligner and Hunalign under-
perform ours on non-monotonic alignment. The
particularly poor performance of Moore’s aligner
has to be accounted for by its requirement of more
than thousands of sentences in bitext input for re-
liable estimation of its parameters. Unfortunately,
our available data has not reached that scale yet.
</bodyText>
<subsectionHeader confidence="0.98539">
3.5 Partially Non-Monotonic Alignment
</subsectionHeader>
<bodyText confidence="0.999971956521739">
Full non-monotonic bitexts are rare in practice.
But partial non-monotonic ones are not. Unlike
traditional alignment approaches, ours does not
found its performance on the degree of monotonic-
ity. To test this, we construct five new versions of
the data set for a series of experiments by ran-
domly choosing and scrambling 0%, 10%, 20%,
40%, 60% and 80% sentence parings. In the-
ory, partial non-monotonicity of various degrees
should have no impact on the performance of our
aligner. It is thus not surprised that it achieves
the same result as reported in last subsection.
NonmoAlign initialized with Hunalign (marked
as NonmoAlign Hun) is also tested. The experi-
mental results are presented in Figure 6. It shows
that both Moore’s aligner and Hunalign work rel-
atively well on bitexts with a low degree of non-
monotonicity, but their performance drops dra-
matically when the non-monotonicity is increased.
Despite the improvement at low non-monotonicity
by seeding our aligner with Hunalign, its per-
formance decreases likewise when the degree of
non-monotonicity increases, due to the quality de-
</bodyText>
<figure confidence="0.560401">
Non−monotonic ratio
</figure>
<figureCaption confidence="0.9985095">
Figure 6: Performance of alignment approaches at
different degrees of non-monotonicity.
</figureCaption>
<bodyText confidence="0.986235">
crease of the initial alignment by Hunalign.
</bodyText>
<subsectionHeader confidence="0.989205">
3.6 Monotonic Alignment
</subsectionHeader>
<bodyText confidence="0.999986533333333">
The proposed alignment approach is also expected
to work well on monotonic sentence alignment.
An evaluation is conducted for this using a mono-
tonic data set constructed from our data set by
discarding all its 126 crossed pairings. Of the
two strategies discussed above, banding is used
to help our aligner incorporate the sequence in-
formation. The initial relation matrix is built with
the aid of a dictionary automatically derived by
Hunalign. Entries of the matrix are derived by
employing a similar strategy as in Varga et al.
(2005). The evaluation results are presented in Ta-
ble 3, which shows that NonmoAlign still achieves
very competitive performance on monotonic sen-
tence alignment.
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999452571428571">
The research of sentence alignment originates in
the early 1990s. Gale and Church (1991) and
Brown (1991) report the early works using length
statistics of bilingual sentences. The general idea
is that the closer two sentences are in length, the
more likely they are to align. A notable difference
of their methods is that the former uses sentence
</bodyText>
<figure confidence="0.9988860625">
Micro−F1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0 % 10% 20% 30% 40% 50% 60% 70% 80%
1.1
1
NonmoAlign
Hunalign
Moore
NonmoAlign_Hun
</figure>
<page confidence="0.984088">
628
</page>
<table confidence="0.9996144">
Type Moore Hunalign NonmoAlign
P R F1 P R F1 P R F1
1-1 0.827 0.828 0.827 0.999 0.972 0.986 0.987 0.987 0.987
1-0 0.359 0.329 0.343 0.330 0.457 0.383 0.729 0.729 0.729
Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.976 0.976 0.976
</table>
<tableCaption confidence="0.999876">
Table 3: Performance of monotonic alignment in comparison with the baseline methods.
</tableCaption>
<bodyText confidence="0.999983586206897">
length in number of characters while the latter in
number of tokens. Both use dynamic program-
ming to search for the best alignment. As shown in
Chen (1993) and Wu (1994), however, sentence-
length based methods suffer when the texts to be
aligned contain small passages, or the languages
involved share few cognates. The subsequent stage
of sentence alignment research is accompanied by
the advent of a handful of well-designed alignment
tools. Moore (2002) proposes a three-pass proce-
dure to find final alignment. Its bitext input is ini-
tially aligned based on sentence length. This step
generates a set of strictly-selected sentence pairs
for use to train an IBM translation model 1 (Brown
et al., 1993). Its final step realigns the bitext using
both sentence length and the discovered word cor-
respondences. Hunalign (Varga et al., 2005), orig-
inally proposed as an ingredient for building paral-
lel corpora, has demonstrated an outstanding per-
formance on sentence alignment. Like many other
aligners, it employs a similar strategy of combin-
ing sentence length and lexical data. In the ab-
sence of a lexicon, it first performs an initial align-
ment wholly relying on sentence length and then
automatically builds a lexicon based on this align-
ment. Using an available lexicon, it produces a
rough translation of the source text by converting
each token to the one of its possible counterparts
that has the highest frequency in the target corpus.
Then, the relation matrix of a bitext is built of sim-
ilarity scores for the rough translation and the ac-
tual translation at sentence level. The similarity of
two sentences is calculated in terms of their com-
mon pairs and length ratio.
To deal with noisy input, Ma (2006) proposes
a lexicon-based sentence aligner - Champollion.
Its distinctive feature is that it assigns different
weights to words in terms of their tf-idf scores,
assuming that words with low sentence frequen-
cies in a text but high occurrences in some local
sentences are more indicative of alignment. Un-
der this assumption, the similarity of any two sen-
tences is calculated accordingly and then a dy-
namic programming algorithm is applied to pro-
duce final alignment. Following this work, Li et
al. (2010) propose a revised version of Champol-
lion, attempting to improve its speed without per-
formance loss. For this purpose, the input bitexts
are first divided into smaller aligned fragments be-
fore applying Champollion to derive finer-grained
sentence pairs. In another related work by Deng et
al. (2007), a generative model is proposed, accom-
panied by two specific alignment strategies, i.e.,
dynamic programming and divisive clustering. Al-
though a non-monotonic search process that toler-
ates two successive chunks in reverse order is in-
volved, their work is essentially targeted at mono-
tonic alignment.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99960915">
In this paper we have proposed and tested
a semisupervised learning approach to non-
monotonic sentence alignment by incorporating
both monolingual and bilingual consistency. The
utility of monolingual consistency in maintain-
ing the consonance of high-affinity monolingual
sentences with their translations has been demon-
strated. This work also exhibits that bilingual con-
sistency of initial alignment of certain quality is
useful to boost alignment performance. Our eval-
uation using real-world data from a legislation
corpus shows that the proposed approach outper-
forms the baseline methods significantly when the
bitext input is composed of non-monotonic sen-
tences. Working on partially non-monotonic data,
this approach also demonstrates a superior per-
formance. Although initially proposed for non-
monotonic alignment, it works well on monotonic
alignment by incorporating the constraint of sen-
tence sequence.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994245">
The research described in this paper was substan-
tially supported by the Research Grants Council
(RGC) of Hong Kong SAR, China, through the
GRF grant 9041597 (CityU 144410).
</bodyText>
<page confidence="0.998829">
629
</page>
<sectionHeader confidence="0.993804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914462365592">
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 2011.
Modern Information Retrieval: The Concepts
and Technology Behind Search, 2nd ed., Harlow:
Addison-Wesley.
Jewel B. Barlow, Moghen M. Monahemi, and Dianne P.
O’Leary. 1992. Constrained matrix Sylvester equa-
tions. In SIAM Journal on Matrix Analysis and Ap-
plications, 13(1):1-9.
Peter F. Brown, Jennifer C. Lai, Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In
Proceedings of ACL’91, pages 169-176.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263-
311.
Stanley F. Chen. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings of
ACL’93, pages 9-16.
Yonggang Deng, Shankar Kumar, and William Byrne.
2007. Segmentation and alignment of parallel text
for statistical machine translation. Natural Lan-
guage Engineering, 13(3): 235-260.
William A. Gale, Kenneth Ward Church. 1991. A Pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of ACL’91, pages 177-184.
Martin Kay and Martin R¨oscheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121-142.
Chunyu Kit, Jonathan J. Webster, King Kui Sin, Haihua
Pan, and Heng Li. 2004. Clause alignment for bilin-
gual HK legal texts: A lexical-based approach. In-
ternational Journal of Corpus Linguistics, 9(1):29-
51.
Chunyu Kit, Xiaoyue Liu, King Kui Sin, and Jonathan
J. Webster. 2005. Harvesting the bitexts of the laws
of Hong Kong from the Web. In The 5th Workshop
on Asian Language Resources, pages 71-78.
Judith L. Klavans and Evelyne Tzoukermann. 1990.
The bicord system: Combining lexical information
from bilingual corpora and machine readable dictio-
naries. In Proceedings of COLING’90, pages 174-
179.
Philippe Langlais, Michel Simard, and Jean V´eronis.
1998. Methods and practical issues in evaluating
alignment techniques. In Proceedings of COLING-
ACL’98, pages 711-717.
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.
2010. Improving statistical machine translation with
monolingual collocation. In Proceedings of ACL
2010, pages 825-833.
Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In LREC 2006, pages 489-492.
Peng Li, Maosong Sun, Ping Xue. 2010. Fast-
Champollion: a fast and robust sentence alignment
algorithm. In Proceedings of ACL 2010: Posters,
pages 710-718.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA 2002, page 135-144.
Jian-Yun Nie, Michel Simard, Pierre Isabelle and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. In Proceedings
of SIGIR’99, pages 74-81.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3): 130-137.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xi-
aodong Shi, Huailin Dong, Qun Liu. 2012. Transla-
tion model adaptation for statistical machine trans-
lation with monolingual topic information. In Pro-
ceedings of ACL 2012, Vol. 1, pages 459-468.
Ben Taskar, Simon Lacoste-Julien and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT/EMNLP 2005,
pages 73-80.
D´aniel Varga, P´eter Hal´acsy, Andr´as Kornai, Viktor
Nagy, L´aszl´o N´emeth, Viktor Tr´on. 2005. Parallel
corpora for medium density languages. In Proceed-
ings of RANLP 2005, pages 590-596.
Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In Proceed-
ings of ACL’94, pages 80-87.
Dekai Wu. 2010. Alignment. Handbook of Natural
Language Processing, 2nd ed., CRC Press.
Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-
son Weston, Bernhard Schlkopf. 2004. Learning
with local and global consistency. Advances in Neu-
ral Information Processing Systems, 16:321-328.
Xiaojin Zhu, Zoubin Ghahramani and John Lafferty.
2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML 2003, pages 912-919.
</reference>
<page confidence="0.99763">
630
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768478">
<title confidence="0.999582">Non-Monotonic Sentence Alignment via Semisupervised Learning</title>
<author confidence="0.991666">Xiaojun Quan</author>
<author confidence="0.991666">Chunyu Kit</author>
<author confidence="0.991666">Yan</author>
<affiliation confidence="0.90121">Department of Chinese, Translation and City University of Hong Kong, HKSAR,</affiliation>
<abstract confidence="0.998388454545454">This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<date>2011</date>
<booktitle>Modern Information Retrieval: The Concepts and Technology Behind Search, 2nd ed.,</booktitle>
<publisher>Addison-Wesley.</publisher>
<location>Harlow:</location>
<contexts>
<context position="13520" citStr="Baeza-Yates and Ribeiro-Neto, 2011" startWordPosition="2263" endWordPosition="2266"> bitexts. In legal documents, for example, translations of many key terms are usually accompanied with their source terms. Also, common numberings can be found in enumerated lists in bitexts. These kinds of anchor strings provide quite reliable information to link bilingual sentences into pairs, and thus can serve as useful cues for sentence alignment. In fact, they can be treated as a special type of highly reliable “bilexicon”. The anchor strings used in this work are derived by searching the bitexts using word-level inverted indexing, a basic technique widely used in information retrieval (Baeza-Yates and Ribeiro-Neto, 2011). For each index term, a list of postings is created. Each posting includes a sentence identifier, the in-sentence frequency and positions of this term. The positions of terms are intersected to find common anchor strings. The anchor strings, once found, are used to calculate the initial affinity ˆFij of two sentences using Dice’s coefficient ˆFij = 2|C1i n C2j |(8) |C1i |+ |C2j| where C1i and C2j are the anchor sets in si and tj, respectively, and |· |is the cardinality of a set. Apart from using anchor strings, other avenues for the initialization are studied in the evaluation section below,</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 2011</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 2011. Modern Information Retrieval: The Concepts and Technology Behind Search, 2nd ed., Harlow: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jewel B Barlow</author>
<author>Moghen M Monahemi</author>
<author>Dianne P O’Leary</author>
</authors>
<title>Constrained matrix Sylvester equations.</title>
<date>1992</date>
<booktitle>In SIAM Journal on Matrix Analysis and Applications,</booktitle>
<pages>13--1</pages>
<marker>Barlow, Monahemi, O’Leary, 1992</marker>
<rawString>Jewel B. Barlow, Moghen M. Monahemi, and Dianne P. O’Leary. 1992. Constrained matrix Sylvester equations. In SIAM Journal on Matrix Analysis and Applications, 13(1):1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL’91,</booktitle>
<pages>169--176</pages>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Peter F. Brown, Jennifer C. Lai, Robert L. Mercer. 1991. Aligning sentences in parallel corpora. In Proceedings of ACL’91, pages 169-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1315" citStr="Brown et al., 1993" startWordPosition="179" endWordPosition="182">hniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data. 1 Introduction Bilingual sentence alignment is a fundamental task to undertake for the purpose of facilitating many important natural language processing applications such as statistical machine translation (Brown et al., 1993), bilingual lexicography (Klavans et al., 1990), and cross-language information retrieval (Nie et al., 1999). Its objective is to identify correspondences between bilingual sentences in given bitexts. As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. Approaches based on the former perform effectively on cognate languages but not on the others. For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two IndoEuropean languages (Wu, 1994). Lexicon-based a</context>
<context position="27788" citStr="Brown et al., 1993" startWordPosition="4613" endWordPosition="4616">c programming to search for the best alignment. As shown in Chen (1993) and Wu (1994), however, sentencelength based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. The subsequent stage of sentence alignment research is accompanied by the advent of a handful of well-designed alignment tools. Moore (2002) proposes a three-pass procedure to find final alignment. Its bitext input is initially aligned based on sentence length. This step generates a set of strictly-selected sentence pairs for use to train an IBM translation model 1 (Brown et al., 1993). Its final step realigns the bitext using both sentence length and the discovered word correspondences. Hunalign (Varga et al., 2005), originally proposed as an ingredient for building parallel corpora, has demonstrated an outstanding performance on sentence alignment. Like many other aligners, it employs a similar strategy of combining sentence length and lexical data. In the absence of a lexicon, it first performs an initial alignment wholly relying on sentence length and then automatically builds a lexicon based on this alignment. Using an available lexicon, it produces a rough translation</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Aligning sentences in bilingual corpora using lexical information.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL’93,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="27240" citStr="Chen (1993)" startWordPosition="4527" endWordPosition="4528"> uses sentence Micro−F1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0 % 10% 20% 30% 40% 50% 60% 70% 80% 1.1 1 NonmoAlign Hunalign Moore NonmoAlign_Hun 628 Type Moore Hunalign NonmoAlign P R F1 P R F1 P R F1 1-1 0.827 0.828 0.827 0.999 0.972 0.986 0.987 0.987 0.987 1-0 0.359 0.329 0.343 0.330 0.457 0.383 0.729 0.729 0.729 Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.976 0.976 0.976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. length in number of characters while the latter in number of tokens. Both use dynamic programming to search for the best alignment. As shown in Chen (1993) and Wu (1994), however, sentencelength based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. The subsequent stage of sentence alignment research is accompanied by the advent of a handful of well-designed alignment tools. Moore (2002) proposes a three-pass procedure to find final alignment. Its bitext input is initially aligned based on sentence length. This step generates a set of strictly-selected sentence pairs for use to train an IBM translation model 1 (Brown et al., 1993). Its final step realigns the bitext using both sent</context>
</contexts>
<marker>Chen, 1993</marker>
<rawString>Stanley F. Chen. 1993. Aligning sentences in bilingual corpora using lexical information. In Proceedings of ACL’93, pages 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Segmentation and alignment of parallel text for statistical machine translation.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>3</issue>
<pages>235--260</pages>
<contexts>
<context position="29597" citStr="Deng et al. (2007)" startWordPosition="4912" endWordPosition="4915">th low sentence frequencies in a text but high occurrences in some local sentences are more indicative of alignment. Under this assumption, the similarity of any two sentences is calculated accordingly and then a dynamic programming algorithm is applied to produce final alignment. Following this work, Li et al. (2010) propose a revised version of Champollion, attempting to improve its speed without performance loss. For this purpose, the input bitexts are first divided into smaller aligned fragments before applying Champollion to derive finer-grained sentence pairs. In another related work by Deng et al. (2007), a generative model is proposed, accompanied by two specific alignment strategies, i.e., dynamic programming and divisive clustering. Although a non-monotonic search process that tolerates two successive chunks in reverse order is involved, their work is essentially targeted at monotonic alignment. 5 Conclusion In this paper we have proposed and tested a semisupervised learning approach to nonmonotonic sentence alignment by incorporating both monolingual and bilingual consistency. The utility of monolingual consistency in maintaining the consonance of high-affinity monolingual sentences with </context>
</contexts>
<marker>Deng, Kumar, Byrne, 2007</marker>
<rawString>Yonggang Deng, Shankar Kumar, and William Byrne. 2007. Segmentation and alignment of parallel text for statistical machine translation. Natural Language Engineering, 13(3): 235-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth Ward Church</author>
</authors>
<title>A Program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL’91,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="26384" citStr="Gale and Church (1991)" startWordPosition="4372" endWordPosition="4375"> our data set by discarding all its 126 crossed pairings. Of the two strategies discussed above, banding is used to help our aligner incorporate the sequence information. The initial relation matrix is built with the aid of a dictionary automatically derived by Hunalign. Entries of the matrix are derived by employing a similar strategy as in Varga et al. (2005). The evaluation results are presented in Table 3, which shows that NonmoAlign still achieves very competitive performance on monotonic sentence alignment. 4 Related Work The research of sentence alignment originates in the early 1990s. Gale and Church (1991) and Brown (1991) report the early works using length statistics of bilingual sentences. The general idea is that the closer two sentences are in length, the more likely they are to align. A notable difference of their methods is that the former uses sentence Micro−F1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0 % 10% 20% 30% 40% 50% 60% 70% 80% 1.1 1 NonmoAlign Hunalign Moore NonmoAlign_Hun 628 Type Moore Hunalign NonmoAlign P R F1 P R F1 P R F1 1-1 0.827 0.828 0.827 0.999 0.972 0.986 0.987 0.987 0.987 1-0 0.359 0.329 0.343 0.330 0.457 0.383 0.729 0.729 0.729 Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William A. Gale, Kenneth Ward Church. 1991. A Program for aligning sentences in bilingual corpora. In Proceedings of ACL’91, pages 177-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
<author>Martin R¨oscheisen</author>
</authors>
<date>1993</date>
<booktitle>Texttranslation alignment. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<marker>Kay, R¨oscheisen, 1993</marker>
<rawString>Martin Kay and Martin R¨oscheisen. 1993. Texttranslation alignment. Computational Linguistics, 19(1):121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
<author>Jonathan J Webster</author>
<author>King Kui Sin</author>
<author>Haihua Pan</author>
<author>Heng Li</author>
</authors>
<title>Clause alignment for bilingual HK legal texts: A lexical-based approach.</title>
<date>2004</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>9--1</pages>
<contexts>
<context position="10795" citStr="Kit et al., 2004" startWordPosition="1738" endWordPosition="1741">1 ˆA. This is in fact a Sylvester equation (Barlow et al., 1992), whose numerical solution can be found by many classical algorithms. In this research, it is solved using LAPACK,1 a software library for numerical linear algebra. Non-positive entries in F* indicate unrealistic correspondences of sentences and are thus set to zero before applying the alignment function. 2.3 Alignment Function Once the optimal F* is acquired, the remaining task is to design an alignment function A to convert it into an alignment solution. An intuitive approach is to use a heuristic search for local optimization (Kit et al., 2004), which produces an alignment with respect to the largest scores in each row and each column. However, this does not guarantee a globally optimal solution. Figure 3 illustrates a mapping relation matrix onto an alignment matrix, which also shows that the optimal alignment cannot be achieved by heuristic search. Banding is another approach frequently used to convert a relation matrix to alignment (Kay and R¨oscheisen, 1993). It is founded on the observation that true monotonic alignment paths usually lie close to the diagonal of a relation matrix. However, it is not applicable to our task due t</context>
</contexts>
<marker>Kit, Webster, Sin, Pan, Li, 2004</marker>
<rawString>Chunyu Kit, Jonathan J. Webster, King Kui Sin, Haihua Pan, and Heng Li. 2004. Clause alignment for bilingual HK legal texts: A lexical-based approach. International Journal of Corpus Linguistics, 9(1):29-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunyu Kit</author>
<author>Xiaoyue Liu</author>
<author>King Kui Sin</author>
<author>Jonathan J Webster</author>
</authors>
<title>Harvesting the bitexts of the laws of Hong Kong from the Web.</title>
<date>2005</date>
<booktitle>In The 5th Workshop on Asian Language Resources,</booktitle>
<pages>71--78</pages>
<marker>Kit, Liu, Sin, Webster, 2005</marker>
<rawString>Chunyu Kit, Xiaoyue Liu, King Kui Sin, and Jonathan J. Webster. 2005. Harvesting the bitexts of the laws of Hong Kong from the Web. In The 5th Workshop on Asian Language Resources, pages 71-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
<author>Evelyne Tzoukermann</author>
</authors>
<title>The bicord system: Combining lexical information from bilingual corpora and machine readable dictionaries.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING’90,</booktitle>
<pages>174--179</pages>
<marker>Klavans, Tzoukermann, 1990</marker>
<rawString>Judith L. Klavans and Evelyne Tzoukermann. 1990. The bicord system: Combining lexical information from bilingual corpora and machine readable dictionaries. In Proceedings of COLING’90, pages 174-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Michel Simard</author>
<author>Jean V´eronis</author>
</authors>
<title>Methods and practical issues in evaluating alignment techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL’98,</booktitle>
<pages>711--717</pages>
<marker>Langlais, Simard, V´eronis, 1998</marker>
<rawString>Philippe Langlais, Michel Simard, and Jean V´eronis. 1998. Methods and practical issues in evaluating alignment techniques. In Proceedings of COLINGACL’98, pages 711-717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhanyi Liu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Sheng Li</author>
</authors>
<title>Improving statistical machine translation with monolingual collocation.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>825--833</pages>
<contexts>
<context position="14348" citStr="Liu et al., 2010" startWordPosition="2398" endWordPosition="2401">r strings. The anchor strings, once found, are used to calculate the initial affinity ˆFij of two sentences using Dice’s coefficient ˆFij = 2|C1i n C2j |(8) |C1i |+ |C2j| where C1i and C2j are the anchor sets in si and tj, respectively, and |· |is the cardinality of a set. Apart from using anchor strings, other avenues for the initialization are studied in the evaluation section below, i.e., using another aligner and an existing lexicon. 2.5 Monolingual Affinity Although various kinds of information from a monolingual corpus have been exploited to boost statistical machine translation models (Liu et al., 2010; Su et al., 2012), we have not yet been exposed to any attempt to leverage monolingual sentence affinity for sentence alignment. In our framework, an attempt to this can be made through the computation of W and V . Let us take W as an example, where the entry Wij represents the affinity of sentence si and sentence sj, and it is set to 0 for i = j in order to avoid self-reinforcement during optimization (Zhou et al., 2004). When two sentences in S or T are not too short, or their content is not divergent in meaning, their semantic similarity can be estimated in terms of common words. Motivated</context>
</contexts>
<marker>Liu, Wang, Wu, Li, 2010</marker>
<rawString>Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2010. Improving statistical machine translation with monolingual collocation. In Proceedings of ACL 2010, pages 825-833.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyi Ma</author>
</authors>
<title>Champollion: A robust parallel text sentence aligner.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<pages>489--492</pages>
<contexts>
<context position="2474" citStr="Ma (2006)" startWordPosition="358" endWordPosition="359">IndoEuropean languages (Wu, 1994). Lexicon-based approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. A few sentence alignment methods and tools have also been explored to combine the two. Moore (2002) proposes a multi-pass search procedure using both sentence length and an automaticallyderived bilingual lexicon. Hunalign (Varga et al., 2005) is another sentence aligner that combines sentence length and a lexicon. Without a lexicon, it backs off to a length-based algorithm and then automatically derives a lexicon from the alignment result. Soon after, Ma (2006) develops the lexicon-based aligner Champollion, assuming that different words have different importance in aligning two sentences. Nevertheless, most existing approaches to sentence alignment follow the monotonicity assumption that coupled sentences in bitexts appear in a similar sequential order in two languages and crossings are not entertained in general (Langlais et al., 1998; Wu, 2010). Consequently the task of sentence alignment becomes handily solvable by means of such basic techniques as dynamic programming. In many scenarios, however, this prerequisite monotonicity cannot be guarante</context>
<context position="28796" citStr="Ma (2006)" startWordPosition="4785" endWordPosition="4786">on, it first performs an initial alignment wholly relying on sentence length and then automatically builds a lexicon based on this alignment. Using an available lexicon, it produces a rough translation of the source text by converting each token to the one of its possible counterparts that has the highest frequency in the target corpus. Then, the relation matrix of a bitext is built of similarity scores for the rough translation and the actual translation at sentence level. The similarity of two sentences is calculated in terms of their common pairs and length ratio. To deal with noisy input, Ma (2006) proposes a lexicon-based sentence aligner - Champollion. Its distinctive feature is that it assigns different weights to words in terms of their tf-idf scores, assuming that words with low sentence frequencies in a text but high occurrences in some local sentences are more indicative of alignment. Under this assumption, the similarity of any two sentences is calculated accordingly and then a dynamic programming algorithm is applied to produce final alignment. Following this work, Li et al. (2010) propose a revised version of Champollion, attempting to improve its speed without performance los</context>
</contexts>
<marker>Ma, 2006</marker>
<rawString>Xiaoyi Ma. 2006. Champollion: A robust parallel text sentence aligner. In LREC 2006, pages 489-492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
</authors>
<title>Maosong Sun, Ping Xue.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010: Posters,</booktitle>
<pages>710--718</pages>
<marker>Li, 2010</marker>
<rawString>Peng Li, Maosong Sun, Ping Xue. 2010. FastChampollion: a fast and robust sentence alignment algorithm. In Proceedings of ACL 2010: Posters, pages 710-718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Fast and accurate sentence alignment of bilingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of AMTA 2002,</booktitle>
<pages>135--144</pages>
<contexts>
<context position="2108" citStr="Moore (2002)" startWordPosition="301" endWordPosition="302">s in given bitexts. As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. Approaches based on the former perform effectively on cognate languages but not on the others. For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two IndoEuropean languages (Wu, 1994). Lexicon-based approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. A few sentence alignment methods and tools have also been explored to combine the two. Moore (2002) proposes a multi-pass search procedure using both sentence length and an automaticallyderived bilingual lexicon. Hunalign (Varga et al., 2005) is another sentence aligner that combines sentence length and a lexicon. Without a lexicon, it backs off to a length-based algorithm and then automatically derives a lexicon from the alignment result. Soon after, Ma (2006) develops the lexicon-based aligner Champollion, assuming that different words have different importance in aligning two sentences. Nevertheless, most existing approaches to sentence alignment follow the monotonicity assumption that c</context>
<context position="22657" citStr="Moore, 2002" startWordPosition="3773" endWordPosition="3774">ncy. Another observation is that the alignment performance goes up along the increase of the percentage of initial alignment while performance gain slows down gradually. When the percentage is very low, the aligner still works quite effectively. 3.4 Non-Monotonic Alignment To test our aligner with non-monotonic sequences of sentences, we have them randomly scrambled in our experimental data. This undoubtedly increases the difficulty of sentence alignment, especially for the traditional approaches critically relying on monotonicity. The baseline methods used for comparison are Moore’s aligner (Moore, 2002) and Hunalign (Varga et al., 2005). Hunalign is configured with the option [-realign], which triggers a three-step procedure: after an initial alignment, Hunalign heuristically enriches its dictionary using word cooccurrences in identified sentence pairs; then, it re-runs the alignment process using the updated 1600 1400 1200 1000 800 600 400 200 0 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100% NonmoAlign initAlign Correctly detected 1−1 pairings 627 Type Moore Hunalign NonmoAlign P R F1 P R F1 P R F1 1-1 0.104 0.104 0.104 0.407 0.229 0.293 0.878 0.878 0.878 1-0 0.288 0.243 0.264 0.033 0.671 0.0</context>
<context position="27540" citStr="Moore (2002)" startWordPosition="4573" endWordPosition="4574">0.729 0.729 Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.976 0.976 0.976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. length in number of characters while the latter in number of tokens. Both use dynamic programming to search for the best alignment. As shown in Chen (1993) and Wu (1994), however, sentencelength based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. The subsequent stage of sentence alignment research is accompanied by the advent of a handful of well-designed alignment tools. Moore (2002) proposes a three-pass procedure to find final alignment. Its bitext input is initially aligned based on sentence length. This step generates a set of strictly-selected sentence pairs for use to train an IBM translation model 1 (Brown et al., 1993). Its final step realigns the bitext using both sentence length and the discovered word correspondences. Hunalign (Varga et al., 2005), originally proposed as an ingredient for building parallel corpora, has demonstrated an outstanding performance on sentence alignment. Like many other aligners, it employs a similar strategy of combining sentence len</context>
</contexts>
<marker>Moore, 2002</marker>
<rawString>Robert C. Moore. 2002. Fast and accurate sentence alignment of bilingual corpora. In Proceedings of AMTA 2002, page 135-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
<author>Michel Simard</author>
<author>Pierre Isabelle</author>
<author>Richard Durand</author>
</authors>
<title>Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web. In</title>
<date>1999</date>
<booktitle>Proceedings of SIGIR’99,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="1423" citStr="Nie et al., 1999" startWordPosition="195" endWordPosition="198"> produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data. 1 Introduction Bilingual sentence alignment is a fundamental task to undertake for the purpose of facilitating many important natural language processing applications such as statistical machine translation (Brown et al., 1993), bilingual lexicography (Klavans et al., 1990), and cross-language information retrieval (Nie et al., 1999). Its objective is to identify correspondences between bilingual sentences in given bitexts. As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. Approaches based on the former perform effectively on cognate languages but not on the others. For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two IndoEuropean languages (Wu, 1994). Lexicon-based approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. A few sentence</context>
</contexts>
<marker>Nie, Simard, Isabelle, Durand, 1999</marker>
<rawString>Jian-Yun Nie, Michel Simard, Pierre Isabelle and Richard Durand. 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web. In Proceedings of SIGIR’99, pages 74-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>130--137</pages>
<contexts>
<context position="18120" citStr="Porter, 1980" startWordPosition="3044" endWordPosition="3045">itexts after filtering out null pages. A text contains several to two hundred sentences. Many bitexts exhibit partially non-monotonic order of sentences. Among them, 175 bitexts are randomly selected for manual alignment. Sentences are identified based on punctuations. OpenNLP Tokenizer3 is applied to segment English sentences into tokens. For Chinese, since there is no reliable segmenter for this genre of text, we have to treat each Chinese character as a single token. In addition, to calculate the monolingual sentence affinity, stemming of English words is performed with the Porter Stemmer (Porter, 1980) after anchor string mining. The manual alignment of the evaluation data set is performed upon the initial alignment by Hunalign (Varga et al., 2005), an effective sentence aligner that uses both sentence length and a bilexicon (if available). For this work, Hunalign relies solely on sentence length. Its output is then double-checked and corrected by two experts in bilingual studies, resulting in a data set of 1747 1-1 and 70 1-0 or 0-1 sentence pairs. The standard deviation Q in (9) is an important parameter for the Gaussian kernel that has to be determined empirically (Zhu et al., 2003; Zhou</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3): 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinsong Su</author>
<author>Hua Wu</author>
</authors>
<title>Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, Qun Liu.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<volume>1</volume>
<pages>459--468</pages>
<marker>Su, Wu, 2012</marker>
<rawString>Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, Qun Liu. 2012. Translation model adaptation for statistical machine translation with monolingual topic information. In Proceedings of ACL 2012, Vol. 1, pages 459-468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<pages>73--80</pages>
<contexts>
<context position="12405" citStr="Taskar et al., 2005" startWordPosition="2083" endWordPosition="2086">5 0 0 0 0 0.6 0 0 0 0 0 0 0 0 0 0 0.2 0 0 0 0 0 0.6 0 0 0 0 0.8 0 0.4 Figure 3: Illustration of sentence alignment from relation matrix to alignment matrix. The scores marked with arrows are the best in each row/column to be used by the heuristic search. The right matrix represents the corresponding alignment matrix by our algorithm. the following optimization A = arg max �m n XijFij (7) X i=1 j=1 m n s.t. Xij G 1, Xij G 1, Xij E 10, 11 i=1 j=1 This turns sentence alignment into a problem to be resolved by binary linear programming (BIP), which has been successfully applied to word alignment (Taskar et al., 2005). Given a scoring matrix, it guarantees an optimal solution. 2.4 Alignment Initialization Once the above alignment function is available, the initial alignment matrix Aˆ can be derived from an initial relation matrix Fˆ obtained by an available alignment method. This work resorts to another approach to initializing the relation matrix. In many genres of bitexts, such as government transcripts or legal documents, there are a certain number of common strings on the two sides of bitexts. In legal documents, for example, translations of many key terms are usually accompanied with their source term</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT/EMNLP 2005, pages 73-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D´aniel Varga</author>
<author>P´eter Hal´acsy</author>
<author>Andr´as Kornai</author>
<author>Viktor Nagy</author>
<author>L´aszl´o N´emeth</author>
<author>Viktor Tr´on</author>
</authors>
<title>Parallel corpora for medium density languages.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP</booktitle>
<pages>590--596</pages>
<marker>Varga, Hal´acsy, Kornai, Nagy, N´emeth, Tr´on, 2005</marker>
<rawString>D´aniel Varga, P´eter Hal´acsy, Andr´as Kornai, Viktor Nagy, L´aszl´o N´emeth, Viktor Tr´on. 2005. Parallel corpora for medium density languages. In Proceedings of RANLP 2005, pages 590-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL’94,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="1898" citStr="Wu, 1994" startWordPosition="270" endWordPosition="271">ation (Brown et al., 1993), bilingual lexicography (Klavans et al., 1990), and cross-language information retrieval (Nie et al., 1999). Its objective is to identify correspondences between bilingual sentences in given bitexts. As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. Approaches based on the former perform effectively on cognate languages but not on the others. For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two IndoEuropean languages (Wu, 1994). Lexicon-based approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. A few sentence alignment methods and tools have also been explored to combine the two. Moore (2002) proposes a multi-pass search procedure using both sentence length and an automaticallyderived bilingual lexicon. Hunalign (Varga et al., 2005) is another sentence aligner that combines sentence length and a lexicon. Without a lexicon, it backs off to a length-based algorithm and then automatically derives a lexicon from the alignment result. Soon after, Ma (2006) develops the lexicon-ba</context>
<context position="27254" citStr="Wu (1994)" startWordPosition="4530" endWordPosition="4531">icro−F1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0 % 10% 20% 30% 40% 50% 60% 70% 80% 1.1 1 NonmoAlign Hunalign Moore NonmoAlign_Hun 628 Type Moore Hunalign NonmoAlign P R F1 P R F1 P R F1 1-1 0.827 0.828 0.827 0.999 0.972 0.986 0.987 0.987 0.987 1-0 0.359 0.329 0.343 0.330 0.457 0.383 0.729 0.729 0.729 Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.976 0.976 0.976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. length in number of characters while the latter in number of tokens. Both use dynamic programming to search for the best alignment. As shown in Chen (1993) and Wu (1994), however, sentencelength based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. The subsequent stage of sentence alignment research is accompanied by the advent of a handful of well-designed alignment tools. Moore (2002) proposes a three-pass procedure to find final alignment. Its bitext input is initially aligned based on sentence length. This step generates a set of strictly-selected sentence pairs for use to train an IBM translation model 1 (Brown et al., 1993). Its final step realigns the bitext using both sentence length an</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>Dekai Wu. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In Proceedings of ACL’94, pages 80-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<date>2010</date>
<booktitle>Alignment. Handbook of Natural Language Processing, 2nd</booktitle>
<editor>ed.,</editor>
<publisher>CRC Press.</publisher>
<contexts>
<context position="1542" citStr="Wu (2010)" startWordPosition="215" endWordPosition="216">at while existing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data. 1 Introduction Bilingual sentence alignment is a fundamental task to undertake for the purpose of facilitating many important natural language processing applications such as statistical machine translation (Brown et al., 1993), bilingual lexicography (Klavans et al., 1990), and cross-language information retrieval (Nie et al., 1999). Its objective is to identify correspondences between bilingual sentences in given bitexts. As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. Approaches based on the former perform effectively on cognate languages but not on the others. For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two IndoEuropean languages (Wu, 1994). Lexicon-based approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. A few sentence alignment methods and tools have also been explored to combine the two. Moore (2002) proposes a multi-pass search proc</context>
<context position="2868" citStr="Wu, 2010" startWordPosition="415" endWordPosition="416">her sentence aligner that combines sentence length and a lexicon. Without a lexicon, it backs off to a length-based algorithm and then automatically derives a lexicon from the alignment result. Soon after, Ma (2006) develops the lexicon-based aligner Champollion, assuming that different words have different importance in aligning two sentences. Nevertheless, most existing approaches to sentence alignment follow the monotonicity assumption that coupled sentences in bitexts appear in a similar sequential order in two languages and crossings are not entertained in general (Langlais et al., 1998; Wu, 2010). Consequently the task of sentence alignment becomes handily solvable by means of such basic techniques as dynamic programming. In many scenarios, however, this prerequisite monotonicity cannot be guaranteed. For example, bilingual clauses in legal bitexts are often coordinated in a way not to keep the same clause order, demanding fully or partially crossing pairings. Figure 1 shows a real excerpt from a legislation corpus. Such monotonicity seriously impairs the existing alignment approaches founded on the monotonicity assumption. This paper is intended to explore the problem of non-monotoni</context>
</contexts>
<marker>Wu, 2010</marker>
<rawString>Dekai Wu. 2010. Alignment. Handbook of Natural Language Processing, 2nd ed., CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Olivier Bousquet</author>
<author>Thomas N Lal</author>
<author>Jason Weston</author>
<author>Bernhard Schlkopf</author>
</authors>
<title>Learning with local and global consistency.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>16--321</pages>
<contexts>
<context position="9552" citStr="Zhou et al. (2004)" startWordPosition="1503" endWordPosition="1506">E are the diagonal matrices with entries Dii = Ej Wij and Eii = E j Vij. The idea behind (3) is that to minimize the cost function, the translations of those monolingual sentences with close relatedness reflected in W and V should also keep similar closeness. The bilingual constraint term Qb(F) is defined as � �2 Fij − ˆAij , (4) where Aˆ is the initial alignment matrix obtained by A : Fˆ → ˆA. Note that Fˆ is the initial relation matrix between S and T. The monolingual constraint term Qm(F) defined above corresponds to the smoothness constraint in the previous semisupervised learning work by Zhou et al. (2004) that assigns higher likelihood to objects with larger similarity to share the same label. On the other hand, Qb(F) corresponds to their fitting constraint, which requires the final alignment to maintain the maximum consistency with the initial alignment. Taking the derivative of Q(F) with respect to F, we have ∂Q(F) ∂F = 2F − 2SFT + 2λF − 2λ ˆA, (5) where S and T are the normalized matrices of W and V , calculated by S = D−1/2WD−1/2 and T = E−1/2V E−1/2. Then, the optimal F* is to be found by solving the equation (1 + λ) F* − SF*T = λ ˆA, (6) which is equivalent to αF* − F*β = γ with α = (1 +</context>
<context position="14774" citStr="Zhou et al., 2004" startWordPosition="2479" endWordPosition="2482">existing lexicon. 2.5 Monolingual Affinity Although various kinds of information from a monolingual corpus have been exploited to boost statistical machine translation models (Liu et al., 2010; Su et al., 2012), we have not yet been exposed to any attempt to leverage monolingual sentence affinity for sentence alignment. In our framework, an attempt to this can be made through the computation of W and V . Let us take W as an example, where the entry Wij represents the affinity of sentence si and sentence sj, and it is set to 0 for i = j in order to avoid self-reinforcement during optimization (Zhou et al., 2004). When two sentences in S or T are not too short, or their content is not divergent in meaning, their semantic similarity can be estimated in terms of common words. Motivated by this, we define Wij (for i =� j) based on the Gaussian kernel as T j 2 Wij = exp −2σ2 C1 Ilvvl lvj I J (9) 625 where Q is the standard deviation parameter, vi and vj are vectors of si and sj with each component corresponding to the tf-idf value of a particular term in S (or T), and II·II is the norm of a vector. The underlying assumption here is that words appearing frequently in a small number of sentences but rarely </context>
<context position="18734" citStr="Zhou et al., 2004" startWordPosition="3148" endWordPosition="3151">980) after anchor string mining. The manual alignment of the evaluation data set is performed upon the initial alignment by Hunalign (Varga et al., 2005), an effective sentence aligner that uses both sentence length and a bilexicon (if available). For this work, Hunalign relies solely on sentence length. Its output is then double-checked and corrected by two experts in bilingual studies, resulting in a data set of 1747 1-1 and 70 1-0 or 0-1 sentence pairs. The standard deviation Q in (9) is an important parameter for the Gaussian kernel that has to be determined empirically (Zhu et al., 2003; Zhou et al., 2004). In addition, the Q function also involves another parameter A to adjust the weight of the bilingual constraint. This work seeks an approach to deriving the optimal parameters without any external training data beyond the initial alignment. A three-fold cross-validation is thus performed on the initial 1-1 alignment and the parameters that give the best average performance are chosen. 3.2 Monolingual Consistency To demonstrate the validity of the monolingual consistency, the semantic similarity defined by vi vj is evaluated as follows. 500 pairs of EnliviNIvA glish sentences with the highest </context>
</contexts>
<marker>Zhou, Bousquet, Lal, Weston, Schlkopf, 2004</marker>
<rawString>Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schlkopf. 2004. Learning with local and global consistency. Advances in Neural Information Processing Systems, 16:321-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML</booktitle>
<pages>912--919</pages>
<contexts>
<context position="18714" citStr="Zhu et al., 2003" startWordPosition="3144" endWordPosition="3147">Stemmer (Porter, 1980) after anchor string mining. The manual alignment of the evaluation data set is performed upon the initial alignment by Hunalign (Varga et al., 2005), an effective sentence aligner that uses both sentence length and a bilexicon (if available). For this work, Hunalign relies solely on sentence length. Its output is then double-checked and corrected by two experts in bilingual studies, resulting in a data set of 1747 1-1 and 70 1-0 or 0-1 sentence pairs. The standard deviation Q in (9) is an important parameter for the Gaussian kernel that has to be determined empirically (Zhu et al., 2003; Zhou et al., 2004). In addition, the Q function also involves another parameter A to adjust the weight of the bilingual constraint. This work seeks an approach to deriving the optimal parameters without any external training data beyond the initial alignment. A three-fold cross-validation is thus performed on the initial 1-1 alignment and the parameters that give the best average performance are chosen. 3.2 Monolingual Consistency To demonstrate the validity of the monolingual consistency, the semantic similarity defined by vi vj is evaluated as follows. 500 pairs of EnliviNIvA glish sentenc</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of ICML 2003, pages 912-919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>