<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000450">
<title confidence="0.987853">
Using Entity-Based Features to Model Coherence in Student Essays
</title>
<author confidence="0.706536">
Jill Burstein
</author>
<note confidence="0.3968482">
Educational Testing Service
Princeton, NJ 08541
Joel Tetreault
Educational Testing Service
Princeton, NJ 08541
</note>
<author confidence="0.471551">
Slava Andreyev
</author>
<affiliation confidence="0.420574">
Educational Testing Service
</affiliation>
<address confidence="0.599008">
Princeton, NJ 08541
</address>
<email confidence="0.869274">
jburstein@ets.org jtetreault@ets.org sandreyev@ets.org
</email>
<sectionHeader confidence="0.991228" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999509777777778">
We show how the Barzilay and Lapata entity-
based coherence algorithm (2008) can be
applied to a new, noisy data domain – student
essays. We demonstrate that by combining
Barzilay and Lapata’s entity-based features
with novel features related to grammar errors
and word usage, one can greatly improve the
performance of automated coherence prediction
for student essays for different populations.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999833301886793">
There is a small body of work that has investigated
using NLP for the problem of identifying
coherence in student essays. For example, Foltz,
Kintsch &amp; Landauer (1998), and Higgins, Burstein,
Marcu &amp; Gentile (2004) have developed systems
that examine coherence in student writing. Foltz,
et al. (1998) systems measure lexical relatedness
between text segments by using vector-based
similarity between adjacent sentences; Higgins et
al’s (2004) system computes similarity across text
segments. Foltz et al.’s (1998) approach is in line
with the earlier TextTiling method that identifies
subtopic structure in text (Hearst, 1997).
Miltsakaki and Kukich (2000) addressed essay
coherence using Centering Theory (Grosz, Joshi &amp;
Weinstein, 1995). More recently, Barzilay and
Lapata’s (2008) approach (henceforth, BL08) used
an entity-based representation to evaluate
coherence. In BL08, entities (nouns and pronouns)
are represented by their sentence roles in a text.
The algorithm keeps track of the distribution of
entity transitions between adjacent sentences, and
computes a value for all transition types based on
their proportion of occurrence in a text. BL08
apply their algorithm to three tasks, using well-
formed newspaper corpora: text ordering, summary
coherence evaluation, and readability assessment.
For each task, their system outperforms a Latent
Semantic Analysis baseline. In addition, best
performance on each task is achieved using
different system and feature configurations. Pitler
&amp; Nenkova (2008) applied BL08 to detect text
coherence in well-formed texts.
Coherence quality is typically present in scoring
criteria for evaluating a student’s essay. This paper
focuses on the development of models to predict
low-and high-coherence ratings for essays.
Student essay data, unlike newspaper text, is
typically noisy, especially when students are non-
native English speakers (NNES). Here, we
evaluate how BL08 algorithm features can be used
to model coherence in a new, noisy data domain --
student essays. We found that coherence can be
best modeled by combining BL08 entity-based
features with novel writing quality features.
Further, our use of data sets from three different
test-taker populations also shows that coherence
models will differ across populations. Different
populations might use language differently which
could affect how coherence is presented. We
expect to incorporate coherence ratings into e-
rater®, ETS’s automated essay scoring system
(Attali &amp; Burstein, 2006).
</bodyText>
<sectionHeader confidence="0.842827" genericHeader="introduction">
2 Corpus and Annotation
</sectionHeader>
<bodyText confidence="0.9791338">
We collected approximately 800 essays (in total)
across three data sets1: 1) adult, NNES test essays
(TOEFL); 2) adult, native and NNES test essays;
(GRE) 3) U.S. middle- and high-school native and
NNES student essay submissions to Criterion®,
ETS’s instructional writing application.
Two annotators were trained to rate coherence
quality based on how easily they could read an
essay without stumbling on a coherence barrier
(i.e., a confusing sentence(s)). Annotators rated
</bodyText>
<footnote confidence="0.9950035">
1 TOEFL® is the Test of English as a Foreign Language,
and GRE® is the Graduate Record Admissions Test.
</footnote>
<page confidence="0.94333">
681
</page>
<subsubsectionHeader confidence="0.744267">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 681–684,
</subsubsectionHeader>
<bodyText confidence="0.9529896">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
essays on a 3-point scale: 1) low coherence, 2)
somewhat coherent, and 3) high coherence. They
were instructed to ignore grammar and spelling
errors, unless they affected essay comprehension.
During training, Kappa agreement statistics
indicated that annotators had difficulty agreeing on
the middle, somewhat coherent category. The
annotation scale was therefore collapsed into a 2-
point scale: somewhat coherent and high
coherence categories were collapsed into the high
coherence class (H), and low-coherence (L)
remained unchanged. Two annotators labeled an
overlapping set of about 100 essays to calculate
inter-rater agreement; weighted Kappa was 0.677.
</bodyText>
<sectionHeader confidence="0.999448" genericHeader="method">
3 System
</sectionHeader>
<subsectionHeader confidence="0.998782">
3.1 BL08 Algorithm
</subsectionHeader>
<bodyText confidence="0.998846162162162">
We implemented BL08’s entity-based algorithm to
build and evaluate coherence models for the essay
data. In short, the algorithm generates a vector of
entity transition probabilities for documents
(essays, here). Vectors are used to build coherence
models. The first step in the algorithm is to
construct an entity grid in which all entities (nouns
and pronouns) are represented by their roles (i.e.,
Subject (S), Object (O), Other (X)). Entity roles
are then used to generate entity transitions – the
role transitions across adjacent sentences (e.g.,
Subject-to-Object, Object-to-Object). Entity
transition probabilities are the proportions of
different entity transition types within a text. The
probability values are used then used as features to
build a coherence model.
Entity roles can be represented in the following
ways. In this study, consistent with 13L08, different
combinations are applied and reported (see Tables
2-4). Entities can be represented in grids with
specified roles (Syntax+) (S,O,X). Alternatively,
roles can be reduced to show only the presence and
absence of an entity (Syntax-) (i.e., Entity Present
(P) or Not (N). Co-referential entities can be
resolved (Coreference+) or not (Coreference-).
Finally, the Salience option reflects the frequency
with which an entity appears in the discourse: if
the entity is mentioned two or more times, it is
salient (Salient+), otherwise, not (Salient-).
Consistent with 13L08, we systematically
completed runs using various configurations of
entity representations (see Section 4).
Given the combination, the entity transition
probabilities were computed for all labeled essays
in each data set. We used n-fold cross-validation
for evaluation. Feature vectors were input to C5.0,
a decision-tree machine learning application.
</bodyText>
<subsectionHeader confidence="0.99908">
3.2 Additional Features
</subsectionHeader>
<bodyText confidence="0.99989575">
In 13L08, augmenting the core coherence features
with additional features improved the power of the
algorithm. We extended the feature set with
writing quality features (Table 1). GUMS features
describe the technical quality of the essay. The
motivation for type/token features (*_TT) is to
measure word variety. For example, a high
probability for a “Subject-to-Subject” transition
indicates that the writer is repeating an entity in
Subject position across adjacent sentences.
However, this does not take into account whether
the same word is repeated or a variety of words are
used. The {S,O,X,SOX}_TT (type/token) features
uncover the actual words collapsed into the entity
transition probabilities. Shell nouns (Atkas &amp;
Cortes, 2008), common in essay writing, might
also affect coherence.
NNES essays can contain many spelling errors.
We evaluated the impact of a context-sensitive
spell checker (SPCR+), as spelling variation will
affect the transition probabilities in the entity grid.
Finally, we experimented with a majority vote
method that combined the best performing feature
combinations.
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9991022">
For all experiments, we used a series of n-fold
cross-validation runs with C5.0 to evaluate
performance for numerous feature configurations.
In Tables 2, 3 and 4, we report: baselines, results
on our data with BL08’s best system configuration
from the summary coherence evaluation task
(closest to our task), and our best systems. In the
Tables, “best systems” combined feature sets and
outperformed baselines. Rows in bold indicate
final independent best systems that contribute to
best performance in the majority vote method.
Agreement is reported as Weighted Kappa (WK),
Precision (P), Recall (R) and F-measure (F).
Baselines. We implemented three non-trivial
baseline systems. E-rater indicates use of the full
</bodyText>
<page confidence="0.996961">
682
</page>
<bodyText confidence="0.998107">
feature set from e-rater. The GUMS (GUMS+)
feature baseline, uses the Grammar (G+), Usage
</bodyText>
<table confidence="0.999474916666667">
Feature Descriptor Feature Description
GUMS Grammar, usage, and
mechanics errors, and style
features from an AES system
S_TT Type/token ratios for actual
O_TT words recovered from the
X_TT entity grid, using the entity
SOX_TT2 roles.
P_TT
S_TT_Shellnouns Type/token ratio of non-topic
O_TT_Shellnouns content, shell nouns (e.g.,
X_TT_Shellnouns approach, aspect, challenge)
</table>
<tableCaption confidence="0.999969">
Table 1: New feature category description
</tableCaption>
<bodyText confidence="0.997535464285714">
(U+), Mechanics (M+), and Style (ST+) flags
(subset of e-rater features) to evaluate a coherence
model. The third baseline represents the best run
using type/token features ({S,O,X,SOX}_TT), and
{S,O,X}_TT_Shellnouns feature sets (Table 1).
The baseline majority voting system includes e-
rater, GUMS, and the best performing type/token
baseline (see Tables 2-4).
Extended System. We combined our writing
quality features with the core BL08 feature set.
The combination improved performance over the
three baselines, and over the best performing BL08
feature. Type/token features added to BL08 entity
transitions probabilities improved performance of
all single systems. This supports the need to
recover actual word use. In Table 2, for TOEFL
data, spell correction improved performance with
the Mechanics error feature (where Spelling is
evaluated). This would suggest that annotators
were trying to ignore spelling errors when labeling
coherence. In Table 3, for GRE data, spell
correction improved performance with the
Grammar error feature. Spell correction did
change grammar errors detected: annotators may
have self-corrected for grammar. Finally, the
majority vote outperformed all systems. In Tables
3 and 4, Kappa was comparable to human
agreement (K=0.677).
</bodyText>
<sectionHeader confidence="0.991945" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.994399166666667">
We have evaluated how the BL08 algorithm
features can be used to model coherence for
2 Indicates an aggregate feature that computes the type/token
ratio for entities that appear in any of S,O,X role.
student essays across three different populations.
We found that the best coherence models for
essays are built by combining BL08 entity-based
features with writing quality features. BL08’s
outcomes showed that optimal performance was
obtained by using different feature sets for
different tasks. Our task was most similar to
BL08’s summary coherence task, but we used
noisy essay data. The difference in the data types
might also explain the need for our systems to
include additional writing quality features.
Our majority vote method outperformed three
baselines (and a baseline majority vote). For two of
the populations, Weighted Kappa between system
and human agreement was comparable. These
results show promise toward development of an
entity-based method that produces reliable
coherence ratings for noisy essay data. We plan to
evaluate this method on additional data sets, and in
the context of automated essay scoring.
</bodyText>
<sectionHeader confidence="0.99804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997251633333333">
Aktas, R. N., &amp; Cortes, V. (2008). Shell nouns as
cohesive devices in published and ESL student
writing. Journal of English for Academic Purposes,
7(1), 3–14.
Attali, Y., &amp; Burstein, J. (2006). Automated essay
scoring with e-rater v.2.0 . Journal of Technology,
Learning, and Assessment, 4(3).
Barzilay, R. and Lapata, M. (2008). Modeling local
coherence: An entity-based approach.
Computational Linguistics, 34(1), 1-34.
Foltz, P., Kintsch, W., and Landauer, T. K. (1998). The
measurement of textual coherence with Latent
Semantic Analysis. Discourse Processes,
25(2&amp;3):285–307.
Higgins, D., Burstein, J., Marcu, D., &amp; Gentile, C.
(2004). Evaluating multiple aspects of coherence in
student essays . In Proceedings of HLT-NAACL
2004, Boston, MA.
Grosz, B., Joshi, A., and Weinstein, S. 1995, Centering:
A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2): 203-
226.
Hearst, M. A. (1997). TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–6
Miltsakaki, E. and Kukich, K. (2000). Automated
evaluation of coherence in student essays. In
Proceedings of LREC 2000, Athens, Greece
Pitler, E.,and Nenkova, A (2008). Revisiting
Readability: A Unified Framework for Predicting
</reference>
<page confidence="0.998487">
683
</page>
<table confidence="0.986756">
Text Quality. In Proceedings of EMNLP 2008,
Honolulu, Hawaii.
L (n=64) H (n=196) L+H (n=260)
BASELINES: NO BL08 FEATURES WK P R F P R F P R F
(a) E-rater 0.472 56 69 62 89 82 86 79 79 79
(b) GUMS 0.455 55 66 60 88 83 85 79 79 79
(c) SOX_TT3 0.484 66 55 60 86 91 88 82 82 82
SYSTEMS: Includes BL08 FEATURES
Coreference-Syntax+Salient+ (B&amp;L08 0.253 49 34 40 81 88 84 75 75 75
summary task configuration)
(d) Coreference-Syntax-Salient-SPCR+M+ 0.472 76 45 57 84 95 90 83 83 83
(e) Coreference+Syntax+Salient-GUMS+ 0.590 68 70 69 90 89 90 85 85 85
(f) Coreference+Syntax+Salient- 0.595 68 72 70 91 89 90 85 85 85
GUMS+O_TT_Shellnouns+
Baseline Majority vote: (a),(b), (c) 0.450 55 64 59 88 83 85 79 79 79
Majority vote: (d), (e), (f) 0.598 69 70 70 90 90 90 85 85 85
</table>
<tableCaption confidence="0.996304">
Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement
</tableCaption>
<table confidence="0.502273357142857">
L (n=48) H (n=210) L+H (n=258)
BASELINES: NO BL08 FEATURES WK P R F P R F P R F
(a) E-rater 0.383 79 31 45 86 98 92 86 86 86
(b) GUMS 0.316 68 27 39 85 97 91 84 84 84
(c) e-rater+SOX_TT4 0.359 78 29 42 86 98 92 85 85 85
SYSTEMS: INCLUDES BL08 FEATURES
Coreference-Syntax+Salient+ (BL08 summary 0.120 35 17 23 83 93 88 79 79 79
task configuration)
(d) Coreference+Syntax+Salient-SPCR+G+ 0.547 1.0 43 60 89 1.0 94 90 90 90
(e) Coreference+Syntax-Salient-P_TT+ 0.462 70 44 54 88 96 92 86 86 86
(f) Coreference+Syntax+Salient+GUMS+ 0.580 71 60 65 91 94 93 88 88 88
SOX_TT+
Baseline Majority vote: (a),(b), (c) 0.383 79 31 45 86 98 92 86 86 86
Majority vote: (d), (e), (f) 0.610 1.0 49 66 90 1.0 95 91 91 91
</table>
<tableCaption confidence="0.99819">
Table 3: Native and Non-Native English Speaker Test-taker Data (GRE): Annotator/System Agreement
</tableCaption>
<table confidence="0.998726833333333">
L (n=37) H (n=226) L+H (n=263)
BASELINES: NO BL08 FEATURES WK P R F P R F P R F
(a) E-rater 0.315 39 46 42 91 88 89 82 82 82
(b) GUMS 0.350 47 41 43 90 92 91 85 85 85
(c) SOX_TT 0.263 78 19 30 88 99 93 88 88 88
SYSTEMS: INCLUDES BL08 FEATURES
(d) Coreference-Syntax+Salient+ (BL08 0.383 79 30 43 90 99 94 89 89 89
summary task configuration)
(e) Coreference-Syntax-Salient-SPCR+ 0.424 67 38 48 90 97 94 89 89 89
(f) Coreference+Syntax+Salient+S_TT+ 0.439 65 41 50 91 96 94 89 89 89
Baseline Majority vote: (a),(b), (c) 0.324 43 41 42 90 91 91 84 84 84
Majority vote: (d), (e), (f) 0.471 82 38 52 91 99 94 90 90 90
</table>
<tableCaption confidence="0.596117666666667">
Table 4: Criterion Essay Data: Annotator/System Agreement
3 Type/token ratios from all roles using a Coreference+Syntax+Salient+ grid.
4 Type/token ratios from all roles using Coreference+Syntax+Salient- grid.
</tableCaption>
<page confidence="0.996457">
684
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.403864">
<title confidence="0.999931">Using Entity-Based Features to Model Coherence in Student Essays</title>
<author confidence="0.987094">Jill</author>
<affiliation confidence="0.958008">Educational Testing</affiliation>
<address confidence="0.999306">Princeton, NJ 08541</address>
<author confidence="0.999726">Joel Tetreault</author>
<affiliation confidence="0.976254">Educational Testing Service</affiliation>
<address confidence="0.999058">Princeton, NJ 08541</address>
<email confidence="0.50796">Slava</email>
<affiliation confidence="0.904765">Educational Testing</affiliation>
<address confidence="0.999729">Princeton, NJ 08541</address>
<email confidence="0.95985">jburstein@ets.orgjtetreault@ets.orgsandreyev@ets.org</email>
<abstract confidence="0.998138">We show how the Barzilay and Lapata entitybased coherence algorithm (2008) can be to a new, noisy data domain We demonstrate that by combining and Lapata’s features with novel features related to grammar errors and word usage, one can greatly improve the performance of automated coherence prediction for student essays for different populations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R N Aktas</author>
<author>V Cortes</author>
</authors>
<title>Shell nouns as cohesive devices in published and ESL student writing.</title>
<date>2008</date>
<journal>Journal of English for Academic Purposes,</journal>
<volume>7</volume>
<issue>1</issue>
<pages>3--14</pages>
<marker>Aktas, Cortes, 2008</marker>
<rawString>Aktas, R. N., &amp; Cortes, V. (2008). Shell nouns as cohesive devices in published and ESL student writing. Journal of English for Academic Purposes, 7(1), 3–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Attali</author>
<author>J Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.2.0 .</title>
<date>2006</date>
<journal>Journal of Technology, Learning, and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="3226" citStr="Attali &amp; Burstein, 2006" startWordPosition="460" endWordPosition="463">ish speakers (NNES). Here, we evaluate how BL08 algorithm features can be used to model coherence in a new, noisy data domain -- student essays. We found that coherence can be best modeled by combining BL08 entity-based features with novel writing quality features. Further, our use of data sets from three different test-taker populations also shows that coherence models will differ across populations. Different populations might use language differently which could affect how coherence is presented. We expect to incorporate coherence ratings into erater®, ETS’s automated essay scoring system (Attali &amp; Burstein, 2006). 2 Corpus and Annotation We collected approximately 800 essays (in total) across three data sets1: 1) adult, NNES test essays (TOEFL); 2) adult, native and NNES test essays; (GRE) 3) U.S. middle- and high-school native and NNES student essay submissions to Criterion®, ETS’s instructional writing application. Two annotators were trained to rate coherence quality based on how easily they could read an essay without stumbling on a coherence barrier (i.e., a confusing sentence(s)). Annotators rated 1 TOEFL® is the Test of English as a Foreign Language, and GRE® is the Graduate Record Admissions T</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Attali, Y., &amp; Burstein, J. (2006). Automated essay scoring with e-rater v.2.0 . Journal of Technology, Learning, and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>1--34</pages>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Barzilay, R. and Lapata, M. (2008). Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1), 1-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Foltz</author>
<author>W Kintsch</author>
<author>T K Landauer</author>
</authors>
<title>The measurement of textual coherence with Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="1028" citStr="Foltz, et al. (1998)" startWordPosition="143" endWordPosition="146">o a new, noisy data domain – student essays. We demonstrate that by combining Barzilay and Lapata’s entity-based features with novel features related to grammar errors and word usage, one can greatly improve the performance of automated coherence prediction for student essays for different populations. 1 Introduction There is a small body of work that has investigated using NLP for the problem of identifying coherence in student essays. For example, Foltz, Kintsch &amp; Landauer (1998), and Higgins, Burstein, Marcu &amp; Gentile (2004) have developed systems that examine coherence in student writing. Foltz, et al. (1998) systems measure lexical relatedness between text segments by using vector-based similarity between adjacent sentences; Higgins et al’s (2004) system computes similarity across text segments. Foltz et al.’s (1998) approach is in line with the earlier TextTiling method that identifies subtopic structure in text (Hearst, 1997). Miltsakaki and Kukich (2000) addressed essay coherence using Centering Theory (Grosz, Joshi &amp; Weinstein, 1995). More recently, Barzilay and Lapata’s (2008) approach (henceforth, BL08) used an entity-based representation to evaluate coherence. In BL08, entities (nouns and </context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Foltz, P., Kintsch, W., and Landauer, T. K. (1998). The measurement of textual coherence with Latent Semantic Analysis. Discourse Processes, 25(2&amp;3):285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
<author>J Burstein</author>
<author>D Marcu</author>
<author>C Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays .</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<location>Boston, MA.</location>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>Higgins, D., Burstein, J., Marcu, D., &amp; Gentile, C. (2004). Evaluating multiple aspects of coherence in student essays . In Proceedings of HLT-NAACL 2004, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>A Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>203--226</pages>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Grosz, B., Joshi, A., and Weinstein, S. 1995, Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2): 203-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1354" citStr="Hearst, 1997" startWordPosition="190" endWordPosition="191">ll body of work that has investigated using NLP for the problem of identifying coherence in student essays. For example, Foltz, Kintsch &amp; Landauer (1998), and Higgins, Burstein, Marcu &amp; Gentile (2004) have developed systems that examine coherence in student writing. Foltz, et al. (1998) systems measure lexical relatedness between text segments by using vector-based similarity between adjacent sentences; Higgins et al’s (2004) system computes similarity across text segments. Foltz et al.’s (1998) approach is in line with the earlier TextTiling method that identifies subtopic structure in text (Hearst, 1997). Miltsakaki and Kukich (2000) addressed essay coherence using Centering Theory (Grosz, Joshi &amp; Weinstein, 1995). More recently, Barzilay and Lapata’s (2008) approach (henceforth, BL08) used an entity-based representation to evaluate coherence. In BL08, entities (nouns and pronouns) are represented by their sentence roles in a text. The algorithm keeps track of the distribution of entity transitions between adjacent sentences, and computes a value for all transition types based on their proportion of occurrence in a text. BL08 apply their algorithm to three tasks, using wellformed newspaper co</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, M. A. (1997). TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–6</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>K Kukich</author>
</authors>
<title>Automated evaluation of coherence in student essays.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC 2000,</booktitle>
<location>Athens, Greece</location>
<contexts>
<context position="1384" citStr="Miltsakaki and Kukich (2000)" startWordPosition="192" endWordPosition="195"> that has investigated using NLP for the problem of identifying coherence in student essays. For example, Foltz, Kintsch &amp; Landauer (1998), and Higgins, Burstein, Marcu &amp; Gentile (2004) have developed systems that examine coherence in student writing. Foltz, et al. (1998) systems measure lexical relatedness between text segments by using vector-based similarity between adjacent sentences; Higgins et al’s (2004) system computes similarity across text segments. Foltz et al.’s (1998) approach is in line with the earlier TextTiling method that identifies subtopic structure in text (Hearst, 1997). Miltsakaki and Kukich (2000) addressed essay coherence using Centering Theory (Grosz, Joshi &amp; Weinstein, 1995). More recently, Barzilay and Lapata’s (2008) approach (henceforth, BL08) used an entity-based representation to evaluate coherence. In BL08, entities (nouns and pronouns) are represented by their sentence roles in a text. The algorithm keeps track of the distribution of entity transitions between adjacent sentences, and computes a value for all transition types based on their proportion of occurrence in a text. BL08 apply their algorithm to three tasks, using wellformed newspaper corpora: text ordering, summary </context>
</contexts>
<marker>Miltsakaki, Kukich, 2000</marker>
<rawString>Miltsakaki, E. and Kukich, K. (2000). Automated evaluation of coherence in student essays. In Proceedings of LREC 2000, Athens, Greece</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
</authors>
<title>Nenkova, A</title>
<date>2008</date>
<marker>Pitler, 2008</marker>
<rawString>Pitler, E.,and Nenkova, A (2008). Revisiting Readability: A Unified Framework for Predicting</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>