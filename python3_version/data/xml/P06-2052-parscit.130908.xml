<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.980378">
Efficient sentence retrieval based on syntactic structure
</title>
<author confidence="0.997518">
Ichikawa Hiroshi, Hakoda Keita, Hashimoto Taiichi and Tokunaga Takenobu
</author>
<affiliation confidence="0.999364">
Department of Computer Science, Tokyo Institute of Technology
</affiliation>
<email confidence="0.997766">
{ichikawa,hokoda,taiichi,take}@cl.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933055555556">
This paper proposes an efficient method
of sentence retrieval based on syntactic
structure. Collins proposed Tree Kernel
to calculate structural similarity. However,
structual retrieval based on Tree Kernel
is not practicable because the size of the
index table by Tree Kernel becomes im-
practical. We propose more efficient al-
gorithms approximating Tree Kernel: Tree
Overlapping and Subpath Set. These algo-
rithms are more efficient than Tree Kernel
because indexing is possible with practical
computation resources. The results of the
experiments comparing these three algo-
rithms showed that structural retrieval with
Tree Overlapping and Subpath Set were
faster than that with Tree Kernel by 100
times and 1,000 times respectively.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999835529411765">
Retrieving similar sentences has attracted much
attention in recent years, and several methods
have been already proposed. They are useful for
many applications such as information retrieval
and machine translation. Most of the methods
are based on frequencies of surface information
such as words and parts of speech. These methods
might work well concerning similarity of topics or
contents of sentences. Although the surface infor-
mation of two sentences is similar, their syntactic
structures can be completely different (Figure 1).
If a translation system regards these sentences as
similar, the translation would fail. This is because
conventional retrieval techniques exploit only sim-
ilarity of surface information such as words and
parts-of-speech, but not more abstract information
such as syntactic structures.
</bodyText>
<figureCaption confidence="0.850135">
Figure 1: Sentences similar in appearance but dif-
fer in syntactic structure
</figureCaption>
<bodyText confidence="0.998731">
Collins et al. (Collins, 2001a; Collins, 2001b)
proposed Tree Kernel, a method to calculate a sim-
ilarity between syntactic structures. Tree Kernel
defines the similarity between two syntactic struc-
tures as the number of shared subtrees. Retrieving
similar sentences in a huge corpus requires cal-
culating the similarity between a given query and
each of sentences in the corpus. Building an index
table in advance could improve retrieval efficiency,
but indexing with Tree Kernel is impractical due to
the size of its index table.
In this paper, we propose two efficient algo-
</bodyText>
<figure confidence="0.998053727272727">
NP
VP
ribbon
He knows the girl with a
stick
He beats a dog with a
S
NP
NP
N V DET N P DET
N
NP
PP
VP
S
VP
NP
PP
NP
NP
N V DET N P DET
N
</figure>
<page confidence="0.987209">
399
</page>
<note confidence="0.7259475">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 399–406,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998690125">
rithms to calculate similarity of syntactic struc-
tures: Tree Overlapping and Subpath Set. These
algorithms are more efficient than Tree Kernel be-
cause it is possible to make an index table in rea-
sonable size. The experiments comparing these
three algorithms showed that Tree Overlapping is
100 times faster and Subpath Set is 1,000 times
faster than Tree Kernel when being used for struc-
tural retrieval.
After briefly reviewing Tree Kernel in section 2,
in what follows, we describe two algorithms in
section 3 and 4. Section 5 describes experiments
to compare these three algorithms and discussion
on the results. Finally, we conclude the paper and
look at the future direction of our research in sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.996782" genericHeader="method">
2 Tree Kernel
</sectionHeader>
<subsectionHeader confidence="0.997293">
2.1 Definition of similarity
</subsectionHeader>
<bodyText confidence="0.997218533333333">
Tree Kernel is proposed by Collins et al. (Collins,
2001a; Collins, 2001b) as a method to calculate
similarity between tree structures. Tree Kernel de-
fines similarity between two trees as the number
of shared subtrees. Subtree S of tree T is defined
as any tree subsumed by T, and consisting of more
than one node, and all child nodes are included if
any.
Tree Kernel is not always suitable because the
desired properties of similarity are different de-
pending on applications. Takahashi et al. pro-
posed three types of similarity based on Tree Ker-
nel (Takahashi, 2002). We use one of the similar-
ity measures (equation (1)) proposed by Takahashi
et al.
</bodyText>
<equation confidence="0.9954305">
KC(T1,T2) = max C(n1,n2) (1)
n1EN1, n2EN2
</equation>
<bodyText confidence="0.999925">
where C(n1, n2) is the number of shared subtrees
by two trees rooted at nodes n1 and n2.
</bodyText>
<subsectionHeader confidence="0.999815">
2.2 Algorithm to calculate similarity
</subsectionHeader>
<bodyText confidence="0.999627666666667">
Collins et al. (Collins, 2001a; Collins, 2001b)
proposed an efficient method to calculate Tree
Kernel by using C(n1, n2) as follows.
</bodyText>
<listItem confidence="0.986289142857143">
• If the productions at n1 and n2 are different
C(n1, n2) = 0
• If the productions at n1 and n2 are the
same, and n1 and n2 are pre-terminals, then
C(n1, n2) = 1
• Else if the productions at n1 and n2 are the
same and n1 and n2 are not pre-terminals,
</listItem>
<equation confidence="0.777362">
(1 + C(ch(n1, i), ch(n2, i)))
</equation>
<bodyText confidence="0.98116775">
(2)
where nc(n) is the number of children of node n
and ch(n, i) is the i’th child node of n. Equa-
tion (2) recursively calculates C on its child node,
and calculating Cs in postorder avoids recalcula-
tion. Thus, the time complexity of KC(T1, T2) is
O(mn), where m and n are the numbers of nodes
in T1 and T2 respectively.
</bodyText>
<subsectionHeader confidence="0.998802">
2.3 Algorithm to retrieve sentences
</subsectionHeader>
<bodyText confidence="0.999984470588235">
Neither Collins nor Takahashi discussed retrieval
algorithms using Tree Kernel. We use the follow-
ing simple algorithm. First we calculate the simi-
larity KC(T1, T2) between a query tree and every
tree in the corpus and rank them in descending or-
der of KC.
Tree Kernel exploits all subtrees shared by trees.
Therefore, it requires considerable amount of time
in retrieval because similarity calculation must be
performed for every pair of trees. To improve re-
trieval time, an index table can be used in general.
However, indexing by all subtrees is difficult be-
cause a tree often includes millions of subtrees.
For example, one sentence in Titech Corpus (Noro
et al., 2005) with 22 words and 87 nodes includes
8,213,574,246 subtrees. The number of subtrees
in a tree with N nodes is bounded above by 2N.
</bodyText>
<sectionHeader confidence="0.998088" genericHeader="method">
3 Tree Overlapping
</sectionHeader>
<subsectionHeader confidence="0.999973">
3.1 Definition of similarity
</subsectionHeader>
<bodyText confidence="0.999179071428571">
When putting an arbitrary node n1 of tree T1 on
node n2 of tree T2, there might be the same pro-
duction rule overlapping in T1 and T2. We define
CTO(n1,n2) as the number of such overlapping
production rules when n1 overlaps n2 (Figure 2).
We will define CTO(n1,n2) more precisely.
First we define L(n1, n2) of node n1 of T1 and
node n2 of T2. L(n1, n2) represents a set of pairs
of nodes which overlap each other when putting
n1 on n2. For example in Figure 2, L(b11, b21) =
I(b11,b21), (d11,d21),(e11, e21), (911,921), (i11,j21)}.
L(n1, n2) is defined as follows. Here ni and mi
are nodes of tree Ti, ch(n, i) is the i’th child of
node n.
</bodyText>
<equation confidence="0.995752">
1. (n1, n2) E L(n1, n2)
C(n1, n2) =
nc(n1)
�
i=1
</equation>
<page confidence="0.996214">
400
</page>
<figure confidence="0.9989453">
1
1
a
1
b c
1
j
CTO(g11,g21) = 1
1
1
(3)
2
1
i
d e
2
1
1
i1
g
b2
1
1
gg1
2
1
2
1
2
1
2
1
1
d11 e
a1
1
1
i
1
1
2
2
g
g
2
1
1
1
j
j
i 2
1
2
1
1
a T2 a
1
1
1
2
1
1
b c
1
b2
1
g
1
1
2
i
1
d e
1
1
d e
2
1
(1) T1
1
1
a
a2
1
(2)
1
b 2
b c
1 1
1
1
i2
1
e 2
1
1
1
g g 2
2
1
1
2
1
g
d e
d 2 1
1 1
2
1
CTO(b11,b21) = 2
</figure>
<figureCaption confidence="0.996154">
Figure 2: Example of similarity calculation
</figureCaption>
<listItem confidence="0.571166833333333">
2. If (m1, m2) ∈ L(n1, n2),
(ch(m1, i), ch(m2, i)) ∈ L(n1, n2)
3. If (ch(m1, i), ch(m2, i)) ∈ L(n1, n2),
(m1, m2) ∈ L(n1, n2)
4. L(n1, n2) includes only pairs generated by
applying 2. and 3. recursively.
</listItem>
<equation confidence="0.900762571428571">
CTO(n1, n2) is defined by using L(n1, n2) as
follows.
CTO(n1, n2)
= �� (m1, m2) ��������� m1 ∈ NT(T1) } ,
���� ∧ m2 ∈ NT(T2)
��� ∧ (m1, m2) ∈ L(n1, n2)
���� ∧ PR(m1) = PR(m2)
</equation>
<bodyText confidence="0.917538277777778">
���
� �
where NT (T) is a set of nonterminal nodes in tree
T, PR(n) is a production rule rooted at node n.
Tree Overlapping similarity STO(T1,T2) is de-
fined as follows by using CTO(n1, n2).
CTO(n1, n2)
This formula corresponds to equation (1) of Tree
Kernel.
As an example, we calculate STO(T1, T2) in
Figure 2 (1). Putting b11 on b2 1 gives Figure 2 (2)
in which two production rules b → d e and e → g
overlap respectively. Thus, CTO(b1 1, b21) becomes
2. While overlapping g11 and g21 gives Figure 2 (3)
in which only one production rule g → i overlaps.
Thus, CTO(g11, g21) becomes 1. Since there are no
other node pairs which gives larger CTO than 2,
STO(T1,T2) becomes 2.
</bodyText>
<tableCaption confidence="0.997359">
Table 1: Example of the index table
</tableCaption>
<figure confidence="0.990194857142857">
p I[p]
a → b c {a11}
b → d e {b11,b21}
e → g {e11, e21}
g→ i {g11, g21}
a → g b {a21}
g → j {g21}
</figure>
<subsectionHeader confidence="0.99311">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.999955307692308">
Let us take an example in Figure 3 to explain the
algorithm. Suppose that T0 is a query tree and the
corpus has only two trees, T1 and T2.
The method to find the most similar tree to a
given query tree is basically the same as Tree Ker-
nel’s (section 2.2). However, unlike Tree Kernel,
Tree Overlapping-based retrieval can be acceler-
ated by indexing the corpus in advance. Thus,
given a tree corpus, we build an index table I[p]
which maps a production rule p to its occurrences.
Occurrences of production rules are represented
by their left-hand side symbols, and are distin-
guished with respect to trees including the rule and
</bodyText>
<equation confidence="0.995357">
STO(T1, T2) = max
n1ENT(T1) n2ENT(T2)
</equation>
<page confidence="0.992547">
401
</page>
<figure confidence="0.998050797872341">
0 1 2
(1) T0 a T1 a T2 a
1 1 1
11
2
0
1
1
b c
1
b 10 c
2
b 1
1
0
g
2
d e
1
1
d e
2
1
d e
0
1
2
i
1
1
1
1
1
1
1
2
g
g2
j2
1
1
1
i
(2) a 1 0
a (3) a
1 1
2
a0 1
1
1
1
2
1
g
2
b b c
0
1 1
b c
b1 0
1 1
c0
1
0
1
d e
d 2 0
1
1
e 2
1
0
2
i
1
1
d e
d 1 0
1
1
e1
1
0
1
2
g
1
1
i 1
1
g2
j2
1
Score: 2 pt. Score: 1 pt.
</figure>
<figureCaption confidence="0.999985">
Figure 3: Example of Tree Overlapping-based retrieval
</figureCaption>
<bodyText confidence="0.940339416666667">
the position in the tree. I[p] is defined as follows.
where F is the corpus (here {T1, T21) and the
meaning of other symbols is the same as the defi-
nition of CTO (equation (3)).
Table 1 shows an example of the index table
generated from T1 and T2 in Figure 3 (1). In Ta-
ble 1, a superscript of a nonterminal symbol iden-
tifies a tree, and a subscript identifies a position in
the tree.
By using the index table, we calculate C[n, m]
with the following algorithm.
for all (n, m) do C[n, m] := 0 end
</bodyText>
<equation confidence="0.9112375">
foreach n in NT(T0) do
foreach m in I[PR(n)] do
(n0, m0) := top(n, m)
C[n0, m0] := C[n0, m0] + 1
end
end
</equation>
<bodyText confidence="0.9999874">
where top(n, m) returns the upper-most pair of
overlapped nodes when node n and m overlap.
The value of top uniquely identifies a situation of
overlapping two trees. Function top(n, m) is cal-
culated by the following algorithm.
</bodyText>
<equation confidence="0.882833777777778">
function top(n, m);
begin
(n0, m0) := (n, m)
while order(n0) = order(m0) do
n0 := parent(n0)
m0 := parent(m0)
end
return (n0, m0)
end
</equation>
<bodyText confidence="0.999681222222222">
where parent(n) is the parent node of n, and
order(n) is the order of node n among its siblings.
Table 2 shows example values of top(n, m) gen-
erated by overlapping T0 and T1 in Figure 3. Note
that top maps every pair of corresponding nodes
in a certain overlapping situation to a pair of the
upper-most nodes of that situation. This enables
us to use the value of top as an identifier of a situ-
ation of overlap.
</bodyText>
<tableCaption confidence="0.826231">
Table 2: Examples of top(n, m)
</tableCaption>
<equation confidence="0.8627077">
(n, m) top(n, m)
(a01, a11) (a01, a11)
(b01, b11) (a01, a11)
(c01, c11) (a01, a1 1)
Now C[top(n, m)] = CTO(n, m), therefore the
tree similarity between a query tree T0 and each
tree T in the corpus STO(T0, T )can be calculated
by:
STO(T0, T) = max C[top(n, m)]
n∈NT(T0), m∈NT(T) (6)
</equation>
<subsectionHeader confidence="0.998953">
3.3 Comparison with Tree Kernel
</subsectionHeader>
<bodyText confidence="0.998577">
The value of STO(T1,T2) roughly corresponds to
the number of production rules included in the
largest sub-tree shared by T1 and T2. Therefore,
this value represents the size of the subtree shared
</bodyText>
<figure confidence="0.583820333333333">
I[p] = I m ������� TEF } (5)
∧ m E NT(T)
∧ p = PR(m)
</figure>
<page confidence="0.995428">
402
</page>
<bodyText confidence="0.9997338">
by both trees, like Tree Kernel’s Kc, though the
definition of the subtree size is different.
One difference is that Tree Overlapping consid-
ers shared subtrees even though they are split by a
nonshared node as shown in Figure 4. In Figure 4,
T1 and T2 share two subtrees rooted at b and c, but
their parent nodes are not identical. While Tree
Kernel does not consider the superposition putting
node a on h, Tree Overlapping considers putting a
on h and assigns count 2 to this superposition.
</bodyText>
<figureCaption confidence="0.9677365">
Figure 4: Example of counting two separated
shared subtrees as one
</figureCaption>
<bodyText confidence="0.999598">
Another, more important, difference is that Tree
Overlapping retrieval can be accelerated by index-
ing the corpus in advance. The number of indexes
is bounded above by the number of production
rules, which is within a practical index size.
</bodyText>
<sectionHeader confidence="0.997197" genericHeader="method">
4 Subpath Set
</sectionHeader>
<subsectionHeader confidence="0.999625">
4.1 Definition of similarity
</subsectionHeader>
<bodyText confidence="0.9945902">
Subpath Set similarity between two trees is de-
fined as the number of subpaths shared by the
trees. Given a tree, its subpaths is defined as a
set of every path from the root node to leaves and
their partial paths.
</bodyText>
<figureCaption confidence="0.71854125">
Figure 5 (2) shows all subpaths in T1 and T2 in
Figure 5(1). Here we denotes a path as a sequence
of node names such as (a, b, d). Therefore, Sub-
path Set similarity of T1 and T2 becomes 15.
</figureCaption>
<subsectionHeader confidence="0.992652">
4.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.999798">
Suppose T0 is a query tree, T5 is a set of trees in
the corpus and P(T) is a set of subpaths of T. We
can build an index table I[p] for each production
rule p as follows.
</bodyText>
<equation confidence="0.961351">
I[p] = {T|T E T5 n p E P(T)} (7)
</equation>
<bodyText confidence="0.998446">
Using the index table, we can calculate the num-
ber of shared subpaths by T0 and T, 5[T], by the
following algorithm:
</bodyText>
<equation confidence="0.886722833333333">
for all T 5[T] := 0;
foreach p in P(T0) do
foreach T in I[p] do
5[T] := 5[T] + 1
end
end
</equation>
<subsectionHeader confidence="0.997294">
4.3 Comparison with Tree Kernel
</subsectionHeader>
<bodyText confidence="0.999963285714286">
As well as Tree Overlapping, Subpath Set retrieval
can be accelerated by indexing the corpus. The
number of indexes is bounded above by L x D2
where L is the maximum number of leaves of trees
(the number of words in a sentence) and D is the
maximum depth of syntactic trees. Moreover, con-
sidering a subpath as an index term, we can use
existing retrieval tools.
Subpath Set uses less structural information
than Tree Kernel and Tree Overlapping. It does
not distinguish the order and number of child
nodes. Therefore, the retrieval result tends to be
noisy. However, Subpath Set is faster than Tree
Overlapping, because the algorithm is simpler.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99985775">
This section describes the experiments which were
conducted to compare the performance of struc-
ture retrieval based on Tree Kernel, Tree Overlap-
ping and Subpath Set.
</bodyText>
<subsectionHeader confidence="0.988081">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.9999816">
We conducted two experiments using different an-
notated corpora. Titech corpus (Noro et al., 2005)
consists of about 20,000 sentences of Japanese
newspaper articles (Mainiti Shimbun). Each sen-
tence has been syntactically annotated by hand.
Due to the limitation of computational resources,
we used randomly selected 2,483 sentences as a
data collection.
Iwanami dictionary (Nishio et al., 1994) is a
Japanese dictionary. We extracted 57,982 sen-
tences from glosses in the dictionary. Each sen-
tences was analyzed with a morphological an-
alyzer, ChaSen (Asahara et al., 1996) and the
MSLR parser (Shirai et al., 2000) to obtain syntac-
tic structure candidates. The most probable struc-
ture with respect to PGLR model (Inui et al., 1996)
was selected from the output of the parser. Since
they were not investigated manually, some sen-
tences might have been assigned incorrect struc-
tures.
</bodyText>
<subsectionHeader confidence="0.999019">
5.2 Method
</subsectionHeader>
<bodyText confidence="0.997115">
We conducted two experiments Experiment I and
Experiment II with different corpora. The queries
</bodyText>
<figure confidence="0.997894076923077">
h
(3)
ah
(1) T1 a (2) T2
d e f g
STO(T1,T2) = 2
b c
bb cc
d e f g
d e f g
d e
f g
b c
</figure>
<page confidence="0.669511">
403
</page>
<figure confidence="0.996254230769231">
i j
b c
d e
g
g
i
g
b
d e
(1) T1 a T2 a
(2) Subpaths of T1
(c),
(a,c),
(e,g,i),
(b,e,g,i),
(a,b,e,g,i)
(a), (b), (d), (e), (g), (i),
(a,b), (b,d), (b,e), (e,g), (g,i),
(a,b,d), (a, b, e), (b,e,g),
(a,b,e,g)
(j),
(a,g), (g,j),
(a,g,i), (e,g,j),
(b,e,g,j),
(a,b,e,g,j)
SSS(T1,T2) = 15 Subpaths of T2
</figure>
<figureCaption confidence="0.99998">
Figure 5: Example of subpaths
</figureCaption>
<bodyText confidence="0.994104">
were extracted from these corpora. The algorithms
described in the preceding sections were imple-
mented with Ruby 1.8.2. Table 3 outlines the ex-
periments.
</bodyText>
<tableCaption confidence="0.994514">
Table 3: Summary of experiments
</tableCaption>
<table confidence="0.629416714285714">
Experiment I II
Target corpus Titech Corpus Iwanami dict.
Corpus size 2,483 sent. 57,982 sent.
No. of queries 100 1,000
CPU Intel Xeon PowerPC G5
(2.4GHz) (2.3GHz)
Memory 2GB 2GB
</table>
<subsectionHeader confidence="0.719021">
5.3 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999834333333333">
Since we select a query from the target corpus,
the query is always ranked in the first place in the
retrieval result. In what follows, we exclude the
query tree as an answer from the result.
We evaluated the algorithms based on the fol-
lowing two factors: average retrieval time (CPU
time) (Table 4) and the rank of the tree which was
top-ranked in other algorithm (Table 5). For ex-
ample, in Experiment I of Table 5, the column
“≥5th” of the row “TO/TK” means that there were
73 % of the cases in which the top-ranked tree by
Tree Kernel (TK) was ranked 5th or above by Tree
Overlapping (TO).
We consider Tree Kernel (TK) as the baseline
method because it is a well-known existing simi-
larity measure and exploits more information than
others. Table 4 shows that in both corpora, the
retrieval speed of Tree Overlapping (TO) is about
</bodyText>
<tableCaption confidence="0.993821">
Table 4: Average retrieval time per query [sec]
</tableCaption>
<table confidence="0.61373325">
Algorithm Experiment I Experiment II
TK 529.42 3796.1
TO 6.29 38.3
SS 0.47 5.1
</table>
<bodyText confidence="0.997599037037037">
100 times faster than that of Tree Kernel, and the
retrieval speed of Subpath Set (SS) is about 1,000
times faster than that of Tree Kernel. This re-
sults show we have successfully accelerated the
retrieval speed.
The retrieval time of Tree Overlapping, 6.29
and 38.3 sec./per query, seems be a bit long. How-
ever, we can shorten this time if we tune the im-
plementation by using a compiler-type language.
Note that the current implementation uses Ruby,
an interpreter-type language.
Comparing Tree Overlapping and Subpath Set
with respect to Tree Kernel (see rows “TK/TO”
and “TK/SS”), the top-ranked trees by Tree Kernel
are ranked in higher places by Tree Overlapping
than by Subpath Set. This means Tree Overlap-
ping is better than Subpath Set in approximating
Tree Kernel.
Although the corpus of Experiment II is 20
times larger than that of Experiment I, the figures
of Experiment II is better than that of Experiment I
in Table 5. This could be explained as follows.
In Experiment II, we used sentences from glosses
in the dictionary, which tend to be formulaic and
short. Therefore we could find similar sentences
easier than in Experiment I.
To summarize the results, when being used in
</bodyText>
<page confidence="0.999249">
404
</page>
<tableCaption confidence="0.9757305">
Table 5: The rank of the top-ranked tree by other
algorithm [%]
</tableCaption>
<table confidence="0.977269">
Experiment I
A/B 1st &gt; 5th &gt; 10th
TO/TK 34.0 73.0 82.0
SS/TK 16.0 35.0 45.0
TK/TO 29.0 41.0 51.0
SS/TO 27.0 49.0 58.0
TK/SS 17.0 29.0 37.0
TO/SS 29.0 58.0 69.0
Experiment II
A/B 1st &gt; 5th &gt; 10th
TO/TK 74.6 88.0 92.0
SS/TK 65.3 78.8 84.1
TK/TO 71.1 81.0 84.6
SS/TO 73.4 86.0 89.8
TK/SS 65.5 75.9 79.7
TO/SS 76.1 87.7 92.0
</table>
<bodyText confidence="0.999745541666667">
structure with the current one. For such purpose,
Tree Overlapping and Subpath Set algorithms con-
tribute to speed up the retrieval process, thus make
the annotation process more efficient.
However, “similarity” of sentences is affected
by semantic aspects as well as structural aspects.
The output of the algorithms do not always con-
form with human’s intuition. For example, the
two sentences in Figure 6 have very similar struc-
tures including particles, but they are hardly con-
sidered similar from human’s viewpoint. With this
respect, it is hardly to say which algorithm is su-
perior to others.
As a future work, we need to develop a method
to integrate both content-based and structure-
based similarity measures. To this end, we have
to evaluate the algorithms in real application envi-
ronments (e.g. information retrieval and machine
translation) because desired properties of similar-
ity are different depending on applications.
similarity calculation of tree structure retrieval,
Tree Overlapping approximates Tree Kernel bet-
ter than Subpath Set, while Subpath Set is faster
than Tree Overlapping.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999816">
We proposed two fast algorithms to retrieve sen-
tences which have a similar syntactic structure:
Tree Overlapping (TO) and Subpath Set (SS). And
we compared them with Tree Kernel (TK) to ob-
tain the following results.
</bodyText>
<listItem confidence="0.833729125">
• Tree Overlapping-based retrieval outputs
similar results to Tree Kernel-based retrieval
and is 100 times faster than Tree Kernel-
based retrieval.
• Subpath Set-based retrieval is not so good
at approximating Tree Kernel-based retrieval,
but is 1,000 times faster than Tree Kernel-
based retrieval.
</listItem>
<bodyText confidence="0.999915666666667">
Structural retrieval is useful for annotationg cor-
pora with syntactic information (Yoshida et al.,
2004). We are developing a corpus annotation tool
named “eBonsai” which supports human to anno-
tate corpora with syntactic information and to re-
trieve syntactic structures. Integrating annotation
and retrieval enables annotators to annotate a new
instance with looking back at the already anno-
tated instances which share the similar syntactic
</bodyText>
<sectionHeader confidence="0.999096" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860233333334">
Asahara, M. and Matsumoto, Y., Extended Models and
Tools for High-performance Part-of-Speech Tagger.
Proceedings of COLING 2000, 2000.
Collins, M. and Duffy, N. Parsing with a Single Neu-
ron: Convolution Kernels for Natural Language
Problems. Technical report UCSC-CRL-01-01, Uni-
versity of California at Santa Cruz, 2001.
Collins, M. and Duffy, N. Convolution Kernels for Nat-
ural Language. In Proceedings of NIPS 2001, 2001.
Inui, K., Shirai, K., Tokunaga T. and Tanaka H., The In-
tegration of Statistics-based Techniques in the Anal-
ysis of Japanese Sentences. Special Interest Group
of Natural Language Processing, Information Pro-
cessing Society of Japan, Vol. 96, No. 114, 1996.
Nagao, M. A framework of a mechanical translation
between Japanese and English by analogy principle.
In Alick Elithorn and Ranan Banerji, editors, Artif-
ical and Human Intelligence, pages 173-180. Ams-
terdam, 1984.
Noro, T., Koike, C., Hashimoto, T., Tokunaga, T. and
Tanaka, H. Evaluation of a Japanese CFG Derived
from a Syntactically Annotated Corpus with respect
to Dependency Measures, The 5th Workshop on
Asian Language Resources, pp.9-16, 2005.
Nishio, M., Iwabuchi, E. and Mizutani, S. (ed.)
Iwanami Kokugo Jiten, Iwanamishoten, 5th Edition,
1994.
Shirai, K., Ueki, M. Hashimoto, T., Tokunaga, T. and
Tanaka, H., MSLR Parser Tool Kit - Tools for Natu-
ral Language Analysis. Journal of Natural Language
</reference>
<page confidence="0.998028">
405
</page>
<figure confidence="0.922773333333333">
... (classroom) (to) (young) (a teaching material company) (of) (man) (SBJ) (came)
&amp;quot;A young man of a teaching material company came to the classroom&amp;quot;
&amp;quot;A piece of the exploded bombshell hit his head&amp;quot;
</figure>
<figureCaption confidence="0.999226">
Figure 6: Example of a retrieved similar sentence
</figureCaption>
<reference confidence="0.9974803125">
Processing, Vol. 7, No. 5, pp. 93-112, 2000. (in
Japanese)
Somers, H., McLean, I., Jones, D. Experiments in mul-
tilingual example-based generation. CSNLP 1994:
3rd conference on the Cognitive Science of Natural
Language Processing, Dublin, 1994.
Takahashi, T., Inui K., and Matsumoto, Y.. Methods
of Estimating Syntactic Similarity. Special Interest
Group of Natural Language Processing, Information
Processing Society of Japan, NL-150-7, 2002. (in
Japanese)
Yoshida, K., Hashimoto, T., Tokunaga, T. and Tanaka,
H.. Retrieving annotated corpora for corpus annota-
tion. Proceedings of 4th International Conference on
Language Resources and Evaluation: LREC 2004.
pp.1775 – 1778. 2004.
</reference>
<figure confidence="0.999270133333333">
が
きました
Query
PP
NP
N P ADJ N P N
学級 に、 若い 教材会社 の 青年
NP
...
V
PP
VP
S
P
直撃した
が
(to) (exploded) (bombshell) (of) (piece) (SBJ) (hit)
... (head)
Top- ranked
PP
PP
NP
N P ADJ N P N
頭 に、 着弾した 砲弾 の 破片
NP
...
V
S
VP
P
</figure>
<page confidence="0.989859">
406
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924010">
<title confidence="0.998811">Efficient sentence retrieval based on syntactic structure</title>
<author confidence="0.980764">Hakoda Keita Hiroshi</author>
<author confidence="0.980764">Hashimoto Taiichi Takenobu</author>
<affiliation confidence="0.999294">Department of Computer Science, Tokyo Institute of Technology</affiliation>
<abstract confidence="0.996720894736842">This paper proposes an efficient method of sentence retrieval based on syntactic structure. Collins proposed Tree Kernel to calculate structural similarity. However, structual retrieval based on Tree Kernel is not practicable because the size of the index table by Tree Kernel becomes impractical. We propose more efficient algorithms approximating Tree Kernel: Tree Overlapping and Subpath Set. These algorithms are more efficient than Tree Kernel because indexing is possible with practical computation resources. The results of the experiments comparing these three algorithms showed that structural retrieval with Tree Overlapping and Subpath Set were faster than that with Tree Kernel by 100 times and 1,000 times respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Asahara</author>
<author>Y Matsumoto</author>
</authors>
<title>Extended Models and Tools for High-performance Part-of-Speech Tagger.</title>
<date>2000</date>
<booktitle>Proceedings of COLING</booktitle>
<marker>Asahara, Matsumoto, 2000</marker>
<rawString>Asahara, M. and Matsumoto, Y., Extended Models and Tools for High-performance Part-of-Speech Tagger. Proceedings of COLING 2000, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Parsing with a Single Neuron: Convolution Kernels for Natural Language Problems.</title>
<date>2001</date>
<tech>Technical report UCSC-CRL-01-01,</tech>
<institution>University of California at Santa Cruz,</institution>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, M. and Duffy, N. Parsing with a Single Neuron: Convolution Kernels for Natural Language Problems. Technical report UCSC-CRL-01-01, University of California at Santa Cruz, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS</booktitle>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, M. and Duffy, N. Convolution Kernels for Natural Language. In Proceedings of NIPS 2001, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>K Shirai</author>
<author>T Tokunaga</author>
<author>H Tanaka</author>
</authors>
<title>The Integration of Statistics-based Techniques in the Analysis of Japanese Sentences.</title>
<date>1996</date>
<journal>Special Interest Group of Natural Language Processing, Information Processing Society of Japan,</journal>
<volume>96</volume>
<contexts>
<context position="14468" citStr="Inui et al., 1996" startWordPosition="2724" endWordPosition="2727">bout 20,000 sentences of Japanese newspaper articles (Mainiti Shimbun). Each sentence has been syntactically annotated by hand. Due to the limitation of computational resources, we used randomly selected 2,483 sentences as a data collection. Iwanami dictionary (Nishio et al., 1994) is a Japanese dictionary. We extracted 57,982 sentences from glosses in the dictionary. Each sentences was analyzed with a morphological analyzer, ChaSen (Asahara et al., 1996) and the MSLR parser (Shirai et al., 2000) to obtain syntactic structure candidates. The most probable structure with respect to PGLR model (Inui et al., 1996) was selected from the output of the parser. Since they were not investigated manually, some sentences might have been assigned incorrect structures. 5.2 Method We conducted two experiments Experiment I and Experiment II with different corpora. The queries h (3) ah (1) T1 a (2) T2 d e f g STO(T1,T2) = 2 b c bb cc d e f g d e f g d e f g b c 403 i j b c d e g g i g b d e (1) T1 a T2 a (2) Subpaths of T1 (c), (a,c), (e,g,i), (b,e,g,i), (a,b,e,g,i) (a), (b), (d), (e), (g), (i), (a,b), (b,d), (b,e), (e,g), (g,i), (a,b,d), (a, b, e), (b,e,g), (a,b,e,g) (j), (a,g), (g,j), (a,g,i), (e,g,j), (b,e,g,j)</context>
</contexts>
<marker>Inui, Shirai, Tokunaga, Tanaka, 1996</marker>
<rawString>Inui, K., Shirai, K., Tokunaga T. and Tanaka H., The Integration of Statistics-based Techniques in the Analysis of Japanese Sentences. Special Interest Group of Natural Language Processing, Information Processing Society of Japan, Vol. 96, No. 114, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japanese and English by analogy principle.</title>
<date>1984</date>
<booktitle>In Alick Elithorn and Ranan Banerji, editors, Artifical and Human Intelligence,</booktitle>
<pages>173--180</pages>
<location>Amsterdam,</location>
<marker>Nagao, 1984</marker>
<rawString>Nagao, M. A framework of a mechanical translation between Japanese and English by analogy principle. In Alick Elithorn and Ranan Banerji, editors, Artifical and Human Intelligence, pages 173-180. Amsterdam, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Noro</author>
<author>C Koike</author>
<author>T Hashimoto</author>
<author>T Tokunaga</author>
<author>H Tanaka</author>
</authors>
<title>Evaluation of a Japanese CFG Derived from a Syntactically Annotated Corpus with respect to Dependency Measures,</title>
<date>2005</date>
<booktitle>The 5th Workshop on Asian Language Resources,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="5774" citStr="Noro et al., 2005" startWordPosition="947" endWordPosition="950">orithms using Tree Kernel. We use the following simple algorithm. First we calculate the similarity KC(T1, T2) between a query tree and every tree in the corpus and rank them in descending order of KC. Tree Kernel exploits all subtrees shared by trees. Therefore, it requires considerable amount of time in retrieval because similarity calculation must be performed for every pair of trees. To improve retrieval time, an index table can be used in general. However, indexing by all subtrees is difficult because a tree often includes millions of subtrees. For example, one sentence in Titech Corpus (Noro et al., 2005) with 22 words and 87 nodes includes 8,213,574,246 subtrees. The number of subtrees in a tree with N nodes is bounded above by 2N. 3 Tree Overlapping 3.1 Definition of similarity When putting an arbitrary node n1 of tree T1 on node n2 of tree T2, there might be the same production rule overlapping in T1 and T2. We define CTO(n1,n2) as the number of such overlapping production rules when n1 overlaps n2 (Figure 2). We will define CTO(n1,n2) more precisely. First we define L(n1, n2) of node n1 of T1 and node n2 of T2. L(n1, n2) represents a set of pairs of nodes which overlap each other when putt</context>
<context position="13836" citStr="Noro et al., 2005" startWordPosition="2624" endWordPosition="2627">ndex term, we can use existing retrieval tools. Subpath Set uses less structural information than Tree Kernel and Tree Overlapping. It does not distinguish the order and number of child nodes. Therefore, the retrieval result tends to be noisy. However, Subpath Set is faster than Tree Overlapping, because the algorithm is simpler. 5 Experiments This section describes the experiments which were conducted to compare the performance of structure retrieval based on Tree Kernel, Tree Overlapping and Subpath Set. 5.1 Data We conducted two experiments using different annotated corpora. Titech corpus (Noro et al., 2005) consists of about 20,000 sentences of Japanese newspaper articles (Mainiti Shimbun). Each sentence has been syntactically annotated by hand. Due to the limitation of computational resources, we used randomly selected 2,483 sentences as a data collection. Iwanami dictionary (Nishio et al., 1994) is a Japanese dictionary. We extracted 57,982 sentences from glosses in the dictionary. Each sentences was analyzed with a morphological analyzer, ChaSen (Asahara et al., 1996) and the MSLR parser (Shirai et al., 2000) to obtain syntactic structure candidates. The most probable structure with respect t</context>
</contexts>
<marker>Noro, Koike, Hashimoto, Tokunaga, Tanaka, 2005</marker>
<rawString>Noro, T., Koike, C., Hashimoto, T., Tokunaga, T. and Tanaka, H. Evaluation of a Japanese CFG Derived from a Syntactically Annotated Corpus with respect to Dependency Measures, The 5th Workshop on Asian Language Resources, pp.9-16, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nishio</author>
<author>E Iwabuchi</author>
</authors>
<date>1994</date>
<booktitle>Iwanami Kokugo Jiten, Iwanamishoten, 5th Edition,</booktitle>
<editor>and Mizutani, S. (ed.)</editor>
<marker>Nishio, Iwabuchi, 1994</marker>
<rawString>Nishio, M., Iwabuchi, E. and Mizutani, S. (ed.) Iwanami Kokugo Jiten, Iwanamishoten, 5th Edition, 1994.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Shirai</author>
<author>M Hashimoto Ueki</author>
<author>T Tokunaga</author>
<author>T</author>
<author>H Tanaka</author>
</authors>
<title>MSLR Parser Tool Kit - Tools for Natural Language Analysis.</title>
<journal>Journal of Natural Language</journal>
<marker>Shirai, Ueki, Tokunaga, T, Tanaka, </marker>
<rawString>Shirai, K., Ueki, M. Hashimoto, T., Tokunaga, T. and Tanaka, H., MSLR Parser Tool Kit - Tools for Natural Language Analysis. Journal of Natural Language</rawString>
</citation>
<citation valid="true">
<authors>
<author>Processing</author>
</authors>
<date>2000</date>
<volume>7</volume>
<pages>93--112</pages>
<note>(in Japanese)</note>
<marker>Processing, 2000</marker>
<rawString>Processing, Vol. 7, No. 5, pp. 93-112, 2000. (in Japanese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Somers</author>
<author>I McLean</author>
<author>D Jones</author>
</authors>
<title>Experiments in multilingual example-based generation.</title>
<date>1994</date>
<booktitle>CSNLP 1994: 3rd conference on the Cognitive Science of Natural Language Processing,</booktitle>
<location>Dublin,</location>
<marker>Somers, McLean, Jones, 1994</marker>
<rawString>Somers, H., McLean, I., Jones, D. Experiments in multilingual example-based generation. CSNLP 1994: 3rd conference on the Cognitive Science of Natural Language Processing, Dublin, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takahashi</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Methods of Estimating Syntactic Similarity.</title>
<date>2002</date>
<booktitle>Special Interest Group of Natural Language Processing, Information Processing Society of Japan,</booktitle>
<pages>150--7</pages>
<note>(in Japanese)</note>
<marker>Takahashi, Inui, Matsumoto, 2002</marker>
<rawString>Takahashi, T., Inui K., and Matsumoto, Y.. Methods of Estimating Syntactic Similarity. Special Interest Group of Natural Language Processing, Information Processing Society of Japan, NL-150-7, 2002. (in Japanese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yoshida</author>
<author>T Hashimoto</author>
<author>T Tokunaga</author>
<author>H Tanaka</author>
</authors>
<title>Retrieving annotated corpora for corpus annotation.</title>
<date>2004</date>
<booktitle>Proceedings of 4th International Conference on Language Resources and Evaluation: LREC 2004. pp.1775 –</booktitle>
<pages>1778</pages>
<marker>Yoshida, Hashimoto, Tokunaga, Tanaka, 2004</marker>
<rawString>Yoshida, K., Hashimoto, T., Tokunaga, T. and Tanaka, H.. Retrieving annotated corpora for corpus annotation. Proceedings of 4th International Conference on Language Resources and Evaluation: LREC 2004. pp.1775 – 1778. 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>