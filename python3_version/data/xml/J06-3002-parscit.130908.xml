<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994372">
The Notion of Argument in Prepositional
Phrase Attachment
</title>
<author confidence="0.999573">
Paola Merlo* Eva Esteve Ferrer†
</author>
<affiliation confidence="0.999785">
University of Geneva University of Sussex
</affiliation>
<figureCaption confidence="0.970307583333333">
In this article we refine the formulation of the problem of prepositional phrase (PP) attachment as
a four-way disambiguation problem. We argue that, in interpreting PPs, both knowledge about
the site of the attachment (the traditional noun–verb attachment distinction) and the nature of
the attachment (the distinction of arguments from adjuncts) are needed. We introduce a method
to learn arguments and adjuncts based on a definition of arguments as a vector offeatures. In
a series of supervised classification experiments, first we explore the features that enable us to
learn the distinction between arguments and adjuncts. We find that both linguistic diagnostics
of argumenthood and lexical semantic classes are useful. Second, we investigate the best method
to reach the four-way classification of potentially ambiguous prepositional phrases. We find that
whereas it is overall better to solve the problem as a single four-way classification task, verb
arguments are sometimes more precisely identified if the classification is done as a two-step
process, first choosing the attachment site and then labeling it as argument or adjunct.
</figureCaption>
<sectionHeader confidence="0.929288" genericHeader="abstract">
1. Motivation
</sectionHeader>
<bodyText confidence="0.96804175">
Incorrect attachment of prepositional phrases (PPs) often constitutes the largest single
source of errors in current parsing systems. Correct attachment of PPs is necessary to
construct a parse tree that will support the proper interpretation of constituents in the
sentence. Consider the timeworn example
</bodyText>
<equation confidence="0.631111">
(1) I saw the man with the telescope.
</equation>
<bodyText confidence="0.9999572">
It is important to determine if the PP with the telescope is to be attached as a sister to
the noun the man, restricting its interpretation, or if it is to be attached to the verb,
thereby indicating the instrument of the main action described by the sentence. Based on
examples of this sort, recent approaches have formalized the problem of disambiguating
PP attachments as a binary choice, distinguishing between attachment of a PP to a given
verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and
Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina
and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004).
This is, however, a simplification of the problem, which does not take the nature
of the attachment into account. Precisely, it does not distinguish PP arguments from
</bodyText>
<affiliation confidence="0.7006485">
* Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland.
† Department of Informatics, University of Sussex, Falmer, Brighton BN19QH, UK.
</affiliation>
<note confidence="0.781913333333333">
Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication:
4 November 2005.
© 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
PP adjuncts. Consider the following example, which contains two PPs, both modifying
the verb.
</note>
<listItem confidence="0.501075">
(2) Put the block on the table in the morning.
</listItem>
<bodyText confidence="0.9830502">
The first PP is a locative PP required by the subcategorization frame of the verb
put, whereas in the morning is an optional descriptor of the time at which the ac-
tion was performed. Although both are attached to the verb, the two PPs entertain
different relationships with the verb—the first is an argument whereas the latter is
an adjunct. Analogous examples could be built for attachments to the noun. (See
examples 7a, b.)
Thus, PPs cannot only vary depending on the site to which they attach in the
structure, such as in example (1), but they can fulfill different functions in the sen-
tence, such as in example (2). In principle, then, a given PP could be four-way am-
biguous. In practice, it is difficult and moderately unnatural to construct examples
of four-way ambiguous sentences, sentences that only a good amount of linguis-
tic and extralinguistic knowledge can disambiguate among the noun-attached and
verb-attached option, with an argument or adjunct interpretation. It is, however, not
impossible.
Consider benefactive constructions, such as the sentence below.
</bodyText>
<listItem confidence="0.954355">
(3) Darcy baked a cake for Elizabeth.
</listItem>
<bodyText confidence="0.8945475">
In this case the for is a benefactive, hence an argument of the verb bake. However, the
for-PP is optional; thus other non-argument PPs can occur in the same position.
</bodyText>
<listItem confidence="0.93072">
(4) Darcy baked a cake for 5 shillings/for an hour.
</listItem>
<bodyText confidence="0.960429666666667">
Whereas in sentence (3) the PP is an argument, in (4) the PP is an adjunct, as indicated
by the different status of the corresponding passive sentences and by the ordering of the
PPs (arguments prefer to come first), as shown in (5) and (6).
</bodyText>
<listItem confidence="0.980534">
(5a) Elizabeth was baked a cake by Darcy
(5b) *5 shillings/an hour were baked a cake by Darcy
(6a) Darcy baked a cake for Elizabeth for 5 shillings/for an hour
(6b) ??Darcy baked a cake for 5 shillings/for an hour for Elizabeth
</listItem>
<bodyText confidence="0.9972468">
This kind of ambiguity also occurs in sentences in which the for-PP is modifying
the object noun phrase. Depending on the head noun in object position, and under
the assumption that a beneficiary is an argument, as we have assumed in the sen-
tences above, the PP will be an argument or an adjunct, as in the following examples,
respectively.
</bodyText>
<listItem confidence="0.9258615">
(7a) Darcy baked [cakes for children]
(7b) Darcy baked [cakes for 5 shillings]
</listItem>
<page confidence="0.988339">
342
</page>
<note confidence="0.955175">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.999538619047619">
Modeling both the site and the nature of the attachment of a PP into the tree
structure is important. Distinguishing arguments from adjuncts is key to identifying
the elements that belong to the semantic kernel of a sentence. Extracting the kernel
of a sentence or phrase, in turn, is necessary for automatic acquisition of important
lexical knowledge, such as subcategorization frames and argument structures, which
is used in several natural language processing (NLP) tasks and applications, such
as parsing, machine translation, and information extraction (Srinivas and Joshi 1999;
Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature
and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002;
Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also
the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of
arguments from adjuncts and an appropriate thematic labeling of the complements of a
predicate, verb, or noun are necessary, as confirmed by the annotations adopted by cur-
rent corpora. Framenet makes a distinction between complements and satellites (Baker,
Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between
arguments and adjuncts directly into the level of specificity of their annotation. They
adopt labels that are common across verbs for adjuncts. They inherit these labels from
the Penn Treebank annotation. Arguments are annotated instead with labels specific to
each verb (Xue 2004; Palmer, Gildea, and Kingsbury 2005).
From a quantitative point of view, arguments and adjuncts have different statistical
properties. For example, Hindle and Rooth (1993) clearly indicate that their lexical
association technique performs much better for arguments than for adjuncts, whether
the attachment is to the verb or to the noun.
Researchers have abstracted away from this distinction, because identifying ar-
guments and adjuncts is a notoriously difficult task, taxing many native speakers’
intuitions. The usual expectation has been that this discrimination is not amenable to
a corpus-based treatment. In recent preliminary work, however, we have succeeded
in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold
2001; Merlo 2003). Our method develops corpus-based statistical correlates for the
diagnostics used in linguistics to decide whether a PP is an argument or an adjunct.
A numerical vectorial representation of the notion of argumenthood is provided, which
supports automatic classification. In the current article, we expand and improve on this
work, by developing new measures and refining the previous ones. We also extend that
work to attachment to nouns. This extension enables us to explore in what way the dis-
tinction between argument and adjunct is best integrated in the traditional attachment
disambiguation problem.
We treat PP attachment as a four-way classification of PPs into noun argument
PPs, noun adjunct PPs, verb argument PPs, and verb adjunct PPs. We investigate this
new approach to PP attachment disambiguation through several sets of experiments,
testing different hypotheses on the argument/adjunct distinction of PPs and on its
interaction with the disambiguation of the PP attachment site. The two main claims
can be formulated as follows.
</bodyText>
<listItem confidence="0.994242">
• Hypothesis 1: The argument/adjunct distinction can be performed based
on information collected from a minimally annotated corpus,
approximating deeper semantic information statistically.
• Hypothesis 2: The learning features developed for the notion of argument
and adjunct can be usefully integrated in a finer-grained formulation of
the problem of PP attachment as a four-way classification.
</listItem>
<page confidence="0.997579">
343
</page>
<note confidence="0.805188">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999902105263158">
To test these two hypotheses, we illustrate our technique to distinguish argu-
ments from adjuncts (Section 2), and we report results on this binary classification
(Sections 3 and 4). The intuition behind the technique is that we do not need to represent
the distinction between arguments and adjuncts directly, but that the distinction can
be indirectly represented as a numerical vector. The feature values in the vector are
corpus-based numerical equivalents of the grammaticality diagnostics used by linguists
to decide whether a PP is an argument or an adjunct. For example, one of the values in
the vector indicates if the PP is optional, whereas another one indicates if the PP can be
iterated. Optionality and iterability are two of the criteria used by linguists to determine
whether a PP is an argument or an adjunct. In Section 5, we show how this distinction
supports a more refined formulation of the problem of PP attachment. We compare two
methods to reach a four-way classification. One method is a two-step process that first
classifies PPs as attached to the noun or to the verb, and then refines the classification
by assigning argument or adjunct status to the disambiguated PPs. The other method
is a one-step process that performs the four-way classification directly. We find that
the latter has better overall performance, confirming our expectation (Hypothesis 2).
In Section 6 we discuss the implications of the results for a definition of the notion of
argument and compare our work to that of the few researchers who have attempted to
perform the same distinction.
</bodyText>
<sectionHeader confidence="0.909806" genericHeader="keywords">
2. Distinguishing Arguments from Adjuncts
</sectionHeader>
<bodyText confidence="0.996213">
Solving the four-way classification task described in the introduction crucially relies
on the ability to distinguish arguments from adjuncts, using corpus counts. The ability
to automatically make this distinction is necessary for the correct automatic acquisi-
tion of important lexical knowledge, such as subcategorization frames and argument
structures, which is used in parsing, generation, machine translation, and information
extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach
1998). Yet, few attempts have been made to make this distinction automatically.
The core difficulty in this enterprise is to define the notion of argument precisely
enough that it can be used automatically. There is a consensus in linguistics that argu-
ments and adjuncts are different both with respect to their function in the sentence and
in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and
Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the
relation described by its associated head, whereas an adjunct predicates a separate prop-
erty of its associate head or phrase. With respect to their interpretation, a complement
is an argument if its interpretation depends exclusively on the head with which it is
associated, whereas it is an adjunct if its interpretation remains relatively constant when
associating with different heads (Grimshaw 1990, page 108). These semantic differences
give rise to some observable distributional consequences: for a given interpretation, an
adjunct can co-occur with a relatively broad range of heads, whereas arguments are
limited to co-occurrence with a (semantically restricted) class of heads (Pollard and Sag
1987, page 136).
Restricting the discussion to PPs, these differences are illustrated in the following
examples (PP-argument in bold); see also Sch¨utze (1995, page 100).
</bodyText>
<listItem confidence="0.903867">
(8a) Maria is a student of physics.
(8b) Maria is a student from Phoenix.
</listItem>
<page confidence="0.995888">
344
</page>
<note confidence="0.954285">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.9986385">
In example (8a), the head student implies that a subject is being studied. The sentence
tells us only one property of Maria: that she is a student of physics. In example (8b),
the PP instead predicates a different property of the student, namely her geographical
origin, which is not implied by the head student.
</bodyText>
<listItem confidence="0.8869275">
(9a) Kim camps/jogs/meditates on Sunday.
(9b) Kim depended/blamed the arson on Sandy.
</listItem>
<bodyText confidence="0.9999609">
In example (9a) the PP on Sunday can be construed without any reference to the pre-
ceding part of the sentence, and it preserves its meaning even when combining with
different heads. This is, however, not the case for (9b). Here, the PP can only be properly
understood in connection with the rest of the sentence: Sandy is the person on whom
someone depends or the person on which the arson is blamed.
These semantic distinctions between arguments and adjuncts surface in observable
syntactic differences and can be detected automatically both by using general formal
features and by specific lexical semantic features, which group together the arguments
of a lexical head. Unfortunately, the linguistic diagnostics that are used to determine
whether a PP is an adjunct or an argument are not accurate in all circumstances, they
often partition the set of the examples differently, and they give rise to relative, and not
absolute, acceptability judgments.
We propose a methodology that retains both the linguistic insight of the grammat-
ical tests and the ability to effectively combine several gradient, partial diagnostics,
typical of automatic induction methods. Specifically, we first find countable diagnostics
for the argument–adjunct distinction, which we approximate statistically and estimate
using corpus counts. We also augment the feature vector with information encoding
the semantic classes of the input words. The diagnostics and the semantic classes are
then automatically combined in a decision tree induction algorithm. The diagnostics
are presented below.
</bodyText>
<subsectionHeader confidence="0.939023">
2.1 The Diagnostics
</subsectionHeader>
<bodyText confidence="0.999864153846154">
Many diagnostics for argumenthood have been proposed in the literature (Sch¨utze
1995). Some of them require complex syntactic manipulation of the sentence, such as wh-
extraction, and are therefore too difficult to apply automatically. We choose six formal
diagnostics that can be captured by simple corpus counts: head dependence, optionality,
iterativity, ordering, copular paraphrase, and deverbal nominalization. These diagnos-
tics tap into the deeper semantic properties that distinguish arguments from adjuncts,
without requiring that the distinctions be made explicit.
Head Dependence. Arguments depend on their lexical heads because they form an inte-
gral part of the phrase. Adjuncts do not. Consequently, PP-arguments can only appear
with the specific verbal head by which they are lexically selected, whereas PP-adjuncts
can co-occur with a far greater range of different heads than arguments because they
are necessary for the correct interpretation of the semantics of the verb, as illustrated in
the following example sentences.
</bodyText>
<listItem confidence="0.9991245">
(10a) a man/woman/dog/moppet/scarecrow with gray hair
(10b) a menu/napkin/glass/waitress/matchbook from Rosie’s
</listItem>
<page confidence="0.99447">
345
</page>
<note confidence="0.601829">
Computational Linguistics Volume 32, Number 3
</note>
<listItem confidence="0.9315385">
(11a) a member/*dog/*moppet/*scarecrow of Parliament
(11b) a student/*punk/*watermelon/*Martian/*poodle/*VCR of physics
</listItem>
<bodyText confidence="0.998637555555555">
We expect an argument PP to occur with fewer heads, whereas an adjunct PP will occur
with more heads, as it is not required by a specific verb, but it can in principle adjoin to
any verb or noun head.
We capture this insight by estimating the dispersion of the distribution of the
different heads that co-occur with a given PP in a corpus. We expect adjunct PPs to have
higher dispersion than argument PPs. We use entropy as a measure of the dispersion
of the distribution, as indicated in equation (1) (h indicates the noun or verb head to
which the PP is attached, X is the random variable whose outcomes are the values
of h).
</bodyText>
<equation confidence="0.998679">
hdep(PP) ≈ HPP(X) = −Σh∈Xp(h)log2p(h) (1)
</equation>
<bodyText confidence="0.7224775">
Optionality. In most cases, PP-arguments are obligatory elements of a given sentence
whose absence leads to ungrammaticality, while adjuncts do not contribute to the
semantics of any particular verb, hence they are optional, as illustrated in the following
examples (PP-argument in bold):
</bodyText>
<listItem confidence="0.99922675">
(12a) John put the book in the room.
(12b) ∗John put the book.
(12c) John saw/read the book in the room.
(12d) John saw/read the book.
</listItem>
<bodyText confidence="0.996534666666667">
Since arguments are obligatory complements of a verb, whereas adjuncts are not, we
expect knowledge of a given verb to be more informative with respect to the probability
of existence of an argument than of an adjunct. Thus we expect that the predictive
power of a verb with regard to its complements will be greater for arguments than for
adjuncts.1 The notion of optionality can be captured by the conditional probability of a
PP given a particular verbal head, as indicated in equation (2).
</bodyText>
<equation confidence="0.997483">
opt(PP) ≈ P(PP|v) (2)
</equation>
<bodyText confidence="0.9958584">
Iterativity and Ordering. Because they receive a semantic role from the selecting verb,
arguments of the same type cannot be iterated because verbs can only assign any given
type of role once. Moreover, in English, arguments must be adjacent to the selecting
lexical head. Neither of these two restrictions apply to adjuncts, which can be iterated,
and follow arguments in a sequence of PPs. Consequently, in a sequence of several PPs
</bodyText>
<footnote confidence="0.997490714285714">
1 Notice that this diagnostic can only be interpreted as a statistical tendency, and not as a strict test,
because not all arguments are obligatory (but all adjuncts are indeed optional). The best known
descriptive exception to the criterion of optionality is the class of so-called object-drop verbs (Levin 1993).
Here a given verb may tolerate the omission of its argument. In other words, a transitive verb, such as
kiss, can also act like an intransitive. With respect to optional PPs, it has been argued that instrumentals
are arguments (Sch¨utze 1995). While keeping these exceptions in mind, we maintain optionality as a
valid diagnostic here.
</footnote>
<page confidence="0.997581">
346
</page>
<note confidence="0.717856">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.936404">
only the first one can be an argument, whereas the others must be adjuncts, as illustrated
in the examples below.
</bodyText>
<listItem confidence="0.9850305">
(13a) ∗Chris rented the gazebo to yuppies, to libertarians.
(13b) Kim met Sandy in Baltimore in the hotel lobby in a corner.
</listItem>
<bodyText confidence="0.9783122">
These two criteria combined give rise to one diagnostic. The probability of a PP
being able to iterate, and consequently being an adjunct, can be approximated as
the probability of its occurrence in second position in a sequence of PPs, as indi-
cated in equation (3), where we indicate the position of the PP in a sequence as a
subscript.
</bodyText>
<equation confidence="0.99932">
iter(PP1) ≈ P(PP2) (3)
</equation>
<bodyText confidence="0.992106">
Copular Paraphrase. The diagnostic of copular paraphrase is specific to the distinction of
NPs arguments and adjuncts, following Sch¨utze (1995, page 103). It does not apply to
VP arguments and adjuncts, as it requires paraphrasing the PP with a relative clause.
Arguments cannot be paraphrased by a copular relative clause, as the examples in (15)
show, whereas adjuncts can, as is shown in (14):
</bodyText>
<listItem confidence="0.9947085">
(14) a. a man from Paris a man who was from Paris
b. the albums on the shelf the albums that were on the shelf
c. the people on the payroll the people who were on the payroll
(15) a. the destruction of the city *the destruction that was of the city
b. the weight of the cow *the weight that was of the cow
c. a member of Parliament *a member who was of Parliament
</listItem>
<bodyText confidence="0.999040375">
This is because a PP attached to a noun as an argument does not predicate a secondary
property of the noun, but it specifies the same property that is indicated by the head.
To be able to use the relative clause construction, there must be two properties that are
being predicated of the same entity.
Thus, the probability that a PP is able to be paraphrased, and therefore that it
is an adjunct, can be approximated by the probability of its occurrence following a
construction headed by a copular verb, be, become, appear, seem, remain (Quirk et al. 1985),
as indicated in equation (4), where ≺ indicates linear precedence.
</bodyText>
<equation confidence="0.988947">
para(PP) ≈ P(vcopula ≺ PP) (4)
</equation>
<bodyText confidence="0.9948706">
Deverbal Nouns. This diagnostic is based on the observation that PPs following a de-
verbal noun are likely to be arguments, as the noun shares the argument structure
of the verb.2 Proper counting of this feature requires identifying a deverbal noun in
the head noun position of a noun phrase. We identify deverbal nouns by inspect-
ing their morphology (Quirk et al. 1985). Specifically, the suffixes that can combine
</bodyText>
<footnote confidence="0.9960295">
2 Doubts have been cast on the validity of this diagnostic (Sch¨utze 1995), based on work in theoretical
linguistics (Grimshaw 1990). Argaman and Pearlmutter (2002), however, have shown that the argument
structures of verbs and related nouns are highly correlated. Hence, we keep deverbal noun as a valid
diagnostic here, although we show later that it is not very effective.
</footnote>
<page confidence="0.988687">
347
</page>
<note confidence="0.302194">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.998014586206897">
with verb bases to form deverbal nouns are listed and exemplified in Figure 1 on
page 348. This diagnostic can be captured by a probability indicator function, which
assigns probability 1 of being an argument to PPs following a deverbal noun and
0 otherwise.
� deverb PP — 1 if deverbal n ≺ PP 5
( ) 0 otherwise ( )
In conclusion, the diagnostics of head dependence, optionality, iterativity, ordering,
copular paraphrase, and deverbal nominalization are promising indicators of the status
of PPs as either arguments or adjuncts. In Section 3 we illustrate how they can be
quantified in a faithful way and, thanks to their simplicity, how they can be estimated
in a sufficiently large corpus by simple counts.
Another class of features is also very important for the distinction between argu-
ments and adjuncts, the lexical semantic class to which the lexical heads belong, as we
illustrate below.
Lexical Semantic Class Features. According to Levin (1993), there is a regular mapping
between the syntactic and semantic behavior of a verb. This gives rise to a lexicon where
verbs that share similar syntactic and semantic properties are organized into classes.
More specifically, it is assumed that similar underlying components of meaning give rise
to similar subcategorization frames and projections of arguments at the syntactic level.
Since an argument participates in the subcategorization frame of the verb, whereas
an adjunct does not, we expect the argument-taking properties of verbs to be also
organized around semantic classes. We expect, therefore, that knowledge of the class of
the verb will be beneficial to the acquisition of the distinction between arguments and
adjuncts for an individual verb. For example, all verbs of giving take a dative indirect
object and all benefactive verbs can take a benefactive prepositional phrase complement
(see examples 16). An analogous prediction can be made for nouns. Unlike the diagnos-
tics features, these lexical features do not have a quantitative counterpart, but they are
represented as discrete nominal values that indicate the lexical semantic class the words
belong to.
</bodyText>
<listItem confidence="0.9969045">
(16a) Darcy offered a gift to Elizabeth.
(16b) Darcy cooked a roast for Elizabeth.
</listItem>
<footnote confidence="0.663698909090909">
-ANT inhabitant, contestant, informant, participant, lubricant.
-EE appointee, payee, nominee, absentee, refugee.
-ER, OR singer, writer, driver, employer, accelerator, incubator, supervisor.
-AGE breakage, coverage, drainage, leverage, shrinkage, wastage.
-AL refusal, revival, dismissal.
-ION exploration, starvation, ratification, victimization, foundation.
-SION invasion, evasion.
-ING building, opening, filling, earnings, savings, shavings, wedding.
-MENT arrangement, amazement, puzzlement, embodiment, equipment.
Figure 1
Nominal endings that indicate deverbal derivation.
</footnote>
<page confidence="0.99447">
348
</page>
<note confidence="0.711858">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.999948363636364">
We use all these diagnostics, which in linguistics are used as tests of the argu-
ment status of PPs, as a distributed representation of argumenthood itself. We do not
assume that the syntactic representation of arguments is different from the represen-
tation of adjuncts; for example, we do not assume they have a different attachment
configuration, rather the diagnostics themselves determine a multidimensional space
in which PPs are positioned with different degrees of argumenthood. For such an
approach to work, we need to be able to transform each diagnostic into a symbolic
or numeric feature and combine the features in a precise way. In the two following
sections we illustrate how to calculate the values of each diagnostic using corpus-based
approximations and how to combine them with widely used automatic acquisition
techniques.
</bodyText>
<sectionHeader confidence="0.9941" genericHeader="introduction">
3. Methodology
</sectionHeader>
<bodyText confidence="0.999971">
The diagnostics described above can be estimated by simple corpus counts. The accu-
racy of the data collection is key to the success of the classifier induction based on these
counts. We explain the details of our methodology below.
</bodyText>
<subsectionHeader confidence="0.992289">
3.1 Materials
</subsectionHeader>
<bodyText confidence="0.999757724137931">
We construct two corpora comprising examples of PP sequences. A PP is approximated
as the preposition and the PP-internal head noun. For example, with very many little chil-
dren will be represented as the bigram with children. One corpus contains data encoding
information for attachment of single PPs in the form of four head words (verb, object
noun, preposition, and PP internal noun) indicating the two possible attachment sites
and the most important words in the PP for each instance of PP attachments found in
the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data
item consists of verb, direct object, and the two following PPs. This corpus is only used
to estimate the feature Iterativity. All the data were extracted from the Penn Treebank
using the tgrep tools (Marcus, Santorini, and Marcinkiewicz 1993). Our goal is to create a
more comprehensive and possibly more accurate corpus than the corpora used by Merlo
and Leybold (2001), Merlo, Crocker, and Berthouzoz (1997), and Collins and Brooks
(1995), among others. To improve coverage, we extracted all cases of PPs following
transitive and intransitive verbs and following nominal phrases. We include passive
sentences and sentences containing a sentential object. To improve accuracy, we insured
that we did not extract overlapping data, contrary to practice in previous PP corpora
construction, where multiple PP sequences were extracted more than once, each time
as part of a different structural configuration. For example, in previous corpora, the
sequence using crocidolite in filters in 1956, which is a sequence of two PPs, is counted
both as an example of a two PPs sequence as well as an example of a single PP
sequence, using crocidolite in filters. This technique of using subsequences as independent
examples is used both in the corpora used in Merlo and Leybold (2001) and (Merlo,
Crocker, and Berthouzoz 1997), and to an even larger extent in the corpus used in
Collins and Brooks (1995), who would also have in their corpus the artificially con-
structed sequence using crocidolites in 1956. This method increases the number of avail-
able examples, and it is therefore hoped that it will be beneficial to the learning accuracy.
However, as shown in Merlo, Crocker, and Berthouzoz (1997), it rests on the incorrect
assumption that the probability of an attachment is independent of the position of the
PP to be disambiguated in a sequence of multiple PPs. Therefore, we have decided not
</bodyText>
<page confidence="0.996523">
349
</page>
<note confidence="0.304007">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.998955428571429">
to decompose the examples into smaller sequences. The possible grammatical config-
urations that we have taken into account to construct the corpus are exemplified in
Appendix 1.
Once the quadruples constituting the data are extracted from the text corpus,
it is necessary to separate the statistics corpus from the training and test corpus.3
Before illustrating the adopted solution to this problem, let us define the following
terms.
</bodyText>
<listItem confidence="0.9992417">
• The statistics corpus CSt is the part of the corpus that is used to extract the
tuples that are used to calculate the features.
• The training corpus CTr is the part of the corpus that is used to extract the
tuples that are used as training data for the classifier.
• The testing corpus CTe is the part of the corpus that used to extract the
tuples that are used as testing data to evaluate the classifier.
• The training data STr is the set of tuples in CTr, augmented with the
features calculated using CSt.
• The testing data STe is the set of tuples in CTe, augmented with the features
calculated using CSt.
</listItem>
<bodyText confidence="0.999803375">
Note that for the testing data STe to be an independent test set, the testing corpus
CTe must be disjoint both from CTr, the training corpus, but also it must be disjoint
from CSt, the corpus on which the statistics are calculated. One possible solution, for
example, is to equate the statistics and the training corpus, CSt = CTr, and to assign them
Sections 1–22, while CTe = Section 23, thus making the testing corpus CTe be disjoint
from both the statistics and the training corpus. The problem with this solution is that
the training data STr is no longer extracted using the same process as the testing data
STe, and is therefore not good data from which to generalize. In particular, all tuples in
the training data STr necessarily also occur in the statistics corpus CSt, and therefore no
vectors in the training data STr involve data unseen in the statistics corpus. In contrast,
the testing data STe can be expected to include tuples that did not also occur in the
statistics corpus, and so the classifier might not generalize to these tuples using the
features we calculate.
The solution we adopt is to split Sections 1 to 22 into two disjoint subcorpora.
Because the Penn Treebank is not uniformly annotated across sections, we do not
assign whole sections to either the statistical or the training corpus, but instead ran-
domly assign individual sentences to either corpus. Since data sparseness is a more
important issue when calculating the features than it is for training the decision tree, we
assigned a larger proportion of the corpus to the statistical subcorpus. We assigned 25%
of Sections 1–22 to CTr and the rest to CSt. Section 24 is used as a development corpus
and Section 23 is the testing corpus CTe. In this setting, since the statistics, training, and
testing corpora are all mutually disjoint, all issues of dependence are resolved, and the
training data are representative of the real data we are interested in, so we can expect
our classifier to be able to generalize to new data.
</bodyText>
<footnote confidence="0.720749">
3 We thank Eric Joanis for his help in correctly sampling the corpus and calculating the features.
</footnote>
<page confidence="0.99544">
350
</page>
<note confidence="0.824358">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<subsectionHeader confidence="0.998053">
3.2 The Counts of the Learning Features
</subsectionHeader>
<bodyText confidence="0.999789333333333">
As we said above, accurate estimates of the values of the features are crucial for the au-
tomatic learning algorithm to be successful. We illustrate the estimation of the features
below. Often several ways of estimating the features have been implemented, mostly to
address sparseness of data.
Lexical Word Classes. As indicated in the previous section, the head word of the governor
of the PP, the noun or the verb, is very directly related to the status of the PP. For
example, all giving verbs take a PP introduced by to, which is an argument of the verb.
The lexical semantic class of the head words is therefore going to be very relevant to the
disambiguation task.
The semantic grouping has been done automatically, using the lexicographic classes
in WordNet 1.7 (Miller et al. 1990). Nouns are classified in different classes, among
which, for example, are animal, artifact, attribute, body, cognition, communication, event,
feeling, food, location, motive, person, plant, process, quantity, relation, shape, and substance.
This classification required selecting the most frequent WordNet sense for those polyse-
mous nouns being classified and generalizing to its class.4
For all the features below and where appropriate, we assume the following no-
tation. Let h be the head, that is, the verb in the features related to verb attachments
and the noun in the features related to noun attachment. Let p be the preposition, and
n2 be the object of the preposition. Let hcl and n2cl be the WordNet class of h and n2,
respectively. Let C(h, p, n2) be the frequency with which p, n2 co-occurs as a preposi-
tional phrase with h. Let C(h) be the frequency of h.
Head Dependence. Head dependence of a PP on a head is approximated by estimating
the dispersion of the PP over the possible heads. In a previous attempt to capture this
notion, we approximated by simply measuring the cardinality of the set of heads that
co-occur with a given PP in a corpus, as indicated in equation (6). The expectation was
that a low number indicated argument status, whereas a high number indicated adjunct
status (Merlo and Leybold 2001).
</bodyText>
<equation confidence="0.950633">
hdep(h, p, n2) _ |{h1, h2, h3, ... , hn}p,n2 |(6)
</equation>
<bodyText confidence="0.998392923076923">
By measuring the cardinality of the set of heads, we approximate the dispersion
of the distribution of heads by its range. This is a very rough approximation, as the
range of a distribution does not give any information on the distribution’s shape. The
range of a distribution might have the same value for a uniform distribution or a very
skewed distribution. Intuitively, we would like a measure that tells us that the former
corresponds to a verb with a much lower head dependence than the latter. Entropy is
4 The automatic annotation of nouns and verbs in the corpus has been done by matching them with
the WordNet database files. Before doing the annotation, though, some preprocessing of the data
was required to maximize the matching between our corpus and WordNet. The changes made
were inspired by those described in Stetina and Nagao (1997, page 75). To lemmatize the words
we used “morpha,” a lemmatizer developed by John A. Carroll and freely available at the address:
http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html. Upon simple observation,
it showed a better performance than the frequently used Porter Stemmer for this task.
</bodyText>
<page confidence="0.980625">
351
</page>
<note confidence="0.287098">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.9987855">
a more informative measure of the dispersion of a distribution, which depends both on
the range and on the shape of a distribution.
The head dependence measure based on entropy, then, is calculated as indicated
in equation (7), which calculates the entropy of the probability distribution gener-
ated by the random variable X, whose values are all the heads that co-occur with a
given PP.
</bodyText>
<equation confidence="0.9952905">
hdep(h,p,n2) = Hp,n2(X) ≈ −Σh∈X C(h,p,n2) loge C(h,p,n2) (7)
ΣiC(hi, p, n2) ΣiC(hi, p, n2)
</equation>
<bodyText confidence="0.999936615384615">
The counts that are used to estimate this measure will depend on finding exactly
PPs with the same PP internal noun, and on attaching to exactly the same lexical
head. We can expect these measures to suffer from sparse data. We implement then
some variants of this measure, where we cluster PPs according to the semantic content
of the PP-internal nouns and we cluster nominal heads according to their class. The
semantic grouping has been done automatically, as indicated in the paragraph above,
on calculating word classes. Since WordNet has a much finer-grained top-level classifi-
cation for nouns than for verbs, we found that grouping head nouns into classes yielded
useful generalizations, but it did not do so for verbs.
Therefore, we calculate head dependency in three different variants: One measure is
based on PP-internal noun tokens, another variant is based on noun classes for the PP-
internal noun position, and another variant is based on classes for both the PP-internal
and the head noun position, as indicated in equation (8).
</bodyText>
<equation confidence="0.999975166666667">
hdep(h,p,n2) = { C(h,p,n2) C(h,p,n2)
Hp,n2(X) ≈ −Σh∈XΣiC(hi,p,n2) log2 ΣiC(hi,p,n2), or
C(h,p,n2cl) C(h,p,n2cl)
Hp,n2cl(X) ≈ −Σh∈X ΣiC(hi,p,n2cl) log2 ΣiC(hi,p,n2cl), or
Hp,n2cl(Xcl) ≈ − Σhcl∈XΣC(hcl,p,n2cl) l E(hcl,p,n,2cl) if h = noun.
iC(hcl,i,p,n2cl) g2ΣiC(hcl,i,pn2cl), (8)
</equation>
<bodyText confidence="0.9988594">
Optionality. As explained above, we expect that the predictive power of a verbal
head—recall that optionality does not apply to noun attachments—about its com-
plements will be greater for arguments than for adjuncts. This insight can be cap-
tured by the conditional probability of a PP given a particular verb, as indicated in
equation (9).
</bodyText>
<equation confidence="0.996160333333333">
C(v, p, n2)
opt(v, p, n2) ≈ (9)
C(v)
</equation>
<bodyText confidence="0.999979428571429">
Analogously to the measure of head dependence for noun attachments, optionality
is measured in three variants. First, it is calculated as a conditional probability based
on simple word counts in the corpus of single PPs, as indicated in equation (9) above.
Second, we also implement a variant that relies on verb classes instead of individual
verbs to address the problem of sparse data. Finally, we also implement a variant that
relies on noun classes for the PP-internal noun and verb classes instead of individual
verbs. For both these measures, verbs and nouns were grouped into classes using
</bodyText>
<page confidence="0.974735">
352
</page>
<note confidence="0.648337">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.7745135">
WordNet 1.7 with the same method as for head dependence. The three measures of
optionality are indicated in equation (10).
</bodyText>
<equation confidence="0.998838166666667">
opt(v,p,n2) = { P(p,n2|v) ≈ C(v,p,n2) (10)
C(v) , or
P(p, n2|vcl) ≈ C(vcl,p,n2)
C(vcl) , or
P(p, n2cl|vcl) ≈ C(vcl,p,n2cl)
C(vcl)
</equation>
<bodyText confidence="0.997007727272727">
Iterativity and Ordering. Iterativity and ordering are approximated by collecting counts
indicating the proportion of cases in which a given PP in first position had been found
in second position in a sequence of multiple PPs over the total of occurrences in any
position, as indicated in equation (11). The problem of sparse data here is especially
serious because of the small frequencies of multiple PPs. We addressed this problem by
using a backed-off estimation, where we replace lexical items by their WordNet classes
and collect counts on this representation. Specifically, the iterativity measure has been
implemented as follows.
Let C2nd(h, p, n2) be the frequency with which p, n2 occurs as a second prepositional
phrase with h, and Cany(h, p, n2) be the frequency with which it occurs with h in any
position.5 Then:
</bodyText>
<equation confidence="0.999071111111111">
C2nd(h, p, n2) if Cany(h, p, n2) # 0, or else
Cany(h, p, n2)
C2nd(h, p, n2cl) if
Cany(h, p, n2cl) = 0, or else
iter(h,p,n2) ≈ {
Cany(h, p, n2cl) ,
C2nd(hcl, p, n2cl)
Cany(hcl, p, n2cl)
(11)
</equation>
<bodyText confidence="0.999826875">
Copular Paraphrase. Copular paraphrase is captured by calculating the proportion of
times a given PP is found in a copular paraphrase. We approximate this diagnostic by
making the hypothesis that a PP following a nominal head is an adjunct if it is also
found following a copular verb, be, become, appear, seem, remain (Quirk et al. 1985). We
calculate then the proportion of times a given PP follows a copular verb over the times
it appears following any verb. This count is an approximation because even when we
find a copular verb, it might not be part of a relative clause. Here again, we back off to
the noun classes of the PP-internal noun to address the problem of sparse data.
</bodyText>
<equation confidence="0.59128975">
para(h,p,n2) ≈ I C(vcopula ≺ (p, n2)) (12)
ΣiC(vi ≺ (p, n2)), if C(vcopula) =� 0, or else
C(vcopula ≺ (p,n2cl))
ΣiC(vi ≺ (p, n2cl))
</equation>
<footnote confidence="0.7806865">
5 Note that we approximate the prepositions occurring in any position by looking only at the first two
prepositions attached to the verb phrase.
</footnote>
<page confidence="0.993197">
353
</page>
<note confidence="0.290929">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.994108">
Deverbal Nominalization. The diagnostic of deverbal nouns is implemented as a binary
feature that simply indicates if the PP follows a deverbal noun or not.
</bodyText>
<equation confidence="0.817463">
� 1 if deverbal n ≺ (p,n2)
deverb(n, p, n2) _ (13)
0 otherwise
</equation>
<bodyText confidence="0.999973636363636">
Deverbal nouns are identified by inspecting their morphology (Quirk et al. 1985).
As our corpus is lemmatized, we are confident that all the nouns in it are in their base
forms. The suffixes that can combine with verb bases to form deverbal nouns are shown
in Figure 1.
The counts that are collected in the way described above constitute a quanti-
fied vector corresponding to a single PP exemplar. These exemplars are the input to
an automatic classifier that distinguishes arguments from adjuncts, as described in
Section 4. Before we describe the experiments, however, attention must be paid to the
method that will be used to determine the target attribute—argument or adjunct—that
will be used to train the learner in the learning phase and to evaluate the accuracy of
the learned classifier in the testing phase.
</bodyText>
<subsectionHeader confidence="0.999125">
3.3 The Target Attribute
</subsectionHeader>
<bodyText confidence="0.999922310344828">
Since we are planning to use a supervised learning method, we need to label each exam-
ple with a target attribute. Deciding whether an example is an instance of an argument
or of an adjunct requires making a distinction that the Penn Treebank annotators did
not intend to make. The automatic annotation of this attribute therefore must rely on
the existing labels for the PP that have been given by the Penn Treebank annotators,
inferring from them information that was not explicitly marked. We discuss here the
motivation for our interpretation.
The PTB annotators found that consistent annotation of argument status and se-
mantic role was not possible (Marcus et al. 1994). The solution adopted, then, was
to structurally distinguish arguments from adjuncts only when the distinction was
straightforward and to label only some clearly distinguishable semantic roles. Doubt-
ful cases were left untagged. In the Penn Treebank structural distinctions concerning
arguments and adjuncts have been oversimplified: All constituents attached to VP are
structurally treated as arguments, whereas all constituents attached to NP are treated
as adjuncts. The only exception are the arguments of some deverbal nouns, which are
represented as arguments. Information about the distinction between arguments and
adjuncts, then, must be gleaned from the semantic and function tags that have been
assigned to the nodes. Constituents are labeled with up to four tags (including numer-
ical indices) that account for the syntactic category of the constituent, its grammatical
function, and its semantic role (Bies et al. 1995). Figure 2 illustrates the tags that involve
PP constituents.
From the description of this set of tags we can already infer some information about
the argument status of the PPs. PPs with a semantic tag (LOC, MNR, PRP, TMP) are
adjuncts, whereas labels indicating PP complements of ditransitive verbs (BNF, DTV)
or locative verbs like put are arguments. There are, though, some cases that remain am-
biguous and therefore require a deeper study. These are untagged PPs and PPs tagged
-CLR. For these cases, we will necessarily only approximate the desired distinction. We
have interpreted untagged PPs as arguments of the verb. The motivation for this choice
comes both from an overall observation of sentences and from the documentation,
</bodyText>
<page confidence="0.995069">
354
</page>
<figure confidence="0.84176875">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
-CLR dative object if dative shift not possible (e.g., donate); phrasal verbs;
predication adjuncts
-DTV dative object if dative shift possible (e.g., give)
-BNF benefactive (dative object of for)
-PRD non VP predicates
-PUT locative complement of put
-DIR direction and trajectory
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
</figure>
<figureCaption confidence="0.98357">
Figure 2
</figureCaption>
<bodyText confidence="0.98795275">
Grammatical function and semantic tags that involve PP constituents in the Penn Treebank.
in which it is stated that “NPs and Ss which are clearly arguments of the verb are
unmarked by any tag” (Marcus et al. 1994, page 4), and that “Direct Object NPs and
Indirect Object NPs are all untagged” (Bies et al. 1995, page 12). Although the case of
PP constituents is not specifically addressed, we have interpreted these statements as
supporting evidence for our choice.
The tag -CLR stands for “closely related,” and its meaning varies, depending on
the element it is attached to. It indicates argument status when it labels the dative
object of ditransitive verbs that cannot undergo dative shift, such as in donate money to
the museum, and in phrasal verbs, such as pay for the horse. It indicates adjunct status
when it labels a predication adjunct as defined by Quirk et al. (1985). We interpret
the tag -CLR as an argument tag in order not to lose the few cases for which the
differentiation is certain: the ditransitive verbs and some phrasal verbs. This choice
apparently misclassifies predication adjuncts as arguments. However, for some cases,
such as obligatory predication adjuncts, an argument status might in fact be more
appropriate than an adjunct status. According to Quirk et al. (1985, Sections 8.27–35,
15.22, pages 16–48), there are three types of adjuncts, differentiated by the degree of
”centrality” they have in the sentence. They can be classified into predication adjuncts
and sentence adjuncts. Predication adjuncts can be obligatory or optional. Obligatory
predication adjuncts resemble objects as they are obligatory in the sentence and they
have a relatively fixed position, as in He lived in Chicago. Optional predication adjuncts
are similarly central in the sentence but are not obligatory, as in He kissed his mother
on the cheek. Sentence adjuncts, on the contrary, have a lower degree of centrality in the
sentence, as in He kissed his mother on the platform. As a conclusion, obligatory predication
adjuncts as described in Quirk et al. (1985) could be interpreted as arguments, as they
are required to interpret the verb (the interpretation of lived in He lived differs from the
one in He lived in Chicago).
To recapitulate, we have labeled our examples as follows:
</bodyText>
<listItem confidence="0.9852405">
• Adjuncts: All PPs tagged with a semantic tag (DIR, LOC, MNR, PRP, TMP)
are adjuncts.
• Arguments: All untagged PPs or PPs tagged with CLR, EXT, PUT, DTV,
BNF, or PRD are arguments.
</listItem>
<bodyText confidence="0.945167">
Validating the Target Attribute and Creating a Gold Standard. The overall mapping of
Penn Treebank function labels onto the argument–adjunct distinction is certainly too
coarse, as some function types of PPs can be both arguments or adjuncts, depending
</bodyText>
<page confidence="0.985124">
355
</page>
<note confidence="0.289593">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999848875">
on the head they co-occur with. We assessed the overall validity of the mapping as
follows. First, for each of the function tags mentioned earlier, we sampled the Penn
Treebank (one example from each section) for a total of 22 examples for each tag. Then,
we manually inspected the examples to determine if the mapping onto argument or
adjunct was correct. On a first inspection, the tags PUT, DTV, and PRD are correctly
mapped as argument in the majority of cases, as well as DIR, LOC, TMP, and PRP,
which are correctly considered adjuncts. Those samples tagged MNR, CLR, BNF, or
untagged show a more mixed behavior, sometimes appearing to label arguments and
sometimes adjuncts. For these labels, we used a more elaborate procedure to determine
if the example was an argument or an adjunct. We concentrate on PPs attached to the
verb, as these cases appear to be more ambiguous.
All the test examples attached to a verb that had a CLR, PP, or MNR label were
extracted. We did not investigate BNF as there aren’t any in our test file. We constructed
test suites for each example by applying to it the typical linguistic diagnostics used to
determine argumenthood, along the lines already discussed in Section 2. Five tests were
selected, which were found to be the most discriminating in a pilot study over 224 sen-
tences: optionality, ordering, head dependence, extraction with preposition stranding,
and extraction with pied-piping, as illustrated in Figure 3. It can be noticed that we were
able to use more complex tests than those used by the algorithm; in particular we use
extraction tests (Sch¨utze 1995).6 A native speaker gave binary acceptability judgments
over the 1,100 sentences thus generated. The acceptability judgments were assigned to
the sentences over the course of several days. Once the judgments to each quintuple of
sentences were collected, they were combined into a single binary-valued target feature
for each sentence by the first author. The decision was based on the relative importance
and reliability of the tests according to the linguistic literature (Sch¨utze 1995), as follows:
If the optionality test is negative then the example is an argument, else if extraction can take
place then the example is an argument, else the majority label according to the outcome of
the grammaticality judgments is assigned. In a few cases, the judgment was overidden
if the negative outcome of the tests clearly derived from the fact that the V + PP was
an idiom rather than an adjunct: for example, make fools of themselves, have a ring to it,
and live up to.
In the end, we find the following correlations between the automatic labels and
those assigned based on the accurate collection of native speaker judgments. Among the
PPs that do not have a function label, 65 are adjuncts and 42 are arguments, according
to the manual annotation procedure. The label PP-CLR corresponds to 18 adjuncts and
159 arguments, according to our manual annotation procedure, whereas the label PP-
MNR corresponds to 5 adjuncts and 1 argument. Clearly, the assignments of CLR PPs to
arguments and MNR PPs to adjunct are confirmed. Prepositional phrases without any
functional labels, on the other hand, are much more evenly divided between argument
and adjunct, as we suspected, given the heterogeneous nature of the label (the label PP
</bodyText>
<footnote confidence="0.9492534">
6 Some of the extracted sentences had to be simplified so that the verb and prepositional phrases were in a
main clause. For example buy shares from sellers, which is generated from the sentence On days like Friday,
that means they must buy shares from sellers when no one else is willing to becomes They must buy shares from
sellers. In some cases the sentences had to be further simplified to allow extraction tests to apply, which
would be violated for reasons unrelated to the argument-adjunct distinction in the PP, such as negation,
or complex NP islands. For example, Hill democrats are particularly angry over Mr. Bush’s claim that the
capital-gains cut was part of April’s budget accord and his insistence on combining it with the deficit-reduction
legislation yields Mr. Bush combines capital gains cut with the deficit-reduction legislation, which gives rise to
the following extraction examples: What do you wonder whether Mr. Bush combines capital gains cut with?
With what do you wonder whether Mr. Bush combines capital gains cut?
</footnote>
<page confidence="0.982236">
356
</page>
<table confidence="0.971400818181818">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
Americans will learn more about making products [ for the Soviets ].
optionality Americans will learn more about making products.
order Americans will learn more about making products these coming years
for the Soviets.
head dependence Americans will learn more about making/selling/reading products for
the Soviets.
extraction Who do you wonder whether Americans will learn more about making
products for?
For who(m) do you wonder whether Americans will learn more about
making products?
</table>
<figureCaption confidence="0.654463">
Figure 3
</figureCaption>
<bodyText confidence="0.989261869565217">
Example sentence and related list of tests and test sentences.
is assigned not only to those cases that are clear arguments, but also to those cases for
which a label cannot be decided). Moreover, if the hand-annotated label is reliable, it in-
dicates that untagged PPs are somewhat more likely to be adjuncts. Our initial mapping
was incorrect. In retrospect, we must conclude that other researchers had applied what
appears to be the correct mapping for this data set (Buchholz 1999). We do not, however,
modify the label of our training set, as that would be methodologically incorrect. We
have relabeled the test set and are therefore bound to ignore any further knowledge
we have gathered in relabeling, as that would amount to tailoring our training set to
our test set. The consequence of this difference between our label and what we found
to be true for the gold standard is that all results tested on the manually labeled test
set will have to be interpreted as lower bounds of the performance to be expected on a
consistently labeled data set. Besides validating the automatically annotated test set, the
manually annotated test set serves as a gold standard. Performance measures on this set
will support comparison across methodologies.
Therefore, we conclude that the mapping we have assumed is coherent with the
judgments of a native speaker, although the agreement is not perfect. PPs without
a function tag are an exception. Thus, the automatic mapping we have defined will
provide the value of the target feature in the experiments that we illustrate in the two
following sections. When appropriate we report two performance measures, one for
the automatic label and one for the partly manual labels. We will also report some
comparative results on a test set that does not contain the noisy PPs without function
labels.
</bodyText>
<sectionHeader confidence="0.867465" genericHeader="method">
4. Distinguishing Arguments from Adjuncts
</sectionHeader>
<bodyText confidence="0.999992545454545">
Having collected the necessary data and established the value of the target attribute
for each example, we can now perform experiments to test several different hypotheses
concerning the learning of the argument-adjunct distinction. First of all we need to show
that the distinction under discussion can be learned to a good degree. Furthermore, we
investigate the properties that, singly or in combination, lead to an improvement in
learning. We are particularly interested in comparing the baseline to a simple model,
where learning is done using only lexical heads. In this case, we investigate the rele-
vance of the simple words in detecting the difference between arguments and adjuncts.
We also verify the usefulness of knowing lexical classes on the accuracy of learning.
Finally, we show that the diagnostic features and the lexical classes developed above
bring more information to the classification than what can be gathered by simple
</bodyText>
<page confidence="0.976132">
357
</page>
<note confidence="0.448248">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.8731255">
lexical heads. We summarize these expectations below, where we repeat and elaborate
Hypothesis 1 formulated in the introduction.
</bodyText>
<listItem confidence="0.9991914">
• Hypothesis 1: The argument-adjunct distinction can be performed based
on information collected from an annotated corpus.
• Hypothesis 1’: The argument-adjunct distinction can be improved by
lexical classes and linguistic diagnostic features, (i) over a simple baseline,
but also (ii) over a model using lexical features.
</listItem>
<bodyText confidence="0.9964002">
Demonstration of these hypotheses requires showing that the distinction can be
learned from corpus evidence, even with a simple method (performance is better than
chance). Hypothesis 1’ imposes the more stringent condition that we can considerably
improve a simple learner by using more linguistically informed statistics (performance
is better than the simple method).
</bodyText>
<subsectionHeader confidence="0.981096">
4.1 The Input Data and the Classifier
</subsectionHeader>
<bodyText confidence="0.999963409090909">
In order to test the argument and adjunct attachment problem independently of
whether the PP is attached to a verb or to a noun, we create two sets of input data,
one for verb attachment and one for noun attachment.
Each input vector for verb attachment contains training features comprising the
four lexical heads and their WordNet classes (v, n1, p, n2, vcl, n1cl, and n2cl), all the
different variants of the implementation of the diagnostics for the argument-adjunct
distinction concerning PPs attached to a verb, and one binary target feature, indicating
the type of attachment, whether argument or adjunct. More specifically, the features
implementing the diagnostics for verbs consist of the variants of the measures of head
dependence (hdepv1,hdepv2), the variants of the measure of optionality (opt1, opt2, opt3),
and the measure of iterativity (iterv).
Each input vector for noun attachment contains 14 training features. They comprise
the four lexical heads and their WordNet classes, as above (v, n1, p, n2, vcl, n1cl, and
n2cl), all the different variants of the implementation of the diagnostics for PPs attached
to a noun, and one binary target feature, indicating the type of attachment. The features
implementing the diagnostics for nouns are: the variants of the measures of head de-
pendence (hdepn1, hdepn2, hdepn3), the measures of iterativity (itern), and the measures
for copular paraphrase and deverbal noun, respectively (para, deverb).
For both types of experiments—distinction of arguments from adjuncts of PPs at-
tached to the noun or PPs attached to the verb—we use the C5.0 Decision Tree Induction
Algorithm (Quinlan 1993) and Support Vector Machines (LIBSVM), version 2.71 (Chang
and Lin 2001).
</bodyText>
<subsectionHeader confidence="0.713902">
4.2 Results on V Attachment Cases
</subsectionHeader>
<bodyText confidence="0.999879666666667">
We have run experiments on the automatically labeled training and test sets with
very many different feature combinations. A summary of the most interesting patterns
of results are indicated in Tables 1 and 2. Significance of results is tested using a
McNemar test.
Contribution of Lexical Items and Their Classes. In this set of experiments we use only
lexical features or features that encode the lexical semantic classes of the open class
</bodyText>
<page confidence="0.99567">
358
</page>
<note confidence="0.966745">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<tableCaption confidence="0.687089333333333">
Table 1
Accuracy of the argument–adjunct distinction for VP-attached PPs, using combinations of lexical
features. The training and test sets are automatically annotated.
</tableCaption>
<table confidence="0.999553818181818">
Feature used Auto accuracy (%)
1. Chance (args) 55.8
2. Prep (baseline) 67.9
3. Lexical features (verb, prep, n2) 67.9
4. vcl, prep 73.8
5. Prep, n2cl 71.9
6. Lexical features and classes (v, vcl, p, n2, n2cl) 68.2
7. Only classes (vcl, p, n2cl) 75.0
8. Only verb classes (vcl, p, n2) 73.1
9. Only noun classes (v, p, n2cl) 70.3
10. All features 68.2
</table>
<bodyText confidence="0.9947829375">
words in question. Lines 2 to 5 of Table 1 report better classification values than
line 1. These results indicate that it is possible, at different degrees of accuracy, to
leverage information encoded in lexical items to infer the semantic distinction between
arguments and adjuncts without explicit deep semantic knowledge. One interesting
fact (lines 2 and 3) is that the preposition is very informative, as informative as all
the lexical items together. An analysis of the distribution of arguments and adjuncts
by preposition indicates that whereas most prepositions are ambiguous, they have a
strong preference for either arguments or adjuncts. Only a few equibiased prepositions,
such as for, exist. An expected result (lines 4 and 5) is that the PP-internal noun class
is useful, in combination with the preposition, as well as the combination of verb
class and preposition (the difference is marginally significant). We find the best re-
sult (line 7) in the experiment that uses class combinations (difference from baseline
p &lt; .001). We see that classes of open class items, nouns, and verbs associated with
the closed class item preposition give the best performance. Line 7 is considerably
better than line 6 (p &lt; .001), indicating that the actual presence of individual lexical
items is disruptive. This indicates that regularities in the distinction of arguments
from adjuncts is indeed a class phenomenon and not an item-specific phenomenon.
This is expected, according to current linguistic theories, such as the one proposed by
Levin (1993).
This analysis is confirmed by observing which are the most discriminative features,
those that are closest to the root of the decision tree. The topmost feature of the best
result (line 7) is preposition, followed by the class of the verb and the class of the PP
internal noun. Predictably, the class of the object noun phrase is not used because it is
not very informative. The same features constitute the tree yielding the results of line 6.
However, the presence of lexical items makes a difference to the tree that is built, which
is much more compact, but in the end less accurate.
Combinations of All Features. Table 2 reports the accuracy in the argument-adjunct dis-
tinction of experiments that use only the most useful lexical and class features, the
preposition and the verb class, and the diagnostic-based features, using combinations
of diagnostic features. The combinations of features shown are those that yielded the
best results over a development set of tuples extracted from Section 24 of the Penn
Treebank. The results reported are calculated over a test corresponding to the tuples
</bodyText>
<page confidence="0.991013">
359
</page>
<table confidence="0.570447">
Computational Linguistics Volume 32, Number 3
</table>
<tableCaption confidence="0.988923">
Table 2
</tableCaption>
<table confidence="0.488907333333333">
Best results using preposition and combination of diagnostic-based features, in different
variants. The training and test sets are automatically annotated.
Feature used Auto accuracy (%)
</table>
<listItem confidence="0.815194">
1. vcl, prep, hdepv1, hdepv2, opt1, opt2, opt3, iterv 79.3
2. vcl, prep, hdepv1, hdepv2, opt1, opt3, iterv 79.8
3. vcl, prep, hdepv1, hdepv2, opt2, opt3, iterv 79.0
4. vcl, prep, hdepv1, opt2, opt3, iterv 78.2
</listItem>
<bodyText confidence="0.999530777777778">
in Section 23 of the Penn Treebank. The best combination, line 2, yields a 37% reduction
of the error rate over the baseline. All the differences in performance among these
configurations are significant. What all these combinations have in common is that
they are combinations of three levels of granularity, mixing lexical information, class
information, and higher level syntactic-semantic information, encoded indirectly in the
diagnostics. All the combinations that are not shown here have lower accuracies.
Table 3 shows the confusion matrix for the combination of features with the best
accuracy listed above. These figures yield a precision and recall for arguments of 80%
and 85%, respectively (F measure = 82%); and a precision and recall for adjuncts of 80%
and 73%, respectively (F measure = 76%). Clearly, although both kinds of PPs are well
identified, arguments are better identified than adjuncts, an observation already made
by several other authors, especially Hindle and Rooth (1993) in their detailed discussion
of the errors in a noun or verb PP-attachment task. In particular, we notice that more
adjuncts are misclassified as arguments than vice versa.
The results of these experiments confirm that corpus information is conducive
to learning the distinction under discussion without explicitly represented complex
semantic knowledge. They also confirm that this distinction is essentially a word class
phenomenon—and not an individual lexical-item phenomenon—as would be expected
under current theories of the syntax–semantics interface. Finally, the combination of
lexical items, classes, and linguistic diagnostics yields the best results. This indicates
that using features of different levels of granularity is beneficial, probably because
the algorithm has the option of using more specific information when reliable, while
abstracting to coarser-grained information when lexical features suffer from sparse data.
This interpretation of the results is supported by observing which features are at the top
of the tree. Interestingly, here the topmost feature is head dependence (the lexical variant,
hdepv1), on one side of which we find preposition as the second most discriminative
feature, followed by head dependence (hdepv2) again, and optionality (class variants). On
</bodyText>
<tableCaption confidence="0.942874666666667">
Table 3
Confusion matrix of the best classification of PPs attached to the verb. Training and test set
established automatically.
</tableCaption>
<table confidence="0.9230812">
Assigned classes
Arguments Adjuncts Total
Actual classes Arguments 300 51 351
adjuncts 76 202 278
Total 376 253 629
</table>
<page confidence="0.973033">
360
</page>
<note confidence="0.830214">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.997601736842105">
the other side of the tree, we find preposition as the second most informative feature and
verb class as the third most discriminative feature.
Results on Partly Manually Labeled Set. Tables 4 and 5 report the results obtained by train-
ing the classifier on the automatically labeled training set and testing on the manually
labeled test set. They illustrate the effect of training the decision tree classifier on a
training set that has different properties from the test set. This experiment provides a
lower bound of performance across different samples and shows which are the features
with the greatest generalization ability. We can draw several conclusions. First, the
lexical features do better than chance, but do not do better than the baseline established
by using only the preposition as a feature (lines 1, 2, and 3 of Table 4). Secondly,
classes do better than the baseline (line 7 of Table 4) and so do the diagnostic features
(Table 5). Since we are using a training and a test set with different properties, these
results indicate that classes and diagnostics capture a level of generality that the lexical
features do not have and will be more useful across domains and corpora. Finally,
the rank of performance for different feature combinations holds across training and
testing methods, whether established automatically or manually, as can be confirmed
by a comparison of Tables 1 and 4 and also 2 and 5. The difference in performance with
diagnostics (line 3 of Table 5) and without, using only classes (line 7 of Table 4), is only
marginally significant, indicating that diagnostics are not useless.
</bodyText>
<tableCaption confidence="0.66061775">
Table 4
Accuracy of the argument–adjunct distinction for VP-attached PPs, using combinations
of lexical features. The training set is automatically annotated while the test set is in part
annotated by hand.
</tableCaption>
<table confidence="0.941898636363636">
Feature used Manual accuracy (%)
1. Chance (args) 37.0
2. Prep (baseline) 61.2
3. Lexical features (verb, prep, n2) 61.2
4. vcl, prep 67.1
5. Prep, n2cl 66.8
6. Lexical features and classes (v, vcl, p, n2, n2cl) 62.8
7. Only classes (vcl, p, n2cl) 70.0
8. Only verb classes (vcl, p, n2) 67.7
9. Only noun classes (v, p, n2cl) 64.9
10. All features 66.0
</table>
<tableCaption confidence="0.814198">
Table 5
Best results using prepositions and combination of diagnostic-based features in different
variants. The training set is automatically annotated, whereas the test set is in part annotated
by hand.
</tableCaption>
<table confidence="0.299088">
Feature used Manual accuracy (%)
1. Baseline (prep) 67.3
</table>
<page confidence="0.9583422">
2. vcl, prep, hdepv1, hdepv2, opt1, opt2, opt3, iter 67.2
3. vcl, prep, hdepv1, hdepv2, opt1, opt3, iter 69.0
4. vcl, prep, hdepv1, hdepv2, opt2, opt3, iter 67.6
5. vcl, prep, hdepv1, opt2, opt3, iter 65.7
361
</page>
<note confidence="0.421512">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.943190333333333">
Results on Test Set without Bare PPs. The biggest discrepancy in validating the automatic
labeling was found for PPs without functional tags. The automatic labeling had classi-
fied bare PPs as argument but the manual gold standard assigns more than half of them
to the adjunct class. They are therefore a source of noise in establishing reliable results. If
we remove these PPs from the training and test set, results improve and become almost
identical across the manually and automatically labeled sets, as illustrated in Table 6.7
</bodyText>
<subsectionHeader confidence="0.971572">
4.3 Results on N Attachment Cases
</subsectionHeader>
<bodyText confidence="0.999989833333333">
Experiments on learning the distinction between argument PPs and adjunct PPs at-
tached to a noun show a different pattern, probably due to a ceiling effect. The ex-
periments reported below are performed on examples of prepositional phrases whose
preposition is not of. The reason to exclude the preposition of is that it is 99.8% of
the time attached as an argument. Moreover, it accounts for approximately half of the
cases of NP attachment. Results including this preposition would therefore be overly
optimistic and not be representative of the performance of the algorithm in general.
The size of the resulting corpus—without of prepositional phrases—is 3,364 tuples.
The results illustrated in Tables 7 and 8 show that head dependence is the only feature
that improves numerically the performance above the already very high baseline that
can be obtained by using only the feature preposition. This difference is not statistically
significant, indicating that neither class nor diagnostics add useful information.
</bodyText>
<subsectionHeader confidence="0.959184">
4.4 Results Using a Different Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999988142857143">
In the previous sections, we have shown that different combinations of features yield
significantly different performances. We would like to investigate, at least on a first
approximation, if these results hold across different learning algorithms. To test the
stability of the results, we compare the main performances obtained with a decision tree
to those obtained with a different learning algorithm. In recent years, a lot of attention
has been paid to large margin classifiers, in particular support vector machines (SVMs)
(Vapnik 1995). They have been shown to perform quite well in many NLP tasks. The
fact that they search for a large separating margin between classes makes them less
prone to overfitting. We expect them, then, to perform well on the task trained on
automatically labeled data and tested on manually labeled data, where a great ability
to generalize is needed. Despite being very powerful, however, SVMs are complex
algorithms, often opaque in their output. They are harder to interpret than decision
trees, where the position of the features in the tree is a clear indication of their relevance
for the classification. Finally, SVMs take a longer time to train than decision trees. For
these reasons they constitute an interestingly different learning technique from decision
trees, but not a substitute for them if clear interpretation of the induced learner is needed
and many experiments need to be run.
All the experiments reported below were performed with the LIBSVM package
(Chang and Lin 2001, version 2.71). The SVM parameters were set by a grid search on
the training set, by 10-fold cross-validation. Given the longer training times, we perform
only a few experiments. For V attachment, the baseline using only the preposition
</bodyText>
<footnote confidence="0.59419">
7 The features indicated here as best are the ones used in the other experiments and kept for the sake of
comparison. But, in fact, a different feature combination found by accident gives a result of 79.3%
accuracy in classifying the automatically labeled set.
</footnote>
<page confidence="0.992699">
362
</page>
<note confidence="0.973554">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<tableCaption confidence="0.98413">
Table 6
</tableCaption>
<bodyText confidence="0.86807">
Best results using preposition and combination of diagnostic-based features, in different
variants, taking out PP examples.
Feature used Manual accuracy (%) Auto accuracy (%)
</bodyText>
<listItem confidence="0.942897">
1. Chance baseline (args) 67.3 37.0
2. Prep baseline 64.4 64.6
3. Classes 74.5 74.7
4. Best features combination 76.1 77.0
</listItem>
<tableCaption confidence="0.675914">
Table 7
Baselines and performances using lexical heads and classes for N-attached PPs.
</tableCaption>
<equation confidence="0.564081833333333">
Features used Accuracy (%)
chance (arg) 58.3
p (baseline) 93.3
n1, p, n2 93.3
n1cl, p, n2cl 93.3
n1, n1cl, p, n2, n2cl 93.3
</equation>
<bodyText confidence="0.998259833333333">
reaches an accuracy of 62.2% on the manually labeled test set, whereas performance
using the same best combination of features as the decision tree reaches 70.6% accuracy.
There is, then, a little improvement over the 69% of the decision tree learner, as expected.
The performance on the N-attached cases, on the other hand, is surprisingly poor, with
a low 43% accuracy on testing data. This result is probably due to overfitting, since the
best accuracy on the training set is around 95%.
</bodyText>
<subsectionHeader confidence="0.919813">
4.5 Conclusions
</subsectionHeader>
<bodyText confidence="0.999975">
The results reported in this section show that the argument-adjunct distinction can be
learned based on information collected from an annotated corpus with good accuracy.
For verb attachment, they show in particular that using lexical features yields better
performance than the baseline, especially when we use lexical classes. For automatically
labeled data, diagnostics based on linguistic theory improve the performance even
further. Thus, the hypotheses we were testing with these experiments are confirmed.
The reported results are good enough to be practically useful. In particular, the
distinction between arguments and adjuncts attached to nouns is probably performed
as well as possible with an automatic method, even by simply using prepositions
as features. For the attachment to verbs, known to be more difficult, more room for
</bodyText>
<tableCaption confidence="0.761650333333333">
Table 8
Performances using some combinations of features for N-attached PPs.
Features used Accuracy (%)
n1cl, prep, n2cl, hdepn1, hdepn2, hdepn3, itern, para, deverb 93.9
prep, hdepn1, hdepn2, hdepn3, itern, para, deverb 93.9
prep, hdepn1 94.1
</tableCaption>
<page confidence="0.995182">
363
</page>
<note confidence="0.492506">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.978156">
improvement exists, especially in the recovery of adjuncts. The comparison of decision
trees to SVMs does not appear to indicate that one learning algorithm is consistently
better than the other.
</bodyText>
<sectionHeader confidence="0.765962" genericHeader="method">
5. Hypothesis 2: PP Attachment Disambiguation
</sectionHeader>
<bodyText confidence="0.999930487804878">
Once we have established the fact that arguments and adjuncts can be learned from
a corpus with reasonable accuracy using cues correlated to linguistic diagnostics, we
are ready to investigate how this distinction can be integrated in the disambiguation of
ambiguously attached PPs, the PP attachment problem as usually defined.
The first question that must be asked is whether the distinction between arguments
and adjuncts is so highly correlated with the attachment site of the ambiguous PP to
be almost entirely derivative. For example, the PTB annotators have annotated all noun
attachments as adjuncts and all verb attachments as arguments. If this were the correct
representation of the linguistic facts, having established an independent procedure to
discriminate argument from adjuncts PPs would be of little value in the disambiguation
problem. In fact, there is no theoretical reason to think that the notion of argument is
closely correlated to the choice of attachment site of a PP, given that both verb and noun
attached PPs can have either an argument or an adjunct function. It might be, however,
that some distributional differences that are lexically related, or simply nonlinguistic,
exist and that they can be exploited in an automatic learning process.
We can test the independence of the distribution of arguments and adjuncts from
the distribution of noun or verb attachment with a χ2 test. The test tells us that the two
distributions are not independent (p &lt; .001). It remains, however, to be established if
the dependence of the two distributions is sufficiently strong to improve learning of
one of the two classifications, if the other is known. This question can be investigated
empirically by augmenting the training features for a learning algorithm that solves
the usual binary attachment problem with the diagnostic features for argumenthood.
If this augmentation results in an improvement in the accuracy of the PP attachment,
then we can say that the notion of argument is, at least in part, related to the attachment
site of a PP. If no improvement is found, then this confirms that the argument status
of a PP must be established independently. Conversely, we can augment the input to
the classification into argument and adjuncts with information related to the attach-
ment site to reach analogous conclusions about the distinction between arguments
and adjuncts.
Corpora and Materials. The data for the experiments illustrated below are drawn from
the same corpora as those used in the previous experiments. In particular, recall that the
corpus from which we draw the statistics is different from the corpus from which we
draw the training examples and also from the testing corpus. In both sets of experiments
described below, we restrict our observation to those examples in the corpus that are
ambiguous between the two attachment sites, as is usual in studies of PP attachment.
The values of the learning features were calculated on all the instances in the statistics
corpus, so in practice we use both the unambiguous cases and ambiguous cases in the
estimation of the features of the ambiguous cases.
The Input Data. Each input vector represents an instance of an ambiguous PP attachment,
which could be both noun or verb attached, either as an argument or as an adjunct.
Each vector contains 20 training features. They comprise the four lexical heads, their
</bodyText>
<page confidence="0.99572">
364
</page>
<note confidence="0.830659">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.999907909090909">
WordNet classes (v, n1, p, n2, vcl, n1cl, and n2cl), and all the different variants of the im-
plementation of the diagnostics. Finally, depending on the experiments reported below
we use either a two-valued target feature (N or V) or a four-valued target feature (Narg,
Nadj, Varg, Vadj), indicating the type of attachment. More specifically, the features
implementing the diagnostics are the variants of the measures of head dependence for
the PPs attached to verbs and nouns, respectively (hdepv1, hdepv2, and hdepn1, hdepn2,
hdepn3); the variants of the measure of optionality (opt1, opt2, opt3); the measures of
iterativity for verb-attached and noun-attached PPs, respectively (iterv, itern); and finally
the measures for copular paraphrase and deverbal noun, respectively (para, deverb).
We use the C5.0 Decision Tree Induction Algorithm (Quinlan 1993), and the imple-
mentation of SVMs provided in version 2.71 of LIBSVM (Chang and Lin 2001).
</bodyText>
<subsectionHeader confidence="0.978067">
5.1 Relationship of Noun–Verb Attachment Disambiguation to the
Argument-Adjunct Distinction and Vice Versa
</subsectionHeader>
<bodyText confidence="0.99928175">
Here we report on results for the task of disambiguating noun or verb attachment
first, using, among other input features, those that have been established to make the
argument-adjunct distinction. The same corpora described above were used, with a
two-valued target (N or V). We report results for two sets of experiments. One set of
experiments takes all examples into account. In another set of experiments, examples
containing the preposition of were not considered, as this preposition is extremely
frequent (it adds up to almost half of the noun attachment cases) and it is almost always
attached to a noun as argument. It has therefore a very peculiar behavior. The best
combination of features reported below was established using Section 24 of the Penn
Treebank; all the tests reported here are in Section 23.
Table 9 reports the disambiguation accuracy of the comparative experiments per-
formed. The first line reports the baseline accuracy for the task, calculated by per-
forming the classification using only the feature preposition. The best result is obtained
by a combination of features in which lexical classes act as the predominant learning
feature, either in combination with the lexical items or alone (line 2 with of, line 3
without of). We note, however, that the diagnostic features that are included in the best
diagnostic feature combination are those based in part on individual words and not
those based entirely on classes. Most importantly, those diagnostics that are meant to
directly indicate the argument or adjunct status of a PP do not help in the resolution of
PP attachment, as expected.
</bodyText>
<tableCaption confidence="0.502955">
Table 9
Percent accuracy using combinations of features for two-way attachment disambiguation.
Best combinations for experiments with of is (opt1, hdepv1, hdepn1, para) and for
experiments without of is (opt1, opt2, hdepv1, hdepv2, hdepn1, hdepn2, para).
Features used Accuracy with of (%) Accuracy without of (%)
</tableCaption>
<listItem confidence="0.997225714285714">
1. Prep (baseline) 70.9 59.5
2. Prep + classes 78.1 71.3
3. Only classes 70.2 72.3
4. Only all diagnostics 75.9 64.5
5. Prep + all diagnostics 77.1 68.2
6. Prep + best feature combination 75.8 67.8
7. Prep + classes + best feature combination 76.6 67.5
</listItem>
<page confidence="0.9963">
365
</page>
<note confidence="0.499404">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999939228571428">
We conclude, then, that the notion of argument and adjunct is only partially cor-
related to the classification of PPs into those that attach to the noun and those that
attach to the verb. Clearly, diagnostics are not related to the attachment site, but lexical
classes appear to be. On the one hand, this result indicates that the notion of argument
is not entirely derivative from the attachment site. On the other hand, it shows that
some features developed to distinguish arguments from adjuncts could improve the
disambiguation of the attachment site.
The same conclusion is confirmed by a simpler, much more direct experiment,
where the classification into noun or verb attachment is performed with a single input
attribute. This attribute is the feature indicating if the example is an argument or an
adjunct, and it is calculated by a binary decision tree classifier using the best feature
combination on the argument-adjunct discrimination task. In this case too the classifi-
cation accuracy is a little (2.5%) better than chance baseline.
The converse experiment does not reveal any correlations, confirming that the
interdependence between the two factors is weak. The attachment status of the am-
biguous PP, whether noun or verb attached, is input among other features, to determine
whether the PP is an argument or an adjunct. Results are shown in Table 10, where
the attachment feature is called NVstatus. Lines 1 and 2 show that NVstatus is a
better baseline than chance. Lines 3 and 4 indicate that the feature preposition offers a
good baseline over which NVstatus improves only if the preposition of is included,
as expected. Lines 5 and 6 and lines 7 and 8 show that adding NVstatus to the other
features does not improve performance. As previously, the lexical classes are the best
performing features.
The same conclusion is reached by a simple direct experiment where we classify
PPs into arguments and adjuncts using as only input feature the output of a classifier
between noun or verb attachment. This attachment classifier is trained on the best
feature combination, preposition, and word classes. We find that the attachment status
has no effect on the accuracy of the classification, as the feature is not used.
Overall, these results indicate that there is a small interdependence between the two
classification problems and therefore weakly support a view of the PP disambiguation
problem where both the site of the attachment and the argument or adjunct function
of the PP are disambiguated together in one step. In the next section, we explore this
hypothesis, while also investigating which feature combinations give the best results in
a four-way PP classification, where PPs are classified as noun arguments, noun adjuncts,
verb arguments, and verb adjuncts.
</bodyText>
<tableCaption confidence="0.50856775">
Table 10
Percent accuracy using combinations of features for argument adjunct classification, including
NV as input feature. Best features combination is (opt1, opt3, hdepv1, hdepv2, hdepn1).
Features used Accuracy with of (%) Accuracy without of (%)
</tableCaption>
<listItem confidence="0.993324375">
1. Chance (args) 69.6 55.9
2. NVstatus 73.0 62.3
3. Prep (baseline) 81.6 82.0
4. NVstatus + prep 87.2 81.5
5. Prep + classes 89.2 84.6
6. Prep + classes + NVstatus 89.0 84.1
7. Prep + classes + best features + NVstatus 88.2 82.9
8. Prep + classes + best features 88.2 83.6
</listItem>
<page confidence="0.997629">
366
</page>
<note confidence="0.943292">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<subsectionHeader confidence="0.925957">
5.2 One- and Two-step Four-way Classification
</subsectionHeader>
<bodyText confidence="0.998958068181818">
Having shown that argumenthood of a PP is not entirely derivative of its attachment
site, but that the two tasks are weakly correlated, the task of PP attachment itself is
reformulated as a four-way classification, yielding a finer-grained and more informative
classification of PP types.
As discussed in the introduction, those applications for which the resolution of the
PP attachment ambiguity is important often need to know more about the PP than its
attachment site, in particular one might need to know whether the PP is an argument
or not. For example, the output of a parser might be used to determine the kernel of a
sentence—the predicate with its arguments—for further text processing, for translation,
or for constructing a lexicon. We redefine therefore the problem of PP attachment as a
four-way classification problem. We investigate here what features are best predictors
of this classification. Again, we report results for two sets of experiments. One set of
experiments takes all examples into account, whereas the other excludes all examples
including the preposition of. As usual, the best combination of features reported below
was established using Section 24; all the tests reported here are in Section 23.
To classify PPs into four classes, we have two options: We can construct a single
four-class classifier or we can build a sequence of binary classifiers. The discrimination
between noun and verb attachment can be performed first, and then further refined into
attachment as argument or adjunct, performing the four-way classification in two steps.
The two-step approach would be the natural way of extending current PP attachment
disambiguation methods to the more specific four-way attachment we propose here.
However, based on the previous experiments, which showed a limited amount of
dependence between the two tasks, previous work on a similar data set (Merlo 2003),
and general wisdom in machine learning, there is reason to believe that it is better
to solve the four-way classification problem directly rather than first solving a more
general problem and then specializing the classification.
To test these expectations, we performed both kinds of experiments—a direct four-
way classification experiment and a two-step classification experiment—to investigate
which of the two methods is better. The direct four-way classification uses the attributes
described above to build a single classifier. For comparability, we created a two-step
experimental setup as follows. We created three binary classifiers. The first one performs
the noun–verb attachment classification. Its learning features comprise the four lexical
heads and their WordNet classes. We also train two classifiers that learn to distinguish
arguments from adjuncts. One classifier is trained only on verb-attachment exemplars
and uses only the best verb-attachment-related features. The third classifier is trained
only on noun-attachment exemplars and utilizes only the best noun-attachment-related
features. The test data is first given to the noun–verb attachment classifier. Then, the
test examples classified as verbs are given to the verb argument-adjunct classifier, and
the test examples classified as nouns are given to the noun argument-adjunct classifier.
Thus, this cascade of classifiers performs the same task as the four-way classifier, but it
does so in two passes.
Table 11 shows that overall the one-step classification is better than the two-step
classification, confirming the intuition that the two labeling problems should be solved
at the same time.8 However, if we break down the performance, we see that recall of
</bodyText>
<footnote confidence="0.880065">
8 The difference between the two results is significant (p &lt; .05) according to the randomized test described
in Yeh (2000).
</footnote>
<page confidence="0.987709">
367
</page>
<note confidence="0.457436">
Computational Linguistics Volume 32, Number 3
</note>
<tableCaption confidence="0.993028">
Table 11
</tableCaption>
<table confidence="0.994172">
Percent precision, recall, and F score for the best two-step and one-step four-way classification
of PPs, including and not including the preposition of.
Two-step + of One-step + of
Prec Rec F Prec Rec F
V-arg 37.5 45.6 41.2 42.2 29.3 34.6
V-adj 56.2 52.2 54.1 59.6 60.2 59.9
N-arg 83.0 83.5 83.2 81.3 91.3 86.0
N-adj 71.2 57.5 63.6 69.5 56.2 62.1
Accuracy 68.9 72.0
Two-step − of One-step − of
Prec Rec F Prec Rec F
V-arg 41.3 50.0 45.3 42.2 31.4 36.0
V-adj 52.8 41.6 46.5 59.6 60.2 59.9
N-arg 67.3 70.0 68.6 65.4 80.7 71.9
N-adj 60.3 60.3 60.3 69.5 56.2 62.1
Accuracy 56.6 60.9
</table>
<bodyText confidence="0.998925703703704">
V-arg is lower in the one-step procedure than in the two-step procedure, in both cases,
and that the overall performance for V-arg is worse in the one-step procedure. This
might indicate that which procedure to use depends on whether precision or recall or
overall performance is most important for the application at hand.
Table 12 reports the confusion matrix of the classification that reaches the best
performance without the preposition of, which corresponds to the lower right panel
of Table 11. It can be observed that performances are reasonably good for verb adjuncts,
and noun arguments and adjuncts, but they are quite poor for the classification of prepo-
sitional phrases that are arguments of the verb. It is not clear why verb arguments are so
badly classified. We tested the hypothesis that this result is a side effect of the mapping
we have defined from the Penn Treebank label to the label we use in this classifier,
arguments or adjuncts. Recall that untagged PPs have been mapped onto the argument
label, but these are highly inconsistent labels, as we have seen in the manual validation
of the target attribute in Section 3. Then, verb arguments might be represented by noisier
training examples. This hypothesis is not confirmed. In a little experiment where the
training data did not contain untagged verb-attached PPs, the overall performance in
identifying the verb argument class did not improve. An improvement in precision was
counteracted by a loss in recall, yielding slightly worse F measures. Another observation
related to verb arguments can be drawn by comparing the experiments reported in
Section 4 to the experiments reported in the current section. This comparison shows
that the low performance in classifying verb arguments does not arise because of an
inability to distinguish verb arguments from verb adjuncts. Rather, it is the interac-
tion of making this distinction and disambiguating the attachment site as a single
classification task that creates the problem. This is also confirmed by the considerable
number of cases of noun arguments and verb arguments that are incorrectly classi-
fied, as shown in Table 12. Clearly, further study of the properties of verb arguments
is needed.
</bodyText>
<page confidence="0.99762">
368
</page>
<note confidence="0.98155">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<tableCaption confidence="0.7628065">
Table 12
Confusion matrix of the best one-step four-way classification of PPs without the preposition of.
</tableCaption>
<table confidence="0.971922571428571">
Assigned classes
V-arg V-adj N-arg N-adj Total
Actual classes V-arg 27 17 39 3 86
V-adj 15 68 18 12 113
N-arg 19 7 121 3 150
N-adj 3 22 7 41 73
Total 64 144 185 59 422
</table>
<bodyText confidence="0.99939225">
Overall, it is interesting to notice that solving the four-way task causes only a little
degradation to the accuracy of the original disambiguation between attachment to the
noun or to the verb. On this data set, the accuracy of disambiguating the attachment
site is of 83.6% (without PPs containing of). Accuracy decreases a little to 82.7% if the
binary attachment disambiguation result is calculated on the output of the four-way
task. This little degradation is to be expected as the four-way task is more difficult. The
accuracy of the four-way task on the simple noun or verb binary attachment distinc-
tion seems, however, acceptable, if one considers that a finer-grained discrimination
is provided.
Table 13 reports the classification accuracy of a set of comparative experiments.
Here again, the first line reports the baseline accuracy for the task, calculated by
performing the classification using only the feature preposition. We notice that in both
columns the best results are obtained by the same combination of features that includes
some lexical features, some classes, and some diagnostic features. This shows that the
distinction between arguments and adjuncts is not exclusively a syntactic phenom-
enon and lexical variability plays an important role. Similarly to the previous set of
experiments, we note that the diagnostic features that are included in the best feature
combination are those based, at least in part, on individual words, and not those based
entirely on classes. The importance of lexical classes, however, is confirmed by the
fact that the best result is only marginally better than the second best result, in which
lexical classes act as the predominant learning feature, either in combination with the
lexical items or alone (line 3 with of, line 2 without of). We can conclude from these
observations that using various features defined at different levels of granularity allows
the learner to better use lexical information when available, and to use more abstract
</bodyText>
<tableCaption confidence="0.76267">
Table 13
</tableCaption>
<bodyText confidence="0.7312815">
Percent accuracy using combinations of features for a one-step four-way classification of PPs.
Best combination = (vcl, n1cl, p, opt1, opt2, hdepv1, hdepv2, hdepn1, para).
</bodyText>
<listItem confidence="0.909700714285714">
Features Accuracy (%) with of Without of (%)
1. Prep (baseline) 64.2 49.5
2. Prep + classes 68.9 60.2
3. Only classes 71.5 49.3
4. All features 68.9 54.5
5. Only diagnostics 67.4 54.3
6. Best combination 72.0 60.9
</listItem>
<page confidence="0.996289">
369
</page>
<note confidence="0.601129">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.9995904">
levels of generalization when finer-grained information is not available. Across the two
tasks (illustrated in Tables 9 and 13) we notice that the best diagnostics features are
almost always the same. Variants change, but the kinds of diagnostics that are useful
remain stable. This probably indicates that some diagnostics are reliably estimated,
whereas others are not, and cannot be used fruitfully.
</bodyText>
<subsectionHeader confidence="0.975503">
5.3 Results Using Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.999988727272727">
As mentioned above, SVMs have yielded very good results in many important applica-
tions in NLP. It is reasonable to wonder if we can improve the results for the four-way
classification, and especially the less than satisfactory performance on verb arguments,
using this learning algorithm. Table 14 shows the results to be compared to those in the
right-hand panel of Table 11.
If we consider the F-measures of this table and the right-hand panel of Table 11, the
most striking difference is that V-arguments, although still the worst cell in the table,
have improved by almost 20% (36.0% vs. 55.9%) in performance for the experiments
without of. Notice that verb arguments are now better classified than in the two-step
method. Also, the overall accuracy is significantly improved by several percentage
points, especially for the condition without the preposition of (p &lt; .02).
</bodyText>
<subsectionHeader confidence="0.983347">
5.4 Conclusion
</subsectionHeader>
<bodyText confidence="0.999947916666667">
In this section we have shown that the notion of argument is of limited help in dis-
ambiguating the attachment of ambiguous PPs, indicating that the two notions are
not strictly related and must be established independently. In a series of four-way
classification experiments, we show that the classification performances are reasonably
good for verb adjuncts, noun arguments, and noun adjuncts, but they are poor for
the classification of prepositional phrases that are arguments of the verb, if decision
trees are used. Overall performance and especially identification of verb arguments
is improved if support vector machines are used. We also show that better accuracy
is achieved by performing the four-way classification in one step. The features that
appear to be most effective are lexical classes, thus confirming current linguistic theories
that postulate that a given head’s argument structure depends on the head’s lexical
semantics, especially for verbs (Levin 1993).
</bodyText>
<tableCaption confidence="0.826598">
Table 14
</tableCaption>
<bodyText confidence="0.615518">
Percent precision, recall, and F-score for the best four-way classification of PPs, including
and not including the preposition of using SVMs.
</bodyText>
<table confidence="0.991865285714286">
One-step + of One-step − of
Prec Rec F Prec Rec F
V-arg 59.5 47.8 53.0 60.0 52.3 55.9
V-adj 63.2 63.7 63.4 62.8 62.8 62.8
N-arg 83.6 93.4 88.2 69.9 85.3 76.9
N-adj 72.5 50.7 59.7 72.5 50.7 59.7
Accuracy 75.9 66.6
</table>
<page confidence="0.992023">
370
</page>
<note confidence="0.95385">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<sectionHeader confidence="0.999624" genericHeader="evaluation">
6. Related Work
</sectionHeader>
<bodyText confidence="0.9999316">
The resolution of the attachment of ambiguous PPs is one of the staple problems in
computational linguistics. It serves as a good testing ground for new methods as it
is clearly defined and self-contained. We have argued, however, that it is somewhat
oversimplified, because knowing only the attachment site of a PP is of relatively
little value in a real application. It would be more useful to know where the PP
is attached and with what function. We review below the few pieces of work that
have tackled the problem of labeling PPs by function (arguments or adjuncts) as a
separate labeling problem. Other pieces of work have asked a similar question in
the context of acquiring high-precision subcategorization frames. We review a few of
them below.
</bodyText>
<subsectionHeader confidence="0.990257">
6.1 On the Automatic Distinction of Arguments and Adjuncts
</subsectionHeader>
<bodyText confidence="0.999993588235294">
A few other pieces of work attempt to distinguish PP arguments from adjuncts auto-
matically (Buchholz 1999; Merlo and Leybold 2001; Villavicencio 2002). We extend and
modify here the preliminary work reported in Merlo and Leybold (2001) by extending
the method to noun attachment, elaborating more learning features, including cases
specifically developed for noun attachment, refining all the counting methods, thus
validating and extending the approach.
The current work on automatic binary argument-adjunct classifiers appears to
compare favorably to the only other study on this topic (Buchholz 1999). Buchholz
(1999) reports an accuracy of 77% for the argument-adjunct distinction of PPs per-
formed with a memory-based learning approach, to be compared with our 80% and
94% for verb and noun attachments, respectively. However, the comparison cannot
be very direct, as Buchholz considers all types of attachment sites, not just verbs
and nouns.
More recently, Villavicencio (2002) has explored the performance of an argu-
ment identifier, developed in the framework of a model of child language learning.
Villavicencio concentrates on locative PPs proposing that the distinction between oblig-
atory arguments, optional arguments, and adjuncts is made based on two features: a
feature derived from a semantically motivated hierarchy of prepositions and predicates
and a simple frequency cutoff of 80% of co-occurrence between the verb and the PP
that distinguishes obligatory arguments from the other two classes. She evaluates the
verbs put, come, and draw (whose locative arguments belong to the three classes above,
respectively). The approach is not directly comparable, as it is not entirely corpus-based
(the input to the algorithm is an impoverished logical form), and the evaluation is on a
smaller scale than the present work. On a test set of the occurrences of three verbs, which
is the same set inspected to develop the learning features, Villavicencio gets perfect
performance. These are very promising results, but because they are not calculated on
a previously unseen test set, the generability of the approach is not clear. Moreover,
Villavicencio applies only one diagnostic test to determine if a PP is an argument or
an adjunct, whereas our extensive validation study has shown that several tests are
necessary to reach a reliable judgment.
In a study about the automatic acquisition of lexical dependencies for lexicon build-
ing, Fabre and Bourigault (2001) discuss the relation of the problem of PP attachment
and the notion of argument and adjunct. They correctly notice that approaches such
as theirs, inspired by Hindle and Rooth (1993), are based on the assumption that high
</bodyText>
<page confidence="0.991302">
371
</page>
<note confidence="0.570233">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999727916666667">
co-occurrence between words is an indication of a lexical argumenthood relation. As
also noticed in Merlo and Leybold (2001), this is not always the case: some adjuncts
frequently co-occur with certain heads too. Fabre and Bourigault propose a notion of
productivity that strongly resembles our notions of optionality and head dependence to
capture the two intuitions about the distribution of arguments and adjuncts. Arguments
are strongly selected by the head (the head to complement relation is not productive),
whereas adjuncts can be selected by a wide spread of heads (the complement to head
selection is highly productive). They propose, but do not test, the hypothesis that
this notion might be useful for the general problem of PP attachment. The results
in the current article show that this is not the case. In fact, we have argued that
there is no real reason to believe that the two notions should be related, other than
marginally.
</bodyText>
<subsectionHeader confidence="0.993149">
6.2 On the Distinction of Argument from Adjunct PPs for
Subcategorization Acquisition
</subsectionHeader>
<bodyText confidence="0.9999655">
As far as we are aware, this is the first attempt to integrate the notion of argumenthood
in a more comprehensive formulation of the problem of disambiguating the attachment
of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the
semantic factors in the disambiguation of a PP, indicating that verb complements are the
most difficult. We confirm their finding that noun arguments are more easily identified,
whereas verb complements (either arguments or adjuncts) are more difficult. Other
pieces of work address the current problem in the larger perspective of distinguishing
arguments from adjuncts for subcategorization acquisition (Korhonen 2002a; Aldezabal
et al. 2002).
The goal of the work of Korhonen (2002a, 2002b) is to develop a semantically driven
approach to subcategorization frame hypothesis selection that can be used to improve
large-scale subcategorization frame acquisition. The main idea underlying the approach
is to leverage the well-known mapping between syntax and semantics, inspired by
Levin’s (1993) work. Korhonen uses statistics over semantic classes of verbs to smooth
a distribution of subcategorization frames and then applies a simple frequency cutoff to
select the most reliable subcategorization frames. Her work is related to ours in several
ways. First, the automatic acquisition task leverages correspondences between syntax
and semantics, particularly clear in the organization of the verb lexicon, similarly
to Merlo and Stevenson (2001). Some of our current results are also based on this
correspondence, as we assume that the notion of argument is a notion at the interface
of the syntactic and semantic levels, and participates in both, determining not only the
valency of a verb but also its subcategorization frame. Our work confirms the results
reported in Korhonen (2002a), which indicate that using word classes improves the
extraction of subcategorization frames. Differently from Korhonen, however, we do
not allow feedback between levels. In her work, syntactic similarity of verbs’ subcat-
egorization sets based on an external resource (LDOCE codes) are used to determine a
semantic classification of verbs—or rather to partially reorganize Levin’s classification.
This semantic classification is then used to collect statistics that are used to smooth a
subcategorization distribution. In our work, instead, we do use WordNet in some places
to give us information on verb classes, but we never use explicit semantic information
on the set of verb subcategorization frames to determine the notion of argument
or adjunct.
Aldezabal et al. (2002) is another piece of work related to our current proposal.
In this article, the distinction between arguments and adjuncts is made to determine
</bodyText>
<page confidence="0.987667">
372
</page>
<note confidence="0.865439">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<bodyText confidence="0.999773333333333">
the subcategorization frames of verbs for a language, Basque, for which not many de-
veloped resources exist. This illustrates another use of the distinction between argument
and adjuncts, which is not apparent when working on English.
</bodyText>
<subsectionHeader confidence="0.99832">
6.3 On Learning Abstract Notions Using Corpus-based Statistical Methods
</subsectionHeader>
<bodyText confidence="0.999986375">
Learning arguments and adjuncts is an example of learning simplified semantic in-
formation by using syntactic and lexical semantic correlates. We learn the target con-
cepts of arguments and adjuncts by using corpus-based indicators of their proper-
ties. It remains to be determined if we just learn correlates of a unified notion, or if
the distinction between arguments and adjuncts is a clustering of possibly unrelated
properties.
As explained in the introduction, native speakers’ judgments on the argument and
adjunct status of PPs are very unstable. No explanation is usually proposed of the
fact that the tests of argumenthood are often difficult to judge or even contradict each
other. As a possible explanation for the difficulty in pinpointing exactly the properties
of arguments and adjuncts, Manning (2003) suggests that the notion of argument or
adjunct is not categorical. The different properties of argument and adjuncts are not the
reflex of a single grammatical underlying notion, but they can be ascribed to different
mechanisms. What appears as a not entirely unified behavior is in fact better explained
as separate properties.
The current article provides a representation that can support both the categor-
ical and the gradient approach to the distinction between arguments and adjuncts.
We have decomposed the notion of argument into a vector of features. The notion
of argumenthood is no longer necessarily binary, but it allows several dimensions of
variation, each potentially related to a different principle of grammar. In the current
article, we have adopted a supervised approach to the learning task and adopted
a binary classification. To pursue a line of reasoning where a gradient representa-
tion of the notion of argument is preferred, we would no longer be interested in
classifying the vectorial information according to a predetermined binary or four-
way target value, as was done in the supervised learning experiments. The appropri-
ate framework would then be unsupervised learning, where several algorithms are
available to explore the hidden regularities of a vectorial representation of clusters
of PPs.
Whether a linguistic notion should be considered categorical or gradient is both
a matter of empirical fact and of the explanatory power of the theory into which the
notion is embedded. Assessing the strengths and weaknesses of these two approaches
is beyond the scope of the current article.
</bodyText>
<sectionHeader confidence="0.993483" genericHeader="conclusions">
7. Conclusions
</sectionHeader>
<bodyText confidence="0.99977425">
We have proposed an augmentation of the problem of PP attachment as a four-way
disambiguation problem, arguing that what is needed in interpreting prepositional
phrases is knowledge about both the structural attachment site (the traditional noun–
verb attachment distinction) and the nature of the attachment (the distinction of
arguments from adjuncts). Practically, we have proposed a method to learn arguments
and adjuncts based on a definition of arguments as a vector of features. Each feature
is either a lexical element or its semantic class or it is a numerical representation of
a diagnostic that is used by linguists to determine if a PP is an argument or not. We
</bodyText>
<page confidence="0.995422">
373
</page>
<note confidence="0.571776">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.9953461875">
have shown in particular that using lexical classes as features yields good results, and
that diagnostics based on linguistic theory improve the performance even further. We
have also argued that the notion of argument does not help much in disambiguating
the attachment site of PPs, indicating that the two notions are not closely correlated
and must be established independently. We have performed a series of four-way
classification experiments, where we classify PPs as arguments or adjuncts of a noun,
and as arguments or adjuncts of a verb. We show that the classification performances are
reasonably good for verb adjuncts, noun arguments, and noun adjuncts, independent
of the learning algorithm. Classification performances of prepositional phrases that are
arguments of the verb are poor if decision trees are used, but are greatly improved by
the use of a large margin classifier. The features that appear to be most effective are
lexical classes, thus confirming current linguistic theories that postulate that a verb’s
argument structure depends on a verb’s lexical semantics (Levin 1993). Future work
lies in further investigating the difference between arguments and adjuncts to achieve
even finer-grained classifications and to model more precisely the semantic core of
a sentence.
</bodyText>
<figure confidence="0.751150625">
1. Appendix: PP Configurations
Sequence of single PP attached to a verb
Configuration Structure Example
Transitive [vp V NP PP] join board as director
Passive [vp NP PP] tracked (yield) by report
Sentential Object [vp V NP PP] continued (to slide) amid signs
Intransitive [vp V PP] talking about years
Sequence of single PP attached to a noun
</figure>
<bodyText confidence="0.959578538461539">
Noun phrase [np NP PP] form of asbestos
Transitive [vp V [np NP PP]] have information on users
Transitive with two PPs, one attached to verb, other to noun
[vp V [np NP PP] PP] dumped sacks of material into bin
Noun phrase with two PPs attached low
[np NP [pp P [np NP PP]]] exports at end of year
Transitive verb with two PPs attached low
[vp V [np NP [pp P [np NP PP]]]] lead team of researchers from institute
Transitive with two PPs, one attached to verb, other to other PP
[vp V NP [pp P [np NP PP]]] imposed ban on all of asbestos
Intransitive with two PPs, one attached to verb, other to noun
[vp V [pp P [np NP PP]]] appear in journal of medicine
Phrasal object with two PPs
</bodyText>
<table confidence="0.7325292">
[vp V NP [pp P [np NP PP]]] continued (to surge) on rumors of buying
Passive form with two PPs
[vp V NP [pp P [np NP PP]]] approved (request) by houses of Congress
Sequence of 2 PPs attached to a verb
Intransitive [vp V PP PP] grew by billion during week
Passive [vp V NP PP PP] passed (bill) by Senate in forms
Transitive [vp V NP PP PP] giving 16 to graders at school
Sequence of 2 PPs attached to a noun
Noun [np NP PP PP] sales of buses in October
Transitive [vp V [np NP PP PP]] meet demands for products in Korea
</table>
<page confidence="0.996601">
374
</page>
<note confidence="0.957875">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<sectionHeader confidence="0.997044" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998150111111111">
Most of this research was conducted thanks
to the generous support of the Swiss
National Science Foundation, under grant
1114-065328.01, while the second author
was a master’s student at the University
of Geneva. We thank Eric Joanis for his
precious help in constituting the database of
PP tuples. We also thank Jamie Henderson
for providing native-speaker judgments.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843514285714">
Aldezabal, Izaskun, Maxux Aranzabe, Koldo
Gojenola, Kepa Sarasola, and Aitziber
Atutxa. 2002. Learning argument/adjunct
distinction for Basque. In Proceedings of the
Workshop of the ACL Special Interest Group
on the Lexicon on Unsupervised Lexical
Acquisition, pages 42–50, Philadelphia, PA.
Argaman, Vered and Neal Pearlmutter. 2002.
Lexical semantics as a basis for argument
structure frequency bias. In Paola Merlo
and Suzanne Stevenson, editors, The
Lexical Basis of Sentence Processing: Formal,
Computational and Experimental Issues. John
Benjamins, Amsterdam/Philadelphia,
pages 303–324.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings
of the Thirty-sixth Annual Meeting
of the Association for Computational
Linguistics and Seventeenth International
Conference on Computational Linguistics
(ACL-COLING’98), pages 86–90,
Montreal, Canada.
Bies, Ann, Mark Ferguson, Karen Katz, and
Robert MacIntyre. 1995. Bracketing
guidelines for Treebank II style, Penn
Treebank Project. Technical report,
University of Pennsylvania, Philadephia.
Buchholz, Sabine. 1999. Distinguishing
complements from adjuncts using
memory-based learning. ILK,
Computational Linguistics, Tilburg
University.
Chang, Chih-Chung and Chih-Jen Lin.
2001. LIBSVM: A Library for Support
Vector Machines. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/
libsvm.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment
through a backed-off model. In
Proceedings of the Third Workshop on Very
Large Corpora, pages 27–38,
Cambridge, MA.
CoNNL. 2004. Eighth Conference on
Computational Natural Language Learning
(CoNLL-2004). Boston, MA.
CoNLL. 2005. Ninth Conference on
Computational Natural Language Learning
(CoNLL-2005). Ann Arbor, MI.
Dorr, Bonnie. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation.
Machine Translation, 12(4):1–55.
Fabre, C´ecile and Didier Bourigault. 2001.
Linguistic clues for corpus-based
acquisition of lexical dependencies. In
Proceedings of the Corpus Linguistics
Conference, pages 176–184, Lancaster, UK.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245–288.
Grimshaw, Jane. 1990. Argument Structure.
MIT Press.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103–120.
Jackendoff, Ray. 1977. X&apos; Syntax:
A Study of Phrase Structure. MIT Press,
Cambridge, MA.
Korhonen, Anna. 2002a. Semantically
motivated subcategorization acquisition.
In Proceedings of the Workshop of the ACL
Special Interest Group on the Lexicon on
Unsupervised Lexical Acquisition,
pages 51–58, Philadelphia, PA, July.
Korhonen, Anna. 2002b. Subcategorisation
Acquisition. Ph.D. thesis, University of
Cambridge.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago, IL.
Manning, Christopher. 2003. Probabilistic
syntax. In Rens Bod, Jennifer Hay, and
Stephanie Jannedy, editors, Probabilistic
Linguistics. MIT Press, pages 289–314.
Marantz, Alex. 1984. On the Nature of
Grammatical Relations. MIT Press,
Cambridge, MA.
Marcus, M., G. Kim, A. Marcinkiewicz,
R. Macintyre, A. Bies, M. Ferguson,
K. Katz, and B. Schasberger.1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 114–119, Plainsboro, NJ.
Marcus, Mitch, Beatrice Santorini, and
Mary Ann Marcinkiewicz.1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313–330.
Merlo, Paola. 2003. Generalised
PP-attachment disambiguation using
corpus-based linguistic diagnostics.
</reference>
<page confidence="0.970669">
375
</page>
<reference confidence="0.993946605042017">
Computational Linguistics Volume 32, Number 3
In Proceedings of the Tenth Conference
of the European Chapter of the
Association for Computational Linguistics
(EACL’03), pages 251–258, Budapest,
Hungary.
Merlo, Paola, Matt Crocker, and Cathy
Berthouzoz.1997. Attaching multiple
prepositional phrases: Generalized
backed-off estimation. In Proceedings
of the Second Conference on Empirical
Methods in Natural Language Processing,
pages 145–154, Providence, RI.
Merlo, Paola and Matthias Leybold. 2001.
Automatic distinction of arguments and
modifiers: The case of prepositional
phrases. In Proceedings of the Fifth
Computational Natural Language Learning
Workshop (CoNLL-2001), pages 121–128,
Toulouse, France.
Merlo, Paola and Suzanne Stevenson. 2001.
Automatic verb classification based on
statistical distributions of argument
structure. Computational Linguistics,
27(3):373–408.
Miller, George, Richard Beckwith,
Christiane Fellbaum, Derek Gross,
and Katherine Miller. 1990. Five
papers on Wordnet. Technical report,
Cognitive Science Laboratory, Princeton
University.
Nielsen, Rodney and Sameer Pradhan. 2004.
Mixing weak learners in semantic parsing.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP-2004), pages 80–87, Barcelona,
Spain, July.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31:71–105.
Phillips, William and Ellen Riloff. 2002.
Exploiting strong syntactic heuristics and
co-training to learn semantic lexicons. In
Proceedings of the 2002 Conference on
Empirical Methods in Natural Language
Processing (EMNLP 2002), pages 125–132,
Philadelphia, PA.
Pollard, Carl and Ivan Sag. 1987. An
Information-based Syntax and Semantics,
volume 13. CSLI Lecture Notes,
Stanford University.
Quinlan, J. Ross. 1993. C4.5 : Programs for
Machine Learning. Series in Machine
Learning. Morgan Kaufmann,
San Mateo, CA.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, London.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based on
maximum entropy models. In Proceedings
of the Second Conference on Empirical
Methods in Natural Language Processing,
pages 1–10, Providence, RI.
Ratnaparkhi, Adwait, Jeffrey Reynar, and
Salim Roukos.1994. A maximum entropy
model for prepositional phrase
attachment. In Proceedings of the ARPA
Workshop on Human Language Technology,
pages 250–255, Plainsboro, NJ.
Riloff, Ellen and Mark Schmelzenbach.
1998. An empirical approach to conceptual
case frame acquisition. In Proceedings
of the Sixth Workshop on Very Large Corpora,
pages 49–56, Montreal.
Sch¨utze, Carson T. 1995. PP Attachment and
Argumenthood. MIT Working Papers in
Linguistics, 26:95–151.
SENSEVAL-3.2004. Third International
Workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL-3).
Barcelona, Spain.
Srinivas, Bangalore and Aravind K. Joshi.
1999. Supertagging: An approach to
almost parsing. Computational Linguistics,
25(2):237–265.
Stede, Manfred. 1998. A generative
perspective on verb alternations.
Computational Linguistics, 24(3):401–430.
Stetina, Jiri and Makoto Nagao. 1997.
Corpus based PP attachment ambiguity
resolution with a semantic dictionary. In
Proceedings of the Fifth Workshop on Very
Large Corpora, pages 66–80, Beijing/
Hong Kong.
Swier, Robert and Suzanne Stevenson.
2005. Exploiting a verb lexicon in
automatic semantic role labelling.
In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP-05), pages 883–890,
Vancouver, Canada.
Vapnik, V. 1995. The Nature of Statistical
Learning Theory. Springer.
Villavicencio, Aline. 2002. Learning to
distinguish PP arguments from adjuncts.
In Proceedings of the 6th Conference on
Natural Language Learning (CoNLL-2002),
pages 84–90, Taipei, Taiwan.
Xue, Nianwen. 2004. Handling dislocated
and discontinuous constituents in Chinese
semantic role labelling. In Proceedings of the
Fourth Workshop on Asian Language
Resources (ALR04), pages 19–26, Hainan
Island, China.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
</reference>
<page confidence="0.996551">
376
</page>
<note confidence="0.907922">
Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment
</note>
<reference confidence="0.9992101875">
labeling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004),
pages 88–94, Barcelona, Spain.
Yeh, Alexander. 2000. More accurate tests for
the statistical significance of result
differences. In Proceedings of the 18th
International Conference in Computational
Linguistics (COLING 2000), pages 947–953,
Saarbruecken, Germany.
Zhao, Shaojun and Dekang Lin. 2004. A
nearest-neighbor method for resolving
PP-attachment ambiguities. In The First
International Joint Conference on Natural
Language Processing (IJCNLP-04),
pages 545–554, Hainan Island, China.
</reference>
<page confidence="0.998383">
377
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.180220">
<title confidence="0.988882">The Notion of Argument in Prepositional Phrase Attachment</title>
<author confidence="0.920641">Esteve</author>
<affiliation confidence="0.987122">University of Geneva University of Sussex</affiliation>
<abstract confidence="0.963832192307692">In this article we refine the formulation of the problem of prepositional phrase (PP) attachment as a four-way disambiguation problem. We argue that, in interpreting PPs, both knowledge about the site of the attachment (the traditional noun–verb attachment distinction) and the nature of the attachment (the distinction of arguments from adjuncts) are needed. We introduce a method to learn arguments and adjuncts based on a definition of arguments as a vector offeatures. In a series of supervised classification experiments, first we explore the features that enable us to learn the distinction between arguments and adjuncts. We find that both linguistic diagnostics of argumenthood and lexical semantic classes are useful. Second, we investigate the best method to reach the four-way classification of potentially ambiguous prepositional phrases. We find that whereas it is overall better to solve the problem as a single four-way classification task, verb arguments are sometimes more precisely identified if the classification is done as a two-step process, first choosing the attachment site and then labeling it as argument or adjunct. 1. Motivation Incorrect attachment of prepositional phrases (PPs) often constitutes the largest single source of errors in current parsing systems. Correct attachment of PPs is necessary to construct a parse tree that will support the proper interpretation of constituents in the sentence. Consider the timeworn example (1) I saw the man with the telescope. is important to determine if the PP the telescope to be attached as a sister to noun restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004).</abstract>
<note confidence="0.903752">This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. of Informatics, University of Sussex, Falmer, Brighton BN19QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3</note>
<abstract confidence="0.988842504672897">PP adjuncts. Consider the following example, which contains two PPs, both modifying the verb. (2) Put the block on the table in the morning. The first PP is a locative PP required by the subcategorization frame of the verb whereas the morning an optional descriptor of the time at which the action was performed. Although both are attached to the verb, the two PPs entertain different relationships with the verb—the first is an argument whereas the latter is an adjunct. Analogous examples could be built for attachments to the noun. (See examples 7a, b.) Thus, PPs cannot only vary depending on the site to which they attach in the structure, such as in example (1), but they can fulfill different functions in the sentence, such as in example (2). In principle, then, a given PP could be four-way ambiguous. In practice, it is difficult and moderately unnatural to construct examples of four-way ambiguous sentences, sentences that only a good amount of linguistic and extralinguistic knowledge can disambiguate among the noun-attached and verb-attached option, with an argument or adjunct interpretation. It is, however, not impossible. Consider benefactive constructions, such as the sentence below. (3) Darcy baked a cake for Elizabeth. this case the a benefactive, hence an argument of the verb However, the is optional; thus other non-argument PPs can occur in the same position. (4) Darcy baked a cake for 5 shillings/for an hour. Whereas in sentence (3) the PP is an argument, in (4) the PP is an adjunct, as indicated by the different status of the corresponding passive sentences and by the ordering of the PPs (arguments prefer to come first), as shown in (5) and (6). (5a) Elizabeth was baked a cake by Darcy (5b) *5 shillings/an hour were baked a cake by Darcy (6a) Darcy baked a cake for Elizabeth for 5 shillings/for an hour (6b) ??Darcy baked a cake for 5 shillings/for an hour for Elizabeth kind of ambiguity also occurs in sentences in which the is modifying the object noun phrase. Depending on the head noun in object position, and under the assumption that a beneficiary is an argument, as we have assumed in the sentences above, the PP will be an argument or an adjunct, as in the following examples, respectively. (7a) Darcy baked [cakes for children] (7b) Darcy baked [cakes for 5 shillings] 342 Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment Modeling both the site and the nature of the attachment of a PP into the tree structure is important. Distinguishing arguments from adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are common across verbs for adjuncts. They inherit these labels from the Penn Treebank annotation. Arguments are annotated instead with labels specific to each verb (Xue 2004; Palmer, Gildea, and Kingsbury 2005). From a quantitative point of view, arguments and adjuncts have different statistical properties. For example, Hindle and Rooth (1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many native speakers’ intuitions. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent preliminary work, however, we have succeeded in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold 2001; Merlo 2003). Our method develops corpus-based statistical correlates for the diagnostics used in linguistics to decide whether a PP is an argument or an adjunct. A numerical vectorial representation of the notion of argumenthood is provided, which supports automatic classification. In the current article, we expand and improve on this work, by developing new measures and refining the previous ones. We also extend that work to attachment to nouns. This extension enables us to explore in what way the distinction between argument and adjunct is best integrated in the traditional attachment disambiguation problem. We treat PP attachment as a four-way classification of PPs into noun argument PPs, noun adjunct PPs, verb argument PPs, and verb adjunct PPs. We investigate this new approach to PP attachment disambiguation through several sets of experiments, testing different hypotheses on the argument/adjunct distinction of PPs and on its interaction with the disambiguation of the PP attachment site. The two main claims can be formulated as follows. • Hypothesis 1: The argument/adjunct distinction can be performed based on information collected from a minimally annotated corpus, approximating deeper semantic information statistically. • Hypothesis 2: The learning features developed for the notion of argument and adjunct can be usefully integrated in a finer-grained formulation of the problem of PP attachment as a four-way classification. 343 Computational Linguistics Volume 32, Number 3 To test these two hypotheses, we illustrate our technique to distinguish arguments from adjuncts (Section 2), and we report results on this binary classification (Sections 3 and 4). The intuition behind the technique is that we do not need to represent the distinction between arguments and adjuncts directly, but that the distinction can be indirectly represented as a numerical vector. The feature values in the vector are corpus-based numerical equivalents of the grammaticality diagnostics used by linguists to decide whether a PP is an argument or an adjunct. For example, one of the values in the vector indicates if the PP is optional, whereas another one indicates if the PP can be iterated. Optionality and iterability are two of the criteria used by linguists to determine whether a PP is an argument or an adjunct. In Section 5, we show how this distinction supports a more refined formulation of the problem of PP attachment. We compare two methods to reach a four-way classification. One method is a two-step process that first classifies PPs as attached to the noun or to the verb, and then refines the classification by assigning argument or adjunct status to the disambiguated PPs. The other method is a one-step process that performs the four-way classification directly. We find that the latter has better overall performance, confirming our expectation (Hypothesis 2). In Section 6 we discuss the implications of the results for a definition of the notion of argument and compare our work to that of the few researchers who have attempted to perform the same distinction.</abstract>
<intro confidence="0.976537">2. Distinguishing Arguments from Adjuncts</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Izaskun Aldezabal</author>
</authors>
<title>Maxux Aranzabe, Koldo Gojenola, Kepa Sarasola, and Aitziber Atutxa.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon on Unsupervised Lexical Acquisition,</booktitle>
<pages>42--50</pages>
<location>Philadelphia, PA.</location>
<marker>Aldezabal, 2002</marker>
<rawString>Aldezabal, Izaskun, Maxux Aranzabe, Koldo Gojenola, Kepa Sarasola, and Aitziber Atutxa. 2002. Learning argument/adjunct distinction for Basque. In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon on Unsupervised Lexical Acquisition, pages 42–50, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vered Argaman</author>
<author>Neal Pearlmutter</author>
</authors>
<title>Lexical semantics as a basis for argument structure frequency bias.</title>
<date>2002</date>
<booktitle>In Paola Merlo and Suzanne Stevenson, editors, The Lexical Basis of Sentence Processing: Formal, Computational and Experimental Issues. John Benjamins, Amsterdam/Philadelphia,</booktitle>
<pages>303--324</pages>
<contexts>
<context position="21476" citStr="Argaman and Pearlmutter (2002)" startWordPosition="3400" endWordPosition="3403"> linear precedence. para(PP) ≈ P(vcopula ≺ PP) (4) Deverbal Nouns. This diagnostic is based on the observation that PPs following a deverbal noun are likely to be arguments, as the noun shares the argument structure of the verb.2 Proper counting of this feature requires identifying a deverbal noun in the head noun position of a noun phrase. We identify deverbal nouns by inspecting their morphology (Quirk et al. 1985). Specifically, the suffixes that can combine 2 Doubts have been cast on the validity of this diagnostic (Sch¨utze 1995), based on work in theoretical linguistics (Grimshaw 1990). Argaman and Pearlmutter (2002), however, have shown that the argument structures of verbs and related nouns are highly correlated. Hence, we keep deverbal noun as a valid diagnostic here, although we show later that it is not very effective. 347 Computational Linguistics Volume 32, Number 3 with verb bases to form deverbal nouns are listed and exemplified in Figure 1 on page 348. This diagnostic can be captured by a probability indicator function, which assigns probability 1 of being an argument to PPs following a deverbal noun and 0 otherwise. � deverb PP — 1 if deverbal n ≺ PP 5 ( ) 0 otherwise ( ) In conclusion, the dia</context>
</contexts>
<marker>Argaman, Pearlmutter, 2002</marker>
<rawString>Argaman, Vered and Neal Pearlmutter. 2002. Lexical semantics as a basis for argument structure frequency bias. In Paola Merlo and Suzanne Stevenson, editors, The Lexical Basis of Sentence Processing: Formal, Computational and Experimental Issues. John Benjamins, Amsterdam/Philadelphia, pages 303–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the Thirty-sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (ACL-COLING’98),</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the Thirty-sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (ACL-COLING’98), pages 86–90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for Treebank II style, Penn Treebank Project.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania, Philadephia.</institution>
<contexts>
<context position="42701" citStr="Bies et al. 1995" startWordPosition="6858" endWordPosition="6861">een oversimplified: All constituents attached to VP are structurally treated as arguments, whereas all constituents attached to NP are treated as adjuncts. The only exception are the arguments of some deverbal nouns, which are represented as arguments. Information about the distinction between arguments and adjuncts, then, must be gleaned from the semantic and function tags that have been assigned to the nodes. Constituents are labeled with up to four tags (including numerical indices) that account for the syntactic category of the constituent, its grammatical function, and its semantic role (Bies et al. 1995). Figure 2 illustrates the tags that involve PP constituents. From the description of this set of tags we can already infer some information about the argument status of the PPs. PPs with a semantic tag (LOC, MNR, PRP, TMP) are adjuncts, whereas labels indicating PP complements of ditransitive verbs (BNF, DTV) or locative verbs like put are arguments. There are, though, some cases that remain ambiguous and therefore require a deeper study. These are untagged PPs and PPs tagged -CLR. For these cases, we will necessarily only approximate the desired distinction. We have interpreted untagged PPs </context>
<context position="44184" citStr="Bies et al. 1995" startWordPosition="7098" endWordPosition="7101">ate); phrasal verbs; predication adjuncts -DTV dative object if dative shift possible (e.g., give) -BNF benefactive (dative object of for) -PRD non VP predicates -PUT locative complement of put -DIR direction and trajectory -LOC location -MNR manner -PRP purpose and reason -TMP temporal phrases Figure 2 Grammatical function and semantic tags that involve PP constituents in the Penn Treebank. in which it is stated that “NPs and Ss which are clearly arguments of the verb are unmarked by any tag” (Marcus et al. 1994, page 4), and that “Direct Object NPs and Indirect Object NPs are all untagged” (Bies et al. 1995, page 12). Although the case of PP constituents is not specifically addressed, we have interpreted these statements as supporting evidence for our choice. The tag -CLR stands for “closely related,” and its meaning varies, depending on the element it is attached to. It indicates argument status when it labels the dative object of ditransitive verbs that cannot undergo dative shift, such as in donate money to the museum, and in phrasal verbs, such as pay for the horse. It indicates adjunct status when it labels a predication adjunct as defined by Quirk et al. (1985). We interpret the tag -CLR a</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Bies, Ann, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style, Penn Treebank Project. Technical report, University of Pennsylvania, Philadephia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
</authors>
<title>Distinguishing complements from adjuncts using memory-based learning.</title>
<date>1999</date>
<journal>ILK, Computational</journal>
<institution>Linguistics, Tilburg University.</institution>
<contexts>
<context position="52100" citStr="Buchholz 1999" startWordPosition="8392" endWordPosition="8393"> learn more about making products for? For who(m) do you wonder whether Americans will learn more about making products? Figure 3 Example sentence and related list of tests and test sentences. is assigned not only to those cases that are clear arguments, but also to those cases for which a label cannot be decided). Moreover, if the hand-annotated label is reliable, it indicates that untagged PPs are somewhat more likely to be adjuncts. Our initial mapping was incorrect. In retrospect, we must conclude that other researchers had applied what appears to be the correct mapping for this data set (Buchholz 1999). We do not, however, modify the label of our training set, as that would be methodologically incorrect. We have relabeled the test set and are therefore bound to ignore any further knowledge we have gathered in relabeling, as that would amount to tailoring our training set to our test set. The consequence of this difference between our label and what we found to be true for the gold standard is that all results tested on the manually labeled test set will have to be interpreted as lower bounds of the performance to be expected on a consistently labeled data set. Besides validating the automat</context>
<context position="96039" citStr="Buchholz 1999" startWordPosition="15393" endWordPosition="15394">te of a PP is of relatively little value in a real application. It would be more useful to know where the PP is attached and with what function. We review below the few pieces of work that have tackled the problem of labeling PPs by function (arguments or adjuncts) as a separate labeling problem. Other pieces of work have asked a similar question in the context of acquiring high-precision subcategorization frames. We review a few of them below. 6.1 On the Automatic Distinction of Arguments and Adjuncts A few other pieces of work attempt to distinguish PP arguments from adjuncts automatically (Buchholz 1999; Merlo and Leybold 2001; Villavicencio 2002). We extend and modify here the preliminary work reported in Merlo and Leybold (2001) by extending the method to noun attachment, elaborating more learning features, including cases specifically developed for noun attachment, refining all the counting methods, thus validating and extending the approach. The current work on automatic binary argument-adjunct classifiers appears to compare favorably to the only other study on this topic (Buchholz 1999). Buchholz (1999) reports an accuracy of 77% for the argument-adjunct distinction of PPs performed wit</context>
</contexts>
<marker>Buchholz, 1999</marker>
<rawString>Buchholz, Sabine. 1999. Distinguishing complements from adjuncts using memory-based learning. ILK, Computational Linguistics, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/∼cjlin/ libsvm.</title>
<date>2001</date>
<contexts>
<context position="57072" citStr="Chang and Lin 2001" startWordPosition="9171" endWordPosition="9174">nostics for PPs attached to a noun, and one binary target feature, indicating the type of attachment. The features implementing the diagnostics for nouns are: the variants of the measures of head dependence (hdepn1, hdepn2, hdepn3), the measures of iterativity (itern), and the measures for copular paraphrase and deverbal noun, respectively (para, deverb). For both types of experiments—distinction of arguments from adjuncts of PPs attached to the noun or PPs attached to the verb—we use the C5.0 Decision Tree Induction Algorithm (Quinlan 1993) and Support Vector Machines (LIBSVM), version 2.71 (Chang and Lin 2001). 4.2 Results on V Attachment Cases We have run experiments on the automatically labeled training and test sets with very many different feature combinations. A summary of the most interesting patterns of results are indicated in Tables 1 and 2. Significance of results is tested using a McNemar test. Contribution of Lexical Items and Their Classes. In this set of experiments we use only lexical features or features that encode the lexical semantic classes of the open class 358 Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment Table 1 Accuracy of the argument–adjunct distincti</context>
<context position="69739" citStr="Chang and Lin 2001" startWordPosition="11184" endWordPosition="11187">erful, however, SVMs are complex algorithms, often opaque in their output. They are harder to interpret than decision trees, where the position of the features in the tree is a clear indication of their relevance for the classification. Finally, SVMs take a longer time to train than decision trees. For these reasons they constitute an interestingly different learning technique from decision trees, but not a substitute for them if clear interpretation of the induced learner is needed and many experiments need to be run. All the experiments reported below were performed with the LIBSVM package (Chang and Lin 2001, version 2.71). The SVM parameters were set by a grid search on the training set, by 10-fold cross-validation. Given the longer training times, we perform only a few experiments. For V attachment, the baseline using only the preposition 7 The features indicated here as best are the ones used in the other experiments and kept for the sake of comparison. But, in fact, a different feature combination found by accident gives a result of 79.3% accuracy in classifying the automatically labeled set. 362 Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment Table 6 Best results using pr</context>
<context position="77134" citStr="Chang and Lin 2001" startWordPosition="12353" endWordPosition="12356">e specifically, the features implementing the diagnostics are the variants of the measures of head dependence for the PPs attached to verbs and nouns, respectively (hdepv1, hdepv2, and hdepn1, hdepn2, hdepn3); the variants of the measure of optionality (opt1, opt2, opt3); the measures of iterativity for verb-attached and noun-attached PPs, respectively (iterv, itern); and finally the measures for copular paraphrase and deverbal noun, respectively (para, deverb). We use the C5.0 Decision Tree Induction Algorithm (Quinlan 1993), and the implementation of SVMs provided in version 2.71 of LIBSVM (Chang and Lin 2001). 5.1 Relationship of Noun–Verb Attachment Disambiguation to the Argument-Adjunct Distinction and Vice Versa Here we report on results for the task of disambiguating noun or verb attachment first, using, among other input features, those that have been established to make the argument-adjunct distinction. The same corpora described above were used, with a two-valued target (N or V). We report results for two sets of experiments. One set of experiments takes all examples into account. In another set of experiments, examples containing the preposition of were not considered, as this preposition </context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chang, Chih-Chung and Chih-Jen Lin. 2001. LIBSVM: A Library for Support Vector Machines. Software available at http://www.csie.ntu.edu.tw/∼cjlin/ libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional phrase attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<contexts>
<context position="2200" citStr="Collins and Brooks 1995" startWordPosition="338" endWordPosition="341">e timeworn example (1) I saw the man with the telescope. It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from * Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN19QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computat</context>
<context position="26755" citStr="Collins and Brooks (1995)" startWordPosition="4216" endWordPosition="4219">hment sites and the most important words in the PP for each instance of PP attachments found in the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data item consists of verb, direct object, and the two following PPs. This corpus is only used to estimate the feature Iterativity. All the data were extracted from the Penn Treebank using the tgrep tools (Marcus, Santorini, and Marcinkiewicz 1993). Our goal is to create a more comprehensive and possibly more accurate corpus than the corpora used by Merlo and Leybold (2001), Merlo, Crocker, and Berthouzoz (1997), and Collins and Brooks (1995), among others. To improve coverage, we extracted all cases of PPs following transitive and intransitive verbs and following nominal phrases. We include passive sentences and sentences containing a sentential object. To improve accuracy, we insured that we did not extract overlapping data, contrary to practice in previous PP corpora construction, where multiple PP sequences were extracted more than once, each time as part of a different structural configuration. For example, in previous corpora, the sequence using crocidolite in filters in 1956, which is a sequence of two PPs, is counted both </context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Collins, Michael and James Brooks. 1995. Prepositional phrase attachment through a backed-off model. In Proceedings of the Third Workshop on Very Large Corpora, pages 27–38,</rawString>
</citation>
<citation valid="false">
<location>Cambridge, MA.</location>
<marker></marker>
<rawString>Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CoNNL</author>
</authors>
<date>2004</date>
<booktitle>Eighth Conference on Computational Natural Language Learning (CoNLL-2004).</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="6169" citStr="CoNNL (2004" startWordPosition="986" endWordPosition="987">nce or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are common across verbs for adjuncts. They inherit these labels from the Penn Treebank annotation.</context>
</contexts>
<marker>CoNNL, 2004</marker>
<rawString>CoNNL. 2004. Eighth Conference on Computational Natural Language Learning (CoNLL-2004). Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2005</date>
<booktitle>Ninth Conference on Computational Natural Language Learning (CoNLL-2005).</booktitle>
<location>Ann Arbor, MI.</location>
<marker>CoNLL, 2005</marker>
<rawString>CoNLL. 2005. Ninth Conference on Computational Natural Language Learning (CoNLL-2005). Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>Large-scale dictionary construction for foreign language tutoring and interlingual machine translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="5894" citStr="Dorr 1997" startWordPosition="942" endWordPosition="943">ase Attachment Modeling both the site and the nature of the attachment of a PP into the tree structure is important. Distinguishing arguments from adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and L</context>
<context position="11228" citStr="Dorr 1997" startWordPosition="1752" endWordPosition="1753">at of the few researchers who have attempted to perform the same distinction. 2. Distinguishing Arguments from Adjuncts Solving the four-way classification task described in the introduction crucially relies on the ability to distinguish arguments from adjuncts, using corpus counts. The ability to automatically make this distinction is necessary for the correct automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, whereas an a</context>
</contexts>
<marker>Dorr, 1997</marker>
<rawString>Dorr, Bonnie. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation, 12(4):1–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ecile Fabre</author>
<author>Didier Bourigault</author>
</authors>
<title>Linguistic clues for corpus-based acquisition of lexical dependencies.</title>
<date>2001</date>
<booktitle>In Proceedings of the Corpus Linguistics Conference,</booktitle>
<pages>176--184</pages>
<location>Lancaster, UK.</location>
<contexts>
<context position="98410" citStr="Fabre and Bourigault (2001)" startWordPosition="15754" endWordPosition="15757">of the occurrences of three verbs, which is the same set inspected to develop the learning features, Villavicencio gets perfect performance. These are very promising results, but because they are not calculated on a previously unseen test set, the generability of the approach is not clear. Moreover, Villavicencio applies only one diagnostic test to determine if a PP is an argument or an adjunct, whereas our extensive validation study has shown that several tests are necessary to reach a reliable judgment. In a study about the automatic acquisition of lexical dependencies for lexicon building, Fabre and Bourigault (2001) discuss the relation of the problem of PP attachment and the notion of argument and adjunct. They correctly notice that approaches such as theirs, inspired by Hindle and Rooth (1993), are based on the assumption that high 371 Computational Linguistics Volume 32, Number 3 co-occurrence between words is an indication of a lexical argumenthood relation. As also noticed in Merlo and Leybold (2001), this is not always the case: some adjuncts frequently co-occur with certain heads too. Fabre and Bourigault propose a notion of productivity that strongly resembles our notions of optionality and head </context>
</contexts>
<marker>Fabre, Bourigault, 2001</marker>
<rawString>Fabre, C´ecile and Didier Bourigault. 2001. Linguistic clues for corpus-based acquisition of lexical dependencies. In Proceedings of the Corpus Linguistics Conference, pages 176–184, Lancaster, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="6054" citStr="Gildea and Jurafsky 2002" startWordPosition="964" endWordPosition="967">m adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They a</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Argument Structure.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11707" citStr="Grimshaw 1990" startWordPosition="1828" endWordPosition="1829">, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, whereas an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, whereas it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw 1990, page 108). These semantic differences give rise to some observable distributional consequences: for a given interpretation, an adj</context>
<context position="21444" citStr="Grimshaw 1990" startWordPosition="3398" endWordPosition="3399">here ≺ indicates linear precedence. para(PP) ≈ P(vcopula ≺ PP) (4) Deverbal Nouns. This diagnostic is based on the observation that PPs following a deverbal noun are likely to be arguments, as the noun shares the argument structure of the verb.2 Proper counting of this feature requires identifying a deverbal noun in the head noun position of a noun phrase. We identify deverbal nouns by inspecting their morphology (Quirk et al. 1985). Specifically, the suffixes that can combine 2 Doubts have been cast on the validity of this diagnostic (Sch¨utze 1995), based on work in theoretical linguistics (Grimshaw 1990). Argaman and Pearlmutter (2002), however, have shown that the argument structures of verbs and related nouns are highly correlated. Hence, we keep deverbal noun as a valid diagnostic here, although we show later that it is not very effective. 347 Computational Linguistics Volume 32, Number 3 with verb bases to form deverbal nouns are listed and exemplified in Figure 1 on page 348. This diagnostic can be captured by a probability indicator function, which assigns probability 1 of being an argument to PPs following a deverbal noun and 0 otherwise. � deverb PP — 1 if deverbal n ≺ PP 5 ( ) 0 othe</context>
</contexts>
<marker>Grimshaw, 1990</marker>
<rawString>Grimshaw, Jane. 1990. Argument Structure. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="2137" citStr="Hindle and Rooth 1993" startWordPosition="329" endWordPosition="332">r interpretation of constituents in the sentence. Consider the timeworn example (1) I saw the man with the telescope. It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from * Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN19QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted fo</context>
<context position="7017" citStr="Hindle and Rooth (1993)" startWordPosition="1109" endWordPosition="1112">ent corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are common across verbs for adjuncts. They inherit these labels from the Penn Treebank annotation. Arguments are annotated instead with labels specific to each verb (Xue 2004; Palmer, Gildea, and Kingsbury 2005). From a quantitative point of view, arguments and adjuncts have different statistical properties. For example, Hindle and Rooth (1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many native speakers’ intuitions. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent preliminary work, however, we have succeeded in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold 2001; Merlo 2003). Ou</context>
<context position="62260" citStr="Hindle and Rooth (1993)" startWordPosition="10002" endWordPosition="10005">ctic-semantic information, encoded indirectly in the diagnostics. All the combinations that are not shown here have lower accuracies. Table 3 shows the confusion matrix for the combination of features with the best accuracy listed above. These figures yield a precision and recall for arguments of 80% and 85%, respectively (F measure = 82%); and a precision and recall for adjuncts of 80% and 73%, respectively (F measure = 76%). Clearly, although both kinds of PPs are well identified, arguments are better identified than adjuncts, an observation already made by several other authors, especially Hindle and Rooth (1993) in their detailed discussion of the errors in a noun or verb PP-attachment task. In particular, we notice that more adjuncts are misclassified as arguments than vice versa. The results of these experiments confirm that corpus information is conducive to learning the distinction under discussion without explicitly represented complex semantic knowledge. They also confirm that this distinction is essentially a word class phenomenon—and not an individual lexical-item phenomenon—as would be expected under current theories of the syntax–semantics interface. Finally, the combination of lexical item</context>
<context position="98593" citStr="Hindle and Rooth (1993)" startWordPosition="15784" endWordPosition="15787">they are not calculated on a previously unseen test set, the generability of the approach is not clear. Moreover, Villavicencio applies only one diagnostic test to determine if a PP is an argument or an adjunct, whereas our extensive validation study has shown that several tests are necessary to reach a reliable judgment. In a study about the automatic acquisition of lexical dependencies for lexicon building, Fabre and Bourigault (2001) discuss the relation of the problem of PP attachment and the notion of argument and adjunct. They correctly notice that approaches such as theirs, inspired by Hindle and Rooth (1993), are based on the assumption that high 371 Computational Linguistics Volume 32, Number 3 co-occurrence between words is an indication of a lexical argumenthood relation. As also noticed in Merlo and Leybold (2001), this is not always the case: some adjuncts frequently co-occur with certain heads too. Fabre and Bourigault propose a notion of productivity that strongly resembles our notions of optionality and head dependence to capture the two intuitions about the distribution of arguments and adjuncts. Arguments are strongly selected by the head (the head to complement relation is not producti</context>
<context position="99920" citStr="Hindle and Rooth (1993)" startWordPosition="15999" endWordPosition="16002">hly productive). They propose, but do not test, the hypothesis that this notion might be useful for the general problem of PP attachment. The results in the current article show that this is not the case. In fact, we have argued that there is no real reason to believe that the two notions should be related, other than marginally. 6.2 On the Distinction of Argument from Adjunct PPs for Subcategorization Acquisition As far as we are aware, this is the first attempt to integrate the notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb complements are the most difficult. We confirm their finding that noun arguments are more easily identified, whereas verb complements (either arguments or adjuncts) are more difficult. Other pieces of work address the current problem in the larger perspective of distinguishing arguments from adjuncts for subcategorization acquisition (Korhonen 2002a; Aldezabal et al. 2002). The goal of the work of Korhonen (2002a, 2002b) is to develop a semantically driven approach to su</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, Donald and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>X&apos; Syntax: A Study of Phrase Structure.</title>
<date>1977</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11655" citStr="Jackendoff 1977" startWordPosition="1820" endWordPosition="1821">h as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, whereas an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, whereas it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw 1990, page 108). These semantic differences give rise to some observable distributio</context>
</contexts>
<marker>Jackendoff, 1977</marker>
<rawString>Jackendoff, Ray. 1977. X&apos; Syntax: A Study of Phrase Structure. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Semantically motivated subcategorization acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon on Unsupervised Lexical Acquisition,</booktitle>
<pages>51--58</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="100395" citStr="Korhonen 2002" startWordPosition="16068" endWordPosition="16069">notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb complements are the most difficult. We confirm their finding that noun arguments are more easily identified, whereas verb complements (either arguments or adjuncts) are more difficult. Other pieces of work address the current problem in the larger perspective of distinguishing arguments from adjuncts for subcategorization acquisition (Korhonen 2002a; Aldezabal et al. 2002). The goal of the work of Korhonen (2002a, 2002b) is to develop a semantically driven approach to subcategorization frame hypothesis selection that can be used to improve large-scale subcategorization frame acquisition. The main idea underlying the approach is to leverage the well-known mapping between syntax and semantics, inspired by Levin’s (1993) work. Korhonen uses statistics over semantic classes of verbs to smooth a distribution of subcategorization frames and then applies a simple frequency cutoff to select the most reliable subcategorization frames. Her work i</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Korhonen, Anna. 2002a. Semantically motivated subcategorization acquisition. In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon on Unsupervised Lexical Acquisition, pages 51–58, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorisation Acquisition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="100395" citStr="Korhonen 2002" startWordPosition="16068" endWordPosition="16069">notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb complements are the most difficult. We confirm their finding that noun arguments are more easily identified, whereas verb complements (either arguments or adjuncts) are more difficult. Other pieces of work address the current problem in the larger perspective of distinguishing arguments from adjuncts for subcategorization acquisition (Korhonen 2002a; Aldezabal et al. 2002). The goal of the work of Korhonen (2002a, 2002b) is to develop a semantically driven approach to subcategorization frame hypothesis selection that can be used to improve large-scale subcategorization frame acquisition. The main idea underlying the approach is to leverage the well-known mapping between syntax and semantics, inspired by Levin’s (1993) work. Korhonen uses statistics over semantic classes of verbs to smooth a distribution of subcategorization frames and then applies a simple frequency cutoff to select the most reliable subcategorization frames. Her work i</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Korhonen, Anna. 2002b. Subcategorisation Acquisition. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="18468" citStr="Levin 1993" startWordPosition="2882" endWordPosition="2883">ause verbs can only assign any given type of role once. Moreover, in English, arguments must be adjacent to the selecting lexical head. Neither of these two restrictions apply to adjuncts, which can be iterated, and follow arguments in a sequence of PPs. Consequently, in a sequence of several PPs 1 Notice that this diagnostic can only be interpreted as a statistical tendency, and not as a strict test, because not all arguments are obligatory (but all adjuncts are indeed optional). The best known descriptive exception to the criterion of optionality is the class of so-called object-drop verbs (Levin 1993). Here a given verb may tolerate the omission of its argument. In other words, a transitive verb, such as kiss, can also act like an intransitive. With respect to optional PPs, it has been argued that instrumentals are arguments (Sch¨utze 1995). While keeping these exceptions in mind, we maintain optionality as a valid diagnostic here. 346 Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment only the first one can be an argument, whereas the others must be adjuncts, as illustrated in the examples below. (13a) ∗Chris rented the gazebo to yuppies, to libertarians. (13b) Kim met Sa</context>
<context position="22693" citStr="Levin (1993)" startWordPosition="3603" endWordPosition="3604">ics of head dependence, optionality, iterativity, ordering, copular paraphrase, and deverbal nominalization are promising indicators of the status of PPs as either arguments or adjuncts. In Section 3 we illustrate how they can be quantified in a faithful way and, thanks to their simplicity, how they can be estimated in a sufficiently large corpus by simple counts. Another class of features is also very important for the distinction between arguments and adjuncts, the lexical semantic class to which the lexical heads belong, as we illustrate below. Lexical Semantic Class Features. According to Levin (1993), there is a regular mapping between the syntactic and semantic behavior of a verb. This gives rise to a lexicon where verbs that share similar syntactic and semantic properties are organized into classes. More specifically, it is assumed that similar underlying components of meaning give rise to similar subcategorization frames and projections of arguments at the syntactic level. Since an argument participates in the subcategorization frame of the verb, whereas an adjunct does not, we expect the argument-taking properties of verbs to be also organized around semantic classes. We expect, there</context>
<context position="59699" citStr="Levin (1993)" startWordPosition="9596" endWordPosition="9597">esult (line 7) in the experiment that uses class combinations (difference from baseline p &lt; .001). We see that classes of open class items, nouns, and verbs associated with the closed class item preposition give the best performance. Line 7 is considerably better than line 6 (p &lt; .001), indicating that the actual presence of individual lexical items is disruptive. This indicates that regularities in the distinction of arguments from adjuncts is indeed a class phenomenon and not an item-specific phenomenon. This is expected, according to current linguistic theories, such as the one proposed by Levin (1993). This analysis is confirmed by observing which are the most discriminative features, those that are closest to the root of the decision tree. The topmost feature of the best result (line 7) is preposition, followed by the class of the verb and the class of the PP internal noun. Predictably, the class of the object noun phrase is not used because it is not very informative. The same features constitute the tree yielding the results of line 6. However, the presence of lexical items makes a difference to the tree that is built, which is much more compact, but in the end less accurate. Combinatio</context>
<context position="94666" citStr="Levin 1993" startWordPosition="15161" endWordPosition="15162">guments, and noun adjuncts, but they are poor for the classification of prepositional phrases that are arguments of the verb, if decision trees are used. Overall performance and especially identification of verb arguments is improved if support vector machines are used. We also show that better accuracy is achieved by performing the four-way classification in one step. The features that appear to be most effective are lexical classes, thus confirming current linguistic theories that postulate that a given head’s argument structure depends on the head’s lexical semantics, especially for verbs (Levin 1993). Table 14 Percent precision, recall, and F-score for the best four-way classification of PPs, including and not including the preposition of using SVMs. One-step + of One-step − of Prec Rec F Prec Rec F V-arg 59.5 47.8 53.0 60.0 52.3 55.9 V-adj 63.2 63.7 63.4 62.8 62.8 62.8 N-arg 83.6 93.4 88.2 69.9 85.3 76.9 N-adj 72.5 50.7 59.7 72.5 50.7 59.7 Accuracy 75.9 66.6 370 Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment 6. Related Work The resolution of the attachment of ambiguous PPs is one of the staple problems in computational linguistics. It serves as a good testing ground </context>
<context position="107137" citStr="Levin 1993" startWordPosition="17108" endWordPosition="17109">uncts of a noun, and as arguments or adjuncts of a verb. We show that the classification performances are reasonably good for verb adjuncts, noun arguments, and noun adjuncts, independent of the learning algorithm. Classification performances of prepositional phrases that are arguments of the verb are poor if decision trees are used, but are greatly improved by the use of a large margin classifier. The features that appear to be most effective are lexical classes, thus confirming current linguistic theories that postulate that a verb’s argument structure depends on a verb’s lexical semantics (Levin 1993). Future work lies in further investigating the difference between arguments and adjuncts to achieve even finer-grained classifications and to model more precisely the semantic core of a sentence. 1. Appendix: PP Configurations Sequence of single PP attached to a verb Configuration Structure Example Transitive [vp V NP PP] join board as director Passive [vp NP PP] tracked (yield) by report Sentential Object [vp V NP PP] continued (to slide) amid signs Intransitive [vp V PP] talking about years Sequence of single PP attached to a noun Noun phrase [np NP PP] form of asbestos Transitive [vp V [np</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, Beth. 1993. English Verb Classes and Alternations. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>Probabilistic syntax.</title>
<date>2003</date>
<pages>289--314</pages>
<editor>In Rens Bod, Jennifer Hay, and Stephanie Jannedy, editors, Probabilistic Linguistics.</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="103678" citStr="Manning (2003)" startWordPosition="16566" endWordPosition="16567">sing corpus-based indicators of their properties. It remains to be determined if we just learn correlates of a unified notion, or if the distinction between arguments and adjuncts is a clustering of possibly unrelated properties. As explained in the introduction, native speakers’ judgments on the argument and adjunct status of PPs are very unstable. No explanation is usually proposed of the fact that the tests of argumenthood are often difficult to judge or even contradict each other. As a possible explanation for the difficulty in pinpointing exactly the properties of arguments and adjuncts, Manning (2003) suggests that the notion of argument or adjunct is not categorical. The different properties of argument and adjuncts are not the reflex of a single grammatical underlying notion, but they can be ascribed to different mechanisms. What appears as a not entirely unified behavior is in fact better explained as separate properties. The current article provides a representation that can support both the categorical and the gradient approach to the distinction between arguments and adjuncts. We have decomposed the notion of argument into a vector of features. The notion of argumenthood is no longer</context>
</contexts>
<marker>Manning, 2003</marker>
<rawString>Manning, Christopher. 2003. Probabilistic syntax. In Rens Bod, Jennifer Hay, and Stephanie Jannedy, editors, Probabilistic Linguistics. MIT Press, pages 289–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Marantz</author>
</authors>
<title>On the Nature of Grammatical Relations.</title>
<date>1984</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11669" citStr="Marantz 1984" startWordPosition="1822" endWordPosition="1823">ation frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, whereas an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, whereas it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw 1990, page 108). These semantic differences give rise to some observable distributional consequenc</context>
</contexts>
<marker>Marantz, 1984</marker>
<rawString>Marantz, Alex. 1984. On the Nature of Grammatical Relations. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>A Marcinkiewicz</author>
<author>R Macintyre</author>
<author>A Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger 1994</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>114--119</pages>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="41769" citStr="Marcus et al. 1994" startWordPosition="6721" endWordPosition="6724">hod, we need to label each example with a target attribute. Deciding whether an example is an instance of an argument or of an adjunct requires making a distinction that the Penn Treebank annotators did not intend to make. The automatic annotation of this attribute therefore must rely on the existing labels for the PP that have been given by the Penn Treebank annotators, inferring from them information that was not explicitly marked. We discuss here the motivation for our interpretation. The PTB annotators found that consistent annotation of argument status and semantic role was not possible (Marcus et al. 1994). The solution adopted, then, was to structurally distinguish arguments from adjuncts only when the distinction was straightforward and to label only some clearly distinguishable semantic roles. Doubtful cases were left untagged. In the Penn Treebank structural distinctions concerning arguments and adjuncts have been oversimplified: All constituents attached to VP are structurally treated as arguments, whereas all constituents attached to NP are treated as adjuncts. The only exception are the arguments of some deverbal nouns, which are represented as arguments. Information about the distinctio</context>
<context position="44086" citStr="Marcus et al. 1994" startWordPosition="7080" endWordPosition="7083">gument in Prepositional Phrase Attachment -CLR dative object if dative shift not possible (e.g., donate); phrasal verbs; predication adjuncts -DTV dative object if dative shift possible (e.g., give) -BNF benefactive (dative object of for) -PRD non VP predicates -PUT locative complement of put -DIR direction and trajectory -LOC location -MNR manner -PRP purpose and reason -TMP temporal phrases Figure 2 Grammatical function and semantic tags that involve PP constituents in the Penn Treebank. in which it is stated that “NPs and Ss which are clearly arguments of the verb are unmarked by any tag” (Marcus et al. 1994, page 4), and that “Direct Object NPs and Indirect Object NPs are all untagged” (Bies et al. 1995, page 12). Although the case of PP constituents is not specifically addressed, we have interpreted these statements as supporting evidence for our choice. The tag -CLR stands for “closely related,” and its meaning varies, depending on the element it is attached to. It indicates argument status when it labels the dative object of ditransitive verbs that cannot undergo dative shift, such as in donate money to the museum, and in phrasal verbs, such as pay for the horse. It indicates adjunct status w</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, Macintyre, Bies, Ferguson, Katz, 1994, </marker>
<rawString>Marcus, M., G. Kim, A. Marcinkiewicz, R. Macintyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger.1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the ARPA Workshop on Human Language Technology, pages 114–119, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann</author>
</authors>
<title>Marcinkiewicz.1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date></date>
<marker>Marcus, Santorini, Ann, </marker>
<rawString>Marcus, Mitch, Beatrice Santorini, and Mary Ann Marcinkiewicz.1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
</authors>
<title>Generalised PP-attachment disambiguation using corpus-based linguistic diagnostics.</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<volume>32</volume>
<contexts>
<context position="7613" citStr="Merlo 2003" startWordPosition="1197" endWordPosition="1198">nd Rooth (1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many native speakers’ intuitions. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent preliminary work, however, we have succeeded in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold 2001; Merlo 2003). Our method develops corpus-based statistical correlates for the diagnostics used in linguistics to decide whether a PP is an argument or an adjunct. A numerical vectorial representation of the notion of argumenthood is provided, which supports automatic classification. In the current article, we expand and improve on this work, by developing new measures and refining the previous ones. We also extend that work to attachment to nouns. This extension enables us to explore in what way the distinction between argument and adjunct is best integrated in the traditional attachment disambiguation pr</context>
<context position="84818" citStr="Merlo 2003" startWordPosition="13591" endWordPosition="13592">can construct a single four-class classifier or we can build a sequence of binary classifiers. The discrimination between noun and verb attachment can be performed first, and then further refined into attachment as argument or adjunct, performing the four-way classification in two steps. The two-step approach would be the natural way of extending current PP attachment disambiguation methods to the more specific four-way attachment we propose here. However, based on the previous experiments, which showed a limited amount of dependence between the two tasks, previous work on a similar data set (Merlo 2003), and general wisdom in machine learning, there is reason to believe that it is better to solve the four-way classification problem directly rather than first solving a more general problem and then specializing the classification. To test these expectations, we performed both kinds of experiments—a direct fourway classification experiment and a two-step classification experiment—to investigate which of the two methods is better. The direct four-way classification uses the attributes described above to build a single classifier. For comparability, we created a two-step experimental setup as fo</context>
</contexts>
<marker>Merlo, 2003</marker>
<rawString>Merlo, Paola. 2003. Generalised PP-attachment disambiguation using corpus-based linguistic diagnostics. Computational Linguistics Volume 32, Number 3</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics (EACL’03),</booktitle>
<pages>251--258</pages>
<location>Budapest, Hungary.</location>
<marker></marker>
<rawString>In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics (EACL’03), pages 251–258, Budapest, Hungary.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Paola Merlo</author>
<author>Matt Crocker</author>
<author>Cathy Berthouzoz 1997</author>
</authors>
<title>Attaching multiple prepositional phrases: Generalized backed-off estimation.</title>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>145--154</pages>
<location>Providence, RI.</location>
<marker>Merlo, Crocker, 1997, </marker>
<rawString>Merlo, Paola, Matt Crocker, and Cathy Berthouzoz.1997. Attaching multiple prepositional phrases: Generalized backed-off estimation. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 145–154, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Matthias Leybold</author>
</authors>
<title>Automatic distinction of arguments and modifiers: The case of prepositional phrases.</title>
<date>2001</date>
<booktitle>In Proceedings of the Fifth Computational Natural Language Learning Workshop (CoNLL-2001),</booktitle>
<pages>121--128</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="7600" citStr="Merlo and Leybold 2001" startWordPosition="1193" endWordPosition="1196">s. For example, Hindle and Rooth (1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many native speakers’ intuitions. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent preliminary work, however, we have succeeded in distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold 2001; Merlo 2003). Our method develops corpus-based statistical correlates for the diagnostics used in linguistics to decide whether a PP is an argument or an adjunct. A numerical vectorial representation of the notion of argumenthood is provided, which supports automatic classification. In the current article, we expand and improve on this work, by developing new measures and refining the previous ones. We also extend that work to attachment to nouns. This extension enables us to explore in what way the distinction between argument and adjunct is best integrated in the traditional attachment disa</context>
<context position="26685" citStr="Merlo and Leybold (2001)" startWordPosition="4206" endWordPosition="4209"> preposition, and PP internal noun) indicating the two possible attachment sites and the most important words in the PP for each instance of PP attachments found in the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data item consists of verb, direct object, and the two following PPs. This corpus is only used to estimate the feature Iterativity. All the data were extracted from the Penn Treebank using the tgrep tools (Marcus, Santorini, and Marcinkiewicz 1993). Our goal is to create a more comprehensive and possibly more accurate corpus than the corpora used by Merlo and Leybold (2001), Merlo, Crocker, and Berthouzoz (1997), and Collins and Brooks (1995), among others. To improve coverage, we extracted all cases of PPs following transitive and intransitive verbs and following nominal phrases. We include passive sentences and sentences containing a sentential object. To improve accuracy, we insured that we did not extract overlapping data, contrary to practice in previous PP corpora construction, where multiple PP sequences were extracted more than once, each time as part of a different structural configuration. For example, in previous corpora, the sequence using crocidolit</context>
<context position="33644" citStr="Merlo and Leybold 2001" startWordPosition="5380" endWordPosition="5383">rdNet class of h and n2, respectively. Let C(h, p, n2) be the frequency with which p, n2 co-occurs as a prepositional phrase with h. Let C(h) be the frequency of h. Head Dependence. Head dependence of a PP on a head is approximated by estimating the dispersion of the PP over the possible heads. In a previous attempt to capture this notion, we approximated by simply measuring the cardinality of the set of heads that co-occur with a given PP in a corpus, as indicated in equation (6). The expectation was that a low number indicated argument status, whereas a high number indicated adjunct status (Merlo and Leybold 2001). hdep(h, p, n2) _ |{h1, h2, h3, ... , hn}p,n2 |(6) By measuring the cardinality of the set of heads, we approximate the dispersion of the distribution of heads by its range. This is a very rough approximation, as the range of a distribution does not give any information on the distribution’s shape. The range of a distribution might have the same value for a uniform distribution or a very skewed distribution. Intuitively, we would like a measure that tells us that the former corresponds to a verb with a much lower head dependence than the latter. Entropy is 4 The automatic annotation of nouns </context>
<context position="96063" citStr="Merlo and Leybold 2001" startWordPosition="15395" endWordPosition="15398">f relatively little value in a real application. It would be more useful to know where the PP is attached and with what function. We review below the few pieces of work that have tackled the problem of labeling PPs by function (arguments or adjuncts) as a separate labeling problem. Other pieces of work have asked a similar question in the context of acquiring high-precision subcategorization frames. We review a few of them below. 6.1 On the Automatic Distinction of Arguments and Adjuncts A few other pieces of work attempt to distinguish PP arguments from adjuncts automatically (Buchholz 1999; Merlo and Leybold 2001; Villavicencio 2002). We extend and modify here the preliminary work reported in Merlo and Leybold (2001) by extending the method to noun attachment, elaborating more learning features, including cases specifically developed for noun attachment, refining all the counting methods, thus validating and extending the approach. The current work on automatic binary argument-adjunct classifiers appears to compare favorably to the only other study on this topic (Buchholz 1999). Buchholz (1999) reports an accuracy of 77% for the argument-adjunct distinction of PPs performed with a memory-based learnin</context>
<context position="98807" citStr="Merlo and Leybold (2001)" startWordPosition="15817" endWordPosition="15820">hereas our extensive validation study has shown that several tests are necessary to reach a reliable judgment. In a study about the automatic acquisition of lexical dependencies for lexicon building, Fabre and Bourigault (2001) discuss the relation of the problem of PP attachment and the notion of argument and adjunct. They correctly notice that approaches such as theirs, inspired by Hindle and Rooth (1993), are based on the assumption that high 371 Computational Linguistics Volume 32, Number 3 co-occurrence between words is an indication of a lexical argumenthood relation. As also noticed in Merlo and Leybold (2001), this is not always the case: some adjuncts frequently co-occur with certain heads too. Fabre and Bourigault propose a notion of productivity that strongly resembles our notions of optionality and head dependence to capture the two intuitions about the distribution of arguments and adjuncts. Arguments are strongly selected by the head (the head to complement relation is not productive), whereas adjuncts can be selected by a wide spread of heads (the complement to head selection is highly productive). They propose, but do not test, the hypothesis that this notion might be useful for the genera</context>
</contexts>
<marker>Merlo, Leybold, 2001</marker>
<rawString>Merlo, Paola and Matthias Leybold. 2001. Automatic distinction of arguments and modifiers: The case of prepositional phrases. In Proceedings of the Fifth Computational Natural Language Learning Workshop (CoNLL-2001), pages 121–128, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatic verb classification based on statistical distributions of argument structure.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="101223" citStr="Merlo and Stevenson (2001)" startWordPosition="16186" endWordPosition="16189">scale subcategorization frame acquisition. The main idea underlying the approach is to leverage the well-known mapping between syntax and semantics, inspired by Levin’s (1993) work. Korhonen uses statistics over semantic classes of verbs to smooth a distribution of subcategorization frames and then applies a simple frequency cutoff to select the most reliable subcategorization frames. Her work is related to ours in several ways. First, the automatic acquisition task leverages correspondences between syntax and semantics, particularly clear in the organization of the verb lexicon, similarly to Merlo and Stevenson (2001). Some of our current results are also based on this correspondence, as we assume that the notion of argument is a notion at the interface of the syntactic and semantic levels, and participates in both, determining not only the valency of a verb but also its subcategorization frame. Our work confirms the results reported in Korhonen (2002a), which indicate that using word classes improves the extraction of subcategorization frames. Differently from Korhonen, however, we do not allow feedback between levels. In her work, syntactic similarity of verbs’ subcategorization sets based on an external</context>
</contexts>
<marker>Merlo, Stevenson, 2001</marker>
<rawString>Merlo, Paola and Suzanne Stevenson. 2001. Automatic verb classification based on statistical distributions of argument structure. Computational Linguistics, 27(3):373–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Five papers on Wordnet.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>Cognitive Science Laboratory, Princeton University.</institution>
<contexts>
<context position="32317" citStr="Miller et al. 1990" startWordPosition="5159" endWordPosition="5162"> features below. Often several ways of estimating the features have been implemented, mostly to address sparseness of data. Lexical Word Classes. As indicated in the previous section, the head word of the governor of the PP, the noun or the verb, is very directly related to the status of the PP. For example, all giving verbs take a PP introduced by to, which is an argument of the verb. The lexical semantic class of the head words is therefore going to be very relevant to the disambiguation task. The semantic grouping has been done automatically, using the lexicographic classes in WordNet 1.7 (Miller et al. 1990). Nouns are classified in different classes, among which, for example, are animal, artifact, attribute, body, cognition, communication, event, feeling, food, location, motive, person, plant, process, quantity, relation, shape, and substance. This classification required selecting the most frequent WordNet sense for those polysemous nouns being classified and generalizing to its class.4 For all the features below and where appropriate, we assume the following notation. Let h be the head, that is, the verb in the features related to verb attachments and the noun in the features related to noun a</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, George, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Five papers on Wordnet. Technical report, Cognitive Science Laboratory, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney Nielsen</author>
<author>Sameer Pradhan</author>
</authors>
<title>Mixing weak learners in semantic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2004),</booktitle>
<pages>80--87</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6080" citStr="Nielsen and Pradhan 2004" startWordPosition="968" endWordPosition="971">ifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are commo</context>
</contexts>
<marker>Nielsen, Pradhan, 2004</marker>
<rawString>Nielsen, Rodney and Sameer Pradhan. 2004. Mixing weak learners in semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2004), pages 80–87, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--71</pages>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Phillips</author>
<author>Ellen Riloff</author>
</authors>
<title>Exploiting strong syntactic heuristics and co-training to learn semantic lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>125--132</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5921" citStr="Phillips and Riloff 2002" startWordPosition="944" endWordPosition="947">ent Modeling both the site and the nature of the attachment of a PP into the tree structure is important. Distinguishing arguments from adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers o</context>
</contexts>
<marker>Phillips, Riloff, 2002</marker>
<rawString>Phillips, William and Ellen Riloff. 2002. Exploiting strong syntactic heuristics and co-training to learn semantic lexicons. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 125–132, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>An Information-based Syntax and Semantics, volume 13. CSLI Lecture Notes,</title>
<date>1987</date>
<institution>Stanford University.</institution>
<contexts>
<context position="11691" citStr="Pollard and Sag 1987" startWordPosition="1824" endWordPosition="1827">nd argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, whereas an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, whereas it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw 1990, page 108). These semantic differences give rise to some observable distributional consequences: for a given interp</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl and Ivan Sag. 1987. An Information-based Syntax and Semantics, volume 13. CSLI Lecture Notes, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<date>1993</date>
<booktitle>C4.5 : Programs for Machine Learning. Series in Machine Learning.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="57000" citStr="Quinlan 1993" startWordPosition="9162" endWordPosition="9163">2cl), all the different variants of the implementation of the diagnostics for PPs attached to a noun, and one binary target feature, indicating the type of attachment. The features implementing the diagnostics for nouns are: the variants of the measures of head dependence (hdepn1, hdepn2, hdepn3), the measures of iterativity (itern), and the measures for copular paraphrase and deverbal noun, respectively (para, deverb). For both types of experiments—distinction of arguments from adjuncts of PPs attached to the noun or PPs attached to the verb—we use the C5.0 Decision Tree Induction Algorithm (Quinlan 1993) and Support Vector Machines (LIBSVM), version 2.71 (Chang and Lin 2001). 4.2 Results on V Attachment Cases We have run experiments on the automatically labeled training and test sets with very many different feature combinations. A summary of the most interesting patterns of results are indicated in Tables 1 and 2. Significance of results is tested using a McNemar test. Contribution of Lexical Items and Their Classes. In this set of experiments we use only lexical features or features that encode the lexical semantic classes of the open class 358 Merlo and Esteve Ferrer Argument in Prepositio</context>
<context position="77046" citStr="Quinlan 1993" startWordPosition="12339" endWordPosition="12340">ed target feature (Narg, Nadj, Varg, Vadj), indicating the type of attachment. More specifically, the features implementing the diagnostics are the variants of the measures of head dependence for the PPs attached to verbs and nouns, respectively (hdepv1, hdepv2, and hdepn1, hdepn2, hdepn3); the variants of the measure of optionality (opt1, opt2, opt3); the measures of iterativity for verb-attached and noun-attached PPs, respectively (iterv, itern); and finally the measures for copular paraphrase and deverbal noun, respectively (para, deverb). We use the C5.0 Decision Tree Induction Algorithm (Quinlan 1993), and the implementation of SVMs provided in version 2.71 of LIBSVM (Chang and Lin 2001). 5.1 Relationship of Noun–Verb Attachment Disambiguation to the Argument-Adjunct Distinction and Vice Versa Here we report on results for the task of disambiguating noun or verb attachment first, using, among other input features, those that have been established to make the argument-adjunct distinction. The same corpora described above were used, with a two-valued target (N or V). We report results for two sets of experiments. One set of experiments takes all examples into account. In another set of exper</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J. Ross. 1993. C4.5 : Programs for Machine Learning. Series in Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<location>Longman, London.</location>
<contexts>
<context position="20797" citStr="Quirk et al. 1985" startWordPosition="3290" endWordPosition="3293">ow c. a member of Parliament *a member who was of Parliament This is because a PP attached to a noun as an argument does not predicate a secondary property of the noun, but it specifies the same property that is indicated by the head. To be able to use the relative clause construction, there must be two properties that are being predicated of the same entity. Thus, the probability that a PP is able to be paraphrased, and therefore that it is an adjunct, can be approximated by the probability of its occurrence following a construction headed by a copular verb, be, become, appear, seem, remain (Quirk et al. 1985), as indicated in equation (4), where ≺ indicates linear precedence. para(PP) ≈ P(vcopula ≺ PP) (4) Deverbal Nouns. This diagnostic is based on the observation that PPs following a deverbal noun are likely to be arguments, as the noun shares the argument structure of the verb.2 Proper counting of this feature requires identifying a deverbal noun in the head noun position of a noun phrase. We identify deverbal nouns by inspecting their morphology (Quirk et al. 1985). Specifically, the suffixes that can combine 2 Doubts have been cast on the validity of this diagnostic (Sch¨utze 1995), based on </context>
<context position="39360" citStr="Quirk et al. 1985" startWordPosition="6310" endWordPosition="6313">, and Cany(h, p, n2) be the frequency with which it occurs with h in any position.5 Then: C2nd(h, p, n2) if Cany(h, p, n2) # 0, or else Cany(h, p, n2) C2nd(h, p, n2cl) if Cany(h, p, n2cl) = 0, or else iter(h,p,n2) ≈ { Cany(h, p, n2cl) , C2nd(hcl, p, n2cl) Cany(hcl, p, n2cl) (11) Copular Paraphrase. Copular paraphrase is captured by calculating the proportion of times a given PP is found in a copular paraphrase. We approximate this diagnostic by making the hypothesis that a PP following a nominal head is an adjunct if it is also found following a copular verb, be, become, appear, seem, remain (Quirk et al. 1985). We calculate then the proportion of times a given PP follows a copular verb over the times it appears following any verb. This count is an approximation because even when we find a copular verb, it might not be part of a relative clause. Here again, we back off to the noun classes of the PP-internal noun to address the problem of sparse data. para(h,p,n2) ≈ I C(vcopula ≺ (p, n2)) (12) ΣiC(vi ≺ (p, n2)), if C(vcopula) =� 0, or else C(vcopula ≺ (p,n2cl)) ΣiC(vi ≺ (p, n2cl)) 5 Note that we approximate the prepositions occurring in any position by looking only at the first two prepositions attac</context>
<context position="44755" citStr="Quirk et al. (1985)" startWordPosition="7193" endWordPosition="7196">t Object NPs are all untagged” (Bies et al. 1995, page 12). Although the case of PP constituents is not specifically addressed, we have interpreted these statements as supporting evidence for our choice. The tag -CLR stands for “closely related,” and its meaning varies, depending on the element it is attached to. It indicates argument status when it labels the dative object of ditransitive verbs that cannot undergo dative shift, such as in donate money to the museum, and in phrasal verbs, such as pay for the horse. It indicates adjunct status when it labels a predication adjunct as defined by Quirk et al. (1985). We interpret the tag -CLR as an argument tag in order not to lose the few cases for which the differentiation is certain: the ditransitive verbs and some phrasal verbs. This choice apparently misclassifies predication adjuncts as arguments. However, for some cases, such as obligatory predication adjuncts, an argument status might in fact be more appropriate than an adjunct status. According to Quirk et al. (1985, Sections 8.27–35, 15.22, pages 16–48), there are three types of adjuncts, differentiated by the degree of ”centrality” they have in the sentence. They can be classified into predica</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Quirk, Randolph, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<location>Providence, RI.</location>
<contexts>
<context position="2279" citStr="Ratnaparkhi 1997" startWordPosition="351" endWordPosition="352">if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from * Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN19QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 PP adjuncts. Co</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Ratnaparkhi, Adwait. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 1–10, Providence, RI.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeffrey Reynar</author>
<author>Salim Roukos 1994</author>
</authors>
<title>A maximum entropy model for prepositional phrase attachment.</title>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>250--255</pages>
<location>Plainsboro, NJ.</location>
<marker>Ratnaparkhi, Reynar, 1994, </marker>
<rawString>Ratnaparkhi, Adwait, Jeffrey Reynar, and Salim Roukos.1994. A maximum entropy model for prepositional phrase attachment. In Proceedings of the ARPA Workshop on Human Language Technology, pages 250–255, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Mark Schmelzenbach</author>
</authors>
<title>An empirical approach to conceptual case frame acquisition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>49--56</pages>
<location>Montreal.</location>
<contexts>
<context position="11260" citStr="Riloff and Schmelzenbach 1998" startWordPosition="1754" endWordPosition="1757">ew researchers who have attempted to perform the same distinction. 2. Distinguishing Arguments from Adjuncts Solving the four-way classification task described in the introduction crucially relies on the ability to distinguish arguments from adjuncts, using corpus counts. The ability to automatically make this distinction is necessary for the correct automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, whereas an adjunct predicates a separate pro</context>
</contexts>
<marker>Riloff, Schmelzenbach, 1998</marker>
<rawString>Riloff, Ellen and Mark Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 49–56, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carson T Sch¨utze</author>
</authors>
<date>1995</date>
<booktitle>PP Attachment and Argumenthood. MIT Working Papers in Linguistics,</booktitle>
<pages>26--95</pages>
<marker>Sch¨utze, 1995</marker>
<rawString>Sch¨utze, Carson T. 1995. PP Attachment and Argumenthood. MIT Working Papers in Linguistics, 26:95–151.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>SENSEVAL-3.2004. Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3).</booktitle>
<location>Barcelona,</location>
<marker></marker>
<rawString>SENSEVAL-3.2004. Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3). Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bangalore Srinivas</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="5883" citStr="Srinivas and Joshi 1999" startWordPosition="938" endWordPosition="941">ment in Prepositional Phrase Attachment Modeling both the site and the nature of the attachment of a PP into the tree structure is important. Distinguishing arguments from adjuncts is key to identifying the elements that belong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fill</context>
<context position="11205" citStr="Srinivas and Joshi 1999" startWordPosition="1746" endWordPosition="1749">f argument and compare our work to that of the few researchers who have attempted to perform the same distinction. 2. Distinguishing Arguments from Adjuncts Solving the four-way classification task described in the introduction crucially relies on the ability to distinguish arguments from adjuncts, using corpus counts. The ability to automatically make this distinction is necessary for the correct automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associ</context>
</contexts>
<marker>Srinivas, Joshi, 1999</marker>
<rawString>Srinivas, Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>A generative perspective on verb alternations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="11217" citStr="Stede 1998" startWordPosition="1750" endWordPosition="1751">r work to that of the few researchers who have attempted to perform the same distinction. 2. Distinguishing Arguments from Adjuncts Solving the four-way classification task described in the introduction crucially relies on the ability to distinguish arguments from adjuncts, using corpus counts. The ability to automatically make this distinction is necessary for the correct automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in parsing, generation, machine translation, and information extraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach 1998). Yet, few attempts have been made to make this distinction automatically. The core difficulty in this enterprise is to define the notion of argument precisely enough that it can be used automatically. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard and Sag 1987; Grimshaw 1990). With respect to their function, an argument fills a role in the relation described by its associated head, w</context>
</contexts>
<marker>Stede, 1998</marker>
<rawString>Stede, Manfred. 1998. A generative perspective on verb alternations. Computational Linguistics, 24(3):401–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
<author>Makoto Nagao</author>
</authors>
<title>Corpus based PP attachment ambiguity resolution with a semantic dictionary.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Workshop on Very Large Corpora,</booktitle>
<pages>66--80</pages>
<location>Beijing/ Hong Kong.</location>
<contexts>
<context position="2261" citStr="Stetina and Nagao 1997" startWordPosition="347" endWordPosition="350"> important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from * Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN19QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number</context>
<context position="34546" citStr="Stetina and Nagao (1997" startWordPosition="5535" endWordPosition="5538">distribution’s shape. The range of a distribution might have the same value for a uniform distribution or a very skewed distribution. Intuitively, we would like a measure that tells us that the former corresponds to a verb with a much lower head dependence than the latter. Entropy is 4 The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files. Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet. The changes made were inspired by those described in Stetina and Nagao (1997, page 75). To lemmatize the words we used “morpha,” a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html. Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task. 351 Computational Linguistics Volume 32, Number 3 a more informative measure of the dispersion of a distribution, which depends both on the range and on the shape of a distribution. The head dependence measure based on entropy, then, is calculated as indicated in equation (7), which calcu</context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Stetina, Jiri and Makoto Nagao. 1997. Corpus based PP attachment ambiguity resolution with a semantic dictionary. In Proceedings of the Fifth Workshop on Very Large Corpora, pages 66–80, Beijing/ Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Exploiting a verb lexicon in automatic semantic role labelling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-05),</booktitle>
<pages>883--890</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="6128" citStr="Swier and Stevenson 2005" startWordPosition="976" endWordPosition="979"> kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are common across verbs for adjuncts. They inherit these </context>
</contexts>
<marker>Swier, Stevenson, 2005</marker>
<rawString>Swier, Robert and Suzanne Stevenson. 2005. Exploiting a verb lexicon in automatic semantic role labelling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-05), pages 883–890, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="68753" citStr="Vapnik 1995" startWordPosition="11025" endWordPosition="11026"> useful information. 4.4 Results Using a Different Learning Algorithm In the previous sections, we have shown that different combinations of features yield significantly different performances. We would like to investigate, at least on a first approximation, if these results hold across different learning algorithms. To test the stability of the results, we compare the main performances obtained with a decision tree to those obtained with a different learning algorithm. In recent years, a lot of attention has been paid to large margin classifiers, in particular support vector machines (SVMs) (Vapnik 1995). They have been shown to perform quite well in many NLP tasks. The fact that they search for a large separating margin between classes makes them less prone to overfitting. We expect them, then, to perform well on the task trained on automatically labeled data and tested on manually labeled data, where a great ability to generalize is needed. Despite being very powerful, however, SVMs are complex algorithms, often opaque in their output. They are harder to interpret than decision trees, where the position of the features in the tree is a clear indication of their relevance for the classificat</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, V. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>Learning to distinguish PP arguments from adjuncts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>84--90</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="96084" citStr="Villavicencio 2002" startWordPosition="15399" endWordPosition="15400">e in a real application. It would be more useful to know where the PP is attached and with what function. We review below the few pieces of work that have tackled the problem of labeling PPs by function (arguments or adjuncts) as a separate labeling problem. Other pieces of work have asked a similar question in the context of acquiring high-precision subcategorization frames. We review a few of them below. 6.1 On the Automatic Distinction of Arguments and Adjuncts A few other pieces of work attempt to distinguish PP arguments from adjuncts automatically (Buchholz 1999; Merlo and Leybold 2001; Villavicencio 2002). We extend and modify here the preliminary work reported in Merlo and Leybold (2001) by extending the method to noun attachment, elaborating more learning features, including cases specifically developed for noun attachment, refining all the counting methods, thus validating and extending the approach. The current work on automatic binary argument-adjunct classifiers appears to compare favorably to the only other study on this topic (Buchholz 1999). Buchholz (1999) reports an accuracy of 77% for the argument-adjunct distinction of PPs performed with a memory-based learning approach, to be com</context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Villavicencio, Aline. 2002. Learning to distinguish PP arguments from adjuncts. In Proceedings of the 6th Conference on Natural Language Learning (CoNLL-2002), pages 84–90, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Handling dislocated and discontinuous constituents in Chinese semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth Workshop on Asian Language Resources (ALR04),</booktitle>
<pages>19--26</pages>
<location>Hainan Island, China.</location>
<contexts>
<context position="6845" citStr="Xue 2004" startWordPosition="1087" endWordPosition="1088">adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are common across verbs for adjuncts. They inherit these labels from the Penn Treebank annotation. Arguments are annotated instead with labels specific to each verb (Xue 2004; Palmer, Gildea, and Kingsbury 2005). From a quantitative point of view, arguments and adjuncts have different statistical properties. For example, Hindle and Rooth (1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many native speakers’ intuitions. The usual expectation has been that this discrimination is not amenable to a corpus-base</context>
</contexts>
<marker>Xue, 2004</marker>
<rawString>Xue, Nianwen. 2004. Handling dislocated and discontinuous constituents in Chinese semantic role labelling. In Proceedings of the Fourth Workshop on Asian Language Resources (ALR04), pages 19–26, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004),</booktitle>
<pages>88--94</pages>
<location>Barcelona,</location>
<contexts>
<context position="6101" citStr="Xue and Palmer 2004" startWordPosition="972" endWordPosition="975">elong to the semantic kernel of a sentence. Extracting the kernel of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999; Dorr 1997; Phillips and Riloff 2002). This task is fundamentally syntactic in nature and complements the task of assigning thematic role labels (Gildea and Jurafsky 2002; Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005). See also the common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004). Both a distinction of arguments from adjuncts and an appropriate thematic labeling of the complements of a predicate, verb, or noun are necessary, as confirmed by the annotations adopted by current corpora. Framenet makes a distinction between complements and satellites (Baker, Fillmore, and Lowe 1998). The developers of PropBank integrate the difference between arguments and adjuncts directly into the level of specificity of their annotation. They adopt labels that are common across verbs for ad</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Xue, Nianwen and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004), pages 88–94, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference in Computational</booktitle>
<contexts>
<context position="86683" citStr="Yeh (2000)" startWordPosition="13867" endWordPosition="13868">nt-adjunct classifier, and the test examples classified as nouns are given to the noun argument-adjunct classifier. Thus, this cascade of classifiers performs the same task as the four-way classifier, but it does so in two passes. Table 11 shows that overall the one-step classification is better than the two-step classification, confirming the intuition that the two labeling problems should be solved at the same time.8 However, if we break down the performance, we see that recall of 8 The difference between the two results is significant (p &lt; .05) according to the randomized test described in Yeh (2000). 367 Computational Linguistics Volume 32, Number 3 Table 11 Percent precision, recall, and F score for the best two-step and one-step four-way classification of PPs, including and not including the preposition of. Two-step + of One-step + of Prec Rec F Prec Rec F V-arg 37.5 45.6 41.2 42.2 29.3 34.6 V-adj 56.2 52.2 54.1 59.6 60.2 59.9 N-arg 83.0 83.5 83.2 81.3 91.3 86.0 N-adj 71.2 57.5 63.6 69.5 56.2 62.1 Accuracy 68.9 72.0 Two-step − of One-step − of Prec Rec F Prec Rec F V-arg 41.3 50.0 45.3 42.2 31.4 36.0 V-adj 52.8 41.6 46.5 59.6 60.2 59.9 N-arg 67.3 70.0 68.6 65.4 80.7 71.9 N-adj 60.3 60.</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Yeh, Alexander. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th International Conference in Computational</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistics</author>
</authors>
<date>2000</date>
<pages>947--953</pages>
<location>Saarbruecken, Germany.</location>
<marker>Linguistics, 2000</marker>
<rawString>Linguistics (COLING 2000), pages 947–953, Saarbruecken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaojun Zhao</author>
<author>Dekang Lin</author>
</authors>
<title>A nearest-neighbor method for resolving PP-attachment ambiguities.</title>
<date>2004</date>
<booktitle>In The First International Joint Conference on Natural Language Processing (IJCNLP-04),</booktitle>
<pages>545--554</pages>
<location>Hainan Island, China.</location>
<contexts>
<context position="2299" citStr="Zhao and Lin 2004" startWordPosition="353" endWordPosition="356"> telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalized the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb’s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetina and Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from * Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gen`eve 4, Switzerland. † Department of Informatics, University of Sussex, Falmer, Brighton BN19QH, UK. Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication: 4 November 2005. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 PP adjuncts. Consider the following</context>
</contexts>
<marker>Zhao, Lin, 2004</marker>
<rawString>Zhao, Shaojun and Dekang Lin. 2004. A nearest-neighbor method for resolving PP-attachment ambiguities. In The First International Joint Conference on Natural Language Processing (IJCNLP-04), pages 545–554, Hainan Island, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>