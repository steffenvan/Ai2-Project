<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000194">
<title confidence="0.928947">
Learning Better Data Representation using Inference-Driven Metric
Learning
</title>
<author confidence="0.511253">
Paramveer S. Dhillon Partha Pratim Talukdar* Koby Crammer
</author>
<affiliation confidence="0.458027">
CIS Deptt., Univ. of Penn. Search Labs, Microsoft Research Deptt. of Electrical Engg.
</affiliation>
<address confidence="0.814896">
Philadelphia, PA, U.S.A Mountain View, CA, USA The Technion, Haifa, Israel
</address>
<email confidence="0.997732">
dhillon@cis.upenn.edu partha@talukdar.net koby@ee.technion.ac.il
</email>
<sectionHeader confidence="0.993862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966181818182">
We initiate a study comparing effective-
ness of the transformed spaces learned by
recently proposed supervised, and semi-
supervised metric learning algorithms
to those generated by previously pro-
posed unsupervised dimensionality reduc-
tion methods (e.g., PCA). Through a va-
riety of experiments on different real-
world datasets, we find IDML-IT, a semi-
supervised metric learning algorithm to be
the most effective.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999792166666667">
Because of the high-dimensional nature of NLP
datasets, estimating a large number of parameters
(a parameter for each dimension), often from a
limited amount of labeled data, is a challenging
task for statistical learners. Faced with this chal-
lenge, various unsupervised dimensionality reduc-
tion methods have been developed over the years,
e.g., Principal Components Analysis (PCA).
Recently, several supervised metric learning al-
gorithms have been proposed (Davis et al., 2007;
Weinberger and Saul, 2009). IDML-IT (Dhillon et
al., 2010) is another such method which exploits
labeled as well as unlabeled data during metric
learning. These methods learn a Mahalanobis dis-
tance metric to compute distance between a pair
of data instances, which can also be interpreted as
learning a transformation of the input data, as we
shall see in Section 2.1.
In this paper, we make the following contribu-
tions:
Even though different supervised and semi-
supervised metric learning algorithms have
recently been proposed, effectiveness of the
transformed spaces learned by them in NLP
</bodyText>
<footnote confidence="0.657784">
* Research carried out while at the University of Penn-
sylvania, Philadelphia, PA, USA.
</footnote>
<bodyText confidence="0.999837333333333">
datasets has not been studied before. In
this paper, we address that gap: we com-
pare effectiveness of classifiers trained on the
transformed spaces learned by metric learn-
ing methods to those generated by previ-
ously proposed unsupervised dimensionality
reduction methods. We find IDML-IT, a
semi-supervised metric learning algorithm to
be the most effective.
</bodyText>
<sectionHeader confidence="0.997817" genericHeader="method">
2 Metric Learning
</sectionHeader>
<subsectionHeader confidence="0.9938215">
2.1 Relationship between Metric Learning
and Linear Projection
</subsectionHeader>
<bodyText confidence="0.954339857142857">
We first establish the well-known equivalence be-
tween learning a Mahalanobis distance measure
and Euclidean distance in a linearly transformed
space of the data (Weinberger and Saul, 2009). Let
A be a dxd positive definite matrix which param-
eterizes the Mahalanobis distance, dA(xi, xj), be-
tween instances xi and xj, as shown in Equation
</bodyText>
<listItem confidence="0.970784">
1. Since A is positive definite, we can decompose
it as A = PTP, where P is another matrix of size
</listItem>
<equation confidence="0.90162775">
dxd.
dA(xi, xj) = (xi − xj)TA(xi − xj) (1)
= (Pxi − Pxj)T(Pxi − Pxj)
= dEudidean(Pxi,Pxj)
</equation>
<bodyText confidence="0.998323545454545">
Hence, computing Mahalanobis distance pa-
rameterized by A is equivalent to first projecting
the instances into a new space using an appropriate
transformation matrix P and then computing Eu-
clidean distance in the linearly transformed space.
In this paper, we are interested in learning a better
representation of the data (i.e., projection matrix
P), and we shall achieve that goal by learning the
corresponding Mahalanobis distance parameter A.
We shall now review two recently proposed
metric learning algorithms.
</bodyText>
<page confidence="0.979242">
377
</page>
<note confidence="0.6717575">
Proceedings of the ACL 2010 Conference Short Papers, pages 377–381,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.7105315">
2.2 Information-Theoretic Metric Learning
(ITML): Supervised
</subsectionHeader>
<bodyText confidence="0.998975833333333">
Information-Theoretic Metric Learning (ITML)
(Davis et al., 2007) assumes the availability of
prior knowledge about inter-instance distances. In
this scheme, two instances are considered simi-
lar if the Mahalanobis distance between them is
upper bounded, i.e., dA(xi, xj) &lt; u, where u
is a non-trivial upper bound. Similarly, two in-
stances are considered dissimilar if the distance
between them is larger than certain threshold l,
i.e., dA(xi, xj) &gt; l. Similar instances are rep-
resented by set S, while dissimilar instances are
represented by set D.
In addition to prior knowledge about inter-
instance distances, sometimes prior information
about the matrix A, denoted by A0, itself may
also be available. For example, Euclidean dis-
tance (i.e., A0 = I) may work well in some do-
mains. In such cases, we would like the learned
matrix A to be as close as possible to the prior ma-
trix A0. ITML combines these two types of prior
information, i.e., knowledge about inter-instance
distances, and prior matrix A0, in order to learn
the matrix A by solving the optimization problem
shown in (2).
</bodyText>
<equation confidence="0.952588142857143">
Dld(A, A0) (2)
s.t. tr{A(xi − xj)(xi − xj)T} &lt; u,
b(i, j) E S
tr{A(xi − xj)(xi − xj)T} &gt; l,
b(i, j) E D
where Dld(A, A0) = tr(AA�1
0 ) − log det(AA�1
</equation>
<bodyText confidence="0.940278333333333">
0 )
−n, is the LogDet divergence.
To handle situations where exactly solving the
problem in (2) is not possible, slack variables may
be introduced to the ITML objective. To solve this
optimization problem, an algorithm involving re-
peated Bregman projections is presented in (Davis
et al., 2007), which we use for the experiments re-
ported in this paper.
</bodyText>
<sectionHeader confidence="0.5773545" genericHeader="method">
2.3 Inference-Driven Metric Learning
(IDML): Semi-Supervised
</sectionHeader>
<bodyText confidence="0.999584890909091">
Notations: We first define the necessary notations.
Let X be the d x n matrix of n instances in a
d-dimensional space. Out of the n instances, nl
instances are labeled, while the remaining nu in-
stances are unlabeled, with n = nl + nu. Let S be
a n x n diagonal matrix with Sii = 1 iff instance
xi is labeled. m is the total number of labels. Y
is the n x m matrix storing training label informa-
tion, if any. Y� is the n x m matrix of estimated la-
bel information, i.e., output of any classifier, with
Yil denoting score of label l at node i. .
The ITML metric learning algorithm, which we
reviewed in Section 2.2, is supervised in nature,
and hence it does not exploit widely available un-
labeled data. In this section, we review Infer-
ence Driven Metric Learning (IDML) (Algorithm
1) (Dhillon et al., 2010), a recently proposed met-
ric learning framework which combines an exist-
ing supervised metric learning algorithm (such as
ITML) along with transductive graph-based la-
bel inference to learn a new distance metric from
labeled as well as unlabeled data combined. In
self-training styled iterations, IDML alternates be-
tween metric learning and label inference; with
output of label inference used during next round
of metric learning, and so on.
IDML starts out with the assumption that ex-
isting supervised metric learning algorithms, such
as ITML, can learn a better metric if the number
of available labeled instances is increased. Since
we are focusing on the semi-supervised learning
(SSL) setting with nl labeled and nu unlabeled
instances, the idea is to automatically label the
unlabeled instances using a graph based SSL al-
gorithm, and then include instances with low as-
signed label entropy (i.e., high confidence label
assignments) in the next round of metric learning.
The number of instances added in each iteration
depends on the threshold β1. This process is con-
tinued until no new instances can be added to the
set of labeled instances, which can happen when
either all the instances are already exhausted, or
when none of the remaining unlabeled instances
can be assigned labels with high confidence.
The IDML framework is presented in Algo-
rithm 1. In Line 3, any supervised metric
learner, such as ITML, may be used as the
METRICLEARNER. Using the distance metric
learned in Line 3, a new k-NN graph is constructed
in Line 4 , whose edge weight matrix is stored in
W. In Line 5 , GRAPHLABELINF optimizes over
the newly constructed graph, the GRF objective
(Zhu et al., 2003) shown in (3).
Y�STLY�S}, s.t. S�Y� = S�Y�S (3)
where L = D − W is the (unnormalized) Lapla-
</bodyText>
<footnote confidence="0.887643">
1During the experiments in Section 3, we set ,3 = 0.05
</footnote>
<figure confidence="0.914935">
min
A&gt;-0
tr{
min
Y&apos;
</figure>
<page confidence="0.935647">
378
</page>
<construct confidence="0.2469545">
Algorithm 1: Inference Driven Metric Learn-
ing (IDML)
</construct>
<bodyText confidence="0.61880525">
Input: instances X, training labels Y , training
instance indicator S, label entropy threshold Q,
neighborhood size k
Output: Mahalanobis distance parameter A
</bodyText>
<listItem confidence="0.9769385">
1: Y�+— Y , S�+— S
2: repeat
3: A +— METRICLEARNER(X, S, Y)
4: W +— CONSTRUCTKNNGRAPH(X, A, k)
5: Y�0 +— GRAPHLABELINF(W, S,Y)
Y�0, �S, Q)
9: until convergence (i.e., Uii = 0, Vi)
10: return A
</listItem>
<bodyText confidence="0.992128">
cian, and D is a diagonal matrix with Dii =
Ej Wij. The constraint, S�Y� = S�Y�0, in (3)
makes sure that labels on training instances are not
changed during inference. In Line 6, a currently
unlabeled instance xi (i.e., Sii = 0) is consid-
ered a new labeled training instance, i.e., Uii = 1,
for next round of metric learning if the instance
has been assigned labels with high confidence in
the current iteration, i.e., if its label distribution
has low entropy (i.e., ENTROPY(Y�0
i:) G Q). Fi-
nally in Line 7, training instance label information
is updated. This iterative process is continued till
no new labeled instance can be added, i.e., when
Uii = 0 Vi. IDML returns the learned matrix A
which can be used to compute Mahalanobis dis-
tance using Equation 1.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.924096">
3.1 Setup
</subsectionHeader>
<table confidence="0.985367666666667">
Dataset Dimension Balanced
Electronics 84816 Yes
Books 139535 Yes
Kitchen 73539 Yes
DVDs 155465 Yes
WebKB 44261 Yes
</table>
<tableCaption confidence="0.808602666666667">
Table 1: Description of the datasets used in Sec-
tion 3. All datasets are binary with 1500 total in-
stances in each.
</tableCaption>
<bodyText confidence="0.997314076923077">
Description of the datasets used during experi-
ments in Section 3 are presented in Table 1. The
first four datasets – Electronics, Books, Kitchen,
and DVDs – are from the sentiment domain and
previously used in (Blitzer et al., 2007). WebKB
is a text classification dataset derived from (Sub-
ramanya and Bilmes, 2008). For details regard-
ing features and data pre-processing, we refer the
reader to the origin of these datasets cited above.
One extra preprocessing that we did was that we
only considered features which occurred more 20
times in the entire dataset to make the problem
more computationally tractable and also since the
infrequently occurring features usually contribute
noise. We use classification error (lower is better)
as the evaluation metric. We experiment with the
following ways of estimating transformation ma-
trix P:
Original2: We set P = I, where I is the
d x d identity matrix. Hence, the data is not
transformed in this case.
RP: The data is first projected into a lower
dimensional space using the Random Pro-
jection (RP) method (Bingham and Mannila,
2001). Dimensionality of the target space
was set at d0 = E�l0 n1, as prescribed in
</bodyText>
<equation confidence="0.865559">
g E
</equation>
<bodyText confidence="0.991401615384615">
(Bingham and Mannila, 2001). We use the
projection matrix constructed by RP as P. c
was set to 0.25 for the experiments in Sec-
tion 3, which has the effect of projecting the
data into a much lower dimensional space
(84 for the experiments in this section). This
presents an interesting evaluation setting as
we already run evaluations in much higher di-
mensional space (e.g., Original).
PCA: Data instances are first projected into
a lower dimensional space using Principal
Components Analysis (PCA) (Jolliffe, 2002)
. Following (Weinberger and Saul, 2009), di-
mensionality of the projected space was set
at 250 for all experiments. In this case, we
used the projection matrix generated by PCA
as P.
ITML: A is learned by applying ITML (see
Section 2.2) on the Original space (above),
and then we decompose A as A = PTP to
obtain P.
2Note that “Original” in the results tables refers to orig-
inal space with features occurring more than 20 times. We
also ran experiments with original set of features (without
any thresholding) and the results were worse or comparable
to the ones reported in the tables.
</bodyText>
<equation confidence="0.797707333333333">
6: U +— SELECTLOWENTINST(
7: Y� +— Y�+ U Y�0
8: S� � S�+ U
</equation>
<page confidence="0.962875">
379
</page>
<table confidence="0.999835">
Datasets Original RP PCA ITML IDML-IT
µ ± Q µ ± Q µ ± Q µ ± Q µ ± Q
Electronics 31.3 ± 0.9 42.5 ± 1.0 46.4 ± 2.0 33.0 ± 1.0 30.7±0.7
Books 37.5 ± 1.1 45.0 ± 1.1 34.8 ± 1.4 35.0 ± 1.1 32.0±0.9
Kitchen 33.7 ± 1.0 43.0 ± 1.1 34.0 ± 1.6 30.9 ± 0.7 29.0±1.0
DVDs 39.0 ± 1.2 47.7 ± 1.2 36.2 ± 1.6 37.0 ± 0.8 33.9±1.0
WebKB 31.4 ± 0.9 33.0 ± 1.0 27.9 ± 1.3 28.9 ± 1.0 25.5±1.0
</table>
<tableCaption confidence="0.980064333333333">
Table 2: Comparison of SVM % classification errors (lower is better), with 50 labeled instances (Sec.
3.2). nl=50. and nu = 1450. All results are averaged over ten trials. All hyperparameters are tuned on a
separate random split.
</tableCaption>
<table confidence="0.999875285714286">
Datasets Original RP PCA ITML IDML-IT
µ ± Q µ ± Q µ ± Q µ ± Q µ ± Q
Electronics 27.0 ± 0.9 40.0 ± 1.0 41.2 ± 1.0 27.5 ± 0.8 25.3±0.8
Books 31.0 ± 0.7 42.9 ± 0.6 31.3 ± 0.7 29.9 ± 0.5 27.7±0.7
Kitchen 26.3 ± 0.5 41.9 ± 0.7 27.0 ± 0.9 26.1 ± 0.8 24.8±0.9
DVDs 34.7 ± 0.4 46.8 ± 0.6 32.9 ± 0.8 34.0 ± 0.8 31.8±0.9
WebKB 25.7 ± 0.5 31.1 ± 0.5 24.9 ± 0.6 25.6 ± 0.4 23.9±0.4
</table>
<tableCaption confidence="0.909834333333333">
Table 3: Comparison of SVM % classification errors (lower is better), with 100 labeled instances (Sec.
3.2). nl=100. and nu = 1400. All results are averaged over ten trials. All hyperparameters are tuned on
a separate random split.
</tableCaption>
<bodyText confidence="0.9984626">
IDML-IT: A is learned by applying IDML
(Algorithm 1) (see Section 2.3) on the Orig-
inal space (above); with ITML used as
METRICLEARNER in IDML (Line 3 in Al-
gorithm 1). In this case, we treat the set of
test instances (without their gold labels) as
the unlabeled data. In other words, we essen-
tially work in the transductive setting (Vap-
nik, 2000). Once again, we decompose A as
A = PTP to obtain P.
We also experimented with the supervised
large-margin metric learning algorithm (LMNN)
presented in (Weinberger and Saul, 2009). We
found ITML to be more effective in practice than
LMNN, and hence we report results based on
ITML only. Each input instance, x, is now pro-
jected into the transformed space as Px. We
now train different classifiers on this transformed
space. All results are averaged over ten random
trials.
</bodyText>
<subsectionHeader confidence="0.996462">
3.2 Supervised Classification
</subsectionHeader>
<bodyText confidence="0.999931333333334">
We train a SVM classifier, with an RBF kernel, on
the transformed space generated by the projection
matrix P. SVM hyperparameter, C and RBF ker-
nel bandwidth, were tuned on a separate develop-
ment split. Experimental results with 50 and 100
labeled instances are shown in Table 2, and Ta-
ble 3, respectively. From these results, we observe
that IDML-IT consistently achieves the best per-
formance across all experimental settings. We also
note that in Table 3, performance difference be-
tween ITML and IDML-IT in the Electronics and
Kitchen domains are statistically significant.
</bodyText>
<subsectionHeader confidence="0.976671">
3.3 Semi-Supervised Classification
</subsectionHeader>
<bodyText confidence="0.999737117647059">
In this section, we trained the GRF classifier (see
Equation 3), a graph-based semi-supervised learn-
ing (SSL) algorithm (Zhu et al., 2003), using
Gaussian kernel parameterized by A = PTP to
set edge weights. During graph construction, each
node was connected to its k nearest neighbors,
with k treated as a hyperparameter and tuned on
a separate development set. Experimental results
with 50 and 100 labeled instances are shown in
Table 4, and Table 5, respectively. As before, we
experimented with nl = 50 and nl = 100. Once
again, we observe that IDML-IT is the most effec-
tive method, with the GRF classifier trained on the
data representation learned by IDML-IT achieving
best performance in all settings. Here also, we ob-
serve that IDML-IT achieves the best performance
across all experimental settings.
</bodyText>
<page confidence="0.989991">
380
</page>
<table confidence="0.999689">
Datasets Original RP PCA ITML IDML-IT
A ± Q A ± Q A ± Q A ± Q A ± Q
Electronics 47.9 ± 1.1 49.0 ± 1.2 43.2 ± 0.9 34.9 ± 0.5 34.0±0.5
Books 50.0 ± 1.0 49.4 ± 1.0 47.9 ± 0.7 42.1 ± 0.7 40.6±0.7
Kitchen 49.8 ± 1.1 49.6 ± 0.9 48.6 ± 0.8 31.1 ± 0.5 30.0±0.5
DVDs 50.1 ± 0.5 49.9 ± 0.7 49.4 ± 0.6 42.1 ± 0.4 41.2±0.5
WebKB 33.1 ± 0.4 33.1 ± 0.3 33.1 ± 0.3 30.0 ± 0.4 28.7±0.5
</table>
<tableCaption confidence="0.977140666666667">
Table 4: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with ni = 50 and n,, = 1450. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
</tableCaption>
<table confidence="0.999825">
Datasets Original RP PCA ITML IDML-IT
A ± Q A ± Q A ± Q A ± Q A ± Q
Electronics 43.5 ± 0.7 47.2 ± 0.8 39.1 ± 0.7 31.3 ± 0.2 30.8±0.3
Books 48.3 ± 0.5 48.9 ± 0.3 43.3 ± 0.4 35.2 ± 0.5 33.3±0.6
Kitchen 45.3 ± 0.6 48.2 ± 0.5 41.0 ± 0.7 30.7 ± 0.6 29.9±0.3
DVDs 48.6 ± 0.3 49.3 ± 0.5 45.9 ± 0.5 42.6 ± 0.4 41.7±0.3
WebKB 33.4 ± 0.4 33.4 ± 0.4 33.4 ± 0.3 30.4 ± 0.5 28.6±0.7
</table>
<tableCaption confidence="0.705075666666667">
Table 5: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with ni = 100 and n,, = 1400. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
</tableCaption>
<sectionHeader confidence="0.999457" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999795">
In this paper, we compared the effectiveness
of the transformed spaces learned by recently
proposed supervised, and semi-supervised metric
learning algorithms to those generated by previ-
ously proposed unsupervised dimensionality re-
duction methods (e.g., PCA). To the best of our
knowledge, this is the first study of its kind in-
volving NLP datasets. Through a variety of ex-
periments on different real-world NLP datasets,
we demonstrated that supervised as well as semi-
supervised classifiers trained on the space learned
by IDML-IT consistently result in the lowest clas-
sification errors. Encouraged by these early re-
sults, we plan to explore further the applicability
of IDML-IT in other NLP tasks (e.g., entity classi-
fication, word sense disambiguation, polarity lexi-
con induction, etc.) where better representation of
the data is a pre-requisite for effective learning.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9977725">
Thanks to Kuzman Ganchev for providing detailed
feedback on a draft of this paper. This work
was supported in part by NSF IIS-0447972 and
DARPA HRO1107-1-0029.
</bodyText>
<sectionHeader confidence="0.999379" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999576769230769">
E. Bingham and H. Mannila. 2001. Random projec-
tion in dimensionality reduction: applications to im-
age and text data. In ACM SIGKDD.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In
ACL.
J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon.
2007. Information-theoretic metric learning. In
ICML.
P. S. Dhillon, P. P. Talukdar, and K. Crammer. 2010.
Inference-driven metric learning for graph construc-
tion. Technical Report MS-CIS-10-18, CIS Depart-
ment, University of Pennsylvania, May.
IT Jolliffe. 2002. Principal component analysis.
Springer verlag.
A. Subramanya and J. Bilmes. 2008. Soft-Supervised
Learning for Text Classification. In EMNLP.
V.N. Vapnik. 2000. The nature of statistical learning
theory. Springer Verlag.
K.Q. Weinberger and L.K. Saul. 2009. Distance metric
learning for large margin nearest neighbor classifica-
tion. The Journal of Machine Learning Research.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In ICML.
</reference>
<page confidence="0.998708">
381
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.964674">
<title confidence="0.999798">Learning Better Data Representation using Inference-Driven Metric Learning</title>
<author confidence="0.994067">S Dhillon Partha Pratim Crammer</author>
<affiliation confidence="0.997132">CIS Deptt., Univ. of Penn. Search Labs, Microsoft Research Deptt. of Electrical Engg.</affiliation>
<address confidence="0.99196">Philadelphia, PA, U.S.A Mountain View, CA, USA The Technion, Haifa, Israel</address>
<email confidence="0.989274">dhillon@cis.upenn.edupartha@talukdar.netkoby@ee.technion.ac.il</email>
<abstract confidence="0.999208166666667">We initiate a study comparing effectiveness of the transformed spaces learned by proposed and semilearning algorithms to those generated by previously proreduction methods (e.g., PCA). Through a variety of experiments on different realdatasets, we find IDML-IT, a semilearning algorithm to be the most effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bingham</author>
<author>H Mannila</author>
</authors>
<title>Random projection in dimensionality reduction: applications to image and text data.</title>
<date>2001</date>
<booktitle>In ACM SIGKDD.</booktitle>
<contexts>
<context position="10459" citStr="Bingham and Mannila, 2001" startWordPosition="1725" endWordPosition="1728">essing that we did was that we only considered features which occurred more 20 times in the entire dataset to make the problem more computationally tractable and also since the infrequently occurring features usually contribute noise. We use classification error (lower is better) as the evaluation metric. We experiment with the following ways of estimating transformation matrix P: Original2: We set P = I, where I is the d x d identity matrix. Hence, the data is not transformed in this case. RP: The data is first projected into a lower dimensional space using the Random Projection (RP) method (Bingham and Mannila, 2001). Dimensionality of the target space was set at d0 = E�l0 n1, as prescribed in g E (Bingham and Mannila, 2001). We use the projection matrix constructed by RP as P. c was set to 0.25 for the experiments in Section 3, which has the effect of projecting the data into a much lower dimensional space (84 for the experiments in this section). This presents an interesting evaluation setting as we already run evaluations in much higher dimensional space (e.g., Original). PCA: Data instances are first projected into a lower dimensional space using Principal Components Analysis (PCA) (Jolliffe, 2002) . </context>
</contexts>
<marker>Bingham, Mannila, 2001</marker>
<rawString>E. Bingham and H. Mannila. 2001. Random projection in dimensionality reduction: applications to image and text data. In ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9610" citStr="Blitzer et al., 2007" startWordPosition="1585" endWordPosition="1588">e added, i.e., when Uii = 0 Vi. IDML returns the learned matrix A which can be used to compute Mahalanobis distance using Equation 1. 3 Experiments 3.1 Setup Dataset Dimension Balanced Electronics 84816 Yes Books 139535 Yes Kitchen 73539 Yes DVDs 155465 Yes WebKB 44261 Yes Table 1: Description of the datasets used in Section 3. All datasets are binary with 1500 total instances in each. Description of the datasets used during experiments in Section 3 are presented in Table 1. The first four datasets – Electronics, Books, Kitchen, and DVDs – are from the sentiment domain and previously used in (Blitzer et al., 2007). WebKB is a text classification dataset derived from (Subramanya and Bilmes, 2008). For details regarding features and data pre-processing, we refer the reader to the origin of these datasets cited above. One extra preprocessing that we did was that we only considered features which occurred more 20 times in the entire dataset to make the problem more computationally tractable and also since the infrequently occurring features usually contribute noise. We use classification error (lower is better) as the evaluation metric. We experiment with the following ways of estimating transformation mat</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J V Davis</author>
<author>B Kulis</author>
<author>P Jain</author>
<author>S Sra</author>
<author>I S Dhillon</author>
</authors>
<title>Information-theoretic metric learning.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1270" citStr="Davis et al., 2007" startWordPosition="173" endWordPosition="176">iments on different realworld datasets, we find IDML-IT, a semisupervised metric learning algorithm to be the most effective. 1 Introduction Because of the high-dimensional nature of NLP datasets, estimating a large number of parameters (a parameter for each dimension), often from a limited amount of labeled data, is a challenging task for statistical learners. Faced with this challenge, various unsupervised dimensionality reduction methods have been developed over the years, e.g., Principal Components Analysis (PCA). Recently, several supervised metric learning algorithms have been proposed (Davis et al., 2007; Weinberger and Saul, 2009). IDML-IT (Dhillon et al., 2010) is another such method which exploits labeled as well as unlabeled data during metric learning. These methods learn a Mahalanobis distance metric to compute distance between a pair of data instances, which can also be interpreted as learning a transformation of the input data, as we shall see in Section 2.1. In this paper, we make the following contributions: Even though different supervised and semisupervised metric learning algorithms have recently been proposed, effectiveness of the transformed spaces learned by them in NLP * Rese</context>
<context position="3714" citStr="Davis et al., 2007" startWordPosition="552" endWordPosition="555">omputing Euclidean distance in the linearly transformed space. In this paper, we are interested in learning a better representation of the data (i.e., projection matrix P), and we shall achieve that goal by learning the corresponding Mahalanobis distance parameter A. We shall now review two recently proposed metric learning algorithms. 377 Proceedings of the ACL 2010 Conference Short Papers, pages 377–381, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics 2.2 Information-Theoretic Metric Learning (ITML): Supervised Information-Theoretic Metric Learning (ITML) (Davis et al., 2007) assumes the availability of prior knowledge about inter-instance distances. In this scheme, two instances are considered similar if the Mahalanobis distance between them is upper bounded, i.e., dA(xi, xj) &lt; u, where u is a non-trivial upper bound. Similarly, two instances are considered dissimilar if the distance between them is larger than certain threshold l, i.e., dA(xi, xj) &gt; l. Similar instances are represented by set S, while dissimilar instances are represented by set D. In addition to prior knowledge about interinstance distances, sometimes prior information about the matrix A, denote</context>
<context position="5178" citStr="Davis et al., 2007" startWordPosition="808" endWordPosition="811">es of prior information, i.e., knowledge about inter-instance distances, and prior matrix A0, in order to learn the matrix A by solving the optimization problem shown in (2). Dld(A, A0) (2) s.t. tr{A(xi − xj)(xi − xj)T} &lt; u, b(i, j) E S tr{A(xi − xj)(xi − xj)T} &gt; l, b(i, j) E D where Dld(A, A0) = tr(AA�1 0 ) − log det(AA�1 0 ) −n, is the LogDet divergence. To handle situations where exactly solving the problem in (2) is not possible, slack variables may be introduced to the ITML objective. To solve this optimization problem, an algorithm involving repeated Bregman projections is presented in (Davis et al., 2007), which we use for the experiments reported in this paper. 2.3 Inference-Driven Metric Learning (IDML): Semi-Supervised Notations: We first define the necessary notations. Let X be the d x n matrix of n instances in a d-dimensional space. Out of the n instances, nl instances are labeled, while the remaining nu instances are unlabeled, with n = nl + nu. Let S be a n x n diagonal matrix with Sii = 1 iff instance xi is labeled. m is the total number of labels. Y is the n x m matrix storing training label information, if any. Y� is the n x m matrix of estimated label information, i.e., output of a</context>
</contexts>
<marker>Davis, Kulis, Jain, Sra, Dhillon, 2007</marker>
<rawString>J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon. 2007. Information-theoretic metric learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Dhillon</author>
<author>P P Talukdar</author>
<author>K Crammer</author>
</authors>
<title>Inference-driven metric learning for graph construction.</title>
<date>2010</date>
<tech>Technical Report MS-CIS-10-18,</tech>
<institution>CIS Department, University of Pennsylvania,</institution>
<contexts>
<context position="1330" citStr="Dhillon et al., 2010" startWordPosition="182" endWordPosition="185">a semisupervised metric learning algorithm to be the most effective. 1 Introduction Because of the high-dimensional nature of NLP datasets, estimating a large number of parameters (a parameter for each dimension), often from a limited amount of labeled data, is a challenging task for statistical learners. Faced with this challenge, various unsupervised dimensionality reduction methods have been developed over the years, e.g., Principal Components Analysis (PCA). Recently, several supervised metric learning algorithms have been proposed (Davis et al., 2007; Weinberger and Saul, 2009). IDML-IT (Dhillon et al., 2010) is another such method which exploits labeled as well as unlabeled data during metric learning. These methods learn a Mahalanobis distance metric to compute distance between a pair of data instances, which can also be interpreted as learning a transformation of the input data, as we shall see in Section 2.1. In this paper, we make the following contributions: Even though different supervised and semisupervised metric learning algorithms have recently been proposed, effectiveness of the transformed spaces learned by them in NLP * Research carried out while at the University of Pennsylvania, Ph</context>
<context position="6102" citStr="Dhillon et al., 2010" startWordPosition="980" endWordPosition="983">ining nu instances are unlabeled, with n = nl + nu. Let S be a n x n diagonal matrix with Sii = 1 iff instance xi is labeled. m is the total number of labels. Y is the n x m matrix storing training label information, if any. Y� is the n x m matrix of estimated label information, i.e., output of any classifier, with Yil denoting score of label l at node i. . The ITML metric learning algorithm, which we reviewed in Section 2.2, is supervised in nature, and hence it does not exploit widely available unlabeled data. In this section, we review Inference Driven Metric Learning (IDML) (Algorithm 1) (Dhillon et al., 2010), a recently proposed metric learning framework which combines an existing supervised metric learning algorithm (such as ITML) along with transductive graph-based label inference to learn a new distance metric from labeled as well as unlabeled data combined. In self-training styled iterations, IDML alternates between metric learning and label inference; with output of label inference used during next round of metric learning, and so on. IDML starts out with the assumption that existing supervised metric learning algorithms, such as ITML, can learn a better metric if the number of available lab</context>
</contexts>
<marker>Dhillon, Talukdar, Crammer, 2010</marker>
<rawString>P. S. Dhillon, P. P. Talukdar, and K. Crammer. 2010. Inference-driven metric learning for graph construction. Technical Report MS-CIS-10-18, CIS Department, University of Pennsylvania, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IT Jolliffe</author>
</authors>
<title>Principal component analysis.</title>
<date>2002</date>
<publisher>Springer verlag.</publisher>
<contexts>
<context position="11056" citStr="Jolliffe, 2002" startWordPosition="1828" endWordPosition="1829">and Mannila, 2001). Dimensionality of the target space was set at d0 = E�l0 n1, as prescribed in g E (Bingham and Mannila, 2001). We use the projection matrix constructed by RP as P. c was set to 0.25 for the experiments in Section 3, which has the effect of projecting the data into a much lower dimensional space (84 for the experiments in this section). This presents an interesting evaluation setting as we already run evaluations in much higher dimensional space (e.g., Original). PCA: Data instances are first projected into a lower dimensional space using Principal Components Analysis (PCA) (Jolliffe, 2002) . Following (Weinberger and Saul, 2009), dimensionality of the projected space was set at 250 for all experiments. In this case, we used the projection matrix generated by PCA as P. ITML: A is learned by applying ITML (see Section 2.2) on the Original space (above), and then we decompose A as A = PTP to obtain P. 2Note that “Original” in the results tables refers to original space with features occurring more than 20 times. We also ran experiments with original set of features (without any thresholding) and the results were worse or comparable to the ones reported in the tables. 6: U +— SELEC</context>
</contexts>
<marker>Jolliffe, 2002</marker>
<rawString>IT Jolliffe. 2002. Principal component analysis. Springer verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Subramanya</author>
<author>J Bilmes</author>
</authors>
<title>Soft-Supervised Learning for Text Classification. In</title>
<date>2008</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="9693" citStr="Subramanya and Bilmes, 2008" startWordPosition="1597" endWordPosition="1601">be used to compute Mahalanobis distance using Equation 1. 3 Experiments 3.1 Setup Dataset Dimension Balanced Electronics 84816 Yes Books 139535 Yes Kitchen 73539 Yes DVDs 155465 Yes WebKB 44261 Yes Table 1: Description of the datasets used in Section 3. All datasets are binary with 1500 total instances in each. Description of the datasets used during experiments in Section 3 are presented in Table 1. The first four datasets – Electronics, Books, Kitchen, and DVDs – are from the sentiment domain and previously used in (Blitzer et al., 2007). WebKB is a text classification dataset derived from (Subramanya and Bilmes, 2008). For details regarding features and data pre-processing, we refer the reader to the origin of these datasets cited above. One extra preprocessing that we did was that we only considered features which occurred more 20 times in the entire dataset to make the problem more computationally tractable and also since the infrequently occurring features usually contribute noise. We use classification error (lower is better) as the evaluation metric. We experiment with the following ways of estimating transformation matrix P: Original2: We set P = I, where I is the d x d identity matrix. Hence, the da</context>
</contexts>
<marker>Subramanya, Bilmes, 2008</marker>
<rawString>A. Subramanya and J. Bilmes. 2008. Soft-Supervised Learning for Text Classification. In EMNLP. V.N. Vapnik. 2000. The nature of statistical learning theory. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Q Weinberger</author>
<author>L K Saul</author>
</authors>
<title>Distance metric learning for large margin nearest neighbor classification.</title>
<date>2009</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="1298" citStr="Weinberger and Saul, 2009" startWordPosition="177" endWordPosition="180">realworld datasets, we find IDML-IT, a semisupervised metric learning algorithm to be the most effective. 1 Introduction Because of the high-dimensional nature of NLP datasets, estimating a large number of parameters (a parameter for each dimension), often from a limited amount of labeled data, is a challenging task for statistical learners. Faced with this challenge, various unsupervised dimensionality reduction methods have been developed over the years, e.g., Principal Components Analysis (PCA). Recently, several supervised metric learning algorithms have been proposed (Davis et al., 2007; Weinberger and Saul, 2009). IDML-IT (Dhillon et al., 2010) is another such method which exploits labeled as well as unlabeled data during metric learning. These methods learn a Mahalanobis distance metric to compute distance between a pair of data instances, which can also be interpreted as learning a transformation of the input data, as we shall see in Section 2.1. In this paper, we make the following contributions: Even though different supervised and semisupervised metric learning algorithms have recently been proposed, effectiveness of the transformed spaces learned by them in NLP * Research carried out while at th</context>
<context position="2579" citStr="Weinberger and Saul, 2009" startWordPosition="374" endWordPosition="377">tasets has not been studied before. In this paper, we address that gap: we compare effectiveness of classifiers trained on the transformed spaces learned by metric learning methods to those generated by previously proposed unsupervised dimensionality reduction methods. We find IDML-IT, a semi-supervised metric learning algorithm to be the most effective. 2 Metric Learning 2.1 Relationship between Metric Learning and Linear Projection We first establish the well-known equivalence between learning a Mahalanobis distance measure and Euclidean distance in a linearly transformed space of the data (Weinberger and Saul, 2009). Let A be a dxd positive definite matrix which parameterizes the Mahalanobis distance, dA(xi, xj), between instances xi and xj, as shown in Equation 1. Since A is positive definite, we can decompose it as A = PTP, where P is another matrix of size dxd. dA(xi, xj) = (xi − xj)TA(xi − xj) (1) = (Pxi − Pxj)T(Pxi − Pxj) = dEudidean(Pxi,Pxj) Hence, computing Mahalanobis distance parameterized by A is equivalent to first projecting the instances into a new space using an appropriate transformation matrix P and then computing Euclidean distance in the linearly transformed space. In this paper, we are</context>
<context position="11096" citStr="Weinberger and Saul, 2009" startWordPosition="1832" endWordPosition="1835">ality of the target space was set at d0 = E�l0 n1, as prescribed in g E (Bingham and Mannila, 2001). We use the projection matrix constructed by RP as P. c was set to 0.25 for the experiments in Section 3, which has the effect of projecting the data into a much lower dimensional space (84 for the experiments in this section). This presents an interesting evaluation setting as we already run evaluations in much higher dimensional space (e.g., Original). PCA: Data instances are first projected into a lower dimensional space using Principal Components Analysis (PCA) (Jolliffe, 2002) . Following (Weinberger and Saul, 2009), dimensionality of the projected space was set at 250 for all experiments. In this case, we used the projection matrix generated by PCA as P. ITML: A is learned by applying ITML (see Section 2.2) on the Original space (above), and then we decompose A as A = PTP to obtain P. 2Note that “Original” in the results tables refers to original space with features occurring more than 20 times. We also ran experiments with original set of features (without any thresholding) and the results were worse or comparable to the ones reported in the tables. 6: U +— SELECTLOWENTINST( 7: Y� +— Y�+ U Y�0 8: S� � </context>
<context position="13433" citStr="Weinberger and Saul, 2009" startWordPosition="2296" endWordPosition="2299"> All results are averaged over ten trials. All hyperparameters are tuned on a separate random split. IDML-IT: A is learned by applying IDML (Algorithm 1) (see Section 2.3) on the Original space (above); with ITML used as METRICLEARNER in IDML (Line 3 in Algorithm 1). In this case, we treat the set of test instances (without their gold labels) as the unlabeled data. In other words, we essentially work in the transductive setting (Vapnik, 2000). Once again, we decompose A as A = PTP to obtain P. We also experimented with the supervised large-margin metric learning algorithm (LMNN) presented in (Weinberger and Saul, 2009). We found ITML to be more effective in practice than LMNN, and hence we report results based on ITML only. Each input instance, x, is now projected into the transformed space as Px. We now train different classifiers on this transformed space. All results are averaged over ten random trials. 3.2 Supervised Classification We train a SVM classifier, with an RBF kernel, on the transformed space generated by the projection matrix P. SVM hyperparameter, C and RBF kernel bandwidth, were tuned on a separate development split. Experimental results with 50 and 100 labeled instances are shown in Table </context>
</contexts>
<marker>Weinberger, Saul, 2009</marker>
<rawString>K.Q. Weinberger and L.K. Saul. 2009. Distance metric learning for large margin nearest neighbor classification. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semisupervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7787" citStr="Zhu et al., 2003" startWordPosition="1259" endWordPosition="1262">ontinued until no new instances can be added to the set of labeled instances, which can happen when either all the instances are already exhausted, or when none of the remaining unlabeled instances can be assigned labels with high confidence. The IDML framework is presented in Algorithm 1. In Line 3, any supervised metric learner, such as ITML, may be used as the METRICLEARNER. Using the distance metric learned in Line 3, a new k-NN graph is constructed in Line 4 , whose edge weight matrix is stored in W. In Line 5 , GRAPHLABELINF optimizes over the newly constructed graph, the GRF objective (Zhu et al., 2003) shown in (3). Y�STLY�S}, s.t. S�Y� = S�Y�S (3) where L = D − W is the (unnormalized) Lapla1During the experiments in Section 3, we set ,3 = 0.05 min A&gt;-0 tr{ min Y&apos; 378 Algorithm 1: Inference Driven Metric Learning (IDML) Input: instances X, training labels Y , training instance indicator S, label entropy threshold Q, neighborhood size k Output: Mahalanobis distance parameter A 1: Y�+— Y , S�+— S 2: repeat 3: A +— METRICLEARNER(X, S, Y) 4: W +— CONSTRUCTKNNGRAPH(X, A, k) 5: Y�0 +— GRAPHLABELINF(W, S,Y) Y�0, �S, Q) 9: until convergence (i.e., Uii = 0, Vi) 10: return A cian, and D is a diagonal</context>
<context position="14505" citStr="Zhu et al., 2003" startWordPosition="2468" endWordPosition="2471"> and RBF kernel bandwidth, were tuned on a separate development split. Experimental results with 50 and 100 labeled instances are shown in Table 2, and Table 3, respectively. From these results, we observe that IDML-IT consistently achieves the best performance across all experimental settings. We also note that in Table 3, performance difference between ITML and IDML-IT in the Electronics and Kitchen domains are statistically significant. 3.3 Semi-Supervised Classification In this section, we trained the GRF classifier (see Equation 3), a graph-based semi-supervised learning (SSL) algorithm (Zhu et al., 2003), using Gaussian kernel parameterized by A = PTP to set edge weights. During graph construction, each node was connected to its k nearest neighbors, with k treated as a hyperparameter and tuned on a separate development set. Experimental results with 50 and 100 labeled instances are shown in Table 4, and Table 5, respectively. As before, we experimented with nl = 50 and nl = 100. Once again, we observe that IDML-IT is the most effective method, with the GRF classifier trained on the data representation learned by IDML-IT achieving best performance in all settings. Here also, we observe that ID</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semisupervised learning using Gaussian fields and harmonic functions. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>