<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.980019">
Boosting-based parse reranking with subtree features
</title>
<author confidence="0.986847">
Taku Kudo * Jun Suzuki Hideki Isozaki
</author>
<affiliation confidence="0.910841">
NTT Communication Science Laboratories.
</affiliation>
<address confidence="0.949852">
2-4 Hikaridai, Seika-cho, Soraku, Kyoto, Japan
</address>
<email confidence="0.994406">
{taku,jun,isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.994931" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972714285714">
This paper introduces a new application of boost-
ing for parse reranking. Several parsers have been
proposed that utilize the all-subtrees representa-
tion (e.g., tree kernel and data oriented parsing).
This paper argues that such an all-subtrees repre-
sentation is extremely redundant and a compara-
ble accuracy can be achieved using just a small
set of subtrees. We show how the boosting algo-
rithm can be applied to the all-subtrees representa-
tion and how it selects a small and relevant feature
set efficiently. Two experiments on parse rerank-
ing show that our method achieves comparable or
even better performance than kernel methods and
also improves the testing efficiency.
</bodyText>
<sectionHeader confidence="0.998111" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999507625">
Recent work on statistical natural language pars-
ing and tagging has explored discriminative tech-
niques. One of the novel discriminative approaches
is reranking, where discriminative machine learning
algorithms are used to rerank the n-best outputs of
generative or conditional parsers. The discrimina-
tive reranking methods allow us to incorporate vari-
ous kinds of features to distinguish the correct parse
tree from all other candidates.
With such feature design flexibility, it is non-
trivial to employ an appropriate feature set that has
a good discriminative ability for parse reranking. In
early studies, feature sets were given heuristically by
simply preparing task-dependent feature templates
(Collins, 2000; Collins, 2002). These ad-hoc solu-
tions might provide us with reasonable levels of per-
</bodyText>
<note confidence="0.506591">
*Currently, Google Japan Inc., taku@google.com
</note>
<bodyText confidence="0.999827259259259">
formance. However, they are highly task dependent
and require careful design to create the optimal fea-
ture set for each task. Kernel methods offer an ele-
gant solution to these problems. They can work on a
potentially huge or even infinite number of features
without a loss of generalization. The best known
kernel for modeling a tree is the tree kernel (Collins
and Duffy, 2002), which argues that a feature vec-
tor is implicitly composed of the counts of subtrees.
Although kernel methods are general and can cover
almost all useful features, the set of subtrees that is
used is extremely redundant. The main question ad-
dressed in this paper concerns whether it is possible
to achieve a comparable or even better accuracy us-
ing just a small and non-redundant set of subtrees.
In this paper, we present a new application of
boosting for parse reranking. While tree kernel
implicitly uses the all-subtrees representation, our
boosting algorithm uses it explicitly. Although this
set-up makes the feature space large, the l1-norm
regularization achived by boosting automatically se-
lects a small and relevant feature set. Such a small
feature set is useful in practice, as it is interpretable
and makes the parsing (reranking) time faster. We
also incorporate a variant of the branch-and-bound
technique to achieve efficient feature selection in
each boosting iteration.
</bodyText>
<sectionHeader confidence="0.857101" genericHeader="method">
2 General setting of parse reranking
</sectionHeader>
<bodyText confidence="0.997677">
We describe the general setting of parse reranking.
</bodyText>
<listItem confidence="0.9923926">
• Training data T is a set of input/output pairs, e.g.,
T = {(x1, y1), ... , (xL, yL)1, where xi is an in-
put sentence, and yi is a correct parse associated
with the sentence xi.
• Let Y(x) be a function that returns a set of candi-
</listItem>
<page confidence="0.563227">
189
</page>
<note confidence="0.9982085">
Proceedings of the 43rd Annual Meeting of the ACL, pages 189–196,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.9358">
date parse trees for a particular sentence x.
</bodyText>
<listItem confidence="0.998233833333333">
• We assume that Y(xi) contains the correct parse
tree yi, i.e., yi ∈ Y(xi) *
• Let -b(y) ∈ Rd be a feature function that maps
the given parse tree y into Rd space. w ∈ Rd is
a parameter vector of the model. The output parse
y� of this model on input
</listItem>
<equation confidence="0.986091">
sentence x is given as:
y� = argmaxyEY(x) w · b(y).
</equation>
<bodyText confidence="0.9999792">
There are two questions as regards this formula-
tion. One is how to set the parameters w, and the
other is how to design the feature function -b(y). We
briefly describe the well-known solutions to these
two problems in the next subsections.
</bodyText>
<subsectionHeader confidence="0.947298">
2.1 Parameter estimation
</subsectionHeader>
<bodyText confidence="0.99999725">
We usually adopt a general loss function Loss(w),
and set the parameters w that minimize the loss,
i.e., w� = argminwERd Loss(w). Generally, the loss
function has the following form:
</bodyText>
<equation confidence="0.996462666666667">
L
Loss(w) = L(w, -b(yi), xi),
i=1
</equation>
<bodyText confidence="0.999663166666667">
where L(w, -b(yi), xi) is an arbitrary loss function.
We can design a variety of parameter estimation
methods by changing the loss function. The follow-
ing three loss functions, LogLoss, HingeLoss, and
BoostLoss, have been widely used in parse rerank-
ing tasks.
</bodyText>
<equation confidence="0.996623">
�LogLoss =
HingeLoss =
BoostLos =
</equation>
<bodyText confidence="0.998023909090909">
LogLoss is based on the standard maximum like-
lihood optimization, and is used with maximum en-
tropy models. HingeLoss captures the errors only
when w · [-b(yi) − -b(y)]) &lt; 1. This loss is closely
related to the maximum margin strategy in SVMs
(Vapnik, 1998). BoostLoss is analogous to the
boosting algorithm and is used in (Collins, 2000;
Collins, 2002).
In the real setting, we cannot assume this condition. In this
case, we select the parse tree y� that is the most similar to yi and
take y� as the correct parse tree yi.
</bodyText>
<subsectionHeader confidence="0.999668">
2.2 Definition of feature function
</subsectionHeader>
<bodyText confidence="0.999980826086957">
It is non-trivial to define an appropriate feature func-
tion 4b(y) that has a good ability to distinguish the
correct parse yi from all other candidates
In early studies, the feature functions were given
heuristically by simply preparing feature templates
(Collins, 2000; Collins, 2002). However, such
heuristic selections are task dependent and would
not cover all useful features that contribute to overall
accuracy.
When we select the special family of loss func-
tions, the problem can be reduced to a dual form that
depends only on the inner products of two instances
4b(y1) · 4b(y2). This property is important as we can
use a kernel trick and we do not need to provide an
explicit feature function. For example, tree kernel
(Collins and Duffy, 2002), one of the convolution
kernels, implicitly maps the instance represented in
a tree into all-subtrees space. Even though the fea-
ture space is large, inner products under this feature
space can be calculated efficiently using dynamic
programming. Tree kernel is more general than fea-
ture templates since it can use the all-subtrees repre-
sentation without loss of efficiency.
</bodyText>
<sectionHeader confidence="0.901984" genericHeader="method">
3 RankBoost with subtree features
</sectionHeader>
<bodyText confidence="0.999989142857143">
A simple question related to kernel-based parse
reranking asks whether all subtrees are really needed
to construct the final parameters w. Suppose we
have two large trees t and t&apos;, where t&apos; is simply gen-
erated by attaching a single node to t. In most cases,
these two trees yield an almost equivalent discrimi-
native ability, since they are very similar and highly
correlated with each other. Even when we exploit all
subtrees, most of them are extremely redundant.
The motivation of this paper is based on the above
observation. We think that only a small set of sub-
trees is needed to express the final parameters. A
compact, non-redundant, and highly relevant feature
set is useful in practice, as it is interpretable and in-
creases the parsing (reranking) speed.
To realize this goal, we propose a new boosting-
based reranking algorithm based on the all-subtrees
representation. First, we describe the architecture of
our reranking method. Second, we show a connec-
tion between boosting and SVMs, and describe how
the algorithm realizes the sparse feature representa-
</bodyText>
<equation confidence="0.995525">
− log exp w · [&apos;(yi) − ,D(y)]
YEY(Xi)
E max(0, 1 − w · [4)(yi) − 4)(y)])
YEY(Xi)
E exp − w · [4)(yi) − 4)(y)]
YEY(Xi)
</equation>
<page confidence="0.625376">
190
</page>
<figureCaption confidence="0.945898">
Figure 1: Labeled ordered tree and subtree relation
tion described above.
</figureCaption>
<subsectionHeader confidence="0.996474">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.951339411764706">
Let us introduce a labeled ordered tree (or simply
’tree’), its definition and notations, first.
Definition 1 Labeled ordered tree (Tree)
A labeled ordered tree is a tree where each node is
associated with a label and is ordered among its sib-
lings, that is, there is a first child, second child, third
child, etc.
Definition 2 Subtree
Let t and u be labeled ordered trees. We say that t
matches u, or t is a subtree of u (t ⊆ u), if there is a
one-to-one function 0 from nodes in t to u, satisfying
the conditions: (1) 0 preserves the parent-daughter
relation, (2) 0 preserves the sibling relation, (3) 0
preserves the labels.
We denote the number of nodes in t as |t|. Figure 1
shows an example of a labeled ordered tree and its
subtree and non-subtree.
</bodyText>
<subsectionHeader confidence="0.999753">
3.2 Feature space given by subtrees
</subsectionHeader>
<bodyText confidence="0.999975333333333">
We first assume that a parse tree y is represented in
a labeled ordered tree. Note that the outputs of part-
of-speech tagging, shallow parsing, and dependency
analysis can be modeled as labeled ordered trees.
The feature set F consists of all subtrees seen in
the training data, i.e.,
</bodyText>
<equation confidence="0.819062">
F = ∪i,yEY(xi){t  |t ⊆ y}.
</equation>
<bodyText confidence="0.9980985">
The feature mapping Φ(y) is then given by letting
the existence of a tree t be a single dimension, i.e.,
</bodyText>
<equation confidence="0.996573">
Φ(y) = {I(t1 ⊆ y), ..., I(tm ⊆ y)} ∈ {0,1}m,
</equation>
<bodyText confidence="0.9921926">
where I(·) is the indicator function, m = |F|, and
{t1, ... , tm} ∈ F. The feature space is essentially
the same as that of tree kernel †
†Strictly speaking, tree kernel uses the cardinality of each
subtree
</bodyText>
<subsectionHeader confidence="0.999484">
3.3 RankBoost algorithm
</subsectionHeader>
<bodyText confidence="0.999941888888889">
The parameter estimation method we adopt is a vari-
ant of the RankBoost algorithm introduced in (Fre-
und et al., 2003). Collins et al. used RankBoost to
parse reranking tasks (Collins, 2000; Collins, 2002).
The algorithm proceeds for K iterations and tries to
minimize the BoostLoss for given training data$.
At each iteration, a single feature (hypothesis) is
chosen, and its weight is updated.
Suppose we have current parameters:
</bodyText>
<equation confidence="0.944458">
w = {w1, w2, ... , wm} ∈ Rm.
</equation>
<bodyText confidence="0.998064666666667">
New parameters w*(k,δ) ∈ Rm are then given by
selecting a single feature k and updating the weight
through an increment S:
</bodyText>
<equation confidence="0.985717">
*
w (k,δ) = {w1, w2, ... , wk + S, ... , wm}.
After the update, the new loss is given:
ELoss(w�(k,a)) = exp − w *(k,a) - [Φ(yi) − Φ(y)] . (1)
i, YEY(Xi)
</equation>
<bodyText confidence="0.9927255">
The RankBoost algorithm iteratively selects the op-
timal pair hˆk, ˆSi that minimizes the loss, i.e.,
</bodyText>
<equation confidence="0.991311">
h ˆk, ˆSi = argmin
(k,δ) Loss(w*(k,δ)).
</equation>
<bodyText confidence="0.998328">
By setting the differential of (1) at 0, the following
optimal solutions are obtained:
</bodyText>
<equation confidence="0.877808428571429">
1
k = argmax �W� 1�i&apos;±
— �W� , and S = log k , (2)
=1,...,� 2 Wk
where Wkb = Ei,yEY(xi) D(yi, y) · I[I(tk ⊆ yi) −
I(tk ⊆ y) = b], b ∈{+1, −1}, and D(yi, y) =
exp ( − w · [Φ(yi) − Φ(y)]).
</equation>
<bodyText confidence="0.9693065">
Following (Freund et al., 2003; Collins, 2000), we
introduce smoothing to prevent the case when either
Wk+ or Wk� is 0 §:
E, where Z = D(yi, y) and 2 E R+.
The function Y(x) is usually performed by a
probabilistic history-based parser, which can output
not only a parse tree but the log probability of the
*In our experiments, optimal settings for K were selected
by using development data.
§For simplicity, we fix 2 at 0.001 in all our experiments.
</bodyText>
<equation confidence="0.965864625">
log
1
δ=
2
Wk + 2Z
W�_
+ 2Z ki,YEY(Xi)
191
</equation>
<bodyText confidence="0.910346">
tree. We incorporate the log probability into the
reranking by using it as a feature:
</bodyText>
<equation confidence="0.999729">
-b(y) = {L(y), I(t1 C y), ... , I(tm C y)}, and
w = {w0,w1,w2,...,wm},
</equation>
<bodyText confidence="0.999983571428571">
where L(y) is the log probability of a tree y un-
der the base parser and w0 is the parameter of L(y).
Note that the update algorithm (2) does not allow us
to calculate the parameter w0, since (2) is restricted
to binary features. To prevent this problem, we use
the approximation technique introduced in (Freund
et al., 2003).
</bodyText>
<subsectionHeader confidence="0.996459">
3.4 Sparse feature representation
</subsectionHeader>
<bodyText confidence="0.999945038461538">
Recent studies (Schapire et al., 1997; R¨atsch, 2001)
have shown that both boosting and SVMs (Vapnik,
1998) work according to similar strategies: con-
structing optimal parameters w that maximize the
smallest margin between positive and negative ex-
amples. The critical difference is the definition of
margin or the way they regularize the vector w.
(R¨atsch, 2001) shows that the iterative feature selec-
tion performed in boosting asymptotically realizes
an l1-norm ||w||1 regularization. In contrast, it is
well known that SVMs are reformulated as an l2-
norm ||w||2 regularized algorithm.
The relationship between two regularizations has
been studied in the machine learning community.
(Perkins et al., 2003) reported that l1-norm should
be chosen for a problem where most given features
are irrelevant. On the other hand, l2-norm should be
chosen when most given features are relevant. An
advantage of the l1-norm regularizer is that it often
leads to sparse solutions where most wk are exactly
0. The features assigned zero weight are thought to
be irrelevant features as regards classifications.
The l1-norm regularization is useful for our set-
ting, since most features (subtrees) are redundant
and irrelevant, and these redundant features are au-
tomatically eliminated.
</bodyText>
<sectionHeader confidence="0.994691" genericHeader="method">
4 Efficient Computation
</sectionHeader>
<bodyText confidence="0.966346">
In each boosting iteration, we have to solve the fol-
lowing optimization problem:
</bodyText>
<equation confidence="0.984328666666667">
�
k = argmax gain(tk),
k=1,...,m
</equation>
<bodyText confidence="0.985957727272727">
where gain(tk) = VWk+ − VW; .
It is non-trivial to find the optimal tree tˆk that maxi-
mizes gain(tk), since the number of subtrees is ex-
ponential to its size. In fact, the problem is known
to be NP-hard (Yang, 2004). However, in real appli-
cations, the problem is manageable, since the max-
imum number of subtrees is usually bounded by a
constant. To solve the problem efficiently, we now
adopt a variant of the branch-and-bound algorithm,
similar to that described in (Kudo and Matsumoto,
2004)
</bodyText>
<subsectionHeader confidence="0.988596">
4.1 Efficient Enumeration of Trees
</subsectionHeader>
<bodyText confidence="0.924318722222222">
Abe and Zaki independently proposed an efficient
method, rightmost-extension, for enumerating all
subtrees from a given tree (Abe et al., 2002; Zaki,
2002). First, the algorithm starts with a set of trees
consisting of single nodes, and then expands a given
tree of size (n−1) by attaching a new node to it to
obtain trees of size n. However, it would be inef-
ficient to expand nodes at arbitrary positions of the
tree, as duplicated enumeration is inevitable. The
algorithm, rightmost extension, avoids such dupli-
cated enumerations by restricting the position of at-
tachment. Here we give the definition of rightmost
extension to describe this restriction in detail.
Definition 3 Rightmost Extension (Abe et al., 2002;
Zaki, 2002)
Let t and t&apos; be labeled ordered trees. We say t&apos; is a
rightmost extension of t, if and only if t and t&apos; satisfy
the following three conditions:
</bodyText>
<listItem confidence="0.9903725">
(1) t&apos; is created by adding a single node to t, (i.e.,
t C t&apos; and |t |+ 1 = |t&apos;|).
(2) A node is added to a node existing on the unique
path from the root to the rightmost leaf (rightmost-
path) in t.
(3) A node is added as the rightmost sibling.
</listItem>
<bodyText confidence="0.9995722">
Consider Figure 2, which illustrates example tree t
with labels drawn from the set L = {a, b, c}. For
the sake of convenience, each node in this figure has
its original number (depth-first enumeration). The
rightmost-path of the tree t is (a(c(b))), and it oc-
curs at positions 1, 4 and 6 respectively. The set of
rightmost extended trees is then enumerated by sim-
ply adding a single node to a node on the rightmost
path. Since there are three nodes on the rightmost
path and the size of the label set is 3 (= |L|), a to-
</bodyText>
<page confidence="0.473321">
192
</page>
<figureCaption confidence="0.998694">
Figure 2: Rightmost extension
</figureCaption>
<bodyText confidence="0.99932275">
tal of 9 trees are enumerated from the original tree
t. By repeating the rightmost-extension process re-
cursively, we can create a search space in which all
trees drawn from the set L are enumerated.
</bodyText>
<subsectionHeader confidence="0.986198">
4.2 Pruning
</subsectionHeader>
<bodyText confidence="0.999909">
Rightmost extension defines a canonical search
space in which we can enumerate all subtrees from
a given set of trees. Here we consider an upper
bound of the gain that allows subspace pruning in
this canonical search space. The following obser-
vation provides a convenient way of computing an
upper bound of the gain(tk) for any super-tree tk&apos;
of tk.
</bodyText>
<equation confidence="0.8563169">
Observation 1 Upper bound of the gain(tk)
For any tk&apos; tk, the gain of tk&apos; is bounded by
µ(tk): -
q q
gain(tk0) = Wk0 � − Wk0 �
q q
&lt; max( W � k0, W k0 � )
q q
&lt; max( Wk � , Wk � ) = µ(tk),
since tk0 _D tk =:&gt;&apos; Wbk0 &lt; Wbk, b E {+1, −1}.
</equation>
<bodyText confidence="0.999728545454545">
We can efficiently prune the search space spanned
by the rightmost extension using the upper bound of
gain µ(t). During the traverse of the subtree lattice
built by the recursive process of rightmost extension,
we always maintain the temporally suboptimal gain
T of all the previously calculated gains. If µ(t) &lt; T,
the gain of any super-tree t&apos; Q t is no greater than T,
and therefore we can safely prune the search space
spanned from the subtree t. In contrast, if µ(t) &gt; T,
we cannot prune this space, since there might be a
super-tree t&apos; Q t such that gain(t&apos;) &gt; T.
</bodyText>
<subsectionHeader confidence="0.994681">
4.3 Ad-hoc techniques
</subsectionHeader>
<bodyText confidence="0.9991735">
In real applications, we also employ the following
practical methods to reduce the training costs.
</bodyText>
<listItem confidence="0.950301">
• Size constraint
</listItem>
<bodyText confidence="0.9996154">
Larger trees are usually less effective to discrimi-
nation. Thus, we give a size threshold s, and use
subtrees whose size is no greater than s. This con-
straint is easily realized by controlling the right-
most extension according to the size of the trees.
</bodyText>
<listItem confidence="0.907409">
• Frequency constraint
</listItem>
<bodyText confidence="0.999893555555555">
The frequency-based cut-off has been widely used
in feature selections. We employ a frequency
threshold f, and use subtrees seen on at least one
parse for at least f different sentences. Note that
a similar branch-and-bound technique can also be
applied to the cut-off. When we find that the fre-
quency of a tree t is no greater than f, we can safely
prune the space spanned from t as the frequencies
of any super-trees t&apos; Q t are also no greater than f.
</bodyText>
<listItem confidence="0.981581">
• Pseudo iterations
</listItem>
<bodyText confidence="0.999197">
After several 5- or 10-iterations of boosting, we al-
ternately perform 100- or 300 pseudo iterations, in
which the optimal feature (subtree) is selected from
the cache that maintains the features explored in the
previous iterations. The idea is based on our ob-
servation that a feature in the cache tends to be re-
used as the number of boosting iterations increases.
Pseudo iterations converge very fast, and help the
branch-and-bound algorithm find new features that
are not in the cache.
</bodyText>
<sectionHeader confidence="0.999806" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999728">
5.1 Parsing Wall Street Journal Text
</subsectionHeader>
<bodyText confidence="0.999213">
In our experiments, we used the same data set that
used in (Collins, 2000). Sections 2-21 of the Penn
Treebank were used as training data, and section
23 was used as test data. The training data con-
tains about 40,000 sentences, each of which has an
average of 27 distinct parses. Of the 40,000 train-
ing sentences, the first 36,000 sentences were used
to perform the RankBoost algorithm. The remain-
ing 4,000 sentences were used as development data.
Model2 of (Collins, 1999) was used to parse both
the training and test data.
To capture the lexical information of the parse
trees, we did not use a standard CFG tree but a
lexicalized-CFG tree where each non-terminal node
has an extra lexical node labeled with the head word
of the constituent. Figure 3 shows an example of the
lexicalized-CFG tree used in our experiments. The
</bodyText>
<figure confidence="0.986837875">
rightmost extension
t’
1
a
{a,b,c
}
t
c
</figure>
<page confidence="0.516157">
7
</page>
<figure confidence="0.992788774193548">
1
a
3 c 5 a 6 b
1
a
2b 4
c
2b 4
c
3
1
a
3 c 5 a 6 b
2b 4
c
{a,b,c
}
{a,b,c
}
L={a,b,c
}
b
2 4
rightmost- path
c 5 a 6 b
3 c 5 a 6 b
7
7
193
TOP
S
</figure>
<bodyText confidence="0.99976284">
size parameter s and frequency parameter f were ex-
perimentally set at 6 and 10, respectively. As the
data set is very large, it is difficult to employ the ex-
periments with more unrestricted parameters.
Table 1 lists results on test data for the Model2 of
(Collins, 1999), for several previous studies, and for
our best model. We achieve recall and precision of
89.3/%89.6% and 89.9%/90.1% for sentences with
&lt; 100 words and &lt; 40 words, respectively. The
method shows a 1.2% absolute improvement in av-
erage precision and recall (from 88.2% to 89.4% for
sentences &lt; 100 words), a 10.1% relative reduc-
tion in error. (Collins, 2000) achieved 89.6%/89.9%
recall and precision for the same datasets (sen-
tences &lt; 100 words) using boosting and manu-
ally constructed features. (Charniak, 2000) extends
PCFG and achieves similar performance to (Collins,
2000). The tree kernel method of (Collins and
Duffy, 2002) uses the all-subtrees representation and
achieves 88.6%/88.9% recall and precision, which
are slightly worse than the results obtained with our
model. (Bod, 2001) also uses the all-subtrees repre-
sentation with a very different parameter estimation
method, and realizes 90.06%/90.08% recall and pre-
cision for sentences of &lt; 40 words.
</bodyText>
<subsectionHeader confidence="0.999393">
5.2 Shallow Parsing
</subsectionHeader>
<bodyText confidence="0.9999023">
We used the same data set as the CoNLL 2000
shared task (Tjong Kim Sang and Buchholz, 2000).
Sections 15-18 of the Penn Treebank were used as
training data, and section 20 was used as test data.
As a baseline model, we used a shallow parser
based on Conditional Random Fields (CRFs), very
similar to that described in (Sha and Pereira, 2003).
CRFs have shown remarkable results in a number
of tagging and chunking tasks in NLP. n-best out-
puts were obtained by a combination of forward
</bodyText>
<table confidence="0.999689285714286">
MODEL &lt; 40 Words (2245 sentences)
LR LP CBs 0 CBs 2 CBs
CO99 88.5% 88.7% 0.92 66.7% 87.1%
CH00 90.1% 90.1% 0.74 70.1% 89.6%
CO00 90.1% 90.4% 0.74 70.3% 89.6%
CO02 89.1% 89.4% 0.85 69.3% 88.2%
Boosting 89.9% 90.1% 0.77 70.5% 89.4%
MODEL &lt; 100 Words (2416 sentences)
LR LP CBs 0 CBs 2 CBs
CO99 88.1% 88.3% 1.06 64.0% 85.1%
CH00 89.6% 89.5% 0.88 67.6% 87.7%
CO00 89.6% 89.9% 0.87 68.3% 87.7%
CO02 88.6% 88.9% 0.99 66.5% 86.3%
Boosting 89.3% 89.6% 0.90 67.9% 87.5%
</table>
<tableCaption confidence="0.999148">
Table 1: Results for section 23 of the WSJ Treebank
</tableCaption>
<bodyText confidence="0.999160058823529">
LR/LP = labeled recall/precision. CBs is the average number
of cross brackets per sentence. 0 CBs, and 2CBs are the per-
centage of sentences with 0 or &lt; 2 crossing brackets, respec-
tively. COL99 = Model 2 of (Collins, 1999). CH00 = (Char-
niak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy,
2002).
Viterbi search and backward A* search. Note that
this search algorithm yields optimal n-best results
in terms of the CRFs score. Each sentence has at
most 20 distinct parses. The log probability from
the CRFs shallow parser was incorporated into the
reranking. Following (Collins, 2000), the training
set was split into 5 portions, and the CRFs shallow
parser was trained on 4/5 of the data, then used to
decode the remaining 1/5. The outputs of the base
parser, which consist of base phrases, were con-
verted into right-branching trees by assuming that
two adjacent base phrases are in a parent-child re-
lationship. Figure 4 shows an example of the tree
for shallow parsing task. We also put two virtual
nodes, left/right boundaries, to capture local transi-
tions. The size parameter s and frequency parameter
f were experimentally set at 6 and 5, respectively.
Table 2 lists results on test data for the baseline
CRFs parser, for several previous studies, and for
our best model. Our model achieves a 94.12 F-
measure, and outperforms the baseline CRFs parser
and the SVMs parser (Kudo and Matsumoto, 2001).
(Zhang et al., 2002) reported a higher F-measure
with a generalized winnow using additional linguis-
tic features. The accuracy of our model is very simi-
lar to that of (Zhang et al., 2002) without using such
additional features. Table 3 shows the results for our
best model per chunk type.
</bodyText>
<figure confidence="0.98467425">
VP
(saw) VBD
NN
a girl
</figure>
<figureCaption confidence="0.808938">
Figure 3: Lexicalized CFG tree for WSJ parsing
</figureCaption>
<figure confidence="0.951712444444444">
head word, e.g., (saw), is put as a leftmost constituent
(saw) NP
(I) PRP
I
NP
saw (girl) DT
194
TOP
NP
</figure>
<figureCaption confidence="0.994419">
Figure 4: Tree representation for shallow parsing
</figureCaption>
<table confidence="0.981058166666667">
Represented in a right-branching tree with two virtual nodes
MODEL F3_i
CRFs (baseline) 93.76
8 SVMs-voting (Kudo and Matsumoto, 2001) 93.91
RW + linguistic features (Zhang et al., 2002) 94.17
Boosting (our model) 94.12
</table>
<tableCaption confidence="0.999492">
Table 2: Results of shallow parsing
</tableCaption>
<note confidence="0.55286">
Fp=1 is the harmonic mean of precision and recall.
</note>
<sectionHeader confidence="0.999664" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.99417">
6.1 Interpretablity and Efficiency
</subsectionHeader>
<bodyText confidence="0.9999464">
The numbers of active (non-zero) features selected
by boosting are around 8,000 and 3,000 in the WSJ
parsing and shallow parsing, respectively. Although
almost all the subtrees are used as feature candi-
dates, boosting selects a small and highly relevant
subset of features. When we explicitly enumerate
the subtrees used in tree kernel, the number of ac-
tive features might amount to millions or more. Note
that the accuracies under such sparse feature spaces
are still comparable to those obtained with tree ker-
nel. This result supports our first intuition that we
do not always need all the subtrees to construct the
parameters.
The sparse feature representations are useful in
practice as they allow us to analyze what kinds of
features are relevant. Table 4 shows examples of
active features along with their weights wk. In the
shallow parsing tasks, subordinate phrases (SBAR)
are difficult to analyze without seeing long depen-
dencies. Subordinate phrases usually precede a sen-
tence (NP and VP). However, Markov-based shal-
low parsers, such as MEMM or CRFs, cannot cap-
ture such a long dependency. Our model automat-
ically selects useful subtrees to obtain an improve-
ment on subordinate phrases. It is interesting that the
</bodyText>
<table confidence="0.999975916666667">
Precision Recall Fa=1
ADJP 80.35% 73.41% 76.72
ADVP 83.88% 82.33% 83.10
CONJP 42.86% 66.67% 52.17
INTJ 50.00% 50.00% 50.00
LST 0.00% 0.00% 0.00
NP 94.45% 94.36% 94.41
PP 97.24% 98.07% 97.65
PRT 76.92% 75.47% 76.19
SBAR 90.70% 89.35% 90.02
VP 93.95% 94.72% 94.33
Overall 94.11% 94.13% 94.12
</table>
<tableCaption confidence="0.999916">
Table 3: Results of shallow parsing per chunk type
</tableCaption>
<bodyText confidence="0.999956272727273">
tree (SBAR(IN(for))(NP(VP(TO)))) has a large positive
weight, while the tree (SBAR((IN(for))(NP(O)))) has a
negative weight. The improvement on subordinate
phrases is considerable. We achieve 19% of the rel-
ative error reduction for subordinate phrase (from
87.68 to 90.02 in F-measure)
The testing speed of our model is much higher
than that of other models. The speeds of rerank-
ing for WSJ parsing and shallow parsing are 0.055
sec./sent. and 0.042 sec./sent. respectively, which
are fast enough for real applications If.
</bodyText>
<subsectionHeader confidence="0.996542">
6.2 Relationship to previous work
</subsectionHeader>
<bodyText confidence="0.999821619047619">
Tree kernel uses the all-subtrees representation not
explicitly but implicitly by reducing the problem to
the calculation of the inner-products of two trees.
The implicit calculation yields a practical computa-
tion in training. However, in testing, kernel meth-
ods require a number of kernel evaluations, which
are too heavy to allow us to realize real applications.
Moreover, tree kernel needs to incorporate a decay
factor to downweight the contribution of larger sub-
trees. It is non-trivial to set the optimal decay factor
as the accuracies are sensitive to its selection.
Similar to our model, data oriented parsing (DOP)
methods (Bod, 1998) deal with the all-subtrees rep-
resentation explicitly. Since the exact computa-
tion of scores for DOP is NP-complete, several ap-
proximations are employed to perform an efficient
parsing. The critical difference between our model
and DOP is that our model leads to an extremely
sparse solution and automatically eliminates redun-
dant subtrees. With the DOP methods, (Bod, 2001)
also employs constraints (e.g., depth of subtrees) to
</bodyText>
<figure confidence="0.936690230769231">
¶We ran these tests on a Linux PC with Pentium 4 3.2 Ghz.
PRP
(L) I (R)
VP
VBD
(L) saw (R)
NP
DT
(L) a
NN
girl (R)
EOS
195
</figure>
<table confidence="0.984199227272727">
WSJ parsing
W active trees that contain the word “in”
0.3864 (VP(NP(NNS(plants)))(PP(in)))
0.3326 (VP(VP(PP)(PP(in)))(VP))
0.2196 (NP(VP(VP(PP)(PP(in)))))
0.1748 (S(NP(NNP))(PP(in)(NP)))
... ...
-1.1217 (PP(in)(NP(NP(effect))))
-1.1634 (VP(yield)(PP(PP))(PP(in)))
-1.3574 (NP(PP(in)(NP(NN(way)))))
-1.8030 (NP(PP(in)(NP(trading)(JJ))))
shallow parsing
W active trees that contain the phrase “SBAR”
1.4500 (SBAR(IN(for))(NP(VP(TO))))
0.6177 (VP(SBAR(NP(VBD)))
0.6173 (SBAR(NP(VP(“))))
0.5644 (VP(SBAR(NP(VP(JJ)))))
.. ..
-0.9034 (SBAR(IN(for))(NP(O)))
-0.9181 (SBAR(NP(O)))
-1.0695 (ADVP(NP(SBAR(NP(VP)))))
-1.1699 (SBAR(NP(NN)(NP)))
</table>
<tableCaption confidence="0.999621">
Table 4: Examples of active features (subtrees)
</tableCaption>
<bodyText confidence="0.996198333333333">
All trees are represented in S-expression. In the shallow parsing
task, O is a special phrase that means “out of chunk”.
select relevant subtrees and achieves the best results
for WSJ parsing. However, these techniques are not
based on the regularization framework focused on
this paper and do not always eliminate all the re-
dundant subtrees. Even using the methods of (Bod,
2001), millions of subtrees are still exploited, which
leads to inefficiency in real problems.
</bodyText>
<sectionHeader confidence="0.999489" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999940888888889">
In this paper, we presented a new application of
boosting for parse reranking, in which all subtrees
are potentially used as distinct features. Although
this set-up greatly increases the feature space, the
11-norm regularization performed by boosting se-
lects a compact and relevant feature set. Our model
achieved a comparable or even better accuracy than
kernel methods even with an extremely small num-
ber of features (subtrees).
</bodyText>
<sectionHeader confidence="0.998463" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922448275862">
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and
Setsuo Arikawa. 2002. Optimized substructure discovery
for semi-structured data. In Proc. of PKDD, pages 1–14.
Rens Bod. 1998. Beyond Grammar: An Experience Based The-
ory of Language. CSLI Publications/Cambridge University
Press.
Rens Bod. 2001. What is the minimal set of fragments that
achieves maximal parse accuracy? In Proc. of ACL, pages
66–73.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. ofNAACL, pages 132–139.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proc. ofACL.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. ofICML, pages 175–182.
Michael Collins. 2002. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In Proc. of
ACL, pages 489–496.
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for combining
preferences. Journal ofMachine Learning Research, 4:933–
969.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support
vector machines. In Proc. ofNAACL, pages 192–199.
Taku Kudo and Yuji Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In Proc. of
EMNLP, pages 301–308.
Simon Perkins, Kevin Lacker, and James Thiler. 2003. Graft-
ing: Fast, incremental feature selection by gradient descent
in function space. Journal of Machine Learning Research,
3:1333–1356.
Gunnar. R¨atsch. 2001. Robust Boosting via Convex Optimiza-
tion. Ph.D. thesis, Department of Computer Science, Uni-
versity of Potsdam.
Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun
Lee. 1997. Boosting the margin: a new explanation for the
effectiveness of voting methods. In Proc. of ICML, pages
322–330.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proc. of HLT-NAACL, pages
213–220.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In Proc.
of CoNLL-2000 and LLL-2000, pages 127–132.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
Guizhen Yang. 2004. The complexity of mining maximal fre-
quent itemsets and maximal frequent patterns. In Proc. of
SIGKDD.
Mohammed Zaki. 2002. Efficiently mining frequent trees in a
forest. In Proc. of SIGKDD, pages 71–80.
Tong Zhang, Fred Damerau, and David Johnson. 2002. Text
chunking based on a generalization of winnow. Journal of
Machine Learning Research, 2:615–637.
</reference>
<page confidence="0.952865">
196
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974217">
<title confidence="0.997422">Boosting-based parse reranking with subtree features</title>
<author confidence="0.999661">Kudo Suzuki Hideki Isozaki</author>
<affiliation confidence="0.99967">NTT Communication Science Laboratories.</affiliation>
<address confidence="0.986541">2-4 Hikaridai, Seika-cho, Soraku, Kyoto, Japan</address>
<abstract confidence="0.999354333333333">This paper introduces a new application of boosting for parse reranking. Several parsers have been proposed that utilize the all-subtrees representation (e.g., tree kernel and data oriented parsing). This paper argues that such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees. We show how the boosting algorithm can be applied to the all-subtrees representation and how it selects a small and relevant feature set efficiently. Two experiments on parse reranking show that our method achieves comparable or even better performance than kernel methods and also improves the testing efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenji Abe</author>
</authors>
<title>Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and Setsuo Arikawa.</title>
<date>2002</date>
<booktitle>Proc. of PKDD,</booktitle>
<pages>1--14</pages>
<marker>Abe, 2002</marker>
<rawString>Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and Setsuo Arikawa. 2002. Optimized substructure discovery for semi-structured data. In Proc. of PKDD, pages 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Beyond Grammar: An Experience Based Theory of Language. CSLI</title>
<date>1998</date>
<publisher>Publications/Cambridge University Press.</publisher>
<contexts>
<context position="26069" citStr="Bod, 1998" startWordPosition="4493" endWordPosition="4494"> representation not explicitly but implicitly by reducing the problem to the calculation of the inner-products of two trees. The implicit calculation yields a practical computation in training. However, in testing, kernel methods require a number of kernel evaluations, which are too heavy to allow us to realize real applications. Moreover, tree kernel needs to incorporate a decay factor to downweight the contribution of larger subtrees. It is non-trivial to set the optimal decay factor as the accuracies are sensitive to its selection. Similar to our model, data oriented parsing (DOP) methods (Bod, 1998) deal with the all-subtrees representation explicitly. Since the exact computation of scores for DOP is NP-complete, several approximations are employed to perform an efficient parsing. The critical difference between our model and DOP is that our model leads to an extremely sparse solution and automatically eliminates redundant subtrees. With the DOP methods, (Bod, 2001) also employs constraints (e.g., depth of subtrees) to ¶We ran these tests on a Linux PC with Pentium 4 3.2 Ghz. PRP (L) I (R) VP VBD (L) saw (R) NP DT (L) a NN girl (R) EOS 195 WSJ parsing W active trees that contain the word</context>
</contexts>
<marker>Bod, 1998</marker>
<rawString>Rens Bod. 1998. Beyond Grammar: An Experience Based Theory of Language. CSLI Publications/Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>What is the minimal set of fragments that achieves maximal parse accuracy?</title>
<date>2001</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="19841" citStr="Bod, 2001" startWordPosition="3456" endWordPosition="3457">method shows a 1.2% absolute improvement in average precision and recall (from 88.2% to 89.4% for sentences &lt; 100 words), a 10.1% relative reduction in error. (Collins, 2000) achieved 89.6%/89.9% recall and precision for the same datasets (sentences &lt; 100 words) using boosting and manually constructed features. (Charniak, 2000) extends PCFG and achieves similar performance to (Collins, 2000). The tree kernel method of (Collins and Duffy, 2002) uses the all-subtrees representation and achieves 88.6%/88.9% recall and precision, which are slightly worse than the results obtained with our model. (Bod, 2001) also uses the all-subtrees representation with a very different parameter estimation method, and realizes 90.06%/90.08% recall and precision for sentences of &lt; 40 words. 5.2 Shallow Parsing We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Sections 15-18 of the Penn Treebank were used as training data, and section 20 was used as test data. As a baseline model, we used a shallow parser based on Conditional Random Fields (CRFs), very similar to that described in (Sha and Pereira, 2003). CRFs have shown remarkable results in a number of tagging and chun</context>
<context position="26443" citStr="Bod, 2001" startWordPosition="4551" endWordPosition="4552"> a decay factor to downweight the contribution of larger subtrees. It is non-trivial to set the optimal decay factor as the accuracies are sensitive to its selection. Similar to our model, data oriented parsing (DOP) methods (Bod, 1998) deal with the all-subtrees representation explicitly. Since the exact computation of scores for DOP is NP-complete, several approximations are employed to perform an efficient parsing. The critical difference between our model and DOP is that our model leads to an extremely sparse solution and automatically eliminates redundant subtrees. With the DOP methods, (Bod, 2001) also employs constraints (e.g., depth of subtrees) to ¶We ran these tests on a Linux PC with Pentium 4 3.2 Ghz. PRP (L) I (R) VP VBD (L) saw (R) NP DT (L) a NN girl (R) EOS 195 WSJ parsing W active trees that contain the word “in” 0.3864 (VP(NP(NNS(plants)))(PP(in))) 0.3326 (VP(VP(PP)(PP(in)))(VP)) 0.2196 (NP(VP(VP(PP)(PP(in))))) 0.1748 (S(NP(NNP))(PP(in)(NP))) ... ... -1.1217 (PP(in)(NP(NP(effect)))) -1.1634 (VP(yield)(PP(PP))(PP(in))) -1.3574 (NP(PP(in)(NP(NN(way))))) -1.8030 (NP(PP(in)(NP(trading)(JJ)))) shallow parsing W active trees that contain the phrase “SBAR” 1.4500 (SBAR(IN(for))(NP</context>
</contexts>
<marker>Bod, 2001</marker>
<rawString>Rens Bod. 2001. What is the minimal set of fragments that achieves maximal parse accuracy? In Proc. of ACL, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="19560" citStr="Charniak, 2000" startWordPosition="3415" endWordPosition="3416">ith more unrestricted parameters. Table 1 lists results on test data for the Model2 of (Collins, 1999), for several previous studies, and for our best model. We achieve recall and precision of 89.3/%89.6% and 89.9%/90.1% for sentences with &lt; 100 words and &lt; 40 words, respectively. The method shows a 1.2% absolute improvement in average precision and recall (from 88.2% to 89.4% for sentences &lt; 100 words), a 10.1% relative reduction in error. (Collins, 2000) achieved 89.6%/89.9% recall and precision for the same datasets (sentences &lt; 100 words) using boosting and manually constructed features. (Charniak, 2000) extends PCFG and achieves similar performance to (Collins, 2000). The tree kernel method of (Collins and Duffy, 2002) uses the all-subtrees representation and achieves 88.6%/88.9% recall and precision, which are slightly worse than the results obtained with our model. (Bod, 2001) also uses the all-subtrees representation with a very different parameter estimation method, and realizes 90.06%/90.08% recall and precision for sentences of &lt; 40 words. 5.2 Shallow Parsing We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Sections 15-18 of the Penn Treebank</context>
<context position="21276" citStr="Charniak, 2000" startWordPosition="3712" endWordPosition="3714">4 70.3% 89.6% CO02 89.1% 89.4% 0.85 69.3% 88.2% Boosting 89.9% 90.1% 0.77 70.5% 89.4% MODEL &lt; 100 Words (2416 sentences) LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% CO02 88.6% 88.9% 0.99 66.5% 86.3% Boosting 89.3% 89.6% 0.90 67.9% 87.5% Table 1: Results for section 23 of the WSJ Treebank LR/LP = labeled recall/precision. CBs is the average number of cross brackets per sentence. 0 CBs, and 2CBs are the percentage of sentences with 0 or &lt; 2 crossing brackets, respectively. COL99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy, 2002). Viterbi search and backward A* search. Note that this search algorithm yields optimal n-best results in terms of the CRFs score. Each sentence has at most 20 distinct parses. The log probability from the CRFs shallow parser was incorporated into the reranking. Following (Collins, 2000), the training set was split into 5 portions, and the CRFs shallow parser was trained on 4/5 of the data, then used to decode the remaining 1/5. The outputs of the base parser, which consist of base phrases, were converted into right-branching trees by assum</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. ofNAACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In</title>
<date>2002</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="2142" citStr="Collins and Duffy, 2002" startWordPosition="319" endWordPosition="322">udies, feature sets were given heuristically by simply preparing task-dependent feature templates (Collins, 2000; Collins, 2002). These ad-hoc solutions might provide us with reasonable levels of per*Currently, Google Japan Inc., taku@google.com formance. However, they are highly task dependent and require careful design to create the optimal feature set for each task. Kernel methods offer an elegant solution to these problems. They can work on a potentially huge or even infinite number of features without a loss of generalization. The best known kernel for modeling a tree is the tree kernel (Collins and Duffy, 2002), which argues that a feature vector is implicitly composed of the counts of subtrees. Although kernel methods are general and can cover almost all useful features, the set of subtrees that is used is extremely redundant. The main question addressed in this paper concerns whether it is possible to achieve a comparable or even better accuracy using just a small and non-redundant set of subtrees. In this paper, we present a new application of boosting for parse reranking. While tree kernel implicitly uses the all-subtrees representation, our boosting algorithm uses it explicitly. Although this s</context>
<context position="6027" citStr="Collins and Duffy, 2002" startWordPosition="986" endWordPosition="989">m all other candidates In early studies, the feature functions were given heuristically by simply preparing feature templates (Collins, 2000; Collins, 2002). However, such heuristic selections are task dependent and would not cover all useful features that contribute to overall accuracy. When we select the special family of loss functions, the problem can be reduced to a dual form that depends only on the inner products of two instances 4b(y1) · 4b(y2). This property is important as we can use a kernel trick and we do not need to provide an explicit feature function. For example, tree kernel (Collins and Duffy, 2002), one of the convolution kernels, implicitly maps the instance represented in a tree into all-subtrees space. Even though the feature space is large, inner products under this feature space can be calculated efficiently using dynamic programming. Tree kernel is more general than feature templates since it can use the all-subtrees representation without loss of efficiency. 3 RankBoost with subtree features A simple question related to kernel-based parse reranking asks whether all subtrees are really needed to construct the final parameters w. Suppose we have two large trees t and t&apos;, where t&apos; i</context>
<context position="19678" citStr="Collins and Duffy, 2002" startWordPosition="3431" endWordPosition="3434">everal previous studies, and for our best model. We achieve recall and precision of 89.3/%89.6% and 89.9%/90.1% for sentences with &lt; 100 words and &lt; 40 words, respectively. The method shows a 1.2% absolute improvement in average precision and recall (from 88.2% to 89.4% for sentences &lt; 100 words), a 10.1% relative reduction in error. (Collins, 2000) achieved 89.6%/89.9% recall and precision for the same datasets (sentences &lt; 100 words) using boosting and manually constructed features. (Charniak, 2000) extends PCFG and achieves similar performance to (Collins, 2000). The tree kernel method of (Collins and Duffy, 2002) uses the all-subtrees representation and achieves 88.6%/88.9% recall and precision, which are slightly worse than the results obtained with our model. (Bod, 2001) also uses the all-subtrees representation with a very different parameter estimation method, and realizes 90.06%/90.08% recall and precision for sentences of &lt; 40 words. 5.2 Shallow Parsing We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Sections 15-18 of the Penn Treebank were used as training data, and section 20 was used as test data. As a baseline model, we used a shallow parser based</context>
<context position="21330" citStr="Collins and Duffy, 2002" startWordPosition="3717" endWordPosition="3720">2% Boosting 89.9% 90.1% 0.77 70.5% 89.4% MODEL &lt; 100 Words (2416 sentences) LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% CO02 88.6% 88.9% 0.99 66.5% 86.3% Boosting 89.3% 89.6% 0.90 67.9% 87.5% Table 1: Results for section 23 of the WSJ Treebank LR/LP = labeled recall/precision. CBs is the average number of cross brackets per sentence. 0 CBs, and 2CBs are the percentage of sentences with 0 or &lt; 2 crossing brackets, respectively. COL99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy, 2002). Viterbi search and backward A* search. Note that this search algorithm yields optimal n-best results in terms of the CRFs score. Each sentence has at most 20 distinct parses. The log probability from the CRFs shallow parser was incorporated into the reranking. Following (Collins, 2000), the training set was split into 5 portions, and the CRFs shallow parser was trained on 4/5 of the data, then used to decode the remaining 1/5. The outputs of the base parser, which consist of base phrases, were converted into right-branching trees by assuming that two adjacent base phrases are in a parent-chi</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18235" citStr="Collins, 1999" startWordPosition="3157" endWordPosition="3158">ast, and help the branch-and-bound algorithm find new features that are not in the cache. 5 Experiments 5.1 Parsing Wall Street Journal Text In our experiments, we used the same data set that used in (Collins, 2000). Sections 2-21 of the Penn Treebank were used as training data, and section 23 was used as test data. The training data contains about 40,000 sentences, each of which has an average of 27 distinct parses. Of the 40,000 training sentences, the first 36,000 sentences were used to perform the RankBoost algorithm. The remaining 4,000 sentences were used as development data. Model2 of (Collins, 1999) was used to parse both the training and test data. To capture the lexical information of the parse trees, we did not use a standard CFG tree but a lexicalized-CFG tree where each non-terminal node has an extra lexical node labeled with the head word of the constituent. Figure 3 shows an example of the lexicalized-CFG tree used in our experiments. The rightmost extension t’ 1 a {a,b,c } t c 7 1 a 3 c 5 a 6 b 1 a 2b 4 c 2b 4 c 3 1 a 3 c 5 a 6 b 2b 4 c {a,b,c } {a,b,c } L={a,b,c } b 2 4 rightmost- path c 5 a 6 b 3 c 5 a 6 b 7 7 193 TOP S size parameter s and frequency parameter f were experiment</context>
<context position="21251" citStr="Collins, 1999" startWordPosition="3708" endWordPosition="3709">.6% CO00 90.1% 90.4% 0.74 70.3% 89.6% CO02 89.1% 89.4% 0.85 69.3% 88.2% Boosting 89.9% 90.1% 0.77 70.5% 89.4% MODEL &lt; 100 Words (2416 sentences) LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% CO02 88.6% 88.9% 0.99 66.5% 86.3% Boosting 89.3% 89.6% 0.90 67.9% 87.5% Table 1: Results for section 23 of the WSJ Treebank LR/LP = labeled recall/precision. CBs is the average number of cross brackets per sentence. 0 CBs, and 2CBs are the percentage of sentences with 0 or &lt; 2 crossing brackets, respectively. COL99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy, 2002). Viterbi search and backward A* search. Note that this search algorithm yields optimal n-best results in terms of the CRFs score. Each sentence has at most 20 distinct parses. The log probability from the CRFs shallow parser was incorporated into the reranking. Following (Collins, 2000), the training set was split into 5 portions, and the CRFs shallow parser was trained on 4/5 of the data, then used to decode the remaining 1/5. The outputs of the base parser, which consist of base phrases, were converted into right</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. ofICML,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="1630" citStr="Collins, 2000" startWordPosition="237" endWordPosition="238">es. One of the novel discriminative approaches is reranking, where discriminative machine learning algorithms are used to rerank the n-best outputs of generative or conditional parsers. The discriminative reranking methods allow us to incorporate various kinds of features to distinguish the correct parse tree from all other candidates. With such feature design flexibility, it is nontrivial to employ an appropriate feature set that has a good discriminative ability for parse reranking. In early studies, feature sets were given heuristically by simply preparing task-dependent feature templates (Collins, 2000; Collins, 2002). These ad-hoc solutions might provide us with reasonable levels of per*Currently, Google Japan Inc., taku@google.com formance. However, they are highly task dependent and require careful design to create the optimal feature set for each task. Kernel methods offer an elegant solution to these problems. They can work on a potentially huge or even infinite number of features without a loss of generalization. The best known kernel for modeling a tree is the tree kernel (Collins and Duffy, 2002), which argues that a feature vector is implicitly composed of the counts of subtrees. A</context>
<context position="5052" citStr="Collins, 2000" startWordPosition="822" endWordPosition="823">, xi) is an arbitrary loss function. We can design a variety of parameter estimation methods by changing the loss function. The following three loss functions, LogLoss, HingeLoss, and BoostLoss, have been widely used in parse reranking tasks. �LogLoss = HingeLoss = BoostLos = LogLoss is based on the standard maximum likelihood optimization, and is used with maximum entropy models. HingeLoss captures the errors only when w · [-b(yi) − -b(y)]) &lt; 1. This loss is closely related to the maximum margin strategy in SVMs (Vapnik, 1998). BoostLoss is analogous to the boosting algorithm and is used in (Collins, 2000; Collins, 2002). In the real setting, we cannot assume this condition. In this case, we select the parse tree y� that is the most similar to yi and take y� as the correct parse tree yi. 2.2 Definition of feature function It is non-trivial to define an appropriate feature function 4b(y) that has a good ability to distinguish the correct parse yi from all other candidates In early studies, the feature functions were given heuristically by simply preparing feature templates (Collins, 2000; Collins, 2002). However, such heuristic selections are task dependent and would not cover all useful featur</context>
<context position="9384" citStr="Collins, 2000" startWordPosition="1575" endWordPosition="1576">aining data, i.e., F = ∪i,yEY(xi){t |t ⊆ y}. The feature mapping Φ(y) is then given by letting the existence of a tree t be a single dimension, i.e., Φ(y) = {I(t1 ⊆ y), ..., I(tm ⊆ y)} ∈ {0,1}m, where I(·) is the indicator function, m = |F|, and {t1, ... , tm} ∈ F. The feature space is essentially the same as that of tree kernel † †Strictly speaking, tree kernel uses the cardinality of each subtree 3.3 RankBoost algorithm The parameter estimation method we adopt is a variant of the RankBoost algorithm introduced in (Freund et al., 2003). Collins et al. used RankBoost to parse reranking tasks (Collins, 2000; Collins, 2002). The algorithm proceeds for K iterations and tries to minimize the BoostLoss for given training data$. At each iteration, a single feature (hypothesis) is chosen, and its weight is updated. Suppose we have current parameters: w = {w1, w2, ... , wm} ∈ Rm. New parameters w*(k,δ) ∈ Rm are then given by selecting a single feature k and updating the weight through an increment S: * w (k,δ) = {w1, w2, ... , wk + S, ... , wm}. After the update, the new loss is given: ELoss(w�(k,a)) = exp − w *(k,a) - [Φ(yi) − Φ(y)] . (1) i, YEY(Xi) The RankBoost algorithm iteratively selects the opti</context>
<context position="17836" citStr="Collins, 2000" startWordPosition="3089" endWordPosition="3090">s After several 5- or 10-iterations of boosting, we alternately perform 100- or 300 pseudo iterations, in which the optimal feature (subtree) is selected from the cache that maintains the features explored in the previous iterations. The idea is based on our observation that a feature in the cache tends to be reused as the number of boosting iterations increases. Pseudo iterations converge very fast, and help the branch-and-bound algorithm find new features that are not in the cache. 5 Experiments 5.1 Parsing Wall Street Journal Text In our experiments, we used the same data set that used in (Collins, 2000). Sections 2-21 of the Penn Treebank were used as training data, and section 23 was used as test data. The training data contains about 40,000 sentences, each of which has an average of 27 distinct parses. Of the 40,000 training sentences, the first 36,000 sentences were used to perform the RankBoost algorithm. The remaining 4,000 sentences were used as development data. Model2 of (Collins, 1999) was used to parse both the training and test data. To capture the lexical information of the parse trees, we did not use a standard CFG tree but a lexicalized-CFG tree where each non-terminal node has</context>
<context position="19405" citStr="Collins, 2000" startWordPosition="3392" endWordPosition="3393"> s and frequency parameter f were experimentally set at 6 and 10, respectively. As the data set is very large, it is difficult to employ the experiments with more unrestricted parameters. Table 1 lists results on test data for the Model2 of (Collins, 1999), for several previous studies, and for our best model. We achieve recall and precision of 89.3/%89.6% and 89.9%/90.1% for sentences with &lt; 100 words and &lt; 40 words, respectively. The method shows a 1.2% absolute improvement in average precision and recall (from 88.2% to 89.4% for sentences &lt; 100 words), a 10.1% relative reduction in error. (Collins, 2000) achieved 89.6%/89.9% recall and precision for the same datasets (sentences &lt; 100 words) using boosting and manually constructed features. (Charniak, 2000) extends PCFG and achieves similar performance to (Collins, 2000). The tree kernel method of (Collins and Duffy, 2002) uses the all-subtrees representation and achieves 88.6%/88.9% recall and precision, which are slightly worse than the results obtained with our model. (Bod, 2001) also uses the all-subtrees representation with a very different parameter estimation method, and realizes 90.06%/90.08% recall and precision for sentences of &lt; 40 </context>
<context position="21298" citStr="Collins, 2000" startWordPosition="3715" endWordPosition="3716">% 89.4% 0.85 69.3% 88.2% Boosting 89.9% 90.1% 0.77 70.5% 89.4% MODEL &lt; 100 Words (2416 sentences) LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% CO02 88.6% 88.9% 0.99 66.5% 86.3% Boosting 89.3% 89.6% 0.90 67.9% 87.5% Table 1: Results for section 23 of the WSJ Treebank LR/LP = labeled recall/precision. CBs is the average number of cross brackets per sentence. 0 CBs, and 2CBs are the percentage of sentences with 0 or &lt; 2 crossing brackets, respectively. COL99 = Model 2 of (Collins, 1999). CH00 = (Charniak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy, 2002). Viterbi search and backward A* search. Note that this search algorithm yields optimal n-best results in terms of the CRFs score. Each sentence has at most 20 distinct parses. The log probability from the CRFs shallow parser was incorporated into the reranking. Following (Collins, 2000), the training set was split into 5 portions, and the CRFs shallow parser was trained on 4/5 of the data, then used to decode the remaining 1/5. The outputs of the base parser, which consist of base phrases, were converted into right-branching trees by assuming that two adjacent </context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. ofICML, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>489--496</pages>
<contexts>
<context position="1646" citStr="Collins, 2002" startWordPosition="239" endWordPosition="240">novel discriminative approaches is reranking, where discriminative machine learning algorithms are used to rerank the n-best outputs of generative or conditional parsers. The discriminative reranking methods allow us to incorporate various kinds of features to distinguish the correct parse tree from all other candidates. With such feature design flexibility, it is nontrivial to employ an appropriate feature set that has a good discriminative ability for parse reranking. In early studies, feature sets were given heuristically by simply preparing task-dependent feature templates (Collins, 2000; Collins, 2002). These ad-hoc solutions might provide us with reasonable levels of per*Currently, Google Japan Inc., taku@google.com formance. However, they are highly task dependent and require careful design to create the optimal feature set for each task. Kernel methods offer an elegant solution to these problems. They can work on a potentially huge or even infinite number of features without a loss of generalization. The best known kernel for modeling a tree is the tree kernel (Collins and Duffy, 2002), which argues that a feature vector is implicitly composed of the counts of subtrees. Although kernel m</context>
<context position="5068" citStr="Collins, 2002" startWordPosition="824" endWordPosition="825">itrary loss function. We can design a variety of parameter estimation methods by changing the loss function. The following three loss functions, LogLoss, HingeLoss, and BoostLoss, have been widely used in parse reranking tasks. �LogLoss = HingeLoss = BoostLos = LogLoss is based on the standard maximum likelihood optimization, and is used with maximum entropy models. HingeLoss captures the errors only when w · [-b(yi) − -b(y)]) &lt; 1. This loss is closely related to the maximum margin strategy in SVMs (Vapnik, 1998). BoostLoss is analogous to the boosting algorithm and is used in (Collins, 2000; Collins, 2002). In the real setting, we cannot assume this condition. In this case, we select the parse tree y� that is the most similar to yi and take y� as the correct parse tree yi. 2.2 Definition of feature function It is non-trivial to define an appropriate feature function 4b(y) that has a good ability to distinguish the correct parse yi from all other candidates In early studies, the feature functions were given heuristically by simply preparing feature templates (Collins, 2000; Collins, 2002). However, such heuristic selections are task dependent and would not cover all useful features that contribu</context>
<context position="9400" citStr="Collins, 2002" startWordPosition="1577" endWordPosition="1578">e., F = ∪i,yEY(xi){t |t ⊆ y}. The feature mapping Φ(y) is then given by letting the existence of a tree t be a single dimension, i.e., Φ(y) = {I(t1 ⊆ y), ..., I(tm ⊆ y)} ∈ {0,1}m, where I(·) is the indicator function, m = |F|, and {t1, ... , tm} ∈ F. The feature space is essentially the same as that of tree kernel † †Strictly speaking, tree kernel uses the cardinality of each subtree 3.3 RankBoost algorithm The parameter estimation method we adopt is a variant of the RankBoost algorithm introduced in (Freund et al., 2003). Collins et al. used RankBoost to parse reranking tasks (Collins, 2000; Collins, 2002). The algorithm proceeds for K iterations and tries to minimize the BoostLoss for given training data$. At each iteration, a single feature (hypothesis) is chosen, and its weight is updated. Suppose we have current parameters: w = {w1, w2, ... , wm} ∈ Rm. New parameters w*(k,δ) ∈ Rm are then given by selecting a single feature k and updating the weight through an increment S: * w (k,δ) = {w1, w2, ... , wk + S, ... , wm}. After the update, the new loss is given: ELoss(w�(k,a)) = exp − w *(k,a) - [Φ(yi) − Φ(y)] . (1) i, YEY(Xi) The RankBoost algorithm iteratively selects the optimal pair hˆk, ˆS</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron. In Proc. of ACL, pages 489–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj D Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<volume>4</volume>
<pages>969</pages>
<contexts>
<context position="9313" citStr="Freund et al., 2003" startWordPosition="1561" endWordPosition="1565">beled ordered trees. The feature set F consists of all subtrees seen in the training data, i.e., F = ∪i,yEY(xi){t |t ⊆ y}. The feature mapping Φ(y) is then given by letting the existence of a tree t be a single dimension, i.e., Φ(y) = {I(t1 ⊆ y), ..., I(tm ⊆ y)} ∈ {0,1}m, where I(·) is the indicator function, m = |F|, and {t1, ... , tm} ∈ F. The feature space is essentially the same as that of tree kernel † †Strictly speaking, tree kernel uses the cardinality of each subtree 3.3 RankBoost algorithm The parameter estimation method we adopt is a variant of the RankBoost algorithm introduced in (Freund et al., 2003). Collins et al. used RankBoost to parse reranking tasks (Collins, 2000; Collins, 2002). The algorithm proceeds for K iterations and tries to minimize the BoostLoss for given training data$. At each iteration, a single feature (hypothesis) is chosen, and its weight is updated. Suppose we have current parameters: w = {w1, w2, ... , wm} ∈ Rm. New parameters w*(k,δ) ∈ Rm are then given by selecting a single feature k and updating the weight through an increment S: * w (k,δ) = {w1, w2, ... , wk + S, ... , wm}. After the update, the new loss is given: ELoss(w�(k,a)) = exp − w *(k,a) - [Φ(yi) − Φ(y)</context>
<context position="11321" citStr="Freund et al., 2003" startWordPosition="1946" endWordPosition="1949"> selected by using development data. §For simplicity, we fix 2 at 0.001 in all our experiments. log 1 δ= 2 Wk + 2Z W�_ + 2Z ki,YEY(Xi) 191 tree. We incorporate the log probability into the reranking by using it as a feature: -b(y) = {L(y), I(t1 C y), ... , I(tm C y)}, and w = {w0,w1,w2,...,wm}, where L(y) is the log probability of a tree y under the base parser and w0 is the parameter of L(y). Note that the update algorithm (2) does not allow us to calculate the parameter w0, since (2) is restricted to binary features. To prevent this problem, we use the approximation technique introduced in (Freund et al., 2003). 3.4 Sparse feature representation Recent studies (Schapire et al., 1997; R¨atsch, 2001) have shown that both boosting and SVMs (Vapnik, 1998) work according to similar strategies: constructing optimal parameters w that maximize the smallest margin between positive and negative examples. The critical difference is the definition of margin or the way they regularize the vector w. (R¨atsch, 2001) shows that the iterative feature selection performed in boosting asymptotically realizes an l1-norm ||w||1 regularization. In contrast, it is well known that SVMs are reformulated as an l2- norm ||w||2</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferences. Journal ofMachine Learning Research, 4:933– 969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="22435" citStr="Kudo and Matsumoto, 2001" startWordPosition="3902" endWordPosition="3905">f base phrases, were converted into right-branching trees by assuming that two adjacent base phrases are in a parent-child relationship. Figure 4 shows an example of the tree for shallow parsing task. We also put two virtual nodes, left/right boundaries, to capture local transitions. The size parameter s and frequency parameter f were experimentally set at 6 and 5, respectively. Table 2 lists results on test data for the baseline CRFs parser, for several previous studies, and for our best model. Our model achieves a 94.12 Fmeasure, and outperforms the baseline CRFs parser and the SVMs parser (Kudo and Matsumoto, 2001). (Zhang et al., 2002) reported a higher F-measure with a generalized winnow using additional linguistic features. The accuracy of our model is very similar to that of (Zhang et al., 2002) without using such additional features. Table 3 shows the results for our best model per chunk type. VP (saw) VBD NN a girl Figure 3: Lexicalized CFG tree for WSJ parsing head word, e.g., (saw), is put as a leftmost constituent (saw) NP (I) PRP I NP saw (girl) DT 194 TOP NP Figure 4: Tree representation for shallow parsing Represented in a right-branching tree with two virtual nodes MODEL F3_i CRFs (baseline</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proc. ofNAACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>301--308</pages>
<contexts>
<context position="13260" citStr="Kudo and Matsumoto, 2004" startWordPosition="2253" endWordPosition="2256">cient Computation In each boosting iteration, we have to solve the following optimization problem: � k = argmax gain(tk), k=1,...,m where gain(tk) = VWk+ − VW; . It is non-trivial to find the optimal tree tˆk that maximizes gain(tk), since the number of subtrees is exponential to its size. In fact, the problem is known to be NP-hard (Yang, 2004). However, in real applications, the problem is manageable, since the maximum number of subtrees is usually bounded by a constant. To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004) 4.1 Efficient Enumeration of Trees Abe and Zaki independently proposed an efficient method, rightmost-extension, for enumerating all subtrees from a given tree (Abe et al., 2002; Zaki, 2002). First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (n−1) by attaching a new node to it to obtain trees of size n. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable. The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of a</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proc. of EMNLP, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Perkins</author>
<author>Kevin Lacker</author>
<author>James Thiler</author>
</authors>
<title>Grafting: Fast, incremental feature selection by gradient descent in function space.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1333</pages>
<contexts>
<context position="12064" citStr="Perkins et al., 2003" startWordPosition="2056" endWordPosition="2059">d SVMs (Vapnik, 1998) work according to similar strategies: constructing optimal parameters w that maximize the smallest margin between positive and negative examples. The critical difference is the definition of margin or the way they regularize the vector w. (R¨atsch, 2001) shows that the iterative feature selection performed in boosting asymptotically realizes an l1-norm ||w||1 regularization. In contrast, it is well known that SVMs are reformulated as an l2- norm ||w||2 regularized algorithm. The relationship between two regularizations has been studied in the machine learning community. (Perkins et al., 2003) reported that l1-norm should be chosen for a problem where most given features are irrelevant. On the other hand, l2-norm should be chosen when most given features are relevant. An advantage of the l1-norm regularizer is that it often leads to sparse solutions where most wk are exactly 0. The features assigned zero weight are thought to be irrelevant features as regards classifications. The l1-norm regularization is useful for our setting, since most features (subtrees) are redundant and irrelevant, and these redundant features are automatically eliminated. 4 Efficient Computation In each boo</context>
</contexts>
<marker>Perkins, Lacker, Thiler, 2003</marker>
<rawString>Simon Perkins, Kevin Lacker, and James Thiler. 2003. Grafting: Fast, incremental feature selection by gradient descent in function space. Journal of Machine Learning Research, 3:1333–1356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R¨atsch</author>
</authors>
<title>Robust Boosting via Convex Optimization.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Potsdam.</institution>
<marker>R¨atsch, 2001</marker>
<rawString>Gunnar. R¨atsch. 2001. Robust Boosting via Convex Optimization. Ph.D. thesis, Department of Computer Science, University of Potsdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoav Freund</author>
<author>Peter Bartlett</author>
<author>Wee Sun Lee</author>
</authors>
<title>Boosting the margin: a new explanation for the effectiveness of voting methods.</title>
<date>1997</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>322--330</pages>
<contexts>
<context position="11394" citStr="Schapire et al., 1997" startWordPosition="1956" endWordPosition="1959">in all our experiments. log 1 δ= 2 Wk + 2Z W�_ + 2Z ki,YEY(Xi) 191 tree. We incorporate the log probability into the reranking by using it as a feature: -b(y) = {L(y), I(t1 C y), ... , I(tm C y)}, and w = {w0,w1,w2,...,wm}, where L(y) is the log probability of a tree y under the base parser and w0 is the parameter of L(y). Note that the update algorithm (2) does not allow us to calculate the parameter w0, since (2) is restricted to binary features. To prevent this problem, we use the approximation technique introduced in (Freund et al., 2003). 3.4 Sparse feature representation Recent studies (Schapire et al., 1997; R¨atsch, 2001) have shown that both boosting and SVMs (Vapnik, 1998) work according to similar strategies: constructing optimal parameters w that maximize the smallest margin between positive and negative examples. The critical difference is the definition of margin or the way they regularize the vector w. (R¨atsch, 2001) shows that the iterative feature selection performed in boosting asymptotically realizes an l1-norm ||w||1 regularization. In contrast, it is well known that SVMs are reformulated as an l2- norm ||w||2 regularized algorithm. The relationship between two regularizations has </context>
</contexts>
<marker>Schapire, Freund, Bartlett, Lee, 1997</marker>
<rawString>Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. 1997. Boosting the margin: a new explanation for the effectiveness of voting methods. In Proc. of ICML, pages 322–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="20373" citStr="Sha and Pereira, 2003" startWordPosition="3545" endWordPosition="3548"> precision, which are slightly worse than the results obtained with our model. (Bod, 2001) also uses the all-subtrees representation with a very different parameter estimation method, and realizes 90.06%/90.08% recall and precision for sentences of &lt; 40 words. 5.2 Shallow Parsing We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Sections 15-18 of the Penn Treebank were used as training data, and section 20 was used as test data. As a baseline model, we used a shallow parser based on Conditional Random Fields (CRFs), very similar to that described in (Sha and Pereira, 2003). CRFs have shown remarkable results in a number of tagging and chunking tasks in NLP. n-best outputs were obtained by a combination of forward MODEL &lt; 40 Words (2245 sentences) LR LP CBs 0 CBs 2 CBs CO99 88.5% 88.7% 0.92 66.7% 87.1% CH00 90.1% 90.1% 0.74 70.1% 89.6% CO00 90.1% 90.4% 0.74 70.3% 89.6% CO02 89.1% 89.4% 0.85 69.3% 88.2% Boosting 89.9% 90.1% 0.77 70.5% 89.4% MODEL &lt; 100 Words (2416 sentences) LR LP CBs 0 CBs 2 CBs CO99 88.1% 88.3% 1.06 64.0% 85.1% CH00 89.6% 89.5% 0.88 67.6% 87.7% CO00 89.6% 89.9% 0.87 68.3% 87.7% CO02 88.6% 88.9% 0.99 66.5% 86.3% Boosting 89.3% 89.6% 0.90 67.9% 8</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of HLT-NAACL, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>In Proc. of CoNLL-2000 and LLL-2000,</booktitle>
<pages>127--132</pages>
<contexts>
<context position="20123" citStr="Sang and Buchholz, 2000" startWordPosition="3501" endWordPosition="3504"> boosting and manually constructed features. (Charniak, 2000) extends PCFG and achieves similar performance to (Collins, 2000). The tree kernel method of (Collins and Duffy, 2002) uses the all-subtrees representation and achieves 88.6%/88.9% recall and precision, which are slightly worse than the results obtained with our model. (Bod, 2001) also uses the all-subtrees representation with a very different parameter estimation method, and realizes 90.06%/90.08% recall and precision for sentences of &lt; 40 words. 5.2 Shallow Parsing We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Sections 15-18 of the Penn Treebank were used as training data, and section 20 was used as test data. As a baseline model, we used a shallow parser based on Conditional Random Fields (CRFs), very similar to that described in (Sha and Pereira, 2003). CRFs have shown remarkable results in a number of tagging and chunking tasks in NLP. n-best outputs were obtained by a combination of forward MODEL &lt; 40 Words (2245 sentences) LR LP CBs 0 CBs 2 CBs CO99 88.5% 88.7% 0.92 66.7% 87.1% CH00 90.1% 90.1% 0.74 70.1% 89.6% CO00 90.1% 90.4% 0.74 70.3% 89.6% CO02 89.1% 89.4% 0.85 69.3% 88.2% Boosting 89.9%</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. In Proc. of CoNLL-2000 and LLL-2000, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="4972" citStr="Vapnik, 1998" startWordPosition="809" endWordPosition="810">tion has the following form: L Loss(w) = L(w, -b(yi), xi), i=1 where L(w, -b(yi), xi) is an arbitrary loss function. We can design a variety of parameter estimation methods by changing the loss function. The following three loss functions, LogLoss, HingeLoss, and BoostLoss, have been widely used in parse reranking tasks. �LogLoss = HingeLoss = BoostLos = LogLoss is based on the standard maximum likelihood optimization, and is used with maximum entropy models. HingeLoss captures the errors only when w · [-b(yi) − -b(y)]) &lt; 1. This loss is closely related to the maximum margin strategy in SVMs (Vapnik, 1998). BoostLoss is analogous to the boosting algorithm and is used in (Collins, 2000; Collins, 2002). In the real setting, we cannot assume this condition. In this case, we select the parse tree y� that is the most similar to yi and take y� as the correct parse tree yi. 2.2 Definition of feature function It is non-trivial to define an appropriate feature function 4b(y) that has a good ability to distinguish the correct parse yi from all other candidates In early studies, the feature functions were given heuristically by simply preparing feature templates (Collins, 2000; Collins, 2002). However, su</context>
<context position="11464" citStr="Vapnik, 1998" startWordPosition="1969" endWordPosition="1970">corporate the log probability into the reranking by using it as a feature: -b(y) = {L(y), I(t1 C y), ... , I(tm C y)}, and w = {w0,w1,w2,...,wm}, where L(y) is the log probability of a tree y under the base parser and w0 is the parameter of L(y). Note that the update algorithm (2) does not allow us to calculate the parameter w0, since (2) is restricted to binary features. To prevent this problem, we use the approximation technique introduced in (Freund et al., 2003). 3.4 Sparse feature representation Recent studies (Schapire et al., 1997; R¨atsch, 2001) have shown that both boosting and SVMs (Vapnik, 1998) work according to similar strategies: constructing optimal parameters w that maximize the smallest margin between positive and negative examples. The critical difference is the definition of margin or the way they regularize the vector w. (R¨atsch, 2001) shows that the iterative feature selection performed in boosting asymptotically realizes an l1-norm ||w||1 regularization. In contrast, it is well known that SVMs are reformulated as an l2- norm ||w||2 regularized algorithm. The relationship between two regularizations has been studied in the machine learning community. (Perkins et al., 2003)</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guizhen Yang</author>
</authors>
<title>The complexity of mining maximal frequent itemsets and maximal frequent patterns.</title>
<date>2004</date>
<booktitle>In Proc. of SIGKDD.</booktitle>
<contexts>
<context position="12982" citStr="Yang, 2004" startWordPosition="2210" endWordPosition="2211">igned zero weight are thought to be irrelevant features as regards classifications. The l1-norm regularization is useful for our setting, since most features (subtrees) are redundant and irrelevant, and these redundant features are automatically eliminated. 4 Efficient Computation In each boosting iteration, we have to solve the following optimization problem: � k = argmax gain(tk), k=1,...,m where gain(tk) = VWk+ − VW; . It is non-trivial to find the optimal tree tˆk that maximizes gain(tk), since the number of subtrees is exponential to its size. In fact, the problem is known to be NP-hard (Yang, 2004). However, in real applications, the problem is manageable, since the maximum number of subtrees is usually bounded by a constant. To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004) 4.1 Efficient Enumeration of Trees Abe and Zaki independently proposed an efficient method, rightmost-extension, for enumerating all subtrees from a given tree (Abe et al., 2002; Zaki, 2002). First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (n−1) by attach</context>
</contexts>
<marker>Yang, 2004</marker>
<rawString>Guizhen Yang. 2004. The complexity of mining maximal frequent itemsets and maximal frequent patterns. In Proc. of SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Zaki</author>
</authors>
<title>Efficiently mining frequent trees in a forest.</title>
<date>2002</date>
<booktitle>In Proc. of SIGKDD,</booktitle>
<pages>71--80</pages>
<contexts>
<context position="13451" citStr="Zaki, 2002" startWordPosition="2283" endWordPosition="2284">tˆk that maximizes gain(tk), since the number of subtrees is exponential to its size. In fact, the problem is known to be NP-hard (Yang, 2004). However, in real applications, the problem is manageable, since the maximum number of subtrees is usually bounded by a constant. To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004) 4.1 Efficient Enumeration of Trees Abe and Zaki independently proposed an efficient method, rightmost-extension, for enumerating all subtrees from a given tree (Abe et al., 2002; Zaki, 2002). First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (n−1) by attaching a new node to it to obtain trees of size n. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable. The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of attachment. Here we give the definition of rightmost extension to describe this restriction in detail. Definition 3 Rightmost Extension (Abe et al., 2002; Zaki, 2002) Let t and t&apos; be labeled o</context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed Zaki. 2002. Efficiently mining frequent trees in a forest. In Proc. of SIGKDD, pages 71–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David Johnson</author>
</authors>
<title>Text chunking based on a generalization of winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--615</pages>
<contexts>
<context position="22457" citStr="Zhang et al., 2002" startWordPosition="3906" endWordPosition="3909">ed into right-branching trees by assuming that two adjacent base phrases are in a parent-child relationship. Figure 4 shows an example of the tree for shallow parsing task. We also put two virtual nodes, left/right boundaries, to capture local transitions. The size parameter s and frequency parameter f were experimentally set at 6 and 5, respectively. Table 2 lists results on test data for the baseline CRFs parser, for several previous studies, and for our best model. Our model achieves a 94.12 Fmeasure, and outperforms the baseline CRFs parser and the SVMs parser (Kudo and Matsumoto, 2001). (Zhang et al., 2002) reported a higher F-measure with a generalized winnow using additional linguistic features. The accuracy of our model is very similar to that of (Zhang et al., 2002) without using such additional features. Table 3 shows the results for our best model per chunk type. VP (saw) VBD NN a girl Figure 3: Lexicalized CFG tree for WSJ parsing head word, e.g., (saw), is put as a leftmost constituent (saw) NP (I) PRP I NP saw (girl) DT 194 TOP NP Figure 4: Tree representation for shallow parsing Represented in a right-branching tree with two virtual nodes MODEL F3_i CRFs (baseline) 93.76 8 SVMs-voting </context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>Tong Zhang, Fred Damerau, and David Johnson. 2002. Text chunking based on a generalization of winnow. Journal of Machine Learning Research, 2:615–637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>