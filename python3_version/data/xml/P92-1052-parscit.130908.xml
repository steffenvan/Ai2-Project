<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000549">
<title confidence="0.70208">
SEXTANT: EXPLORING UNEXPLORED CONTEXTS FOR
SEMANTIC EXTRACTION FROM SYNTACTIC ANALYSIS
</title>
<author confidence="0.972032">
Gregory Grefenstette
</author>
<affiliation confidence="0.98935">
Computer Science Department, University of Pittsburgh, Pittsburgh, PA 15260
</affiliation>
<note confidence="0.640673">
grefen@cs .pitt .edu
</note>
<sectionHeader confidence="0.984647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999057266666667">
For a very long time, it has been con-
sidered that the only way of automati-
cally extracting similar groups of words
from a text collection for which no se-
mantic information exists is to use docu-
ment co-occurrence data. But, with ro-
bust syntactic parsers that are becom-
ing more frequently available, syntacti-
cally recognizable phenomena about word
usage can be confidently noted in large
collections of texts. We present here a
new system called SEXTANT which uses
these parsers and the finer-grained con-
texts they produce to judge word similar-
ity.
</bodyText>
<sectionHeader confidence="0.972099" genericHeader="keywords">
BACKGROUND
</sectionHeader>
<bodyText confidence="0.983287455882354">
Many machine-based approaches to term sim-
ilarity, such as found in TRUMP (Jacobs
and Zernick 1988) and FERRET (Mauldin
1991), can be characterized as knowledge-rich
in that they presuppose that known lexical
items possess Conceptual Dependence(CD)-
like descriptions. Such an approach neces-
sitates a great amount of manual encoding
of semantic information and suffers from the
drawbacks of cost (in terms of initial coding,
coherence checking, maintenance after modi-
fications, and costs derivable from a host of
other software engineering concern); of do-
main dependence (a semantic structure de-
veloped for one domain would not be applica-
ble to another. For example, sugar would have
very different semantic relations in a medi-
cal domain than in a commodities exchange
domain); and of rigidity (even within well-
established domain, new subdomains spring
up, e.g. AIDS. Can hand-coded systems keep
up with new discoveries and new relations
with an acceptable latency?)
In the Information Retrieval community.
researchers have consistently considered that
&amp;quot;the linguistic apparatus required for effec-
tive domain-independent analysis is not yet
at hand,&amp;quot; and have concentrated on counting
document co-occurrence statistics (Peat and
Willet 1991), based on the idea that words
appearing in the same document must share
some semantic similarity. But document co-
occurrence suffers from two problems: granu-
larity (every word in the document is consid-
ered potentially related to every other word,
no matter what the distance between them)
and co-occurrence (for two words to be seen
as similar they must physically appear in the
same document. As an illustration, consider
the words tumor and tumour. These words
certainly share the same contexts, but would
never appear in the same document.) In gen-
eral different words used to describe similar
concepts might not be used in the same doc-
ument, and are missed by these methods.
Recently, a middle ground between these
two approaches has begun to be broken. Re-
searchers such as (Evans et at. 1991) and
(Church and Hanks 1990) have applied robust
grammars and statistical techniques over large
corpora to extract interesting noun phrases
and subject-verb, verb-object pairs. (Hearst
1992) has shown that certain lexical-syntactic
templates can reliably extract hyponym re-
lations froM text. (Ruge 1991) shows that
modifier-head relations in noun phrases ex-
tracted from a large corpus provide a use-
ful context for extracting similar words. The
common thread of all these techniques is that
they require no hand-coded domain knowl-
edge, but they examine more cleanly defined
contexts than simple document co-occurrence
methods.
Similarly, our SEXTANT 1 uses fine-
grained syntactically derived contexts, but de-
rives its measures of similarity from consider-
&apos;Semantic EXtraction from Text via Analyzed Net-
works of Terms
</bodyText>
<page confidence="0.982838">
324
</page>
<bodyText confidence="0.99998">
ing not the co-occurrence of two words in the
same context, but rather the overlapping of
all the contexts associated with words over an
entire corpus. Calculation of the amount of
shared weighted contexts produces a similar-
ity measure between two words.
</bodyText>
<sectionHeader confidence="0.821747" genericHeader="introduction">
SEXTANT
</sectionHeader>
<bodyText confidence="0.979197194444444">
SEXTANT can be run on any English text,
without any pre-coding of domain knowledge
or manual editing of the text. The input text
passes through the following steps: (I) Mor-
phological analysis. Each word is morpholog-
ically analyzed and looked up in a 100,000
word dictionary to find its possible parts of
speech. (II) Grammatical Disambiguation. A
stochastic parser assigns one grammatical cat-
egory to each word in the text. These first
two steps use CLAM&apos; programs (Evans et al.
1991). (III) Noun and Verb Phrase Splitting.
Each sentence is divided into verb and noun
phrases by a simple regular grammar. (IV)
Syntagmatic Relation Extraction. A four-
pass algorithm attaches modifiers to nouns,
noun phrases to noun phrases and verbs to
noun phrases. (Grefenstette 1992a) (V) Con-
text Isolation. The modifying words attached
to each word in the text are isolated for all
nouns. Thus the context of each noun is
given by all the words with which it is asso-
ciated throughout the corpus. (VI) Similarity
matching. Contexts are compared by using
similarity measures developed in the Social
Sciences, such as a weighted Jaccard measure.
As an example, consider the following sen-
tence extracted from a medical corpus.
Cyclophosphamide markedly prolonged induction
time and suppressed peak titer irrespective of
the time of antigen administration.
Each word is looked up in a online dictionary.
After grammatical ambiguities are removed
by the stochastic parser, the phrase is divided
into noun phrases(NP) and verb phrases(VP),
giving,
</bodyText>
<table confidence="0.893143333333333">
NP cyclophosphamide (on)
markedly (adv)
VP prolong (vt-past)
NP induction (on) time (sn)
and (cnj)
VP suppress (vt -past)
NP peak (on) titer (on) irrespective-of (prep)
the (d) time (on) of (prep) antigen (sn)
administration (an)
</table>
<bodyText confidence="0.988296461538462">
Once each sentence in the text is divided into
phrases, intra- and inter-phrase structural re-
lations are extracted. First noun phrases
are scanned from left to right(NPLR), hook-
ing up articles, adjectives and modifier nouns
to their head nouns. Then, noun phrases
are scanned right to left(NPRL), connecting
nouns over prepositions. Then, starting from
verb phrases, phrases are scanned before the
verb phrase for an unconnected head which
becomes the subject(VPRL), and likewise to
the right of the verb for objects(VPLR), pro-
ducing for the example:
</bodyText>
<figure confidence="0.71327025">
VPRL cyclophosphamide , prolong &lt; SUBJ
NPRL time , induction &lt; NN
VPLR prolong , time &lt; DOBJ
VPRL cyclophosphamide , suppress &lt; SUBJ
NPRL titer , peak &lt; NN
VPLR suppress , titer &lt; DOLT
NPLR titer , time &lt; NNPREP
NPRL administration , antigen &lt; NN
</figure>
<bodyText confidence="0.9838062">
Next SEXTANT extracts a user specified set
of relations that are considered as each word&apos;s
context for similarity calculations. For exam-
ple, one set of relations extracted by SEX-
TANT for the above sentence can be
</bodyText>
<figure confidence="0.891747444444444">
cyclophosphamide prolong-SUBJ
time induction
time prolong-DOLT
cyclophosphamide suppress-SUBJ
titer peak
titer suppress-DOLT
titer time
administration antigen
time administration
</figure>
<bodyText confidence="0.999666875">
In this example, the word time is found mod-
ified by the words induction, prolong-DOBJ
and administration, while administration is
only considered by this set of relations to be
modified by antigen. Over the whole corpus
of 160,000 words, one can consider what mod-
ifies administration. Isolating these modifiers
gives a list such as
</bodyText>
<listItem confidence="0.943909">
• • .
</listItem>
<tableCaption confidence="0.470566555555556">
administration androgen
administration antigen
administration aortic
administration aramine
administration associate-DOLT
administration associate-SUBJ
administration azathioprine
administration carbon-dioxide
administration case
</tableCaption>
<subsectionHeader confidence="0.92116">
administration cause -SUBJ
</subsectionHeader>
<bodyText confidence="0.9999865">
At this point SEXTANT compares all the
other words in the corpus, using a user-
specified similarity measure such the Jaccard
measure, to find which words are most simi-
lar to which others. For example, the words
found as most similar to administration in this
medical corpus were the following words in or-
der of most to least similar:
</bodyText>
<page confidence="0.998193">
325
</page>
<construct confidence="0.436098">
administration injection, treatment, therapy,
infusion, dose, response, ...
</construct>
<bodyText confidence="0.949454175">
As can be seen, the sense of administra-
tion as in the &amp;quot;administration of drugs and
medicines&amp;quot; is clearly extracted here, since ad-
ministration in this corpus is most similarly
used as other words such as injection and ther-
apy having to do with dispensing drugs and
medicines. One of the interesting aspects of
this approach, contrary to the coarse-grained
document co-occurrence approach, is that ad-
ministration and injection need never appear
in the same document for them to be recog-
nized as semantically similar. In the case of
this corpus, administration and injection were
considered similar because they shared the fol-
lowing modifiers:
acid follow-DOBJ growth prior produce-IOBJ
dose extract increase-SUBJ intravenous
treat-IOBJ associate-SUBJ associate-DOBJ
rapid cause-SUBJ antigen adrenalectomy
aortic hormone subside-IOBJ alter-IOBJ
folic-acid and folate
It is hard to select any one word which would
indicate that these two words were similar,
but the fact that they do share so many words,
and more so than other words, indicates that
these words share close semantic characteris-
tics in this corpus.
When the same procedure is run over a
corpus of library science abstracts, adminis-
tration is recognized as closest to
administration graduate, office, campus,
education, director, ...
Similarly circulation was found to be closest to
flow in the medical corpus and to date in the
library corpus. Cause was found to be closest
to etiology in the medical corpus and to deter-
minant in the library corpus. Frequently oc-
curring words, possessing enough context, are
generally ranked by SEXTANT with words in-
tuitively related within the defining corpus.
</bodyText>
<sectionHeader confidence="0.999758" genericHeader="discussions">
DISCUSSION
</sectionHeader>
<bodyText confidence="0.999784769230769">
While finding similar words in a corpus with-
out any domain knowledge is interesting in
itself, such a tool is practically useful in a
number of areas. A lexicographer building a
domain-specific dictionary would find such a
tool invaluable, given a large corpus of rep-
resentative text for that domain. Similarly,
a Knowledge Engineer creating a natural lan-
guage interface to an expert system could use
this system to cull similar terminology in a
field. We have shown elsewhere (Grefenstette
1992b), in an Information Retrieval setting,
that expanding queries using the closest terms
to query terms derived by SEXTANT can im-
prove recall and precision. We find that one
of the most interesting results from a linguis-
tic point of view, is the possibility automati-
cally creating corpus defined thesauri, as can
be seen above in the differences between re-
lations extracted from medical and from in-
formation science corpora. In conclusion, we
feel that this fine grained approach to context
extraction from large corpora, and similarity
calculation employing those contexts, even us-
ing imperfect syntactic analysis tools, shows
much promise for the future.
</bodyText>
<sectionHeader confidence="0.999653" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989167342857143">
(Church and Hanks 1990) K.W. Church and
P. Hanks. Word association norms, mutual
information, and lexicography. Computa-
tional Linguistics, 16(1), Mar 90.
(Evans et al. 1991) D.A. Evans, S.K. Hender-
son, R.G. Lefferts, and I.A. Monarch. A
summary of the CLAIM project. TR
CMU-LCL-91-2, Carnegie-Mellon, Nov 91.
(Grefenstette 1992a) G. Grefenstette. Sex-
tant: Extracting semantics from raw text,
implementation details. TR CS92-05, Uni-
versity of Pittsburgh, Feb 92.
(Grefenstette 1992b) G. Grefenstette. Use of
syntactic context to produce term associ-
ation lists for text retrieval. SIGIR&apos;92,
Copenhagen, June 21-24 1992. ACM.
(Hearst 1992) M.A. Hearst. Automatic acqui-
sition of hyponyms from large text corpora.
COLING&apos;92, Nantes, France, July 92.
(Jacobs and Zernick 1988) P. S. Jacobs and
U. Zernick. Acquiring lexical knowledge
from text: A case study. In Proceedings
Seventh National Conference on Artificial
Intelligence, 739-744, Morgan Kaufmann.
(Mauldin 1991) M. L. Mauldin. Conceptual
Information Retrieval: A case study in
adaptive parsing. Kluwer, Norwell, 91.
(Peat and Willet 1991) H.J. Peat and P. Wil-
let. The limitations of term co-occurrence
data for query expansion in document re-
trieval systems. JASIS, 42(5), 1991.
(Ruge 1991) G. Ruge. Experiments on lin-
guistically based term associations. In
RIAO&apos;91, 528-545, Barcelona, Apr 91.
CID, Paris.
</reference>
<page confidence="0.999095">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.005182">
<title confidence="0.9982715">SEXTANT: EXPLORING UNEXPLORED CONTEXTS FOR SEMANTIC EXTRACTION FROM SYNTACTIC ANALYSIS</title>
<author confidence="0.989911">Gregory Grefenstette</author>
<address confidence="0.482429">Computer Science Department, University of Pittsburgh, Pittsburgh, PA 15260</address>
<note confidence="0.968328">grefen@cs .pitt .edu</note>
<abstract confidence="0.999647656716418">For a very long time, it has been considered that the only way of automatically extracting similar groups of words from a text collection for which no semantic information exists is to use document co-occurrence data. But, with robust syntactic parsers that are becoming more frequently available, syntactically recognizable phenomena about word usage can be confidently noted in large collections of texts. We present here a new system called SEXTANT which uses these parsers and the finer-grained contexts they produce to judge word similarity. BACKGROUND Many machine-based approaches to term similarity, such as found in TRUMP (Jacobs and Zernick 1988) and FERRET (Mauldin 1991), can be characterized as knowledge-rich in that they presuppose that known lexical items possess Conceptual Dependence(CD)like descriptions. Such an approach necessitates a great amount of manual encoding of semantic information and suffers from the drawbacks of cost (in terms of initial coding, coherence checking, maintenance after modifications, and costs derivable from a host of other software engineering concern); of domain dependence (a semantic structure developed for one domain would not be applicato another. For example, have very different semantic relations in a medical domain than in a commodities exchange domain); and of rigidity (even within wellestablished domain, new subdomains spring up, e.g. AIDS. Can hand-coded systems keep up with new discoveries and new relations with an acceptable latency?) In the Information Retrieval community. researchers have consistently considered that &amp;quot;the linguistic apparatus required for effective domain-independent analysis is not yet at hand,&amp;quot; and have concentrated on counting document co-occurrence statistics (Peat and Willet 1991), based on the idea that words appearing in the same document must share some semantic similarity. But document cosuffers from two problems: granuword in the document is considered potentially related to every other word, no matter what the distance between them) and co-occurrence (for two words to be seen as similar they must physically appear in the same document. As an illustration, consider words tumor and words certainly share the same contexts, but would never appear in the same document.) In general different words used to describe similar concepts might not be used in the same document, and are missed by these methods. Recently, a middle ground between these two approaches has begun to be broken. Resuch as (Evans at. and (Church and Hanks 1990) have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subject-verb, verb-object pairs. (Hearst 1992) has shown that certain lexical-syntactic templates can reliably extract hyponym relations froM text. (Ruge 1991) shows that modifier-head relations in noun phrases extracted from a large corpus provide a useful context for extracting similar words. The common thread of all these techniques is that they require no hand-coded domain knowledge, but they examine more cleanly defined contexts than simple document co-occurrence methods. our SEXTANT 1uses finegrained syntactically derived contexts, but deits measures of similarity from consider- &apos;Semantic EXtraction from Text via Analyzed Networks of Terms 324 ing not the co-occurrence of two words in the same context, but rather the overlapping of all the contexts associated with words over an entire corpus. Calculation of the amount of shared weighted contexts produces a similarity measure between two words. SEXTANT SEXTANT can be run on any English text, without any pre-coding of domain knowledge or manual editing of the text. The input text passes through the following steps: (I) Morphological analysis. Each word is morphologically analyzed and looked up in a 100,000 word dictionary to find its possible parts of speech. (II) Grammatical Disambiguation. A stochastic parser assigns one grammatical category to each word in the text. These first steps use CLAM&apos; programs (Evans al. 1991). (III) Noun and Verb Phrase Splitting. Each sentence is divided into verb and noun phrases by a simple regular grammar. (IV) Syntagmatic Relation Extraction. A fourpass algorithm attaches modifiers to nouns, noun phrases to noun phrases and verbs to noun phrases. (Grefenstette 1992a) (V) Context Isolation. The modifying words attached to each word in the text are isolated for all nouns. Thus the context of each noun is given by all the words with which it is associated throughout the corpus. (VI) Similarity matching. Contexts are compared by using similarity measures developed in the Social Sciences, such as a weighted Jaccard measure. As an example, consider the following sentence extracted from a medical corpus. Cyclophosphamide markedly prolonged induction time and suppressed peak titer irrespective of the time of antigen administration. Each word is looked up in a online dictionary. After grammatical ambiguities are removed by the stochastic parser, the phrase is divided into noun phrases(NP) and verb phrases(VP), giving, (on) markedly (adv) VP prolong (vt-past) NP induction (on) time (sn) and (cnj) VP suppress (vt -past) NP peak (on) titer (on) irrespective-of (prep) the (d) time (on) of (prep) antigen (sn) administration (an) Once each sentence in the text is divided into intraand inter-phrase structural relations are extracted. First noun phrases are scanned from left to right(NPLR), hooking up articles, adjectives and modifier nouns to their head nouns. Then, noun phrases are scanned right to left(NPRL), connecting nouns over prepositions. Then, starting from verb phrases, phrases are scanned before the verb phrase for an unconnected head which becomes the subject(VPRL), and likewise to the right of the verb for objects(VPLR), producing for the example: , prolong &lt; SUBJ , induction &lt; , time &lt; DOBJ , suppress &lt; SUBJ , peak &lt; , titer &lt; DOLT , time &lt; , antigen &lt; Next SEXTANT extracts a user specified set of relations that are considered as each word&apos;s context for similarity calculations. For example, one set of relations extracted by SEX- TANT for the above sentence can be cyclophosphamide prolong-SUBJ time induction time prolong-DOLT cyclophosphamide suppress-SUBJ titer peak titer suppress-DOLT titer time administration antigen time administration this example, the word found modby the words prolong-DOBJ only considered by this set of relations to be by the whole corpus of 160,000 words, one can consider what modthese modifiers gives a list such as • • . administration androgen administration antigen administration aortic administration aramine administration associate-DOLT administration associate-SUBJ administration azathioprine administration carbon-dioxide administration case administration cause -SUBJ At this point SEXTANT compares all the other words in the corpus, using a userspecified similarity measure such the Jaccard measure, to find which words are most similar to which others. For example, the words as most similar to this medical corpus were the following words in order of most to least similar: 325 administration injection, treatment, therapy, infusion, dose, response, ... can be seen, the sense of administraas the &amp;quot;administration of drugs and is clearly extracted here, since adthis corpus is most similarly as other words such as therto do with dispensing drugs and medicines. One of the interesting aspects of this approach, contrary to the coarse-grained co-occurrence approach, is that adnever appear in the same document for them to be recognized as semantically similar. In the case of corpus, considered similar because they shared the following modifiers: prior extract adrenalectomy hormone folic-acid and folate It is hard to select any one word which would indicate that these two words were similar, but the fact that they do share so many words, and more so than other words, indicates that these words share close semantic characteristics in this corpus. When the same procedure is run over a of library science abstracts, adminisrecognized as closest to graduate, office, education, director, ... found to be closest to the medical corpus and to the corpus. found to be closest the medical corpus and to deterthe library corpus. Frequently occurring words, possessing enough context, are generally ranked by SEXTANT with words intuitively related within the defining corpus. DISCUSSION While finding similar words in a corpus without any domain knowledge is interesting in itself, such a tool is practically useful in a number of areas. A lexicographer building a domain-specific dictionary would find such a tool invaluable, given a large corpus of representative text for that domain. Similarly, a Knowledge Engineer creating a natural language interface to an expert system could use this system to cull similar terminology in a field. We have shown elsewhere (Grefenstette 1992b), in an Information Retrieval setting, that expanding queries using the closest terms to query terms derived by SEXTANT can improve recall and precision. We find that one of the most interesting results from a linguistic point of view, is the possibility automatically creating corpus defined thesauri, as can be seen above in the differences between relations extracted from medical and from information science corpora. In conclusion, we feel that this fine grained approach to context extraction from large corpora, and similarity calculation employing those contexts, even using imperfect syntactic analysis tools, shows much promise for the future.</abstract>
<note confidence="0.7428268">References (Church and Hanks 1990) K.W. Church and P. Hanks. Word association norms, mutual and lexicography. Computa- Linguistics, Mar 90.</note>
<author confidence="0.706065">D A Evans</author>
<author confidence="0.706065">S K Henderson</author>
<author confidence="0.706065">R G Lefferts</author>
<author confidence="0.706065">I A Monarch A</author>
<abstract confidence="0.846034333333333">summary of the CLAIM project. TR CMU-LCL-91-2, Carnegie-Mellon, Nov 91. (Grefenstette 1992a) G. Grefenstette. Sextant: Extracting semantics from raw text, implementation details. TR CS92-05, University of Pittsburgh, Feb 92. (Grefenstette 1992b) G. Grefenstette. Use of syntactic context to produce term associlists for text retrieval.</abstract>
<note confidence="0.568438666666667">Copenhagen, June 21-24 1992. ACM. (Hearst 1992) M.A. Hearst. Automatic acquisition of hyponyms from large text corpora. France, July 92. (Jacobs and Zernick 1988) P. S. Jacobs and U. Zernick. Acquiring lexical knowledge</note>
<title confidence="0.6218265">text: A case study. In Seventh National Conference on Artificial</title>
<author confidence="0.795114">Morgan Kaufmann</author>
<note confidence="0.978467">1991) M. L. Mauldin. Information Retrieval: A case study in parsing. Norwell, 91. (Peat and Willet 1991) H.J. Peat and P. Wil-</note>
<abstract confidence="0.8674196">let. The limitations of term co-occurrence data for query expansion in document resystems. 1991. (Ruge 1991) G. Ruge. Experiments on linguistically based term associations. In</abstract>
<address confidence="0.697357">Barcelona, Apr 91. CID, Paris. 326</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date></date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>(Church and Hanks 1990)</marker>
<rawString>K.W. Church and P. Hanks. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1), Mar 90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Evans</author>
<author>S K Henderson</author>
<author>R G Lefferts</author>
<author>I A Monarch</author>
</authors>
<title>A summary of the CLAIM project.</title>
<date></date>
<tech>TR CMU-LCL-91-2,</tech>
<location>Carnegie-Mellon,</location>
<marker>(Evans et al. 1991)</marker>
<rawString>D.A. Evans, S.K. Henderson, R.G. Lefferts, and I.A. Monarch. A summary of the CLAIM project. TR CMU-LCL-91-2, Carnegie-Mellon, Nov 91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Sextant: Extracting semantics from raw text, implementation details.</title>
<date></date>
<tech>TR CS92-05,</tech>
<institution>University of Pittsburgh,</institution>
<marker>(Grefenstette 1992a)</marker>
<rawString>G. Grefenstette. Sextant: Extracting semantics from raw text, implementation details. TR CS92-05, University of Pittsburgh, Feb 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Use of syntactic context to produce term association lists for text retrieval.</title>
<date>1992</date>
<booktitle>SIGIR&apos;92,</booktitle>
<publisher>ACM.</publisher>
<location>Copenhagen,</location>
<marker>(Grefenstette 1992b)</marker>
<rawString>G. Grefenstette. Use of syntactic context to produce term association lists for text retrieval. SIGIR&apos;92, Copenhagen, June 21-24 1992. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora. COLING&apos;92,</title>
<date></date>
<location>Nantes, France,</location>
<marker>(Hearst 1992)</marker>
<rawString>M.A. Hearst. Automatic acquisition of hyponyms from large text corpora. COLING&apos;92, Nantes, France, July 92.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P S Jacobs</author>
<author>U Zernick</author>
</authors>
<title>Acquiring lexical knowledge from text: A case study.</title>
<booktitle>In Proceedings Seventh National Conference on Artificial Intelligence,</booktitle>
<pages>739--744</pages>
<publisher>Morgan Kaufmann.</publisher>
<marker>(Jacobs and Zernick 1988)</marker>
<rawString>P. S. Jacobs and U. Zernick. Acquiring lexical knowledge from text: A case study. In Proceedings Seventh National Conference on Artificial Intelligence, 739-744, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Mauldin</author>
</authors>
<title>Conceptual Information Retrieval: A case study in adaptive parsing.</title>
<date></date>
<publisher>Kluwer,</publisher>
<location>Norwell,</location>
<marker>(Mauldin 1991)</marker>
<rawString>M. L. Mauldin. Conceptual Information Retrieval: A case study in adaptive parsing. Kluwer, Norwell, 91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Peat</author>
<author>P Willet</author>
</authors>
<title>The limitations of term co-occurrence data for query expansion in document retrieval systems.</title>
<date>1991</date>
<journal>JASIS,</journal>
<volume>42</volume>
<issue>5</issue>
<marker>(Peat and Willet 1991)</marker>
<rawString>H.J. Peat and P. Willet. The limitations of term co-occurrence data for query expansion in document retrieval systems. JASIS, 42(5), 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ruge</author>
</authors>
<title>Experiments on linguistically based term associations.</title>
<date></date>
<booktitle>In RIAO&apos;91,</booktitle>
<pages>528--545</pages>
<publisher>CID,</publisher>
<location>Barcelona,</location>
<marker>(Ruge 1991)</marker>
<rawString>G. Ruge. Experiments on linguistically based term associations. In RIAO&apos;91, 528-545, Barcelona, Apr 91. CID, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>