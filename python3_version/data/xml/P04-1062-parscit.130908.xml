<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.929506">
Annealing Techniques for Unsupervised Statistical Language Learning
</title>
<author confidence="0.950608">
Noah A. Smith and Jason Eisner
</author>
<affiliation confidence="0.929534">
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
</affiliation>
<email confidence="0.997834">
{nasmith,jason}@cs.jhu.edu
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999729933333333">
Exploiting unannotated natural language data is hard
largely because unsupervised parameter estimation is
hard. We describe deterministic annealing (Rose et al.,
1990) as an appealing alternative to the Expectation-
Maximization algorithm (Dempster et al., 1977). Seek-
ing to avoid search error, DA begins by globally maxi-
mizing an easy concave function and maintains a local
maximum as it gradually morphs the function into the
desired non-concave likelihood function. Applying DA
to parsing and tagging models is shown to be straight-
forward; significant improvements over EM are shown
on a part-of-speech tagging task. We describe a vari-
ant, skewed DA, which can incorporate a good initializer
when it is available, and show significant improvements
over EM on a grammar induction task.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956384615384">
Unlabeled data remains a tantalizing potential re-
source for NLP researchers. Some tasks can thrive
on a nearly pure diet of unlabeled data (Yarowsky,
1995; Collins and Singer, 1999; Cucerzan and
Yarowsky, 2003). But for other tasks, such as ma-
chine translation (Brown et al., 1990), the chief
merit of unlabeled data is simply that nothing else
is available; unsupervised parameter estimation is
notorious for achieving mediocre results.
The standard starting point is the Expectation-
Maximization (EM) algorithm (Dempster et al.,
1977). EM iteratively adjusts a model’s parame-
ters from an initial guess until it converges to a lo-
cal maximum. Unfortunately, likelihood functions
in practice are riddled with suboptimal local max-
ima (e.g., Charniak, 1993, ch. 7). Moreover, max-
imizing likelihood is not equivalent to maximizing
task-defined accuracy (e.g., Merialdo, 1994).
Here we focus on the search error problem. As-
sume that one has a model for which improving
likelihood really will improve accuracy (e.g., at pre-
dicting hidden part-of-speech (POS) tags or parse
trees). Hence, we seek methods that tend to locate
mountaintops rather than hilltops of the likelihood
function. Alternatively, we might want methods that
find hilltops with other desirable properties.1
</bodyText>
<note confidence="0.56453">
1Wang et al. (2003) suggest that one should seek a high-
</note>
<bodyText confidence="0.999939105263158">
In §2 we review deterministic annealing (DA)
and show how it generalizes the EM algorithm. §3
shows how DA can be used for parameter estimation
for models of language structure that use dynamic
programming to compute posteriors over hidden
structure, such as hidden Markov models (HMMs)
and stochastic context-free grammars (SCFGs). In
§4 we apply DA to the problem of learning a tri-
gram POS tagger without labeled data. We then de-
scribe how one of the received strengths of DA—
its robustness to the initializing model parameters—
can be a shortcoming in situations where the ini-
tial parameters carry a helpful bias. We present
a solution to this problem in the form of a new
algorithm, skewed deterministic annealing (SDA;
§5). Finally we apply SDA to a grammar induc-
tion model and demonstrate significantly improved
performance over EM (§6). §7 highlights future di-
rections for this work.
</bodyText>
<sectionHeader confidence="0.985789" genericHeader="introduction">
2 Deterministic annealing
</sectionHeader>
<bodyText confidence="0.998866875">
Suppose our data consist of a pairs of random vari-
ables X and Y , where the value of X is observed
and Y is hidden. For example, X might range
over sentences in English and Y over POS tag se-
quences. We use X and Y to denote the sets of
possible values of X and Y , respectively. We seek
to build a model that assigns probabilities to each
(x, y) E XxY. Let x� = {x1, x2,..., xn} be a corpus
of unlabeled examples. Assume the class of models
is fixed (for example, we might consider only first-
order HMMs with s states, corresponding notion-
ally to POS tags). Then the task is to find good pa-
rameters B E RN for the model. The criterion most
commonly used in building such models from un-
labeled data is maximum likelihood (ML); we seek
the parameters B∗:
</bodyText>
<equation confidence="0.97893">
Pr(xi, y  |B) (1)
</equation>
<bodyText confidence="0.9391625">
entropy hilltop. They argue that to account for partially-
observed (unlabeled) data, one should choose the distribution
with the highest Shannon entropy, subject to certain data-driven
constraints. They show that this desirable distribution is one of
the local maxima of likelihood. Whether high-entropy local
maxima really predict test data better is an empirical question.
</bodyText>
<figure confidence="0.8910525">
argmax Pr(x  |B) = argmax �n 11
θ� θ� i=1 y∈Y
</figure>
<figureCaption confidence="0.99926">
Fig. 1: The EM algorithm.
</figureCaption>
<bodyText confidence="0.999969689655173">
Each parameter θj corresponds to the conditional
probability of a single model event, e.g., a state tran-
sition in an HMM or a rewrite in a PCFG. Many
NLP models make it easy to maximize the likeli-
hood of supervised training data: simply count the
model events in the observed (xi, yi) pairs, and set
the conditional probabilities θi to be proportional to
the counts. In our unsupervised setting, the yi are
unknown, but solving (1) is almost as easy provided
that we can obtain the posterior distribution of Y
given each xi (that is, Pr(y xi) for each y E �
and each xi). The only difference is that we must
now count the model events fractionally, using the
expected number of occurrences of each (xi, y) pair.
This intuition leads to the EM algorithm in Fig. 1.
It is guaranteed that Pr(~x ~θ(i+1)) &gt; Pr(~x ~θ(i)).
For language-structure models like HMMs and
SCFGs, efficient dynamic programming algorithms
(forward-backward, inside-outside) are available to
compute the distribution p˜ at the E step of Fig. 1
and use it at the M step. These algorithms run in
polynomial time and space by structure-sharing the
possible y (tag sequences or parse trees) for each
xi, of which there may be exponentially many in
the length of xi. Even so, the majority of time spent
by EM for such models is on the E steps. In this pa-
per, we can fairly compare the runtime of EM and
other training procedures by counting the number of
E steps they take on a given training set and model.
</bodyText>
<subsectionHeader confidence="0.986066">
2.1 Generalizing EM
</subsectionHeader>
<bodyText confidence="0.999424333333333">
Figure 2 shows the deterministic annealing (DA) al-
gorithm derived from the framework of Rose et al.
(1990). It is quite similar to EM.2 However, DA
adds an outer loop that iteratively increases a value
β, and computation of the posterior in the E step is
modified to involve this β.
</bodyText>
<footnote confidence="0.990743666666667">
2Other expositions of DA abound; we have couched ours in
data-modeling language. Readers interested in the Lagrangian-
based derivations and analogies to statistical physics (including
phase transitions and the role of β as the inverse of temperature
in free-energy minimization) are referred to Rose (1998) for a
thorough discussion.
</footnote>
<figureCaption confidence="0.996493">
Fig. 2: The DA algorithm: a generalization of EM.
</figureCaption>
<bodyText confidence="0.996414">
When β = 1, DA’s inner loop will behave exactly
like EM, computing p˜ at the E step by the same for-
mula that EM uses. When β Pz� 0, p˜ will be close
to a uniform distribution over the hidden variable ~y,
since each numerator Pr(~x, y~ ~θ)β pz� 1. At such
β-values, DA effectively ignores the current param-
eters θ when choosing the posterior p˜ and the new
parameters. Finally, as β —* +oo, p˜ tends to place
nearly all of the probability mass on the single most
likely ~y. This winner-take-all situation is equivalent
to the “Viterbi” variant of the EM algorithm.
</bodyText>
<subsectionHeader confidence="0.997964">
2.2 Gradated difficulty
</subsectionHeader>
<bodyText confidence="0.886653">
In both the EM and DA algorithms, the E step se-
lects a posterior p˜ over the hidden variable Y~ and
~
the M step selects parameters θ. Neal and Hinton
(1998) show how the EM algorithm can be viewed
as optimizing a single objective function over both θ~
and ˜p. DA can also be seen this way; DA’s objective
function at a given β is
</bodyText>
<equation confidence="0.995328">
� � h i
F ~θ, ˜p, β = 1 βH(˜p) + E˜p(~Y ) log Pr(~x, Y~ ~θ) (2)
</equation>
<bodyText confidence="0.999792">
The EM version simply sets β = 1. A complete
derivation is not difficult but is too lengthy to give
here; it is a straightforward extension of that given
by Neal and Hinton for EM.
It is clear that the value of β allows us to manip-
ulate the relative importance of the two terms when
maximizing T. When β is close to 0, only the H
term matters. The H term is the Shannon entropy
of the posterior distribution ˜p, which is known to be
concave in ˜p. Maximizing it is simple: set all x to be
equiprobable (the uniform distribution). Therefore
a sufficiently small β drives up the importance of
H relative to the other term, and the entire problem
becomes concave with a single global maximum to
which we expect to converge.
In gradually increasing β from near 0 to 1, we
start out by solving an easy concave maximization
problem and use the result to initialize the next max-
</bodyText>
<equation confidence="0.998666333333333">
Input: ~x, ~θ(0) Output: ~θ*
i +— 0
do:
˜p(~y)+— Pr(~x,~y|~θ(i)) ~y0∈Yn Pr(~x,~y0|~θ(i)),d~y
h i
~θ(i+1) +— argmax~θ E˜p(~Y ) log Pr(~x, Y~ ~θ)
i + —i + 1
θ* +—
until ~θ(i) � ~θ(i_1)
~ ~θ(i)
(E)
(M)
Input: ~x, ~θ(0), βmax &gt;βmin &gt;0, α&gt;1 Output: ~θ*
i +— 0; β +— βmin
while β &lt; βmax:
do:
d�
(E) ˜p(~y)
� EY Pr(~x,~y0 |~θ(i) β ,y
θ(i+1) +— argmax~θ E˜p(~Y ) [log Pr(~x, Y~ ~θ)
1i &apos;
i + —i + 1
until ~θ(i) ,: ~θ(i_1)
β +— α &apos; β
end while
θ* +— θ(i)
(M)
</equation>
<bodyText confidence="0.999955432432432">
imization problem, which is slightly more difficult
(i.e., less concave). This continues, with the solu-
tion to each problem in the series being used to ini-
tialize the subsequent problem. When Q reaches 1,
DA behaves just like EM. Since the objective func-
tion is continuous in Q where Q &gt; 0, we can vi-
sualize DA as gradually morphing the easy concave
objective function into the one we really care about
(likelihood); we hope to “ride the maximum” as Q
moves toward 1.
DA guarantees iterative improvement of the ob-
jective function (see Ueda and Nakano (1998) for
proofs). But it does not guarantee convergence to
a global maximum, or even to a better local maxi-
mum than EM will find, even with extremely slow
Q-raising. A new mountain on the surface of the
objective function could arise at any stage that is
preferable to the one that we will ultimately find.
To run DA, we must choose a few control param-
eters. In this paper we set Qmax = 1 so that DA
will approach EM and finish at a local maximum of
likelihood. Qmin and the Q-increase factor α can be
set high for speed, but at a risk of introducing lo-
cal maxima too quickly for DA to work as intended.
(Note that a “fast” schedule that tries only a few Q
values is not as fast as one might expect, since it will
generally take longer to converge at each Q value.)
To conclude the theoretical discussion of DA, we
review its desirable properties. DA is robust to ini-
tial parameters, since when Q is close to 0 the ob-
jective hardly depends on 0. DA gradually increases
the difficulty of search, which may lead to the avoid-
ance of some local optima. By modifying the an-
nealing schedule, we can change the runtime of the
DA algorithm. DA is almost exactly like EM in im-
plementation, requiring only a slight modification to
the E step (see §3) and an additional outer loop.
</bodyText>
<subsectionHeader confidence="0.956113">
2.3 Prior work
</subsectionHeader>
<bodyText confidence="0.999959666666667">
DA was originally described as an algorithm for
clustering data in RN (Rose et al., 1990). Its pre-
decessor, simulated annealing, modifies the objec-
tive function during search by applying random per-
turbations of gradually decreasing size (Kirkpatrick
et al., 1983). Deterministic annealing moves the
randomness “inside” the objective function by tak-
ing expectations. DA has since been applied to
many problems (Rose, 1998); we describe two key
applications in language and speech processing.
Pereira, Tishby, and Lee (1993) used DA for soft
hierarchical clustering of English nouns, based on
the verbs that select them as direct objects. In their
case, when Q is close to 0, each noun is fuzzily
placed in each cluster so that Pr(cluster  |noun)
is nearly uniform. On the M step, this results in
clusters that are almost exactly identical; there is
one effective cluster. As Q is increased, it becomes
increasingly attractive for the cluster centroids to
move apart, or “split” into two groups (two effective
clusters), and eventually they do so. Continuing to
increase Q yields a hierarchical clustering through
repeated splits. Pereira et al. describe the tradeoff
given through Q as a control on the locality of influ-
ence of each noun on the cluster centroids, so that as
Q is raised, each noun exerts less influence on more
distant centroids and more on the nearest centroids.
DA has also been applied in speech recognition.
Rao and Rose (2001) used DA for supervised dis-
criminative training of HMMs. Their goal was
to optimize not likelihood but classification error
rate, a difficult objective function that is piecewise-
constant (hence not differentiable everywhere) and
riddled with shallow local minima. Rao and Rose
applied DA,3 moving from training a nearly uni-
form classifier with a concave cost surface (Q ≈ 0)
toward the desired deterministic classifier (Q →
+oc). They reported substantial gains in spoken
letter recognition accuracy over both a ML-trained
classifier and a localized error-rate optimizer.
Brown et al. (1990) gradually increased learn-
ing difficulty using a series of increasingly complex
models for machine translation. Their training al-
gorithm began by running an EM approximation on
the simplest model, then used the result to initialize
the next, more complex model (which had greater
predictive power and many more parameters), and
so on. Whereas DA provides gradated difficulty
in parameter search, their learning method involves
gradated difficulty among classes of models. The
two are orthogonal and could be used together.
</bodyText>
<sectionHeader confidence="0.975547" genericHeader="method">
3 DA with dynamic programming
</sectionHeader>
<bodyText confidence="0.9999965">
We turn now to the practical use of determinis-
tic annealing in NLP. Readers familiar with the
EM algorithm will note that, for typical stochas-
tic models of language structure (e.g., HMMs and
SCFGs), the bulk of the computational effort is re-
quired by the E step, which is accomplished by
a two-pass dynamic programming (DP) algorithm
(like the forward-backward algorithm). The M step
for these models normalizes the posterior expected
counts from the E step to get probabilities.4
</bodyText>
<footnote confidence="0.890846714285714">
3With an M step modified for their objective function: it im-
proved expected accuracy under ˜p, not expected log-likelihood.
4That is, assuming the usual generative parameterization of
such models; if we generalize to Markov random fields (also
known as log-linear or maximum entropy models) the M step,
while still concave, might entail an auxiliary optimization rou-
tine such as iterative scaling or a gradient-based method.
</footnote>
<bodyText confidence="0.999226823529412">
Running DA for such models is quite simple and
requires no modifications to the usual DP algo-
rithms. The only change to make is in the values
of the parameters passed to the DP algorithm: sim-
ply replace each θj by θβj . For a given x, the forward
pass of the DP computes (in a dense representation)
Pr(y  |x, ~θ) for all y. Each Pr(y  |x, ~θ) is a product
of some of the θj (each θj is multiplied in once for
each time its corresponding model event is present
in (x, y)). Raising the θj to a power will also raise
their product to that power, so the forward pass will
compute Pr(y  |x, ~θ)β when given ~θβ as parameter
values. The backward pass normalizes to the sum;
in this case it is the sum of the Pr(y  |x, ~θ)β, and
we have the E step described in Figure 2. We there-
fore expect an EM iteration of DA to take the same
amount of time as a normal EM iteration.5
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="method">
4 Part-of-speech tagging
</sectionHeader>
<bodyText confidence="0.9999703">
We turn now to the task of inducing a trigram POS
tagging model (second-order HMM) from an unla-
beled corpus. This experiment is inspired by the
experiments in Merialdo (1994). As in that work,
complete knowledge of the tagging dictionary is as-
sumed. The task is to find the trigram transition
probabilities Pr(tagi  |tagi−1, tagi−2) and emis-
sion probabilities Pr(wordi  |tagi). Merialdo’s key
result:6 If some labeled data were used to initialize
the parameters (by taking the ML estimate), then it
was not helpful to improve the model’s likelihood
through EM iterations, because this almost always
hurt the accuracy of the model’s Viterbi tagging on
a held-out test set. If only a small amount of labeled
data was used (200 sentences), then some accuracy
improvement was possible using EM, but only for
a few iterations. When no labeled data were used,
EM was able to improve the accuracy of the tagger,
and this improvement continued in the long term.
Our replication of Merialdo’s experiment used
the Wall Street Journal portion of the Penn Tree-
bank corpus, reserving a randomly selected 2,000
sentences (48,526 words) for testing. The remain-
ing 47,208 sentences (1,125,240 words) were used
in training, without any tags. The tagging dictionary
was constructed using the entire corpus (as done by
Merialdo). To initialize, the conditional transition
and emission distributions in the HMM were set to
uniform with slight perturbation. Every distribution
was smoothed using add-0.1 smoothing (at every M
</bodyText>
<footnote confidence="0.785679666666667">
5With one caveat: less pruning may be appropriate because
probability mass is spread more uniformly over different recon-
structions of the hidden data. This paper uses no pruning.
6Similar results were found by Elworthy (1994).
0 200 400 600 800 1000 1200
EM iterations
</footnote>
<bodyText confidence="0.995331">
step). The criterion for convergence is that the rela-
tive increase in the objective function between two
iterations fall below 10−9.
</bodyText>
<subsectionHeader confidence="0.888654">
4.1 Experiment
</subsectionHeader>
<bodyText confidence="0.999939606060606">
In the DA condition, we set βmin = 0.0001, βmax =
1, and α = 1.2. Results for the completely unsuper-
vised condition (no labeled data) are shown in Fig-
ure 3 and Table 1. Accuracy was nearly monotonic:
the final model is approximately the most accurate.
DA happily obtained a 10% reduction in tag er-
ror rate on training data, and an 11% reduction on
test data. On the other hand, it did not manage to
improve likelihood over EM. So was the accuracy
gain mere luck? Perhaps not. DA may be more re-
sistant to overfitting, because it may favor models
whose posteriors p˜ have high entropy. At least in
this experiment, its initial bias toward such models
carried over to the final learned model.7
In other words, the higher-entropy local maxi-
mum found by DA, in this case, explained the ob-
served data almost as well without overcommit-
ting to particular tag sequences. The maximum en-
tropy and latent maximum entropy principles (Wang
et al., 2003, discussed in footnote 1) are best justi-
fied as ways to avoid overfitting.
For a supervised tagger, the maximum entropy
principle prefers a conditional model Pr(~y  |~x) that
is maximally unsure about what tag sequence y~ to
apply to the training word sequence x~ (but expects
the same feature counts as the true ~y). Such a model
is hoped to generalize better to unsupervised data.
We can make the same argument. But in our case,
the split between supervised/unsupervised data is
not the split between training/test data. Our super-
vised data are, roughly, the fragments of the training
corpus that are unambiguously tagged thanks to the
tag dictionary.8 The EM model may overfit some
</bodyText>
<footnote confidence="0.998524666666667">
7We computed the entropy over possible tags for each word
in the test corpus, given the sentence the word occurs in. On
average, the DA model had 0.082 bits per tag, while EM had
only 0.057 bits per tag, a statistically significant difference (p &lt;
10−6) under a binomial sign test on word tokens.
8Without the tag dictionary, our learners would treat the tag
</footnote>
<figure confidence="0.940841375">
75
70
45
40
65
60
55
50
</figure>
<figureCaption confidence="0.713374">
Fig. 3: Learning curves for
</figureCaption>
<table confidence="0.930297142857143">
EM and DA. Steps in DA’s curve
correspond to −changes. The shape of
�
the DA curve is partly a function of the an−
nealing schedule, which only gradually (and
EM
in steps) allows the parameters to move
away from the uniform distribution.
DA
% correct ambiguous test tags
E steps final training cross- final test cross- % correct training tags % correct test tags
entropy (bits/word) entropy (bits/word) (all) (ambiguous) (all) (ambiguous)
EM 279 9.136 9.321 82.04 66.61 82.08 66.63
DA 1200 9.138 9.325 83.85 70.02 84.00 70.25
</table>
<tableCaption confidence="0.997376">
Table 1: EM vs. DA on unsupervised trigram POS tagging, using a tag dictionary. Each of the accuracy results is significant when
</tableCaption>
<bodyText confidence="0.879592857142857">
accuracy is compared at either the word-level or sentence-level. (Significance at p &lt; 10−6 under a binomial sign test in each
case. E.g., on the test set, the DA model correctly tagged 1,652 words that EM’s model missed while EM correctly tagged 726
words that DA missed. Similarly, the DA model had higher accuracy on 850 sentences, while EM had higher accuracy on only 287.
These differences are extremely unlikely to occur due to chance.) The differences in cross-entropy, compared by sentence, were
significant in the training set but not the test set (p &lt; 0.01 under a binomial sign test). Recall that lower cross entropy means higher
likelihood.
parameters to these fragments. The higher-entropy
DA model may be less likely to overfit, allowing it
to do better on the unsupervised data—i.e., the rest
of the training corpus and the entire test corpus.
We conclude that DA has settled on a local maxi-
mum of the likelihood function that (unsurprisingly)
corresponds well with the entropy criterion, and per-
haps as a result, does better on accuracy.
</bodyText>
<subsectionHeader confidence="0.944748">
4.2 Significance
</subsectionHeader>
<bodyText confidence="0.999990416666667">
Seeking to determine how well this result general-
ized, we randomly split the corpus into ten equally-
sized, nonoverlapping parts. EM and DA were run
on each portion;9 the results were inconclusive. DA
achieved better test accuracy than EM on three of
ten trials, better training likelihood on five trials,
and better test likelihood on all ten trials.10 Cer-
tainly decreasing the amount of data by an order of
magnitude results in increased variance of the per-
formance of any algorithm—so ten small corpora
were not enough to determine whether to expect an
improvement from DA more often than not.
</bodyText>
<subsectionHeader confidence="0.999846">
4.3 Mixing labeled and unlabeled data (I)
</subsectionHeader>
<bodyText confidence="0.86284896875">
In the other conditions described by Merialdo, vary-
ing amounts of labeled data (ranging from 100 sen-
tences to nearly half of the corpus) were used to
~
initialize the parameters θ, which were then trained
using EM on the remaining unlabeled data. Only
in the case where 100 labeled examples were used,
and only for a few iterations, did EM improve the
names as interchangeable and could not reasonably be evalu-
ated on gold-standard accuracy.
9The smoothing parameters were scaled down so as to be
proportional to the corpus size.
10It is also worth noting that runtimes were longer with the
10%-sized corpora than the full corpus (EM took 1.5 times as
many E steps; DA, 1.3 times). Perhaps the algorithms traveled
farther to find a local maximum. We know of no study of the
effect of unlabeled training set size on the likelihood surface,
but suggest two issues for future exploration. Larger datasets
contain more idiosyncrasies but provide a stronger overall sig-
nal. Hence, we might expect them to yield a bumpier likelihood
surface whose local maxima are more numerous but also dif-
fer more noticeably in height. Both these tendencies of larger
datasets would in theory increase DA’s advantage over EM.
accuracy of this model. We replicated these experi-
ments and compared EM with DA; DA damaged the
models even more than EM. This is unsurprising; as
noted before, DA effectively ignores the initial pa-
~
rameters θ(0). Therefore, even if initializing with a
model trained on small amounts of labeled data had
helped EM, DA would have missed out on this ben-
efit. In the next section we address this issue.
</bodyText>
<sectionHeader confidence="0.96586" genericHeader="method">
5 Skewed deterministic annealing
</sectionHeader>
<bodyText confidence="0.960013875">
The EM algorithm is quite sensitive to the initial pa-
~
rameters θ(0). We touted DA’s insensitivity to those
parameters as an advantage, but in scenarios where
well-chosen initial parameters can be provided (as
in §4.3), we wish for DA to be able exploit them.
In particular, there are at least two cases where
“good” initializers might be known. One is the
case explored by Merialdo, where some labeled data
were available to build an initial model. The other is
a situation where a good distribution is known over
the labels y; we will see an example of this in §6.
We wish to find a way to incorporate an initializer
into DA and still reap the benefit of gradated diffi-
culty. To see how this will come about, consider
again the E step for DA, which for all y:
</bodyText>
<equation confidence="0.678189">
˜p(y) ← Pr(x, y  |~θ)β
where u is the uniform distribution over � and
Z&apos;(~θ,β) and Z(~θ,β) = Z&apos;(~θ,β) · u(y)1−β are nor-
</equation>
<bodyText confidence="0.990666714285714">
malizing terms. (Note that Z( ~θ, β) does not depend
on y because u(y) is constant with respect to y.) Of
course, when β is close to 0, DA chooses the uni-
form posterior because it has the highest entropy.
Seen this way, DA is interpolating in the log do-
main between two posteriors: the one given by y
~
and θ and the uniform one u; the interpolation coef-
ficient is β. To generalize DA, we will replace the
uniform u with another posterior, the “skew” pos-
terior ´p, which is an input to the algorithm. This
posterior might be specified directly, as it will be in
§6, or it might be computed using an M step from
some good initial ~θ(0).
</bodyText>
<equation confidence="0.999245888888889">
Z&apos;( ~θ, β)
Pr(x, y |
=
Z(
~θ)βu(y)1−β
~θ, β)
The skewed DA (SDA) E step is given by:
˜p(y) ← Z(β) Pr(x, y  |θ)β ´p(y)1−β
1 (3)
</equation>
<bodyText confidence="0.9993065">
When β is close to 0, the E step will choose p˜ to
be very close to ´p. With small β, SDA is a “cau-
tious” EM variant that is wary of moving too far
from the initializing posterior p´ (or, equivalently, the
initial parameters ~θ(0)). As β approaches 1, the ef-
fect of p´ will diminish, and when β = 1, the algo-
rithm becomes identical to EM. The overall objec-
tive (matching (2) except for the boxed term) is:
</bodyText>
<equation confidence="0.989104333333333">
F� ( ) [ ( )]
~θ, ˜p, β = β 1 H(˜p) + E˜p(~Y ) log Pr ~x, Y~  |θ~
+
</equation>
<bodyText confidence="0.999916">
Mixing labeled and unlabeled data (II) Return-
ing to Merialdo’s mixed conditions (§4.3), we found
that SDA repaired the damage done by DA but did
not offer any benefit over EM. Its behavior in the
100-labeled sentence condition was similar to that
of EM’s, with a slightly but not significantly higher
peak in training set accuracy. In the other condi-
tions, SDA behaved like EM, with steady degrada-
tion of accuracy as training proceeded. It ultimately
damaged performance only as much as EM did or
did slightly better than EM (but still hurt).
This is unsurprising: Merialdo’s result demon-
strated that ML and maximizing accuracy are gener-
ally not the same; the EM algorithm consistently de-
graded the accuracy of his supervised models. SDA
is simply another search algorithm with the same
criterion as EM. SDA did do what it was expected
to do—it used the initializer, repairing DA damage.
</bodyText>
<sectionHeader confidence="0.995038" genericHeader="method">
6 Grammar induction
</sectionHeader>
<bodyText confidence="0.999978888888889">
We turn next to the problem of statistical grammar
induction: inducing parse trees over unlabeled text.
An excellent recent result is by Klein and Manning
(2002). The constituent-context model (CCM) they
present is a generative, deficient channel model of
POS tag strings given binary tree bracketings. We
first review the model and describe a small mod-
ification that reduces the deficiency, then compare
both models under EM and DA.
</bodyText>
<subsectionHeader confidence="0.980589">
6.1 Constituent-context model
</subsectionHeader>
<bodyText confidence="0.999860947368421">
Let (x, y) be a (tag sequence, binary tree) pair. xji
denotes the subsequence of x from the ith to the
jth word. Let yi,j be 1 if the yield from i to j is a
constituent in the tree y and 0 if it is not. The CCM
gives to a pair (x, y) the following probability:
ble tag-sequence yields (given whether the yield is
a constituent or not) and χ is a conditional distribu-
tion over possible contexts of one tag on either side
of the yield (given whether the yield is a constituent
or not). There are therefore four distributions to be
estimated; Pr(y) is taken to be uniform.
The model is initialized using expected counts of
the constituent and context features given that all
the trees are generated according to a random-split
model.11
The CCM generates each tag not once but O(n2)
times, once by every constituent or non-constituent
span that dominates it. We suggest the following
modification to alleviate some of the deficiency:
</bodyText>
<equation confidence="0.696719666666667">
[( ��� yi,j, j − i + 1)
ψ xj i
]·χ (xi−1, xj+1 |yi,j)
</equation>
<bodyText confidence="0.999958555555556">
The change is to condition the yield feature ψ on
the length of the yield. This decreases deficiency by
disallowing, for example, a constituent over a four-
tag yield to generate a seven-tag sequence. It also
decreases inter-parameter dependence by breaking
the constituent (and non-constituent) distributions
into a separate bin for each possible constituent
length. We will refer to Klein and Manning’s CCM
and our version as models 1 and 2, respectively.
</bodyText>
<subsectionHeader confidence="0.997296">
6.2 Experiment
</subsectionHeader>
<bodyText confidence="0.9999774375">
We ran experiments using both CCM models on
the tag sequences of length ten or less in the Wall
Street Journal Penn Treebank corpus, after extract-
ing punctuation. This corpus consists of 7,519 sen-
tences (52,837 tag tokens, 38 types). We report
PARSEVAL scores averaged by constituent (rather
than by sentence), and do not give the learner credit
for getting full sentences or single tags as con-
stituents.12 Because the E step for this model is
computationally intensive, we set the DA parame-
ters at βmin = 0.01, α = 1.5 so that fewer E steps
would be necessary.13 The convergence criterion
was relative improvement &lt; 10−9 in the objective.
The results are shown in Table 2. The first point
to notice is that a uniform initializer is a bad idea,
as Klein and Manning predicted. All conditions but
</bodyText>
<footnote confidence="0.995001909090909">
11We refer readers to Klein and Manning (2002) or Cover
and Thomas (1991, p. 72) for details; computing expected
counts for a sentence is a closed form operation. Klein and
Manning’s argument for this initialization step is that it is less
biased toward balanced trees than the uniform model used dur-
ing learning; we also found that it works far better in practice.
12This is why the CCM 1 performance reported here differs
from Klein and Manning’s; our implementation of the EM con-
dition gave virtually identical results under either evaluation
scheme (D. Klein, personal communication).
13A pilot study got very similar results for βmin = 10−6.
</footnote>
<bodyText confidence="0.5278915">
[( ��� yi,j )
ψ xj i
]· χ(xi−1,xj+1 |yi,j)
where ψ is a conditional distribution over possi-
</bodyText>
<equation confidence="0.971568">
H
Pr(x, y) = Pr(y) ·
1&lt;i&lt;j&lt;JxJ
[ ( )]
β E˜p(~Y ) log p´ Y~
1 − β
H
Pr(x, y) = Pr(y) ·
1&lt;i&lt;j&lt;JxJ
</equation>
<table confidence="0.991284">
E steps cross-entropy (bits/tag) UR UP F CB
CCM 1 EM (uniform) 146 103.1654 61.20 45.62 52.27 1.69
DA 403 103.1542 55.13 41.10 47.09 1.91
EM (split) 124 103.1951 78.14 58.24 66.74 0.98
SDA (split) 339 103.1651 62.71 46.75 53.57 1.62
CCM 2 EM (uniform) 26 84.8106 57.60 42.94 49.20 1.86
DA 331 84.7899 40.81 30.42 34.86 2.66
EM (split) 44 84.8049 78.56 58.56 67.10 0.98
SDA (split) 290 84.7940 79.64 59.37 68.03 0.93
</table>
<tableCaption confidence="0.9689258">
Table 2: The two CCM models, trained with two unsupervised algorithms, each with two initializers. Note that DA is equivalent
to SDA initialized with a uniform distribution. The third line corresponds to the setup reported by Klein and Manning (2002).
UR is unlabeled recall, UP is unlabeled precision, F is their harmonic mean, and CB is the average number of crossing brackets
per sentence. All evaluation is on the same data used for unsupervised learning (i.e., there is no training/test split). The high
cross-entropy values arise from the deficiency of models 1 and 2, and are not comparable across models.
</tableCaption>
<bodyText confidence="0.999943739130435">
one find better structure when initialized with Klein
and Manning’s random-split model. (The exception
is SDA on model 1; possibly the high deficiency of
model 1 interacts poorly with SDA’s search in some
way.)
Next we note that with the random-split initial-
izer, our model 2 is a bit better than model 1 on
PARSEVAL measures and converges more quickly.
Every instance of DA or SDA achieved higher
log-likelihood than the corresponding EM condi-
tion. This is what we hoped to gain from annealing:
better local maxima. In the case of model 2 with
the random-split initializer, SDA significantly out-
performed EM (comparing both matches and cross-
ing brackets per sentence under a binomial sign test,
p &lt; 10−6); we see a &gt; 5% reduction in average
crossing brackets per sentence. Thus, our strategy
of using DA but modifying it to accept an initial-
izer worked as desired in this case, yielding our best
overall performance.
The systematic results we describe next suggest
that these patterns persist across different training
sets in this domain.
</bodyText>
<subsectionHeader confidence="0.989492">
6.3 Significance
</subsectionHeader>
<bodyText confidence="0.999986111111111">
The difficulty we experienced in finding generaliza-
tion to small datasets, discussed in §4.2, was appar-
ent here as well. For 10-way and 3-way random,
nonoverlapping splits of the dataset, we did not have
consistent results in favor of either EM or SDA. In-
terestingly, we found that training model 2 (using
EM or SDA) on 10% of the corpus resulted on av-
erage in models that performed nearly as well on
their respective training sets as the full corpus con-
dition did on its training set; see Table 3. In ad-
dition, SDA sometimes performed as well as EM
under model 1. For a random two-way split, EM
and SDA converged to almost identical solutions on
one of the sub-corpora, and SDA outperformed EM
significantly on the other (on model 2).
In order to get multiple points of comparison of
EM and SDA on this task with a larger amount of
data, we jack-knifed the WSJ-10 corpus by split-
ting it randomly into ten equally-sized nonoverlap-
ping parts then training models on the corpus with
each of the ten sub-corpora excluded.14 These trials
are not independent of each other; any two of the
sub-corpora have 89 of their training data in com-
mon. Aggregate results are shown in Table 3. Using
model 2, SDA always outperformed EM, and in 8 of
10 cases the difference was significant when com-
paring matching constituents per sentence (7 of 10
when comparing crossing constituents).15 The vari-
ance of SDA was far less than that of EM; SDA not
only always performed better with model 2, but its
performance was more consistent over the trials.
We conclude this experimental discussion by cau-
tioning that both CCM models are highly deficient
models, and it is unknown how well they generalize
to corpora of longer sentences, other languages, or
corpora of words (rather than POS tags).
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
7 Future work
</sectionHeader>
<bodyText confidence="0.8780887">
There are a number of interesting directions for fu-
ture work. Noting the simplicity of the DA algo-
rithm, we hope that current devotees of EM will
run comparisons of their models with DA (or SDA).
Not only might this improve performance of exist-
14Note that this is not a cross-validation experiment; results
are reported on the unlabeled training set, and the excluded sub-
corpus remains unused.
15Binomial sign test, with significance defined as p &lt; 0.05,
though all significant results had p &lt; 0.001.
</bodyText>
<table confidence="0.996961666666667">
10% corpus 90% corpus
µF QF µF QF
CCM 1 EM 65.00 1.091 66.12 0.6643
SDA 63.00 4.689 53.53 0.2135
CCM 2 EM 66.74 1.402 67.24 0.7077
SDA 66.77 1.034 68.07 0.1193
</table>
<tableCaption confidence="0.998109">
Table 3: The mean p and standard deviation v of F-measure
performance for 10 trials using 10% of the corpus and 10 jack-
knifed trials using 90% of the corpus.
</tableCaption>
<bodyText confidence="0.99995354054054">
ing systems, it will contribute to the general under-
standing of the likelihood surface for a variety of
problems (e.g., this paper has raised the question of
how factors like dataset size and model deficiency
affect the likelihood surface).
DA provides a very natural way to gradually
introduce complexity to clustering models (Rose
et al., 1990; Pereira et al., 1993). This comes about
by manipulating the β parameter; as it rises, the
number of effective clusters is allowed to increase.
An open question is whether the analogues of “clus-
ters” in tagging and parsing models—tag symbols
and grammatical categories, respectively—might be
treated in a similar manner under DA. For instance,
we might begin with the CCM, the original formula-
tion of which posits only one distinction about con-
stituency (whether a span is a constituent or not) and
gradually allow splits in constituent-label space, re-
sulting in multiple grammatical categories that, we
hope, arise naturally from the data.
In this paper, we used βmax = 1. It would
be interesting to explore the effect on accuracy of
“quenching,” a phase at the end of optimization
that rapidly raises β from 1 to the winner-take-all
(Viterbi) variant at β = +∞.
Finally, certain practical speedups may be possi-
ble. For instance, increasing βmin and α, as noted
in §2.2, will vary the number of E steps required for
convergence. We suggested that the change might
result in slower or faster convergence; optimizing
the schedule using an online algorithm (or deter-
mining precisely how these parameters affect the
schedule in practice) may prove beneficial. Another
possibility is to relax the convergence criterion for
earlier β values, requiring fewer E steps before in-
creasing β, or even raising β slightly after every E
step (collapsing the outer and inner loops).
</bodyText>
<sectionHeader confidence="0.999168" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999991333333333">
We have reviewed the DA algorithm, describing
it as a generalization of EM with certain desir-
able properties, most notably the gradual increase
of difficulty of learning and the ease of imple-
mentation for NLP models. We have shown how
DA can be used to improve the accuracy of a tri-
gram POS tagger learned from an unlabeled cor-
pus. We described a potential shortcoming of DA
for NLP applications—its failure to exploit good
initializers—and then described a novel algorithm,
skewed DA, that solves this problem. Finally, we re-
ported significant improvements to a state-of-the-art
grammar induction model using SDA and a slight
modification to the parameterization of that model.
These results support the case that annealing tech-
niques in some cases offer performance gains over
the standard EM approach to learning from unla-
beled corpora, particularly with large corpora.
</bodyText>
<sectionHeader confidence="0.997277" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998400545454546">
This work was supported by a fellowship to the first au-
thor from the Fannie and John Hertz Foundation, and
by an NSF ITR grant to the second author. The views
expressed are not necessarily endorsed by the sponsors.
The authors thank Shankar Kumar, Charles Schafer,
David Smith, and Roy Tromble for helpful comments
and discussions; three ACL reviewers for advice that im-
proved the paper; Eric Goldlust for keeping the Dyna
compiler (Eisner et al., 2004) up to date with the de-
mands made by this work; and Dan Klein for sharing
details of his CCM implementation.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999449288888889">
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Je-
linek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990.
A statistical approach to machine translation. Computational
Linguistics, 16(2):79–85.
E. Charniak. 1993. Statistical Language Learning. MIT Press.
M. Collins and Y. Singer. 1999. Unsupervised models for
named-entity classification. In Proc. of EMNLP.
T. M. Cover and J. A. Thomas. 1991. Elements ofInformation
Theory. John Wiley and Sons.
S. Cucerzan and D. Yarowsky. 2003. Minimally supervised
induction of grammatical gender. In Proc. of HLT/NAACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likeli-
hood estimation from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B, 39:1–38.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declar-
ative language for implementing dynamic programs. In Proc.
ofACL (companion volume).
D. Elworthy. 1994. Does Baum-Welch re-estimation help tag-
gers? In Proc. ofANLP.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimiza-
tion by simulated annealing. Science, 220:671–680.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for grammar induction. In Proc. ofACL.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155–72.
R. Neal and G. Hinton. 1998. A view of the EM algorithm
that justifies incremental, sparse, and other variants. In M. I.
Jordan, editor, Learning in Graphical Models. Kluwer.
F. C. N. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proc. ofACL.
A. Rao and K. Rose. 2001. Deterministically annealed design
of Hidden Markov Model speech recognizers. IEEE Transac-
tions on Speech and Audio Processing, 9(2):111–126.
K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical me-
chanics and phase transitions in clustering. Physical Review
Letters, 65(8):945–948.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimization
problems. Proc. of the IEEE, 86(11):2210–2239.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271–282.
S. Wang, D. Schuurmans, and Y. Zhao. 2003. The latent maxi-
mum entropy principle. In review.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. ofACL.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969154">
<title confidence="0.99998">Annealing Techniques for Unsupervised Statistical Language Learning</title>
<author confidence="0.999701">A Smith Eisner</author>
<affiliation confidence="0.999595">Department of Computer Science / Center for Language and Speech Processing</affiliation>
<address confidence="0.993631">Johns Hopkins University, Baltimore, MD 21218 USA</address>
<abstract confidence="0.998316625">Exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard. We describe deterministic annealing (Rose et al., 1990) as an appealing alternative to the Expectation- Maximization algorithm (Dempster et al., 1977). Seeking to avoid search error, DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1342" citStr="Brown et al., 1990" startWordPosition="197" endWordPosition="200">on. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we f</context>
<context position="12854" citStr="Brown et al. (1990)" startWordPosition="2229" endWordPosition="2232">gnition. Rao and Rose (2001) used DA for supervised discriminative training of HMMs. Their goal was to optimize not likelihood but classification error rate, a difficult objective function that is piecewiseconstant (hence not differentiable everywhere) and riddled with shallow local minima. Rao and Rose applied DA,3 moving from training a nearly uniform classifier with a concave cost surface (Q ≈ 0) toward the desired deterministic classifier (Q → +oc). They reported substantial gains in spoken letter recognition accuracy over both a ML-trained classifier and a localized error-rate optimizer. Brown et al. (1990) gradually increased learning difficulty using a series of increasingly complex models for machine translation. Their training algorithm began by running an EM approximation on the simplest model, then used the result to initialize the next, more complex model (which had greater predictive power and many more parameters), and so on. Whereas DA provides gradated difficulty in parameter search, their learning method involves gradated difficulty among classes of models. The two are orthogonal and could be used together. 3 DA with dynamic programming We turn now to the practical use of determinist</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<publisher>MIT</publisher>
<contexts>
<context position="1813" citStr="Charniak, 1993" startWordPosition="269" endWordPosition="270"> (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we focus on the search error problem. Assume that one has a model for which improving likelihood really will improve accuracy (e.g., at predicting hidden part-of-speech (POS) tags or parse trees). Hence, we seek methods that tend to locate mountaintops rather than hilltops of the likelihood function. Alternatively, we might want methods that find hilltops with other desirable properties.1 1Wang et al. (2003) suggest that one should seek a highIn §2 we review deterministi</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>E. Charniak. 1993. Statistical Language Learning. MIT Press. M. Collins and Y. Singer. 1999. Unsupervised models for named-entity classification. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements ofInformation Theory.</title>
<date>1991</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="28965" citStr="Cover and Thomas (1991" startWordPosition="5028" endWordPosition="5031">RSEVAL scores averaged by constituent (rather than by sentence), and do not give the learner credit for getting full sentences or single tags as constituents.12 Because the E step for this model is computationally intensive, we set the DA parameters at βmin = 0.01, α = 1.5 so that fewer E steps would be necessary.13 The convergence criterion was relative improvement &lt; 10−9 in the objective. The results are shown in Table 2. The first point to notice is that a uniform initializer is a bad idea, as Klein and Manning predicted. All conditions but 11We refer readers to Klein and Manning (2002) or Cover and Thomas (1991, p. 72) for details; computing expected counts for a sentence is a closed form operation. Klein and Manning’s argument for this initialization step is that it is less biased toward balanced trees than the uniform model used during learning; we also found that it works far better in practice. 12This is why the CCM 1 performance reported here differs from Klein and Manning’s; our implementation of the EM condition gave virtually identical results under either evaluation scheme (D. Klein, personal communication). 13A pilot study got very similar results for βmin = 10−6. [( ��� yi,j ) ψ xj i ]· χ</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T. M. Cover and J. A. Thomas. 1991. Elements ofInformation Theory. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Minimally supervised induction of grammatical gender.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="1271" citStr="Cucerzan and Yarowsky, 2003" startWordPosition="184" endWordPosition="187"> it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalen</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2003</marker>
<rawString>S. Cucerzan and D. Yarowsky. 2003. Minimally supervised induction of grammatical gender. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood estimation from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<pages>39--1</pages>
<contexts>
<context position="1596" citStr="Dempster et al., 1977" startWordPosition="233" endWordPosition="236"> and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we focus on the search error problem. Assume that one has a model for which improving likelihood really will improve accuracy (e.g., at predicting hidden part-of-speech (POS) tags or parse trees). Hence, we seek methods that tend to locate mountaintops rathe</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>E Goldlust</author>
<author>N A Smith</author>
</authors>
<title>Dyna: A declarative language for implementing dynamic programs.</title>
<date>2004</date>
<booktitle>In Proc. ofACL</booktitle>
<marker>Eisner, Goldlust, Smith, 2004</marker>
<rawString>J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A declarative language for implementing dynamic programs. In Proc. ofACL (companion volume).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proc. ofANLP.</booktitle>
<contexts>
<context position="16927" citStr="Elworthy (1994)" startWordPosition="2911" endWordPosition="2912">words) for testing. The remaining 47,208 sentences (1,125,240 words) were used in training, without any tags. The tagging dictionary was constructed using the entire corpus (as done by Merialdo). To initialize, the conditional transition and emission distributions in the HMM were set to uniform with slight perturbation. Every distribution was smoothed using add-0.1 smoothing (at every M 5With one caveat: less pruning may be appropriate because probability mass is spread more uniformly over different reconstructions of the hidden data. This paper uses no pruning. 6Similar results were found by Elworthy (1994). 0 200 400 600 800 1000 1200 EM iterations step). The criterion for convergence is that the relative increase in the objective function between two iterations fall below 10−9. 4.1 Experiment In the DA condition, we set βmin = 0.0001, βmax = 1, and α = 1.2. Results for the completely unsupervised condition (no labeled data) are shown in Figure 3 and Table 1. Accuracy was nearly monotonic: the final model is approximately the most accurate. DA happily obtained a 10% reduction in tag error rate on training data, and an 11% reduction on test data. On the other hand, it did not manage to improve l</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proc. ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kirkpatrick</author>
<author>C D Gelatt</author>
<author>M P Vecchi</author>
</authors>
<title>Optimization by simulated annealing.</title>
<date>1983</date>
<journal>Science,</journal>
<pages>220--671</pages>
<contexts>
<context position="11080" citStr="Kirkpatrick et al., 1983" startWordPosition="1943" endWordPosition="1946"> hardly depends on 0. DA gradually increases the difficulty of search, which may lead to the avoidance of some local optima. By modifying the annealing schedule, we can change the runtime of the DA algorithm. DA is almost exactly like EM in implementation, requiring only a slight modification to the E step (see §3) and an additional outer loop. 2.3 Prior work DA was originally described as an algorithm for clustering data in RN (Rose et al., 1990). Its predecessor, simulated annealing, modifies the objective function during search by applying random perturbations of gradually decreasing size (Kirkpatrick et al., 1983). Deterministic annealing moves the randomness “inside” the objective function by taking expectations. DA has since been applied to many problems (Rose, 1998); we describe two key applications in language and speech processing. Pereira, Tishby, and Lee (1993) used DA for soft hierarchical clustering of English nouns, based on the verbs that select them as direct objects. In their case, when Q is close to 0, each noun is fuzzily placed in each cluster so that Pr(cluster |noun) is nearly uniform. On the M step, this results in clusters that are almost exactly identical; there is one effective cl</context>
</contexts>
<marker>Kirkpatrick, Gelatt, Vecchi, 1983</marker>
<rawString>S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimization by simulated annealing. Science, 220:671–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>A generative constituentcontext model for grammar induction.</title>
<date>2002</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="26339" citStr="Klein and Manning (2002)" startWordPosition="4573" endWordPosition="4576">ately damaged performance only as much as EM did or did slightly better than EM (but still hurt). This is unsurprising: Merialdo’s result demonstrated that ML and maximizing accuracy are generally not the same; the EM algorithm consistently degraded the accuracy of his supervised models. SDA is simply another search algorithm with the same criterion as EM. SDA did do what it was expected to do—it used the initializer, repairing DA damage. 6 Grammar induction We turn next to the problem of statistical grammar induction: inducing parse trees over unlabeled text. An excellent recent result is by Klein and Manning (2002). The constituent-context model (CCM) they present is a generative, deficient channel model of POS tag strings given binary tree bracketings. We first review the model and describe a small modification that reduces the deficiency, then compare both models under EM and DA. 6.1 Constituent-context model Let (x, y) be a (tag sequence, binary tree) pair. xji denotes the subsequence of x from the ith to the jth word. Let yi,j be 1 if the yield from i to j is a constituent in the tree y and 0 if it is not. The CCM gives to a pair (x, y) the following probability: ble tag-sequence yields (given wheth</context>
<context position="28939" citStr="Klein and Manning (2002)" startWordPosition="5023" endWordPosition="5026">ens, 38 types). We report PARSEVAL scores averaged by constituent (rather than by sentence), and do not give the learner credit for getting full sentences or single tags as constituents.12 Because the E step for this model is computationally intensive, we set the DA parameters at βmin = 0.01, α = 1.5 so that fewer E steps would be necessary.13 The convergence criterion was relative improvement &lt; 10−9 in the objective. The results are shown in Table 2. The first point to notice is that a uniform initializer is a bad idea, as Klein and Manning predicted. All conditions but 11We refer readers to Klein and Manning (2002) or Cover and Thomas (1991, p. 72) for details; computing expected counts for a sentence is a closed form operation. Klein and Manning’s argument for this initialization step is that it is less biased toward balanced trees than the uniform model used during learning; we also found that it works far better in practice. 12This is why the CCM 1 performance reported here differs from Klein and Manning’s; our implementation of the EM condition gave virtually identical results under either evaluation scheme (D. Klein, personal communication). 13A pilot study got very similar results for βmin = 10−6.</context>
<context position="30393" citStr="Klein and Manning (2002)" startWordPosition="5280" endWordPosition="5283">B CCM 1 EM (uniform) 146 103.1654 61.20 45.62 52.27 1.69 DA 403 103.1542 55.13 41.10 47.09 1.91 EM (split) 124 103.1951 78.14 58.24 66.74 0.98 SDA (split) 339 103.1651 62.71 46.75 53.57 1.62 CCM 2 EM (uniform) 26 84.8106 57.60 42.94 49.20 1.86 DA 331 84.7899 40.81 30.42 34.86 2.66 EM (split) 44 84.8049 78.56 58.56 67.10 0.98 SDA (split) 290 84.7940 79.64 59.37 68.03 0.93 Table 2: The two CCM models, trained with two unsupervised algorithms, each with two initializers. Note that DA is equivalent to SDA initialized with a uniform distribution. The third line corresponds to the setup reported by Klein and Manning (2002). UR is unlabeled recall, UP is unlabeled precision, F is their harmonic mean, and CB is the average number of crossing brackets per sentence. All evaluation is on the same data used for unsupervised learning (i.e., there is no training/test split). The high cross-entropy values arise from the deficiency of models 1 and 2, and are not comparable across models. one find better structure when initialized with Klein and Manning’s random-split model. (The exception is SDA on model 1; possibly the high deficiency of model 1 interacts poorly with SDA’s search in some way.) Next we note that with the</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. A generative constituentcontext model for grammar induction. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1931" citStr="Merialdo, 1994" startWordPosition="285" endWordPosition="286">ation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we focus on the search error problem. Assume that one has a model for which improving likelihood really will improve accuracy (e.g., at predicting hidden part-of-speech (POS) tags or parse trees). Hence, we seek methods that tend to locate mountaintops rather than hilltops of the likelihood function. Alternatively, we might want methods that find hilltops with other desirable properties.1 1Wang et al. (2003) suggest that one should seek a highIn §2 we review deterministic annealing (DA) and show how it generalizes the EM algorithm. §3 shows how DA can be used for parameter estimation fo</context>
<context position="15373" citStr="Merialdo (1994)" startWordPosition="2665" endWordPosition="2666">t is present in (x, y)). Raising the θj to a power will also raise their product to that power, so the forward pass will compute Pr(y |x, ~θ)β when given ~θβ as parameter values. The backward pass normalizes to the sum; in this case it is the sum of the Pr(y |x, ~θ)β, and we have the E step described in Figure 2. We therefore expect an EM iteration of DA to take the same amount of time as a normal EM iteration.5 4 Part-of-speech tagging We turn now to the task of inducing a trigram POS tagging model (second-order HMM) from an unlabeled corpus. This experiment is inspired by the experiments in Merialdo (1994). As in that work, complete knowledge of the tagging dictionary is assumed. The task is to find the trigram transition probabilities Pr(tagi |tagi−1, tagi−2) and emission probabilities Pr(wordi |tagi). Merialdo’s key result:6 If some labeled data were used to initialize the parameters (by taking the ML estimate), then it was not helpful to improve the model’s likelihood through EM iterations, because this almost always hurt the accuracy of the model’s Viterbi tagging on a held-out test set. If only a small amount of labeled data was used (200 sentences), then some accuracy improvement was poss</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Neal</author>
<author>G Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In M. I. Jordan, editor,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="7420" citStr="Neal and Hinton (1998)" startWordPosition="1246" endWordPosition="1249">β Pz� 0, p˜ will be close to a uniform distribution over the hidden variable ~y, since each numerator Pr(~x, y~ ~θ)β pz� 1. At such β-values, DA effectively ignores the current parameters θ when choosing the posterior p˜ and the new parameters. Finally, as β —* +oo, p˜ tends to place nearly all of the probability mass on the single most likely ~y. This winner-take-all situation is equivalent to the “Viterbi” variant of the EM algorithm. 2.2 Gradated difficulty In both the EM and DA algorithms, the E step selects a posterior p˜ over the hidden variable Y~ and ~ the M step selects parameters θ. Neal and Hinton (1998) show how the EM algorithm can be viewed as optimizing a single objective function over both θ~ and ˜p. DA can also be seen this way; DA’s objective function at a given β is � � h i F ~θ, ˜p, β = 1 βH(˜p) + E˜p(~Y ) log Pr(~x, Y~ ~θ) (2) The EM version simply sets β = 1. A complete derivation is not difficult but is too lengthy to give here; it is a straightforward extension of that given by Neal and Hinton for EM. It is clear that the value of β allows us to manipulate the relative importance of the two terms when maximizing T. When β is close to 0, only the H term matters. The H term is the </context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>R. Neal and G. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="34787" citStr="Pereira et al., 1993" startWordPosition="6037" endWordPosition="6040">43 SDA 63.00 4.689 53.53 0.2135 CCM 2 EM 66.74 1.402 67.24 0.7077 SDA 66.77 1.034 68.07 0.1193 Table 3: The mean p and standard deviation v of F-measure performance for 10 trials using 10% of the corpus and 10 jackknifed trials using 90% of the corpus. ing systems, it will contribute to the general understanding of the likelihood surface for a variety of problems (e.g., this paper has raised the question of how factors like dataset size and model deficiency affect the likelihood surface). DA provides a very natural way to gradually introduce complexity to clustering models (Rose et al., 1990; Pereira et al., 1993). This comes about by manipulating the β parameter; as it rises, the number of effective clusters is allowed to increase. An open question is whether the analogues of “clusters” in tagging and parsing models—tag symbols and grammatical categories, respectively—might be treated in a similar manner under DA. For instance, we might begin with the CCM, the original formulation of which posits only one distinction about constituency (whether a span is a constituent or not) and gradually allow splits in constituent-label space, resulting in multiple grammatical categories that, we hope, arise natura</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. C. N. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rao</author>
<author>K Rose</author>
</authors>
<title>Deterministically annealed design of Hidden Markov Model speech recognizers.</title>
<date>2001</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="12263" citStr="Rao and Rose (2001)" startWordPosition="2139" endWordPosition="2142">identical; there is one effective cluster. As Q is increased, it becomes increasingly attractive for the cluster centroids to move apart, or “split” into two groups (two effective clusters), and eventually they do so. Continuing to increase Q yields a hierarchical clustering through repeated splits. Pereira et al. describe the tradeoff given through Q as a control on the locality of influence of each noun on the cluster centroids, so that as Q is raised, each noun exerts less influence on more distant centroids and more on the nearest centroids. DA has also been applied in speech recognition. Rao and Rose (2001) used DA for supervised discriminative training of HMMs. Their goal was to optimize not likelihood but classification error rate, a difficult objective function that is piecewiseconstant (hence not differentiable everywhere) and riddled with shallow local minima. Rao and Rose applied DA,3 moving from training a nearly uniform classifier with a concave cost surface (Q ≈ 0) toward the desired deterministic classifier (Q → +oc). They reported substantial gains in spoken letter recognition accuracy over both a ML-trained classifier and a localized error-rate optimizer. Brown et al. (1990) graduall</context>
</contexts>
<marker>Rao, Rose, 2001</marker>
<rawString>A. Rao and K. Rose. 2001. Deterministically annealed design of Hidden Markov Model speech recognizers. IEEE Transactions on Speech and Audio Processing, 9(2):111–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
<author>E Gurewitz</author>
<author>G C Fox</author>
</authors>
<title>Statistical mechanics and phase transitions in clustering.</title>
<date>1990</date>
<journal>Physical Review Letters,</journal>
<volume>65</volume>
<issue>8</issue>
<contexts>
<context position="6115" citStr="Rose et al. (1990)" startWordPosition="1015" endWordPosition="1018">ution p˜ at the E step of Fig. 1 and use it at the M step. These algorithms run in polynomial time and space by structure-sharing the possible y (tag sequences or parse trees) for each xi, of which there may be exponentially many in the length of xi. Even so, the majority of time spent by EM for such models is on the E steps. In this paper, we can fairly compare the runtime of EM and other training procedures by counting the number of E steps they take on a given training set and model. 2.1 Generalizing EM Figure 2 shows the deterministic annealing (DA) algorithm derived from the framework of Rose et al. (1990). It is quite similar to EM.2 However, DA adds an outer loop that iteratively increases a value β, and computation of the posterior in the E step is modified to involve this β. 2Other expositions of DA abound; we have couched ours in data-modeling language. Readers interested in the Lagrangianbased derivations and analogies to statistical physics (including phase transitions and the role of β as the inverse of temperature in free-energy minimization) are referred to Rose (1998) for a thorough discussion. Fig. 2: The DA algorithm: a generalization of EM. When β = 1, DA’s inner loop will behave </context>
<context position="10906" citStr="Rose et al., 1990" startWordPosition="1918" endWordPosition="1921"> value.) To conclude the theoretical discussion of DA, we review its desirable properties. DA is robust to initial parameters, since when Q is close to 0 the objective hardly depends on 0. DA gradually increases the difficulty of search, which may lead to the avoidance of some local optima. By modifying the annealing schedule, we can change the runtime of the DA algorithm. DA is almost exactly like EM in implementation, requiring only a slight modification to the E step (see §3) and an additional outer loop. 2.3 Prior work DA was originally described as an algorithm for clustering data in RN (Rose et al., 1990). Its predecessor, simulated annealing, modifies the objective function during search by applying random perturbations of gradually decreasing size (Kirkpatrick et al., 1983). Deterministic annealing moves the randomness “inside” the objective function by taking expectations. DA has since been applied to many problems (Rose, 1998); we describe two key applications in language and speech processing. Pereira, Tishby, and Lee (1993) used DA for soft hierarchical clustering of English nouns, based on the verbs that select them as direct objects. In their case, when Q is close to 0, each noun is fu</context>
<context position="34764" citStr="Rose et al., 1990" startWordPosition="6033" endWordPosition="6036">00 1.091 66.12 0.6643 SDA 63.00 4.689 53.53 0.2135 CCM 2 EM 66.74 1.402 67.24 0.7077 SDA 66.77 1.034 68.07 0.1193 Table 3: The mean p and standard deviation v of F-measure performance for 10 trials using 10% of the corpus and 10 jackknifed trials using 90% of the corpus. ing systems, it will contribute to the general understanding of the likelihood surface for a variety of problems (e.g., this paper has raised the question of how factors like dataset size and model deficiency affect the likelihood surface). DA provides a very natural way to gradually introduce complexity to clustering models (Rose et al., 1990; Pereira et al., 1993). This comes about by manipulating the β parameter; as it rises, the number of effective clusters is allowed to increase. An open question is whether the analogues of “clusters” in tagging and parsing models—tag symbols and grammatical categories, respectively—might be treated in a similar manner under DA. For instance, we might begin with the CCM, the original formulation of which posits only one distinction about constituency (whether a span is a constituent or not) and gradually allow splits in constituent-label space, resulting in multiple grammatical categories that</context>
</contexts>
<marker>Rose, Gurewitz, Fox, 1990</marker>
<rawString>K. Rose, E. Gurewitz, and G. C. Fox. 1990. Statistical mechanics and phase transitions in clustering. Physical Review Letters, 65(8):945–948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression, and related optimization problems.</title>
<date>1998</date>
<booktitle>Proc. of the IEEE,</booktitle>
<volume>86</volume>
<issue>11</issue>
<contexts>
<context position="6597" citStr="Rose (1998)" startWordPosition="1095" endWordPosition="1096">l. 2.1 Generalizing EM Figure 2 shows the deterministic annealing (DA) algorithm derived from the framework of Rose et al. (1990). It is quite similar to EM.2 However, DA adds an outer loop that iteratively increases a value β, and computation of the posterior in the E step is modified to involve this β. 2Other expositions of DA abound; we have couched ours in data-modeling language. Readers interested in the Lagrangianbased derivations and analogies to statistical physics (including phase transitions and the role of β as the inverse of temperature in free-energy minimization) are referred to Rose (1998) for a thorough discussion. Fig. 2: The DA algorithm: a generalization of EM. When β = 1, DA’s inner loop will behave exactly like EM, computing p˜ at the E step by the same formula that EM uses. When β Pz� 0, p˜ will be close to a uniform distribution over the hidden variable ~y, since each numerator Pr(~x, y~ ~θ)β pz� 1. At such β-values, DA effectively ignores the current parameters θ when choosing the posterior p˜ and the new parameters. Finally, as β —* +oo, p˜ tends to place nearly all of the probability mass on the single most likely ~y. This winner-take-all situation is equivalent to t</context>
<context position="11238" citStr="Rose, 1998" startWordPosition="1968" endWordPosition="1969">nge the runtime of the DA algorithm. DA is almost exactly like EM in implementation, requiring only a slight modification to the E step (see §3) and an additional outer loop. 2.3 Prior work DA was originally described as an algorithm for clustering data in RN (Rose et al., 1990). Its predecessor, simulated annealing, modifies the objective function during search by applying random perturbations of gradually decreasing size (Kirkpatrick et al., 1983). Deterministic annealing moves the randomness “inside” the objective function by taking expectations. DA has since been applied to many problems (Rose, 1998); we describe two key applications in language and speech processing. Pereira, Tishby, and Lee (1993) used DA for soft hierarchical clustering of English nouns, based on the verbs that select them as direct objects. In their case, when Q is close to 0, each noun is fuzzily placed in each cluster so that Pr(cluster |noun) is nearly uniform. On the M step, this results in clusters that are almost exactly identical; there is one effective cluster. As Q is increased, it becomes increasingly attractive for the cluster centroids to move apart, or “split” into two groups (two effective clusters), and</context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. Proc. of the IEEE, 86(11):2210–2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueda</author>
<author>R Nakano</author>
</authors>
<title>Deterministic annealing EM algorithm.</title>
<date>1998</date>
<journal>Neural Networks,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="9535" citStr="Ueda and Nakano (1998)" startWordPosition="1659" endWordPosition="1662">i + 1 until ~θ(i) ,: ~θ(i_1) β +— α &apos; β end while θ* +— θ(i) (M) imization problem, which is slightly more difficult (i.e., less concave). This continues, with the solution to each problem in the series being used to initialize the subsequent problem. When Q reaches 1, DA behaves just like EM. Since the objective function is continuous in Q where Q &gt; 0, we can visualize DA as gradually morphing the easy concave objective function into the one we really care about (likelihood); we hope to “ride the maximum” as Q moves toward 1. DA guarantees iterative improvement of the objective function (see Ueda and Nakano (1998) for proofs). But it does not guarantee convergence to a global maximum, or even to a better local maximum than EM will find, even with extremely slow Q-raising. A new mountain on the surface of the objective function could arise at any stage that is preferable to the one that we will ultimately find. To run DA, we must choose a few control parameters. In this paper we set Qmax = 1 so that DA will approach EM and finish at a local maximum of likelihood. Qmin and the Q-increase factor α can be set high for speed, but at a risk of introducing local maxima too quickly for DA to work as intended. </context>
</contexts>
<marker>Ueda, Nakano, 1998</marker>
<rawString>N. Ueda and R. Nakano. 1998. Deterministic annealing EM algorithm. Neural Networks, 11(2):271–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>D Schuurmans</author>
<author>Y Zhao</author>
</authors>
<title>The latent maximum entropy principle.</title>
<date>2003</date>
<booktitle>In review.</booktitle>
<contexts>
<context position="2349" citStr="Wang et al. (2003)" startWordPosition="349" endWordPosition="352">nctions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we focus on the search error problem. Assume that one has a model for which improving likelihood really will improve accuracy (e.g., at predicting hidden part-of-speech (POS) tags or parse trees). Hence, we seek methods that tend to locate mountaintops rather than hilltops of the likelihood function. Alternatively, we might want methods that find hilltops with other desirable properties.1 1Wang et al. (2003) suggest that one should seek a highIn §2 we review deterministic annealing (DA) and show how it generalizes the EM algorithm. §3 shows how DA can be used for parameter estimation for models of language structure that use dynamic programming to compute posteriors over hidden structure, such as hidden Markov models (HMMs) and stochastic context-free grammars (SCFGs). In §4 we apply DA to the problem of learning a trigram POS tagger without labeled data. We then describe how one of the received strengths of DA— its robustness to the initializing model parameters— can be a shortcoming in situatio</context>
<context position="18057" citStr="Wang et al., 2003" startWordPosition="3110" endWordPosition="3113">a, and an 11% reduction on test data. On the other hand, it did not manage to improve likelihood over EM. So was the accuracy gain mere luck? Perhaps not. DA may be more resistant to overfitting, because it may favor models whose posteriors p˜ have high entropy. At least in this experiment, its initial bias toward such models carried over to the final learned model.7 In other words, the higher-entropy local maximum found by DA, in this case, explained the observed data almost as well without overcommitting to particular tag sequences. The maximum entropy and latent maximum entropy principles (Wang et al., 2003, discussed in footnote 1) are best justified as ways to avoid overfitting. For a supervised tagger, the maximum entropy principle prefers a conditional model Pr(~y |~x) that is maximally unsure about what tag sequence y~ to apply to the training word sequence x~ (but expects the same feature counts as the true ~y). Such a model is hoped to generalize better to unsupervised data. We can make the same argument. But in our case, the split between supervised/unsupervised data is not the split between training/test data. Our supervised data are, roughly, the fragments of the training corpus that a</context>
</contexts>
<marker>Wang, Schuurmans, Zhao, 2003</marker>
<rawString>S. Wang, D. Schuurmans, and Y. Zhao. 2003. The latent maximum entropy principle. In review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="1215" citStr="Yarowsky, 1995" startWordPosition="178" endWordPosition="179"> function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>