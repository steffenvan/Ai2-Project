<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.987513">
Context Comparison of Bursty Events in Web Search and Online Media
</title>
<author confidence="0.999082">
Yunliang Jiang Cindy Xide Lin Qiaozhu Mei
</author>
<affiliation confidence="0.999955">
University of Illinois University of Illinois University of Michigan
</affiliation>
<address confidence="0.78593">
Urbana, IL, 61801 Urbana, IL, 61801 Ann Arbor, MI, 48109
</address>
<email confidence="0.999447">
jiang8@illinois.edu xidelin2@illinois.edu qmei@umich.edu
</email>
<sectionHeader confidence="0.993912" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999793882352941">
In this paper, we conducted a systematic com-
parative analysis of language in different con-
texts of bursty topics, including web search,
news media, blogging, and social bookmark-
ing. We analyze (1) the content similarity and
predictability between contexts, (2) the cov-
erage of search content by each context, and
(3) the intrinsic coherence of information in
each context. Our experiments show that so-
cial bookmarking is a better predictor to the
bursty search queries, but news media and so-
cial blogging media have a much more com-
pelling coverage. This comparison provides
insights on how the search behaviors and so-
cial information sharing behaviors of users are
correlated to the professional news media in
the context of bursty events.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999706615384615">
Search is easy. Every day people are repeating the
queries they have used before, trying to access the
same web pages. A smart search engine tracks the
preference and returns it next time when it sees the
same query. When I search for “msr” I always try to
access Microsoft research; and even if I misspelled
it, a smart search engine could suggest a correct
query based on my query history, the current ses-
sion of queries, and/or the queries that other people
have been using.
Search is hard. I search for “social computing”
because there was such a new program in NSF; but
the search engine might have not yet noticed that.
People use “msg” to access monosodium glutamate
in most of the cases, but tonight there is a big game
in Madison square garden. H1N1 suddenly became
a hot topic, followed by a burst of the rumor that
it was a hoax, and then the vaccine. The informa-
tion need of users changed dramatically during such
a period. When a new event happens, the burst of
new contents and new interests make it hard to pre-
dict what people would search and to suggest what
queries they should use.
Web search is easy when the information need of
the users is stable and when we have enough histor-
ical clicks. It becomes much more difficult when a
new information need knocks the door or when there
is a sudden change of the information need. Such a
shift of the information need is usually caused by a
burst of new events or new interests.
When we are lack of enough historical observa-
tions, why don’t we seek help from other sources?
A bursting event will not only influence what we
search, but hopefully also affect what we read, what
we write, and what we tag. Indeed, there is al-
ready considerable effort in seeking help from these
sources, by the integration of news and blogs into
search results or the use of social bookmarks to
enhance search. These conclusions, however, are
mostly drawn in a general context (e.g., with gen-
eral search queries). To what extent are they use-
ful when dealing with busty events? How is the
bursting content in web search, news media, social
media, and social bookmarks correlating and dif-
ferent from each other? Prior to the development
of desirable applications (e.g. enhancing search re-
sults, query suggestion, keyword bidding on adver-
tisement, etc) by integrating the information from all
these sources, it is appealing to have an investigation
of feasibility.
In this work, we conduct a systematic compara-
tive study of what we search, what we read, what
</bodyText>
<page confidence="0.970003">
1077
</page>
<note confidence="0.8177715">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1077–1087,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99939694117647">
we write, and what we tag in the scenarios of bursty
events. Specifically, we analyze the language used
in different contexts of bursty events, including two
different query log contexts, two news media con-
texts, two blog contexts, and an additional con-
text of social bookmarks. A variety of experiments
have been conducted, including the content similar-
ity and cross-entropy between sources, the coverage
of search queries in online media, and an in-depth
semantic comparison of sources based on language
networks.
In the rest of this paper, a summary of related
work is briefly described in Section 2. We then
present the experiments setup in Section 3, The re-
sults of the experiments is presented in Section 4. Fi-
nally, our major findings from the comparative anal-
ysis are drawn in Section 5.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997141025641">
Recently, a rich body of work has focused on how
to find the bursting patterns from time-series data
using various approaches such as time-graph analy-
sis (Kleinberg, 2003; Kuman et al., 2003), context-
based analysis (Gabrilovich et al., 2004), moving-
average analysis (Vlachos et al., 2004), and fre-
quency analysis (Gruhl et al., 2005), etc. These
methods are all related to the preprocessing step of
our analysis: detecting bursty queries from the query
log effectively.
The comparison of two web sources at a time is
widely studied recently. (Sood et al., 2007) dis-
cussed how to leverage the relation between social
tags and web blogs. (Lloyd et al., 2006; Gamon et
al., 2008; Cointet et al., 2008) investigated the rela-
tions between news and blogs. Also some work has
aimed to utilize one external web source to help web
search. For example, (Diaz, 2009) integrated the
news results into general search. (Bao et al., 2007;
Heymann et al., 2008; Krause et al., 2008; Bischoff
et al., 2008) focused on improving search by the so-
cial tags. Compared with the above, our comparison
analysis tries to explore the interactions among mul-
tiple web sources including the search logs.
Similar to our work, some recent work (Adar et
al., 2007; Sun et al., 2008) has addressed the com-
parison among multiple web sources. For exam-
ple, (Adar et al., 2007) did a comprehensive corre-
lation study among queries, blogs, news and TV re-
sults. However, different from the content-free anal-
ysis above, our work compares the sources based on
the content.
Our work can lead to many useful search applica-
tions, such as query suggestion which takes as in-
put a specific query and returns as output one or
several suggested queries. The approaches include
query term cooccurrence (Jones et al., 2006), query
sessions (Radlinski and Joachims, 2005), and click-
through (Mei et al., 2008), respectively.
</bodyText>
<sectionHeader confidence="0.942666" genericHeader="method">
3 Analysis Setup
</sectionHeader>
<bodyText confidence="0.999979346153846">
Tasks of web information retrieval such as web
search generally perform very well on frequent
and navigational queries (Broder, 2002) such like
“chicago” or “yahoo movies.” A considerable chal-
lenge in web search remains in how to handle infor-
mational queries, especially queries that reflect new
information need and suddenly changed information
need of users. Many such scenarios are caused by
the emergence of bursty events (e.g., “van gogh” be-
came a hot query in May 2006 since a Van Goghs
portrait was sold for 40.3 million in New York dur-
ing that time). The focus of this paper is to analyze
how other online media sources react to those bursty
events and how those reactions compare to the re-
action in web search. This analysis thus serves as
an primitive investigation of the feasibility of lever-
aging other sources to enhance the search of bursty
topics.
Therefore, we focus on the “event-related” topics
which present as bursty queries submitted to a search
engine. These queries not only reflect the suddenly
changed information need of users, but also trigger
the correlated reactions in other online sources, such
as news media, blog media, social bookmarks, etc.
We begin with the extraction of bursty topics from
the query log.
</bodyText>
<subsectionHeader confidence="0.999549">
3.1 Bursty Topic Extraction
</subsectionHeader>
<bodyText confidence="0.999981833333333">
Search engine logs (or query logs) store the history
of users’ search behaviors, which reflect users’ in-
terests and information need. The query log of a
commercial search engine consists of a huge amount
of search records, each of which typically contains
the following information: the query submitted by
</bodyText>
<page confidence="0.99354">
1078
</page>
<bodyText confidence="0.999867888888889">
a user, the time at which the query was submitted,
and/or the URL which the user clicked on after the
query was submitted, etc. It is common practice
to segment query log into search sessions, each of
which represents one user’s searching activities in a
short period of time.
We explore a sample of the log of the Microsoft
Live search engine1, which contains 14.9M search
records over 1 month (May 2006).
</bodyText>
<subsectionHeader confidence="0.960481">
3.1.1 Find bursty queries from query log
</subsectionHeader>
<bodyText confidence="0.984513916666667">
How to extract the queries that represent bursty
events? We believe that bursty queries present the
pattern that its day-by-day search volume shows a
significant spike – that is, the frequency that the user
submit this query should suddenly increase at one
specific time and drop down after a while. This as-
sumption is consistent with existing work of finding
bursty patterns in emails, scientific literature (Klein-
berg, 2003), and blogs (Gruhl et al., 2005).
Following (Gruhl et al., 2005), we utilize a simple
but effective method to collect bursty topics in the
query log data as follows:
</bodyText>
<listItem confidence="0.932941565217391">
• We choose bigrams as the basic presentation of
bursty topics since bigrams present the information
need of users more clearly and completely than un-
igrams and also have a larger coverage in the query
log comparing to n-grams (n &gt; 3).
• We only consider the bigram queries which ap-
pear more frequently than a threshold s per month.
This is reasonable since a bursty event usually
causes a large volume of search activities.
• Let fmax(q) be the maximum search volume of
a query q in one day (i.e., day d). Let �f−5(q) be the
upper bound of the daily search volume of q out-
side a time window of 5 days centered at day d. If
fmax(q) is “significantly higher” than �f−5(q) (i.e.,
rm = �fmax(q)/f−5(q) &gt; m), we consider q as a
query with a spike pattern (m is an empirical thresh-
old).
• The ratio above may be vulnerable to the query
that has more than one spike. To solve this, we de-
fine �f−5(q) as the average of daily search volume
of q outside the same time window. This gives us
an alternative ratio ra = fmax(q)/ f−5(q). We fur-
ther balance these two ratios by ranking the bursty
</listItem>
<footnote confidence="0.965025">
1Now known as Bing: www.bing.com
</footnote>
<equation confidence="0.9206835">
queries using
score(q) = a · rm(q) + (1 − a) · ra(q) (1)
</equation>
<bodyText confidence="0.9985635">
By setting s = 20, m = 2.5, a = 0.8 (based on
several tests), we select the top 130 bigram queries
which form the pool of bursty topics for our anal-
ysis. Table 1 shows some of these topics, covering
multiple domains: politics, science, art, sports, en-
tertainment, etc.
</bodyText>
<table confidence="0.997375125">
ID Topic ID Topic
1 kentucky election 66 orlando hernandez
2 indiana election 75 daniel biechele
8 van goph 81 hurricane forecast
24 north korea 92 93 memorial
34 pacific quake 113 holloway case
52 florida fires 128 stephen colbert
63 hunger strike 130 bear attack
</table>
<tableCaption confidence="0.999898">
Table 1: Examples of News Topics
</tableCaption>
<subsectionHeader confidence="0.999662">
3.2 Context extraction from multiple sources
</subsectionHeader>
<bodyText confidence="0.999508833333333">
Once we select the pool of bursty topics, we gather
the contexts of each topic from multiple sources:
query log, news media, blog media, and social book-
marks. We assume that the language in these con-
texts will reflect the reactions of the bursty events in
corresponding online media.
</bodyText>
<subsectionHeader confidence="0.970529">
3.2.1 Super query context
</subsectionHeader>
<bodyText confidence="0.9997172">
The most straightforward context of bursty events
in web search is the query string, which directly
reflects the users’ interests and perspectives in the
topic. We therefore define the first type of context of
a bursty topic in query log as the set of surrounding
terms of that bursty bigram in the (longer) queries.
For example, the word aftermath in the query “haiti
earthquake aftermath” is a term in the context of the
bursty topic haiti earthquake.
Formally, we define a Super Query of a bursty
topic t, sq(t), as the query which contains the bi-
gram query t lexically as a substring. For each
bursty topic t, we scan the whole query log Q and
retrieve all the super queries of t to form the context
which is represented by SQ(t).
</bodyText>
<equation confidence="0.803726">
SQ(t) = {q|q E Q and q = sq(t)}
</equation>
<page confidence="0.943366">
1079
</page>
<bodyText confidence="0.999825666666667">
SQ(t) is defined as the super query context of t.
For example, the super query context of ”kentucky
election” contains terms such as “2006,” “results,”
“christian county,” etc. These terms indicate what
aspects the users are most interested in Kentucky
Election during May 2006.
The super query context is widely explored by
search engines to provide query expansion and
query completion (Jones et al., 2006).
</bodyText>
<subsectionHeader confidence="0.945959">
3.2.2 Query session context
</subsectionHeader>
<bodyText confidence="0.999973909090909">
Another interesting context of a bursty topic in
query log is the sequence of queries that a user
searches after he submitted the bursty query q. This
context usually reflects how a user reformulates the
representation of his information need and implicitly
clarifies his interests in the topic.
We define a Query Session containing a bursty
topic t, qs(t), as the queries which are issued by the
same user after he issued t, within 30 minutes. For
each bursty topic t, we collect all the qs(t) to form
the query session context of t, QS(t):
</bodyText>
<equation confidence="0.9053">
QS(t) = {q|q E Q and q E qs(t)}
</equation>
<bodyText confidence="0.999804">
In web search, the query session context is usu-
ally utilized to provide query suggestion and query
reformulation (Radlinski and Joachims, 2005).
</bodyText>
<subsectionHeader confidence="0.87402">
3.2.3 News contexts
</subsectionHeader>
<bodyText confidence="0.999944384615385">
News articles written by critics and journalists re-
flect the reactions and perspectives of such profes-
sional group of people to a bursty event. We col-
lect news articles about these 130 bursty topics from
Google News2, by finding the most relevant news
articles which (1) match the bursty topic t, (2) were
published in May, 2006, and (3) were published by
any of the five major news medias: CNN, NBC,
ABC, New York Times and Washington Post.
We then retrieve the title and body of each news
article. This provides us two contexts of each bursty
topic t: the set of relevant news titles, NT(t), and
the set of relevant news bodies, NB(t).
</bodyText>
<subsectionHeader confidence="0.96528">
3.2.4 Blog contexts
</subsectionHeader>
<bodyText confidence="0.999582">
Compared with news articles, blog articles are
written by common users in the online communi-
ties, which are supposed to reflect the reactions and
</bodyText>
<footnote confidence="0.73766">
2http://news.google.com/
</footnote>
<bodyText confidence="0.9997366">
opinions of the public to the bursty events. We col-
lect blog articles about these 130 topics from Google
Blog3, by finding the most relevant blog articles
which (1) match the bursty topic t, (2) were pub-
lished in May, 2006 (3) were published in the most
popular blog community, Blogspot4. We then re-
trieve the title and body of each relevant blog post re-
spectively. This provides another two contexts: the
set of relevant blog titles, BT (t), and the set of rel-
evant blog bodies, BB(t).
</bodyText>
<subsectionHeader confidence="0.970795">
3.2.5 Social bookmarking context
</subsectionHeader>
<bodyText confidence="0.999976733333333">
Social bookmarks form a new source of social
media that allows the users to tag the webpages they
are interested in and share their tags with others. The
tags are supposed to reflect how the users describe
the content of the pages and their perspectives of the
content in a concise way.
We use a sample of Delicious5 bookmarks in May,
2006, which contains around 1.37M unique URLs.
We observe that the bursty bigram queries are also
frequently used as tags in Delicious. We thus con-
struct another context of bursty events by collecting
all the tags that are used to tag the same URLs as the
bursty topic.
Formally, we define DT (t) as the context of so-
cial tags of a topic t,
</bodyText>
<equation confidence="0.917456">
DT(t) = {tag|]url, s.t. tag, t E B(url)},
</equation>
<bodyText confidence="0.9994735">
where url is a URL and B(url) stands for the set of
all bookmarks of url.
</bodyText>
<subsectionHeader confidence="0.999795">
3.3 Context Statistics
</subsectionHeader>
<bodyText confidence="0.9878043">
Now we have constructed the set of 130 bursty
topics and 7 corresponding contexts from various
sources. We believe that these contexts well repre-
sent the various types of online media and sources.
For each context, we then clean the data by re-
moving stopwords and the bursty topic keywords
themselves. We then represent it as either the set
of unigrams or bigrams from this context. Table 2
shows the basic statistics of each context:
From Table 2 we observe the following facts:
</bodyText>
<listItem confidence="0.989941">
• The query session context covers more terms
(both unigrams and bigrams) than the super query
</listItem>
<footnote confidence="0.999903666666667">
3http://blogsearch.google.com/
4http://www.blogspot.com/
5http://delicious.com/
</footnote>
<page confidence="0.834952">
1080
</page>
<table confidence="0.917887466666667">
N T S M S A U M U A B M B
SQ 130 76k 5.3k 32.7 390 24.3 235
QS 126 108k 5.8k 224 1.5k 150 1062
NT 118 4.7k 411 105 627 102 722
NB 118 4.7k 411 4.7k 22k 22k 257k
BT 128 5.8k 99 184 459 169 451
BB 128 5.8k 99 4.1k 15k 12k 69k
DT 71 2.3k 475 137 2.0k N/A N/A
N: The number of topics covered
T S: The total number of records/documents
M S: The max number of records/documents per topic
A U: The avg number of unique unigrams per topic
M U: The max number of unique unigrams
A B: The avg number of unique bigrams
M B: The max number of unique bigrams
</table>
<tableCaption confidence="0.999481">
Table 2: Basic statistics of collections
</tableCaption>
<bodyText confidence="0.983358">
context. In both contexts, the average number of
unique bigrams is smaller than unigrams. This is
because queries in search are usually very short. Af-
ter removing stopwords and topic keywords, quite a
few queries have no bigram in these contexts.
</bodyText>
<listItem confidence="0.866964142857143">
• News articles and blog articles cover most of the
bursty topics and contain a rich set of unigrams and
bigrams in the corresponding contexts.
• The Delicious context only covers less than 60%
of bursty topics. We couldn’t extract bigrams from
bookmarks since delicious provides a “bag-of-tags”
interface.
</listItem>
<bodyText confidence="0.993698333333333">
In Section 4, we present a comprehensive analy-
sis of these different contexts of bursty topics, with
three different types of comparison.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.9996765">
In this section, we present a comprehensive compar-
ative analysis of the different contexts, which repre-
sent the reactions to the bursty topics in correspond-
ing sources.
</bodyText>
<subsectionHeader confidence="0.992302">
4.1 Similarity &amp; Predictability analysis
</subsectionHeader>
<bodyText confidence="0.999980714285714">
Our first task is to compare the content similarity
of these sources. This will help us to understand
how well the language usage in one context can be
leveraged to predict the language usage in another
context. This is especially useful to predict the con-
tent in web search. By representing each context
of a bursty topic as a vector space model of uni-
grams/bigrams, we first compute and compare the
average cosine similarity between contexts. We only
include contexts with more than 5 unigram/bigrams
into this comparison. The results are shown in Table
A and Table B, respectively. Each table is followed
by a heat map to visualize the pattern.
To investigate how well one source can predict
the content of another, we also represent each con-
text of a bursty topic as a unigram/bigram language
model and compute the Cross Entropy (Kullback
and Leibler, 1951) between every pairs of contexts.
Cross Entropy measures how certain one probabil-
ity distribution predicts another. We calculate such
measure based on the following definition:
</bodyText>
<equation confidence="0.998733">
HCE(m||n) = H(m) + DKL(m||n)
</equation>
<bodyText confidence="0.9999588">
We smooth the unigram language models using
Laplace smoothing (Field, 1988) and the bigram lan-
guage models using Katz back-off model (Katz,
1987).
The results are shown in Table C and Table D,
followed by the corresponding heat maps. For each
value HCE(m||n) in the table cell, m stands for the
context in the row and n stands for the context in
the column. Please note that in Figure 3, 4, a larger
HCE value corresponds to a lighter cell.
</bodyText>
<sectionHeader confidence="0.942632" genericHeader="evaluation">
4.1.1 Results
</sectionHeader>
<bodyText confidence="0.999613">
From the results shown in Table A-D, or in Fig-
ure 1- 4 more visually, some interesting phenomena
can be observed:
</bodyText>
<listItem confidence="0.793045722222222">
• Compared with other contexts, query session
is much more similar to the super query. This
makes sense because many super queries would be
included in the query session.
• Compared with news and blog, the delicious
context is closer to the query log context. In fact, de-
licious is reasonably close to all the other contexts.
This means social tags could be an effective source
to enhance bursty topics in web search in terms of
query suggestion. However, as Table 2 shows, only
less than 60% of topics can be covered by delicious
tag. We have to explore other sources to make a
comprehensive prediction.
• In the news and blog contexts, the title contexts
are more similar to the query contexts than the body
contexts. This may be because titles usually con-
cisely describe the topic while bodies contain much
more details and irrelevant contents.
</listItem>
<page confidence="0.8416">
1081
</page>
<table confidence="0.977256823529412">
Context SQ QS NT NB BT BB DT
SQ 1.0 0.405 0.122 0.072 0.119 0.061 0.188
QS 0.405 1.0 0.049 0.062 0.066 0.054 0.112
NT 0.122 0.049 1.0 0.257 0.186 0.152 0.120
NB 0.072 0.062 0.257 1.0 0.191 0.362 0.114
BT 0.119 0.066 0.186 0.191 1.0 0.242 0.141
BB 0.061 0.054 0.152 0.362 0.242 1.0 0.107
DT 0.188 0.112 0.120 0.114 0.141 0.107 1.0
Table A: Cosine similarity for unigram vectors Figure 1: Heat map of table A
Source SQ QS NT NB BT BB
SQ 1.0 0.290 0.028 0.024 0.047 0.027
QS 0.290 1.0 0.004 0.010 0.011 0.009
NT 0.028 0.004 1.0 0.041 0.026 0.011
NB 0.024 0.010 0.041 1.0 0.023 0.040
BT 0.047 0.011 0.026 0.023 1.0 0.044
BB 0.027 0.009 0.011 0.040 0.044 1.0
We do not build bigram vector for DT
</table>
<tableCaption confidence="0.962238">
Table B: Cosine similarity for bigram vectors Figure 2: Heat map of table B
</tableCaption>
<table confidence="0.962127058823529">
Source SQ QS NT NB BT BB DT
SQ 1.698 4.911 7.538 8.901 7.948 9.050 7.498
QS 7.569 3.842 9.487 11.130 9.997 11.546 8.972
NT 8.957 10.868 3.718 7.946 9.006 9.605 8.825
NB 11.217 12.897 11.317 7.241 12.282 11.582 11.739
BT 9.277 11.084 9.085 10.295 4.637 9.365 9.180
BB 11.053 12.842 11.593 11.742 12.001 7.232 11.525
DT 8.457 9.794 8.521 9.511 8.831 9.473 2.990
Table C: Cross entropy for unigram distribution Figure 3: Heat map of table C
Source SQ QS NT NB BT BB
SQ 1.891 2.685 4.290 4.540 4.319 4.607
QS 6.800 3.430 8.144 9.049 8.528 9.304
NT 5.444 5.499 3.652 4.733 5.106 5.218
NB 11.572 11.797 11.254 8.731 11.544 11.073
BT 5.664 5.674 5.503 5.495 4.597 5.301
BB 10.745 10.796 10.517 10.455 10.526 8.518
We do not build bigram distribution for DT
</table>
<tableCaption confidence="0.997">
Table D: Cross Entropy for bigram distribution Figure 4: Heat map of table D
</tableCaption>
<page confidence="0.952166">
1082
</page>
<table confidence="0.92014475">
HCE(SQ||n)
n: NT BT NB BB
Uni: 7.538 7.948 8.901 9.050
Bi: 4.290 4.319 4.540 4.607
HCE(M||SQ)
m: NT BT NB BB
Uni: 8.927 9.277 11.217 11.053
Bi: 5.445 5.664 11.572 10.745
</table>
<tableCaption confidence="0.998721">
Table 3: Cross-entropy among three sources
</tableCaption>
<listItem confidence="0.938300965517242">
• News would be a better predictor of the query
than blog in general. This is interesting, which indi-
cates that many search activities may be initialized
by reading the news.
• News and blogs are much more similar to each
other than query logs. We hypothesize that this re-
sult reflects the behavior how people write blogs
about bursty events – typically they may have read
several news articles before writing their own blog.
In the blog, they may directly quote or retell a part
of the news article and then add their opinion.
• Table 3 reveals the generation relations among
three sources: query, news and blog. From the
upper table, we can observe that queries are more
likely to be generated by news articles, rather than
blog articles. From the lower table, we can observe
that queries are more likely to generate blog arti-
cles(body), rather than news articles(body). This re-
sult is quite interesting, which indicates the users’
actual behaviors: when a bursty event happens, users
would search them from web after they read it from
some news articles. And users would write their
own blogs to discuss the event after they retrieve and
digest information from the web.
• From Table 3 we also find that queries are more
likely to generate news title, rather than blog title. It
is natural since blogs are written by kinds of people.
The content especially the title part contains more
uncertainty.
</listItem>
<subsectionHeader confidence="0.954881">
4.1.2 Case study
</subsectionHeader>
<bodyText confidence="0.998138875">
We then conduct the analysis to the level of indi-
vidual topics. Table 4 shows the correlation of each
pair of contexts, computed based on the similarity
between topics in SQ and corresponding topics in
these contexts. We can observe that News and Blog
are correlated with each other tightly. If one is a
good predictor of bursty queries, the other one also
tends to be.
</bodyText>
<table confidence="0.998608666666667">
QS NT NB BT BB DT
QS 0.46 0.59 0.58 0.75 0.46
NT 0.73 0.79 0.59 0.61
NB 0.71 0.68 0.61
BT 0.78 0.59
BB 0.48
</table>
<tableCaption confidence="0.999878">
Table 4: Correlations of the similarity with SQ
</tableCaption>
<bodyText confidence="0.99993244">
For some topics like “stephen colbert,” and “three
gorges,” both News and Blog are quite similar to
the queries, which implies some intrinsic properties
(coherence) of these topics: users would refer to the
same content when using the topic terms in different
sources.
We also find that a few topics like “hot
dogs,” “bear attack,” for which the similarity of
(SQ, News) and (SQ, Blog) are both low. It is
probably because these topics are too diverse and
carries a lot of ambiguity.
Although in most cases they are correlated, some-
times News and Blog show different trends in the
similarity to the queries. For example, News is
quite similar to the queries on the topics such as
“holloway case” and “jazz fest” while Blog is dis-
similar. For these unfamiliar topics, users possi-
bly search the web “after” they read the news arti-
cles and express their diverse opinions in the blog.
In contrast, on the topics like “insurance rate” or
“consolidation loans,” Blog is similar to the queries
while News is not. For these daily-life-related
queries, users would express the similar opinions
when they search or write blogs, while news articles
typically report such “professional” viewpoints.
</bodyText>
<subsectionHeader confidence="0.999803">
4.2 Coverage analysis
</subsectionHeader>
<bodyText confidence="0.999864222222222">
Are social bookmarks the best source to predict
bursty content in search? It looks so from the sim-
ilarity comparison, if they have a good coverage of
search contents. In this experiment, we analyze the
coverage of query contexts in other contexts in a sys-
tematic way. If the majority of terms in the super
query context would be covered by a small propor-
tion of top words from another source, this source
has the potential.
</bodyText>
<page confidence="0.980807">
1083
</page>
<subsectionHeader confidence="0.815172">
4.2.1 Unigram coverage
</subsectionHeader>
<bodyText confidence="0.996618384615385">
We first analyze the coverage of unigrams from
the super query context in four other contexts: Q5,
DT, News (the combination of NT and NB) and
Blog (the combination of BT and BB) to compare
with 5Q. For each source, we rank the unigrams by
frequency. Figure 5(a) shows the average trend of
SQ-unigram coverage in different sources. The x-
coordinate refers to the ratio of top unigrams in one
source to the number of unigrams in 5Q. For ex-
ample, if 5Q contains n unigrams, the ratio 2 stands
for the top 2n unigrams in the other source. The y-
coordinate refers to the coverage rate of 5Q. We can
observe that:
</bodyText>
<listItem confidence="0.991618625">
• Query Session naturally covers most of the su-
per query terms (over 70%).
• Though delicious tags are more similar to
queries than news and blog, as well as a relatively
higher coverage rate than the other two while size
ratio is small, the overall coverage rate is quite low:
only 21.28%. Note that this is contradict to existing
comparative studies between social bookmarks and
search logs (Bischoff et al., 2008). Clearly, when
considering bursty queries, the coverage and effec-
tiveness of social bookmarks is much lower than
considering all queries. Handling bursty queries is
much more difficult; only using social bookmarks to
predict queries is not a good choice. Other useful
sources should be enrolled.
• As the growth of the size ratio, the coverage
rate of news and blogs are both gradually increased.
When stable, both of them arrive at a relatively high
level (news: 66.36%, blog: 63.80%), which means
news and blogs have a higher potential to predict the
bursty topics in search. Moreover, in most cases,
news is still prior to blog – not only the overall rate,
but also the size ratio comparison while the coverage
rate reaches 50% (news:109 &lt; blog:183).
</listItem>
<subsectionHeader confidence="0.700374">
4.2.2 Bigram Coverage
</subsectionHeader>
<bodyText confidence="0.999961578947369">
Also we analyze the bigram coverage. This time
we only have 3 sources (no DT). We rank the bi-
grams by the pointwise mutual information instead
of frequency, since not all the bigrams are “real” col-
locations. Figure 5(b) shows the results.
Different from the unigram coverage, except that
the query session can naturally keep a high coverage
rate (66.07%), both news and blog cover poorly. For
this issue, we should re-consider the behavior that
users search and write articles. News or blog arti-
cles consist of completed sentences and paragraphs
which would contain plenty of meaningful bigrams.
However search queries consist of keywords – rel-
atively discrete and regardless of order. Therefore,
except some proper nouns such as person’s name, a
lot of bigrams in the query log are formed in an ad-
hoc way. Since the different expressions of search
and writing, detecting unigrams is more informa-
tional than bigrams.
</bodyText>
<subsectionHeader confidence="0.999992">
4.3 Coherence analysis
</subsectionHeader>
<bodyText confidence="0.999969071428571">
The above two experiments discuss the inter-
relations among different contexts. In this section
we will discuss the inner-relation within each par-
ticular context – when it comes to a particular bursty
topic, how coherent is the information in each con-
text? Does the discussion keep consistent, or slip
into ambiguity?
We represent all the terms forming each context of
a bursty topic as a weighted graph: G = (V, E, W),
where each v E V stands for each term, wv stands
for the weight of vertex v in G, and each e E E
stands for the semantic closeness between a pair of
terms (u, v) measured by sim(u, v). We define the
density of such a semantic graph as follows:
</bodyText>
<equation confidence="0.991735666666667">
Den(G) = Eu,
vEV,u=,4vsim(u, v)wuwv (2)
rIu,vEV,u=j4vwuwv
</equation>
<bodyText confidence="0.999724214285714">
If sim(u, v) values the semantic similarity between
u and v, a high value of Den(G) implies that the
whole context is semantically consistent. Otherwise,
it may be diverse or ambiguous.
We build the graph of each context based on
WordNet6. For a pair of words, WordNet provides a
series of measures of the semantic similarity (Peder-
sen et al., 2004). We use the Path Distance Similar-
ity (path for short) and Lin Similarity (lin for short)
to measure sim(u, v). Both measures range in [0, 1].
For the convenience of computation, we choose
the top 1100 unigrams ranked by term frequency in
each source (if any) to represent the whole context
on one specific topic.
</bodyText>
<footnote confidence="0.98214">
6http://wordnet.princeton.edu/
</footnote>
<page confidence="0.996531">
1084
</page>
<figure confidence="0.999005">
(a) Unigram (b) Bigram
</figure>
<figureCaption confidence="0.999493">
Figure 5: Coverage results
</figureCaption>
<subsectionHeader confidence="0.67111">
4.3.1 Overall
</subsectionHeader>
<bodyText confidence="0.7655315">
Table 5 shows the average overall density of each
sources over all the topics. From the table we can
</bodyText>
<table confidence="0.996777625">
Source path lin
SQ 0.098 0.128
QS 0.071 0.082
NT 0.103 0.129
NB 0.109 0.139
BT 0.099 0.109
BB 0.116 0.147
DT 0.102 0.127
</table>
<tableCaption confidence="0.998716">
Table 5: Overall Density
</tableCaption>
<bodyText confidence="0.999979294117647">
observe that Q5 has the lowest density in both of
the measures. It is because the queries in one user
session can easily shift to other (irrelevant) topics
even in a short time.
Another interesting phenomenon comes out that
for either news or blog, the body is denser than the
title, even if the body context contains much more
terms. It can be explained by the roles of the title
and the body in one article: the title contains a series
of words which briefly summarize a topic while the
body part would describe and discuss the title in de-
tails. When it maps to the semantic word network,
the title tends to contain the vertices scattered in the
graph, while the context of the body part would add
more semantically related vertices around the origi-
nal vertices to strength the relations. Thus, the body
part has a higher density than the title part.
</bodyText>
<subsectionHeader confidence="0.635305">
4.3.2 The trend analysis
</subsectionHeader>
<bodyText confidence="0.998137047619048">
Figure 6 shows the tendency of the density in each
source. The x-coordinate refers to the TopN uni-
grams ranked by the term frequency in each source.
From Figure 6 we can find that in most cases, the av-
erage density will gradually decrease while less im-
portant terms are added, which implies that the most
important terms are denser, and other terms would
disperse the topic.
To better evaluate this tendency of each source,
Table 6 shows the change rate of the highest density
to the overall density measured by lin. We can easily
find the following facts:
• The highest density is achieved when a small
proportion of top terms are counted (6 sources for
Top5 and one for Top20), which also supports our
hypothesis: the more important, the more coherent.
• BB’s density drops the fastest of all (15.1%),
following by DT(10.6%). It may be because both
blog and delicious tag are generated by many users.
And the diversity of the users leads to different
prospectives, which dilutes the context significantly.
</bodyText>
<listItem confidence="0.892210181818182">
• Both NT and NB drop quite slowly (5.8%,
6.8%), which means the professional journalists
would have the relatively similar prospectives on the
same topic. Thus the topic does not disperse too
much. BT also keeps a high stability.
• Compared with news, blog is easier to disperse,
which can be reflected by the density comparison
between NT and BT. Although the density of BB
is still higher than NT, we should notice that these
two sources are not completely covered – about 3/4
unigrams in these two contexts are not included in
</listItem>
<page confidence="0.978215">
1085
</page>
<figure confidence="0.99866">
(a) path (b) lin
</figure>
<figureCaption confidence="0.99999">
Figure 6: Trend of Density
</figureCaption>
<bodyText confidence="0.996603">
the semantic networks. The curves clearly shows
BB dropped faster than NB. One can expect that
NB becomes denser than BB if all the unigrams in
both sources are included in the network.
</bodyText>
<table confidence="0.998216875">
Source Highest Den. Overall Den. Change
SQ 0.140(Top5) 0.128 -8.6%
QS 0.091(Top5) 0.082 -9.9%
NT 0.137(Top5) 0.129 -5.8%
NB 0.148(Top5) 0.139 -6.8%
BT 0.114(Top20) 0.109 -4.4%
BB 0.172(Top5) 0.147 -15.1%
DT 0.142(Top5) 0.127 -10.6%
</table>
<tableCaption confidence="0.999403">
Table 6: Tendency analysis of Density (Lin)
</tableCaption>
<subsectionHeader confidence="0.95135">
4.3.3 Case Study
</subsectionHeader>
<bodyText confidence="0.99996415">
From these 130 news topics, some of them shows
a special tendency of coherence. For example,
when more words are included, the density of the
topic“three gorge” drops rapidly in most of the
sources. The topic “florida fires” has the same
trend. These topics are typically “focus” topics,
which means users clearly pursue the unique event
while they use these terms. Thus, the density in top
unigrams is very high. It drops rapidly since users’
personal interests and opinions toward to this event
will be enrolled gradually.
In contrast, some topics like “heather mills,”, “in-
surance rate” express differently: their densities
gradually increase with the growth of the terms. By
observing these topics we find they are usually di-
verse topics (e.g: famous person name or entity
name), which may lead to diverse search intentions
of users. So the density of top unigrams is low and
gradually increased since one main aspect is proba-
bly strengthened.
</bodyText>
<sectionHeader confidence="0.993986" genericHeader="conclusions">
5 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999984">
In this paper, we have studied and compared how
the web content reacts to bursty events in multi-
ple contexts of web search and online media. Af-
ter a series of comprehensive experiments including
content similarity and predictability, the coverage
of search content, and semantic diversity, we found
that social bookmarks are not enough to predict the
queries because of a low coverage. Other sources
like news and blogs need to be added. Furthermore,
news can be seen as a consistent source which would
not only trigger the discussion of bursty events in
blogs but also in search queries.
When the target is to diversify the search results
and query suggestions, blogs and social bookmarks
are potentially useful accessory sources because of
the high diversity of content.
Our work serves as a feasibility investigation of
query suggestion for bursty events. Future work
would address on how to systematically predict and
recommend the bursty queries using online media,
as well as a reasonable evaluation metrics upon it.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9980616">
We thank Prof. Kevin Chang for his support in data
and useful discussion. We thank the three anony-
mous reviewers for their useful comments. This
work is in part supported by the National Science
Foundation under award number IIS-0968489.
</bodyText>
<page confidence="0.994378">
1086
</page>
<sectionHeader confidence="0.995877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999265990196078">
Jon Kleinberg 2003. Bursty and Hierarchical Structure
in Streams Data Mining and Knowledge Discovery,
Vol 7(4):373-397
Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak and
Andrew Tomkins 2005. The Predictive Power of On-
line Chatter KDD ’05: Proceedings of the eleventh
ACM SIGKDD international conference on Knowl-
edge discovery in data mining, 78-87.
Ravi Kumar, Jasmine Novak, Prabhakar Raghavan and
Andrew Tomkins 2003. On the Bursty Evolution of
Blogspace WWW ’03: Proc. of the 12th International
World Wide Web Conference, 568-576.
Michail Vlachos and Christopher Meek and Zografoula
Vagena and Dimitrios Gunopulos 2004. Identifying
similarities, periodicities and bursts for online search
queries SIGMOD ’04: Proceedings of the 2004 ACM
SIGMOD international conference on Management of
data, 131-142.
Evgeniy Gabrilovich, Susan Dumais and Eric Horvitz
2004. Newsjunkie: providing personalized newsfeeds
via analysis of information novelty WWW ’04: Pro-
ceedings of the 13th international conference on World
Wide Web, 482-490.
Eytan Adar, Daniel S. Weld, and Brian N. Bershad and
Steven S. Gribble 2007. Why we search: visualizing
and predicting user behavior WWW ’07: Proceedings
of the 16th international conference on World Wide
Web, 161-170.
Aixin Sun, Meishan Hu and Ee-Peng Lim 2008. Search-
ing blogs and news: a study on popular queries SI-
GIR ’08: Proceedings of the 31st annual international
ACM SIGIR conference on Research and development
in information retrieval, 729-730.
JeanPhilippe Cointet, Emmanuel Faure and Camille Roth
2008. Intertemporal topic correlations in online media
: A Comparative Study on Weblogs and News Web-
sites ICWSM ’08: International Coference on We-
blogs and Social Media
Levon Lloyd, Prachi Kaulgud and Steven Skiena 2006.
Newspapers vs. Blogs: Who Gets the Scoop? AAAI
Spring Symposium on Computational Approaches to
Analyzing Weblogs
Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel
Fisher, Matthew Hurst, and Arnd Christian Konig
2008. BLEWS: Using Blogs to Provide Context for
News Articles ICWSM ’08: International Coference
on Weblogs and Social Media
Sanjay Sood, Sara Owsley, Kristian Hammond and Larry
Birnbaum 2007. TagAssist: Automatic Tag Sugges-
tion for Blog Posts ICWSM ’07: International Cofer-
ence on Weblogs and Social Media
Fernando Diaz 2009. Integration of news content into
web results WSDM ’09: Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, 182-191.
Beate Krause, Andreas Hotho and Gerd Stumme 2008.
A Comparison of Social Bookmarking with Tradi-
tional Search Advances in Information Retrieval, Vol
4956/2008:101-113.
Paul Heymann, Georgia Koutrika and Hector Garcia-
Molina 2008. Can social bookmarking improve web
search? WSDM ’08: Proceedings of the international
conference on Web search and web data mining, 195-
206.
Shenghua Bao, Guirong Xue, Xiaoyuan Wu, Yong Yu,
Ben Fei and Zhong Su 2007. Optimizing web search
using social annotations WWW ’07: Proceedings of
the 16th international conference on World Wide Web,
501-510.
Kerstin Bischoff, Claudiu S. Firan, Wolfgang Nejdl and
Raluca Paiu 2008. Can all tags be used for search?
CIKM ’08: Proceeding of the 17th ACM conference
on Information and knowledge management, 193-202.
Rosie Jones, Benjamin Rey and Omid Madani 2006.
Generating query substitutions Proceedings of the
15th international conference on World Wide Web,,
387-396.
Filip Radlinski and Thorsten Joachims 2005. Query
chains: learning to rank from implicit feedback Pro-
ceedings of the 11th ACM SIGKDD international con-
ference on Knowledge discovery in data mining, 239-
248.
Qiaozhu Mei, Dengyong Zhou and Kenneth Church
2008. Query suggestion using hitting time CIKM ’08:
Proceeding of the 17th ACM conference on Informa-
tion and knowledge management, 469-478
Andrei Broder 2002. A Taxonomy of Web Search SIGIR
Forum, Vol 36(2):3-10.
Solomon Kullback and Richard Leibler 1951. On In-
formation and Sufficience Annals of Mathematical
Statistics, Vol 22(1):79-86.
David A. Field 1988. Laplacian Smoothing and Delau-
nay Triangulations Communications in Applied Nu-
merical Methods, Vol 4:709-712.
Stephen M. Katz 1987 Estimation of probabilities from
sparse data for the language model component of a
speech recogniser IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3), 400-401.
Ted Pedersen, Siddharth Patwardhan and Jason Miche-
lizzi 2004. WordNet::Similarity: measuring the relat-
edness of concepts PHLT-NAACL ’04: Demonstration
Papers at HLT-NAACL 2004 on XX, 38-41.
</reference>
<page confidence="0.99578">
1087
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945004">
<title confidence="0.991048">Context Comparison of Bursty Events in Web Search and Online Media</title>
<author confidence="0.996574">Yunliang Jiang Cindy Xide Lin Qiaozhu Mei</author>
<affiliation confidence="0.99999">University of Illinois University of Illinois University of Michigan</affiliation>
<address confidence="0.99994">Urbana, IL, 61801 Urbana, IL, 61801 Ann Arbor, MI, 48109</address>
<email confidence="0.998727">jiang8@illinois.eduxidelin2@illinois.eduqmei@umich.edu</email>
<abstract confidence="0.996572055555556">In this paper, we conducted a systematic comparative analysis of language in different contexts of bursty topics, including web search, news media, blogging, and social bookmarking. We analyze (1) the content similarity and predictability between contexts, (2) the coverage of search content by each context, and (3) the intrinsic coherence of information in each context. Our experiments show that social bookmarking is a better predictor to the bursty search queries, but news media and social blogging media have a much more compelling coverage. This comparison provides insights on how the search behaviors and social information sharing behaviors of users are correlated to the professional news media in the context of bursty events.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
</authors>
<date>2003</date>
<booktitle>Bursty and Hierarchical Structure in Streams Data Mining and Knowledge Discovery, Vol</booktitle>
<pages>7--4</pages>
<contexts>
<context position="4739" citStr="Kleinberg, 2003" startWordPosition="794" endWordPosition="795">s-entropy between sources, the coverage of search queries in online media, and an in-depth semantic comparison of sources based on language networks. In the rest of this paper, a summary of related work is briefly described in Section 2. We then present the experiments setup in Section 3, The results of the experiments is presented in Section 4. Finally, our major findings from the comparative analysis are drawn in Section 5. 2 Related Work Recently, a rich body of work has focused on how to find the bursting patterns from time-series data using various approaches such as time-graph analysis (Kleinberg, 2003; Kuman et al., 2003), contextbased analysis (Gabrilovich et al., 2004), movingaverage analysis (Vlachos et al., 2004), and frequency analysis (Gruhl et al., 2005), etc. These methods are all related to the preprocessing step of our analysis: detecting bursty queries from the query log effectively. The comparison of two web sources at a time is widely studied recently. (Sood et al., 2007) discussed how to leverage the relation between social tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has a</context>
<context position="8912" citStr="Kleinberg, 2003" startWordPosition="1491" endWordPosition="1493">rt period of time. We explore a sample of the log of the Microsoft Live search engine1, which contains 14.9M search records over 1 month (May 2006). 3.1.1 Find bursty queries from query log How to extract the queries that represent bursty events? We believe that bursty queries present the pattern that its day-by-day search volume shows a significant spike – that is, the frequency that the user submit this query should suddenly increase at one specific time and drop down after a while. This assumption is consistent with existing work of finding bursty patterns in emails, scientific literature (Kleinberg, 2003), and blogs (Gruhl et al., 2005). Following (Gruhl et al., 2005), we utilize a simple but effective method to collect bursty topics in the query log data as follows: • We choose bigrams as the basic presentation of bursty topics since bigrams present the information need of users more clearly and completely than unigrams and also have a larger coverage in the query log comparing to n-grams (n &gt; 3). • We only consider the bigram queries which appear more frequently than a threshold s per month. This is reasonable since a bursty event usually causes a large volume of search activities. • Let fma</context>
</contexts>
<marker>Kleinberg, 2003</marker>
<rawString>Jon Kleinberg 2003. Bursty and Hierarchical Structure in Streams Data Mining and Knowledge Discovery, Vol 7(4):373-397</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gruhl</author>
<author>R Guha</author>
<author>Ravi Kumar</author>
<author>Jasmine Novak</author>
<author>Andrew Tomkins</author>
</authors>
<date>2005</date>
<booktitle>The Predictive Power of Online Chatter KDD ’05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,</booktitle>
<pages>78--87</pages>
<contexts>
<context position="4902" citStr="Gruhl et al., 2005" startWordPosition="818" endWordPosition="821">st of this paper, a summary of related work is briefly described in Section 2. We then present the experiments setup in Section 3, The results of the experiments is presented in Section 4. Finally, our major findings from the comparative analysis are drawn in Section 5. 2 Related Work Recently, a rich body of work has focused on how to find the bursting patterns from time-series data using various approaches such as time-graph analysis (Kleinberg, 2003; Kuman et al., 2003), contextbased analysis (Gabrilovich et al., 2004), movingaverage analysis (Vlachos et al., 2004), and frequency analysis (Gruhl et al., 2005), etc. These methods are all related to the preprocessing step of our analysis: detecting bursty queries from the query log effectively. The comparison of two web sources at a time is widely studied recently. (Sood et al., 2007) discussed how to leverage the relation between social tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has aimed to utilize one external web source to help web search. For example, (Diaz, 2009) integrated the news results into general search. (Bao et al., 2007; Heymann e</context>
<context position="8944" citStr="Gruhl et al., 2005" startWordPosition="1496" endWordPosition="1499"> a sample of the log of the Microsoft Live search engine1, which contains 14.9M search records over 1 month (May 2006). 3.1.1 Find bursty queries from query log How to extract the queries that represent bursty events? We believe that bursty queries present the pattern that its day-by-day search volume shows a significant spike – that is, the frequency that the user submit this query should suddenly increase at one specific time and drop down after a while. This assumption is consistent with existing work of finding bursty patterns in emails, scientific literature (Kleinberg, 2003), and blogs (Gruhl et al., 2005). Following (Gruhl et al., 2005), we utilize a simple but effective method to collect bursty topics in the query log data as follows: • We choose bigrams as the basic presentation of bursty topics since bigrams present the information need of users more clearly and completely than unigrams and also have a larger coverage in the query log comparing to n-grams (n &gt; 3). • We only consider the bigram queries which appear more frequently than a threshold s per month. This is reasonable since a bursty event usually causes a large volume of search activities. • Let fmax(q) be the maximum search volum</context>
</contexts>
<marker>Gruhl, Guha, Kumar, Novak, Tomkins, 2005</marker>
<rawString>Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak and Andrew Tomkins 2005. The Predictive Power of Online Chatter KDD ’05: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 78-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Kumar</author>
<author>Jasmine Novak</author>
<author>Prabhakar Raghavan</author>
<author>Andrew Tomkins</author>
</authors>
<date>2003</date>
<booktitle>On the Bursty Evolution of Blogspace WWW ’03: Proc. of the 12th International World Wide Web Conference,</booktitle>
<pages>568--576</pages>
<marker>Kumar, Novak, Raghavan, Tomkins, 2003</marker>
<rawString>Ravi Kumar, Jasmine Novak, Prabhakar Raghavan and Andrew Tomkins 2003. On the Bursty Evolution of Blogspace WWW ’03: Proc. of the 12th International World Wide Web Conference, 568-576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michail Vlachos</author>
<author>Christopher Meek</author>
</authors>
<title>Zografoula Vagena and Dimitrios Gunopulos</title>
<date>2004</date>
<booktitle>SIGMOD ’04: Proceedings of the 2004 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>131--142</pages>
<marker>Vlachos, Meek, 2004</marker>
<rawString>Michail Vlachos and Christopher Meek and Zografoula Vagena and Dimitrios Gunopulos 2004. Identifying similarities, periodicities and bursts for online search queries SIGMOD ’04: Proceedings of the 2004 ACM SIGMOD international conference on Management of data, 131-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
</authors>
<title>Susan Dumais and Eric Horvitz</title>
<date>2004</date>
<booktitle>WWW ’04: Proceedings of the 13th international conference on World Wide Web,</booktitle>
<pages>482--490</pages>
<marker>Gabrilovich, 2004</marker>
<rawString>Evgeniy Gabrilovich, Susan Dumais and Eric Horvitz 2004. Newsjunkie: providing personalized newsfeeds via analysis of information novelty WWW ’04: Proceedings of the 13th international conference on World Wide Web, 482-490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
<author>Daniel S Weld</author>
<author>Brian N Bershad</author>
<author>Steven S Gribble</author>
</authors>
<title>Why we search: visualizing and predicting user behavior WWW ’07:</title>
<date>2007</date>
<booktitle>Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>161--170</pages>
<contexts>
<context position="5800" citStr="Adar et al., 2007" startWordPosition="972" endWordPosition="975">l tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has aimed to utilize one external web source to help web search. For example, (Diaz, 2009) integrated the news results into general search. (Bao et al., 2007; Heymann et al., 2008; Krause et al., 2008; Bischoff et al., 2008) focused on improving search by the social tags. Compared with the above, our comparison analysis tries to explore the interactions among multiple web sources including the search logs. Similar to our work, some recent work (Adar et al., 2007; Sun et al., 2008) has addressed the comparison among multiple web sources. For example, (Adar et al., 2007) did a comprehensive correlation study among queries, blogs, news and TV results. However, different from the content-free analysis above, our work compares the sources based on the content. Our work can lead to many useful search applications, such as query suggestion which takes as input a specific query and returns as output one or several suggested queries. The approaches include query term cooccurrence (Jones et al., 2006), query sessions (Radlinski and Joachims, 2005), and clickth</context>
</contexts>
<marker>Adar, Weld, Bershad, Gribble, 2007</marker>
<rawString>Eytan Adar, Daniel S. Weld, and Brian N. Bershad and Steven S. Gribble 2007. Why we search: visualizing and predicting user behavior WWW ’07: Proceedings of the 16th international conference on World Wide Web, 161-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aixin Sun</author>
</authors>
<title>Meishan Hu and Ee-Peng Lim</title>
<date>2008</date>
<booktitle>SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>729--730</pages>
<marker>Sun, 2008</marker>
<rawString>Aixin Sun, Meishan Hu and Ee-Peng Lim 2008. Searching blogs and news: a study on popular queries SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, 729-730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JeanPhilippe Cointet</author>
<author>Emmanuel Faure</author>
<author>Camille Roth</author>
</authors>
<title>Intertemporal topic correlations in online media : A Comparative</title>
<date>2008</date>
<booktitle>Study on Weblogs and News Websites ICWSM ’08: International Coference on Weblogs and Social</booktitle>
<location>Media</location>
<contexts>
<context position="5267" citStr="Cointet et al., 2008" startWordPosition="881" endWordPosition="884">s from time-series data using various approaches such as time-graph analysis (Kleinberg, 2003; Kuman et al., 2003), contextbased analysis (Gabrilovich et al., 2004), movingaverage analysis (Vlachos et al., 2004), and frequency analysis (Gruhl et al., 2005), etc. These methods are all related to the preprocessing step of our analysis: detecting bursty queries from the query log effectively. The comparison of two web sources at a time is widely studied recently. (Sood et al., 2007) discussed how to leverage the relation between social tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has aimed to utilize one external web source to help web search. For example, (Diaz, 2009) integrated the news results into general search. (Bao et al., 2007; Heymann et al., 2008; Krause et al., 2008; Bischoff et al., 2008) focused on improving search by the social tags. Compared with the above, our comparison analysis tries to explore the interactions among multiple web sources including the search logs. Similar to our work, some recent work (Adar et al., 2007; Sun et al., 2008) has addressed the comparison among multiple web</context>
</contexts>
<marker>Cointet, Faure, Roth, 2008</marker>
<rawString>JeanPhilippe Cointet, Emmanuel Faure and Camille Roth 2008. Intertemporal topic correlations in online media : A Comparative Study on Weblogs and News Websites ICWSM ’08: International Coference on Weblogs and Social Media</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levon Lloyd</author>
</authors>
<title>Prachi Kaulgud and Steven Skiena</title>
<date>2006</date>
<marker>Lloyd, 2006</marker>
<rawString>Levon Lloyd, Prachi Kaulgud and Steven Skiena 2006. Newspapers vs. Blogs: Who Gets the Scoop? AAAI Spring Symposium on Computational Approaches to Analyzing Weblogs</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Sumit Basu</author>
<author>Dmitriy Belenko</author>
<author>Danyel Fisher</author>
<author>Matthew Hurst</author>
<author>Arnd Christian Konig</author>
</authors>
<date>2008</date>
<booktitle>BLEWS: Using Blogs to Provide Context for News Articles ICWSM ’08: International Coference on Weblogs and Social</booktitle>
<location>Media</location>
<contexts>
<context position="5244" citStr="Gamon et al., 2008" startWordPosition="877" endWordPosition="880">the bursting patterns from time-series data using various approaches such as time-graph analysis (Kleinberg, 2003; Kuman et al., 2003), contextbased analysis (Gabrilovich et al., 2004), movingaverage analysis (Vlachos et al., 2004), and frequency analysis (Gruhl et al., 2005), etc. These methods are all related to the preprocessing step of our analysis: detecting bursty queries from the query log effectively. The comparison of two web sources at a time is widely studied recently. (Sood et al., 2007) discussed how to leverage the relation between social tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has aimed to utilize one external web source to help web search. For example, (Diaz, 2009) integrated the news results into general search. (Bao et al., 2007; Heymann et al., 2008; Krause et al., 2008; Bischoff et al., 2008) focused on improving search by the social tags. Compared with the above, our comparison analysis tries to explore the interactions among multiple web sources including the search logs. Similar to our work, some recent work (Adar et al., 2007; Sun et al., 2008) has addressed the compar</context>
</contexts>
<marker>Gamon, Basu, Belenko, Fisher, Hurst, Konig, 2008</marker>
<rawString>Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel Fisher, Matthew Hurst, and Arnd Christian Konig 2008. BLEWS: Using Blogs to Provide Context for News Articles ICWSM ’08: International Coference on Weblogs and Social Media</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjay Sood</author>
<author>Sara Owsley</author>
<author>Kristian Hammond</author>
<author>Larry Birnbaum</author>
</authors>
<title>TagAssist: Automatic Tag Suggestion for Blog Posts</title>
<date>2007</date>
<booktitle>ICWSM ’07: International Coference on Weblogs and Social</booktitle>
<location>Media</location>
<contexts>
<context position="5130" citStr="Sood et al., 2007" startWordPosition="856" endWordPosition="859">parative analysis are drawn in Section 5. 2 Related Work Recently, a rich body of work has focused on how to find the bursting patterns from time-series data using various approaches such as time-graph analysis (Kleinberg, 2003; Kuman et al., 2003), contextbased analysis (Gabrilovich et al., 2004), movingaverage analysis (Vlachos et al., 2004), and frequency analysis (Gruhl et al., 2005), etc. These methods are all related to the preprocessing step of our analysis: detecting bursty queries from the query log effectively. The comparison of two web sources at a time is widely studied recently. (Sood et al., 2007) discussed how to leverage the relation between social tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has aimed to utilize one external web source to help web search. For example, (Diaz, 2009) integrated the news results into general search. (Bao et al., 2007; Heymann et al., 2008; Krause et al., 2008; Bischoff et al., 2008) focused on improving search by the social tags. Compared with the above, our comparison analysis tries to explore the interactions among multiple web sources including the</context>
</contexts>
<marker>Sood, Owsley, Hammond, Birnbaum, 2007</marker>
<rawString>Sanjay Sood, Sara Owsley, Kristian Hammond and Larry Birnbaum 2007. TagAssist: Automatic Tag Suggestion for Blog Posts ICWSM ’07: International Coference on Weblogs and Social Media</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Diaz</author>
</authors>
<title>Integration of news content into web results WSDM ’09:</title>
<date>2009</date>
<booktitle>Proceedings of the Second ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>182--191</pages>
<contexts>
<context position="5424" citStr="Diaz, 2009" startWordPosition="910" endWordPosition="911">ovingaverage analysis (Vlachos et al., 2004), and frequency analysis (Gruhl et al., 2005), etc. These methods are all related to the preprocessing step of our analysis: detecting bursty queries from the query log effectively. The comparison of two web sources at a time is widely studied recently. (Sood et al., 2007) discussed how to leverage the relation between social tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has aimed to utilize one external web source to help web search. For example, (Diaz, 2009) integrated the news results into general search. (Bao et al., 2007; Heymann et al., 2008; Krause et al., 2008; Bischoff et al., 2008) focused on improving search by the social tags. Compared with the above, our comparison analysis tries to explore the interactions among multiple web sources including the search logs. Similar to our work, some recent work (Adar et al., 2007; Sun et al., 2008) has addressed the comparison among multiple web sources. For example, (Adar et al., 2007) did a comprehensive correlation study among queries, blogs, news and TV results. However, different from the conte</context>
</contexts>
<marker>Diaz, 2009</marker>
<rawString>Fernando Diaz 2009. Integration of news content into web results WSDM ’09: Proceedings of the Second ACM International Conference on Web Search and Data Mining, 182-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Krause</author>
</authors>
<title>Andreas Hotho and Gerd Stumme</title>
<date>2008</date>
<journal>A Comparison of Social Bookmarking with Traditional Search Advances in Information Retrieval,</journal>
<volume>Vol</volume>
<pages>4956--2008</pages>
<marker>Krause, 2008</marker>
<rawString>Beate Krause, Andreas Hotho and Gerd Stumme 2008. A Comparison of Social Bookmarking with Traditional Search Advances in Information Retrieval, Vol 4956/2008:101-113.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Paul Heymann</author>
</authors>
<title>Georgia Koutrika and Hector GarciaMolina 2008. Can social bookmarking improve web search?</title>
<booktitle>WSDM ’08: Proceedings of the international conference on Web search and web data mining,</booktitle>
<pages>195--206</pages>
<marker>Heymann, </marker>
<rawString>Paul Heymann, Georgia Koutrika and Hector GarciaMolina 2008. Can social bookmarking improve web search? WSDM ’08: Proceedings of the international conference on Web search and web data mining, 195-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shenghua Bao</author>
<author>Guirong Xue</author>
<author>Xiaoyuan Wu</author>
<author>Yong Yu</author>
<author>Ben Fei</author>
<author>Zhong Su</author>
</authors>
<title>Optimizing web search using social annotations WWW ’07:</title>
<date>2007</date>
<booktitle>Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>501--510</pages>
<contexts>
<context position="5491" citStr="Bao et al., 2007" startWordPosition="919" endWordPosition="922">alysis (Gruhl et al., 2005), etc. These methods are all related to the preprocessing step of our analysis: detecting bursty queries from the query log effectively. The comparison of two web sources at a time is widely studied recently. (Sood et al., 2007) discussed how to leverage the relation between social tags and web blogs. (Lloyd et al., 2006; Gamon et al., 2008; Cointet et al., 2008) investigated the relations between news and blogs. Also some work has aimed to utilize one external web source to help web search. For example, (Diaz, 2009) integrated the news results into general search. (Bao et al., 2007; Heymann et al., 2008; Krause et al., 2008; Bischoff et al., 2008) focused on improving search by the social tags. Compared with the above, our comparison analysis tries to explore the interactions among multiple web sources including the search logs. Similar to our work, some recent work (Adar et al., 2007; Sun et al., 2008) has addressed the comparison among multiple web sources. For example, (Adar et al., 2007) did a comprehensive correlation study among queries, blogs, news and TV results. However, different from the content-free analysis above, our work compares the sources based on the </context>
</contexts>
<marker>Bao, Xue, Wu, Yu, Fei, Su, 2007</marker>
<rawString>Shenghua Bao, Guirong Xue, Xiaoyuan Wu, Yong Yu, Ben Fei and Zhong Su 2007. Optimizing web search using social annotations WWW ’07: Proceedings of the 16th international conference on World Wide Web, 501-510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Bischoff</author>
<author>Claudiu S Firan</author>
</authors>
<title>Wolfgang Nejdl and Raluca Paiu</title>
<date>2008</date>
<booktitle>CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>193--202</pages>
<marker>Bischoff, Firan, 2008</marker>
<rawString>Kerstin Bischoff, Claudiu S. Firan, Wolfgang Nejdl and Raluca Paiu 2008. Can all tags be used for search? CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management, 193-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
</authors>
<title>Benjamin Rey and Omid Madani</title>
<date>2006</date>
<booktitle>Proceedings of the 15th international conference on World Wide Web,,</booktitle>
<pages>387--396</pages>
<marker>Jones, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey and Omid Madani 2006. Generating query substitutions Proceedings of the 15th international conference on World Wide Web,, 387-396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Thorsten Joachims</author>
</authors>
<title>Query chains: learning to rank from implicit feedback</title>
<date>2005</date>
<booktitle>Proceedings of the 11th ACM SIGKDD international conference on Knowledge discovery in data mining,</booktitle>
<pages>239--248</pages>
<contexts>
<context position="6387" citStr="Radlinski and Joachims, 2005" startWordPosition="1070" endWordPosition="1073"> work, some recent work (Adar et al., 2007; Sun et al., 2008) has addressed the comparison among multiple web sources. For example, (Adar et al., 2007) did a comprehensive correlation study among queries, blogs, news and TV results. However, different from the content-free analysis above, our work compares the sources based on the content. Our work can lead to many useful search applications, such as query suggestion which takes as input a specific query and returns as output one or several suggested queries. The approaches include query term cooccurrence (Jones et al., 2006), query sessions (Radlinski and Joachims, 2005), and clickthrough (Mei et al., 2008), respectively. 3 Analysis Setup Tasks of web information retrieval such as web search generally perform very well on frequent and navigational queries (Broder, 2002) such like “chicago” or “yahoo movies.” A considerable challenge in web search remains in how to handle informational queries, especially queries that reflect new information need and suddenly changed information need of users. Many such scenarios are caused by the emergence of bursty events (e.g., “van gogh” became a hot query in May 2006 since a Van Goghs portrait was sold for 40.3 million in</context>
<context position="13099" citStr="Radlinski and Joachims, 2005" startWordPosition="2241" endWordPosition="2244">at a user searches after he submitted the bursty query q. This context usually reflects how a user reformulates the representation of his information need and implicitly clarifies his interests in the topic. We define a Query Session containing a bursty topic t, qs(t), as the queries which are issued by the same user after he issued t, within 30 minutes. For each bursty topic t, we collect all the qs(t) to form the query session context of t, QS(t): QS(t) = {q|q E Q and q E qs(t)} In web search, the query session context is usually utilized to provide query suggestion and query reformulation (Radlinski and Joachims, 2005). 3.2.3 News contexts News articles written by critics and journalists reflect the reactions and perspectives of such professional group of people to a bursty event. We collect news articles about these 130 bursty topics from Google News2, by finding the most relevant news articles which (1) match the bursty topic t, (2) were published in May, 2006, and (3) were published by any of the five major news medias: CNN, NBC, ABC, New York Times and Washington Post. We then retrieve the title and body of each news article. This provides us two contexts of each bursty topic t: the set of relevant news</context>
</contexts>
<marker>Radlinski, Joachims, 2005</marker>
<rawString>Filip Radlinski and Thorsten Joachims 2005. Query chains: learning to rank from implicit feedback Proceedings of the 11th ACM SIGKDD international conference on Knowledge discovery in data mining, 239-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Dengyong Zhou</author>
<author>Kenneth Church</author>
</authors>
<title>Query suggestion using hitting time</title>
<date>2008</date>
<booktitle>CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>469--478</pages>
<contexts>
<context position="6424" citStr="Mei et al., 2008" startWordPosition="1077" endWordPosition="1080">t al., 2008) has addressed the comparison among multiple web sources. For example, (Adar et al., 2007) did a comprehensive correlation study among queries, blogs, news and TV results. However, different from the content-free analysis above, our work compares the sources based on the content. Our work can lead to many useful search applications, such as query suggestion which takes as input a specific query and returns as output one or several suggested queries. The approaches include query term cooccurrence (Jones et al., 2006), query sessions (Radlinski and Joachims, 2005), and clickthrough (Mei et al., 2008), respectively. 3 Analysis Setup Tasks of web information retrieval such as web search generally perform very well on frequent and navigational queries (Broder, 2002) such like “chicago” or “yahoo movies.” A considerable challenge in web search remains in how to handle informational queries, especially queries that reflect new information need and suddenly changed information need of users. Many such scenarios are caused by the emergence of bursty events (e.g., “van gogh” became a hot query in May 2006 since a Van Goghs portrait was sold for 40.3 million in New York during that time). The focu</context>
</contexts>
<marker>Mei, Zhou, Church, 2008</marker>
<rawString>Qiaozhu Mei, Dengyong Zhou and Kenneth Church 2008. Query suggestion using hitting time CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management, 469-478</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Broder</author>
</authors>
<date>2002</date>
<journal>A Taxonomy of Web Search SIGIR Forum,</journal>
<volume>Vol</volume>
<pages>36--2</pages>
<contexts>
<context position="6590" citStr="Broder, 2002" startWordPosition="1103" endWordPosition="1104">d TV results. However, different from the content-free analysis above, our work compares the sources based on the content. Our work can lead to many useful search applications, such as query suggestion which takes as input a specific query and returns as output one or several suggested queries. The approaches include query term cooccurrence (Jones et al., 2006), query sessions (Radlinski and Joachims, 2005), and clickthrough (Mei et al., 2008), respectively. 3 Analysis Setup Tasks of web information retrieval such as web search generally perform very well on frequent and navigational queries (Broder, 2002) such like “chicago” or “yahoo movies.” A considerable challenge in web search remains in how to handle informational queries, especially queries that reflect new information need and suddenly changed information need of users. Many such scenarios are caused by the emergence of bursty events (e.g., “van gogh” became a hot query in May 2006 since a Van Goghs portrait was sold for 40.3 million in New York during that time). The focus of this paper is to analyze how other online media sources react to those bursty events and how those reactions compare to the reaction in web search. This analysis</context>
</contexts>
<marker>Broder, 2002</marker>
<rawString>Andrei Broder 2002. A Taxonomy of Web Search SIGIR Forum, Vol 36(2):3-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard Leibler</author>
</authors>
<date>1951</date>
<journal>On Information and Sufficience Annals of Mathematical Statistics,</journal>
<volume>Vol</volume>
<pages>22--1</pages>
<contexts>
<context position="18308" citStr="Kullback and Leibler, 1951" startWordPosition="3157" endWordPosition="3160">ful to predict the content in web search. By representing each context of a bursty topic as a vector space model of unigrams/bigrams, we first compute and compare the average cosine similarity between contexts. We only include contexts with more than 5 unigram/bigrams into this comparison. The results are shown in Table A and Table B, respectively. Each table is followed by a heat map to visualize the pattern. To investigate how well one source can predict the content of another, we also represent each context of a bursty topic as a unigram/bigram language model and compute the Cross Entropy (Kullback and Leibler, 1951) between every pairs of contexts. Cross Entropy measures how certain one probability distribution predicts another. We calculate such measure based on the following definition: HCE(m||n) = H(m) + DKL(m||n) We smooth the unigram language models using Laplace smoothing (Field, 1988) and the bigram language models using Katz back-off model (Katz, 1987). The results are shown in Table C and Table D, followed by the corresponding heat maps. For each value HCE(m||n) in the table cell, m stands for the context in the row and n stands for the context in the column. Please note that in Figure 3, 4, a l</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard Leibler 1951. On Information and Sufficience Annals of Mathematical Statistics, Vol 22(1):79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Field</author>
</authors>
<title>Laplacian Smoothing and Delaunay Triangulations</title>
<date>1988</date>
<journal>Communications in Applied Numerical Methods,</journal>
<volume>Vol</volume>
<pages>4--709</pages>
<contexts>
<context position="18589" citStr="Field, 1988" startWordPosition="3200" endWordPosition="3201">esults are shown in Table A and Table B, respectively. Each table is followed by a heat map to visualize the pattern. To investigate how well one source can predict the content of another, we also represent each context of a bursty topic as a unigram/bigram language model and compute the Cross Entropy (Kullback and Leibler, 1951) between every pairs of contexts. Cross Entropy measures how certain one probability distribution predicts another. We calculate such measure based on the following definition: HCE(m||n) = H(m) + DKL(m||n) We smooth the unigram language models using Laplace smoothing (Field, 1988) and the bigram language models using Katz back-off model (Katz, 1987). The results are shown in Table C and Table D, followed by the corresponding heat maps. For each value HCE(m||n) in the table cell, m stands for the context in the row and n stands for the context in the column. Please note that in Figure 3, 4, a larger HCE value corresponds to a lighter cell. 4.1.1 Results From the results shown in Table A-D, or in Figure 1- 4 more visually, some interesting phenomena can be observed: • Compared with other contexts, query session is much more similar to the super query. This makes sense be</context>
</contexts>
<marker>Field, 1988</marker>
<rawString>David A. Field 1988. Laplacian Smoothing and Delaunay Triangulations Communications in Applied Numerical Methods, Vol 4:709-712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>400--401</pages>
<contexts>
<context position="18659" citStr="Katz, 1987" startWordPosition="3212" endWordPosition="3213">llowed by a heat map to visualize the pattern. To investigate how well one source can predict the content of another, we also represent each context of a bursty topic as a unigram/bigram language model and compute the Cross Entropy (Kullback and Leibler, 1951) between every pairs of contexts. Cross Entropy measures how certain one probability distribution predicts another. We calculate such measure based on the following definition: HCE(m||n) = H(m) + DKL(m||n) We smooth the unigram language models using Laplace smoothing (Field, 1988) and the bigram language models using Katz back-off model (Katz, 1987). The results are shown in Table C and Table D, followed by the corresponding heat maps. For each value HCE(m||n) in the table cell, m stands for the context in the row and n stands for the context in the column. Please note that in Figure 3, 4, a larger HCE value corresponds to a lighter cell. 4.1.1 Results From the results shown in Table A-D, or in Figure 1- 4 more visually, some interesting phenomena can be observed: • Compared with other contexts, query session is much more similar to the super query. This makes sense because many super queries would be included in the query session. • Com</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Stephen M. Katz 1987 Estimation of probabilities from sparse data for the language model component of a speech recogniser IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3), 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity: measuring the relatedness of concepts PHLT-NAACL ’04: Demonstration Papers at HLT-NAACL</title>
<date>2004</date>
<pages>38--41</pages>
<note>on XX,</note>
<contexts>
<context position="29164" citStr="Pedersen et al., 2004" startWordPosition="5061" endWordPosition="5065">ach term, wv stands for the weight of vertex v in G, and each e E E stands for the semantic closeness between a pair of terms (u, v) measured by sim(u, v). We define the density of such a semantic graph as follows: Den(G) = Eu, vEV,u=,4vsim(u, v)wuwv (2) rIu,vEV,u=j4vwuwv If sim(u, v) values the semantic similarity between u and v, a high value of Den(G) implies that the whole context is semantically consistent. Otherwise, it may be diverse or ambiguous. We build the graph of each context based on WordNet6. For a pair of words, WordNet provides a series of measures of the semantic similarity (Pedersen et al., 2004). We use the Path Distance Similarity (path for short) and Lin Similarity (lin for short) to measure sim(u, v). Both measures range in [0, 1]. For the convenience of computation, we choose the top 1100 unigrams ranked by term frequency in each source (if any) to represent the whole context on one specific topic. 6http://wordnet.princeton.edu/ 1084 (a) Unigram (b) Bigram Figure 5: Coverage results 4.3.1 Overall Table 5 shows the average overall density of each sources over all the topics. From the table we can Source path lin SQ 0.098 0.128 QS 0.071 0.082 NT 0.103 0.129 NB 0.109 0.139 BT 0.099 </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan and Jason Michelizzi 2004. WordNet::Similarity: measuring the relatedness of concepts PHLT-NAACL ’04: Demonstration Papers at HLT-NAACL 2004 on XX, 38-41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>