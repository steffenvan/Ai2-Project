<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008397">
<title confidence="0.9994225">
Simple Algorithms for Complex Relation Extraction
with Applications to Biomedical IE
</title>
<author confidence="0.998549">
Ryan McDonald1 Fernando Pereira1 Seth Kulick2
</author>
<affiliation confidence="0.916313">
1CIS and 2IRCS, University of Pennsylvania, Philadelphia, PA
</affiliation>
<email confidence="0.993105">
{ryantm,pereira}@cis.upenn.edu, skulick@linc.cis.upenn.edu
</email>
<author confidence="0.998534">
Scott Winters Yang Jin Pete White
</author>
<affiliation confidence="0.99893">
Division of Oncology, Children’s Hospital of Pennsylvania, Philadelphia, PA
</affiliation>
<email confidence="0.997549">
{winters,jin,white}@genome.chop.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942692307692">
A complex relation is any n-ary relation
in which some of the arguments may be
be unspecified. We present here a simple
two-stage method for extracting complex
relations between named entities in text.
The first stage creates a graph from pairs
of entities that are likely to be related, and
the second stage scores maximal cliques
in that graph as potential complex relation
instances. We evaluate the new method
against a standard baseline for extracting
genomic variation relations from biomed-
ical text.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996201431372549">
Most research on text information extraction (IE)
has focused on accurate tagging of named entities.
Successful early named-entity taggers were based
on finite-state generative models (Bikel et al., 1999).
More recently, discriminatively-trained models have
been shown to be more accurate than generative
models (McCallum et al., 2000; Lafferty et al., 2001;
Kudo and Matsumoto, 2001). Both kinds of mod-
els have been developed for tagging entities such
as people, places and organizations in news mate-
rial. However, the rapid development of bioinfor-
matics has recently generated interest on the extrac-
tion of biological entities such as genes (Collier et
al., 2000) and genomic variations (McDonald et al.,
2004b) from biomedical literature.
The next logical step for IE is to begin to develop
methods for extracting meaningful relations involv-
ing named entities. Such relations would be ex-
tremely useful in applications like question answer-
ing, automatic database generation, and intelligent
document searching and indexing. Though not as
well studied as entity extraction, relation extraction
has still seen a significant amount of work. We dis-
cuss some previous approaches at greater length in
Section 2.
Most relation extraction systems focus on the spe-
cific problem of extracting binary relations, such
as the employee of relation or protein-protein in-
teraction relation. Very little work has been done
in recognizing and extracting more complex rela-
tions. We define a complex relation as any n-ary
relation among n typed entities. The relation is
defined by the schema (t1, ... , tn) where ti E T
are entity types. An instance (or tuple) in the rela-
tion is a list of entities (e1, ... , en) such that either
type(ei) = ti, or ei =1 indicating that the ith ele-
ment of the tuple is missing.
For example, assume that the entity types
are T = {person, job, company} and we are
interested in the ternary relation with schema
(person, job, company) that relates a person
to their job at a particular company. For
the sentence “John Smith is the CEO at Inc.
Corp.”, the system would ideally extract the tu-
ple (John Smith, CEO, Inc. Corp.). However, for
the sentence “Everyday John Smith goes to his
office at Inc. Corp.”, the system would extract
(John Smith, 1, Inc. Corp.), since there is no men-
tion of a job title. Hence, the goal of complex re-
lation extraction is to identify all instances of the
relation of interest in some piece of text, including
</bodyText>
<page confidence="0.983463">
491
</page>
<note confidence="0.9917585">
Proceedings of the 43rd Annual Meeting of the ACL, pages 491–498,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.997466846153846">
incomplete instances.
We present here several simple methods for ex-
tracting complex relations. All the methods start by
recognized pairs of entity mentions, that is, binary
relation instances, that appear to be arguments of the
relation of interest. Those pairs can be seen as the
edges of a graph with entity mentions as nodes. The
algorithms then try to reconstruct complex relations
by making tuples from selected maximal cliques in
the graph. The methods are general and can be ap-
plied to any complex relation fitting the above def-
inition. We also assume throughout the paper that
the entities and their type are known a priori in the
text. This is a fair assumption given the current high
standard of state-of-the-art named-entity extractors.
A primary advantage of factoring complex rela-
tions into binary relations is that it allows the use of
standard classification algorithms to decide whether
particular pairs of entity mentions are related. In ad-
dition, the factoring makes training data less sparse
and reduces the computational cost of extraction.
We will discuss these benefits further in Section 4.
We evaluated the methods on a large set of anno-
tated biomedical documents to extract relations re-
lated to genomic variations, demonstrating a consid-
erable improvement over a reasonable baseline.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999922104477612">
A representative approach to relation extraction is
the system of Zelenko et al. (2003), which attempts
to identify binary relations in news text. In that
system, each pair of entity mentions of the correct
types in a sentence is classified as to whether it is
a positive instance of the relation. Consider the bi-
nary relation employee of and the sentence “John
Smith, not Jane Smith, works at IBM”. The pair
(John Smith, IBM) is a positive instance, while the
pair (Jane Smith, IBM) is a negative instance. In-
stances are represented by a pair of entities and their
position in a shallow parse tree for the containing
sentence. Classification is done by a support-vector
classifier with a specialized kernel for that shallow
parse representation.
This approach — enumerating all possible en-
tity pairs and classifying each as positive or nega-
tive — is the standard method in relation extraction.
The main differences among systems are the choice
of trainable classifier and the representation for in-
stances.
For binary relations, this approach is quite
tractable: if the relation schema is (t1, t2), the num-
ber of potential instances is O(|t1 ||t2|), where |t |is
the number of entity mentions of type t in the text
under consideration.
One interesting system that does not belong to
the above class is that of Miller et al. (2000), who
take the view that relation extraction is just a form
of probabilistic parsing where parse trees are aug-
mented to identify all relations. Once this augmen-
tation is made, any standard parser can be trained
and then run on new sentences to extract new re-
lations. Miller et al. show such an approach can
yield good results. However, it can be argued that
this method will encounter problems when consid-
ering anything but binary relations. Complex re-
lations would require a large amount of tree aug-
mentation and most likely result in extremely sparse
probability estimates. Furthermore, by integrating
relation extraction with parsing, the system cannot
consider long-range dependencies due to the local
parsing constraints of current probabilistic parsers.
The higher the arity of a relation, the more likely
it is that entities will be spread out within a piece
of text, making long range dependencies especially
important.
Roth and Yih (2004) present a model in which en-
tity types and relations are classified jointly using a
set of global constraints over locally trained classi-
fiers. This joint classification is shown to improve
accuracy of both the entities and relations returned
by the system. However, the system is based on con-
straints for binary relations only.
Recently, there has also been many results from
the biomedical IE community. Rosario and Hearst
(2004) compare both generative and discriminative
models for extracting seven relationships between
treatments and diseases. Though their models are
very flexible, they assume at most one relation per
sentence, ruling out cases where entities participate
in multiple relations, which is a common occurrence
in our data. McDonald et al. (2004a) use a rule-
based parser combined with a rule-based relation
identifier to extract generic binary relations between
biological entities. As in predicate-argument extrac-
tion (Gildea and Jurafsky, 2002), each relation is
</bodyText>
<page confidence="0.982732">
492
</page>
<bodyText confidence="0.950804214285714">
always associated with a verb in the sentence that a. All possible b. All possible
specifies the relation type. Though this system is relation instances binary relations
very general, it is limited by the fact that the design (John, CEO, Inc. Corp.) (John, CEO)
ignores relations not expressed by a verb, as the em- (John, L, Inc. Corp.) (John, Inc. Corp.)
ployee of relation in“John Smith, CEO ofInc. Corp., (John, CEO, Biz. Corp.) (John, Biz. Corp.)
announced he will resign”. (John, L, Biz. Corp.) (CEO, Inc. Corp.)
Most relation extraction systems work primarily (John, CEO, L) (CEO, Biz. Corp.)
on a sentential level and never consider relations that (Jane, CEO, Inc. Corp.) (Jane, CEO)
cross sentences or paragraphs. Since current data (Jane, L, Inc. Corp.) (Jane, Inc. Corp.)
sets typically only annotate intra-sentence relations, (Jane, CEO, Biz. Corp.) (Jane, Biz. Corp.)
this has not yet proven to be a problem. (Jane, L, Biz. Corp.)
(Jane, CEO, L)
(L, CEO, Inc. Corp.)
(L, CEO, Biz. Corp.)
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="method">
3 Definitions
</sectionHeader>
<subsectionHeader confidence="0.999748">
3.1 Complex Relations
</subsectionHeader>
<bodyText confidence="0.999961416666667">
Recall that a complex n-ary relation is specified by
a schema (t1, ... , tn) where ti E T are entity types.
Instances of the relation are tuples (e1, ... , en)
where either type(ei) = ti, or ei =1 (missing ar-
gument). The only restriction this definition places
on a relation is that the arity must be known. As we
discuss it further in Section 6, this is not required by
our methods but is assumed here for simplicity. We
also assume that the system works on a single rela-
tion type at a time, although the methods described
here are easily generalizable to systems that can ex-
tract many relations at once.
</bodyText>
<subsectionHeader confidence="0.999492">
3.2 Graphs and Cliques
</subsectionHeader>
<bodyText confidence="0.999379777777778">
An undirected graph G = (V, E) is specified by a
set of vertices V and a set of edges E, with each
edge an unordered pair (u, v) of vertices. G&apos; =
(V &apos;, E&apos;) is a subgraph of G if V &apos; C_ V and E&apos; =
{(u, v) : u, v E V &apos;, (u, v) E E}. A clique C of G is
a subgraph of G in which there is an edge between
every pair of vertices. A maximal clique of G is a
clique C = (VC, EC) such that there is no other
clique C&apos; = (VC,, EC,) such that VC C VC,.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.999414375">
We describe now a simple method for extracting
complex relations. This method works by first fac-
toring all complex relations into a set of binary re-
lations. A classifier is then trained in the standard
manner to recognize all pairs of related entities. Fi-
nally a graph is constructed from the output of this
classifier and the complex relations are determined
from the cliques of this graph.
</bodyText>
<figureCaption confidence="0.965389333333333">
Figure 1: Relation factorization of the sentence:
John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively.
</figureCaption>
<subsectionHeader confidence="0.998065">
4.1 Classifying Binary Relations
</subsectionHeader>
<bodyText confidence="0.999920066666667">
Consider again the motivating example of the
(person, job, company) relation and the sentence
“John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively”. This sentence contains two
people, one job title and two companies.
One possible method for extracting the rela-
tion of interest would be to first consider all 12
possible tuples shown in Figure 1a. Using all
these tuples, it should then be possible to train
a classifier to distinguish valid instances such as
(John, CEO, Inc. Corp.) from invalid ones such as
(Jane, CEO, Inc. Corp.). This is analogous to the
approach taken by Zelenko et al. (2003) for binary
relations.
There are problems with this approach. Computa-
tionally, for an n-ary relation, the number of possi-
ble instances is O(|t1 ||t2 |· · · |tn|). Conservatively,
letting m be the smallest |ti|, the run time is O(mn),
exponential in the arity of the relation. The second
problem is how to manage incomplete but correct
instances such as (John, 1, Inc. Corp.) when train-
ing the classifier. If this instance is marked as neg-
ative, then the model might incorrectly disfavor fea-
tures that correlate John to Inc. Corp.. However,
if this instance is labeled positive, then the model
may tend to prefer the shorter and more compact in-
complete relations since they will be abundant in the
positive training examples. We could always ignore
instances of this form, but then the data would be
heavily skewed towards negative instances.
</bodyText>
<page confidence="0.996486">
493
</page>
<bodyText confidence="0.999447104166666">
Instead of trying to classify all possible relation
instances, in this work we first classify pairs of en-
tities as being related or not. Then, as discussed in
Section 4.2, we reconstruct the larger complex rela-
tions from a set of binary relation instances.
Factoring relations into a set of binary decisions
has several advantages. The set of possible pairs is
much smaller then the set of all possible complex
relation instances. This can be seen in Figure 1b,
which only considers pairs that are consistent with
the relation definition. More generally, the num-
ber of pairs to classify is O((Ei Itij)2) , which is
far better than the exponentially many full relation
instances. There is also no ambiguity when label-
ing pairs as positive or negative when constructing
the training data. Finally, we can rely on previous
work on classification for binary relation extraction
to identify pairs of related entities.
To train a classifier to identify pairs of related
entities, we must first create the set of all positive
and negative pairs in the data. The positive in-
stances are all pairs that occur together in a valid
tuple. For the example sentence in Figure 1, these
include the pairs (John, CEO), (John, Inc. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.), (Jane, CEO)
and (Jane, Biz. Corp.). To gather negative in-
stances, we extract all pairs that never occur to-
gether in a valid relation. From the same exam-
ple these would be the pairs (John, Biz. Corp.) and
(Jane, Inc. Corp.).
This leads to a large set of positive and negative
binary relation instances. At this point we could em-
ploy any binary relation classifier and learn to iden-
tify new instances of related pairs of entities. We
use a standard maximum entropy classifier (Berger
et al., 1996) implemented as part of MALLET (Mc-
Callum, 2002). The model is trained using the fea-
tures listed in Table 1.
This is a very simple binary classification model.
No deep syntactic structure such as parse trees is
used. All features are basically over the words sepa-
rating two entities and their part-of-speech tags. Of
course, it would be possible to use more syntactic
information if available in a manner similar to that
of Zelenko et al. (2003). However, the primary pur-
pose of our experiments was not to create a better
binary relation extractor, but to see if complex re-
lations could be extracted through binary factoriza-
</bodyText>
<table confidence="0.9994126">
Feature Set
entity type of e1 and e2
words in e1 and e2
word bigrams in e1 and e2
POS of e1 and e2
words between e1 and e2
word bigrams between e1 and e2
POS between e1 and e2
distance between e1 and e2
concatenations of above features
</table>
<tableCaption confidence="0.952381">
Table 1: Feature set for maximum entropy binary
relation classifier. e1 and e2 are entities.
</tableCaption>
<table confidence="0.329564888888889">
b. Tuples from G
(John, CEO, 1)
(John, 1, Inc. Corp.)
(John, 1, Biz. Corp.)
(Jane, CEO, 1)
(1, CEO, Inc. Corp.)
(1, CEO, Biz. Corp.)
(John, CEO, Inc. Corp.)
(John, CEO, Biz. Corp.)
</table>
<figureCaption confidence="0.9929905">
Figure 2: Example of a relation graph and tuples
from all the cliques in the graph.
</figureCaption>
<bodyText confidence="0.999646666666667">
tion followed by reconstruction. In Section 5.2 we
present an empirical evaluation of the binary relation
classifier.
</bodyText>
<subsectionHeader confidence="0.961932">
4.2 Reconstructing Complex Relations
4.2.1 Maximal Cliques
</subsectionHeader>
<bodyText confidence="0.980642631578947">
Having identified all pairs of related entities in the
text, the next stage is to reconstruct the complex re-
lations from these pairs. Let G = (V, E) be an undi-
rected graph where the vertices V are entity men-
tions in the text and the edges E represent binary
relations between entities. We reconstruct the com-
plex relation instances by finding maximal cliques
in the graphs.
The simplest approach is to create the graph
so that two entities in the graph have an edge
if the binary classifier believes they are related.
For example, consider the binary factoriza-
tion in Figure 1 and imagine the classifier
identified the following pairs as being related:
(John, CEO), (John, Inc. Corp.), (John, Biz. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.) and
(Jane, CEO). The resulting graph can be seen
in Figure 2a.
Looking at this graph, one solution to construct-
</bodyText>
<figure confidence="0.97436">
a. Relation graph G
John Jane
CEO
Inc. Corp. Biz. Corp.
</figure>
<page confidence="0.997067">
494
</page>
<bodyText confidence="0.999982342105264">
ing complex relations would be to consider all the
cliques in the graph that are consistent with the def-
inition of the relation. This is equivalent to having
the system return only relations in which the binary
classifier believes that all of the entities involved are
pairwise related. All the cliques in the example are
shown in Figure 2b. We add  fields to the tuples to
be consistent with the relation definition.
This could lead to a set of overlapping
cliques, for instance (John, CEO, Inc. Corp.) and
(John, CEO, ). Instead of having the system re-
turn all cliques, our system just returns the maximal
cliques, that is, those cliques that are not subsets of
other cliques. Hence, for the example under con-
sideration in Figure 2, the system would return the
one correct relation, (John, CEO, Inc. Corp.), and
two incorrect relations, (John, CEO, Biz. Corp.) and
(Jane, CEO, ). The second is incorrect since it
does not specify the company slot of the relation
even though that information is present in the text.
It is possible to find degenerate sentences in which
perfect binary classification followed by maximal
clique reconstruction will lead to errors. One such
sentence is, “John is C.E.O. and C.F.O. ofInc. Corp.
and Biz. Corp. respectively and Jane vice-versa”.
However, we expect such sentences to be rare; in
fact, they never occur in our data.
The real problem with this approach is that an ar-
bitrary graph can have exponentially many cliques,
negating any efficiency advantage over enumerating
all n-tuples of entities. Fortunately, there are algo-
rithms for finding all maximal cliques that are effi-
cient in practice. We use the algorithm of Bron and
Kerbosch (1973). This is a well known branch and
bound algorithm that has been shown to empirically
run linearly in the number of maximal cliques in the
graph. In our experiments, this algorithm found all
maximal cliques in a matter of seconds.
</bodyText>
<subsubsectionHeader confidence="0.829561">
4.2.2 Probabilistic Cliques
</subsubsectionHeader>
<bodyText confidence="0.9999195">
The above approach has a major shortcom-
ing in that it assumes the output of the bi-
nary classifier to be absolutely correct. For
instance, the classifier may have thought with
probability 0.49, 0.99 and 0.99 that the fol-
lowing pairs were related: (Jane, Biz. Corp.),
(CEO, Biz. Corp.) and (Jane, CEO) respectively.
The maximal clique method would not produce the
tuple (Jane, CEO, Biz. Corp.) since it never consid-
ers the edge between Jane and Biz. Corp. However,
given the probability of the edges, we would almost
certainly want this tuple returned.
What we would really like to model is a belief
that on average a clique represents a valid relation
instance. To do this we use the complete graph
G = (V, E) with edges between all pairs of entity
mentions. We then assign weight w(e) to edge e
equal to the probability that the two entities in e are
related, according to the classifier. We define the
weight of a clique w(C) as the mean weight of the
edges in the clique. Since edge weights represent
probabilities (or ratios), we use the geometric mean
</bodyText>
<equation confidence="0.997531">
w(C) = rl w(e)
(eEEC
</equation>
<bodyText confidence="0.998037896551724">
We decide that a clique C represents a valid tuple if
w(C)  0.5. Hence, the system finds all maximal
cliques as before, but considers only those where
w(C)  0.5, and it may select a non-maximal clique
if the weight of all larger cliques falls below the
threshold. The cutoff of 0.5 is not arbitrary, since it
ensures that the average probability of a clique rep-
resenting a relation instance is at least as large as
the average probability of it not representing a rela-
tion instance. We ran experiments with varying lev-
els of this threshold and found that, roughly, lower
thresholds result in higher precision at the expense
of recall since the system returns fewer but larger
tuples. Optimum results were obtained for a cutoff
of approximately 0.4, but we report results only for
w(C)  0.5.
The major problem with this approach is that
there will always be exponentially many cliques
since the graph is fully connected. However, in our
experiments we pruned all edges that would force
any containing clique C to have w(C) &lt; 0.5. This
typically made the graphs very sparse.
Another problem with this approach is the as-
sumption that the binary relation classifier outputs
probabilities. For maximum entropy and other prob-
abilistic frameworks this is not an issue. However,
many classifiers, such as SVMs, output scores or
distances. It is possible to transform the scores from
those models through a sigmoid to yield probabili-
</bodyText>
<figure confidence="0.4028895">
��/|EC|
�
</figure>
<page confidence="0.994171">
495
</page>
<bodyText confidence="0.997004">
ties, but there is no guarantee that those probability
values will be well calibrated.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999531">
5.1 Problem Description and Data
</subsectionHeader>
<bodyText confidence="0.999001744680851">
We test these methods on the task of extracting ge-
nomic variation events from biomedical text (Mc-
Donald et al., 2004b). Briefly, we define a varia-
tion event as an acquired genomic aberration: a spe-
cific, one-time alteration at the genomic level and
described at the nucleic acid level, amino acid level
or both. Each variation event is identified by the re-
lationship between a type of variation, its location,
and the corresponding state change from an initial-
state to an altered-state. This can be formalized as
the following complex schema
(var-type, location, initial-state, altered-state)
A simple example is the sentence
“At codons 12 and 61, the occurrence of
point mutations from G/A to T/G were observed”
which gives rise to the tuples
(point mutation, codon 12, G, T)
(point mutation, codon 61, A, G)
Our data set consists of 447 abstracts selected
from MEDLINE as being relevant to populating a
database with facts of the form: gene X with vari-
ation event Y is associated with malignancy Z. Ab-
stracts were randomly chosen from a larger corpus
identified as containing variation mentions pertain-
ing to cancer.
The current data consists of 4691 sentences that
have been annotated with 4773 entities and 1218 re-
lations. Of the 1218 relations, 760 have two L ar-
guments, 283 have one L argument, and 175 have
no L arguments. Thus, 38% of the relations tagged
in this data cannot be handled using binary relation
classification alone. In addition, 4% of the relations
annotated in this data are non-sentential. Our sys-
tem currently only produces sentential relations and
is therefore bounded by a maximum recall of 96%.
Finally, we use gold standard entities in our exper-
iments. This way we can evaluate the performance
of the relation extraction system isolated from any
kind of pipelined entity extraction errors. Entities in
this domain can be found with fairly high accuracy
(McDonald et al., 2004b).
It is important to note that just the presence of two
entity types does not entail a relation between them.
In fact, 56% of entity pairs are not related, due either
to explicit disqualification in the text (e.g. “... the
lack of G to T transversion ...”) or ambiguities that
arise from multiple entities of the same type.
</bodyText>
<subsectionHeader confidence="0.926498">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.992897666666667">
Because the data contains only 1218 examples of re-
lations we performed 10-fold cross-validation tests
for all results. We compared three systems:
</bodyText>
<listItem confidence="0.991279125">
• MC: Uses the maximum entropy binary classi-
fier coupled with the maximal clique complex
relation reconstructor.
• PC: Same as above, except it uses the proba-
bilistic clique complex relation reconstructor.
• NE: A maximum entropy classifier that naively
enumerates all possible relation instances as
described in Section 4.1.
</listItem>
<bodyText confidence="0.999886777777778">
In training system NE, all incomplete but correct
instances were marked as positive since we found
this had the best performance. We used the same
pairwise entity features in the binary classifier of
the above two systems. However, we also added
higher order versions of the pairwise features. For
this system we only take maximal relations,that is,
if (John, CEO, Inc. Corp.) and (John, L, Inc. Corp.)
are both labeled positive, the system would only re-
turn the former.
Table 2 contains the results of the maximum en-
tropy binary relation classifier (used in systems MC
and PC). The 1218 annotated complex relations pro-
duced 2577 unique binary pairs of related entities.
We can see that the maximum entropy classifier per-
forms reasonably well, although performance may
be affected by the lack of rich syntactic features,
which have been shown to help performance (Miller
et al., 2000; Zelenko et al., 2003).
Table 3 compares the three systems on the real
problem of extracting complex relations. An ex-
tracted complex relation is considered correct if and
only if all the entities in the relation are correct.
There is no partial credit. All training and clique
finding algorithms took under 5 minutes for the en-
tire data set. Naive enumeration took approximately
26 minutes to train.
</bodyText>
<page confidence="0.998114">
496
</page>
<table confidence="0.92996425">
ACT PRD COR
2577 2722 2101
Prec Rec F-Meas
0.7719 0.8153 0.7930
</table>
<tableCaption confidence="0.9925125">
Table 2: Binary relation classification results for the
maximum entropy classifier. ACT: actual number of
related pairs, PRD: predicted number of related pairs
and COR: correctly identified related pairs.
</tableCaption>
<table confidence="0.99990975">
System Prec Rec F-Meas
NE 0.4588 0.6995 0.5541
MC 0.5812 0.7315 0.6480
PC 0.6303 0.7726 0.6942
</table>
<tableCaption confidence="0.991215">
Table 3: Full relation classification results. For a
</tableCaption>
<bodyText confidence="0.9874225">
relation to be classified correctly, all the entities in
the relation must be correctly identified.
First we observe that the maximal clique method
combined with maximum entropy (system MC) re-
duces the relative error rate over naively enumer-
ating and classifying all instances (system NE) by
21%. This result is very positive. The system based
on binary factorization not only is more efficient
then naively enumerating all instances, but signifi-
cantly outperforms it as well. The main reason naive
enumeration does so poorly is that all correct but
incomplete instances are marked as positive. Thus,
even slight correlations between partially correct en-
tities would be enough to classify an instance as cor-
rect, which results in relatively good recall but poor
precision. We tried training only with correct and
complete positive instances, but the result was a sys-
tem that only returned few relations since negative
instances overwhelmed the training set. With fur-
ther tuning, it may be possible to improve the per-
formance of this system. However, we use it only as
a baseline and to demonstrate that binary factoriza-
tion is a feasible and accurate method for extracting
complex relations.
Furthermore, we see that using probabilistic
cliques (system PC) provides another large im-
provement, a relative error reduction of 13%
over using maximal cliques and 31% reduction
over enumeration. Table 4 shows the breakdown
of relations returned by type. There are three
types of relations, 2-ary, 3-ary and 4-ary, each
with 2, 1 and 0 1 arguments respectively, e.g.
</bodyText>
<table confidence="0.9640705">
System 2-ary 3-ary 4-ary
NE 760:1097:600 283:619:192 175:141:60
MC 760:1025:601 283:412:206 175:95:84
PC 760:870:590 283:429:223 175:194:128
</table>
<tableCaption confidence="0.949384">
Table 4: Breakdown of true positive relations by
</tableCaption>
<bodyText confidence="0.997192888888889">
type that were returned by each system. Each cell
contains three numbers, Actual:Predicted:Correct,
which represents for each arity the actual, predicted
and correct number of relations for each system.
(point mutation, codon 12,1,1) is a 2-ary relation.
Clearly the probabilistic clique method is much
more likely to find larger non-binary relations, veri-
fying the motivation that there are some low proba-
bility edges that can still contribute to larger cliques.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999993333333333">
We presented a method for complex relation extrac-
tion, the core of which was to factorize complex re-
lations into sets of binary relations, learn to identify
binary relations and then reconstruct the complex re-
lations by finding maximal cliques in graphs that
represent relations between pairs of entities. The
primary advantage of this method is that it allows
for the use of almost any binary relation classifier,
which have been well studied and are often accu-
rate. We showed that such a method can be suc-
cessful with an empirical evaluation on a large set
of biomedical data annotated with genomic varia-
tion relations. In fact, this approach is both signifi-
cantly quicker and more accurate then enumerating
and classifying all possible instances. We believe
this work provides a good starting point for contin-
ued research in this area.
A distinction may be made between the factored
system presented here and one that attempts to clas-
sify complex relations without factorization. This
is related to the distinction between methods that
learn local classifiers that are combined with global
constraints after training and methods that incorpo-
rate the global constraints into the learning process.
McCallum and Wellner (2003) showed that learning
binary co-reference relations globally improves per-
formance over learning relations in isolation. How-
ever, their model relied on the transitive property in-
herent in the co-reference relation. Our system can
be seen as an instance of a local learner. Punyakanok
</bodyText>
<page confidence="0.995107">
497
</page>
<bodyText confidence="0.999981416666667">
et al. (2004) argued that local learning actually out-
performs global learning in cases when local deci-
sions can easily be learnt by the classifier. Hence, it
is reasonable to assume that our binary factorization
method will perform well when binary relations can
be learnt with high accuracy.
As for future work, there are many things that we
plan to look at. The binary relation classifier we em-
ploy is quite simplistic and most likely can be im-
proved by using features over a deeper representa-
tion of the data such as parse trees. Other more pow-
erful binary classifiers should be tried such as those
based on tree kernels (Zelenko et al., 2003). We also
plan on running these algorithms on more data sets
to test if the algorithms empirically generalize to dif-
ferent domains.
Perhaps the most interesting open problem is how
to learn the complex reconstruction phase. One pos-
sibility is recent work on supervised clustering. Let-
ting the edge probabilities in the graphs represent a
distance in some space, it may be possible to learn
how to cluster vertices into relational groups. How-
ever, since a vertex/entity can participate in one or
more relation, any clustering algorithm would be re-
quired to produce non-disjoint clusters.
We mentioned earlier that the only restriction of
our complex relation definition is that the arity of
the relation must be known in advance. It turns out
that the algorithms we described can actually handle
dynamic arity relations. All that is required is to
remove the constraint that maximal cliques must be
consistent with the structure of the relation. This
represents another advantage of binary factorization
over enumeration, since it would be infeasible to
enumerate all possible instances for dynamic arity
relations.
</bodyText>
<sectionHeader confidence="0.998227" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99972925">
The authors would like to thank Mark Liberman,
Mark Mandel and Eric Pancoast for useful discus-
sion, suggestions and technical support. This work
was supported in part by NSF grant ITR 0205448.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901647058824">
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1).
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what’s in a name. Machine
Learning Journal Special Issue on Natural Language
Learning, 34(1/3):221–231.
C. Bron and J. Kerbosch. 1973. Algorithm 457: finding
all cliques of an undirected graph. Communications of
the ACM, 16(9):575–577.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting
the names of genes and gene products with a hidden
Markov model. In Proc. COLING.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proc. NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. ICML.
A. K. McCallum. 2002. MALLET: A machine learning
for language toolkit.
D.M. McDonald, H. Chen, H. Su, and B.B. Marshall.
2004a. Extracting gene pathway relations using a hy-
brid grammar: the Arizona Relation Parser. Bioinfor-
matics, 20(18):3370–78.
R.T. McDonald, R.S. Winters, M. Mandel, Y. Jin, P.S.
White, and F. Pereira. 2004b. An entity tagger for
recognizing acquired genomic variations in cancer lit-
erature. Bioinformatics, 20(17):3249–3251.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proc. NAACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Learning via inference over structurally constrained
output. In Workshop on Learning Structured with Out-
put, NIPS.
Barbara Rosario and Marti A. Hearst. 2004. Classifying
semantic relations in bioscience texts. In ACL.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. CoNLL.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. JMLR.
</reference>
<page confidence="0.99807">
498
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.819235">
<title confidence="0.9995175">Simple Algorithms for Complex Relation Extraction with Applications to Biomedical IE</title>
<author confidence="0.99999">Fernando Seth</author>
<affiliation confidence="0.992065">and University of Pennsylvania, Philadelphia, PA</affiliation>
<email confidence="0.998963">skulick@linc.cis.upenn.edu</email>
<author confidence="0.993565">Scott Winters Yang Jin Pete White</author>
<affiliation confidence="0.935893">Division of Oncology, Children’s Hospital of Pennsylvania, Philadelphia, PA</affiliation>
<abstract confidence="0.991750928571428">complex relation is any relation in which some of the arguments may be be unspecified. We present here a simple two-stage method for extracting complex relations between named entities in text. The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="13986" citStr="Berger et al., 1996" startWordPosition="2332" endWordPosition="2335">example sentence in Figure 1, these include the pairs (John, CEO), (John, Inc. Corp.), (CEO, Inc. Corp.), (CEO, Biz. Corp.), (Jane, CEO) and (Jane, Biz. Corp.). To gather negative instances, we extract all pairs that never occur together in a valid relation. From the same example these would be the pairs (John, Biz. Corp.) and (Jane, Inc. Corp.). This leads to a large set of positive and negative binary relation instances. At this point we could employ any binary relation classifier and learn to identify new instances of related pairs of entities. We use a standard maximum entropy classifier (Berger et al., 1996) implemented as part of MALLET (McCallum, 2002). The model is trained using the features listed in Table 1. This is a very simple binary classification model. No deep syntactic structure such as parse trees is used. All features are basically over the words separating two entities and their part-of-speech tags. Of course, it would be possible to use more syntactic information if available in a manner similar to that of Zelenko et al. (2003). However, the primary purpose of our experiments was not to create a better binary relation extractor, but to see if complex relations could be extracted t</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<journal>Machine Learning Journal Special Issue on Natural Language Learning,</journal>
<pages>34--1</pages>
<contexts>
<context position="1132" citStr="Bikel et al., 1999" startWordPosition="153" endWordPosition="156">ere a simple two-stage method for extracting complex relations between named entities in text. The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 1 Introduction Most research on text information extraction (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods fo</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning Journal Special Issue on Natural Language Learning, 34(1/3):221–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bron</author>
<author>J Kerbosch</author>
</authors>
<title>Algorithm 457: finding all cliques of an undirected graph.</title>
<date>1973</date>
<journal>Communications of the ACM,</journal>
<volume>16</volume>
<issue>9</issue>
<contexts>
<context position="17981" citStr="Bron and Kerbosch (1973)" startWordPosition="3021" endWordPosition="3024">s in which perfect binary classification followed by maximal clique reconstruction will lead to errors. One such sentence is, “John is C.E.O. and C.F.O. ofInc. Corp. and Biz. Corp. respectively and Jane vice-versa”. However, we expect such sentences to be rare; in fact, they never occur in our data. The real problem with this approach is that an arbitrary graph can have exponentially many cliques, negating any efficiency advantage over enumerating all n-tuples of entities. Fortunately, there are algorithms for finding all maximal cliques that are efficient in practice. We use the algorithm of Bron and Kerbosch (1973). This is a well known branch and bound algorithm that has been shown to empirically run linearly in the number of maximal cliques in the graph. In our experiments, this algorithm found all maximal cliques in a matter of seconds. 4.2.2 Probabilistic Cliques The above approach has a major shortcoming in that it assumes the output of the binary classifier to be absolutely correct. For instance, the classifier may have thought with probability 0.49, 0.99 and 0.99 that the following pairs were related: (Jane, Biz. Corp.), (CEO, Biz. Corp.) and (Jane, CEO) respectively. The maximal clique method wo</context>
</contexts>
<marker>Bron, Kerbosch, 1973</marker>
<rawString>C. Bron and J. Kerbosch. 1973. Algorithm 457: finding all cliques of an undirected graph. Communications of the ACM, 16(9):575–577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>C Nobata</author>
<author>J Tsujii</author>
</authors>
<title>Extracting the names of genes and gene products with a hidden Markov model.</title>
<date>2000</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="1593" citStr="Collier et al., 2000" startWordPosition="225" endWordPosition="228"> (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involving named entities. Such relations would be extremely useful in applications like question answering, automatic database generation, and intelligent document searching and indexing. Though not as well studied as entity extraction, relation extraction has still seen a significant amount of work. We discuss some previous approaches at greater length in Section 2. Most relation extraction systems focus on the specific pr</context>
</contexts>
<marker>Collier, Nobata, Tsujii, 2000</marker>
<rawString>N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting the names of genes and gene products with a hidden Markov model. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="8092" citStr="Gildea and Jurafsky, 2002" startWordPosition="1291" endWordPosition="1294">ere has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is 492 always associated with a verb in the sentence that a. All possible b. All possible specifies the relation type. Though this system is relation instances binary relations very general, it is limited by the fact that the design (John, CEO, Inc. Corp.) (John, CEO) ignores relations not expressed by a verb, as the em- (John, L, Inc. Corp.) (John, Inc. Corp.) ployee of relation in“John Smith, CEO ofInc. Corp., (John, CEO, Biz. Corp.) (John, Biz. Corp.) announced he will resign”. (John, L, Biz. Corp.) (CEO, Inc. Corp.) Most relation extraction systems work primarily (John, CEO</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="1312" citStr="Kudo and Matsumoto, 2001" startWordPosition="179" endWordPosition="182">lated, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 1 Introduction Most research on text information extraction (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involving named entities. Such relations would be extremely useful in applications like question answering, automatic database generation, and int</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="1285" citStr="Lafferty et al., 2001" startWordPosition="175" endWordPosition="178">hat are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 1 Introduction Most research on text information extraction (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involving named entities. Such relations would be extremely useful in applications like question answering, automatic d</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="28782" citStr="McCallum and Wellner (2003)" startWordPosition="4802" endWordPosition="4805">c variation relations. In fact, this approach is both significantly quicker and more accurate then enumerating and classifying all possible instances. We believe this work provides a good starting point for continued research in this area. A distinction may be made between the factored system presented here and one that attempts to classify complex relations without factorization. This is related to the distinction between methods that learn local classifiers that are combined with global constraints after training and methods that incorporate the global constraints into the learning process. McCallum and Wellner (2003) showed that learning binary co-reference relations globally improves performance over learning relations in isolation. However, their model relied on the transitive property inherent in the co-reference relation. Our system can be seen as an instance of a local learner. Punyakanok 497 et al. (2004) argued that local learning actually outperforms global learning in cases when local decisions can easily be learnt by the classifier. Hence, it is reasonable to assume that our binary factorization method will perform well when binary relations can be learnt with high accuracy. As for future work, </context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="1262" citStr="McCallum et al., 2000" startWordPosition="171" endWordPosition="174">rom pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 1 Introduction Most research on text information extraction (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involving named entities. Such relations would be extremely useful in applications like question</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<contexts>
<context position="14033" citStr="McCallum, 2002" startWordPosition="2341" endWordPosition="2343">s (John, CEO), (John, Inc. Corp.), (CEO, Inc. Corp.), (CEO, Biz. Corp.), (Jane, CEO) and (Jane, Biz. Corp.). To gather negative instances, we extract all pairs that never occur together in a valid relation. From the same example these would be the pairs (John, Biz. Corp.) and (Jane, Inc. Corp.). This leads to a large set of positive and negative binary relation instances. At this point we could employ any binary relation classifier and learn to identify new instances of related pairs of entities. We use a standard maximum entropy classifier (Berger et al., 1996) implemented as part of MALLET (McCallum, 2002). The model is trained using the features listed in Table 1. This is a very simple binary classification model. No deep syntactic structure such as parse trees is used. All features are basically over the words separating two entities and their part-of-speech tags. Of course, it would be possible to use more syntactic information if available in a manner similar to that of Zelenko et al. (2003). However, the primary purpose of our experiments was not to create a better binary relation extractor, but to see if complex relations could be extracted through binary factorizaFeature Set entity type </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A. K. McCallum. 2002. MALLET: A machine learning for language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M McDonald</author>
<author>H Chen</author>
<author>H Su</author>
<author>B B Marshall</author>
</authors>
<title>Extracting gene pathway relations using a hybrid grammar: the Arizona Relation Parser.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>18</issue>
<contexts>
<context position="1639" citStr="McDonald et al., 2004" startWordPosition="232" endWordPosition="235"> entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involving named entities. Such relations would be extremely useful in applications like question answering, automatic database generation, and intelligent document searching and indexing. Though not as well studied as entity extraction, relation extraction has still seen a significant amount of work. We discuss some previous approaches at greater length in Section 2. Most relation extraction systems focus on the specific problem of extracting binary relations, such as </context>
<context position="7891" citStr="McDonald et al. (2004" startWordPosition="1263" endWordPosition="1266">his joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is 492 always associated with a verb in the sentence that a. All possible b. All possible specifies the relation type. Though this system is relation instances binary relations very general, it is limited by the fact that the design (John, CEO, Inc. Corp.) (John, CEO) ignores relations not expressed by a verb, as the em- (John, L, Inc. Corp.) (John, Inc. Corp.) ployee of relation </context>
<context position="21007" citStr="McDonald et al., 2004" startWordPosition="3543" endWordPosition="3547">arse. Another problem with this approach is the assumption that the binary relation classifier outputs probabilities. For maximum entropy and other probabilistic frameworks this is not an issue. However, many classifiers, such as SVMs, output scores or distances. It is possible to transform the scores from those models through a sigmoid to yield probabili��/|EC| � 495 ties, but there is no guarantee that those probability values will be well calibrated. 5 Experiments 5.1 Problem Description and Data We test these methods on the task of extracting genomic variation events from biomedical text (McDonald et al., 2004b). Briefly, we define a variation event as an acquired genomic aberration: a specific, one-time alteration at the genomic level and described at the nucleic acid level, amino acid level or both. Each variation event is identified by the relationship between a type of variation, its location, and the corresponding state change from an initialstate to an altered-state. This can be formalized as the following complex schema (var-type, location, initial-state, altered-state) A simple example is the sentence “At codons 12 and 61, the occurrence of point mutations from G/A to T/G were observed” whi</context>
<context position="22796" citStr="McDonald et al., 2004" startWordPosition="3842" endWordPosition="3845"> one L argument, and 175 have no L arguments. Thus, 38% of the relations tagged in this data cannot be handled using binary relation classification alone. In addition, 4% of the relations annotated in this data are non-sentential. Our system currently only produces sentential relations and is therefore bounded by a maximum recall of 96%. Finally, we use gold standard entities in our experiments. This way we can evaluate the performance of the relation extraction system isolated from any kind of pipelined entity extraction errors. Entities in this domain can be found with fairly high accuracy (McDonald et al., 2004b). It is important to note that just the presence of two entity types does not entail a relation between them. In fact, 56% of entity pairs are not related, due either to explicit disqualification in the text (e.g. “... the lack of G to T transversion ...”) or ambiguities that arise from multiple entities of the same type. 5.2 Results Because the data contains only 1218 examples of relations we performed 10-fold cross-validation tests for all results. We compared three systems: • MC: Uses the maximum entropy binary classifier coupled with the maximal clique complex relation reconstructor. • P</context>
</contexts>
<marker>McDonald, Chen, Su, Marshall, 2004</marker>
<rawString>D.M. McDonald, H. Chen, H. Su, and B.B. Marshall. 2004a. Extracting gene pathway relations using a hybrid grammar: the Arizona Relation Parser. Bioinformatics, 20(18):3370–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>R S Winters</author>
<author>M Mandel</author>
<author>Y Jin</author>
<author>P S White</author>
<author>F Pereira</author>
</authors>
<title>An entity tagger for recognizing acquired genomic variations in cancer literature.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>17</issue>
<contexts>
<context position="1639" citStr="McDonald et al., 2004" startWordPosition="232" endWordPosition="235"> entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involving named entities. Such relations would be extremely useful in applications like question answering, automatic database generation, and intelligent document searching and indexing. Though not as well studied as entity extraction, relation extraction has still seen a significant amount of work. We discuss some previous approaches at greater length in Section 2. Most relation extraction systems focus on the specific problem of extracting binary relations, such as </context>
<context position="7891" citStr="McDonald et al. (2004" startWordPosition="1263" endWordPosition="1266">his joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is 492 always associated with a verb in the sentence that a. All possible b. All possible specifies the relation type. Though this system is relation instances binary relations very general, it is limited by the fact that the design (John, CEO, Inc. Corp.) (John, CEO) ignores relations not expressed by a verb, as the em- (John, L, Inc. Corp.) (John, Inc. Corp.) ployee of relation </context>
<context position="21007" citStr="McDonald et al., 2004" startWordPosition="3543" endWordPosition="3547">arse. Another problem with this approach is the assumption that the binary relation classifier outputs probabilities. For maximum entropy and other probabilistic frameworks this is not an issue. However, many classifiers, such as SVMs, output scores or distances. It is possible to transform the scores from those models through a sigmoid to yield probabili��/|EC| � 495 ties, but there is no guarantee that those probability values will be well calibrated. 5 Experiments 5.1 Problem Description and Data We test these methods on the task of extracting genomic variation events from biomedical text (McDonald et al., 2004b). Briefly, we define a variation event as an acquired genomic aberration: a specific, one-time alteration at the genomic level and described at the nucleic acid level, amino acid level or both. Each variation event is identified by the relationship between a type of variation, its location, and the corresponding state change from an initialstate to an altered-state. This can be formalized as the following complex schema (var-type, location, initial-state, altered-state) A simple example is the sentence “At codons 12 and 61, the occurrence of point mutations from G/A to T/G were observed” whi</context>
<context position="22796" citStr="McDonald et al., 2004" startWordPosition="3842" endWordPosition="3845"> one L argument, and 175 have no L arguments. Thus, 38% of the relations tagged in this data cannot be handled using binary relation classification alone. In addition, 4% of the relations annotated in this data are non-sentential. Our system currently only produces sentential relations and is therefore bounded by a maximum recall of 96%. Finally, we use gold standard entities in our experiments. This way we can evaluate the performance of the relation extraction system isolated from any kind of pipelined entity extraction errors. Entities in this domain can be found with fairly high accuracy (McDonald et al., 2004b). It is important to note that just the presence of two entity types does not entail a relation between them. In fact, 56% of entity pairs are not related, due either to explicit disqualification in the text (e.g. “... the lack of G to T transversion ...”) or ambiguities that arise from multiple entities of the same type. 5.2 Results Because the data contains only 1218 examples of relations we performed 10-fold cross-validation tests for all results. We compared three systems: • MC: Uses the maximum entropy binary classifier coupled with the maximal clique complex relation reconstructor. • P</context>
</contexts>
<marker>McDonald, Winters, Mandel, Jin, White, Pereira, 2004</marker>
<rawString>R.T. McDonald, R.S. Winters, M. Mandel, Y. Jin, P.S. White, and F. Pereira. 2004b. An entity tagger for recognizing acquired genomic variations in cancer literature. Bioinformatics, 20(17):3249–3251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>H Fox</author>
<author>L A Ramshaw</author>
<author>R M Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="6178" citStr="Miller et al. (2000)" startWordPosition="991" endWordPosition="994">r that shallow parse representation. This approach — enumerating all possible entity pairs and classifying each as positive or negative — is the standard method in relation extraction. The main differences among systems are the choice of trainable classifier and the representation for instances. For binary relations, this approach is quite tractable: if the relation schema is (t1, t2), the number of potential instances is O(|t1 ||t2|), where |t |is the number of entity mentions of type t in the text under consideration. One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. Once this augmentation is made, any standard parser can be trained and then run on new sentences to extract new relations. Miller et al. show such an approach can yield good results. However, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrat</context>
<context position="24489" citStr="Miller et al., 2000" startWordPosition="4121" endWordPosition="4124">ons of the pairwise features. For this system we only take maximal relations,that is, if (John, CEO, Inc. Corp.) and (John, L, Inc. Corp.) are both labeled positive, the system would only return the former. Table 2 contains the results of the maximum entropy binary relation classifier (used in systems MC and PC). The 1218 annotated complex relations produced 2577 unique binary pairs of related entities. We can see that the maximum entropy classifier performs reasonably well, although performance may be affected by the lack of rich syntactic features, which have been shown to help performance (Miller et al., 2000; Zelenko et al., 2003). Table 3 compares the three systems on the real problem of extracting complex relations. An extracted complex relation is considered correct if and only if all the entities in the relation are correct. There is no partial credit. All training and clique finding algorithms took under 5 minutes for the entire data set. Naive enumeration took approximately 26 minutes to train. 496 ACT PRD COR 2577 2722 2101 Prec Rec F-Meas 0.7719 0.8153 0.7930 Table 2: Binary relation classification results for the maximum entropy classifier. ACT: actual number of related pairs, PRD: predi</context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Learning via inference over structurally constrained output.</title>
<date>2004</date>
<booktitle>In Workshop on Learning Structured with Output, NIPS.</booktitle>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Learning via inference over structurally constrained output. In Workshop on Learning Structured with Output, NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti A Hearst</author>
</authors>
<title>Classifying semantic relations in bioscience texts.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7556" citStr="Rosario and Hearst (2004)" startWordPosition="1214" endWordPosition="1217">rsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is 492 always associated with a verb in the sent</context>
</contexts>
<marker>Rosario, Hearst, 2004</marker>
<rawString>Barbara Rosario and Marti A. Hearst. 2004. Classifying semantic relations in bioscience texts. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proc. CoNLL.</booktitle>
<contexts>
<context position="7125" citStr="Roth and Yih (2004)" startWordPosition="1144" endWordPosition="1147">owever, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrating relation extraction with parsing, the system cannot consider long-range dependencies due to the local parsing constraints of current probabilistic parsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="4935" citStr="Zelenko et al. (2003)" startWordPosition="780" endWordPosition="783">lations into binary relations is that it allows the use of standard classification algorithms to decide whether particular pairs of entity mentions are related. In addition, the factoring makes training data less sparse and reduces the computational cost of extraction. We will discuss these benefits further in Section 4. We evaluated the methods on a large set of annotated biomedical documents to extract relations related to genomic variations, demonstrating a considerable improvement over a reasonable baseline. 2 Previous work A representative approach to relation extraction is the system of Zelenko et al. (2003), which attempts to identify binary relations in news text. In that system, each pair of entity mentions of the correct types in a sentence is classified as to whether it is a positive instance of the relation. Consider the binary relation employee of and the sentence “John Smith, not Jane Smith, works at IBM”. The pair (John Smith, IBM) is a positive instance, while the pair (Jane Smith, IBM) is a negative instance. Instances are represented by a pair of entities and their position in a shallow parse tree for the containing sentence. Classification is done by a support-vector classifier with </context>
<context position="11386" citStr="Zelenko et al. (2003)" startWordPosition="1889" endWordPosition="1892">Relations Consider again the motivating example of the (person, job, company) relation and the sentence “John and Jane are CEOs at Inc. Corp. and Biz. Corp. respectively”. This sentence contains two people, one job title and two companies. One possible method for extracting the relation of interest would be to first consider all 12 possible tuples shown in Figure 1a. Using all these tuples, it should then be possible to train a classifier to distinguish valid instances such as (John, CEO, Inc. Corp.) from invalid ones such as (Jane, CEO, Inc. Corp.). This is analogous to the approach taken by Zelenko et al. (2003) for binary relations. There are problems with this approach. Computationally, for an n-ary relation, the number of possible instances is O(|t1 ||t2 |· · · |tn|). Conservatively, letting m be the smallest |ti|, the run time is O(mn), exponential in the arity of the relation. The second problem is how to manage incomplete but correct instances such as (John, 1, Inc. Corp.) when training the classifier. If this instance is marked as negative, then the model might incorrectly disfavor features that correlate John to Inc. Corp.. However, if this instance is labeled positive, then the model may ten</context>
<context position="14430" citStr="Zelenko et al. (2003)" startWordPosition="2409" endWordPosition="2412"> could employ any binary relation classifier and learn to identify new instances of related pairs of entities. We use a standard maximum entropy classifier (Berger et al., 1996) implemented as part of MALLET (McCallum, 2002). The model is trained using the features listed in Table 1. This is a very simple binary classification model. No deep syntactic structure such as parse trees is used. All features are basically over the words separating two entities and their part-of-speech tags. Of course, it would be possible to use more syntactic information if available in a manner similar to that of Zelenko et al. (2003). However, the primary purpose of our experiments was not to create a better binary relation extractor, but to see if complex relations could be extracted through binary factorizaFeature Set entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 concatenations of above features Table 1: Feature set for maximum entropy binary relation classifier. e1 and e2 are entities. b. Tuples from G (John, CEO, 1) (John, 1, Inc. Corp.) (John, 1, Biz. Corp.) (Jane, CEO, 1) (</context>
<context position="24512" citStr="Zelenko et al., 2003" startWordPosition="4125" endWordPosition="4128">eatures. For this system we only take maximal relations,that is, if (John, CEO, Inc. Corp.) and (John, L, Inc. Corp.) are both labeled positive, the system would only return the former. Table 2 contains the results of the maximum entropy binary relation classifier (used in systems MC and PC). The 1218 annotated complex relations produced 2577 unique binary pairs of related entities. We can see that the maximum entropy classifier performs reasonably well, although performance may be affected by the lack of rich syntactic features, which have been shown to help performance (Miller et al., 2000; Zelenko et al., 2003). Table 3 compares the three systems on the real problem of extracting complex relations. An extracted complex relation is considered correct if and only if all the entities in the relation are correct. There is no partial credit. All training and clique finding algorithms took under 5 minutes for the entire data set. Naive enumeration took approximately 26 minutes to train. 496 ACT PRD COR 2577 2722 2101 Prec Rec F-Meas 0.7719 0.8153 0.7930 Table 2: Binary relation classification results for the maximum entropy classifier. ACT: actual number of related pairs, PRD: predicted number of related </context>
<context position="29715" citStr="Zelenko et al., 2003" startWordPosition="4960" endWordPosition="4963"> local learning actually outperforms global learning in cases when local decisions can easily be learnt by the classifier. Hence, it is reasonable to assume that our binary factorization method will perform well when binary relations can be learnt with high accuracy. As for future work, there are many things that we plan to look at. The binary relation classifier we employ is quite simplistic and most likely can be improved by using features over a deeper representation of the data such as parse trees. Other more powerful binary classifiers should be tried such as those based on tree kernels (Zelenko et al., 2003). We also plan on running these algorithms on more data sets to test if the algorithms empirically generalize to different domains. Perhaps the most interesting open problem is how to learn the complex reconstruction phase. One possibility is recent work on supervised clustering. Letting the edge probabilities in the graphs represent a distance in some space, it may be possible to learn how to cluster vertices into relational groups. However, since a vertex/entity can participate in one or more relation, any clustering algorithm would be required to produce non-disjoint clusters. We mentioned </context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel methods for relation extraction. JMLR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>