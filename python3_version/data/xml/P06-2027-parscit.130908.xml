<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.848081">
Automatic Creation of Domain Templates
</title>
<author confidence="0.995703">
Elena Filatova*, Vasileios Hatzivassiloglou† and Kathleen McKeown*
</author>
<affiliation confidence="0.996907">
*Department of Computer Science
Columbia University
</affiliation>
<email confidence="0.990726">
{filatova,kathy}@cs.columbia.edu
</email>
<sectionHeader confidence="0.997286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923238095238">
Recently, many Natural Language Processing
(NLP) applications have improved the quality of
their output by using various machine learning tech-
niques to mine Information Extraction (IE) patterns
for capturing information from the input text. Cur-
rently, to mine IE patterns one should know in ad-
vance the type of the information that should be
captured by these patterns. In this work we pro-
pose a novel methodology for corpus analysis based
on cross-examination of several document collec-
tions representing different instances of the same
domain. We show that this methodology can be
used for automatic domain template creation. As the
problem of automatic domain template creation is
rather new, there is no well-defined procedure for
the evaluation of the domain template quality. Thus,
we propose a methodology for identifying what in-
formation should be present in the template. Using
this information we evaluate the automatically cre-
ated domain templates through the text snippets re-
trieved according to the created templates.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999646833333333">
Open-ended question-answering (QA) systems
typically produce a response containing a vari-
ety of specific facts proscribed by the question
type. A biography, for example, might contain the
date of birth, occupation, or nationality of the per-
son in question (Duboue and McKeown, 2003;
Zhou et al., 2004; Weischedel et al., 2004; Fila-
tova and Prager, 2005). A definition may contain
the genus of the term and characteristic attributes
(Blair-Goldensohn et al., 2004). A response to a
question about a terrorist attack might include the
event, victims, perpetrator and date as the tem-
plates designed for the Message Understanding
Conferences (Radev and McKeown, 1998; White
et al., 2001) predicted. Furthermore, the type of in-
formation included varies depending on context. A
biography of an actor would include movie names,
while a biography of an inventor would include the
names of inventions. A description of a terrorist
event in Latin America in the eighties is different
from the description of today’s terrorist events.
How does one determine what facts are im-
portant for different kinds of responses? Often
the types of facts that are important are hand en-
</bodyText>
<affiliation confidence="0.987186">
†Department of Computer Science
The University of Texas at Dallas
</affiliation>
<email confidence="0.945671">
vh@hlt.utdallas.edu
</email>
<bodyText confidence="0.999803066666667">
coded ahead of time by a human expert (e.g., as
in the case of MUC templates). In this paper, we
present an approach that allows a system to learn
the types of facts that are appropriate for a par-
ticular response. We focus on acquiring fact-types
for events, automatically producing a template that
can guide the creation of responses to questions
requiring a description of an event. The template
can be tailored to a specific time period or coun-
try simply by changing the document collections
from which learning takes place.
In this work, a domain is a set of events of a par-
ticular type; earthquakes and presidential elections
are two such domains. Domains can be instanti-
ated by several instances of events of that type
(e.g., the earthquake in Japan in October 2004, the
earthquake in Afghanistan in March 2002, etc.).1
The granularity of domains and instances can be
altered by examining data at different levels of de-
tail, and domains can be hierarchically structured.
An ideal template is a set of attribute-value pairs,
with the attributes specifying particular functional
roles important for the domain events.
In this paper we present a method of domain-
independent on-the-fly template creation. Our
method is completely automatic. As input it re-
quires several document collections describing do-
main instances. We cross-examine the input in-
stances, we identify verbs important for the major-
ity of instances and relationships containing these
verbs. We generalize across multiple domain in-
stances to automatically determine which of these
relations should be used in the template. We re-
port on data collection efforts and results from four
domains. We assess how well the automatically
produced templates satisfy users’ needs, as man-
ifested by questions collected for these domains.
1Unfortunately, NLP terminology is not standardized
across different tasks. Two NLP tasks most close to our
research are Topic Detection and Tracking (TDT) (Fiscus
et al., 1999) and Information Extraction (IE) (Marsh and
Perzanowski, 1997). In TDT terminology, our domains are
topics and our instances are events. In IE terminology, our
domains are scenarios and our domain templates are scenario
templates.
</bodyText>
<page confidence="0.968296">
207
</page>
<note confidence="0.8827075">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207–214,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999218" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996726816326531">
Our system automatically generates a template
that captures the generally most important infor-
mation for a particular domain and is reusable
across multiple instances of that domain. Decid-
ing what slots to include in the template, and what
restrictions to place on their potential fillers, is
a knowledge representation problem (Hobbs and
Israel, 1994). Templates were used in the main
IE competitions, the Message Understanding Con-
ferences (Hobbs and Israel, 1994; Onyshkevych,
1994; Marsh and Perzanowski, 1997). One of the
recent evaluations, ACE,2 uses pre-defined frames
connecting event types (e.g., arrest, release) to a
set of attributes. The template construction task
was not addressed by the participating systems.
The domain templates were created manually by
experts to capture the structure of the facts sought.
Although templates have been extensively used
in information extraction, there has been little
work on their automatic design. In the Concep-
tual Case Frame Acquisition project (Riloff and
Schmelzenbach, 1998), extraction patterns, a do-
main semantic lexicon, and a list of conceptual
roles and associated semantic categories for the
domain are used to produce multiple-slot case
frames with selectional restrictions. The system
requires two sets of documents: those relevant to
the domain and those irrelevant. Our approach
does not require any domain-specific knowledge
and uses only corpus-based statistics.
The GISTexter summarization sys-
tem (Harabagiu and Maiorano, 2002) used
statistics over an arbitrary document collection
together with semantic relations from WordNet.
The created templates heavily depend on the top-
ical relations encoded in WordNet. The template
models an input collection of documents. If there
is only one domain instance described in the input
than the template is created for this particular
instance rather than for a domain. In our work,
we learn domain templates by cross-examining
several collections of documents on the same
topic, aiming for a general domain template. We
rely on relations cross-mentioned in different
instances of the domain to automatically prioritize
roles and relationships for selection.
Topic Themes (Harabagiu and L˘ac˘atus¸u, 2005)
used for multi-document summarization merge
various arguments corresponding to the same se-
</bodyText>
<footnote confidence="0.78648">
2http://www.nist.gov/speech/tests/ace/index.htm
</footnote>
<bodyText confidence="0.999753607843137">
mantic roles for the semantically identical verb
phrases (e.g., arrests and placed under arrest).
Atomic events also model an input document
collection (Filatova and Hatzivassiloglou, 2003)
and are created according to the statistics col-
lected for co-occurrences of named entity pairs
linked through actions. GISTexter, atomic events,
and Topic Themes were used for modeling a col-
lection of documents rather than a domain.
In other closely related work, Sudo et al. (2003)
use frequent dependency subtrees as measured by
TF*IDF to identify named entities and IE patterns
important for a given domain. The goal of their
work is to show how the techniques improve IE
pattern acquisition. To do this, Sudo et al. con-
strain the retrieval of relevant documents for a
MUC scenario and then use unsupervised learn-
ing over descriptions within these documents that
match specific types of named entities (e.g., Ar-
resting Agency, Charge), thus enabling learning
of patterns for specific templates (e.g., the Ar-
rest scenario). In contrast, the goal of our work
is to show how similar techniques can be used to
learn what information is important for a given
domain or event and thus, should be included
into the domain template. Our approach allows,
for example, learning that an arrest along with
other events (e.g., attack) is often part of a ter-
rorist event. We do not assume any prior knowl-
edge about domains. We demonstrate that frequent
subtrees can be used not only to extract specific
named entities for a given scenario but also to
learn domain-important relations. These relations
link domain actions and named entities as well as
general nouns and words belonging to other syn-
tactic categories.
Collier (1998) proposed a fully automatic
method for creating templates for information ex-
traction. The method relies on Luhn’s (1957) idea
of locating statistically significant words in a cor-
pus and uses those to locate the sentences in which
they occur. Then it extracts Subject-Verb-Object
patterns in those sentences to identify the most
important interactions in the input data. The sys-
tem was constructed to create MUC templates for
terrorist attacks. Our work also relies on corpus
statistics, but we utilize arbitrary syntactic pat-
terns and explicitly use multiple domain instances.
Keeping domain instances separated, we cross-
examine them and estimate the importance of a
particular information type in the domain.
</bodyText>
<page confidence="0.998742">
208
</page>
<sectionHeader confidence="0.981348" genericHeader="method">
3 Our Approach to Template Creation
</sectionHeader>
<bodyText confidence="0.999967444444445">
After reading about presidential elections in dif-
ferent countries on different years, a reader has a
general picture of this process. Later, when read-
ing about a new presidential election, the reader al-
ready has in her mind a set of questions for which
she expects answers. This process can be called
domain modeling. The more instances of a partic-
ular domain a person has seen, the better under-
standing she has about what type of information
should be expected in an unseen collection of doc-
uments discussing a new instance of this domain.
Thus, we propose to use a set of document col-
lections describing different instances within one
domain to learn the general characteristics of this
domain. These characteristics can be then used to
create a domain template. We test our system on
four domains: airplane crashes, earthquakes, pres-
idential elections, terrorist attacks.
</bodyText>
<sectionHeader confidence="0.993604" genericHeader="method">
4 Data Description
</sectionHeader>
<subsectionHeader confidence="0.999029">
4.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999988826086957">
To create training document collections we used
BBC Advanced Search3 and submitted queries of
the type (domain title + country). For example,
(“presidential election” USA).
In addition, we used BBC’s Advanced Search
date filter to constrain the results to different date
periods of interest. For example, we used known
dates of elections and allowed a search for articles
published up to five days before or after each such
date. At the same time for the terrorist attacks or
earthquakes domain the time constraints we sub-
mitted were the day of the event plus ten days.
Thus, we identify several instances for each of
our four domains, obtaining a document collec-
tion for each instance. E.g., for the earthquake do-
main we collected documents on the earthquakes
in Afghanistan (March 25, 2002), India (January
26, 2001), Iran (December 26, 2003), Japan (Oc-
tober 26, 2004), and Peru (June 23, 2001). Using
this procedure we retrieve training document col-
lections for 9 instances of airplane crashes, 5 in-
stances of earthquakes, 13 instances of presiden-
tial elections, and 6 instances of terrorist attacks.
</bodyText>
<subsectionHeader confidence="0.999828">
4.2 Test Data
</subsectionHeader>
<bodyText confidence="0.9993775">
To test our system, we used document clusters
from the Topic Detection and Tracking (TDT) cor-
</bodyText>
<footnote confidence="0.763411">
3http://news.bbc.co.uk/shared/bsp/search2/
advanced/news_ifs.stm
</footnote>
<bodyText confidence="0.999890192307693">
pus (Fiscus et al., 1999). Each TDT topic has a
topic label, such as Accidents or Natural Disas-
ters.4 These categories are broader than our do-
mains. Thus, we manually filtered the TDT topics
relevant to our four training domains (e.g., Acci-
dents matching Airplane Crashes). In this way, we
obtained TDT document clusters for 2 instances
of airplane crashes, 3 instances of earthquakes, 6
instances of presidential elections and 3 instances
of terrorist attacks. The number of the documents
corresponding to the instances varies greatly (from
two documents for one of the earthquakes up to
156 documents for one of the terrorist attacks).
This variation in the number of documents per
topic is typical for the TDT corpus. Many of the
current approaches of domain modeling collapse
together different instances and make the decision
on what information is important for a domain
based on this generalized corpus (Collier, 1998;
Barzilay and Lee, 2003; Sudo et al., 2003). We,
on the other hand, propose to cross-examine these
instances keeping them separated. Our goal is to
eliminate dependence on how well the corpus is
balanced and to avoid the possibility of greater
impact on the domain template of those instances
which have more documents.
</bodyText>
<sectionHeader confidence="0.942681" genericHeader="method">
5 Creating Templates
</sectionHeader>
<bodyText confidence="0.982191809523809">
In this work we build domain templates around
verbs which are estimated to be important for the
domains. Using verbs as the starting point we
identify semantic dependencies within sentences.
In contrast to deep semantic analysis (Fillmore
and Baker, 2001; Gildea and Jurafsky, 2002; Prad-
han et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005;
Palmer et al., 2005) we rely only on corpus statis-
tics. We extract the most frequent syntactic sub-
trees which connect verbs to the lexemes used in
the same subtrees. These subtrees are used to cre-
ate domain templates.
For each of the four domains described in Sec-
tion 4, we automatically create domain templates
using the following algorithm.
Step 1: Estimate what verbs are important for
the domain under investigation. We initiate our
algorithm by calculating the probabilities for all
the verbs in the document collection for one do-
main — e.g., the collection containing all the in-
stances in the domain of airplane crashes. We
</bodyText>
<footnote confidence="0.911639">
4In our experiments we analyze TDT topics used in
TDT-2 and TDT-4 evaluations.
</footnote>
<page confidence="0.997918">
209
</page>
<bodyText confidence="0.999952833333333">
discard those verbs that are stop words (Salton,
1971). To take into consideration the distribution
of a verb among different instances of the domain,
we normalize this probability by its VIF value
(verb instance frequency), specifying in how many
domain instances this verb appears.
</bodyText>
<equation confidence="0.9245008">
E
vb; Ecomb coll countvb;
# of domain instances containing vbi
VIF(vbi) _ (2)
# of all domain instances
</equation>
<bodyText confidence="0.988882975">
These verbs are estimated to be the most impor-
tant for the combined document collection for all
the domain instances. Thus, we build the domain
template around these verbs. Here are the top ten
verbs for the terrorist attack domain:
killed, told, found, injured, reported,
happened, blamed, arrested, died, linked.
Step 2: Parse those sentences which contain the
top 50 verbs. After we identify the 50 most impor-
tant verbs for the domain under analysis, we parse
all the sentences in the domain document collec-
tion containing these verbs with the Stanford syn-
tactic parser (Klein and Manning, 2002).
Step 3: Identify most frequent subtrees containing
the top 50 verbs. A domain template should con-
tain not only the most important actions for the do-
main, but also the entities that are linked to these
actions or to each other through these actions. The
lexemes referring to such entities can potentially
be used within the domain template slots. Thus,
we analyze those portions of the syntactic trees
which contain the verbs themselves plus other lex-
emes used in the same subtrees as the verbs. To do
this we use FREQuent Tree miner.5 This software
is an implementation of the algorithm presented
by (Abe et al., 2002; Zaki, 2002), which extracts
frequent ordered subtrees from a set of ordered
trees. Following (Sudo et al., 2003) we are inter-
ested only in the lexemes which are near neighbors
of the most frequent verbs. Thus, we look only for
those subtrees which contain the verbs themselves
and from four to ten tree nodes, where a node is
either a syntactic tag or a lexeme with its tag. We
analyze not only NPs which correspond to the sub-
ject or object of the verb, but other syntactic con-
stituents as well. For example, PPs can potentially
link the verb to locations or dates, and we want to
include this information into the template. Table 1
contains a sample of subtrees for the terrorist at-
tack domain mined from the sentences containing
</bodyText>
<footnote confidence="0.967964">
5http://chasen.org/˜taku/software/freqt/
</footnote>
<table confidence="0.98688725">
s nosubtree
8 (SBAR(S(VP(VBD killed)(NP(QP(IN at))(NNS people)))))
8 (SBAR(S(VP(VBD killed)(NP(QP(JJS least))(NNS people)))))
5 (VP(ADVP)(VBD killed)(NP(NNS people)))
6 (VP(VBD killed)(NP(ADJP(JJ many))(NNS people)))
5 (VP(VP(VBD killed)(NP(NNS people))))
7 (VP(ADVP(NP))(VBD killed)(NP(CD 34)(NNS people)))
6 (VP(ADVP)(VBD killed)(NP(CD 34)(NNS people)))
</table>
<tableCaption confidence="0.999526">
Table 1: Sample subtrees for the terrorist attack domain.
</tableCaption>
<bodyText confidence="0.990815866666667">
the verb killed. The first column of Table 1 shows
how many nodes are in the subtree.
Step 4: Substitute named entities with their re-
spective tags. We are interested in analyzing a
whole domain, not just an instance of this do-
main. Thus, we substitute all the named entities
with their respective tags, and all the exact num-
bers with the tag NUMBER. We speculate that sub-
trees similar to those presented in Table 1 can
be extracted from a document collection repre-
senting any instance of a terrorist attack, with the
only difference being the exact number of causal-
ities. Later, however, we analyze the domain in-
stances separately to identity information typi-
cal for the domain. The procedure of substitut-
ing named entities with their respective tags previ-
ously proved to be useful for various tasks (Barzi-
lay and Lee, 2003; Sudo et al., 2003; Filatova and
Prager, 2005). To get named entity tags we used
BBN’s IdentiFinder (Bikel et al., 1999).
Step 5: Merge together the frequent subtrees. Fi-
nally, we merge together those subtrees which
are identical according to the information encoded
within them. This is a key step in our algorithm
which allows us to bring together subtrees from
different instances of the same domain. For exam-
ple, the information rendered by all the subtrees
from the bottom part of Table 1 is identical. Thus,
these subtrees can be merged into one which con-
tains the longest common pattern:
</bodyText>
<subsubsectionHeader confidence="0.386463">
(VBD killed)(NP(NUMBER)(NNS people))
</subsubsectionHeader>
<bodyText confidence="0.999909583333333">
After this merging procedure we keep only those
subtrees for which each of the domain instances
has at least one of the subtrees from the initial set
of subtrees. This subtree should be used in the in-
stance at least twice. At this step, we make sure
that we keep in the template only the information
which is generally important for the domain rather
than only for a fraction of instances in this domain.
We also remove all the syntactic tags as we want
to make this pattern as general for the domain as
possible. A pattern without syntactic dependencies
contains a verb together with a prospective tem-
</bodyText>
<equation confidence="0.989172666666667">
Score(vbi) _
countvbi
X VIF(vbi) (1)
</equation>
<page confidence="0.902856">
210
</page>
<figure confidence="0.277341666666667">
�
plate slot corresponding to this verb:
killed: (NUMBER) (NNS people)
</figure>
<bodyText confidence="0.9999562">
In the above example, the prospective template
slots appear after the verb killed. In other cases the
domain slots appear in front of the verb. Two ex-
amples of such slots, for the presidential election
and earthquake domains, are shown below:
</bodyText>
<equation confidence="0.7929925">
(PERSON) won
(NN earthquake) struck
</equation>
<bodyText confidence="0.999967285714286">
The above examples show that it is not enough to
analyze only named entities, general nouns con-
tain important information as well. We term the
structure consisting of a verb together with the as-
sociated slots a slot structure. Here is a part of the
slot structure we get for the verb killed after cross-
examination of the terrorist attack instances:
</bodyText>
<equation confidence="0.650557666666667">
killed (NUMBER) (NNS people)
(PERSON) killed
(NN suicide) killed
</equation>
<bodyText confidence="0.99097825">
Slot structures are similar to verb frames, which
are manually created for the PropBank annota-
tion (Palmer et al., 2005).6 An example of the
PropBank frame for the verb to kill is:
</bodyText>
<figure confidence="0.9118325">
Roleset kill.01 ”cause to die”:
Arg0:killer
Arg1:corpse
Arg2:instrument
</figure>
<bodyText confidence="0.99678096">
The difference between the slot structure extracted
by our algorithm and the PropBank frame slots is
that the frame slots assign a semantic role to each
slot, while our algorithm gives either the type of
the named entity that should fill in this slot or puts
a particular noun into the slot (e.g., ORGANIZA-
TION, earthquake, people). An ideal domain tem-
plate should include semantic information but this
problem is outside of the scope of this paper.
Step 6: Creating domain templates. After we get
all the frequent subtrees containing the top 50 do-
main verbs, we merge all the subtrees correspond-
ing to the same verb and create a slot structure for
every verb as described in Step 5. The union of
such slot structures created for all the important
verbs in the domain is called the domain template.
From the created templates we remove the slots
which are used in all the domains. For example,
(PERSON) told.
The presented algorithm can be used to create a
template for any domain. It does not require pre-
defined domain or world knowledge. We learn do-
main templates from cross-examining document
collections describing different instances of the
domain of interest.
</bodyText>
<footnote confidence="0.885029">
6http://www.cs.rochester.edu/˜gildea/Verbs/
</footnote>
<sectionHeader confidence="0.996402" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999860333333333">
The task we deal with is new and there is no well-
defined and standardized evaluation procedure for
it. Sudo et al. (2003) evaluated how well their
IE patterns captured named entities of three pre-
defined types. We are interested in evaluating how
well we capture the major actions as well as their
constituent parts.
There is no set of domain templates which are
built according to a unique set of principles against
which we could compare our automatically cre-
ated templates. Thus, we need to create a gold
standard. In Section 6.1, we describe how the gold
standard is created. Then, in Section 6.2, we eval-
uate the quality of the automatically created tem-
plates by extracting clauses corresponding to the
templates and verifying how many answers from
the questions in the gold standard are answered by
the extracted clauses.
</bodyText>
<subsectionHeader confidence="0.7658455">
6.1 Stage 1. Information Included into
Templates: Interannotator Agreement
</subsectionHeader>
<bodyText confidence="0.999980038461539">
To create a gold standard we asked people to create
a list of questions which indicate what is important
for the domain description. Our decision to aim
for the lists of questions and not for the templates
themselves is based on the following considera-
tions: first, not all of our subjects are familiar with
the field of IE and thus, do not necessarily know
what an IE template is; second, our goal for this
evaluation is to estimate interannotator agreement
for capturing the important aspects for the domain
and not how well the subjects agree on the tem-
plate structure.
We asked our subjects to think of their expe-
rience of reading newswire articles about various
domains.7 Based on what they remember from this
experience, we asked them to come up with a list
of questions about a particular domain. We asked
them to come up with at most 20 questions cover-
ing the information they will be looking for given
an unseen news article about a new event in the
domain. We did not give them any input informa-
tion about the domain but allowed them to use any
sources to learn more information about the do-
main.
We had ten subjects, each of which created one
list of questions for one of the four domains under
</bodyText>
<footnote confidence="0.93228925">
7We thank Rod Adams, Cosmin-Adrian Bejan, Sasha
Blair-Goldensohn, Cyril Cerovic, David Elson, David Evans,
Ovidiu Fortu, Agustin Gravano, Lokesh Shresta, John Yundt-
Pacheco and Kapil Thadani for the submitted questions.
</footnote>
<page confidence="0.992359">
211
</page>
<table confidence="0.999115714285714">
Jaccard metric
Domain subjr and subjr and subj2 and
subj2 (and subj3) MUC MUC
Airplane crash 0.54 - -
Earthquake 0.68 - -
Presidential Election 0.32 - -
Terrorist Attack 0.50 0.63 0.59
</table>
<tableCaption confidence="0.992025">
Table 2: Creating gold standard. Jaccard metric values for in-
terannotator agreement.
</tableCaption>
<bodyText confidence="0.968177878787879">
analysis. Thus, for the earthquake and terrorist at-
tack domains we got two lists of questions; for the
airplane crash and presidential election domains
we got three lists of questions.
After the questions lists were created we studied
the agreement among annotators on what infor-
mation they consider is important for the domain
and thus, should be included in the template. We
matched the questions created by different anno-
tators for the same domain. For some of the ques-
tions we had to make a judgement call on whether
it is a match or not. For example, the following
question created by one of the annotators for the
earthquake domain was:
Did the earthquake occur in a well-known area
for earthquakes (e.g. along the San Andreas
fault), or in an unexpected location?
We matched this question to the following three
questions created by the other annotator:
What is the geological localization?
Is it near a fault line?
Is it near volcanoes?
Usually, the degree of interannotator agreement
is estimated using Kappa. For this task, though,
Kappa statistics cannot be used as they require
knowledge of the expected or chance agreement,
which is not applicable to this task (Fleiss et al.,
1981). To measure interannotator agreement we
use the Jaccard metric, which does not require
knowledge of the expected or chance agreement.
Table 2 shows the values of Jaccard metric for in-
terannotator agreement calculated for all four do-
mains. Jaccard metric values are calculated as
</bodyText>
<equation confidence="0.979478">
Jaccard(domaind) = |QSd i ∩ QSdj  |(3)
|QSd i ∪ QSd j |
</equation>
<bodyText confidence="0.999984755555556">
where QSd and QSd are the sets of questions cre-
ated by subjects i and j for domain d. For the air-
plane crash and presidential election domains we
averaged the three pairwise Jaccard metric values.
The scores in Table 2 show that for some do-
mains the agreement is quite high (e.g., earth-
quake), while for other domains (e.g., presiden-
tial election) it is twice as low. This difference
in scores can be explained by the complexity of
the domains and by the differences in understand-
ing of these domains by different subjects. The
scores for the presidential election domain are pre-
dictably low as in different countries the roles of
presidents are very different: in some countries the
president is the head of the government with a lot
of power, while in other countries the president is
merely a ceremonial figure. In some countries the
presidents are elected by general voting while in
other countries, the presidents are elected by par-
liaments. These variations in the domain cause the
subjects to be interested in different issues of the
domain. Another issue that might influence the in-
terannotator agreement is the distribution of the
presidential election process in time. For example,
one of our subjects was clearly interested in the
pre-voting situation, such as debates between the
candidates, while another subject was interested
only in the outcome of the presidential election.
For the terrorist attack domain we also com-
pared the lists of questions we got from our sub-
jects with the terrorist attack template created by
experts for the MUC competition. In this template
we treated every slot as a separate question, ex-
cluding the first two slots which captured informa-
tion about the text from which the template fillers
were extracted and not about the domain. The re-
sults for this comparison are included in Table 2.
Differences in domain complexity were stud-
ied by IE researchers. Bagga (1997) suggests a
classification methodology to predict the syntac-
tic complexity of the domain-related facts. Hut-
tunen et al. (2002) analyze how component sub-
events of the domain are linked together and dis-
cuss the factors which contribute to the domain
complexity.
</bodyText>
<subsectionHeader confidence="0.999404">
6.2 Stage 2. Quality of the Automatically
Created Templates
</subsectionHeader>
<bodyText confidence="0.999895818181818">
In section 6.1 we showed that not all the domains
are equal. For some of the domains it is much eas-
ier to come to a consensus about what slots should
be present in the domain template than for others.
In this section we describe the evaluation of the
four automatically created templates.
Automatically created templates consist of slot
structures and are not easily readable by human
annotators. Thus, instead of direct evaluation of
the template quality, we evaluate the clauses ex-
tracted according to the created templates and
</bodyText>
<page confidence="0.995521">
212
</page>
<bodyText confidence="0.778943538461539">
check whether these clauses contain the answers
to the questions created by the subjects during the
first stage of the evaluation. We extract the clauses
corresponding to the test instances according to
the following procedure:
1. Identify all the simple clauses in the docu-
ments corresponding to a particular test in-
stance (respective TDT topic). For example,
for the sentence
Her husband, Robert, survived Thursday’s
explosion in a Yemeni harbor that killed at
least six crew members and injured 35.
only one part is output:
</bodyText>
<construct confidence="0.3157505">
that killed at least six crew members and
injured 35
</construct>
<listItem confidence="0.995876454545454">
2. For every domain template slot check all the
simple clauses in the instance (TDT topic)
under analysis. Find the shortest clause (or
sequence of clauses) which includes both the
verb and other words extracted for this slot in
their respective order. Add this clause to the
list of extracted clauses unless this clause has
been already added to this list.
3. Keep adding clauses to the list of extracted
clauses till all the template slots are analyzed
or the size of the list exceeds 20 clauses.
</listItem>
<bodyText confidence="0.999755076923077">
The key step in the above algorithm is Step 2. By
choosing the shortest simple clause or sequence
of simple clauses corresponding to a particular
template slot, we reduce the possibility of adding
more information to the output than is necessary
to cover each particular slot.
In Step 3 we keep only the first twenty clauses
so that the length of the output which potentially
contains an answer to the question of interest is not
larger than the number of questions provided by
each subject. The templates are created from the
slot structures extracted for the top 50 verbs. The
higher the estimated score of the verb (Eq. 1) for
the domain the closer to the top of the template the
slot structure corresponding to this verb will be.
We assume that the important information is more
likely to be covered by the slot structures that are
placed near the top of the template.
The evaluation results for the automatically cre-
ated templates are presented in Figure 1. We cal-
culate what average percentage of the questions is
covered by the outputs created according to the
domain templates. For every domain, we present
the percentage of the covered questions separately
for each annotator and for the intersection of ques-
tions (Section 6.1).
</bodyText>
<figureCaption confidence="0.99975">
Figure 1: Evaluation results.
</figureCaption>
<bodyText confidence="0.979899114285714">
For the questions common for all the annota-
tors we capture about 70% of the answers for
three out of four domains. After studying the re-
sults we noticed that for the earthquake domain
some questions did not result in a template slot
and thus, could not be covered by the extracted
clauses. Here are two of such questions:
Is it near a fault line?
Is it near volcanoes?
According to the template creation procedure,
which is centered around verbs, the chances that
extracted clauses would contain answers to these
questions are low. Indeed, only one of the three
sentence sets extracted for the three TDT earth-
quake topics contain an answer to one of these
questions.
Poor results for the presidential election domain
could be predicted from the Jaccard metric value
for interannotator agreement (Table 2). There is
considerable discrepancy in the questions created
by human annotators which can be attributed to the
great variation in the presidential election domain
itself. It must be also noted that most of the ques-
tions created for the presidential election domain
were clearly referring to the democratic election
procedure, while some of the TDT topics catego-
rized as Elections were about either election fraud
or about opposition taking over power without the
formal resignation of the previous president.
Overall, this evaluation shows that using au-
tomatically created domain templates we extract
sentences which contain a substantial part of the
important information expressed in questions for
that domain. For those domains which have small
diversity our coverage can be significantly higher.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9963275">
In this paper, we presented a robust method for
data-driven discovery of the important fact-types
</bodyText>
<figure confidence="0.99636475">
80.00%
60.00%
50.00%
40.00%
30.00%
20.00%
70.00%
10.00%
0.00%
Attack Earthquake Presidential
election
Plane crash
Intersect
Subj1
Subj2
Subj3
</figure>
<page confidence="0.998875">
213
</page>
<bodyText confidence="0.999967096774193">
for a given domain. In contrast to supervised meth-
ods, the fact-types are not pre-specified. The re-
sulting slot structures can subsequently be used
to guide the generation of responses to questions
about new instances of the same domain. Our ap-
proach features the use of corpus statistics derived
from both lexical and syntactic analysis across
documents. A comparison of our system output
for four domains of interest shows that our ap-
proach can reliably predict the majority of infor-
mation that humans have indicated are of interest.
Our method is flexible: analyzing document col-
lections from different time periods or locations,
we can learn domain descriptions that are tailored
to those time periods and locations.
Acknowledgements. We would like to thank Re-
becca Passonneau and Julia Hirschberg for the
fruitful discussions at the early stages of this work;
Vasilis Vassalos for his suggestions on the eval-
uation instructions; Michel Galley, Agustin Gra-
vano, Panagiotis Ipeirotis and Kapil Thadani for
their enormous help with evaluation.
This material is based upon work supported
in part by the Advanced Research Devel-
opment Agency (ARDA) under Contract No.
NBCHC040040 and in part by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023. Any opinions,
findings and conclusions expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of ARDA and DARPA.
</bodyText>
<sectionHeader confidence="0.999554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878204545455">
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura,
and Setsuo Arikawa. 2002. Optimized substructure dis-
covery for semi-structured data. In Proc. of PKDD.
Amit Bagga. 1997. Analyzing the complexity of a domain
with respect to an Information Extraction task. In Proc.
7th MUC.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of HLT/NAACL.
Daniel Bikel, Richard Schwartz, and Ralph Weischedel.
1999. An algorithm that learns what’s in a name. Ma-
chine Learning Journal Special Issue on Natural Lan-
guage Learning, 34:211–231.
Sasha Blair-Goldensohn, Kathleen McKeown, and An-
drew Hazen Schlaikjer, 2004. Answering Definitional
Questions: A Hybrid Approach. AAAI Press.
Robin Collier. 1998. Automatic Template Creation for Infor-
mation Extraction. Ph.D. thesis, University of Sheffield.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural language
generation. In Proc. of EMNLP.
Elena Filatova and Vasileios Hatzivassiloglou. 2003.
Domain-independent detection, extraction, and labeling of
atomic events. In Proc. of RANLP.
Elena Filatova and John Prager. 2005. Tell me what you do
and I’ll tell you what you are: Learning occupation-related
activities for biographies. In Proc. of EMNLP/HLT.
Charles Fillmore and Collin Baker. 2001. Frame semantics
for text understanding. In Proc. of WordNet and Other
Lexical Resources Workshop, NAACL.
Jon Fiscus, George Doddington, John Garofolo, and Alvin
Martin. 1999. NIST’s 1998 topic detection and tracking
evaluation (TDT2). In Proc. of the 1999 DARPA Broad-
cast News Workshop, pages 19–24.
Joseph Fleiss, Bruce Levin, and Myunghee Cho Paik, 1981.
Statistical Methods for Rates and Proportions. J. Wiley.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288.
Sanda Harabagiu and Finley L˘ac˘atus¸u. 2005. Topic themes
for multi-document summarization. In Proc. of SIGIR.
Sanda Harabagiu and Steven Maiorano. 2002. Multi-docu-
ment summarization with GISTexter. In Proc. of LREC.
Jerry Hobbs and David Israel. 1994. Principles of template
design. In Proc. of the HLT Workshop.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Complexity of event structure in IE scenarios. In
Proc. of COLING.
Dan Klein and Christopher Manning. 2002. Fast exact infer-
ence with a factored model for natural language parsing.
In Proc. of NIPS.
Hans Luhn. 1957. A statistical approach to mechanized en-
coding and searching of literary information. IBM Journal
of Research and Development, 1:309–317.
Elaine Marsh and Dennis Perzanowski. 1997. MUC-7 eval-
uation of IE technology: Overview of results. In Proc. of
the 7th MUC.
Boyan Onyshkevych. 1994. Issues and methodology for
template design for information extraction system. In
Proc. of the HLT Workshop.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71–106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-
tin, and Daniel Jurafsky. 2004. Shallow semantic parsing
using support vector machines. In Proc. of HLT/NAACL.
Dragomir Radev and Kathleen McKeown. 1998. Gener-
ating natural language summaries from multiple on-line
sources. Computational Linguistics, 24(3):469–500.
Ellen Riloff and Mark Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In Proc. of
the 6th Workshop on Very Large Corpora.
Gerard Salton, 1971. The SMART retrieval system. Prentice-
Hall, NJ.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003.
An improved extraction pattern representation model for
automatic IE pattern acquisition. In Proc. of ACL.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan, 2004. Hy-
brid Approach to Answering Biographical Questions.
AAAI Press.
Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng,
David Pierce, and Kiri Wagstaff. 2001. Multi-document
summarization via information extraction. In Proc. of
HLT.
Mohammed Zaki. 2002. Efficiently mining frequent trees in
a forest. In Proc. of SIGKDD.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004. Multi-
document biography summarization. In Proc. of EMNLP.
</reference>
<page confidence="0.998951">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975770">
<title confidence="0.99994">Automatic Creation of Domain Templates</title>
<author confidence="0.999426">Kathleen</author>
<affiliation confidence="0.9991215">of Computer Science Columbia University</affiliation>
<abstract confidence="0.998971181818182">Recently, many Natural Language Processing (NLP) applications have improved the quality of their output by using various machine learning techniques to mine Information Extraction (IE) patterns for capturing information from the input text. Currently, to mine IE patterns one should know in advance the type of the information that should be captured by these patterns. In this work we propose a novel methodology for corpus analysis based on cross-examination of several document collections representing different instances of the same domain. We show that this methodology can be used for automatic domain template creation. As the problem of automatic domain template creation is rather new, there is no well-defined procedure for the evaluation of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenji Abe</author>
</authors>
<title>Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and Setsuo Arikawa.</title>
<date>2002</date>
<booktitle>Proc. of PKDD.</booktitle>
<marker>Abe, 2002</marker>
<rawString>Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and Setsuo Arikawa. 2002. Optimized substructure discovery for semi-structured data. In Proc. of PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
</authors>
<title>Analyzing the complexity of a domain with respect to an Information Extraction task.</title>
<date>1997</date>
<booktitle>In Proc. 7th MUC.</booktitle>
<contexts>
<context position="27356" citStr="Bagga (1997)" startWordPosition="4421" endWordPosition="4422">dates, while another subject was interested only in the outcome of the presidential election. For the terrorist attack domain we also compared the lists of questions we got from our subjects with the terrorist attack template created by experts for the MUC competition. In this template we treated every slot as a separate question, excluding the first two slots which captured information about the text from which the template fillers were extracted and not about the domain. The results for this comparison are included in Table 2. Differences in domain complexity were studied by IE researchers. Bagga (1997) suggests a classification methodology to predict the syntactic complexity of the domain-related facts. Huttunen et al. (2002) analyze how component subevents of the domain are linked together and discuss the factors which contribute to the domain complexity. 6.2 Stage 2. Quality of the Automatically Created Templates In section 6.1 we showed that not all the domains are equal. For some of the domains it is much easier to come to a consensus about what slots should be present in the domain template than for others. In this section we describe the evaluation of the four automatically created te</context>
</contexts>
<marker>Bagga, 1997</marker>
<rawString>Amit Bagga. 1997. Analyzing the complexity of a domain with respect to an Information Extraction task. In Proc. 7th MUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="12815" citStr="Barzilay and Lee, 2003" startWordPosition="1992" endWordPosition="1995"> instances of airplane crashes, 3 instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrorist attacks). This variation in the number of documents per topic is typical for the TDT corpus. Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002;</context>
<context position="17800" citStr="Barzilay and Lee, 2003" startWordPosition="2806" endWordPosition="2810">st an instance of this domain. Thus, we substitute all the named entities with their respective tags, and all the exact numbers with the tag NUMBER. We speculate that subtrees similar to those presented in Table 1 can be extracted from a document collection representing any instance of a terrorist attack, with the only difference being the exact number of causalities. Later, however, we analyze the domain instances separately to identity information typical for the domain. The procedure of substituting named entities with their respective tags previously proved to be useful for various tasks (Barzilay and Lee, 2003; Sudo et al., 2003; Filatova and Prager, 2005). To get named entity tags we used BBN’s IdentiFinder (Bikel et al., 1999). Step 5: Merge together the frequent subtrees. Finally, we merge together those subtrees which are identical according to the information encoded within them. This is a key step in our algorithm which allows us to bring together subtrees from different instances of the same domain. For example, the information rendered by all the subtrees from the bottom part of Table 1 is identical. Thus, these subtrees can be merged into one which contains the longest common pattern: (VBD</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<journal>Machine Learning Journal Special Issue on Natural Language Learning,</journal>
<pages>34--211</pages>
<contexts>
<context position="17921" citStr="Bikel et al., 1999" startWordPosition="2828" endWordPosition="2831">ers with the tag NUMBER. We speculate that subtrees similar to those presented in Table 1 can be extracted from a document collection representing any instance of a terrorist attack, with the only difference being the exact number of causalities. Later, however, we analyze the domain instances separately to identity information typical for the domain. The procedure of substituting named entities with their respective tags previously proved to be useful for various tasks (Barzilay and Lee, 2003; Sudo et al., 2003; Filatova and Prager, 2005). To get named entity tags we used BBN’s IdentiFinder (Bikel et al., 1999). Step 5: Merge together the frequent subtrees. Finally, we merge together those subtrees which are identical according to the information encoded within them. This is a key step in our algorithm which allows us to bring together subtrees from different instances of the same domain. For example, the information rendered by all the subtrees from the bottom part of Table 1 is identical. Thus, these subtrees can be merged into one which contains the longest common pattern: (VBD killed)(NP(NUMBER)(NNS people)) After this merging procedure we keep only those subtrees for which each of the domain in</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel Bikel, Richard Schwartz, and Ralph Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning Journal Special Issue on Natural Language Learning, 34:211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kathleen McKeown</author>
<author>Andrew Hazen Schlaikjer</author>
</authors>
<title>Answering Definitional Questions: A Hybrid Approach.</title>
<date>2004</date>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1709" citStr="Blair-Goldensohn et al., 2004" startWordPosition="250" endWordPosition="253">. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist event in Latin America in the eighties is different from the description of today’s terrorist events. How does one determine what facts are i</context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Schlaikjer, 2004</marker>
<rawString>Sasha Blair-Goldensohn, Kathleen McKeown, and Andrew Hazen Schlaikjer, 2004. Answering Definitional Questions: A Hybrid Approach. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Collier</author>
</authors>
<title>Automatic Template Creation for Information Extraction.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sheffield.</institution>
<contexts>
<context position="8929" citStr="Collier (1998)" startWordPosition="1371" endWordPosition="1372"> to learn what information is important for a given domain or event and thus, should be included into the domain template. Our approach allows, for example, learning that an arrest along with other events (e.g., attack) is often part of a terrorist event. We do not assume any prior knowledge about domains. We demonstrate that frequent subtrees can be used not only to extract specific named entities for a given scenario but also to learn domain-important relations. These relations link domain actions and named entities as well as general nouns and words belonging to other syntactic categories. Collier (1998) proposed a fully automatic method for creating templates for information extraction. The method relies on Luhn’s (1957) idea of locating statistically significant words in a corpus and uses those to locate the sentences in which they occur. Then it extracts Subject-Verb-Object patterns in those sentences to identify the most important interactions in the input data. The system was constructed to create MUC templates for terrorist attacks. Our work also relies on corpus statistics, but we utilize arbitrary syntactic patterns and explicitly use multiple domain instances. Keeping domain instance</context>
<context position="12791" citStr="Collier, 1998" startWordPosition="1990" endWordPosition="1991"> clusters for 2 instances of airplane crashes, 3 instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrorist attacks). This variation in the number of documents per topic is typical for the TDT corpus. Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gi</context>
</contexts>
<marker>Collier, 1998</marker>
<rawString>Robin Collier. 1998. Automatic Template Creation for Information Extraction. Ph.D. thesis, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Duboue</author>
<author>Kathleen McKeown</author>
</authors>
<title>Statistical acquisition of content selection rules for natural language generation.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1527" citStr="Duboue and McKeown, 2003" startWordPosition="221" endWordPosition="224">s no well-defined procedure for the evaluation of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of </context>
</contexts>
<marker>Duboue, McKeown, 2003</marker>
<rawString>Pablo Duboue and Kathleen McKeown. 2003. Statistical acquisition of content selection rules for natural language generation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Domain-independent detection, extraction, and labeling of atomic events.</title>
<date>2003</date>
<booktitle>In Proc. of RANLP.</booktitle>
<contexts>
<context position="7409" citStr="Filatova and Hatzivassiloglou, 2003" startWordPosition="1117" endWordPosition="1120">n templates by cross-examining several collections of documents on the same topic, aiming for a general domain template. We rely on relations cross-mentioned in different instances of the domain to automatically prioritize roles and relationships for selection. Topic Themes (Harabagiu and L˘ac˘atus¸u, 2005) used for multi-document summarization merge various arguments corresponding to the same se2http://www.nist.gov/speech/tests/ace/index.htm mantic roles for the semantically identical verb phrases (e.g., arrests and placed under arrest). Atomic events also model an input document collection (Filatova and Hatzivassiloglou, 2003) and are created according to the statistics collected for co-occurrences of named entity pairs linked through actions. GISTexter, atomic events, and Topic Themes were used for modeling a collection of documents rather than a domain. In other closely related work, Sudo et al. (2003) use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain. The goal of their work is to show how the techniques improve IE pattern acquisition. To do this, Sudo et al. constrain the retrieval of relevant documents for a MUC scenario and then use u</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2003</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2003. Domain-independent detection, extraction, and labeling of atomic events. In Proc. of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>John Prager</author>
</authors>
<title>Tell me what you do and I’ll tell you what you are: Learning occupation-related activities for biographies.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP/HLT.</booktitle>
<contexts>
<context position="1599" citStr="Filatova and Prager, 2005" startWordPosition="233" endWordPosition="237">quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist event in Latin America in the e</context>
<context position="17847" citStr="Filatova and Prager, 2005" startWordPosition="2815" endWordPosition="2818">stitute all the named entities with their respective tags, and all the exact numbers with the tag NUMBER. We speculate that subtrees similar to those presented in Table 1 can be extracted from a document collection representing any instance of a terrorist attack, with the only difference being the exact number of causalities. Later, however, we analyze the domain instances separately to identity information typical for the domain. The procedure of substituting named entities with their respective tags previously proved to be useful for various tasks (Barzilay and Lee, 2003; Sudo et al., 2003; Filatova and Prager, 2005). To get named entity tags we used BBN’s IdentiFinder (Bikel et al., 1999). Step 5: Merge together the frequent subtrees. Finally, we merge together those subtrees which are identical according to the information encoded within them. This is a key step in our algorithm which allows us to bring together subtrees from different instances of the same domain. For example, the information rendered by all the subtrees from the bottom part of Table 1 is identical. Thus, these subtrees can be merged into one which contains the longest common pattern: (VBD killed)(NP(NUMBER)(NNS people)) After this mer</context>
</contexts>
<marker>Filatova, Prager, 2005</marker>
<rawString>Elena Filatova and John Prager. 2005. Tell me what you do and I’ll tell you what you are: Learning occupation-related activities for biographies. In Proc. of EMNLP/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
<author>Collin Baker</author>
</authors>
<title>Frame semantics for text understanding.</title>
<date>2001</date>
<booktitle>In Proc. of WordNet and Other Lexical Resources Workshop,</booktitle>
<location>NAACL.</location>
<contexts>
<context position="13387" citStr="Fillmore and Baker, 2001" startWordPosition="2084" endWordPosition="2087">alized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005; Palmer et al., 2005) we rely only on corpus statistics. We extract the most frequent syntactic subtrees which connect verbs to the lexemes used in the same subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document co</context>
</contexts>
<marker>Fillmore, Baker, 2001</marker>
<rawString>Charles Fillmore and Collin Baker. 2001. Frame semantics for text understanding. In Proc. of WordNet and Other Lexical Resources Workshop, NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Fiscus</author>
<author>George Doddington</author>
<author>John Garofolo</author>
<author>Alvin Martin</author>
</authors>
<title>NIST’s</title>
<date>1999</date>
<booktitle>In Proc. of the 1999 DARPA Broadcast News Workshop,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4463" citStr="Fiscus et al., 1999" startWordPosition="693" endWordPosition="696">nstances, we identify verbs important for the majority of instances and relationships containing these verbs. We generalize across multiple domain instances to automatically determine which of these relations should be used in the template. We report on data collection efforts and results from four domains. We assess how well the automatically produced templates satisfy users’ needs, as manifested by questions collected for these domains. 1Unfortunately, NLP terminology is not standardized across different tasks. Two NLP tasks most close to our research are Topic Detection and Tracking (TDT) (Fiscus et al., 1999) and Information Extraction (IE) (Marsh and Perzanowski, 1997). In TDT terminology, our domains are topics and our instances are events. In IE terminology, our domains are scenarios and our domain templates are scenario templates. 207 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207–214, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Related Work Our system automatically generates a template that captures the generally most important information for a particular domain and is reusable across multiple instances of that domain. Deciding what sl</context>
<context position="11892" citStr="Fiscus et al., 1999" startWordPosition="1843" endWordPosition="1846">ance. E.g., for the earthquake domain we collected documents on the earthquakes in Afghanistan (March 25, 2002), India (January 26, 2001), Iran (December 26, 2003), Japan (October 26, 2004), and Peru (June 23, 2001). Using this procedure we retrieve training document collections for 9 instances of airplane crashes, 5 instances of earthquakes, 13 instances of presidential elections, and 6 instances of terrorist attacks. 4.2 Test Data To test our system, we used document clusters from the Topic Detection and Tracking (TDT) cor3http://news.bbc.co.uk/shared/bsp/search2/ advanced/news_ifs.stm pus (Fiscus et al., 1999). Each TDT topic has a topic label, such as Accidents or Natural Disasters.4 These categories are broader than our domains. Thus, we manually filtered the TDT topics relevant to our four training domains (e.g., Accidents matching Airplane Crashes). In this way, we obtained TDT document clusters for 2 instances of airplane crashes, 3 instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrori</context>
</contexts>
<marker>Fiscus, Doddington, Garofolo, Martin, 1999</marker>
<rawString>Jon Fiscus, George Doddington, John Garofolo, and Alvin Martin. 1999. NIST’s 1998 topic detection and tracking evaluation (TDT2). In Proc. of the 1999 DARPA Broadcast News Workshop, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Fleiss</author>
<author>Bruce Levin</author>
<author>Myunghee Cho Paik</author>
</authors>
<date>1981</date>
<booktitle>Statistical Methods for Rates</booktitle>
<contexts>
<context position="25112" citStr="Fleiss et al., 1981" startWordPosition="4040" endWordPosition="4043">ion created by one of the annotators for the earthquake domain was: Did the earthquake occur in a well-known area for earthquakes (e.g. along the San Andreas fault), or in an unexpected location? We matched this question to the following three questions created by the other annotator: What is the geological localization? Is it near a fault line? Is it near volcanoes? Usually, the degree of interannotator agreement is estimated using Kappa. For this task, though, Kappa statistics cannot be used as they require knowledge of the expected or chance agreement, which is not applicable to this task (Fleiss et al., 1981). To measure interannotator agreement we use the Jaccard metric, which does not require knowledge of the expected or chance agreement. Table 2 shows the values of Jaccard metric for interannotator agreement calculated for all four domains. Jaccard metric values are calculated as Jaccard(domaind) = |QSd i ∩ QSdj |(3) |QSd i ∪ QSd j | where QSd and QSd are the sets of questions created by subjects i and j for domain d. For the airplane crash and presidential election domains we averaged the three pairwise Jaccard metric values. The scores in Table 2 show that for some domains the agreement is qu</context>
</contexts>
<marker>Fleiss, Levin, Paik, 1981</marker>
<rawString>Joseph Fleiss, Bruce Levin, and Myunghee Cho Paik, 1981. Statistical Methods for Rates and Proportions. J. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="13414" citStr="Gildea and Jurafsky, 2002" startWordPosition="2088" endWordPosition="2091">98; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005; Palmer et al., 2005) we rely only on corpus statistics. We extract the most frequent syntactic subtrees which connect verbs to the lexemes used in the same subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document collection for one domain — e</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley L˘ac˘atus¸u</author>
</authors>
<title>Topic themes for multi-document summarization.</title>
<date>2005</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<marker>Harabagiu, L˘ac˘atus¸u, 2005</marker>
<rawString>Sanda Harabagiu and Finley L˘ac˘atus¸u. 2005. Topic themes for multi-document summarization. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Steven Maiorano</author>
</authors>
<title>Multi-document summarization with GISTexter.</title>
<date>2002</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="6364" citStr="Harabagiu and Maiorano, 2002" startWordPosition="971" endWordPosition="974"> in information extraction, there has been little work on their automatic design. In the Conceptual Case Frame Acquisition project (Riloff and Schmelzenbach, 1998), extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions. The system requires two sets of documents: those relevant to the domain and those irrelevant. Our approach does not require any domain-specific knowledge and uses only corpus-based statistics. The GISTexter summarization system (Harabagiu and Maiorano, 2002) used statistics over an arbitrary document collection together with semantic relations from WordNet. The created templates heavily depend on the topical relations encoded in WordNet. The template models an input collection of documents. If there is only one domain instance described in the input than the template is created for this particular instance rather than for a domain. In our work, we learn domain templates by cross-examining several collections of documents on the same topic, aiming for a general domain template. We rely on relations cross-mentioned in different instances of the dom</context>
</contexts>
<marker>Harabagiu, Maiorano, 2002</marker>
<rawString>Sanda Harabagiu and Steven Maiorano. 2002. Multi-document summarization with GISTexter. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>David Israel</author>
</authors>
<title>Principles of template design.</title>
<date>1994</date>
<booktitle>In Proc. of the HLT Workshop.</booktitle>
<contexts>
<context position="5216" citStr="Hobbs and Israel, 1994" startWordPosition="804" endWordPosition="807">vents. In IE terminology, our domains are scenarios and our domain templates are scenario templates. 207 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207–214, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Related Work Our system automatically generates a template that captures the generally most important information for a particular domain and is reusable across multiple instances of that domain. Deciding what slots to include in the template, and what restrictions to place on their potential fillers, is a knowledge representation problem (Hobbs and Israel, 1994). Templates were used in the main IE competitions, the Message Understanding Conferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE,2 uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems. The domain templates were created manually by experts to capture the structure of the facts sought. Although templates have been extensively used in information extraction, there has been little work on their automatic design.</context>
</contexts>
<marker>Hobbs, Israel, 1994</marker>
<rawString>Jerry Hobbs and David Israel. 1994. Principles of template design. In Proc. of the HLT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silja Huttunen</author>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
</authors>
<title>Complexity of event structure in IE scenarios.</title>
<date>2002</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="27482" citStr="Huttunen et al. (2002)" startWordPosition="4437" endWordPosition="4441">ck domain we also compared the lists of questions we got from our subjects with the terrorist attack template created by experts for the MUC competition. In this template we treated every slot as a separate question, excluding the first two slots which captured information about the text from which the template fillers were extracted and not about the domain. The results for this comparison are included in Table 2. Differences in domain complexity were studied by IE researchers. Bagga (1997) suggests a classification methodology to predict the syntactic complexity of the domain-related facts. Huttunen et al. (2002) analyze how component subevents of the domain are linked together and discuss the factors which contribute to the domain complexity. 6.2 Stage 2. Quality of the Automatically Created Templates In section 6.1 we showed that not all the domains are equal. For some of the domains it is much easier to come to a consensus about what slots should be present in the domain template than for others. In this section we describe the evaluation of the four automatically created templates. Automatically created templates consist of slot structures and are not easily readable by human annotators. Thus, ins</context>
</contexts>
<marker>Huttunen, Yangarber, Grishman, 2002</marker>
<rawString>Silja Huttunen, Roman Yangarber, and Ralph Grishman. 2002. Complexity of event structure in IE scenarios. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2002</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="15170" citStr="Klein and Manning, 2002" startWordPosition="2380" endWordPosition="2383"># of all domain instances These verbs are estimated to be the most important for the combined document collection for all the domain instances. Thus, we build the domain template around these verbs. Here are the top ten verbs for the terrorist attack domain: killed, told, found, injured, reported, happened, blamed, arrested, died, linked. Step 2: Parse those sentences which contain the top 50 verbs. After we identify the 50 most important verbs for the domain under analysis, we parse all the sentences in the domain document collection containing these verbs with the Stanford syntactic parser (Klein and Manning, 2002). Step 3: Identify most frequent subtrees containing the top 50 verbs. A domain template should contain not only the most important actions for the domain, but also the entities that are linked to these actions or to each other through these actions. The lexemes referring to such entities can potentially be used within the domain template slots. Thus, we analyze those portions of the syntactic trees which contain the verbs themselves plus other lexemes used in the same subtrees as the verbs. To do this we use FREQuent Tree miner.5 This software is an implementation of the algorithm presented b</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher Manning. 2002. Fast exact inference with a factored model for natural language parsing. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Luhn</author>
</authors>
<title>A statistical approach to mechanized encoding and searching of literary information.</title>
<date>1957</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>1--309</pages>
<marker>Luhn, 1957</marker>
<rawString>Hans Luhn. 1957. A statistical approach to mechanized encoding and searching of literary information. IBM Journal of Research and Development, 1:309–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Marsh</author>
<author>Dennis Perzanowski</author>
</authors>
<title>MUC-7 evaluation of IE technology: Overview of results.</title>
<date>1997</date>
<booktitle>In Proc. of the 7th MUC.</booktitle>
<contexts>
<context position="4525" citStr="Marsh and Perzanowski, 1997" startWordPosition="701" endWordPosition="704"> of instances and relationships containing these verbs. We generalize across multiple domain instances to automatically determine which of these relations should be used in the template. We report on data collection efforts and results from four domains. We assess how well the automatically produced templates satisfy users’ needs, as manifested by questions collected for these domains. 1Unfortunately, NLP terminology is not standardized across different tasks. Two NLP tasks most close to our research are Topic Detection and Tracking (TDT) (Fiscus et al., 1999) and Information Extraction (IE) (Marsh and Perzanowski, 1997). In TDT terminology, our domains are topics and our instances are events. In IE terminology, our domains are scenarios and our domain templates are scenario templates. 207 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207–214, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Related Work Our system automatically generates a template that captures the generally most important information for a particular domain and is reusable across multiple instances of that domain. Deciding what slots to include in the template, and what restrictions to place</context>
</contexts>
<marker>Marsh, Perzanowski, 1997</marker>
<rawString>Elaine Marsh and Dennis Perzanowski. 1997. MUC-7 evaluation of IE technology: Overview of results. In Proc. of the 7th MUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boyan Onyshkevych</author>
</authors>
<title>Issues and methodology for template design for information extraction system.</title>
<date>1994</date>
<booktitle>In Proc. of the HLT Workshop.</booktitle>
<contexts>
<context position="5347" citStr="Onyshkevych, 1994" startWordPosition="825" endWordPosition="826">06 Main Conference Poster Sessions, pages 207–214, Sydney, July 2006. c�2006 Association for Computational Linguistics 2 Related Work Our system automatically generates a template that captures the generally most important information for a particular domain and is reusable across multiple instances of that domain. Deciding what slots to include in the template, and what restrictions to place on their potential fillers, is a knowledge representation problem (Hobbs and Israel, 1994). Templates were used in the main IE competitions, the Message Understanding Conferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE,2 uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems. The domain templates were created manually by experts to capture the structure of the facts sought. Although templates have been extensively used in information extraction, there has been little work on their automatic design. In the Conceptual Case Frame Acquisition project (Riloff and Schmelzenbach, 1998), extraction patterns, a domain semantic lexicon,</context>
</contexts>
<marker>Onyshkevych, 1994</marker>
<rawString>Boyan Onyshkevych. 1994. Issues and methodology for template design for information extraction system. In Proc. of the HLT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="13491" citStr="Palmer et al., 2005" startWordPosition="2101" endWordPosition="2104">cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005; Palmer et al., 2005) we rely only on corpus statistics. We extract the most frequent syntactic subtrees which connect verbs to the lexemes used in the same subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document collection for one domain — e.g., the collection containing all the instances in the domain of airplane cr</context>
<context position="19960" citStr="Palmer et al., 2005" startWordPosition="3173" endWordPosition="3176">election and earthquake domains, are shown below: (PERSON) won (NN earthquake) struck The above examples show that it is not enough to analyze only named entities, general nouns contain important information as well. We term the structure consisting of a verb together with the associated slots a slot structure. Here is a part of the slot structure we get for the verb killed after crossexamination of the terrorist attack instances: killed (NUMBER) (NNS people) (PERSON) killed (NN suicide) killed Slot structures are similar to verb frames, which are manually created for the PropBank annotation (Palmer et al., 2005).6 An example of the PropBank frame for the verb to kill is: Roleset kill.01 ”cause to die”: Arg0:killer Arg1:corpse Arg2:instrument The difference between the slot structure extracted by our algorithm and the PropBank frame slots is that the frame slots assign a semantic role to each slot, while our algorithm gives either the type of the named entity that should fill in this slot or puts a particular noun into the slot (e.g., ORGANIZATION, earthquake, people). An ideal domain template should include semantic information but this problem is outside of the scope of this paper. Step 6: Creating </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<contexts>
<context position="13436" citStr="Pradhan et al., 2004" startWordPosition="2092" endWordPosition="2096"> Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005; Palmer et al., 2005) we rely only on corpus statistics. We extract the most frequent syntactic subtrees which connect verbs to the lexemes used in the same subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document collection for one domain — e.g., the collection co</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Daniel Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Kathleen McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="1908" citStr="Radev and McKeown, 1998" startWordPosition="281" endWordPosition="284">ems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist event in Latin America in the eighties is different from the description of today’s terrorist events. How does one determine what facts are important for different kinds of responses? Often the types of facts that are important are hand en†Department of Computer Science The University of Texas at Dallas vh@hlt.utdallas.edu coded ahead of </context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir Radev and Kathleen McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Mark Schmelzenbach</author>
</authors>
<title>An empirical approach to conceptual case frame acquisition.</title>
<date>1998</date>
<booktitle>In Proc. of the 6th Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="5898" citStr="Riloff and Schmelzenbach, 1998" startWordPosition="904" endWordPosition="907">the Message Understanding Conferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE,2 uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems. The domain templates were created manually by experts to capture the structure of the facts sought. Although templates have been extensively used in information extraction, there has been little work on their automatic design. In the Conceptual Case Frame Acquisition project (Riloff and Schmelzenbach, 1998), extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions. The system requires two sets of documents: those relevant to the domain and those irrelevant. Our approach does not require any domain-specific knowledge and uses only corpus-based statistics. The GISTexter summarization system (Harabagiu and Maiorano, 2002) used statistics over an arbitrary document collection together with semantic relations from WordNet. The created templates heavily de</context>
</contexts>
<marker>Riloff, Schmelzenbach, 1998</marker>
<rawString>Ellen Riloff and Mark Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. In Proc. of the 6th Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>The SMART retrieval system.</title>
<date>1971</date>
<tech>PrenticeHall, NJ.</tech>
<contexts>
<context position="14238" citStr="Salton, 1971" startWordPosition="2229" endWordPosition="2230">e subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document collection for one domain — e.g., the collection containing all the instances in the domain of airplane crashes. We 4In our experiments we analyze TDT topics used in TDT-2 and TDT-4 evaluations. 209 discard those verbs that are stop words (Salton, 1971). To take into consideration the distribution of a verb among different instances of the domain, we normalize this probability by its VIF value (verb instance frequency), specifying in how many domain instances this verb appears. E vb; Ecomb coll countvb; # of domain instances containing vbi VIF(vbi) _ (2) # of all domain instances These verbs are estimated to be the most important for the combined document collection for all the domain instances. Thus, we build the domain template around these verbs. Here are the top ten verbs for the terrorist attack domain: killed, told, found, injured, rep</context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>Gerard Salton, 1971. The SMART retrieval system. PrenticeHall, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An improved extraction pattern representation model for automatic IE pattern acquisition.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7692" citStr="Sudo et al. (2003)" startWordPosition="1163" endWordPosition="1166">, 2005) used for multi-document summarization merge various arguments corresponding to the same se2http://www.nist.gov/speech/tests/ace/index.htm mantic roles for the semantically identical verb phrases (e.g., arrests and placed under arrest). Atomic events also model an input document collection (Filatova and Hatzivassiloglou, 2003) and are created according to the statistics collected for co-occurrences of named entity pairs linked through actions. GISTexter, atomic events, and Topic Themes were used for modeling a collection of documents rather than a domain. In other closely related work, Sudo et al. (2003) use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain. The goal of their work is to show how the techniques improve IE pattern acquisition. To do this, Sudo et al. constrain the retrieval of relevant documents for a MUC scenario and then use unsupervised learning over descriptions within these documents that match specific types of named entities (e.g., Arresting Agency, Charge), thus enabling learning of patterns for specific templates (e.g., the Arrest scenario). In contrast, the goal of our work is to show how similar</context>
<context position="12835" citStr="Sudo et al., 2003" startWordPosition="1996" endWordPosition="1999">rashes, 3 instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrorist attacks). This variation in the number of documents per topic is typical for the TDT corpus. Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 200</context>
<context position="15903" citStr="Sudo et al., 2003" startWordPosition="2505" endWordPosition="2508">e most important actions for the domain, but also the entities that are linked to these actions or to each other through these actions. The lexemes referring to such entities can potentially be used within the domain template slots. Thus, we analyze those portions of the syntactic trees which contain the verbs themselves plus other lexemes used in the same subtrees as the verbs. To do this we use FREQuent Tree miner.5 This software is an implementation of the algorithm presented by (Abe et al., 2002; Zaki, 2002), which extracts frequent ordered subtrees from a set of ordered trees. Following (Sudo et al., 2003) we are interested only in the lexemes which are near neighbors of the most frequent verbs. Thus, we look only for those subtrees which contain the verbs themselves and from four to ten tree nodes, where a node is either a syntactic tag or a lexeme with its tag. We analyze not only NPs which correspond to the subject or object of the verb, but other syntactic constituents as well. For example, PPs can potentially link the verb to locations or dates, and we want to include this information into the template. Table 1 contains a sample of subtrees for the terrorist attack domain mined from the se</context>
<context position="17819" citStr="Sudo et al., 2003" startWordPosition="2811" endWordPosition="2814">omain. Thus, we substitute all the named entities with their respective tags, and all the exact numbers with the tag NUMBER. We speculate that subtrees similar to those presented in Table 1 can be extracted from a document collection representing any instance of a terrorist attack, with the only difference being the exact number of causalities. Later, however, we analyze the domain instances separately to identity information typical for the domain. The procedure of substituting named entities with their respective tags previously proved to be useful for various tasks (Barzilay and Lee, 2003; Sudo et al., 2003; Filatova and Prager, 2005). To get named entity tags we used BBN’s IdentiFinder (Bikel et al., 1999). Step 5: Merge together the frequent subtrees. Finally, we merge together those subtrees which are identical according to the information encoded within them. This is a key step in our algorithm which allows us to bring together subtrees from different instances of the same domain. For example, the information rendered by all the subtrees from the bottom part of Table 1 is identical. Thus, these subtrees can be merged into one which contains the longest common pattern: (VBD killed)(NP(NUMBER)</context>
<context position="21437" citStr="Sudo et al. (2003)" startWordPosition="3418" endWordPosition="3421">for all the important verbs in the domain is called the domain template. From the created templates we remove the slots which are used in all the domains. For example, (PERSON) told. The presented algorithm can be used to create a template for any domain. It does not require predefined domain or world knowledge. We learn domain templates from cross-examining document collections describing different instances of the domain of interest. 6http://www.cs.rochester.edu/˜gildea/Verbs/ 6 Evaluation The task we deal with is new and there is no welldefined and standardized evaluation procedure for it. Sudo et al. (2003) evaluated how well their IE patterns captured named entities of three predefined types. We are interested in evaluating how well we capture the major actions as well as their constituent parts. There is no set of domain templates which are built according to a unique set of principles against which we could compare our automatically created templates. Thus, we need to create a gold standard. In Section 6.1, we describe how the gold standard is created. Then, in Section 6.2, we evaluate the quality of the automatically created templates by extracting clauses corresponding to the templates and </context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An improved extraction pattern representation model for automatic IE pattern acquisition. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Jinxi Xu</author>
<author>Ana Licuanan</author>
</authors>
<title>Hybrid Approach to Answering Biographical Questions.</title>
<date>2004</date>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1571" citStr="Weischedel et al., 2004" startWordPosition="229" endWordPosition="232">n of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist eve</context>
</contexts>
<marker>Weischedel, Xu, Licuanan, 2004</marker>
<rawString>Ralph Weischedel, Jinxi Xu, and Ana Licuanan, 2004. Hybrid Approach to Answering Biographical Questions. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Tanya Korelsky</author>
<author>Claire Cardie</author>
<author>Vincent Ng</author>
<author>David Pierce</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Multi-document summarization via information extraction.</title>
<date>2001</date>
<booktitle>In Proc. of HLT.</booktitle>
<contexts>
<context position="1929" citStr="White et al., 2001" startWordPosition="285" endWordPosition="288">esponse containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist event in Latin America in the eighties is different from the description of today’s terrorist events. How does one determine what facts are important for different kinds of responses? Often the types of facts that are important are hand en†Department of Computer Science The University of Texas at Dallas vh@hlt.utdallas.edu coded ahead of time by a human exper</context>
</contexts>
<marker>White, Korelsky, Cardie, Ng, Pierce, Wagstaff, 2001</marker>
<rawString>Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng, David Pierce, and Kiri Wagstaff. 2001. Multi-document summarization via information extraction. In Proc. of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Zaki</author>
</authors>
<title>Efficiently mining frequent trees in a forest.</title>
<date>2002</date>
<booktitle>In Proc. of SIGKDD.</booktitle>
<contexts>
<context position="15802" citStr="Zaki, 2002" startWordPosition="2491" endWordPosition="2492">st frequent subtrees containing the top 50 verbs. A domain template should contain not only the most important actions for the domain, but also the entities that are linked to these actions or to each other through these actions. The lexemes referring to such entities can potentially be used within the domain template slots. Thus, we analyze those portions of the syntactic trees which contain the verbs themselves plus other lexemes used in the same subtrees as the verbs. To do this we use FREQuent Tree miner.5 This software is an implementation of the algorithm presented by (Abe et al., 2002; Zaki, 2002), which extracts frequent ordered subtrees from a set of ordered trees. Following (Sudo et al., 2003) we are interested only in the lexemes which are near neighbors of the most frequent verbs. Thus, we look only for those subtrees which contain the verbs themselves and from four to ten tree nodes, where a node is either a syntactic tag or a lexeme with its tag. We analyze not only NPs which correspond to the subject or object of the verb, but other syntactic constituents as well. For example, PPs can potentially link the verb to locations or dates, and we want to include this information into </context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed Zaki. 2002. Efficiently mining frequent trees in a forest. In Proc. of SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Miruna Ticrea</author>
<author>Eduard Hovy</author>
</authors>
<title>Multidocument biography summarization.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1546" citStr="Zhou et al., 2004" startWordPosition="225" endWordPosition="228">e for the evaluation of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A descr</context>
</contexts>
<marker>Zhou, Ticrea, Hovy, 2004</marker>
<rawString>Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004. Multidocument biography summarization. In Proc. of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>