<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000686">
<title confidence="0.981437">
Down-stream effects of tree-to-dependency conversions
</title>
<author confidence="0.9974795">
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi†,
Hector Martinez, Anders Søgaard
</author>
<affiliation confidence="0.9991735">
Center for Language Technology, University of Copenhagen
†Institute for Informatics, University of Oslo
</affiliation>
<sectionHeader confidence="0.988104" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876625">
Dependency analysis relies on morphosyntac-
tic evidence, as well as semantic evidence.
In some cases, however, morphosyntactic ev-
idence seems to be in conflict with seman-
tic evidence. For this reason dependency
grammar theories, annotation guidelines and
tree-to-dependency conversion schemes often
differ in how they analyze various syntactic
constructions. Most experiments for which
constituent-based treebanks such as the Penn
Treebank are converted into dependency tree-
banks rely blindly on one of four-five widely
used tree-to-dependency conversion schemes.
This paper evaluates the down-stream effect of
choice of conversion scheme, showing that it
has dramatic impact on end results.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946307692308">
Annotation guidelines used in modern depen-
dency treebanks and tree-to-dependency conversion
schemes for converting constituent-based treebanks
into dependency treebanks are typically based on
a specific dependency grammar theory, such as the
Prague School’s Functional Generative Description,
Meaning-Text Theory, or Hudson’s Word Grammar.
In practice most parsers constrain dependency struc-
tures to be tree-like structures such that each word
has a single syntactic head, limiting diversity be-
tween annotation a bit; but while many dependency
treebanks taking this format agree on how to an-
alyze many syntactic constructions, there are still
many constructions these treebanks analyze differ-
ently. See Figure 1 for a standard overview of clear
and more difficult cases.
The difficult cases in Figure 1 are difficult for
the following reason. In the easy cases morphosyn-
tactic and semantic evidence cohere. Verbs gov-
ern subjects morpho-syntactically and seem seman-
tically more important. In the difficult cases, how-
ever, morpho-syntactic evidence is in conflict with
the semantic evidence. While auxiliary verbs have
the same distribution as finite verbs in head position
and share morpho-syntactic properties with them,
and govern the infinite main verbs, main verbs seem
semantically superior, expressing the main predi-
cate. There may be distributional evidence that com-
plementizers head verbs syntactically, but the verbs
seem more important from a semantic point of view.
Tree-to-dependency conversion schemes used
to convert constituent-based treebanks into
dependency-based ones also take different stands on
the difficult cases. In this paper we consider four dif-
ferent conversion schemes: the Yamada-Matsumoto
conversion scheme yamada,1 the CoNLL 2007
format conll07,2 the conversion scheme ewt used in
the English Web Treebank (Petrov and McDonald,
2012),3 and the lth conversion scheme (Johansson
</bodyText>
<footnote confidence="0.999224230769231">
1The Yamada-Matsumoto scheme can be
replicated by running penn2malt.jar available at
http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html. We
used Malt dependency labels (see website). The Yamada-
Matsumoto scheme is an elaboration of the Collins scheme
(Collins, 1999), which is not included in our experiments.
2The CoNLL 2007 conversion scheme can be
obtained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/with the
’conll07’ flag set.
3The EWT conversion scheme can be repli-
cated using the Stanford converter available at
http://nlp.stanford.edu/software/stanford-dependencies.shtml
</footnote>
<page confidence="0.893626">
617
</page>
<table confidence="0.845722111111111">
Proceedings of NAACL-HLT 2013, pages 617–626,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
Clear cases Difficult cases
Head Dependent ? ?
Verb Subject Auxiliary Main verb
Verb Object Complementizer Verb
Noun Attribute Coordinator Conjuncts
Verb Adverbial Preposition Nominal
Punctuation
</table>
<figureCaption confidence="0.9883145">
Figure 1: Clear and difficult cases in dependency annotation.
and Nugues, 2007).4 We list the differences in
Figure 2. An example of differences in analysis is
presented in Figure 3.
</figureCaption>
<bodyText confidence="0.999947111111111">
In order to access the impact of these conversion
schemes on down-stream performance, we need ex-
trinsic rather than intrinsic evaluation. In general
it is important to remember that while researchers
developing learning algorithms for part-of-speech
(POS) tagging and dependency parsing seem ob-
sessed with accuracies, POS sequences or depen-
dency structures have no interest on their own. The
accuracies reported in the literature are only inter-
esting insofar they correlate with the usefulness of
the structures predicted by our systems. Fortunately,
POS sequences and dependency structures are use-
ful in many applications. When we consider tree-to-
dependency conversion schemes, down-stream eval-
uation becomes particularly important since some
schemes are more fine-grained than others, leading
to lower performance as measured by intrinsic eval-
uation metrics.
</bodyText>
<subsectionHeader confidence="0.58018">
Approach in this work
</subsectionHeader>
<bodyText confidence="0.9999586">
In our experiments below we apply a state-of-the-art
parser to five different natural language processing
(NLP) tasks where syntactic features are known to
be effective: negation resolution, semantic role la-
beling (SRL), statistical machine translation (SMT),
sentence compression and perspective classification.
In all five tasks we use the four tree-to-dependency
conversion schemes mentioned above and evaluate
them in terms of down-stream performance. We also
compare our systems to baseline systems not rely-
</bodyText>
<footnote confidence="0.99618575">
4The LTH conversion scheme can be ob-
tained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/ with the
’oldLTH’ flag set.
</footnote>
<bodyText confidence="0.999923434782609">
ing on syntactic features, when possible, and to re-
sults in the literature, when comparable results exist.
Note that negation resolution and SRL are not end
applications. It is not easy to generalize across five
very different tasks, but the tasks will serve to show
that the choice of conversion scheme has significant
impact on down-stream performance.
We used the most recent release of the Mate parser
first described in Bohnet (2010),5 trained on Sec-
tions 2–21 of the Wall Street Journal section of the
English Treebank (Marcus et al., 1993). The graph-
based parser is similar to, except much faster, and
performs slightly better than the MSTParser (Mc-
Donald et al., 2005), which is known to perform
well on long-distance dependencies often important
for down-stream applications (McDonald and Nivre,
2007; Galley and Manning, 2009; Bender et al.,
2011). This choice may of course have an effect on
what conversion schemes seem superior (Johansson
and Nugues, 2007). Sentence splitting was done us-
ing splitta,6, and the sentences were then tokenized
using PTB-style tokenization7 and tagged using the
in-built Mate POS tagger.
</bodyText>
<subsectionHeader confidence="0.926361">
Previous work
</subsectionHeader>
<bodyText confidence="0.999728555555555">
There has been considerable work on down-stream
evaluation of syntactic parsers in the literature, but
most previous work has focused on evaluating pars-
ing models rather than linguistic theories. No one
has, to the best of our knowledge, compared the
impact of choice of tree-to-dependency conversion
scheme across several NLP tasks.
Johansson and Nugues (2007) compare the im-
pact of yamada and lth on semantic role labeling
</bodyText>
<footnote confidence="0.999959666666667">
5http://code.google.com/p/mate-tools/
6http://code.google.com/p/splitta/
7http://www.cis.upenn.edu/—treebank/tokenizer.sed
</footnote>
<page confidence="0.983257">
618
</page>
<table confidence="0.998846">
FORM1 FORM2 yamada conll07 ewt lth
Auxiliary Main verb 1 1 2 2
Complementizer Verb 1 2 2 2
Coordinator Conjuncts 2 1 2 2
Preposition Nominal 1 1 1 2
</table>
<figureCaption confidence="0.999326">
Figure 2: Head decisions in conversions. Note: yamada also differ from CoNLL 2007 in proper names.
Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions.
</figureCaption>
<bodyText confidence="0.999785372093023">
performance, showing that lth leads to superior per-
formance.
Miyao et al. (2008) measure the impact of syntac-
tic parsers in an information extraction system iden-
tifying protein-protein interactions in biomedical re-
search articles. They evaluate dependency parsers,
constituent-based parsers and deep parsers.
Miwa et al. (2010) evaluate down-stream per-
formance of linguistic representations and parsing
models in biomedical event extraction, but do not
evaluate linguistic representations directly, evaluat-
ing representations and models jointly.
Bender et al. (2011) compare several parsers
across linguistic representations on a carefully de-
signed evaluation set of hard, but relatively frequent
syntactic constructions. They compare dependency
parsers, constituent-based parsers and deep parsers.
The authors argue in favor of evaluating parsers on
diverse and richly annotated data. Others have dis-
cussed various ways of evaluating across annotation
guidelines or translating structures to a common for-
mat (Schwartz et al., 2011; Tsarfaty et al., 2012).
Hall et al. (2011) discuss optimizing parsers for
specific down-stream applications, but consider only
a single annotation scheme.
Yuret et al. (2012) present an overview of the
SemEval-2010 Evaluation Exercises on Semantic
Evaluation track on recognition textual entailment
using dependency parsing. They also compare sev-
eral parsers using the heuristics of the winning sys-
tem for inference. While the shared task is an
example of down-stream evaluation of dependency
parsers, the evaluation examples only cover a subset
of the textual entailments relevant for practical ap-
plications, and the heuristics used in the experiments
assume a fixed set of dependency labels (ewt labels).
Finally, Schwartz et al. (2012) compare the
above conversion schemes and several combinations
thereof in terms of learnability. This is very different
from what is done here. While learnability may be
a theoretically motivated parameter, our results indi-
cate that learnability and downstream performance
do not correlate well.
</bodyText>
<sectionHeader confidence="0.981133" genericHeader="introduction">
2 Applications
</sectionHeader>
<bodyText confidence="0.999888111111111">
Dependency parsing has proven useful for a wide
range of NLP applications, including statistical ma-
chine translation (Galley and Manning, 2009; Xu et
al., 2009; Elming and Haulrich, 2011) and sentiment
analysis (Joshi and Penstein-Rose, 2009; Johansson
and Moschitti, 2010). This section describes the ap-
plications and experimental set-ups included in this
study.
In the five applications considered below we
</bodyText>
<page confidence="0.995434">
619
</page>
<bodyText confidence="0.999962133333334">
use syntactic features in slightly different ways.
While our statistical machine translation and sen-
tence compression systems use dependency rela-
tions as additional information about words and on
a par with POS, our negation resolution system uses
dependency paths, conditioning decisions on both
dependency arcs and labels. In perspective classifi-
cation, we use dependency triples (e.g. SLTBJ(John,
snore)) as features, while the semantic role labeling
system conditions on a lot of information, including
the word form of the head, the dependent and the ar-
gument candidates, the concatenation of the depen-
dency labels of the predicate, and the labeled depen-
dency relations between predicate and its head, its
arguments, dependents or siblings.
</bodyText>
<subsectionHeader confidence="0.985134">
2.1 Negation resolution
</subsectionHeader>
<bodyText confidence="0.999623466666667">
Negation resolution (NR) is the task of finding nega-
tion cues, e.g. the word not, and determining their
scope, i.e. the tokens they affect. NR has recently
seen considerable interest in the NLP community
(Morante and Sporleder, 2012; Velldal et al., 2012)
and was the topic of the 2012 *SEM shared task
(Morante and Blanco, 2012).
The data set used in this work, the Conan Doyle
corpus (CD),8 was released in conjunction with the
*SEM shared task. The annotations in CD extend
on cues and scopes by introducing annotations for
in-scope events that are negated in factual contexts.
The following is an example from the corpus show-
ing the annotations for cues (bold), scopes (under-
lined) and negated events (italicized):
</bodyText>
<listItem confidence="0.655044">
(1) Since we have been so
</listItem>
<bodyText confidence="0.991937153846154">
unfortunate as to miss him [... ]
CD-style scopes can be discontinuous and overlap-
ping. Events are a portion of the scope that is se-
mantically negated, with its truth value reversed by
the negation cue.
The NR system used in this work (Lapponi et al.,
2012), one of the best performing systems in the
*SEM shared task, is a CRF model for scope resolu-
tion that relies heavily on features extracted from de-
pendency graphs. The feature model contains token
distance, direction, n-grams of word forms, lemmas,
POS and combinations thereof, as well as the syntac-
tic features presented in Figure 4. The results in our
</bodyText>
<figure confidence="0.882534818181818">
8http://www.clips.ua.ac.be/sem2012-st-neg/data.html
constituent
dependency relation
parent head POS
grand parent head POS
word form+dependency relation
POS+dependency relation
directed dependency distance
bidirectional dependency distance
dependency path
lexicalized dependency path
</figure>
<figureCaption confidence="0.9952475">
Figure 4: Features used to train the conditional random
field models
</figureCaption>
<bodyText confidence="0.999973583333333">
experiments are obtained from configurations that
differ only in terms of tree-to-dependency conver-
sions, and are trained on the training set and tested
on the development set of CD. Since the negation
cue classification component of the system does not
rely on dependency features at all, the models are
tested using gold cues.
Table 1 shows F1 scores for scopes, events and
full negations, where a true positive correctly as-
signs both scope tokens and events to the rightful
cue. The scores are produced using the evaluation
script provided by the *SEM organizers.
</bodyText>
<subsectionHeader confidence="0.997532">
2.2 Semantic role labeling
</subsectionHeader>
<bodyText confidence="0.999963894736842">
Semantic role labeling (SRL) is the attempt to de-
termine semantic predicates in running text and la-
bel their arguments with semantic roles. In our
experiments we have reproduced the second best-
performing system in the CoNLL 2008 shared task
in syntactic and semantic parsing (Johansson and
Nugues, 2008).9
The English training data for the CoNLL 2008
shared task were obtained from PropBank and
NomBank. For licensing reasons, we used
OntoNotes 4.0, which includes PropBank, but not
NomBank. This means that our system is only
trained to classify verbal predicates. We used
the Clearparser conversion tool10 to convert the
OntoNotes 4.0 and subsequently supplied syntac-
tic dependency trees using our different conversion
schemes. We rely on gold standard argument identi-
fication and focus solely on the performance metric
semantic labeled F1.
</bodyText>
<footnote confidence="0.976099333333333">
9http://nlp.cs.lth.se/software/semantic parsing: propbank
nombank frames
10http://code.google.com/p/clearparser/
</footnote>
<figure confidence="0.7462185">
Syntactic
Cue-dependent
</figure>
<page confidence="0.915703">
620
</page>
<subsectionHeader confidence="0.991681">
2.3 Statistical machine translation
</subsectionHeader>
<bodyText confidence="0.999986333333334">
The effect of the different conversion schemes was
also evaluated on SMT. We used the reordering
by parsing framework described by Elming and
Haulrich (2011). This approach integrates a syn-
tactically informed reordering model into a phrase-
based SMT system. The model learns to predict the
word order of the translation based on source sen-
tence information such as syntactic dependency re-
lations. Syntax-informed SMT is known to be use-
ful for translating between languages with different
word orders (Galley and Manning, 2009; Xu et al.,
2009), e.g. English and German.
The baseline SMT system is created as described
in the guidelines from the original shared task.11
Only modifications are that we use truecasing in-
stead of lowercasing and recasing, and allow train-
ing sentences of up to 80 words. We used data
from the English-German restricted task: —3M par-
allel words of news, —46M parallel words of Eu-
roparl, and —309M words of monolingual Europarl
and news. We use newstest2008 for tuning, new-
stest2009 for development, and newstest2010 for
testing. Distortion limit was set to 10, which is
also where the baseline system performed best. The
phrase table and the lexical reordering model is
trained on the union of all parallel data with a max
phrase length of 7, and the 5-gram language model
is trained on the entire monolingual data set.
We test four different experimental systems that
only differ with the baseline in the addition of a syn-
tactically informed reordering model. The baseline
system was one of the tied best performing system
in the WMT 2011 shared task on this dataset. The
four experimental systems have reordering models
that are trained on the first 25,000 sentences of the
parallel news data that have been parsed with each
of the tree-to-dependency conversion schemes. The
reordering models condition reordering on the word
forms, POS, and syntactic dependency relations of
the words to be reordered, as described in Elming
and Haulrich (2011). The paper shows that while
reordering by parsing leads to significant improve-
ments in standard metrics such as BLEU (Papineni
et al., 2002) and METEOR (Lavie and Agarwal,
2007), improvements are more spelled out with hu-
</bodyText>
<footnote confidence="0.344757">
11 http://www.statmt.org/wmt11/translation-task.html
</footnote>
<bodyText confidence="0.996112666666667">
man judgements. All SMT results reported below
are averages based on 5 MERT runs following Clark
et al. (2011).
</bodyText>
<subsectionHeader confidence="0.994018">
2.4 Sentence compression
</subsectionHeader>
<bodyText confidence="0.999995074074074">
Sentence compression is a restricted form of sen-
tence simplification with numerous usages, includ-
ing text simplification, summarization and recogniz-
ing textual entailment. The most commonly used
dataset in the literature is the Ziff-Davis corpus.12 A
widely used baseline for sentence compression ex-
periments is Knight and Marcu (2002), who intro-
duce two models: the noisy-channel model and a de-
cision tree-based model. Both are tree-based meth-
ods that find the most likely compressed syntactic
tree and outputs the yield of this tree. McDonald et
al. (2006) instead use syntactic features to directly
find the most likely compressed sentence.
Here we learn a discriminative HMM model
(Collins, 2002) of sentence compression using
MIRA (Crammer and Singer, 2003), comparable to
previously explored models of noun phrase chunk-
ing. Our model is thus neither tree-based nor
sentence-based. Instead we think of sentence com-
pression as a sequence labeling problem. We com-
pare a model informed by word forms and predicted
POS with models also informed by predicted depen-
dency labels. The baseline feature model conditions
emission probabilities on word forms and POS us-
ing a ±2 window and combinations thereoff. The
augmented syntactic feature model simply adds de-
pendency labels within the same window.
</bodyText>
<subsectionHeader confidence="0.979286">
2.5 Perspective classification
</subsectionHeader>
<bodyText confidence="0.999912916666667">
Finally, we include a document classification dataset
from Lin and Hauptmann (2006).13 The dataset con-
sists of blog posts posted at bitterlemons.org by Is-
raelis and Palestinians. The bitterlemons.org web-
site is set up to ”contribute to mutual understanding
through the open exchange of ideas.” In the dataset,
each blog post is labeled as either Israeli or Pales-
tinian. Our baseline model is just a standard bag-
of-words model, and the system adds dependency
triplets to the bag-of-words model in a way similar
to Joshi and Penstein-Rose (2009). We do not re-
move stop words, since perspective classification is
</bodyText>
<footnote confidence="0.977602">
12LDC Catalog No.: LDC93T3A.
13https://sites.google.com/site/weihaolinatcmu/data
</footnote>
<page confidence="0.993531">
621
</page>
<table confidence="0.999902058823529">
bl yamada conll07 ewt lth
DEPRELS - 12 21 47 41
PTB-23 (LAS) - 88.99 88.52 81.36* 87.52
PTB-23 (UAS) - 90.21 90.12 84.22* 90.29
Neg: scope F1 - 81.27 80.43 78.70 79.57
Neg: event F1 - 76.19 72.90 73.15 76.24
Neg: full negation F1 - 67.94 63.24 61.60 64.31
SentComp F1 68.47 72.07 64.29 71.56 71.56
SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08
SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51
SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06
SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11
SRL-22-gold - 81.35 83.22 84.72 84.01
SRL-23-gold - 79.09 80.85 80.39 82.01
SRL-22-pred - 74.41 76.22 78.29 66.32
SRL-23-pred - 73.42 74.34 75.80 64.06
bitterlemons.org 96.08 97.06 95.58 96.08 96.57
</table>
<tableCaption confidence="0.9966125">
Table 1: Results. *: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and the
Ontonotes 4.0 release of the English Treebank.
</tableCaption>
<bodyText confidence="0.998939">
similar to authorship attribution, where stop words
are known to be informative. We evaluate perfor-
mance doing cross-validation over the official train-
ing data, setting the parameters of our learning algo-
rithm for each fold doing cross-validation over the
actual training data. We used soft-margin support
vector machine learning (Cortes and Vapnik, 1995),
tuning the kernel (linear or polynomial with degree
3) and C = 10.1,1, 5,10}.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="related work">
3 Results and discussion
</sectionHeader>
<bodyText confidence="0.999991066666667">
Our results are presented in Table 1. The parsing
results are obtained relying on predicted POS rather
than, as often done in the dependency parsing liter-
ature, relying on gold-standard POS. Note that they
comply with the result in Schwartz et al. (2012) that
Yamada-Matsumoto-style annotation is more easily
learnable.
The negation resolution results are significantly
better using syntactic features in yamada annota-
tion. It is not surprising that a syntactically ori-
ented conversion scheme performs well in this task.
Since Lapponi et al. (2012) used Maltparser (Nivre
et al., 2007) with the freely available pre-trained
parsing model for English,14 we decided to also
run that parser with the gold-standard cues, in ad-
</bodyText>
<footnote confidence="0.690193">
14http://www.maltparser.org/mco/english parser/engmalt.html
</footnote>
<bodyText confidence="0.999831923076923">
dition to Mate. The pre-trained model was trained
on Sections 2–21 of the Wall Street Journal sec-
tion of the English Treebank (Marcus et al., 1993),
augmented with 4000 sentences from the Question-
Bank,15 which was converted using the Stanford
converter and thus similar to the ewt annotations
used here. The results were better than using ewt
with Mate trained on Sections 2–21 alone, but worse
than the results obtained here with yamada conver-
sion scheme. Fl score on full negation was 66.92%.
The case-sensitive BLEU evaluation of the
SMT systems indicates that choice of conversion
scheme has no significant impact on overall perfor-
mance. The difference to the baseline system is
significant (p &lt; 0.01), showing that the reorder-
ing model leads to improvement using any of the
schemes. However, the conversion schemes lead to
very different translations. This can be seen, for
example, by the fact that the relative tree edit dis-
tance between translations of different syntactically
informed SMT systems is 12% higher than within
each system (across different MERT optimizations).
The reordering approach puts a lot of weight on
the syntactic dependency relations. As a conse-
quence, the number of relation types used in the
conversion schemes proves important. Consider the
</bodyText>
<footnote confidence="0.847875">
15http://www.computing.dcu.ie/-jjudge/qtreebank/
</footnote>
<page confidence="0.990438">
622
</page>
<table confidence="0.931969571428572">
REFERENCE: Zum Gl¨uck kam ich beim Strassenbahnfahren an die richtige Stelle .
SOURCE: Luckily , on the way to the tram , I found the right place .
yamada: Gl¨ucklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest, dass der richtige Ort.
conll07: Gl¨ucklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest, dass der richtige Ort.
ewt: Zum Gl¨uck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
lth: Zum Gl¨uck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
BASELINE: Zum Gl¨uck hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .
</table>
<figureCaption confidence="0.986231">
Figure 5: Examples of SMT output.
</figureCaption>
<bodyText confidence="0.851039142857143">
ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answering
machine and voice-message handler that links a macintosh to touch-tone phones .
BASELINE: 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler.
yamada 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler.
conll07 68000 sweden ab sweden introduced the teleserve integrated answering
machine and voice-message handler.
ewt 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler.
lth 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler.
HUMAN: 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler.
</bodyText>
<figureCaption confidence="0.998931">
Figure 6: Examples of sentence compression output.
</figureCaption>
<bodyText confidence="0.999986916666667">
example in Figure 5. German requires the verb in
second position, which is obeyed in the much bet-
ter translations produced by the ewt and lth sys-
tems. Interestingly, the four schemes produce virtu-
ally identical structures for the source sentence, but
they differ in their labeling. Where conll07 and ya-
mada use the same relation for the first two con-
stituents (ADV and vMOD, respectively), ewt and
lth distinguish between them (ADVMOD/PREP and
ADV/LOC). This distinction may be what enables
the better translation, since the model may learn to
move the verb after the sentence adverbial. In the
other schemes, sentence adverbials are not distin-
guished from locational adverbials. Generally, ewt
and lth have more than twice as many relation types
as the other schemes.
The schemes ewt and lth lead to better SRL
performance than conll07 and yamada when re-
lying on gold-standard syntactic dependency trees.
This supports the claims put forward in Johansson
and Nugues (2007). These annotations also hap-
pen to use a larger set of dependency labels, how-
ever, and syntactic structures may be harder to re-
construct, as reflected by labeled attachment scores
(LAS) in syntactic parsing. The biggest drop in
SRL performance going from gold-standard to pre-
dicted syntactic trees is clearly for the lth scheme,
at an average 17.8% absolute loss (yamada 5.8%;
conll07 6.8%; ewt 5.5%; lth 17.8%).
The ewt scheme resembles lth in most respects,
but in preposition-noun dependencies it marks the
preposition as the head rather than the noun. This
is an important difference for SRL, because seman-
tic arguments are often nouns embedded in preposi-
tional phrases, like agents in passive constructions.
It may also be that the difference in performance is
simply explained by the syntactic analysis of prepo-
sitional phrases being easier to reconstruct.
The sentence compression results are generally
much better than the models proposed in Knight and
Marcu (2002). Their noisy channel model obtains
an F1 compression score of 14.58%, whereas the
decision tree-based model obtains an F1 compres-
sion score of 31.71%. While F1 scores should be
complemented by human judgements, as there are
typically many good sentence compressions of any
source sentence, we believe that error reductions of
more than 50% indicate that the models used here
</bodyText>
<page confidence="0.995903">
623
</page>
<figure confidence="0.998782833333333">
La
srl
neg
0.30
0.25
0.20
0.15
0.10
0.05
0.00ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD
0.40
0.35
</figure>
<figureCaption confidence="0.9555445">
Figure 7: Distributions of dependency labels in the
Yamada-Matsumoto scheme
</figureCaption>
<bodyText confidence="0.9987766">
(though previously unexplored in the literature) are
fully competitive with state-of-the-art models.
We also see that the models using syntactic fea-
tures perform better than our baseline model, except
for the model using conll07 dependency annotation.
This may be surprising to some, since distributional
information is often considered important in sen-
tence compression (Knight and Marcu, 2002). Some
output examples are presented in Figure 6. Un-
surprisingly, it is seen that the baseline model pro-
duces grammatically incorrect output, and that most
of our syntactic models correct the error leading to
ungrammaticality. The model using ewt annotation
is an exception. We also see that conll07 introduces
another error. We believe that this is due to the way
the conll07 tree-to-dependency conversion scheme
handles coordination. While the word Sweden is not
coordinated, it occurs in a context, surrounded by
commas, that is very similar to coordinated items.
In perspective classification we see that syntactic
features based on yamada and lth annotations lead
to improvements, with yamada leading to slightly
better results than lth. The fact that a syntactically
oriented conversion scheme leads to the best results
may reflect that perspective classification, like au-
thorship attribution, is less about content than stylis-
tics.
While lth seems to lead to the overall best re-
sults, we stress the fact that the five tasks considered
here are incommensurable. What is more interest-
ing is that, task to task, results are so different. The
semantically oriented conversion schemes, ewt and
lth, lead to the best results in SRL, but with a signif-
icant drop for lth when relying on predicted parses,
while the yamada scheme is competitive in the other
four tasks. This may be because distributional infor-
mation is more important in these tasks than in SRL.
The distribution of dependency labels seems rel-
atively stable across applications, but differences in
data may of course also affect the usefulness of dif-
ferent annotations. Note that conll07 leads to very
good results for negation resolution, but bad results
for SRL. See Figure 7 for the distribution of labels
in the conll07 conversion scheme on the SRL and
negation scope resolution data. Many differences
relate to differences in sentence length. The nega-
tion resolution data is literary text with shorter sen-
tences, which therefore uses more punctuation and
has more root dependencies than newspaper articles.
On the other hand we do see very few predicate de-
pendencies in the SRL data. This may affect down-
stream results when classifying verbal predicates in
SRL. We also note that the number of dependency
labels have less impact on results in general than we
would have expected. The number of dependency
labels and the lack of support for some of them may
explain the drop with predicted syntactic parses in
our SRL results, but generally we obtain our best re-
sults with yamada and lth annotations, which have
12 and 41 dependency labels, respectively.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999818071428572">
We evaluated four different tree-to-dependency con-
version schemes, putting more or less emphasis on
syntactic or semantic evidence, in five down-stream
applications, including SMT and negation resolu-
tion. Our results show why it is important to be
precise about exactly what tree-to-dependency con-
version scheme is used. Tools like pennconverter.jar
gives us a wide range of options when converting
constituent-based treebanks, and even small differ-
ences may have significant impact on down-stream
performance. The small differences are also impor-
tant for more linguistic comparisons that also tend to
gloss over exactly what conversion scheme is used,
e.g. Ivanova et al. (2012).
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.993192">
Hector Martinez is funded by the ERC grant
CLARA No. 238405, and Anders Søgaard is
funded by the ERC Starting Grant LOWLANDS
No. 313695.
</bodyText>
<page confidence="0.998377">
624
</page>
<sectionHeader confidence="0.989973" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99986360952381">
Emily Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local dependencies in a large corpus. In EMILP.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In COLIIG.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL.
Mike Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMILP.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273–297.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive algorithms for multiclass problems. In JMLR.
Jakob Elming and Martin Haulrich. 2011. Reordering
by parsing. In Proceedings ofInternational Workshop
on Using Linguistic Information for Hybrid Machine
Translation (LIHMT-2011).
Michel Galley and Christopher Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
EMILP.
Angelina Ivanova, Stephan Oepen, Lilja Øvrelid, and
Dan Flickinger. 2012. Who did what to whom? a con-
trastive study of syntactico-semantic dependencies. In
LAW.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In CoILL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
IODALIDA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoILL.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91–107.
Emanuele Lapponi, Erik Velldal, Lilja Øvrelid, and
Jonathon Read. 2012. UiO2: Sequence-labeling nega-
tion using dependency features. In *SEM.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspectives?
In COLIIG-ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313–330.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsers. In
EMILP-CoILL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Iatural Language Processing
2005, pages 523–530, Vancouver, British Columbia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun’ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In COLIIG.
Yusuke Miyao, Rune Sæ tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In *SEM.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational linguistics, 38(2):223–260.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: a language-
independent system for data-driven dependency pars-
ing. Iatural Language Engineering, 13(2):95–135.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Iotes
of the First Workshop on Syntactic Analysis of Ion-
Canonical Language (SAICL).
Roy Schwartz, and Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency pars-
ing evaluation. In ACL.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLIIG.
</reference>
<page confidence="0.985081">
625
</page>
<reference confidence="0.999627666666667">
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In EACL.
Erik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of synta. Computational linguis-
tics, 38(2):369–410.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In IAACL-
HLT, Boulder, Colorado.
Deniz Yuret, Laura Rimell, and Aydin Han. 2012. Parser
evaluation using textual entailments. Language Re-
sources and Evaluation, Published online 31 October
2012.
</reference>
<page confidence="0.998815">
626
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.176719">
<title confidence="0.779184">Down-stream effects of tree-to-dependency conversions</title>
<author confidence="0.3916705">Anders Johannsen Elming</author>
<author confidence="0.3916705">Sigrid Klerke</author>
<author confidence="0.3916705">Emanuele Hector Martinez</author>
<author confidence="0.3916705">Anders</author>
<affiliation confidence="0.5312575">Center for Language Technology, University of for Informatics, University of Oslo</affiliation>
<abstract confidence="0.998170470588235">Dependency analysis relies on morphosyntactic evidence, as well as semantic evidence. In some cases, however, morphosyntactic evidence seems to be in conflict with semantic evidence. For this reason dependency grammar theories, annotation guidelines and tree-to-dependency conversion schemes often differ in how they analyze various syntactic constructions. Most experiments for which constituent-based treebanks such as the Penn Treebank are converted into dependency treebanks rely blindly on one of four-five widely used tree-to-dependency conversion schemes. This paper evaluates the down-stream effect of choice of conversion scheme, showing that it has dramatic impact on end results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Emily Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
<author>Yi Zhang</author>
</authors>
<title>Parser evaluation over local and nonlocal dependencies in a large corpus.</title>
<date>2011</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="6433" citStr="Bender et al., 2011" startWordPosition="907" endWordPosition="910"> the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6, and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency conversion scheme acro</context>
<context position="8153" citStr="Bender et al. (2011)" startWordPosition="1157" endWordPosition="1160">NLL 2007 (blue) and LTH (red) dependency conversions. performance, showing that lth leads to superior performance. Miyao et al. (2008) measure the impact of syntactic parsers in an information extraction system identifying protein-protein interactions in biomedical research articles. They evaluate dependency parsers, constituent-based parsers and deep parsers. Miwa et al. (2010) evaluate down-stream performance of linguistic representations and parsing models in biomedical event extraction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a sing</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Zhang, 2011</marker>
<rawString>Emily Bender, Dan Flickinger, Stephan Oepen, and Yi Zhang. 2011. Parser evaluation over local and nonlocal dependencies in a large corpus. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="6014" citStr="Bohnet (2010)" startWordPosition="842" endWordPosition="843">ems not rely4The LTH conversion scheme can be obtained by running pennconverter.jar available at http://nlp.cs.lth.se/software/treebank converter/ with the ’oldLTH’ flag set. ing on syntactic features, when possible, and to results in the literature, when comparable results exist. Note that negation resolution and SRL are not end applications. It is not easy to generalize across five very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6, and the sentences were</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="16533" citStr="Clark et al. (2011)" startWordPosition="2447" endWordPosition="2450">rsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with hu11 http://www.statmt.org/wmt11/translation-task.html man judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based methods that find the most likely compressed syntactic tree and outputs the yield of this tree. McDonald et al. (2006) instead use sy</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3144" citStr="Collins, 1999" startWordPosition="435" endWordPosition="436">nto dependency-based ones also take different stands on the difficult cases. In this paper we consider four different conversion schemes: the Yamada-Matsumoto conversion scheme yamada,1 the CoNLL 2007 format conll07,2 the conversion scheme ewt used in the English Web Treebank (Petrov and McDonald, 2012),3 and the lth conversion scheme (Johansson 1The Yamada-Matsumoto scheme can be replicated by running penn2malt.jar available at http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html. We used Malt dependency labels (see website). The YamadaMatsumoto scheme is an elaboration of the Collins scheme (Collins, 1999), which is not included in our experiments. 2The CoNLL 2007 conversion scheme can be obtained by running pennconverter.jar available at http://nlp.cs.lth.se/software/treebank converter/with the ’conll07’ flag set. 3The EWT conversion scheme can be replicated using the Stanford converter available at http://nlp.stanford.edu/software/stanford-dependencies.shtml 617 Proceedings of NAACL-HLT 2013, pages 617–626, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics Clear cases Difficult cases Head Dependent ? ? Verb Subject Auxiliary Main verb Verb Object Complementize</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Mike Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models.</title>
<date>2002</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="17260" citStr="Collins, 2002" startWordPosition="2560" endWordPosition="2561">ges, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based methods that find the most likely compressed syntactic tree and outputs the yield of this tree. McDonald et al. (2006) instead use syntactic features to directly find the most likely compressed sentence. Here we learn a discriminative HMM model (Collins, 2002) of sentence compression using MIRA (Crammer and Singer, 2003), comparable to previously explored models of noun phrase chunking. Our model is thus neither tree-based nor sentence-based. Instead we think of sentence compression as a sequence labeling problem. We compare a model informed by word forms and predicted POS with models also informed by predicted dependency labels. The baseline feature model conditions emission probabilities on word forms and POS using a ±2 window and combinations thereoff. The augmented syntactic feature model simply adds dependency labels within the same window. 2.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="19775" citStr="Cortes and Vapnik, 1995" startWordPosition="2951" endWordPosition="2954">4.41 76.22 78.29 66.32 SRL-23-pred - 73.42 74.34 75.80 64.06 bitterlemons.org 96.08 97.06 95.58 96.08 96.57 Table 1: Results. *: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and the Ontonotes 4.0 release of the English Treebank. similar to authorship attribution, where stop words are known to be informative. We evaluate performance doing cross-validation over the official training data, setting the parameters of our learning algorithm for each fold doing cross-validation over the actual training data. We used soft-margin support vector machine learning (Cortes and Vapnik, 1995), tuning the kernel (linear or polynomial with degree 3) and C = 10.1,1, 5,10}. 3 Results and discussion Our results are presented in Table 1. The parsing results are obtained relying on predicted POS rather than, as often done in the dependency parsing literature, relying on gold-standard POS. Note that they comply with the result in Schwartz et al. (2012) that Yamada-Matsumoto-style annotation is more easily learnable. The negation resolution results are significantly better using syntactic features in yamada annotation. It is not surprising that a syntactically oriented conversion scheme pe</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine Learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative algorithms for multiclass problems.</title>
<date>2003</date>
<booktitle>In JMLR.</booktitle>
<contexts>
<context position="17322" citStr="Crammer and Singer, 2003" startWordPosition="2567" endWordPosition="2570">nd recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based methods that find the most likely compressed syntactic tree and outputs the yield of this tree. McDonald et al. (2006) instead use syntactic features to directly find the most likely compressed sentence. Here we learn a discriminative HMM model (Collins, 2002) of sentence compression using MIRA (Crammer and Singer, 2003), comparable to previously explored models of noun phrase chunking. Our model is thus neither tree-based nor sentence-based. Instead we think of sentence compression as a sequence labeling problem. We compare a model informed by word forms and predicted POS with models also informed by predicted dependency labels. The baseline feature model conditions emission probabilities on word forms and POS using a ±2 window and combinations thereoff. The augmented syntactic feature model simply adds dependency labels within the same window. 2.5 Perspective classification Finally, we include a document cl</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative algorithms for multiclass problems. In JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Martin Haulrich</author>
</authors>
<title>Reordering by parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofInternational Workshop on Using Linguistic Information for Hybrid Machine Translation (LIHMT-2011).</booktitle>
<contexts>
<context position="9856" citStr="Elming and Haulrich, 2011" startWordPosition="1408" endWordPosition="1411">and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we 619 use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classification, we use dependency </context>
<context position="14332" citStr="Elming and Haulrich (2011)" startWordPosition="2092" endWordPosition="2095">al predicates. We used the Clearparser conversion tool10 to convert the OntoNotes 4.0 and subsequently supplied syntactic dependency trees using our different conversion schemes. We rely on gold standard argument identification and focus solely on the performance metric semantic labeled F1. 9http://nlp.cs.lth.se/software/semantic parsing: propbank nombank frames 10http://code.google.com/p/clearparser/ Syntactic Cue-dependent 620 2.3 Statistical machine translation The effect of the different conversion schemes was also evaluated on SMT. We used the reordering by parsing framework described by Elming and Haulrich (2011). This approach integrates a syntactically informed reordering model into a phrasebased SMT system. The model learns to predict the word order of the translation based on source sentence information such as syntactic dependency relations. Syntax-informed SMT is known to be useful for translating between languages with different word orders (Galley and Manning, 2009; Xu et al., 2009), e.g. English and German. The baseline SMT system is created as described in the guidelines from the original shared task.11 Only modifications are that we use truecasing instead of lowercasing and recasing, and al</context>
<context position="16149" citStr="Elming and Haulrich (2011)" startWordPosition="2390" endWordPosition="2393">e test four different experimental systems that only differ with the baseline in the addition of a syntactically informed reordering model. The baseline system was one of the tied best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with hu11 http://www.statmt.org/wmt11/translation-task.html man judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly</context>
</contexts>
<marker>Elming, Haulrich, 2011</marker>
<rawString>Jakob Elming and Martin Haulrich. 2011. Reordering by parsing. In Proceedings ofInternational Workshop on Using Linguistic Information for Hybrid Machine Translation (LIHMT-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher Manning</author>
</authors>
<title>Quadratic-time dependency parsing for machine translation.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6411" citStr="Galley and Manning, 2009" startWordPosition="903" endWordPosition="906"> very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6, and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency </context>
<context position="9811" citStr="Galley and Manning, 2009" startWordPosition="1400" endWordPosition="1403">ments relevant for practical applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we 619 use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In p</context>
<context position="14699" citStr="Galley and Manning, 2009" startWordPosition="2150" endWordPosition="2153">tp://code.google.com/p/clearparser/ Syntactic Cue-dependent 620 2.3 Statistical machine translation The effect of the different conversion schemes was also evaluated on SMT. We used the reordering by parsing framework described by Elming and Haulrich (2011). This approach integrates a syntactically informed reordering model into a phrasebased SMT system. The model learns to predict the word order of the translation based on source sentence information such as syntactic dependency relations. Syntax-informed SMT is known to be useful for translating between languages with different word orders (Galley and Manning, 2009; Xu et al., 2009), e.g. English and German. The baseline SMT system is created as described in the guidelines from the original shared task.11 Only modifications are that we use truecasing instead of lowercasing and recasing, and allow training sentences of up to 80 words. We used data from the English-German restricted task: —3M parallel words of news, —46M parallel words of Europarl, and —309M words of monolingual Europarl and news. We use newstest2008 for tuning, newstest2009 for development, and newstest2010 for testing. Distortion limit was set to 10, which is also where the baseline sys</context>
</contexts>
<marker>Galley, Manning, 2009</marker>
<rawString>Michel Galley and Christopher Manning. 2009. Quadratic-time dependency parsing for machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Ryan McDonald</author>
<author>Jason Katz-Brown</author>
<author>Michael Ringgaard</author>
</authors>
<title>Training dependency parsers by jointly optimizing multiple objectives.</title>
<date>2011</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="8662" citStr="Hall et al. (2011)" startWordPosition="1231" endWordPosition="1234">uate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heuristics used in the e</context>
</contexts>
<marker>Hall, McDonald, Katz-Brown, Ringgaard, 2011</marker>
<rawString>Keith Hall, Ryan McDonald, Jason Katz-Brown, and Michael Ringgaard. 2011. Training dependency parsers by jointly optimizing multiple objectives. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelina Ivanova</author>
<author>Stephan Oepen</author>
<author>Lilja Øvrelid</author>
<author>Dan Flickinger</author>
</authors>
<title>Who did what to whom? a contrastive study of syntactico-semantic dependencies.</title>
<date>2012</date>
<publisher>In LAW.</publisher>
<marker>Ivanova, Oepen, Øvrelid, Flickinger, 2012</marker>
<rawString>Angelina Ivanova, Stephan Oepen, Lilja Øvrelid, and Dan Flickinger. 2012. Who did what to whom? a contrastive study of syntactico-semantic dependencies. In LAW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Syntactic and semantic structure for opinion expression detection.</title>
<date>2010</date>
<booktitle>In CoILL.</booktitle>
<contexts>
<context position="9942" citStr="Johansson and Moschitti, 2010" startWordPosition="1419" endWordPosition="1422"> (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we 619 use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classification, we use dependency triples (e.g. SLTBJ(John, snore)) as features, while the semantic role labeling system</context>
</contexts>
<marker>Johansson, Moschitti, 2010</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2010. Syntactic and semantic structure for opinion expression detection. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In IODALIDA.</booktitle>
<contexts>
<context position="6545" citStr="Johansson and Nugues, 2007" startWordPosition="925" endWordPosition="928">performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6, and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency conversion scheme across several NLP tasks. Johansson and Nugues (2007) compare the impact of yamada and lth on semantic role labeling</context>
<context position="24429" citStr="Johansson and Nugues (2007)" startWordPosition="3679" endWordPosition="3682"> constituents (ADV and vMOD, respectively), ewt and lth distinguish between them (ADVMOD/PREP and ADV/LOC). This distinction may be what enables the better translation, since the model may learn to move the verb after the sentence adverbial. In the other schemes, sentence adverbials are not distinguished from locational adverbials. Generally, ewt and lth have more than twice as many relation types as the other schemes. The schemes ewt and lth lead to better SRL performance than conll07 and yamada when relying on gold-standard syntactic dependency trees. This supports the claims put forward in Johansson and Nugues (2007). These annotations also happen to use a larger set of dependency labels, however, and syntactic structures may be harder to reconstruct, as reflected by labeled attachment scores (LAS) in syntactic parsing. The biggest drop in SRL performance going from gold-standard to predicted syntactic trees is clearly for the lth scheme, at an average 17.8% absolute loss (yamada 5.8%; conll07 6.8%; ewt 5.5%; lth 17.8%). The ewt scheme resembles lth in most respects, but in preposition-noun dependencies it marks the preposition as the head rather than the noun. This is an important difference for SRL, bec</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In IODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic-semantic analysis with propbank and nombank.</title>
<date>2008</date>
<booktitle>In CoILL.</booktitle>
<contexts>
<context position="13458" citStr="Johansson and Nugues, 2008" startWordPosition="1972" endWordPosition="1975">cy features at all, the models are tested using gold cues. Table 1 shows F1 scores for scopes, events and full negations, where a true positive correctly assigns both scope tokens and events to the rightful cue. The scores are produced using the evaluation script provided by the *SEM organizers. 2.2 Semantic role labeling Semantic role labeling (SRL) is the attempt to determine semantic predicates in running text and label their arguments with semantic roles. In our experiments we have reproduced the second bestperforming system in the CoNLL 2008 shared task in syntactic and semantic parsing (Johansson and Nugues, 2008).9 The English training data for the CoNLL 2008 shared task were obtained from PropBank and NomBank. For licensing reasons, we used OntoNotes 4.0, which includes PropBank, but not NomBank. This means that our system is only trained to classify verbal predicates. We used the Clearparser conversion tool10 to convert the OntoNotes 4.0 and subsequently supplied syntactic dependency trees using our different conversion schemes. We rely on gold standard argument identification and focus solely on the performance metric semantic labeled F1. 9http://nlp.cs.lth.se/software/semantic parsing: propbank no</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic-semantic analysis with propbank and nombank. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Carolyn Penstein-Rose</author>
</authors>
<title>Generalizing dependency features for opinion mining.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9910" citStr="Joshi and Penstein-Rose, 2009" startWordPosition="1415" endWordPosition="1418"> fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we 619 use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classification, we use dependency triples (e.g. SLTBJ(John, snore)) as features, while t</context>
<context position="18432" citStr="Joshi and Penstein-Rose (2009)" startWordPosition="2743" endWordPosition="2746">l simply adds dependency labels within the same window. 2.5 Perspective classification Finally, we include a document classification dataset from Lin and Hauptmann (2006).13 The dataset consists of blog posts posted at bitterlemons.org by Israelis and Palestinians. The bitterlemons.org website is set up to ”contribute to mutual understanding through the open exchange of ideas.” In the dataset, each blog post is labeled as either Israeli or Palestinian. Our baseline model is just a standard bagof-words model, and the system adds dependency triplets to the bag-of-words model in a way similar to Joshi and Penstein-Rose (2009). We do not remove stop words, since perspective classification is 12LDC Catalog No.: LDC93T3A. 13https://sites.google.com/site/weihaolinatcmu/data 621 bl yamada conll07 ewt lth DEPRELS - 12 21 47 41 PTB-23 (LAS) - 88.99 88.52 81.36* 87.52 PTB-23 (UAS) - 90.21 90.12 84.22* 90.29 Neg: scope F1 - 81.27 80.43 78.70 79.57 Neg: event F1 - 76.19 72.90 73.15 76.24 Neg: full negation F1 - 67.94 63.24 61.60 64.31 SentComp F1 68.47 72.07 64.29 71.56 71.56 SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08 SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51 SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06 SMT-test-BLEU 14</context>
</contexts>
<marker>Joshi, Penstein-Rose, 2009</marker>
<rawString>Mahesh Joshi and Carolyn Penstein-Rose. 2009. Generalizing dependency features for opinion mining. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<pages>139--91</pages>
<contexts>
<context position="16895" citStr="Knight and Marcu (2002)" startWordPosition="2499" endWordPosition="2502">neni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with hu11 http://www.statmt.org/wmt11/translation-task.html man judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based methods that find the most likely compressed syntactic tree and outputs the yield of this tree. McDonald et al. (2006) instead use syntactic features to directly find the most likely compressed sentence. Here we learn a discriminative HMM model (Collins, 2002) of sentence compression using MIRA (Crammer and Singer, 2003), comparable to previously explored models of noun phrase chunking. Our model is thus neither tree-based nor sentence-based. Instead we think of sentence compression as a se</context>
<context position="25402" citStr="Knight and Marcu (2002)" startWordPosition="3835" endWordPosition="3838">bsolute loss (yamada 5.8%; conll07 6.8%; ewt 5.5%; lth 17.8%). The ewt scheme resembles lth in most respects, but in preposition-noun dependencies it marks the preposition as the head rather than the noun. This is an important difference for SRL, because semantic arguments are often nouns embedded in prepositional phrases, like agents in passive constructions. It may also be that the difference in performance is simply explained by the syntactic analysis of prepositional phrases being easier to reconstruct. The sentence compression results are generally much better than the models proposed in Knight and Marcu (2002). Their noisy channel model obtains an F1 compression score of 14.58%, whereas the decision tree-based model obtains an F1 compression score of 31.71%. While F1 scores should be complemented by human judgements, as there are typically many good sentence compressions of any source sentence, we believe that error reductions of more than 50% indicate that the models used here 623 La srl neg 0.30 0.25 0.20 0.15 0.10 0.05 0.00ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD 0.40 0.35 Figure 7: Distributions of dependency labels in the Yamada-Matsumoto scheme (thoug</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence, 139:91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuele Lapponi</author>
<author>Erik Velldal</author>
<author>Lilja Øvrelid</author>
<author>Jonathon Read</author>
</authors>
<title>UiO2: Sequence-labeling negation using dependency features.</title>
<date>2012</date>
<booktitle>In *SEM.</booktitle>
<contexts>
<context position="11854" citStr="Lapponi et al., 2012" startWordPosition="1728" endWordPosition="1731">rpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [... ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed by the negation cue. The NR system used in this work (Lapponi et al., 2012), one of the best performing systems in the *SEM shared task, is a CRF model for scope resolution that relies heavily on features extracted from dependency graphs. The feature model contains token distance, direction, n-grams of word forms, lemmas, POS and combinations thereof, as well as the syntactic features presented in Figure 4. The results in our 8http://www.clips.ua.ac.be/sem2012-st-neg/data.html constituent dependency relation parent head POS grand parent head POS word form+dependency relation POS+dependency relation directed dependency distance bidirectional dependency distance depend</context>
<context position="20428" citStr="Lapponi et al. (2012)" startWordPosition="3054" endWordPosition="3057">olynomial with degree 3) and C = 10.1,1, 5,10}. 3 Results and discussion Our results are presented in Table 1. The parsing results are obtained relying on predicted POS rather than, as often done in the dependency parsing literature, relying on gold-standard POS. Note that they comply with the result in Schwartz et al. (2012) that Yamada-Matsumoto-style annotation is more easily learnable. The negation resolution results are significantly better using syntactic features in yamada annotation. It is not surprising that a syntactically oriented conversion scheme performs well in this task. Since Lapponi et al. (2012) used Maltparser (Nivre et al., 2007) with the freely available pre-trained parsing model for English,14 we decided to also run that parser with the gold-standard cues, in ad14http://www.maltparser.org/mco/english parser/engmalt.html dition to Mate. The pre-trained model was trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993), augmented with 4000 sentences from the QuestionBank,15 which was converted using the Stanford converter and thus similar to the ewt annotations used here. The results were better than using ewt with Mate trained on Se</context>
</contexts>
<marker>Lapponi, Velldal, Øvrelid, Read, 2012</marker>
<rawString>Emanuele Lapponi, Erik Velldal, Lilja Øvrelid, and Jonathon Read. 2012. UiO2: Sequence-labeling negation using dependency features. In *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="16327" citStr="Lavie and Agarwal, 2007" startWordPosition="2419" endWordPosition="2422">best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with hu11 http://www.statmt.org/wmt11/translation-task.html man judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the </context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Are these documents written from different perspectives?</title>
<date>2006</date>
<booktitle>In COLIIG-ACL.</booktitle>
<contexts>
<context position="17972" citStr="Lin and Hauptmann (2006)" startWordPosition="2667" endWordPosition="2670"> explored models of noun phrase chunking. Our model is thus neither tree-based nor sentence-based. Instead we think of sentence compression as a sequence labeling problem. We compare a model informed by word forms and predicted POS with models also informed by predicted dependency labels. The baseline feature model conditions emission probabilities on word forms and POS using a ±2 window and combinations thereoff. The augmented syntactic feature model simply adds dependency labels within the same window. 2.5 Perspective classification Finally, we include a document classification dataset from Lin and Hauptmann (2006).13 The dataset consists of blog posts posted at bitterlemons.org by Israelis and Palestinians. The bitterlemons.org website is set up to ”contribute to mutual understanding through the open exchange of ideas.” In the dataset, each blog post is labeled as either Israeli or Palestinian. Our baseline model is just a standard bagof-words model, and the system adds dependency triplets to the bag-of-words model in a way similar to Joshi and Penstein-Rose (2009). We do not remove stop words, since perspective classification is 12LDC Catalog No.: LDC93T3A. 13https://sites.google.com/site/weihaolinatc</context>
</contexts>
<marker>Lin, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin and Alexander Hauptmann. 2006. Are these documents written from different perspectives? In COLIIG-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="6122" citStr="Marcus et al., 1993" startWordPosition="859" endWordPosition="862">://nlp.cs.lth.se/software/treebank converter/ with the ’oldLTH’ flag set. ing on syntactic features, when possible, and to results in the literature, when comparable results exist. Note that negation resolution and SRL are not end applications. It is not easy to generalize across five very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6, and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work T</context>
<context position="20809" citStr="Marcus et al., 1993" startWordPosition="3110" endWordPosition="3113">learnable. The negation resolution results are significantly better using syntactic features in yamada annotation. It is not surprising that a syntactically oriented conversion scheme performs well in this task. Since Lapponi et al. (2012) used Maltparser (Nivre et al., 2007) with the freely available pre-trained parsing model for English,14 we decided to also run that parser with the gold-standard cues, in ad14http://www.maltparser.org/mco/english parser/engmalt.html dition to Mate. The pre-trained model was trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993), augmented with 4000 sentences from the QuestionBank,15 which was converted using the Stanford converter and thus similar to the ewt annotations used here. The results were better than using ewt with Mate trained on Sections 2–21 alone, but worse than the results obtained here with yamada conversion scheme. Fl score on full negation was 66.92%. The case-sensitive BLEU evaluation of the SMT systems indicates that choice of conversion scheme has no significant impact on overall performance. The difference to the baseline system is significant (p &lt; 0.01), showing that the reordering model leads </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell Marcus, Mary Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsers.</title>
<date>2007</date>
<booktitle>In EMILP-CoILL.</booktitle>
<contexts>
<context position="6385" citStr="McDonald and Nivre, 2007" startWordPosition="899" endWordPosition="902"> to generalize across five very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6, and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of cho</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsers. In EMILP-CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Iatural Language Processing</booktitle>
<pages>523--530</pages>
<location>Vancouver, British Columbia.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Iatural Language Processing 2005, pages 523–530, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic evidence.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Sampo Pyysalo</author>
<author>Tadayoshi Hara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluating dependency representation for event extraction.</title>
<date>2010</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="7914" citStr="Miwa et al. (2010)" startWordPosition="1126" endWordPosition="1129">l07 ewt lth Auxiliary Main verb 1 1 2 2 Complementizer Verb 1 2 2 2 Coordinator Conjuncts 2 1 2 2 Preposition Nominal 1 1 1 2 Figure 2: Head decisions in conversions. Note: yamada also differ from CoNLL 2007 in proper names. Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions. performance, showing that lth leads to superior performance. Miyao et al. (2008) measure the impact of syntactic parsers in an information extraction system identifying protein-protein interactions in biomedical research articles. They evaluate dependency parsers, constituent-based parsers and deep parsers. Miwa et al. (2010) evaluate down-stream performance of linguistic representations and parsing models in biomedical event extraction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of eva</context>
</contexts>
<marker>Miwa, Pyysalo, Hara, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and Jun’ichi Tsujii. 2010. Evaluating dependency representation for event extraction. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
</authors>
<title>Rune Sæ tre, Kenji Sagae, Takuya Matsuzaki, and Jun’ichi Tsujii.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<marker>Miyao, 2008</marker>
<rawString>Yusuke Miyao, Rune Sæ tre, Kenji Sagae, Takuya Matsuzaki, and Jun’ichi Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Eduardo Blanco</author>
</authors>
<title>sem 2012 shared task: Resolving the scope and focus of negation.</title>
<date>2012</date>
<booktitle>In *SEM.</booktitle>
<contexts>
<context position="11181" citStr="Morante and Blanco, 2012" startWordPosition="1611" endWordPosition="1614"> a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [... ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed by</context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>Roser Morante and Eduardo Blanco. 2012. *sem 2012 shared task: Resolving the scope and focus of negation. In *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Caroline Sporleder</author>
</authors>
<title>Modality and negation: An introduction to the special issue.</title>
<date>2012</date>
<journal>Computational linguistics,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="11084" citStr="Morante and Sporleder, 2012" startWordPosition="1593" endWordPosition="1596">riples (e.g. SLTBJ(John, snore)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [... ] CD-style scopes can be discontinuous and overlapping.</context>
</contexts>
<marker>Morante, Sporleder, 2012</marker>
<rawString>Roser Morante and Caroline Sporleder. 2012. Modality and negation: An introduction to the special issue. Computational linguistics, 38(2):223–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Iatural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="20465" citStr="Nivre et al., 2007" startWordPosition="3060" endWordPosition="3063">, 5,10}. 3 Results and discussion Our results are presented in Table 1. The parsing results are obtained relying on predicted POS rather than, as often done in the dependency parsing literature, relying on gold-standard POS. Note that they comply with the result in Schwartz et al. (2012) that Yamada-Matsumoto-style annotation is more easily learnable. The negation resolution results are significantly better using syntactic features in yamada annotation. It is not surprising that a syntactically oriented conversion scheme performs well in this task. Since Lapponi et al. (2012) used Maltparser (Nivre et al., 2007) with the freely available pre-trained parsing model for English,14 we decided to also run that parser with the gold-standard cues, in ad14http://www.maltparser.org/mco/english parser/engmalt.html dition to Mate. The pre-trained model was trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993), augmented with 4000 sentences from the QuestionBank,15 which was converted using the Stanford converter and thus similar to the ewt annotations used here. The results were better than using ewt with Mate trained on Sections 2–21 alone, but worse than the</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: a languageindependent system for data-driven dependency parsing. Iatural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="16290" citStr="Papineni et al., 2002" startWordPosition="2413" endWordPosition="2416">aseline system was one of the tied best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with hu11 http://www.statmt.org/wmt11/translation-task.html man judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<date>2012</date>
<booktitle>Overview of the 2012 Shared Task on Parsing the Web. In Iotes of the First Workshop on Syntactic Analysis of IonCanonical Language (SAICL).</booktitle>
<contexts>
<context position="2834" citStr="Petrov and McDonald, 2012" startWordPosition="395" endWordPosition="398">nfinite main verbs, main verbs seem semantically superior, expressing the main predicate. There may be distributional evidence that complementizers head verbs syntactically, but the verbs seem more important from a semantic point of view. Tree-to-dependency conversion schemes used to convert constituent-based treebanks into dependency-based ones also take different stands on the difficult cases. In this paper we consider four different conversion schemes: the Yamada-Matsumoto conversion scheme yamada,1 the CoNLL 2007 format conll07,2 the conversion scheme ewt used in the English Web Treebank (Petrov and McDonald, 2012),3 and the lth conversion scheme (Johansson 1The Yamada-Matsumoto scheme can be replicated by running penn2malt.jar available at http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html. We used Malt dependency labels (see website). The YamadaMatsumoto scheme is an elaboration of the Collins scheme (Collins, 1999), which is not included in our experiments. 2The CoNLL 2007 conversion scheme can be obtained by running pennconverter.jar available at http://nlp.cs.lth.se/software/treebank converter/with the ’conll07’ flag set. 3The EWT conversion scheme can be replicated using the Stanford converter av</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. In Iotes of the First Workshop on Syntactic Analysis of IonCanonical Language (SAICL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8618" citStr="Schwartz et al., 2011" startWordPosition="1223" endWordPosition="1226">in biomedical event extraction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical ap</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>Roy Schwartz, and Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Learnability-based syntactic annotation design.</title>
<date>2012</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="9358" citStr="Schwartz et al. (2012)" startWordPosition="1334" endWordPosition="1337">sider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section d</context>
<context position="20134" citStr="Schwartz et al. (2012)" startWordPosition="3012" endWordPosition="3015"> performance doing cross-validation over the official training data, setting the parameters of our learning algorithm for each fold doing cross-validation over the actual training data. We used soft-margin support vector machine learning (Cortes and Vapnik, 1995), tuning the kernel (linear or polynomial with degree 3) and C = 10.1,1, 5,10}. 3 Results and discussion Our results are presented in Table 1. The parsing results are obtained relying on predicted POS rather than, as often done in the dependency parsing literature, relying on gold-standard POS. Note that they comply with the result in Schwartz et al. (2012) that Yamada-Matsumoto-style annotation is more easily learnable. The negation resolution results are significantly better using syntactic features in yamada annotation. It is not surprising that a syntactically oriented conversion scheme performs well in this task. Since Lapponi et al. (2012) used Maltparser (Nivre et al., 2007) with the freely available pre-trained parsing model for English,14 we decided to also run that parser with the gold-standard cues, in ad14http://www.maltparser.org/mco/english parser/engmalt.html dition to Mate. The pre-trained model was trained on Sections 2–21 of th</context>
</contexts>
<marker>Schwartz, Abend, Rappoport, 2012</marker>
<rawString>Roy Schwartz, Omri Abend, and Ari Rappoport. 2012. Learnability-based syntactic annotation design. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Joakim Nivre</author>
<author>Evelina Andersson</author>
</authors>
<title>Cross-framework evaluation for statistical parsing.</title>
<date>2012</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="8642" citStr="Tsarfaty et al., 2012" startWordPosition="1227" endWordPosition="1230">raction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heur</context>
</contexts>
<marker>Tsarfaty, Nivre, Andersson, 2012</marker>
<rawString>Reut Tsarfaty, Joakim Nivre, and Evelina Andersson. 2012. Cross-framework evaluation for statistical parsing. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Lilja Øvrelid</author>
<author>Jonathon Read</author>
<author>Stephan Oepen</author>
</authors>
<title>Speculation and negation: Rules, rankers, and the role of synta.</title>
<date>2012</date>
<journal>Computational linguistics,</journal>
<pages>38--2</pages>
<contexts>
<context position="11107" citStr="Velldal et al., 2012" startWordPosition="1597" endWordPosition="1600">e)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [... ] CD-style scopes can be discontinuous and overlapping. Events are a portion o</context>
</contexts>
<marker>Velldal, Øvrelid, Read, Oepen, 2012</marker>
<rawString>Erik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan Oepen. 2012. Speculation and negation: Rules, rankers, and the role of synta. Computational linguistics, 38(2):369–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve SMT for subject-object-verb languages. In IAACLHLT,</title>
<date>2009</date>
<location>Boulder, Colorado.</location>
<contexts>
<context position="9828" citStr="Xu et al., 2009" startWordPosition="1404" endWordPosition="1407">al applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we 619 use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classi</context>
<context position="14717" citStr="Xu et al., 2009" startWordPosition="2154" endWordPosition="2157">arparser/ Syntactic Cue-dependent 620 2.3 Statistical machine translation The effect of the different conversion schemes was also evaluated on SMT. We used the reordering by parsing framework described by Elming and Haulrich (2011). This approach integrates a syntactically informed reordering model into a phrasebased SMT system. The model learns to predict the word order of the translation based on source sentence information such as syntactic dependency relations. Syntax-informed SMT is known to be useful for translating between languages with different word orders (Galley and Manning, 2009; Xu et al., 2009), e.g. English and German. The baseline SMT system is created as described in the guidelines from the original shared task.11 Only modifications are that we use truecasing instead of lowercasing and recasing, and allow training sentences of up to 80 words. We used data from the English-German restricted task: —3M parallel words of news, —46M parallel words of Europarl, and —309M words of monolingual Europarl and news. We use newstest2008 for tuning, newstest2009 for development, and newstest2010 for testing. Distortion limit was set to 10, which is also where the baseline system performed best</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve SMT for subject-object-verb languages. In IAACLHLT, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Laura Rimell</author>
<author>Aydin Han</author>
</authors>
<title>Parser evaluation using textual entailments.</title>
<date>2012</date>
<booktitle>Language Resources and Evaluation, Published online 31</booktitle>
<contexts>
<context position="8794" citStr="Yuret et al. (2012)" startWordPosition="1249" endWordPosition="1252">rs across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion scheme</context>
</contexts>
<marker>Yuret, Rimell, Han, 2012</marker>
<rawString>Deniz Yuret, Laura Rimell, and Aydin Han. 2012. Parser evaluation using textual entailments. Language Resources and Evaluation, Published online 31 October 2012.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>