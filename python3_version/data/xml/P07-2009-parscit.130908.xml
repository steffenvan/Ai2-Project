<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010675">
<title confidence="0.986492">
Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer
</title>
<author confidence="0.997916">
James R. Curran
</author>
<affiliation confidence="0.997908">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.682555">
NSW 2006, Australia
</address>
<email confidence="0.997187">
james@it.usyd.edu.au
</email>
<sectionHeader confidence="0.999635" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989430377777778">
The statistical modelling of language, together with
advances in wide-coverage grammar development,
have led to high levels of robustness and efficiency
in NLP systems and made linguistically motivated
large-scale language processing a possibility (Mat-
suzaki et al., 2007; Kaplan et al., 2004). This pa-
per describes an NLP system which is based on syn-
tactic and semantic formalisms from theoretical lin-
guistics, and which we have used to analyse the en-
tire Gigaword corpus (1 billion words) in less than
5 days using only 18 processors. This combination
of detail and speed of analysis represents a break-
through in NLP technology.
The system is built around a wide-coverage Com-
binatory Categorial Grammar (CCG) parser (Clark
and Curran, 2004b). The parser not only recovers
the local dependencies output by treebank parsers
such as Collins (2003), but also the long-range dep-
dendencies inherent in constructions such as extrac-
tion and coordination. CCG is a lexicalized gram-
mar formalism, so that each word in a sentence is
assigned an elementary syntactic structure, in CCG’s
case a lexical category expressing subcategorisation
information. Statistical tagging techniques can as-
sign lexical categories with high accuracy and low
ambiguity (Curran et al., 2006). The combination of
finite-state supertagging and highly engineered C++
leads to a parser which can analyse up to 30 sen-
tences per second on standard hardware (Clark and
Curran, 2004a).
The C&amp;C tools also contain a number of Maxi-
mum Entropy taggers, including the CCG supertag-
ger, a POS tagger (Curran and Clark, 2003a), chun-
33
ker, and named entity recogniser (Curran and Clark,
2003b). The taggers are highly efficient, with pro-
cessing speeds of over 100,000 words per second.
Finally, the various components, including the
morphological analyser morpha (Minnen et al.,
2001), are combined into a single program. The out-
put from this program — a CCG derivation, POS tags,
lemmas, and named entity tags — is used by the
module Boxer (Bos, 2005) to produce interpretable
structure in the form of Discourse Representation
Structures (DRSs).
</bodyText>
<sectionHeader confidence="0.932647" genericHeader="method">
2 The CCG Parser
</sectionHeader>
<bodyText confidence="0.999627">
The grammar used by the parser is extracted from
CCGbank, a CCG version of the Penn Treebank
(Hockenmaier, 2003). The grammar consists of 425
lexical categories, expressing subcategorisation in-
formation, plus a small number of combinatory rules
which combine the categories (Steedman, 2000). A
Maximum Entropy supertagger first assigns lexical
categories to the words in a sentence (Curran et al.,
2006), which are then combined by the parser using
the combinatory rules and the CKY algorithm.
Clark and Curran (2004b) describes log-linear
parsing models for CCG. The features in the models
are defined over local parts of CCG derivations and
include word-word dependencies. A disadvantage
of the log-linear models is that they require clus-
ter computing resources for practical training (Clark
and Curran, 2004b). We have also investigated per-
ceptron training for the parser (Clark and Curran,
2007b), obtaining comparable accuracy scores and
similar training times (a few hours) compared with
the log-linear models. The significant advantage of
</bodyText>
<figure confidence="0.616156444444444">
Stephen Clark Johan Bos
Computing Laboratory Dipartimento di Informatica
Oxford University Universit`a di Roma “La Sapienza”
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
stephen.clark@comlab.ox.ac.uk
via Salaria 113
00198 Roma, Italy
bos@di.uniroma1.it
</figure>
<subsubsectionHeader confidence="0.418005">
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36,
</subsubsectionHeader>
<bodyText confidence="0.980986555555555">
Prague, June 2007. c�2007 Association for Computational Linguistics
the perceptron training is that it only requires a sin-
gle processor. The training is online, updating the
model parameters one sentence at a time, and it con-
verges in a few passes over the CCGbank data.
A packed chart representation allows efficient de-
coding, with the same algorithm — the Viterbi al-
gorithm — finding the highest scoring derivation for
the log-linear and perceptron models.
</bodyText>
<subsectionHeader confidence="0.99808">
2.1 The Supertagger
</subsectionHeader>
<bodyText confidence="0.999989076923077">
The supertagger uses Maximum Entropy tagging
techniques (Section 3) to assign a set of lexical cate-
gories to each word (Curran et al., 2006). Supertag-
ging has been especially successful for CCG: Clark
and Curran (2004a) demonstrates the considerable
increases in speed that can be obtained through use
of a supertagger. The supertagger interacts with the
parser in an adaptive fashion: initially it assigns
a small number of categories, on average, to each
word in the sentence, and the parser attempts to cre-
ate a spanning analysis. If this is not possible, the
supertagger assigns more categories, and this pro-
cess continues until a spanning analysis is found.
</bodyText>
<subsectionHeader confidence="0.99802">
2.2 Parser Output
</subsectionHeader>
<bodyText confidence="0.999957130434783">
The parser produces various types of output. Fig-
ure 1 shows the dependency output for the exam-
ple sentence But Mr. Barnum called that a worst-
case scenario. The CCG dependencies are defined in
terms of the arguments within lexical categories; for
example, ((S[dcl]\NP1)/NP2, 2) represents the di-
rect object of a transitive verb. The parser also
outputs grammatical relations (GRs) consistent with
Briscoe et al. (2006). The GRs are derived through a
manually created mapping from the CCG dependen-
cies, together with a python post-processing script
which attempts to remove any differences between
the two annotation schemes (for example the way in
which coordination is analysed).
The parser has been evaluated on the predicate-
argument dependencies in CCGbank, obtaining la-
belled precision and recall scores of 84.8% and
84.5% on Section 23. We have also evaluated the
parser on DepBank, using the Grammatical Rela-
tions output. The parser scores 82.4% labelled pre-
cision and 81.2% labelled recall overall. Clark and
Curran (2007a) gives precison and recall scores bro-
ken down by relation type and also compares the
</bodyText>
<equation confidence="0.996869142857143">
Mr._2 N/N_1 1 Barnum_3
called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 3 that_5
worst-case_7 N/N_1 1 scenario_8
a_6 NP[nb]/N_1 1 scenario_8
called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 2 scenario_8
called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 1 Barnum_3
But_1 S[X]/S[X]_1 1 called_4
(ncmod _ Barnum_3 Mr._2)
(obj2 called_4 that_5)
(ncmod _ scenario_8 worst-case_7)
(det scenario_8 a_6)
(dobj called_4 scenario_8)
(ncsubj called_4 Barnum_3 _)
(conj _called_4 But_1)
</equation>
<figureCaption confidence="0.94754925">
Figure 1: Dependency output in the form of CCG
dependencies and grammatical relations
performance of the CCG parser with the RASP parser
(Briscoe et al., 2006).
</figureCaption>
<sectionHeader confidence="0.986849" genericHeader="method">
3 Maximum Entropy Taggers
</sectionHeader>
<bodyText confidence="0.999968730769231">
The taggers are based on Maximum Entropy tag-
ging methods (Ratnaparkhi, 1996), and can all be
trained on new annotated data, using either GIS or
BFGS training code.
The POS tagger uses the standard set of grammat-
ical categories from the Penn Treebank and, as well
as being highly efficient, also has state-of-the-art ac-
curacy on unseen newspaper text: over 97% per-
word accuracy on Section 23 of the Penn Treebank
(Curran and Clark, 2003a). The chunker recognises
the standard set of grammatical “chunks”: NP, VP,
PP, ADJP, ADVP, and so on. It has been trained on
the CoNLL shared task data.
The named entity recogniser recognises the stan-
dard set of named entities in text: person, loca-
tion, organisation, date, time, monetary amount. It
has been trained on the MUC data. The named en-
tity recogniser contains many more features than the
other taggers; Curran and Clark (2003b) describes
the feature set.
Each tagger can be run as a “multi-tagger”, poten-
tially assigning more than one tag to a word. The
multi-tagger uses the forward-backward algorithm
to calculate a distribution over tags for each word in
the sentence, and a parameter determines how many
tags are assigned to each word.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="method">
4 Boxer
</sectionHeader>
<bodyText confidence="0.99905275">
Boxer is a separate component which takes a CCG
derivation output by the C&amp;C parser and generates a
semantic representation. Boxer implements a first-
order fragment of Discourse Representation Theory,
</bodyText>
<page confidence="0.995499">
34
</page>
<bodyText confidence="0.999537588235294">
DRT (Kamp and Reyle, 1993), and is capable of
generating the box-like structures of DRT known as
Discourse Representation Structures (DRSs). DRT is
a formal semantic theory backed up with a model
theory, and it demonstrates a large coverage of lin-
guistic phenomena. Boxer follows the formal the-
ory closely, introducing discourse referents for noun
phrases and events in the domain of a DRS, and their
properties in the conditions of a DRS.
One deviation with the standard theory is the
adoption of a Neo-Davidsonian analysis of events
and roles. Boxer also implements Van der Sandt’s
theory of presupposition projection treating proper
names and defininite descriptions as anaphoric ex-
pressions, by binding them to appropriate previously
introduced discourse referents, or accommodating
on a suitable level of discourse representation.
</bodyText>
<subsectionHeader confidence="0.982982">
4.1 Discourse Representation Structures
</subsectionHeader>
<bodyText confidence="0.999979666666666">
DRSs are recursive data structures — each DRS com-
prises a domain (a set of discourse referents) and a
set of conditions (possibly introducing new DRSs).
DRS-conditions are either basic or complex. The ba-
sic DRS-conditions supported by Boxer are: equal-
ity, stating that two discourse referents refer to the
same entity; one-place relations, expressing proper-
ties of discourse referents; two place relations, ex-
pressing binary relations between discourse refer-
ents; and names and time expressions. Complex
DRS-conditions are: negation of a DRS; disjunction
of two DRSs; implication (one DRS implying an-
other); and propositional, relating a discourse ref-
erent to a DRS.
Nouns, verbs, adjectives and adverbs introduce
one-place relations, whose meaning is represented
by the corresponding lemma. Verb roles and prepo-
sitions introduce two-place relations.
</bodyText>
<subsectionHeader confidence="0.921117">
4.2 Input and Output
</subsectionHeader>
<bodyText confidence="0.996384444444445">
The input for Boxer is a list of CCG derivations deco-
rated with named entities, POS tags, and lemmas for
nouns and verbs. By default, each CCG derivation
produces one DRS. However, it is possible for one
DRS to span several CCG derivations; this enables
Boxer to deal with cross-sentential phenomena such
as pronouns and presupposition.
Boxer provides various output formats. The de-
fault output is a DRS in Prolog format, with dis-
</bodyText>
<equation confidence="0.999370214285714">
 |x0 x1 x2 x3 |
 ||
 |named(x0,barnum,per) |
 |named(x0,mr,ttl) |
 |thing(x1) |
 |worst-case(x2) |
 |scenario(x2) |
 |call(x3) |
 |but(x3) |
 |event(x3) |
 |agent(x3,x0) |
 |patient(x3,x1) |
 |theme(x3,x2) |
 ||
</equation>
<figureCaption confidence="0.989617">
Figure 2: Easy-to-read output format of Boxer
</figureCaption>
<bodyText confidence="0.981441742857143">
course referents represented as Prolog variables.
Other output options include: a flat structure, in
which the recursive structure of a DRS is unfolded by
labelling each DRS and DRS-condition; an XML for-
mat; and an easy-to-read box-like structure as found
in textbooks and articles on DRT. Figure 2 shows the
easy-to-read output for the sentence But Mr. Barnum
called that a worst-case scenario.
The semantic representations can also be output
as first-order formulas. This is achieved using the
standard translation from DRS to first-order logic
(Kamp and Reyle, 1993), and allows the output
to be pipelined into off-the-shelf theorem provers
or model builders for first-order logic, to perform
consistency or informativeness checking (Blackburn
and Bos, 2005).
5 Usage of the Tools
The taggers (and therefore the parser) can accept
many different input formats and produce many dif-
ferent output formats. These are described using a
“little language” similar to C printf format strings.
For example, the input format %w|%p \n indicates
that the program expects word (%w) and POS tag
(%p) pairs as input, where the words and POS tags
are separated by pipe characters, and each word-POS
tag pair is separated by a single space, and whole
sentences are separated by newlines (\n). Another
feature of the input/output is that other fields can be
read in which are not used in the tagging process,
and also form part of the output.
The C&amp;C tools use a configuration management
system which allows the user to override all of the
default parameters for training and running the tag-
gers and parser. All of the tools can be used as stand-
alone components. Alternatively, a pipeline of the
</bodyText>
<page confidence="0.998309">
35
</page>
<bodyText confidence="0.96241">
tools is provided which supports two modes: local
file reading/writing or SOAP server mode.
</bodyText>
<sectionHeader confidence="0.986803" genericHeader="method">
6 Applications
</sectionHeader>
<bodyText confidence="0.999897105263158">
We have developed an open-domain QA system built
around the C&amp;C tools and Boxer (Ahn et al., 2005).
The parser is well suited to analysing large amounts
of text containing a potential answer, because of
its efficiency. The grammar is also well suited to
analysing questions, because of CCG’s treatment of
long-range dependencies. However, since the CCG
parser is based on the Penn Treebank, which con-
tains few examples of questions, the parser trained
on CCGbank is a poor analyser of questions. Clark
et al. (2004) describes a porting method we have de-
veloped which exploits the lexicalized nature of CCG
by relying on rapid manual annotation at the lexi-
cal category level. We have successfully applied this
method to questions.
The robustness and efficiency of the parser; its
ability to analyses questions; and the detailed out-
put provided by Boxer make it ideal for large-scale
open-domain QA.
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999134">
Linguistically motivated NLP can now be used
for large-scale language processing applications.
The C&amp;C tools plus Boxer are freely available
for research use and can be downloaded from
http://svn.ask.it.usyd.edu.au/trac/candc/wiki.
</bodyText>
<sectionHeader confidence="0.997583" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<copyright confidence="0.600242333333333">
James Curran was funded under ARC Discovery grants
DP0453131 and DP0665973. Johan Bos is supported by a “Ri-
entro dei Cervelli” grant (Italian Ministry for Research).
</copyright>
<sectionHeader confidence="0.998592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999330984615385">
Kisuh Ahn, Johan Bos, James R. Curran, Dave Kor, Malvina
Nissim, and Bonnie Webber. 2005. Question answering
with QED at TREC-2005. In Proceedings of TREC-2005.
Patrick Blackburn and Johan Bos. 2005. Representation and
Inference for Natural Language. A First Course in Compu-
tational Semantics. CSLI.
Johan Bos. 2005. Towards wide-coverage semantic interpreta-
tion. In Proceedings of IWCS-6, pages 42–53, Tilburg, The
Netherlands.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In Proceedings of the
Interactive Demo Session of COLING/ACL-06, Sydney.
Stephen Clark and James R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proceed-
ings of COLING-04, pages 282–288, Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the WSJ
using CCG and log-linear models. In Proceedings of ACL-
04, pages 104–111, Barcelona, Spain.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and DepBank. In
Proceedings of the 45th Annual Meeting of the ACL, Prague,
Czech Republic.
Stephen Clark and James R. Curran. 2007b. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser. In Pro-
ceedings of the ACL Workshop on Deep Linguistic Process-
ing, Prague, Czech Republic.
Stephen Clark, Mark Steedman, and James R. Curran. 2004.
Object-extraction and question-parsing using CCG. In
Proceedings of the EMNLP Conference, pages 111–118,
Barcelona, Spain.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589–637.
James R. Curran and Stephen Clark. 2003a. Investigating GIS
and smoothing for maximum entropy taggers. In Proceed-
ings of the 10th Meeting of the EACL, pages 91–98, Bu-
dapest, Hungary.
James R. Curran and Stephen Clark. 2003b. Language inde-
pendent NER using a maximum entropy tagger. In Proceed-
ings of CoNLL-03, pages 164–167, Edmonton, Canada.
James R. Curran, Stephen Clark, and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In Proceed-
ings of COLING/ACL-06, pages 697–704, Sydney.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D. the-
sis, University of Edinburgh.
H. Kamp and U. Reyle. 1993. From Discourse to Logic; An
Introduction to Modeltheoretic Semantics of Natural Lan-
guage, Formal Logic and DRT. Kluwer, Dordrecht.
Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell
III, Alexander Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic pars-
ing. In Proceedings of HLT and the 4th Meeting ofNAACL,
Boston, MA.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007.
Efficient HPSG parsing with supertagging and CFG-
filtering. In Proceedings ofIJCAI-07, Hyderabad, India.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering, 7(3):207–223.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the EMNLP Conference,
pages 133–142, Philadelphia, PA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
</reference>
<page confidence="0.998942">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929322">
<title confidence="0.996546">Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer</title>
<author confidence="0.999995">James R Curran</author>
<affiliation confidence="0.9997325">School of Information Technologies University of Sydney</affiliation>
<address confidence="0.997446">NSW 2006, Australia</address>
<email confidence="0.935191">james@it.usyd.edu.au</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kisuh Ahn</author>
<author>Johan Bos</author>
<author>James R Curran</author>
<author>Dave Kor</author>
<author>Malvina Nissim</author>
<author>Bonnie Webber</author>
</authors>
<title>Question answering with QED at TREC-2005.</title>
<date>2005</date>
<booktitle>In Proceedings of TREC-2005.</booktitle>
<contexts>
<context position="12281" citStr="Ahn et al., 2005" startWordPosition="1923" endWordPosition="1926"> (\n). Another feature of the input/output is that other fields can be read in which are not used in the tagging process, and also form part of the output. The C&amp;C tools use a configuration management system which allows the user to override all of the default parameters for training and running the taggers and parser. All of the tools can be used as standalone components. Alternatively, a pipeline of the 35 tools is provided which supports two modes: local file reading/writing or SOAP server mode. 6 Applications We have developed an open-domain QA system built around the C&amp;C tools and Boxer (Ahn et al., 2005). The parser is well suited to analysing large amounts of text containing a potential answer, because of its efficiency. The grammar is also well suited to analysing questions, because of CCG’s treatment of long-range dependencies. However, since the CCG parser is based on the Penn Treebank, which contains few examples of questions, the parser trained on CCGbank is a poor analyser of questions. Clark et al. (2004) describes a porting method we have developed which exploits the lexicalized nature of CCG by relying on rapid manual annotation at the lexical category level. We have successfully ap</context>
</contexts>
<marker>Ahn, Bos, Curran, Kor, Nissim, Webber, 2005</marker>
<rawString>Kisuh Ahn, Johan Bos, James R. Curran, Dave Kor, Malvina Nissim, and Bonnie Webber. 2005. Question answering with QED at TREC-2005. In Proceedings of TREC-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Johan Bos</author>
</authors>
<title>Representation and Inference for Natural Language. A First Course in Computational Semantics.</title>
<date>2005</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="11153" citStr="Blackburn and Bos, 2005" startWordPosition="1728" endWordPosition="1731">RS is unfolded by labelling each DRS and DRS-condition; an XML format; and an easy-to-read box-like structure as found in textbooks and articles on DRT. Figure 2 shows the easy-to-read output for the sentence But Mr. Barnum called that a worst-case scenario. The semantic representations can also be output as first-order formulas. This is achieved using the standard translation from DRS to first-order logic (Kamp and Reyle, 1993), and allows the output to be pipelined into off-the-shelf theorem provers or model builders for first-order logic, to perform consistency or informativeness checking (Blackburn and Bos, 2005). 5 Usage of the Tools The taggers (and therefore the parser) can accept many different input formats and produce many different output formats. These are described using a “little language” similar to C printf format strings. For example, the input format %w|%p \n indicates that the program expects word (%w) and POS tag (%p) pairs as input, where the words and POS tags are separated by pipe characters, and each word-POS tag pair is separated by a single space, and whole sentences are separated by newlines (\n). Another feature of the input/output is that other fields can be read in which are </context>
</contexts>
<marker>Blackburn, Bos, 2005</marker>
<rawString>Patrick Blackburn and Johan Bos. 2005. Representation and Inference for Natural Language. A First Course in Computational Semantics. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Towards wide-coverage semantic interpretation.</title>
<date>2005</date>
<booktitle>In Proceedings of IWCS-6,</booktitle>
<pages>42--53</pages>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="2196" citStr="Bos, 2005" startWordPosition="343" endWordPosition="344">nd on standard hardware (Clark and Curran, 2004a). The C&amp;C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algor</context>
<context position="11153" citStr="Bos, 2005" startWordPosition="1730" endWordPosition="1731"> by labelling each DRS and DRS-condition; an XML format; and an easy-to-read box-like structure as found in textbooks and articles on DRT. Figure 2 shows the easy-to-read output for the sentence But Mr. Barnum called that a worst-case scenario. The semantic representations can also be output as first-order formulas. This is achieved using the standard translation from DRS to first-order logic (Kamp and Reyle, 1993), and allows the output to be pipelined into off-the-shelf theorem provers or model builders for first-order logic, to perform consistency or informativeness checking (Blackburn and Bos, 2005). 5 Usage of the Tools The taggers (and therefore the parser) can accept many different input formats and produce many different output formats. These are described using a “little language” similar to C printf format strings. For example, the input format %w|%p \n indicates that the program expects word (%w) and POS tag (%p) pairs as input, where the words and POS tags are separated by pipe characters, and each word-POS tag pair is separated by a single space, and whole sentences are separated by newlines (\n). Another feature of the input/output is that other fields can be read in which are </context>
</contexts>
<marker>Bos, 2005</marker>
<rawString>Johan Bos. 2005. Towards wide-coverage semantic interpretation. In Proceedings of IWCS-6, pages 42–53, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Interactive Demo Session of COLING/ACL-06,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="5255" citStr="Briscoe et al. (2006)" startWordPosition="813" endWordPosition="816">, and the parser attempts to create a spanning analysis. If this is not possible, the supertagger assigns more categories, and this process continues until a spanning analysis is found. 2.2 Parser Output The parser produces various types of output. Figure 1 shows the dependency output for the example sentence But Mr. Barnum called that a worstcase scenario. The CCG dependencies are defined in terms of the arguments within lexical categories; for example, ((S[dcl]\NP1)/NP2, 2) represents the direct object of a transitive verb. The parser also outputs grammatical relations (GRs) consistent with Briscoe et al. (2006). The GRs are derived through a manually created mapping from the CCG dependencies, together with a python post-processing script which attempts to remove any differences between the two annotation schemes (for example the way in which coordination is analysed). The parser has been evaluated on the predicateargument dependencies in CCGbank, obtaining labelled precision and recall scores of 84.8% and 84.5% on Section 23. We have also evaluated the parser on DepBank, using the Grammatical Relations output. The parser scores 82.4% labelled precision and 81.2% labelled recall overall. Clark and Cu</context>
<context position="6543" citStr="Briscoe et al., 2006" startWordPosition="1002" endWordPosition="1005">on type and also compares the Mr._2 N/N_1 1 Barnum_3 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 3 that_5 worst-case_7 N/N_1 1 scenario_8 a_6 NP[nb]/N_1 1 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 2 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 1 Barnum_3 But_1 S[X]/S[X]_1 1 called_4 (ncmod _ Barnum_3 Mr._2) (obj2 called_4 that_5) (ncmod _ scenario_8 worst-case_7) (det scenario_8 a_6) (dobj called_4 scenario_8) (ncsubj called_4 Barnum_3 _) (conj _called_4 But_1) Figure 1: Dependency output in the form of CCG dependencies and grammatical relations performance of the CCG parser with the RASP parser (Briscoe et al., 2006). 3 Maximum Entropy Taggers The taggers are based on Maximum Entropy tagging methods (Ratnaparkhi, 1996), and can all be trained on new annotated data, using either GIS or BFGS training code. The POS tagger uses the standard set of grammatical categories from the Penn Treebank and, as well as being highly efficient, also has state-of-the-art accuracy on unseen newspaper text: over 97% perword accuracy on Section 23 of the Penn Treebank (Curran and Clark, 2003a). The chunker recognises the standard set of grammatical “chunks”: NP, VP, PP, ADJP, ADVP, and so on. It has been trained on the CoNLL </context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the Interactive Demo Session of COLING/ACL-06, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="929" citStr="Clark and Curran, 2004" startWordPosition="138" endWordPosition="141">vels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads </context>
<context position="2824" citStr="Clark and Curran (2004" startWordPosition="437" endWordPosition="440">oduce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Stephen Clark Johan Bos Computing Laboratory Dipartimento di Informati</context>
<context position="4374" citStr="Clark and Curran (2004" startWordPosition="671" endWordPosition="674">rceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding the highest scoring derivation for the log-linear and perceptron models. 2.1 The Supertagger The supertagger uses Maximum Entropy tagging techniques (Section 3) to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging has been especially successful for CCG: Clark and Curran (2004a) demonstrates the considerable increases in speed that can be obtained through use of a supertagger. The supertagger interacts with the parser in an adaptive fashion: initially it assigns a small number of categories, on average, to each word in the sentence, and the parser attempts to create a spanning analysis. If this is not possible, the supertagger assigns more categories, and this process continues until a spanning analysis is found. 2.2 Parser Output The parser produces various types of output. Figure 1 shows the dependency output for the example sentence But Mr. Barnum called that a </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING-04, pages 282–288, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL04,</booktitle>
<pages>104--111</pages>
<location>Barcelona,</location>
<contexts>
<context position="929" citStr="Clark and Curran, 2004" startWordPosition="138" endWordPosition="141">vels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads </context>
<context position="2824" citStr="Clark and Curran (2004" startWordPosition="437" endWordPosition="440">oduce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Stephen Clark Johan Bos Computing Laboratory Dipartimento di Informati</context>
<context position="4374" citStr="Clark and Curran (2004" startWordPosition="671" endWordPosition="674">rceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding the highest scoring derivation for the log-linear and perceptron models. 2.1 The Supertagger The supertagger uses Maximum Entropy tagging techniques (Section 3) to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging has been especially successful for CCG: Clark and Curran (2004a) demonstrates the considerable increases in speed that can be obtained through use of a supertagger. The supertagger interacts with the parser in an adaptive fashion: initially it assigns a small number of categories, on average, to each word in the sentence, and the parser attempts to create a spanning analysis. If this is not possible, the supertagger assigns more categories, and this process continues until a spanning analysis is found. 2.2 Parser Output The parser produces various types of output. Figure 1 shows the dependency output for the example sentence But Mr. Barnum called that a </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proceedings of ACL04, pages 104–111, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3206" citStr="Clark and Curran, 2007" startWordPosition="495" endWordPosition="498">teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Stephen Clark Johan Bos Computing Laboratory Dipartimento di Informatica Oxford University Universit`a di Roma “La Sapienza” Wolfson Building, Parks Road Oxford, OX1 3QD, UK stephen.clark@comlab.ox.ac.uk via Salaria 113 00198 Roma, Italy bos@di.uniroma1.it Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, Prague, June 2007. c�2007 Association for Computational Linguistics the perceptron training is that it only requires a single pr</context>
<context position="5865" citStr="Clark and Curran (2007" startWordPosition="909" endWordPosition="912">t al. (2006). The GRs are derived through a manually created mapping from the CCG dependencies, together with a python post-processing script which attempts to remove any differences between the two annotation schemes (for example the way in which coordination is analysed). The parser has been evaluated on the predicateargument dependencies in CCGbank, obtaining labelled precision and recall scores of 84.8% and 84.5% on Section 23. We have also evaluated the parser on DepBank, using the Grammatical Relations output. The parser scores 82.4% labelled precision and 81.2% labelled recall overall. Clark and Curran (2007a) gives precison and recall scores broken down by relation type and also compares the Mr._2 N/N_1 1 Barnum_3 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 3 that_5 worst-case_7 N/N_1 1 scenario_8 a_6 NP[nb]/N_1 1 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 2 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 1 Barnum_3 But_1 S[X]/S[X]_1 1 called_4 (ncmod _ Barnum_3 Mr._2) (obj2 called_4 that_5) (ncmod _ scenario_8 worst-case_7) (det scenario_8 a_6) (dobj called_4 scenario_8) (ncsubj called_4 Barnum_3 _) (conj _called_4 But_1) Figure 1: Dependency output in the form of CCG dependencies and grammatical relat</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007a. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Perceptron training for a wide-coverage lexicalized-grammar parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Deep Linguistic Processing,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3206" citStr="Clark and Curran, 2007" startWordPosition="495" endWordPosition="498">teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Stephen Clark Johan Bos Computing Laboratory Dipartimento di Informatica Oxford University Universit`a di Roma “La Sapienza” Wolfson Building, Parks Road Oxford, OX1 3QD, UK stephen.clark@comlab.ox.ac.uk via Salaria 113 00198 Roma, Italy bos@di.uniroma1.it Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, Prague, June 2007. c�2007 Association for Computational Linguistics the perceptron training is that it only requires a single pr</context>
<context position="5865" citStr="Clark and Curran (2007" startWordPosition="909" endWordPosition="912">t al. (2006). The GRs are derived through a manually created mapping from the CCG dependencies, together with a python post-processing script which attempts to remove any differences between the two annotation schemes (for example the way in which coordination is analysed). The parser has been evaluated on the predicateargument dependencies in CCGbank, obtaining labelled precision and recall scores of 84.8% and 84.5% on Section 23. We have also evaluated the parser on DepBank, using the Grammatical Relations output. The parser scores 82.4% labelled precision and 81.2% labelled recall overall. Clark and Curran (2007a) gives precison and recall scores broken down by relation type and also compares the Mr._2 N/N_1 1 Barnum_3 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 3 that_5 worst-case_7 N/N_1 1 scenario_8 a_6 NP[nb]/N_1 1 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 2 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 1 Barnum_3 But_1 S[X]/S[X]_1 1 called_4 (ncmod _ Barnum_3 Mr._2) (obj2 called_4 that_5) (ncmod _ scenario_8 worst-case_7) (det scenario_8 a_6) (dobj called_4 scenario_8) (ncsubj called_4 Barnum_3 _) (conj _called_4 But_1) Figure 1: Dependency output in the form of CCG dependencies and grammatical relat</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007b. Perceptron training for a wide-coverage lexicalized-grammar parser. In Proceedings of the ACL Workshop on Deep Linguistic Processing, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
</authors>
<title>Object-extraction and question-parsing using CCG.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>111--118</pages>
<location>Barcelona,</location>
<contexts>
<context position="12698" citStr="Clark et al. (2004)" startWordPosition="1991" endWordPosition="1994">ls is provided which supports two modes: local file reading/writing or SOAP server mode. 6 Applications We have developed an open-domain QA system built around the C&amp;C tools and Boxer (Ahn et al., 2005). The parser is well suited to analysing large amounts of text containing a potential answer, because of its efficiency. The grammar is also well suited to analysing questions, because of CCG’s treatment of long-range dependencies. However, since the CCG parser is based on the Penn Treebank, which contains few examples of questions, the parser trained on CCGbank is a poor analyser of questions. Clark et al. (2004) describes a porting method we have developed which exploits the lexicalized nature of CCG by relying on rapid manual annotation at the lexical category level. We have successfully applied this method to questions. The robustness and efficiency of the parser; its ability to analyses questions; and the detailed output provided by Boxer make it ideal for large-scale open-domain QA. 7 Conclusion Linguistically motivated NLP can now be used for large-scale language processing applications. The C&amp;C tools plus Boxer are freely available for research use and can be downloaded from http://svn.ask.it.u</context>
</contexts>
<marker>Clark, Steedman, Curran, 2004</marker>
<rawString>Stephen Clark, Mark Steedman, and James R. Curran. 2004. Object-extraction and question-parsing using CCG. In Proceedings of the EMNLP Conference, pages 111–118, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1034" citStr="Collins (2003)" startWordPosition="156" endWordPosition="157">g a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Meeting of the EACL,</booktitle>
<pages>91--98</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1768" citStr="Curran and Clark, 2003" startWordPosition="270" endWordPosition="273">lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C&amp;C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a </context>
<context position="7006" citStr="Curran and Clark, 2003" startWordPosition="1081" endWordPosition="1084">Figure 1: Dependency output in the form of CCG dependencies and grammatical relations performance of the CCG parser with the RASP parser (Briscoe et al., 2006). 3 Maximum Entropy Taggers The taggers are based on Maximum Entropy tagging methods (Ratnaparkhi, 1996), and can all be trained on new annotated data, using either GIS or BFGS training code. The POS tagger uses the standard set of grammatical categories from the Penn Treebank and, as well as being highly efficient, also has state-of-the-art accuracy on unseen newspaper text: over 97% perword accuracy on Section 23 of the Penn Treebank (Curran and Clark, 2003a). The chunker recognises the standard set of grammatical “chunks”: NP, VP, PP, ADJP, ADVP, and so on. It has been trained on the CoNLL shared task data. The named entity recogniser recognises the standard set of named entities in text: person, location, organisation, date, time, monetary amount. It has been trained on the MUC data. The named entity recogniser contains many more features than the other taggers; Curran and Clark (2003b) describes the feature set. Each tagger can be run as a “multi-tagger”, potentially assigning more than one tag to a word. The multi-tagger uses the forward-bac</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003a. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 10th Meeting of the EACL, pages 91–98, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent NER using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-03,</booktitle>
<pages>164--167</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1768" citStr="Curran and Clark, 2003" startWordPosition="270" endWordPosition="273">lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C&amp;C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a </context>
<context position="7006" citStr="Curran and Clark, 2003" startWordPosition="1081" endWordPosition="1084">Figure 1: Dependency output in the form of CCG dependencies and grammatical relations performance of the CCG parser with the RASP parser (Briscoe et al., 2006). 3 Maximum Entropy Taggers The taggers are based on Maximum Entropy tagging methods (Ratnaparkhi, 1996), and can all be trained on new annotated data, using either GIS or BFGS training code. The POS tagger uses the standard set of grammatical categories from the Penn Treebank and, as well as being highly efficient, also has state-of-the-art accuracy on unseen newspaper text: over 97% perword accuracy on Section 23 of the Penn Treebank (Curran and Clark, 2003a). The chunker recognises the standard set of grammatical “chunks”: NP, VP, PP, ADJP, ADVP, and so on. It has been trained on the CoNLL shared task data. The named entity recogniser recognises the standard set of named entities in text: person, location, organisation, date, time, monetary amount. It has been trained on the MUC data. The named entity recogniser contains many more features than the other taggers; Curran and Clark (2003b) describes the feature set. Each tagger can be run as a “multi-tagger”, potentially assigning more than one tag to a word. The multi-tagger uses the forward-bac</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003b. Language independent NER using a maximum entropy tagger. In Proceedings of CoNLL-03, pages 164–167, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<pages>697--704</pages>
<location>Sydney.</location>
<contexts>
<context position="1450" citStr="Curran et al., 2006" startWordPosition="216" endWordPosition="219">is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C&amp;C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a </context>
<context position="2711" citStr="Curran et al., 2006" startWordPosition="418" endWordPosition="421">ram — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-l</context>
<context position="4297" citStr="Curran et al., 2006" startWordPosition="659" endWordPosition="662"> Prague, June 2007. c�2007 Association for Computational Linguistics the perceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding the highest scoring derivation for the log-linear and perceptron models. 2.1 The Supertagger The supertagger uses Maximum Entropy tagging techniques (Section 3) to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging has been especially successful for CCG: Clark and Curran (2004a) demonstrates the considerable increases in speed that can be obtained through use of a supertagger. The supertagger interacts with the parser in an adaptive fashion: initially it assigns a small number of categories, on average, to each word in the sentence, and the parser attempts to create a spanning analysis. If this is not possible, the supertagger assigns more categories, and this process continues until a spanning analysis is found. 2.2 Parser Output The parser produces various types of output. Figure 1 shows</context>
</contexts>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R. Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of COLING/ACL-06, pages 697–704, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2420" citStr="Hockenmaier, 2003" startWordPosition="378" endWordPosition="379">ecogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear mode</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="7996" citStr="Kamp and Reyle, 1993" startWordPosition="1245" endWordPosition="1248">ins many more features than the other taggers; Curran and Clark (2003b) describes the feature set. Each tagger can be run as a “multi-tagger”, potentially assigning more than one tag to a word. The multi-tagger uses the forward-backward algorithm to calculate a distribution over tags for each word in the sentence, and a parameter determines how many tags are assigned to each word. 4 Boxer Boxer is a separate component which takes a CCG derivation output by the C&amp;C parser and generates a semantic representation. Boxer implements a firstorder fragment of Discourse Representation Theory, 34 DRT (Kamp and Reyle, 1993), and is capable of generating the box-like structures of DRT known as Discourse Representation Structures (DRSs). DRT is a formal semantic theory backed up with a model theory, and it demonstrates a large coverage of linguistic phenomena. Boxer follows the formal theory closely, introducing discourse referents for noun phrases and events in the domain of a DRS, and their properties in the conditions of a DRS. One deviation with the standard theory is the adoption of a Neo-Davidsonian analysis of events and roles. Boxer also implements Van der Sandt’s theory of presupposition projection treati</context>
<context position="10961" citStr="Kamp and Reyle, 1993" startWordPosition="1701" endWordPosition="1704"> | || Figure 2: Easy-to-read output format of Boxer course referents represented as Prolog variables. Other output options include: a flat structure, in which the recursive structure of a DRS is unfolded by labelling each DRS and DRS-condition; an XML format; and an easy-to-read box-like structure as found in textbooks and articles on DRT. Figure 2 shows the easy-to-read output for the sentence But Mr. Barnum called that a worst-case scenario. The semantic representations can also be output as first-order formulas. This is achieved using the standard translation from DRS to first-order logic (Kamp and Reyle, 1993), and allows the output to be pipelined into off-the-shelf theorem provers or model builders for first-order logic, to perform consistency or informativeness checking (Blackburn and Bos, 2005). 5 Usage of the Tools The taggers (and therefore the parser) can accept many different input formats and produce many different output formats. These are described using a “little language” similar to C printf format strings. For example, the input format %w|%p \n indicates that the program expects word (%w) and POS tag (%p) pairs as input, where the words and POS tags are separated by pipe characters, a</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>H. Kamp and U. Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell Alexander Vasserman</author>
<author>Richard Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT and the 4th Meeting ofNAACL,</booktitle>
<location>Boston, MA.</location>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell III, Alexander Vasserman, and Richard Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proceedings of HLT and the 4th Meeting ofNAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficient HPSG parsing with supertagging and CFGfiltering.</title>
<date>2007</date>
<booktitle>In Proceedings ofIJCAI-07,</booktitle>
<location>Hyderabad, India.</location>
<marker>Matsuzaki, Miyao, Tsujii, 2007</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Efficient HPSG parsing with supertagging and CFGfiltering. In Proceedings ofIJCAI-07, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="2028" citStr="Minnen et al., 2001" startWordPosition="309" endWordPosition="312">cy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C&amp;C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagge</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6647" citStr="Ratnaparkhi, 1996" startWordPosition="1020" endWordPosition="1021">_7 N/N_1 1 scenario_8 a_6 NP[nb]/N_1 1 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 2 scenario_8 called_4 ((S[dcl]\NP_1)/NP_2)/NP_3 1 Barnum_3 But_1 S[X]/S[X]_1 1 called_4 (ncmod _ Barnum_3 Mr._2) (obj2 called_4 that_5) (ncmod _ scenario_8 worst-case_7) (det scenario_8 a_6) (dobj called_4 scenario_8) (ncsubj called_4 Barnum_3 _) (conj _called_4 But_1) Figure 1: Dependency output in the form of CCG dependencies and grammatical relations performance of the CCG parser with the RASP parser (Briscoe et al., 2006). 3 Maximum Entropy Taggers The taggers are based on Maximum Entropy tagging methods (Ratnaparkhi, 1996), and can all be trained on new annotated data, using either GIS or BFGS training code. The POS tagger uses the standard set of grammatical categories from the Penn Treebank and, as well as being highly efficient, also has state-of-the-art accuracy on unseen newspaper text: over 97% perword accuracy on Section 23 of the Penn Treebank (Curran and Clark, 2003a). The chunker recognises the standard set of grammatical “chunks”: NP, VP, PP, ADJP, ADVP, and so on. It has been trained on the CoNLL shared task data. The named entity recogniser recognises the standard set of named entities in text: per</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the EMNLP Conference, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2598" citStr="Steedman, 2000" startWordPosition="402" endWordPosition="403">logical analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extracted from CCGbank, a CCG version of the Penn Treebank (Hockenmaier, 2003). The grammar consists of 425 lexical categories, expressing subcategorisation information, plus a small number of combinatory rules which combine the categories (Steedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curr</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>