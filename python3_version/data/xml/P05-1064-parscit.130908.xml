<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.99586">
A Phonotactic Language Model for Spoken Language Identification
</title>
<author confidence="0.985392">
Haizhou Li and Bin Ma
</author>
<affiliation confidence="0.971525">
Institute for Infocomm Research
</affiliation>
<address confidence="0.907662">
Singapore 119613
</address>
<email confidence="0.836172">
{hli,mabin}@i2r.a-star.edu.sg
</email>
<bodyText confidence="0.997819575">
Orthographic forms of language, ranging from
Latin alphabet to Cyrillic script to Chinese charac-
ters, are far more unique to the language than their
phonetic counterparts. From the speech production
point of view, thousands of spoken languages from
all over the world are phonetically articulated us-
ing only a few hundred distinctive sounds or pho-
nemes (Hieronymus, 1994). In other words,
common sounds are shared considerably across
different spoken languages. In addition, spoken
documents1, in the form of digitized wave files, are
far less structured than written documents and need
to be treated with techniques that go beyond the
bounds of written language. All of this makes the
identification of spoken language based on pho-
netic units much more challenging than the identi-
fication of written language. In fact, the challenge
of LID is inter-disciplinary, involving digital signal
processing, speech recognition and natural lan-
guage processing.
In general, a LID system usually has three fun-
damental components as follows:
1) A voice tokenizer which segments incoming
voice feature frames and associates the seg-
ments with acoustic or phonetic labels, called
tokens;
2) A statistical language model which captures
language dependent phonetic and phonotactic
information from the sequences of tokens;
3) A language classifier which identifies the lan-
guage based on discriminatory characteristics
of acoustic score from the voice tokenizer and
phonotactic score from the language model.
In this paper, we present a novel solution to the
three problems, focusing on the second and third
problems from a computational linguistic perspec-
tive. The paper is organized as follows: In Section
2, we summarize relevant existing approaches to
the LID task. We highlight the shortcomings of
existing approaches and our attempts to address the
</bodyText>
<page confidence="0.306775">
1 A spoken utterance is regarded as a spoken document in this
paper.
</page>
<sectionHeader confidence="0.8073" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950956521739">
We have established a phonotactic lan-
guage model as the solution to spoken
language identification (LID). In this
framework, we define a single set of
acoustic tokens to represent the acoustic
activities in the world’s spoken languages.
A voice tokenizer converts a spoken
document into a text-like document of
acoustic tokens. Thus a spoken document
can be represented by a count vector of
acoustic tokens and token n-grams in the
vector space. We apply latent semantic
analysis to the vectors, in the same way
that it is applied in information retrieval,
in order to capture salient phonotactics
present in spoken documents. The vector
space modeling of spoken utterances con-
stitutes a paradigm shift in LID technol-
ogy and has proven to be very successful.
It presents a 12.4% error rate reduction
over one of the best reported results on
the 1996 NIST Language Recognition
Evaluation database.
</bodyText>
<sectionHeader confidence="0.982409" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994247923076923">
Spoken language and written language are similar
in many ways. Therefore, much of the research in
spoken language identification, LID, has been in-
spired by text-categorization methodology. Both
text and voice are generated from language de-
pendent vocabulary. For example, both can be seen
as stochastic time-sequences corrupted by a chan-
nel noise. The n-gram language model has
achieved equal amounts of success in both tasks,
e.g. n-character slice for text categorization by lan-
guage (Cavnar and Trenkle, 1994) and Phone Rec-
ognition followed by n-gram Language Modeling,
or PRLM (Zissman, 1996) .
</bodyText>
<page confidence="0.973795">
515
</page>
<note confidence="0.991624">
Proceedings of the 43rd Annual Meeting of the ACL, pages 515–522,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.997775428571429">
issues. In Section 3 we propose the bag-of-sounds
paradigm to turn the LID task into a typical text
categorization problem. In Section 4, we study the
effects of different settings in experiments on the
1996 NIST Language Recognition Evaluation
(LRE) database2. In Section 5, we conclude our
study and discuss future work.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.982561764705882">
Formal evaluations conducted by the National In-
stitute of Science and Technology (NIST) in recent
years demonstrated that the most successful ap-
proach to LID used the phonotactic content of the
voice signal to discriminate between a set of lan-
guages (Singer et al., 2003). We briefly discuss
previous work cast in the formalism mentioned
above: tokenization, statistical language modeling,
and language identification. A typical LID system
is illustrated in Figure 1 (Zissman, 1996), where
language dependent voice tokenizers (VT) and lan-
guage models (LM) are deployed in the Parallel
PRLM architecture, or P-PRLM.
Figure 1. L monolingual phoneme recognition
front-ends are used in parallel to tokenize the input
utterance, which is analyzed by LMs to predict the
spoken language
</bodyText>
<subsectionHeader confidence="0.996348">
2.1 Voice Tokenization
</subsectionHeader>
<bodyText confidence="0.999942857142857">
A voice tokenizer is a speech recognizer that
converts a spoken document into a sequence of
tokens. As illustrated in Figure 2, a token can be of
different sizes, ranging from a speech feature
frame, to a phoneme, to a lexical word. A token is
defined to describe a distinct acoustic/phonetic
activity. In early research, low level spectral
</bodyText>
<footnote confidence="0.71476">
2 http://www.nist.gov/speech/tests/
</footnote>
<bodyText confidence="0.9997095">
frames, which are assumed to be independent of
each other, were used as a set of prototypical spec-
tra for each language (Sugiyama, 1991). By adopt-
ing hidden Markov models, people moved beyond
low-level spectral analysis towards modeling a
frame sequence into a larger unit such as a pho-
neme and even a lexical word.
Since the lexical word is language specific, the
phoneme becomes the natural choice when build-
ing a language-independent voice tokenization
front-end. Previous studies show that parallel lan-
guage-dependent phoneme tokenizers effectively
serve as the tokenization front-ends with P-PRLM
being the typical example. However, a language-
independent phoneme set has not been explored
yet experimentally. In this paper, we would like to
explore the potential of voice tokenization using a
unified phoneme set.
</bodyText>
<figure confidence="0.701323">
frame
</figure>
<figureCaption confidence="0.975745">
Figure 2 Tokenization at different resolutions
</figureCaption>
<subsectionHeader confidence="0.996464">
2.2 n-gram Language Model
</subsectionHeader>
<bodyText confidence="0.999868235294118">
With the sequence of tokens, we are able to es-
timate an n-gram language model (LM) from the
statistics. It is generally agreed that phonotactics,
i.e. the rules governing the phone/phonemes se-
quences admissible in a language, carry more lan-
guage discriminative information than the
phonemes themselves. An n-gram LM over the
tokens describes well n-local phonotactics among
neighboring tokens. While some systems model
the phonotactics at the frame level (Torres-
Carrasquillo et al., 2002), others have proposed P-
PRLM. The latter has become one of the most
promising solutions so far (Zissman, 1996).
A variety of cues can be used by humans and
machines to distinguish one language from another.
These cues include phonology, prosody, morphol-
ogy, and syntax in the context of an utterance.
</bodyText>
<figure confidence="0.997439428571428">
LM-1 ... LM-L
spoken utterance
VT-1: Chinese
VT-2: English
VT-L: French
LM-1 ... LM-L
LM-1 ... LM-L
LM-L: French
LM-L: French
LM-L: French
language classifier
hypothesized language
word
phoneme
</figure>
<page confidence="0.997486">
516
</page>
<bodyText confidence="0.999978458333333">
However, global phonotactic cues at the level of
utterance or spoken document remains unexplored
in previous work. In this paper, we pay special at-
tention to it. A spoken language always contains a
set of high frequency function words, prefixes, and
suffixes, which are realized as phonetic token sub-
strings in the spoken document. Individually, those
substrings may be shared across languages. How-
ever, the pattern of their co-occurrences discrimi-
nates one language from another.
Perceptual experiments have shown (Mut-
husamy, 1994) that with adequate training, human
listeners’ language identification ability increases
when given longer excerpts of speech. Experi-
ments have also shown that increased exposure to
each language and longer training sessions im-
prove listeners’ language identification perform-
ance. Although it is not entirely clear how human
listeners make use of the high-order phonotac-
tic/prosodic cues present in longer spans of a spo-
ken document, strong evidence shows that
phonotactics over larger context provides valuable
LID cues beyond n-gram, which will be further
attested by our experiments in Section 4.
</bodyText>
<subsectionHeader confidence="0.999741">
2.3 Language Classifier
</subsectionHeader>
<bodyText confidence="0.9999731">
The task of a language classifier is to make
good use of the LID cues that are encoded in the
model λl to hypothesize l from among L lan-
guages, Λ , as the one that is actually spoken in a
spoken document O. The LID model λl in P-
PRLM refers to extracted information from acous-
tic model and n-gram LM for language l. We have
λl = Jλl M ,λlL M } and λl ∈Λ (l=1,..., L) . A maxi-
mum-likelihood classifier can be formulated as
follows:
</bodyText>
<equation confidence="0.9632425">
ˆ
l = arg max ( / )
P O λl
l∈Λ
argmax∑P(O/T, λlAM)P(T/λLM) l
l∈Λ T∈Γ
</equation>
<bodyText confidence="0.997755">
The exact computation in Eq.(1) involves sum-
ming over all possible decoding of token se-
quences T given O. In many implementations,
∈Γ
it is approximated by the maximum over all se-
quences in the sum by finding the most likely to-
ken sequence, , for each language l, using the
Tˆl
</bodyText>
<equation confidence="0.985349777777778">
Viterbi algorithm:
≈ P O T λ P T λ
AM
ˆ arg max[ / ˆ ,
( ) ( ˆ / ]
LM
l )
l l l l
l∈Λ
</equation>
<bodyText confidence="0.992301290322581">
Intuitively, individual sounds are heavily shared
among different spoken languages due to the com-
mon speech production mechanism of humans.
Thus, the acoustic score has little language dis-
criminative ability. Many experiments (Yan and
Barnard, 1995; Zissman, 1996) have further at-
tested that the n-gram LM score provides more
language discriminative information than their
acoustic counterparts. In Figure 1, the decoding of
voice tokenization is governed by the acoustic
model AM
λl to arrive at an acoustic score
P(O / T , λAM) and a token sequence Tˆl . The n-
gram LM derives the n-local phonotactic score
P T l λ l from the language model LM
( ˆ / LM ) λl .
Clearly, the n-gram LM suffers the major short-
coming of having not exploited the global phono-
tactics in the larger context of a spoken utterance.
Speech recognition researchers have so far chosen
to only use n-gram local statistics for primarily
pragmatic reasons, as this n-gram is easier to attain.
In this work, a language independent voice tokeni-
zation front-end is proposed, that uses a unified
acoustic model AM
λ instead of multiple language
dependent acoustic models AM
λl . The n-gram
LM LM
λl is generalized to model both local and
global phonotactics.
</bodyText>
<sectionHeader confidence="0.99032" genericHeader="method">
3 Bag-of-Sounds Paradigm
</sectionHeader>
<bodyText confidence="0.999985578947369">
The bag-of-sounds concept is analogous to the
bag-of-words paradigm originally formulated in
the context of information retrieval (IR) and text
categorization (TC) (Salton 1971; Berry et al.,
1995; Chu-Caroll and Carpenter, 1999). One focus
of IR is to extract informative features for docu-
ment representation. The bag-of-words paradigm
represents a document as a vector of counts. It is
believed that it is not just the words, but also the
co-occurrence of words that distinguish semantic
domains of text documents.
Similarly, it is generally believed in LID that, al-
though the sounds of different spoken languages
overlap considerably, the phonotactics differenti-
ates one language from another. Therefore, one can
easily draw the analogy between an acoustic token
in bag-of-sounds and a word in bag-of-words.
Unlike words in a text document, the phonotactic
information that distinguishes spoken languages is
</bodyText>
<page confidence="0.976964">
517
</page>
<bodyText confidence="0.9998098">
concealed in the sound waves of spoken languages.
After transcribing a spoken document into a text
like document of tokens, many IR or TC tech-
niques can then be readily applied.
It is beyond the scope of this paper to discuss
what would be a good voice tokenizer. We adopt
phoneme size language-independent acoustic to-
kens to form a unified acoustic vocabulary in our
voice tokenizer. Readers are referred to (Ma et al.,
2005) for details of acoustic modeling.
</bodyText>
<subsectionHeader confidence="0.996011">
3.1 Vector Space Modeling
</subsectionHeader>
<bodyText confidence="0.99915501754386">
In human languages, some words invariably occur
more frequently than others. One of the most
common ways of expressing this idea is known as
Zipf’s Law (Zipf, 1949). This law states that there
is always a set of words which dominates most of
the other words of the language in terms of their
frequency of use. This is true both of written words
and of spoken words. The short-term, or local pho-
notactics, is devised to describe Zipf’s Law.
The local phonotactic constraints can be typi-
cally described by the token n-grams, or phoneme
n-grams as in (Ng et al., 2000), which represents
short-term statistics such as lexical constraints.
Suppose that we have a token sequence, t1 t2 t3 t4.
We derive the unigram statistics from the token
sequence itself. We derive the bigram statistics
from t1(t2) t2(t3) t3(t4) t4(#) where the token vo-
cabulary is expanded over the token’s right context.
Similarly, we derive the trigram statistics from the
t1(#,t2) t2(t1,t3) t3(t2,t4) t4(t3,#) to account for left
and right contexts. The # sign is a place holder for
free context. In the interest of manageability, we
propose to use up to token trigram. In this way, for
an acoustic system of Y tokens, we have poten-
tially Y2bigram and Y3trigram in the vocabulary.
Meanwhile, motivated by the ideas of having
both short-term and long-term phonotactic statis-
tics, we propose to derive global phonotactics in-
formation to account for long-term phonotactics:
The global phonotactic constraint is the high-
order statistics of n-grams. It represents document
level long-term phonotactics such as co-
occurrences of n-grams. By representing a spoken
document as a count vector of n-grams, also called
bag-of-sounds vector, it is possible to explore the
relations and higher-order statistics among the di-
verse n-grams through latent semantic analysis
(LSA).
It is often advantageous to weight the raw
counts to refine the contribution of each n-gram to
LID. We begin by normalizing the vectors repre-
senting the spoken document by making each vec-
tor of unit length. Our second weighting is based
on the notion that an n-gram that only occurs in a
few languages is more discriminative than an n-
gram that occurs in nearly every document. We use
the inverse-document frequency (idf) weighting
scheme (Spark Jones, 1972), in which a word is
weighted inversely to the number of documents in
which it occurs, by means of
idf (w) = log D / d (w) , where w is a word in the
vocabulary of W token n-grams. D is the total num-
ber of documents in the training corpus from L lan-
guages. Since each language has at least one
document in the training corpus, we have D ≥ L .
d(w) is the number of documents containing the
word w. Letting be the count of word w in
</bodyText>
<equation confidence="0.9581945">
cw,d
document d, we have the weighted count as
21/ 2
c ′ = ×
c idf w
w d ( )/( ∑ c ′ ) (3)
w d
, , w d ,
1≤′ ≤
w W
</equation>
<bodyText confidence="0.72110075">
and a vector cd = c′ d c′ d c ′ W d
{ 1, , 2, ,..., , }T to represent
document d. A corpus is then represented by a
term-document matrix H = {c1, c2,..., cD } of W × D.
</bodyText>
<subsectionHeader confidence="0.997646">
3.2 Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.99957025">
The fundamental idea in LSA is to reduce the
dimension of a document vector, W to Q, where
Q &lt;&lt; W and Q &lt;&lt; D , by projecting the problem
into the space spanned by the rows of the closest
rank-Q matrix to H in the Frobenius norm (Deer-
wester et al, 1990). Through singular value de-
composition (SVD) of H, we construct a modified
matrix HQ from the Q-largest singular values:
</bodyText>
<equation confidence="0.797752">
HQ = UQSQVQ T (4)
</equation>
<bodyText confidence="0.984250166666667">
UQ is a W × Q left singular matrix with rows
uw , 1 ≤ w≤ W; SQ is a Q × Q diagonal matrix of Q-
largest singular values of H; VQ is D × Q right sin-
gular matrix with rows vd , 1≤ d≤ D .
With the SVD, we project the D document vec-
tors in H into a reduced space , referred to as
</bodyText>
<sectionHeader confidence="0.372239" genericHeader="method">
VQ
</sectionHeader>
<bodyText confidence="0.997623333333333">
Q-space in the rest of this paper. A test document
cp of unknown language ID is mapped to a
pseudo-document in the Q-space by matrix
</bodyText>
<equation confidence="0.695143666666667">
vp UQ
518
cp v p c p U Q S−
T 1
→ = (5)
Q
</equation>
<bodyText confidence="0.920067666666667">
After SVD, it is straightforward to arrive at a
natural metric for the closeness between two spo-
ken documents and in Q-space instead of
</bodyText>
<equation confidence="0.8123546">
vi vj
their original W-dimensional space and .
ci cj
v⋅vT
g(ci,cj) ≈ cos(vi, vj) =  ||vi |i|⋅||jvj ||
</equation>
<bodyText confidence="0.998827538461538">
g(ci,cj) indicates the similarity between two vec-
tors, which can be transformed to a distance meas-
ure k(ci, cj) = cos−1 g(ci,cj) .
In the forced-choice classification, a test docu-
ment, supposedly monolingual, is classified into
one of the L languages. Note that the test document
is unknown to the H matrix. We assume consis-
tency between the test document’s intrinsic phono-
tactic pattern and one of the D patterns, that is
extracted from the training data and is presented in
the H matrix, so that the SVD matrices still apply
to the test document, and Eq.(5) still holds for di-
mension reduction.
</bodyText>
<subsectionHeader confidence="0.99969">
3.3 Bag-of-Sounds Language Classifier
</subsectionHeader>
<bodyText confidence="0.999643666666667">
The bag-of-sounds phonotactic LM benefits from
several properties of vector space modeling and
LSA.
</bodyText>
<listItem confidence="0.7719886875">
1) It allows for representing a spoken document
as a vector of n-gram features, such as unigram,
bigram, trigram, and the mixture of them;
2) It provides a well-defined distance metric for
measurement of phonotactic distance between
spoken documents;
3) It processes spoken documents in a lower di-
mensional Q-space, that makes the bag-of-
sounds phonotactic language modeling, λLM
and classification computationally manageable.
Suppose we have only one prototypical vector
cl and its projection in the Q-space to represent
vl
language l. Applying LSA to the term-document
matrix H: W × L, a minimum distance classifier is
formulated:
</listItem>
<equation confidence="0.979983833333333">
l = argmin ( p , l )
k v v (7)
l∈Λ
In Eq.(7), is the Q-space projection of , a test
vp cp
document.
</equation>
<bodyText confidence="0.998823230769231">
Apparently, it is very restrictive for each lan-
guage to have just one prototypical vector, also
referred to as a centroid. The pattern of language
distribution is inherently multi-modal, so it is
unlikely well fitted by a single vector. One solution
to this problem is to span the language space with
multiple vectors. Applying LSA to a term-
document matrix H: W × L′, where L L
′ = × M as-
suming each language l is represented by a set of
M vectors, Φl , a new classifier, using k-nearest
neighboring rule (Duda and Hart, 1973) , is formu-
lated, named k-nearest classifier (KNC):
</bodyText>
<equation confidence="0.9987665">
l = arg min ∑ k(vp, vl′ ) (8)
l∈Λ l′∈ φl
</equation>
<bodyText confidence="0.977433833333333">
where φl is the set of k-nearest-neighbor to vp and
φl ⊂ Φ l.
Among many ways to derive the M centroid vec-
tors, here is one option. Suppose that we have a set
of training documents Dl for language l , as subset
of corpus Ω , Dl ⊂ Ω and ∪ l = Dl = Ω . To derive
</bodyText>
<equation confidence="0.847663571428572">
L 1
the M vectors, we choose to carry out vector quan-
tization (VQ) to partition Dl into M cells Dl,m in the
Q-space such that 1 ,
∪m = Dl m = D using similarity
M
l
</equation>
<bodyText confidence="0.997130730769231">
metric Eq.(6). All the documents in each cell
Dl,m can then be merged to form a super-document,
which is further projected into a Q-space vector
vl,m . This results in M prototypical centroids
vl,m∈Φl (m =1,...M) . Using KNC, a test vector is
compared with M vectors to arrive at the k-nearest
neighbors for each language, which can be compu-
tationally expensive when M is large.
Alternatively, one can account for multi-modal
distribution through finite mixture model. A mix-
ture model is to represent the M discrete compo-
nents with soft combination. To extend the KNC
into a statistical framework, it is necessary to map
our distance metric Eq.(6) into a probability meas-
ure. One way is for the distance measure to induce
a family of exponential distributions with pertinent
marginality constraints. In practice, what we need
is a reasonable probability distribution, which
sums to one, to act as a lookup table for the dis-
tance measure. We here choose to use the empiri-
cal multivariate distribution constructed by
allocating the total probability mass in proportion
to the distances observed with the training data. In
short, this reduces the task to a histogram normali-
zation. In this way, we map the distance k(ci, cj )
to a conditional probability distribution p(vi  |vj )
</bodyText>
<equation confidence="0.8271628">
(6)
519
 ||
Ω
subject to ∑ =
</equation>
<bodyText confidence="0.974106">
i 1 p(vi  |vj) =1. Now that we are in the
probability domain, techniques such as mixture
smoothing can be readily applied to model a lan-
guage class with finer fitting.
Let’s re-visit the task of L language forced-
choice classification. Similar to KNC, suppose we
have M centroids vl , m ∈Φl (m =1,...M) in the Q-
space for each language l. Each centroid represents
a class. The class conditional probability can be
described as a linear combination of p(vi  |vl ,m) :
</bodyText>
<figureCaption confidence="0.934059333333333">
Figure 3. A bag-of-sounds classifier. A unified
front-end followed by L parallel bag-of-sounds
phonotactic LMs.
</figureCaption>
<figure confidence="0.988423">
Unified VT
λ
AM
spoken utterance
λLM-1: Chinese
LM 1
λlLM LM-L: French
λ LM-2: English
LM
2
Language Classifier
Hypothesized language
M (9) 4 Experiments
p v λ
(  |)
LM = ∑ p v p v ,
( ) (  |)
v
i l l m
, i l m
1
=
m
</figure>
<bodyText confidence="0.934720142857143">
the probability p(vl ,m , functionally serves as a
)
mixture weight of p(vi  |vl ,m) . Together with a set
of centroids vl ,m ∈Φl (m =1,...M) , p(vi  |vl ,m) and
p(vl ,m define a mixture model LM
) λl . p(vi  |vl,m )
is estimated by histogram normalization and
p(vl ,m) is estimated under the maximum likelihood
criteria, p(vl,m) = Cm,l / Cl , where Cl is total
number of documents in Dl, of which Cm ,l docu-
ments fall into the cell m.
An Expectation-Maximization iterative process
can be devised for training of LM
λl to maximize the
</bodyText>
<equation confidence="0.995586222222222">
likelihood Eq.(9) over the entire training corpus:
L  ||
Dl
(  |)
Ω Λ =∏∏ p v λ
(  |)
LM
p d l
1 d=1
</equation>
<bodyText confidence="0.99914">
Using the phonotactic LM score P(T / λ LM ) for
classification, with T being represented
lby the
bag-of-sounds vector vp , Eq.(2) can be reformu-
lated as Eq.(11), named mixture-model classifier
(MMC):
</bodyText>
<equation confidence="0.9885688">
ˆ
l = arg max p(vλ
p l
l∈Λ
(11)
</equation>
<bodyText confidence="0.999049423076923">
To establish fair comparison with P-PRLM, as
shown in Figure 3, we devise our bag-of-sounds
classifier to solely use the LM score
P(T / λ LM ) for classification decision whereas the
acoustic score P(O/T, λlAM ) may potentially help
as reported in (Singer et al., 2003).
This section will experimentally analyze the per-
formance of the proposed bag-of-sounds frame-
work using the 1996 NIST Language Recognition
Evaluation (LRE) data. The database was intended
to establish a baseline of performance capability
for language recognition of conversational tele-
phone speech. The database contains recorded
speech of 12 languages: Arabic, English, Farsi,
French, German, Hindi, Japanese, Korean, Manda-
rin, Spanish, Tamil and Vietnamese. We use the
training set and development set from LDC Call-
Friend corpus3 as the training data. Each conversa-
tion is segmented into overlapping sessions of
about 30 seconds each, resulting in about 12,000
sessions for each language. The evaluation set con-
sists of 1,492 30-sec sessions, each distributed
among the various languages of interest. We treat a
30-sec session as a spoken document in both train-
ing and testing. We report error rates (ER) of the
1,492 test trials.
</bodyText>
<subsectionHeader confidence="0.999865">
4.1 Effect of Acoustic Vocabulary
</subsectionHeader>
<bodyText confidence="0.9998691">
The choice of n-gram affects the performance of
LID systems. Here we would like to see how a bet-
ter choice of acoustic vocabulary can help convert
a spoken document into a phonotactically dis-
criminative space. There are two parameters that
determine the acoustic vocabulary: the choice of
acoustic token, and the choice of n-grams. In this
paper, the former concerns the size of an acoustic
system Y in the unified front-end. It is studied in
more details in (Ma et al., 2005). We set Y to 32 in
</bodyText>
<footnote confidence="0.99774375">
3 See http://www.ldc.upenn.edu/. The overlap between 1996
NIST evaluation data and CallFriend database has been re-
moved from training data as suggested in the 2003 NIST LRE
website http://www.nist.gov/speech/tests/index.htm
</footnote>
<figure confidence="0.919779375">
(10)
l=
M
= ∑
arg max p(vl,m)p(vp  |vm)
,
l∈Λ m=
1
</figure>
<page confidence="0.985125">
520
</page>
<bodyText confidence="0.925878785714286">
this experiment; the latter decides what features to
be included in the vector space. The vector space
modeling allows for multiple heterogeneous fea-
tures in one vector. We introduce three types of
acoustic vocabulary (AV) with mixture of token
unigram, bigram, and trigram:
a) AV1: 32 broad class phonemes as unigram,
selected from 12 languages, also referred to as
P-ASM as detailed in (Ma et al., 2005)
b) AV2: AV1 augmented by 32 bigrams of
x 32
AV1, amounting to 1,056 tokens
c) AV3: AV2 augmented by 32 x 32x 32 tri-
grams of AV1, amounting to 33,824 tokens
</bodyText>
<table confidence="0.9705025">
AV1 AV2 AV3
ER % 46.1 32.8 28.3
</table>
<tableCaption confidence="0.999581">
Table 1. Effect of acoustic vocabulary (KNC)
</tableCaption>
<bodyText confidence="0.999981375">
We carry out experiments with KNC classifier
of 4,800 centroids. Applying k-nearest-neighboring
rule, k is empirically set to 3. The error rates are
reported in Table 1 for the experiments over the
three AV types. It is found that high-order token n-
grams improve LID performance. This reaffirms
many previous findings that n-gram phonotactics
serves as a valuable cue in LID.
</bodyText>
<subsectionHeader confidence="0.992886">
4.2 Effect of Model Size
</subsectionHeader>
<bodyText confidence="0.999354090909091">
As discussed in KNC, one would expect to im-
prove the phonotactic model by using more cen-
troids. Let’s examine how the number of centroid
vectors M affects the performance of KNC. We set
the acoustic system size Y to 128, k-nearest to 3,
and only use token bigrams in the bag-of-sounds
vector. In Table 2, it is not surprising to find that
the performance improves as M increases. How-
ever, it is not practical to have large M be-
cause L′ = L x M comparisons need to take place in
each test trial.
</bodyText>
<table confidence="0.902041">
#M 1,200 2,400 4,800 12,000
ER % 17.0 15.7 15.4 14.8
</table>
<tableCaption confidence="0.99776">
Table 2. Effect of number of centroids (KNC)
</tableCaption>
<bodyText confidence="0.999710777777778">
To reduce computation, MMC attempts to use
less number of mixtures M to represent the phono-
tactic space. With the smoothing effect of the mix-
ture model, we expect to use less computation to
achieve similar performance as KNC. In the ex-
periment reported in Table 3, we find that MMC
(M=1,024) achieves 14.9% error rate, which al-
most equalizes the best result in the KNC experi-
ment (M=12,000) with much less computation.
</bodyText>
<table confidence="0.7703085">
#M 4 16 64 256 1,024
ER % 29.6 26.4 19.7 16.0 14.9
</table>
<tableCaption confidence="0.99766">
Table 3. Effect of number of mixtures (MMC)
</tableCaption>
<subsectionHeader confidence="0.944725">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999159260869565">
The bag-of-sounds approach has achieved equal
success in both 1996 and 2003 NIST LRE data-
bases. As more results are published on the 1996
NIST LRE database, we choose it as the platform
of comparison. In Table 4, we report the perform-
ance across different approaches in terms of error
rate for a quick comparison. MMC presents a
12.4% ER reduction over the best reported result4
(Torres-Carrasquillo et al., 2002).
It is interesting to note that the bag-of-sounds
classifier outperforms its P-PRLM counterpart by a
wide margin (14.9% vs 22.0%). This is attributed
to the global phonotactic features in LM
Al . The
performance gain in (Torres-Carrasquillo et al.,
2002; Singer et al., 2003) was obtained mainly by
fusing scores from several classifiers, namely
GMM, P-PRLM and SVM, to benefit from both
acoustic and language model scores. Noting that
the bag-of-sounds classifier in this work solely re-
lies on the LM score, it is believed that fusing with
scores from other classifiers will further boost the
LID performance.
</bodyText>
<table confidence="0.992870333333333">
ER %
P-PRLM5 22.0
P-PRLM + GMM acoustic5 19.5
P-PRLM + GMM acoustic + 17.0
GMM tokenizer5
Bag-of-sounds classifier (MMC) 14.9
</table>
<tableCaption confidence="0.998927">
Table 4. Benchmark of different approaches
</tableCaption>
<bodyText confidence="0.9996645">
Besides the error rate reduction, the bag-of-
sounds approach also simplifies the on-line com-
puting procedure over its P-PRLM counterpart. It
would be interesting to estimate the on-line com-
putational need of MMC. The cost incurred has
two main components: 1) the construction of the
</bodyText>
<footnote confidence="0.75434275">
4 Previous results are also reported in DCF, DET, and equal
error rate (EER). Comprehensive benchmarking for bag-of-
sounds phonotactic LM will be reported soon.
5 Results extracted from (Torres-Carrasquillo et al., 2002)
</footnote>
<page confidence="0.993305">
521
</page>
<bodyText confidence="0.971577777777778">
pseudo document vector, as done via Eq.(5); 2)
L′ = L × M vector comparisons. The computing
cost is estimated to be O(Q2) per test trial
(Bellegarda, 2000). For typical values of Q, this
amounts to less than 0.05 Mflops. While this is
more expensive than the usual table look-up in
conventional n-gram LM, the performance im-
provement is able to justify the relatively modest
computing overhead.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985631578948">
We have proposed a phonotactic LM approach to
LID problem. The concept of bag-of-sounds is in-
troduced, for the first time, to model phonotactics
present in a spoken language over a larger context.
With bag-of-sounds phonotactic LM, a spoken
document can be treated as a text-like document of
acoustic tokens. This way, the well-established
LSA technique can be readily applied. This novel
approach not only suggests a paradigm shift in LID,
but also brings 12.4% error rate reduction over one
of the best reported results on the 1996 NIST LRE
data. It has proven to be very successful.
We would like to extend this approach to other
spoken document categorization tasks. In monolin-
gual spoken document categorization, we suggest
that the semantic domain can be characterized by
latent phonotactic features. Thus it is straightfor-
ward to extend the proposed bag-of-sounds frame-
work to spoken document categorization.
</bodyText>
<sectionHeader confidence="0.974518" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9993588">
The authors are grateful to Dr. Alvin F. Martin of
the NIST Speech Group for his advice when pre-
paring the 1996 NIST LRE experiments, to Dr G.
M. White and Ms Y. Chen of Institute for Info-
comm Research for insightful discussions.
</bodyText>
<sectionHeader confidence="0.998921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999499896551724">
Jerome R. Bellegarda. 2000. Exploiting latent semantic
information in statistical language modeling, In Proc.
of the IEEE, 88(8):1279-1296.
M. W. Berry, S.T. Dumais and G.W. O’Brien. 1995.
Using Linear Algebra for intelligent information re-
trieval, SIAM Review, 37(4):573-595.
William B. Cavnar, and John M. Trenkle. 1994. N-
Gram-Based Text Categorization, In Proc. of 3rd
Annual Symposium on Document Analysis and In-
formation Retrieval, pp. 161-169.
Jennifer Chu-Carroll, and Bob Carpenter. 1999. Vector-
based Natural Language Call Routing, Computa-
tional Linguistics, 25(3):361-388.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman, 1990, Indexing by latent semantic
analysis, Journal of the American Society for Infor-
matin Science, 41(6):391-407
Richard O. Duda and Peter E. Hart. 1973. Pattern Clas-
sification and scene analysis. John Wiley &amp; Sons
James L. Hieronymus. 1994. ASCII Phonetic Symbols
for the World’s Languages: Worldbet. Technical Re-
port AT&amp;T Bell Labs.
Spark Jones, K. 1972. A statistical interpretation of
term specificity and its application in retrieval, Jour-
nal of Documentation, 28:11-20
Bin Ma, Haizhou Li and Chin-Hui Lee, 2005. An Acous-
tic Segment Modeling Approach to Automatic Lan-
guage Identification, submitted to Interspeech 2005
Yeshwant K. Muthusamy, Neena Jain, and Ronald A.
Cole. 1994. Perceptual benchmarks for automatic
language identification, In Proc. of ICASSP
Corinna Ng , Ross Wilkinson , Justin Zobel, 2000. Ex-
periments in spoken document retrieval using pho-
neme n-grams , Speech Communication, 32(1-2):61-
77
G. Salton, 1971. The SMART Retrieval System, Pren-
tice-Hall, Englewood Cliffs, NJ, 1971
E. Singer, P.A. Torres-Carrasquillo, T.P. Gleason, W.M.
Campbell and D.A. Reynolds. 2003. Acoustic, Pho-
netic and Discriminative Approaches to Automatic
language recognition, In Proc. of Eurospeech
Masahide Sugiyama. 1991. Automatic language recog-
nition using acoustic features, In Proc. of ICASSP.
Pedro A. Torres-Carrasquillo, Douglas A. Reynolds,
and J.R. Deller. Jr. 2002. Language identification us-
ing Gaussian Mixture model tokenization, in Proc. of
ICASSP.
Yonghong Yan, and Etienne Barnard. 1995. An ap-
proach to automatic language identification based on
language dependent phone recognition, In Proc. of
ICASSP.
George K. Zipf. 1949. Human Behavior and the Princi-
pal of Least effort, an introduction to human ecology.
Addison-Wesley, Reading, Mass.
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of
telephone speech, IEEE Trans. on Speech and Audio
Processing, 4(1):31-44.
</reference>
<page confidence="0.997392">
522
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324104">
<title confidence="0.999974">A Phonotactic Language Model for Spoken Language Identification</title>
<author confidence="0.999609">Li Ma</author>
<affiliation confidence="0.999218">Institute for Infocomm Research</affiliation>
<address confidence="0.759887">Singapore 119613</address>
<email confidence="0.489573">hli@i2r.a-star.edu.sg</email>
<email confidence="0.489573">mabin@i2r.a-star.edu.sg</email>
<abstract confidence="0.995262303030303">Orthographic forms of language, ranging from Latin alphabet to Cyrillic script to Chinese characters, are far more unique to the language than their phonetic counterparts. From the speech production point of view, thousands of spoken languages from all over the world are phonetically articulated using only a few hundred distinctive sounds or phonemes (Hieronymus, 1994). In other words, common sounds are shared considerably across different spoken languages. In addition, spoken in the form of digitized wave files, are far less structured than written documents and need to be treated with techniques that go beyond the bounds of written language. All of this makes the identification of spoken language based on phonetic units much more challenging than the identification of written language. In fact, the challenge of LID is inter-disciplinary, involving digital signal processing, speech recognition and natural language processing. In general, a LID system usually has three fundamental components as follows: 1) A voice tokenizer which segments incoming voice feature frames and associates the segments with acoustic or phonetic labels, called tokens; 2) A statistical language model which captures language dependent phonetic and phonotactic information from the sequences of tokens; 3) A language classifier which identifies the language based on discriminatory characteristics of acoustic score from the voice tokenizer and phonotactic score from the language model. In this paper, we present a novel solution to the three problems, focusing on the second and third problems from a computational linguistic perspective. The paper is organized as follows: In Section 2, we summarize relevant existing approaches to the LID task. We highlight the shortcomings of existing approaches and our attempts to address the spoken utterance is regarded as a spoken document in this paper. Abstract We have established a phonotactic language model as the solution to spoken language identification (LID). In this framework, we define a single set of acoustic tokens to represent the acoustic activities in the world’s spoken languages. A voice tokenizer converts a spoken document into a text-like document of acoustic tokens. Thus a spoken document can be represented by a count vector of tokens and token in the space. We apply semantic the vectors, in the same way that it is applied in information retrieval, in order to capture salient phonotactics present in spoken documents. The vector space modeling of spoken utterances constitutes a paradigm shift in LID technology and has proven to be very successful. It presents a 12.4% error rate reduction over one of the best reported results on the 1996 NIST Language Recognition Evaluation database.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Exploiting latent semantic information in statistical language modeling,</title>
<date>2000</date>
<booktitle>In Proc. of the IEEE,</booktitle>
<pages>88--8</pages>
<contexts>
<context position="27415" citStr="Bellegarda, 2000" startWordPosition="4717" endWordPosition="4718">sounds approach also simplifies the on-line computing procedure over its P-PRLM counterpart. It would be interesting to estimate the on-line computational need of MMC. The cost incurred has two main components: 1) the construction of the 4 Previous results are also reported in DCF, DET, and equal error rate (EER). Comprehensive benchmarking for bag-ofsounds phonotactic LM will be reported soon. 5 Results extracted from (Torres-Carrasquillo et al., 2002) 521 pseudo document vector, as done via Eq.(5); 2) L′ = L × M vector comparisons. The computing cost is estimated to be O(Q2) per test trial (Bellegarda, 2000). For typical values of Q, this amounts to less than 0.05 Mflops. While this is more expensive than the usual table look-up in conventional n-gram LM, the performance improvement is able to justify the relatively modest computing overhead. 5 Conclusion We have proposed a phonotactic LM approach to LID problem. The concept of bag-of-sounds is introduced, for the first time, to model phonotactics present in a spoken language over a larger context. With bag-of-sounds phonotactic LM, a spoken document can be treated as a text-like document of acoustic tokens. This way, the well-established LSA tec</context>
</contexts>
<marker>Bellegarda, 2000</marker>
<rawString>Jerome R. Bellegarda. 2000. Exploiting latent semantic information in statistical language modeling, In Proc. of the IEEE, 88(8):1279-1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>S T Dumais</author>
<author>G W O’Brien</author>
</authors>
<title>Using Linear Algebra for intelligent information retrieval,</title>
<date>1995</date>
<journal>SIAM Review,</journal>
<pages>37--4</pages>
<marker>Berry, Dumais, O’Brien, 1995</marker>
<rawString>M. W. Berry, S.T. Dumais and G.W. O’Brien. 1995. Using Linear Algebra for intelligent information retrieval, SIAM Review, 37(4):573-595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>NGram-Based Text Categorization,</title>
<date>1994</date>
<booktitle>In Proc. of 3rd Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--169</pages>
<contexts>
<context position="3505" citStr="Cavnar and Trenkle, 1994" startWordPosition="539" endWordPosition="542"> over one of the best reported results on the 1996 NIST Language Recognition Evaluation database. 1 Introduction Spoken language and written language are similar in many ways. Therefore, much of the research in spoken language identification, LID, has been inspired by text-categorization methodology. Both text and voice are generated from language dependent vocabulary. For example, both can be seen as stochastic time-sequences corrupted by a channel noise. The n-gram language model has achieved equal amounts of success in both tasks, e.g. n-character slice for text categorization by language (Cavnar and Trenkle, 1994) and Phone Recognition followed by n-gram Language Modeling, or PRLM (Zissman, 1996) . 515 Proceedings of the 43rd Annual Meeting of the ACL, pages 515–522, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics issues. In Section 3 we propose the bag-of-sounds paradigm to turn the LID task into a typical text categorization problem. In Section 4, we study the effects of different settings in experiments on the 1996 NIST Language Recognition Evaluation (LRE) database2. In Section 5, we conclude our study and discuss future work. 2 Related Work Formal evaluations conducted by th</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar, and John M. Trenkle. 1994. NGram-Based Text Categorization, In Proc. of 3rd Annual Symposium on Document Analysis and Information Retrieval, pp. 161-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Bob Carpenter</author>
</authors>
<date>1999</date>
<journal>Vectorbased Natural Language Call Routing, Computational Linguistics,</journal>
<pages>25--3</pages>
<marker>Chu-Carroll, Carpenter, 1999</marker>
<rawString>Jennifer Chu-Carroll, and Bob Carpenter. 1999. Vectorbased Natural Language Call Routing, Computational Linguistics, 25(3):361-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis,</title>
<date>1990</date>
<journal>Journal of the American Society for Informatin Science,</journal>
<pages>41--6</pages>
<contexts>
<context position="15091" citStr="Deerwester et al, 1990" startWordPosition="2508" endWordPosition="2512"> containing the word w. Letting be the count of word w in cw,d document d, we have the weighted count as 21/ 2 c ′ = × c idf w w d ( )/( ∑ c ′ ) (3) w d , , w d , 1≤′ ≤ w W and a vector cd = c′ d c′ d c ′ W d { 1, , 2, ,..., , }T to represent document d. A corpus is then represented by a term-document matrix H = {c1, c2,..., cD } of W × D. 3.2 Latent Semantic Analysis The fundamental idea in LSA is to reduce the dimension of a document vector, W to Q, where Q &lt;&lt; W and Q &lt;&lt; D , by projecting the problem into the space spanned by the rows of the closest rank-Q matrix to H in the Frobenius norm (Deerwester et al, 1990). Through singular value decomposition (SVD) of H, we construct a modified matrix HQ from the Q-largest singular values: HQ = UQSQVQ T (4) UQ is a W × Q left singular matrix with rows uw , 1 ≤ w≤ W; SQ is a Q × Q diagonal matrix of Qlargest singular values of H; VQ is D × Q right singular matrix with rows vd , 1≤ d≤ D . With the SVD, we project the D document vectors in H into a reduced space , referred to as VQ Q-space in the rest of this paper. A test document cp of unknown language ID is mapped to a pseudo-document in the Q-space by matrix vp UQ 518 cp v p c p U Q S− T 1 → = (5) Q After SVD</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman, 1990, Indexing by latent semantic analysis, Journal of the American Society for Informatin Science, 41(6):391-407</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and scene analysis.</title>
<date>1973</date>
<publisher>John Wiley &amp; Sons</publisher>
<contexts>
<context position="17912" citStr="Duda and Hart, 1973" startWordPosition="3030" endWordPosition="3033">gmin ( p , l ) k v v (7) l∈Λ In Eq.(7), is the Q-space projection of , a test vp cp document. Apparently, it is very restrictive for each language to have just one prototypical vector, also referred to as a centroid. The pattern of language distribution is inherently multi-modal, so it is unlikely well fitted by a single vector. One solution to this problem is to span the language space with multiple vectors. Applying LSA to a termdocument matrix H: W × L′, where L L ′ = × M assuming each language l is represented by a set of M vectors, Φl , a new classifier, using k-nearest neighboring rule (Duda and Hart, 1973) , is formulated, named k-nearest classifier (KNC): l = arg min ∑ k(vp, vl′ ) (8) l∈Λ l′∈ φl where φl is the set of k-nearest-neighbor to vp and φl ⊂ Φ l. Among many ways to derive the M centroid vectors, here is one option. Suppose that we have a set of training documents Dl for language l , as subset of corpus Ω , Dl ⊂ Ω and ∪ l = Dl = Ω . To derive L 1 the M vectors, we choose to carry out vector quantization (VQ) to partition Dl into M cells Dl,m in the Q-space such that 1 , ∪m = Dl m = D using similarity M l metric Eq.(6). All the documents in each cell Dl,m can then be merged to form a s</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Richard O. Duda and Peter E. Hart. 1973. Pattern Classification and scene analysis. John Wiley &amp; Sons</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L Hieronymus</author>
</authors>
<title>ASCII Phonetic Symbols for the World’s Languages: Worldbet.</title>
<date>1994</date>
<tech>Technical Report</tech>
<institution>AT&amp;T Bell Labs.</institution>
<marker>Hieronymus, 1994</marker>
<rawString>James L. Hieronymus. 1994. ASCII Phonetic Symbols for the World’s Languages: Worldbet. Technical Report AT&amp;T Bell Labs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spark Jones</author>
<author>K</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval,</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<pages>28--11</pages>
<marker>Jones, K, 1972</marker>
<rawString>Spark Jones, K. 1972. A statistical interpretation of term specificity and its application in retrieval, Journal of Documentation, 28:11-20</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Ma</author>
<author>Haizhou Li</author>
<author>Chin-Hui Lee</author>
</authors>
<title>An Acoustic Segment Modeling Approach to Automatic Language Identification, submitted to Interspeech</title>
<date>2005</date>
<contexts>
<context position="11745" citStr="Ma et al., 2005" startWordPosition="1892" endWordPosition="1895">aw the analogy between an acoustic token in bag-of-sounds and a word in bag-of-words. Unlike words in a text document, the phonotactic information that distinguishes spoken languages is 517 concealed in the sound waves of spoken languages. After transcribing a spoken document into a text like document of tokens, many IR or TC techniques can then be readily applied. It is beyond the scope of this paper to discuss what would be a good voice tokenizer. We adopt phoneme size language-independent acoustic tokens to form a unified acoustic vocabulary in our voice tokenizer. Readers are referred to (Ma et al., 2005) for details of acoustic modeling. 3.1 Vector Space Modeling In human languages, some words invariably occur more frequently than others. One of the most common ways of expressing this idea is known as Zipf’s Law (Zipf, 1949). This law states that there is always a set of words which dominates most of the other words of the language in terms of their frequency of use. This is true both of written words and of spoken words. The short-term, or local phonotactics, is devised to describe Zipf’s Law. The local phonotactic constraints can be typically described by the token n-grams, or phoneme n-gra</context>
<context position="23100" citStr="Ma et al., 2005" startWordPosition="3974" endWordPosition="3977">n as a spoken document in both training and testing. We report error rates (ER) of the 1,492 test trials. 4.1 Effect of Acoustic Vocabulary The choice of n-gram affects the performance of LID systems. Here we would like to see how a better choice of acoustic vocabulary can help convert a spoken document into a phonotactically discriminative space. There are two parameters that determine the acoustic vocabulary: the choice of acoustic token, and the choice of n-grams. In this paper, the former concerns the size of an acoustic system Y in the unified front-end. It is studied in more details in (Ma et al., 2005). We set Y to 32 in 3 See http://www.ldc.upenn.edu/. The overlap between 1996 NIST evaluation data and CallFriend database has been removed from training data as suggested in the 2003 NIST LRE website http://www.nist.gov/speech/tests/index.htm (10) l= M = ∑ arg max p(vl,m)p(vp |vm) , l∈Λ m= 1 520 this experiment; the latter decides what features to be included in the vector space. The vector space modeling allows for multiple heterogeneous features in one vector. We introduce three types of acoustic vocabulary (AV) with mixture of token unigram, bigram, and trigram: a) AV1: 32 broad class phon</context>
</contexts>
<marker>Ma, Li, Lee, 2005</marker>
<rawString>Bin Ma, Haizhou Li and Chin-Hui Lee, 2005. An Acoustic Segment Modeling Approach to Automatic Language Identification, submitted to Interspeech 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yeshwant K Muthusamy</author>
<author>Neena Jain</author>
<author>Ronald A Cole</author>
</authors>
<title>Perceptual benchmarks for automatic language identification,</title>
<date>1994</date>
<booktitle>In Proc. of ICASSP</booktitle>
<marker>Muthusamy, Jain, Cole, 1994</marker>
<rawString>Yeshwant K. Muthusamy, Neena Jain, and Ronald A. Cole. 1994. Perceptual benchmarks for automatic language identification, In Proc. of ICASSP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Wilkinson</author>
</authors>
<title>Experiments in spoken document retrieval using phoneme n-grams ,</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>32--1</pages>
<marker>Wilkinson, 2000</marker>
<rawString>Corinna Ng , Ross Wilkinson , Justin Zobel, 2000. Experiments in spoken document retrieval using phoneme n-grams , Speech Communication, 32(1-2):61-77</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>The SMART Retrieval System, Prentice-Hall,</title>
<date>1971</date>
<location>Englewood Cliffs, NJ,</location>
<contexts>
<context position="10580" citStr="Salton 1971" startWordPosition="1706" endWordPosition="1707">recognition researchers have so far chosen to only use n-gram local statistics for primarily pragmatic reasons, as this n-gram is easier to attain. In this work, a language independent voice tokenization front-end is proposed, that uses a unified acoustic model AM λ instead of multiple language dependent acoustic models AM λl . The n-gram LM LM λl is generalized to model both local and global phonotactics. 3 Bag-of-Sounds Paradigm The bag-of-sounds concept is analogous to the bag-of-words paradigm originally formulated in the context of information retrieval (IR) and text categorization (TC) (Salton 1971; Berry et al., 1995; Chu-Caroll and Carpenter, 1999). One focus of IR is to extract informative features for document representation. The bag-of-words paradigm represents a document as a vector of counts. It is believed that it is not just the words, but also the co-occurrence of words that distinguish semantic domains of text documents. Similarly, it is generally believed in LID that, although the sounds of different spoken languages overlap considerably, the phonotactics differentiates one language from another. Therefore, one can easily draw the analogy between an acoustic token in bag-of-</context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>G. Salton, 1971. The SMART Retrieval System, Prentice-Hall, Englewood Cliffs, NJ, 1971</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Singer</author>
<author>P A Torres-Carrasquillo</author>
<author>T P Gleason</author>
<author>W M Campbell</author>
<author>D A Reynolds</author>
</authors>
<title>Acoustic, Phonetic and Discriminative Approaches to Automatic language recognition,</title>
<date>2003</date>
<booktitle>In Proc. of Eurospeech</booktitle>
<contexts>
<context position="4342" citStr="Singer et al., 2003" startWordPosition="674" endWordPosition="677">inguistics issues. In Section 3 we propose the bag-of-sounds paradigm to turn the LID task into a typical text categorization problem. In Section 4, we study the effects of different settings in experiments on the 1996 NIST Language Recognition Evaluation (LRE) database2. In Section 5, we conclude our study and discuss future work. 2 Related Work Formal evaluations conducted by the National Institute of Science and Technology (NIST) in recent years demonstrated that the most successful approach to LID used the phonotactic content of the voice signal to discriminate between a set of languages (Singer et al., 2003). We briefly discuss previous work cast in the formalism mentioned above: tokenization, statistical language modeling, and language identification. A typical LID system is illustrated in Figure 1 (Zissman, 1996), where language dependent voice tokenizers (VT) and language models (LM) are deployed in the Parallel PRLM architecture, or P-PRLM. Figure 1. L monolingual phoneme recognition front-ends are used in parallel to tokenize the input utterance, which is analyzed by LMs to predict the spoken language 2.1 Voice Tokenization A voice tokenizer is a speech recognizer that converts a spoken docu</context>
<context position="21656" citStr="Singer et al., 2003" startWordPosition="3742" endWordPosition="3745">M λl to maximize the likelihood Eq.(9) over the entire training corpus: L || Dl ( |) Ω Λ =∏∏ p v λ ( |) LM p d l 1 d=1 Using the phonotactic LM score P(T / λ LM ) for classification, with T being represented lby the bag-of-sounds vector vp , Eq.(2) can be reformulated as Eq.(11), named mixture-model classifier (MMC): ˆ l = arg max p(vλ p l l∈Λ (11) To establish fair comparison with P-PRLM, as shown in Figure 3, we devise our bag-of-sounds classifier to solely use the LM score P(T / λ LM ) for classification decision whereas the acoustic score P(O/T, λlAM ) may potentially help as reported in (Singer et al., 2003). This section will experimentally analyze the performance of the proposed bag-of-sounds framework using the 1996 NIST Language Recognition Evaluation (LRE) data. The database was intended to establish a baseline of performance capability for language recognition of conversational telephone speech. The database contains recorded speech of 12 languages: Arabic, English, Farsi, French, German, Hindi, Japanese, Korean, Mandarin, Spanish, Tamil and Vietnamese. We use the training set and development set from LDC CallFriend corpus3 as the training data. Each conversation is segmented into overlappi</context>
<context position="26249" citStr="Singer et al., 2003" startWordPosition="4527" endWordPosition="4530">d 2003 NIST LRE databases. As more results are published on the 1996 NIST LRE database, we choose it as the platform of comparison. In Table 4, we report the performance across different approaches in terms of error rate for a quick comparison. MMC presents a 12.4% ER reduction over the best reported result4 (Torres-Carrasquillo et al., 2002). It is interesting to note that the bag-of-sounds classifier outperforms its P-PRLM counterpart by a wide margin (14.9% vs 22.0%). This is attributed to the global phonotactic features in LM Al . The performance gain in (Torres-Carrasquillo et al., 2002; Singer et al., 2003) was obtained mainly by fusing scores from several classifiers, namely GMM, P-PRLM and SVM, to benefit from both acoustic and language model scores. Noting that the bag-of-sounds classifier in this work solely relies on the LM score, it is believed that fusing with scores from other classifiers will further boost the LID performance. ER % P-PRLM5 22.0 P-PRLM + GMM acoustic5 19.5 P-PRLM + GMM acoustic + 17.0 GMM tokenizer5 Bag-of-sounds classifier (MMC) 14.9 Table 4. Benchmark of different approaches Besides the error rate reduction, the bag-ofsounds approach also simplifies the on-line computi</context>
</contexts>
<marker>Singer, Torres-Carrasquillo, Gleason, Campbell, Reynolds, 2003</marker>
<rawString>E. Singer, P.A. Torres-Carrasquillo, T.P. Gleason, W.M. Campbell and D.A. Reynolds. 2003. Acoustic, Phonetic and Discriminative Approaches to Automatic language recognition, In Proc. of Eurospeech</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahide Sugiyama</author>
</authors>
<title>Automatic language recognition using acoustic features,</title>
<date>1991</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="5387" citStr="Sugiyama, 1991" startWordPosition="838" endWordPosition="839">e the input utterance, which is analyzed by LMs to predict the spoken language 2.1 Voice Tokenization A voice tokenizer is a speech recognizer that converts a spoken document into a sequence of tokens. As illustrated in Figure 2, a token can be of different sizes, ranging from a speech feature frame, to a phoneme, to a lexical word. A token is defined to describe a distinct acoustic/phonetic activity. In early research, low level spectral 2 http://www.nist.gov/speech/tests/ frames, which are assumed to be independent of each other, were used as a set of prototypical spectra for each language (Sugiyama, 1991). By adopting hidden Markov models, people moved beyond low-level spectral analysis towards modeling a frame sequence into a larger unit such as a phoneme and even a lexical word. Since the lexical word is language specific, the phoneme becomes the natural choice when building a language-independent voice tokenization front-end. Previous studies show that parallel language-dependent phoneme tokenizers effectively serve as the tokenization front-ends with P-PRLM being the typical example. However, a languageindependent phoneme set has not been explored yet experimentally. In this paper, we woul</context>
</contexts>
<marker>Sugiyama, 1991</marker>
<rawString>Masahide Sugiyama. 1991. Automatic language recognition using acoustic features, In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jr</author>
</authors>
<title>Language identification using Gaussian Mixture model tokenization, in</title>
<date>2002</date>
<booktitle>Proc. of ICASSP.</booktitle>
<marker>Jr, 2002</marker>
<rawString>Pedro A. Torres-Carrasquillo, Douglas A. Reynolds, and J.R. Deller. Jr. 2002. Language identification using Gaussian Mixture model tokenization, in Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonghong Yan</author>
<author>Etienne Barnard</author>
</authors>
<title>An approach to automatic language identification based on language dependent phone recognition,</title>
<date>1995</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="9403" citStr="Yan and Barnard, 1995" startWordPosition="1505" endWordPosition="1508"> l∈Λ T∈Γ The exact computation in Eq.(1) involves summing over all possible decoding of token sequences T given O. In many implementations, ∈Γ it is approximated by the maximum over all sequences in the sum by finding the most likely token sequence, , for each language l, using the Tˆl Viterbi algorithm: ≈ P O T λ P T λ AM ˆ arg max[ / ˆ , ( ) ( ˆ / ] LM l ) l l l l l∈Λ Intuitively, individual sounds are heavily shared among different spoken languages due to the common speech production mechanism of humans. Thus, the acoustic score has little language discriminative ability. Many experiments (Yan and Barnard, 1995; Zissman, 1996) have further attested that the n-gram LM score provides more language discriminative information than their acoustic counterparts. In Figure 1, the decoding of voice tokenization is governed by the acoustic model AM λl to arrive at an acoustic score P(O / T , λAM) and a token sequence Tˆl . The ngram LM derives the n-local phonotactic score P T l λ l from the language model LM ( ˆ / LM ) λl . Clearly, the n-gram LM suffers the major shortcoming of having not exploited the global phonotactics in the larger context of a spoken utterance. Speech recognition researchers have so fa</context>
</contexts>
<marker>Yan, Barnard, 1995</marker>
<rawString>Yonghong Yan, and Etienne Barnard. 1995. An approach to automatic language identification based on language dependent phone recognition, In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Human Behavior and the Principal of Least effort, an introduction to human ecology.</title>
<date>1949</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Mass.</location>
<contexts>
<context position="11970" citStr="Zipf, 1949" startWordPosition="1931" endWordPosition="1932">anguages. After transcribing a spoken document into a text like document of tokens, many IR or TC techniques can then be readily applied. It is beyond the scope of this paper to discuss what would be a good voice tokenizer. We adopt phoneme size language-independent acoustic tokens to form a unified acoustic vocabulary in our voice tokenizer. Readers are referred to (Ma et al., 2005) for details of acoustic modeling. 3.1 Vector Space Modeling In human languages, some words invariably occur more frequently than others. One of the most common ways of expressing this idea is known as Zipf’s Law (Zipf, 1949). This law states that there is always a set of words which dominates most of the other words of the language in terms of their frequency of use. This is true both of written words and of spoken words. The short-term, or local phonotactics, is devised to describe Zipf’s Law. The local phonotactic constraints can be typically described by the token n-grams, or phoneme n-grams as in (Ng et al., 2000), which represents short-term statistics such as lexical constraints. Suppose that we have a token sequence, t1 t2 t3 t4. We derive the unigram statistics from the token sequence itself. We derive th</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>George K. Zipf. 1949. Human Behavior and the Principal of Least effort, an introduction to human ecology. Addison-Wesley, Reading, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc A Zissman</author>
</authors>
<title>Comparison of four approaches to automatic language identification of telephone speech,</title>
<date>1996</date>
<booktitle>IEEE Trans. on Speech and Audio Processing,</booktitle>
<pages>4--1</pages>
<contexts>
<context position="3589" citStr="Zissman, 1996" startWordPosition="554" endWordPosition="555">e. 1 Introduction Spoken language and written language are similar in many ways. Therefore, much of the research in spoken language identification, LID, has been inspired by text-categorization methodology. Both text and voice are generated from language dependent vocabulary. For example, both can be seen as stochastic time-sequences corrupted by a channel noise. The n-gram language model has achieved equal amounts of success in both tasks, e.g. n-character slice for text categorization by language (Cavnar and Trenkle, 1994) and Phone Recognition followed by n-gram Language Modeling, or PRLM (Zissman, 1996) . 515 Proceedings of the 43rd Annual Meeting of the ACL, pages 515–522, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics issues. In Section 3 we propose the bag-of-sounds paradigm to turn the LID task into a typical text categorization problem. In Section 4, we study the effects of different settings in experiments on the 1996 NIST Language Recognition Evaluation (LRE) database2. In Section 5, we conclude our study and discuss future work. 2 Related Work Formal evaluations conducted by the National Institute of Science and Technology (NIST) in recent years demonstrated t</context>
<context position="6747" citStr="Zissman, 1996" startWordPosition="1046" endWordPosition="1047"> Language Model With the sequence of tokens, we are able to estimate an n-gram language model (LM) from the statistics. It is generally agreed that phonotactics, i.e. the rules governing the phone/phonemes sequences admissible in a language, carry more language discriminative information than the phonemes themselves. An n-gram LM over the tokens describes well n-local phonotactics among neighboring tokens. While some systems model the phonotactics at the frame level (TorresCarrasquillo et al., 2002), others have proposed PPRLM. The latter has become one of the most promising solutions so far (Zissman, 1996). A variety of cues can be used by humans and machines to distinguish one language from another. These cues include phonology, prosody, morphology, and syntax in the context of an utterance. LM-1 ... LM-L spoken utterance VT-1: Chinese VT-2: English VT-L: French LM-1 ... LM-L LM-1 ... LM-L LM-L: French LM-L: French LM-L: French language classifier hypothesized language word phoneme 516 However, global phonotactic cues at the level of utterance or spoken document remains unexplored in previous work. In this paper, we pay special attention to it. A spoken language always contains a set of high f</context>
<context position="9419" citStr="Zissman, 1996" startWordPosition="1509" endWordPosition="1510">utation in Eq.(1) involves summing over all possible decoding of token sequences T given O. In many implementations, ∈Γ it is approximated by the maximum over all sequences in the sum by finding the most likely token sequence, , for each language l, using the Tˆl Viterbi algorithm: ≈ P O T λ P T λ AM ˆ arg max[ / ˆ , ( ) ( ˆ / ] LM l ) l l l l l∈Λ Intuitively, individual sounds are heavily shared among different spoken languages due to the common speech production mechanism of humans. Thus, the acoustic score has little language discriminative ability. Many experiments (Yan and Barnard, 1995; Zissman, 1996) have further attested that the n-gram LM score provides more language discriminative information than their acoustic counterparts. In Figure 1, the decoding of voice tokenization is governed by the acoustic model AM λl to arrive at an acoustic score P(O / T , λAM) and a token sequence Tˆl . The ngram LM derives the n-local phonotactic score P T l λ l from the language model LM ( ˆ / LM ) λl . Clearly, the n-gram LM suffers the major shortcoming of having not exploited the global phonotactics in the larger context of a spoken utterance. Speech recognition researchers have so far chosen to only</context>
</contexts>
<marker>Zissman, 1996</marker>
<rawString>Marc A. Zissman. 1996. Comparison of four approaches to automatic language identification of telephone speech, IEEE Trans. on Speech and Audio Processing, 4(1):31-44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>