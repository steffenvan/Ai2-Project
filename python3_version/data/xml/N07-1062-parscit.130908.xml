<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9798335">
Efficient Phrase-table Representation for Machine Translation with
Applications to Online MT and Speech Translation
</title>
<author confidence="0.986103">
Richard Zens and Hermann Ney
</author>
<affiliation confidence="0.948854333333333">
Human Language Technology and Pattern Recognition
Lehrstuhl f¨ur Informatik 6 – Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
</affiliation>
<email confidence="0.99806">
{zens,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.996648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979461538461">
In phrase-based statistical machine transla-
tion, the phrase-table requires a large amount
of memory. We will present an efficient repre-
sentation with two key properties: on-demand
loading and a prefix tree structure for the
source phrases.
We will show that this representation scales
well to large data tasks and that we are able
to store hundreds of millions of phrase pairs
in the phrase-table. For the large Chinese–
English NIST task, the memory requirements
of the phrase-table are reduced to less than
20 MB using the new representation with no
loss in translation quality and speed. Addi-
tionally, the new representation is not limited
to a specific test set, which is important for
online or real-time machine translation.
One problem in speech translation is the
matching of phrases in the input word graph
and the phrase-table. We will describe a novel
algorithm that effectively solves this com-
binatorial problem exploiting the prefix tree
data structure of the phrase-table. This algo-
rithm enables the use of significantly larger
input word graphs in a more efficient way re-
sulting in improved translation quality.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992525">
In phrase-based statistical machine translation, a
huge number of source and target phrase pairs
is memorized in the so-called phrase-table. For
medium sized tasks and phrase lengths, these
phrase-tables already require several GBs of mem-
ory or even do not fit at all. If the source text, which
is to be translated, is known in advance, a common
trick is to filter the phrase-table and keep a phrase
pair only if the source phrase occurs in the text. This
filtering is a time-consuming task, as we have to
go over the whole phrase-table. Furthermore, we
have to repeat this filtering step whenever we want
to translate a new source text.
To address these problems, we will use an ef-
ficient representation of the phrase-table with two
key properties: on-demand loading and a prefix tree
structure for the source phrases. The prefix tree
structure exploits the redundancy among the source
phrases. Using on-demand loading, we will load
only a small fraction of the overall phrase-table into
memory. The majority will remain on disk.
The on-demand loading is employed on a per sen-
tence basis, i.e. we load only the phrase pairs that
are required for one sentence into memory. There-
fore, the memory requirements are low, e.g. less than
20 MB for the Chin.-Eng. NIST task. Another ad-
vantage of the on-demand loading is that we are able
to translate new source sentences without filtering.
A potential problem is that this on-demand load-
ing might be too slow. To overcome this, we use a
binary format which is a memory map of the internal
representation used during decoding. Additionally,
we load coherent chunks of the tree structure instead
of individual phrases, i.e. we have only few disk ac-
cess operations. In our experiments, the on-demand
loading is not slower than the traditional approach.
As pointed out in (Mathias and Byrne, 2006),
one problem in speech translation is that we have
to match the phrases of our phrase-table against a
word graph representing the alternative ASR tran-
</bodyText>
<page confidence="0.977402">
492
</page>
<note confidence="0.7983215">
Proceedings of NAACL HLT 2007, pages 492–499,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999857384615385">
scriptions. We will present a phrase matching algo-
rithm that effectively solves this combinatorial prob-
lem exploiting the prefix tree data structure of the
phrase-table. This algorithm enables the use of sig-
nificantly larger input word graphs in a more effi-
cient way resulting in improved translation quality.
The remaining part is structured as follows: we
will first discuss related work in Sec. 2. Then, in
Sec. 3, we will describe the phrase-table represen-
tation. Afterwards, we will present applications in
speech translation and online MT in Sec. 4 and 5,
respectively. Experimental results will be presented
in Sec. 6 followed by the conclusions in Sec. 7.
</bodyText>
<sectionHeader confidence="0.999825" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999702722222222">
(Callison-Burch et al., 2005) and (Zhang and Vogel,
2005) presented data structures for a compact rep-
resentation of the word-aligned bilingual data, such
that on-the-fly extraction of long phrases is possi-
ble. The motivation in (Callison-Burch et al., 2005)
is that there are some long source phrases in the
test data that also occur in the training data. How-
ever, the more interesting question is if these long
phrases really help to improve the translation qual-
ity. We have investigated this and our results are in
line with (Koehn et al., 2003) showing that the trans-
lation quality does not improve if we utilize phrases
beyond a certain length. Furthermore, the suffix ar-
ray data structure of (Callison-Burch et al., 2005) re-
quires a fair amount of memory, about 2 GB in their
example, whereas our implementation will use only
a tiny amount of memory, e.g. less than 20 MB for
the large Chinese-English NIST task.
</bodyText>
<sectionHeader confidence="0.938125" genericHeader="method">
3 Efficient Phrase-table Representation
</sectionHeader>
<bodyText confidence="0.999950114754098">
In this section, we will describe the proposed rep-
resentation of the phrase-table. A prefix tree, also
called trie, is an ordered tree data structure used to
store an associative array where the keys are symbol
sequences. In the case of phrase-based MT, the keys
are source phrases, i.e. sequences of source words
and the associated values are the possible transla-
tions of these source phrases. In a prefix tree, all
descendants of any node have a common prefix,
namely the source phrase associated with that node.
The root node is associated with the empty phrase.
The prefix tree data structure is quite common in
automatic speech translation. There, the lexicon, i.e.
the mapping of phoneme sequences to words, is usu-
ally organized as a prefix tree (Ney et al., 1992).
We convert the list of source phrases into a pre-
fix tree and, thus, exploit that many of them share
the same prefix. This is illustrated in Fig. 1 (left).
Within each node of the tree, we store a sorted ar-
ray of possible successor words along with pointers
to the corresponding successor nodes. Additionally,
we store a pointer to the possible translations.
One property of the tree structure is that we can
efficiently access the successor words of a given pre-
fix. This will be a key point to achieve an efficient
phrase matching algorithm in Sec. 4. When looking
for a specific successor word, we perform a binary
search in the sorted array. Alternatively, we could
use hashing to speed up this lookup. We have chosen
an array representation as this can be read very fast
from disk. Additionally, with the exception of the
root node, the branching factor of the tree is small,
i.e. the potential benefit from hashing is limited. At
the root node, however, the branching factor is close
to the vocabulary size of the source language, which
can be large. As we store the words internally as in-
tegers and virtually all words occur as the first word
of some phrase, we can use the integers directly as
the position in the array of the root node. Hence, the
search for the successors at the root node is a simple
table lookup with direct access, i.e. in 0(1).
If not filtered for a specific test set, the phrase-
table becomes huge even for medium-sized tasks.
Therefore, we store the tree structure on disk
and load only the required parts into memory on-
demand. This is illustrated in Fig. 1 (right). Here,
we show the matching phrases for the source sen-
tence ’c a a c’, where the matching phrases are set in
bold and the phrases that are loaded into memory are
set in italics. The dashed part of the tree structure is
not loaded into memory. Note that some nodes of the
tree are loaded even if there is no matching phrase in
that node. These are required to actually verify that
there is no matching phrase. An example is the ’bc’
node in the lower right part of the figure. This node
is loaded to check if the phrase ’c a a’ occurs in the
phrase-table. The translations, however, are loaded
only for matching source phrases.
In the following sections, we will describe appli-
cations of this phrase-table representation for speech
translation and online MT.
</bodyText>
<sectionHeader confidence="0.976634" genericHeader="method">
4 Speech Translation
</sectionHeader>
<bodyText confidence="0.9745505">
In speech translation, the input to the MT system is
not a sentence, but a word graph representing alter-
</bodyText>
<page confidence="0.998799">
493
</page>
<figure confidence="0.999971866666667">
a
a a
a b
abc
b
b a
baa
bab
bac
b b
b c
c
c a
cab
c a c
b
c
a
a
c
b
b
a
b
c
a
b
c
c
a
a
a a
a b
abc
b
b a
baa
bab
bac
b b
b c
c
c a
cab
cac
a
b
c
b
b
c
a
a
a
a
b
b
c
c
c
</figure>
<figureCaption confidence="0.906358333333333">
Figure 1: Illustration of the prefix tree. Left: list of source phrases and the corresponding prefix tree. Right:
list of matching source phrases for sentence ’c a a c’ (bold phrases match, phrases in italics are loaded in
memory) and the corresponding partially loaded prefix tree (the dashed part is not in memory).
</figureCaption>
<equation confidence="0.572116083333333">
fG j,1
�
� ��� ��
G
fj,n � sG j,n
� ���
�
�
�
�
�
�
</equation>
<bodyText confidence="0.844700181818182">
node k are numbered with
m,..., Mk, i.e. an
edge in the prefix tree is identified by a pair (k, m).
The source word labeling the
outgoing edge of
tree node kis denoted as
and the successor
node of this edge is denoted as s
m E 11, ..., K}.
Due to the tree structure, the successor nodes of a
tree node k are all
</bodyText>
<equation confidence="0.866523285714286">
1, ...,
mth
fTk,m
Tk
distinct:
�
G�
fj,N.i
=
#? m =
sTk,m
sTk,m0
m0
(1)
</equation>
<figureCaption confidence="0.746869666666667">
Figure 2: Illustration for graph G: node j with suc-
cessor nodes sGj,1, ..., sGj,n..., sGj,Nj and corresponding
edge labels fGj,1, ..., fGj,n, ..., fG j,Nj.
</figureCaption>
<bodyText confidence="0.939568947368421">
native ASR transcriptions. As pointed out in (Math-
ias and Byrne, 2006), one problem in speech trans-
lation is that we have to match the phrases of our
phrase-table against the input word graph. This re-
sults in a combinatorial problem as the number of
phrases in a word graph increases exponentially with
the phrase length.
node j is denoted as
an
fGj,n
d the successor node of
this edge is denoted as sGj,n E {1, ..., J}. This nota-
tion is illustrated in Fig. 2.
We use a similar notation for the prefix tree T with
nodes 1, ..., k, ..., K. The outgoing edges of a tree
Let
denote the root node of the prefix tree and
let
denote the prefix that leads to tree node k.
</bodyText>
<equation confidence="0.897687333333333">
Furt
k0
˜fk
</equation>
<bodyText confidence="0.999978333333333">
hermore, we define E(k) as the set of possible
slations of the source phrase ˜fk. These are the
entries of the phrase-table, i.e.
</bodyText>
<equation confidence="0.723805666666667">
� �
E(k) = e˜ ��� p(˜e� ˜fk) &gt; 0 (2)
=
...,
=
=
</equation>
<bodyText confidence="0.787633833333333">
Here, the conditions ensure that the edge sequence
is a proper path from node
to node j
in the input graph and that the corresponding source
phrase is
This definition can
</bodyText>
<equation confidence="0.967208">
1,n1,
.,
� ��� �(ji, ni)I
F(j0,j) = f˜ i�1 :f˜
fGj1,n1,
fGjI,nI�∧j1
j0∧ �I−1
i�1sGji,ni=ji+1∧ sjI,nI
j
(ji,ni)Ii�1
j0
f˜=fG
G.
j
.
jI,nI
</equation>
<bodyText confidence="0.918063071428571">
be expressed in a recursive way; the idea is to extend
the phrases of the predecessor nodes by one word:
U 4.1 Problem Definition
F(j0,j) = In this section, we will introduce the notation and
�j00,n��sG j00,n�j state the problem of matching source phrases of
an input graph G and the phrase-table, represented
as prefix tree T. The input graph G has nodes
1, ..., j, ..., J. The outgoing edges of a graph node
j are numbered with 1, ..., n, ..., Nj, i.e. an edge in
the input graph is identified by a pair (j, n). The
source word labeling the nth outgoing edge of graph
tran
We will need similar symbols for the input graph.
Therefore, we define F(j0, j) as the set of source
</bodyText>
<table confidence="0.7494564">
phrases of all paths from graph node j0 to node j, or
formally:
���� f˜ E F (j0, j00) �
˜ffG (3)
j00,n
</table>
<page confidence="0.994923">
494
</page>
<bodyText confidence="0.999949857142857">
Here, the set is expressed as a union over all in-
bound edges (j&apos;&apos;, n) of node j. We concatenate each
source phrase f that ends at the start node of such
an edge, i.e. f E F(j&apos;, j&apos;&apos;), with the corresponding
edge label fG j,,,n. Additionally, we define E(j&apos;, j)
as the set of possible translations of all paths from
graph node j&apos; to
</bodyText>
<equation confidence="0.953939">
graph node j, or formally: l
E(j&apos;,j) = S e gy f E F(j&apos;,j) : P(Ell) &gt; 01 (4)
U= E(k) (5)
k: �fkEF(j,,j)
U= U E(sTk,m) (6)
(j,,,n):sGj,,,n=j k:�fkEF(j,,j,,)
m:fG =fT
j,,,nk,m
</equation>
<bodyText confidence="0.975366583333333">
each inbound edge
n), the inner union verifies
that there exists a corresponding edge (k, m) in the
prefix tree with the same label, i.e. G _ T
fj ,n — fk,m
Our goal is to find all non-empty sets of trans-
lation options E(j&apos;, j). The naive approach would
be to enumerate all paths in the input graph from
node j&apos; to node j, then lookup the corresponding
source phrase in the phrase-table and add the trans-
lations, if there are any, to the set of translation
options E(j&apos;, j). This solution has some obvious
weaknesses: the number of paths between two nodes
is typically huge and the majority of the correspond-
ing source phrases do not occur in the phrase-table.
We omitted the probabilities for notational conve-
nience. The extensions are straightforward. Note
that we store only the target phrases
in the set
of possible translations E(j&apos;, j) and not the source
phrases f. This is based on the assumption that the
models which are conditioned on the source phrase
fare independent of the context outside the phrase
This assumption holds for the stan
</bodyText>
<equation confidence="0.6175448">
(j&apos;&apos;,
.
e�
).
dard
</equation>
<figureCaption confidence="0.668523833333333">
Figure 3: Algorithm
for match-
ing source phrases of input graph G and prefix tree
T. Input: graph G, prefix tree T, translation options
E(k) for all tree nodes k; output: translation options
E(j&apos;, j) for all graph nodes j&apos; and j.
</figureCaption>
<figure confidence="0.829634076923077">
0 FOR j&apos; = 1 TO J DO
1 stack.push(j&apos;,
2 WHILE not
DO
3 (j, k) =
5 FOR n = 1 TO Nj DO
6 IF
= E)
7 THEN
k)
8 ELSE IF (gym :
=
9 THEN
</figure>
<figureCaption confidence="0.9942">
Fig. 3. Starting from a graph node j&apos;, we explore the
</figureCaption>
<bodyText confidence="0.993292125">
part of the graph which corresponds to known source
phrase prefixes and generate the sets
j) incre-
mentally based on Eq. 6. The intermediate states
are represented as pairs (j, k) meaning that there ex-
ists apath in the input graph from node j&apos; to node j
which is labeled with the source phrase fk, i.e. the
source phrase that leads to node k in the prefix tree.
These intermediate states are stored on a stack. After
the initialization in line 1, the main loop starts. We
take one item from the stack and update the transla-
tion options
j) in line 4. Then, we loop over
all outgoing edges of the current graph node j. For
each edge, we first check if the edge is labeled with
an E in line 6. In this special case, we go to the suc-
cessor node in the input graph
remain in the
current node k of the prefix tree. In the regular case,
i.e. the graph edge label is a regular word, we check
in line 8 if the current prefix tree node k has an out-
going edge labeled with that word. If such an edge
is found, we put a new state on the stack with the
two successor nodes in the input graph
</bodyText>
<equation confidence="0.941303142857143">
an
phrase-match
k0)
stack.empty()
stack.pop()4 E(j&apos;, j) = E(j&apos;, j) � E(k)
(fGj,n
stack.push(sGj,n,
fGj,n
fTk,m)
stack.push(sGj,n,sTk,m)
E(j&apos;,
E(j&apos;,
sGj,n, but
sGj,n
</equation>
<bodyText confidence="0.988814885714286">
d the
prefix tree sTk,m, respectively.
ability. It might be violated by lexicalized Here, the definition was first rewritten using Eq. 2
on the configuration); in that and then using Eq. 3. Again, the set is expressed
case we have to store the source phrase along with recursively as a union over the inbound edges. For
the target phrase and the probability, which is again pair (f, e
straightforward. phrase and word translation models. Thus, we have
4.2 Algorithm to keep only the target phrase with the highest prob-
The algorithm for matching the source phrases of the 495
input graph G an 4.3 Computational Complexity
distor- In this section, we will analyze the computational
tion models (dependent complexity of the algorithm. The computational
d the prefix tree T is presented in complexity of lines 5-9 is in O(Nj log Mk), i.e. it
depends on the branching factors of the input graph
and the prefix tree. Both are typically small. An ex-
ception is the branching factor of the root node
of
the prefix tree, which can be rather large, typically it
is the vocabulary size of the source language. But,
as described in Sec. 3, we can
k0
access the successor
nodes of the root node of the prefix tree in O(1), i.e.
in constant time. So, if we are at the root node of the
prefix tree, the computational complexity of lines 5-
9 is in O(Nj). Using hashing at the interior nodes of
the prefix tree would result in a constant time lookup
at these nodes as well. Nevertheless, the sorted ar-
ray implementation that we chose has the advantage
of faster loading from disk which seems to be more
important in practice.
An alternative interpretation of lines 5-9 is that we
have to compute the intersection of the two sets fG
j
and fTk , with
</bodyText>
<equation confidence="0.9778945">
fG j = {fG �� n = 1, ..., Nj } (7)
j,n
fTk T
_ { fk m I m = 1, ... , Mk } . (8)
</equation>
<bodyText confidence="0.998961285714286">
Assuming both sets are sorted, this could be done in
linear time, i.e. in O(Nj + Mk). In our case, only
the edges in the prefix tree are sorted. Obviously, we
could sort the edges in the input graph and then ap-
ply the linear algorithm, resulting in an overall com-
plexity of O(Nj log Nj + Mk). As the algorithm
visits nodes multiple times, we could do even better
by sorting all edges of the graph during the initial-
ization. Then, we could always apply the linear time
method. On the other hand, it is unclear if this pays
off in practice and an experimental comparison has
to be done which we will leave for future work.
The overall complexity of the algorithm depends
on how many phrases of the input graph occur in the
phrase-table. In the worst case, i.e. if all phrases oc-
cur in the phrase-table, the described algorithm is
not more efficient than the naive algorithm which
simply enumerates all phrases. Nevertheless, this
does not happen in practice and we observe an ex-
ponential speed up compared to the naive algorithm,
as will be shown in Sec. 6.3.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="method">
5 Online Machine Translation
</sectionHeader>
<bodyText confidence="0.999913666666667">
Beside speech translation, the presented phrase-
table data structure has other interesting applica-
tions. One of them is online MT, i.e. an MT sys-
tem that is able to translate unseen sentences with-
out significant delay. These online MT systems are
typically required if there is some interaction with
human users, e.g. if the MT system acts as an in-
terpreter in a conversation, or in real-time systems.
This situation is different from the usual research
environment where typically a fair amount of time
is spent to prepare the MT system to translate a cer-
tain set of source sentences. In the research scenario,
</bodyText>
<tableCaption confidence="0.999458">
Table 1: NIST task: corpus statistics.
</tableCaption>
<table confidence="0.99840675">
Chinese English
Train Sentence pairs 7 M
Running words 199 M 213 M
Vocabulary size 222 K 351 K
Test 2002 Sentences 878 3 512
Running words 25 K 105 K
2005 Sentences 1082 4 328
Running words 33K 148 K
</table>
<bodyText confidence="0.992920333333334">
this preparation usually pays off as the same set of
sentences is translated multiple times. In contrast,
an online MT system translates each sentence just
once. One of the more time-consuming parts of this
preparation is the filtering of the phrase-table. Us-
ing the on-demand loading technique we described
in Sec. 3, we can avoid the filtering step and di-
rectly translate the source sentence. An additional
advantage is that we load only small parts of the full
phrase-table into memory. This reduces the mem-
ory requirements significantly, e.g. for the Chinese–
English NIST task, the memory requirement of the
phrase-table is reduced to less than 20 MB using on-
demand loading. This makes the MT system usable
on devices with limited hardware resources.
</bodyText>
<sectionHeader confidence="0.97646" genericHeader="method">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999085">
6.1 Translation System
</subsectionHeader>
<bodyText confidence="0.999982222222222">
For the experiments, we use a state-of-the-art
phrase-based statistical machine translation system
as described in (Zens and Ney, 2004). We use a
log-linear combination of several models: a four-
gram language model, phrase-based and word-based
translation models, word, phrase and distortion
penalty and a lexicalized distortion model. The
model scaling factors are optimized using minimum
error rate training (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.999873">
6.2 Empirical Analysis for a Large Data Task
</subsectionHeader>
<bodyText confidence="0.999620363636364">
In this section, we present an empirical analysis of
the described data structure for the large data track
of the Chinese-English NIST task. The corpus statis-
tics are shown in Tab. 1.
The translation quality is measured using two ac-
curacy measures: the BLEU and the NIST score.
Additionally, we use the two error rates: the word
error rate (WER) and the position-independent word
error rate (PER). These evaluation criteria are com-
puted with respect to four reference translations.
In Tab. 2, we present the translation quality as a
</bodyText>
<page confidence="0.999468">
496
</page>
<tableCaption confidence="0.998628">
Table 2: NIST task: translation quality as a function of the maximum source phrase length.
</tableCaption>
<table confidence="0.987521555555556">
src NIST 2002 set (dev) NIST 2005 set (test)
len
WER[%] PER[%] BLEU[%] NIST WER[%] PER[%] BLEU[%] NIST
1 71.9 46.8 27.07 8.37 78.0 49.0 23.11 7.62
2 62.4 41.2 34.36 9.39 68.5 42.2 30.32 8.74
3 62.0 41.1 34.89 9.33 67.7 42.1 30.90 8.74
4 61.7 41.1 35.05 9.27 67.6 41.9 30.99 8.75
5 61.8 41.2 34.95 9.25 67.6 41.9 30.93 8.72
00 61.8 41.2 34.99 9.25 67.5 41.8 30.90 8.73
</table>
<figure confidence="0.959581777777778">
20
18
16
14
12
10
8
memory usage [MegaByte]
6
</figure>
<tableCaption confidence="0.998409">
Table 3: NIST task: phrase-table statistics.
</tableCaption>
<table confidence="0.997700875">
src number of distinct avg. tgt
len src phrases src-tgt pairs candidates
1 221505 17 456 415 78.8
2 5 000 041 39 436 617 7.9
3 20 649 699 58 503 904 2.8
4 31383 549 58 436 271 1.9
5 32 679145 51255 866 1.6
total 89 933 939 225 089 073 2.5
</table>
<bodyText confidence="0.9996455">
function of the maximum source phrase length. We
observe a large improvement when going beyond
length 1, but this flattens out very fast. Using phrases
of lengths larger than 4 or 5 does not result in fur-
ther improvement. Note that the minor differences
in the evaluation results for length 4 and beyond are
merely statistical noise. Even a length limit of 3, as
proposed by (Koehn et al., 2003), would result in
almost optimal translation quality. In the following
experiments on this task, we will use a limit of 5 for
the source phrase length.
In Tab. 3, we present statistics about the extracted
phrase pairs for the Chinese–English NIST task as
a function of the source phrase length, in this case
for length 1-5. The phrases are not limited to a spe-
cific test set. We show the number of distinct source
phrases, the number of distinct source-target phrase
pairs and the average number of target phrases (or
translation candidates) per source phrase. In the ex-
periments, we limit the number of translation can-
didates per source phrase to 200. We store a to-
tal of almost 90 million distinct source phrases and
more than 225 million distinct source-target phrase
pairs in the described data structure. Obviously, it
would be infeasible to load this huge phrase-table
completely into memory. Nevertheless, using on-
demand loading, we are able to utilize all these
phrase pairs with minimal memory usage.
In Fig. 4, we show the memory usage of the de-
scribed phrase-table data structure per sentence for
</bodyText>
<figure confidence="0.9952745">
0 20 40 60 80 100
percentage of test set
</figure>
<figureCaption confidence="0.9393705">
Figure 4: NIST task: phrase-table memory usage
per sentence (sorted).
</figureCaption>
<bodyText confidence="0.9999198">
the NIST 2002 test set. The sentences were sorted
according to the memory usage. The maximum
amount of memory for the phrase-table is 19 MB;
for more than 95% of the sentences no more than
15 MB are required. Storing all phrase pairs for this
test set in memory requires about 1.7 GB of mem-
ory, i.e. using the described data structures, we not
only avoid the limitation to a specific test set, but we
also reduce the memory requirements by about two
orders of a magnitude.
Another important aspect that should be consid-
ered is translation speed. In our experiments, the
described data structure is not slower than the tradi-
tional approach. We attribute this to the fact that we
use a binary format that is a memory map of the data
structure used internally and that we load the data in
rather large, coherent chunks. Additionally, there is
virtually no initialization time for the phrase-table
which decreases the overhead of parallelization and
therefore speeds up the development cycle.
</bodyText>
<subsectionHeader confidence="0.996929">
6.3 Speech Translation
</subsectionHeader>
<bodyText confidence="0.999947">
The experiments for speech translation were con-
ducted on the European Parliament Plenary Sessions
(EPPS) task. This is a Spanish-English speech-to-
speech translation task collected within the TC-Star
</bodyText>
<page confidence="0.998835">
497
</page>
<tableCaption confidence="0.999693">
Table 4: EPPS task: corpus statistics.
</tableCaption>
<table confidence="0.991356222222222">
Train Spanish English
Sentence pairs 1.2 M
Running words 31 M 30 M
Vocabulary size 140 K 94 K
Test confusion networks Full Pruned
Sentences 1071
Avg. length 23.6
Avg. / max. depth 2.7 / 136 1.3 / 11
Avg. number of paths 1075 264 K
</table>
<bodyText confidence="0.9991801">
project. The training corpus statistics are presented
in Tab. 4. The phrase-tables for this task were kindly
provided by ITC-IRST.
We evaluate the phrase-match algorithm in
the context of confusion network (CN) decoding
(Bertoldi and Federico, 2005), which is one ap-
proach to speech translation. CNs (Mangu et al.,
2000) are interesting for MT because the reordering
can be done similar to single best input. For more
details on CN decoding, please refer to (Bertoldi et
al., 2007). Note that the phrase-match algo-
rithm is not limited to CNs, but can work on arbi-
trary word graphs.
Statistics of the CNs are also presented in Tab. 4.
We distinguish between the full CNs and pruned
CNs. The pruning parameters were chosen such that
the resulting CNs are similar in size to the largest
ones in (Bertoldi and Federico, 2005). The average
depth of the full CNs, i.e. the average number of al-
ternatives per position, is about 2.7 words whereas
the maximum is as high as 136 alternatives.
In Fig. 5, we present the average number of
phrase-table look-ups for the full EPPS CNs as a
function of the source phrase length. The curve ’CN
total’ represents the total number of source phrases
in the CNs for a given length. This is the number
of phrase-table look-ups using the naive algorithm.
Note the exponential growth with increasing phrase
length. Therefore, the naive algorithm is only appli-
cable for very short phrases and heavily pruned CNs,
as e.g. in (Bertoldi and Federico, 2005).
The curve ’CN explored’ is the number of phrase-
table look-ups using the phrase-match algo-
rithm described in Fig. 3. We do not observe the
exponential explosion as for the naive algorithm.
Thus, the presented algorithm effectively solves the
combinatorial problem of matching phrases of the
input CNs and the phrase-table. For comparison,
we plotted also the number of look-ups using the
phrase-match algorithm in the case of single-
</bodyText>
<figureCaption confidence="0.94599">
Figure 5: EPPS task: avg. number of phrase-table
look-ups per sentence as a function of the source
phrase length.
</figureCaption>
<tableCaption confidence="0.602945666666667">
Table 5: EPPS task: translation quality and time for
different input conditions (CN=confusion network,
time in seconds per sentence).
</tableCaption>
<table confidence="0.705488">
Input type BLEU[%] Time [sec]
Single best 37.6 2.7
CN pruned 38.5 4.8
full 38.9 9.2
</table>
<bodyText confidence="0.997268807692308">
best input, labeled ’single-best explored’. The maxi-
mum phrase length for these experiments is seven.
For CN input, this length can be exceeded as the
CNs may contain c-transitions.
In Tab. 5, we present the translation results and
the translation times for different input conditions.
We observe a significant improvement in translation
quality as more ASR alternatives are taken into ac-
count. The best results are achieved for the full
CNs. On the other hand, the decoding time in-
creases only moderately. Using the new algorithm,
the ratio of the time for decoding the CNs and the
time for decoding the single best input is 3.4 for the
full CNs and 1.8 for the pruned CNs. In previous
work (Bertoldi and Federico, 2005), the ratio for the
pruned CNs was about 25 and the full CNs could not
be handled.
To summarize, the presented algorithm has two
main advantages for speech translation: first, it
enables us to utilize large CNs, which was pro-
hibitively expensive beforehand and second, the ef-
ficiency is improved significantly.
Whereas the previous approaches required care-
ful pruning of the CNs, we are able to utilize the un-
pruned CNs. Experiments on other tasks have shown
that even larger CNs are unproblematic.
</bodyText>
<figure confidence="0.9995009375">
0 2 4 6 8 10 12 14
source phrase length
phrase table look-ups
100000000
10000000
1000000
100000
10000
1000
100
0.1
10
1
CN total
CN explored
single-best explored
</figure>
<page confidence="0.995334">
498
</page>
<sectionHeader confidence="0.999329" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999045">
We proposed an efficient phrase-table data structure
which has two key properties:
</bodyText>
<sectionHeader confidence="0.761219" genericHeader="method">
1. On-demand loading.
</sectionHeader>
<bodyText confidence="0.999995555555556">
We are able to store hundreds of millions of
phrase pairs and require only a very small
amount of memory during decoding, e.g. less
than 20 MB for the Chinese-English NIST task.
This enables us to run the MT system on devices
with limited hardware resources or alternatively
to utilize the freed memory for other models. Ad-
ditionally, the usual phrase-table filtering is obso-
lete, which is important for online MT systems.
</bodyText>
<sectionHeader confidence="0.958828" genericHeader="conclusions">
2. Prefix tree data structure.
</sectionHeader>
<bodyText confidence="0.9995575">
Utilizing the prefix tree structure enables us to ef-
ficiently match source phrases against the phrase-
table. This is especially important for speech
translation where the input is a graph represent-
ing a huge number of alternative sentences. Us-
ing the novel algorithm, we are able to handle
large CNs, which was prohibitively expensive
beforehand. This results in more efficient decod-
ing and improved translation quality.
We have shown that this data structure scales very
well to large data tasks like the Chinese-English
NIST task. The implementation of the described
data structure as well as the phrase-match al-
gorithm for confusion networks is available as open
source software in the MOSES toolkit1.
Not only standard phrase-based systems can ben-
efit from this data structure. It should be rather
straightforward to apply this data structure as well as
the phrase-match algorithm to the hierarchical
approach of (Chiang, 2005). As the number of rules
in this approach is typically larger than the number
of phrases in a standard phrase-based system, the
gains should be even larger.
The language model is another model with high
memory requirements. It would be interesting to in-
vestigate if the described techniques and data struc-
tures are applicable for reducing the memory re-
quirements of language models.
Some aspects of the phrase-match algorithm
are similar to the composition of finite-state au-
tomata. An efficient implementation of on-demand
loading (not only on-demand computation) for a
</bodyText>
<footnote confidence="0.852428">
1http://www.statmt.org/moses
</footnote>
<bodyText confidence="0.998105">
finite-state toolkit would make the whole range of
finite-state operations applicable to large data tasks.
</bodyText>
<sectionHeader confidence="0.997323" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.912536714285714">
This material is partly based upon work supported by the
DARPA under Contract No. HR0011-06-C-0023, and was
partly funded by the European Union under the integrated
project TC-STAR (IST-2002-FP6-506738, http://www.tc-
star.org). Additionally, we would like to thank all group
members of the JHU 2006 summer research workshop Open
Source Toolkit for Statistical Machine Translation.
</bodyText>
<sectionHeader confidence="0.992705" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866897959184">
N. Bertoldi and M. Federico. 2005. A new decoder for spo-
ken language translation based on confusion networks. In
Proc. IEEE Automatic Speech Recognition and Understand-
ing Workshop, pages 86–91, Mexico, November/December.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech trans-
lation by confusion networks decoding. In Proc. IEEE
Int. Conf. on Acoustics, Speech, and Signal Processing
(ICASSP), Honolulu, Hawaii, April.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005. Scal-
ing phrase-based statistical machine translation to larger cor-
pora and longer phrases. In Proc. 43rd Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages 255–262,
Ann Arbor, MI, June.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In Proc. 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages 263–
270, Ann Arbor, MI, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. Human Language Technology
Conf. / North American Chapter of the Assoc. for Compu-
tational Linguistics Annual Meeting (HLT-NAACL), pages
127–133, Edmonton, Canada, May/June.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373–400, October.
L. Mathias and W. Byrne. 2006. Statistical phrase-based
speech translation. In Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1, pages
561–564, Toulouse, France, May.
H. Ney, R. Haeb-Umbach, B. H. Tran, and M. Oerder. 1992.
Improvements in beam search for 10000-word continuous
speech recognition. In Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1, pages
9–12, San Francisco, CA, March.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. 41st Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 160–167,
Sapporo, Japan, July.
R. Zens and H. Ney. 2004. Improvements in phrase-based sta-
tistical machine translation. In Proc. Human Language Tech-
nology Conf. / North American Chapter of the Assoc. for
Computational Linguistics Annual Meeting (HLT-NAACL),
pages 257–264, Boston, MA, May.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-phrase
alignment model for arbitrarily long phrases and large cor-
pora. In Proc. 10th Annual Conf. of the European Assoc. for
Machine Translation (EAMT), pages 294–301, Budapest,
Hungary, May.
</reference>
<page confidence="0.99918">
499
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.219177">
<title confidence="0.999683">Efficient Phrase-table Representation for Machine Translation Applications to Online MT and Speech Translation</title>
<author confidence="0.997357">Richard Zens</author>
<author confidence="0.997357">Hermann Ney</author>
<affiliation confidence="0.509122">Human Language Technology and Pattern</affiliation>
<note confidence="0.653371">Lehrstuhl f¨ur Informatik 6 – Computer Science RWTH Aachen University, D-52056 Aachen,</note>
<abstract confidence="0.998749444444444">In phrase-based statistical machine translation, the phrase-table requires a large amount of memory. We will present an efficient reprewith two key properties: a tree for the source phrases. We will show that this representation scales well to large data tasks and that we are able to store hundreds of millions of phrase pairs in the phrase-table. For the large Chinese– English NIST task, the memory requirements of the phrase-table are reduced to less than 20 MB using the new representation with no loss in translation quality and speed. Additionally, the new representation is not limited to a specific test set, which is important for online or real-time machine translation. One problem in speech translation is the matching of phrases in the input word graph and the phrase-table. We will describe a novel algorithm that effectively solves this combinatorial problem exploiting the prefix tree data structure of the phrase-table. This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>M Federico</author>
</authors>
<title>A new decoder for spoken language translation based on confusion networks.</title>
<date>2005</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop,</booktitle>
<pages>86--91</pages>
<location>Mexico, November/December.</location>
<contexts>
<context position="24434" citStr="Bertoldi and Federico, 2005" startWordPosition="4373" endWordPosition="4376">ons (EPPS) task. This is a Spanish-English speech-tospeech translation task collected within the TC-Star 497 Table 4: EPPS task: corpus statistics. Train Spanish English Sentence pairs 1.2 M Running words 31 M 30 M Vocabulary size 140 K 94 K Test confusion networks Full Pruned Sentences 1071 Avg. length 23.6 Avg. / max. depth 2.7 / 136 1.3 / 11 Avg. number of paths 1075 264 K project. The training corpus statistics are presented in Tab. 4. The phrase-tables for this task were kindly provided by ITC-IRST. We evaluate the phrase-match algorithm in the context of confusion network (CN) decoding (Bertoldi and Federico, 2005), which is one approach to speech translation. CNs (Mangu et al., 2000) are interesting for MT because the reordering can be done similar to single best input. For more details on CN decoding, please refer to (Bertoldi et al., 2007). Note that the phrase-match algorithm is not limited to CNs, but can work on arbitrary word graphs. Statistics of the CNs are also presented in Tab. 4. We distinguish between the full CNs and pruned CNs. The pruning parameters were chosen such that the resulting CNs are similar in size to the largest ones in (Bertoldi and Federico, 2005). The average depth of the f</context>
<context position="25664" citStr="Bertoldi and Federico, 2005" startWordPosition="4588" endWordPosition="4591">l CNs, i.e. the average number of alternatives per position, is about 2.7 words whereas the maximum is as high as 136 alternatives. In Fig. 5, we present the average number of phrase-table look-ups for the full EPPS CNs as a function of the source phrase length. The curve ’CN total’ represents the total number of source phrases in the CNs for a given length. This is the number of phrase-table look-ups using the naive algorithm. Note the exponential growth with increasing phrase length. Therefore, the naive algorithm is only applicable for very short phrases and heavily pruned CNs, as e.g. in (Bertoldi and Federico, 2005). The curve ’CN explored’ is the number of phrasetable look-ups using the phrase-match algorithm described in Fig. 3. We do not observe the exponential explosion as for the naive algorithm. Thus, the presented algorithm effectively solves the combinatorial problem of matching phrases of the input CNs and the phrase-table. For comparison, we plotted also the number of look-ups using the phrase-match algorithm in the case of singleFigure 5: EPPS task: avg. number of phrase-table look-ups per sentence as a function of the source phrase length. Table 5: EPPS task: translation quality and time for </context>
<context position="27150" citStr="Bertoldi and Federico, 2005" startWordPosition="4834" endWordPosition="4837">s seven. For CN input, this length can be exceeded as the CNs may contain c-transitions. In Tab. 5, we present the translation results and the translation times for different input conditions. We observe a significant improvement in translation quality as more ASR alternatives are taken into account. The best results are achieved for the full CNs. On the other hand, the decoding time increases only moderately. Using the new algorithm, the ratio of the time for decoding the CNs and the time for decoding the single best input is 3.4 for the full CNs and 1.8 for the pruned CNs. In previous work (Bertoldi and Federico, 2005), the ratio for the pruned CNs was about 25 and the full CNs could not be handled. To summarize, the presented algorithm has two main advantages for speech translation: first, it enables us to utilize large CNs, which was prohibitively expensive beforehand and second, the efficiency is improved significantly. Whereas the previous approaches required careful pruning of the CNs, we are able to utilize the unpruned CNs. Experiments on other tasks have shown that even larger CNs are unproblematic. 0 2 4 6 8 10 12 14 source phrase length phrase table look-ups 100000000 10000000 1000000 100000 10000</context>
</contexts>
<marker>Bertoldi, Federico, 2005</marker>
<rawString>N. Bertoldi and M. Federico. 2005. A new decoder for spoken language translation based on confusion networks. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop, pages 86–91, Mexico, November/December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>R Zens</author>
<author>M Federico</author>
</authors>
<title>Speech translation by confusion networks decoding.</title>
<date>2007</date>
<booktitle>In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="24666" citStr="Bertoldi et al., 2007" startWordPosition="4414" endWordPosition="4417">4 K Test confusion networks Full Pruned Sentences 1071 Avg. length 23.6 Avg. / max. depth 2.7 / 136 1.3 / 11 Avg. number of paths 1075 264 K project. The training corpus statistics are presented in Tab. 4. The phrase-tables for this task were kindly provided by ITC-IRST. We evaluate the phrase-match algorithm in the context of confusion network (CN) decoding (Bertoldi and Federico, 2005), which is one approach to speech translation. CNs (Mangu et al., 2000) are interesting for MT because the reordering can be done similar to single best input. For more details on CN decoding, please refer to (Bertoldi et al., 2007). Note that the phrase-match algorithm is not limited to CNs, but can work on arbitrary word graphs. Statistics of the CNs are also presented in Tab. 4. We distinguish between the full CNs and pruned CNs. The pruning parameters were chosen such that the resulting CNs are similar in size to the largest ones in (Bertoldi and Federico, 2005). The average depth of the full CNs, i.e. the average number of alternatives per position, is about 2.7 words whereas the maximum is as high as 136 alternatives. In Fig. 5, we present the average number of phrase-table look-ups for the full EPPS CNs as a funct</context>
</contexts>
<marker>Bertoldi, Zens, Federico, 2007</marker>
<rawString>N. Bertoldi, R. Zens, and M. Federico. 2007. Speech translation by confusion networks decoding. In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Honolulu, Hawaii, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Bannard</author>
<author>J Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>255--262</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="4303" citStr="Callison-Burch et al., 2005" startWordPosition="686" endWordPosition="689">this combinatorial problem exploiting the prefix tree data structure of the phrase-table. This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality. The remaining part is structured as follows: we will first discuss related work in Sec. 2. Then, in Sec. 3, we will describe the phrase-table representation. Afterwards, we will present applications in speech translation and online MT in Sec. 4 and 5, respectively. Experimental results will be presented in Sec. 6 followed by the conclusions in Sec. 7. 2 Related Work (Callison-Burch et al., 2005) and (Zhang and Vogel, 2005) presented data structures for a compact representation of the word-aligned bilingual data, such that on-the-fly extraction of long phrases is possible. The motivation in (Callison-Burch et al., 2005) is that there are some long source phrases in the test data that also occur in the training data. However, the more interesting question is if these long phrases really help to improve the translation quality. We have investigated this and our results are in line with (Koehn et al., 2003) showing that the translation quality does not improve if we utilize phrases beyon</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>C. Callison-Burch, C. Bannard, and J. Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proc. 43rd Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 255–262, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="29318" citStr="Chiang, 2005" startWordPosition="5191" endWordPosition="5192">tively expensive beforehand. This results in more efficient decoding and improved translation quality. We have shown that this data structure scales very well to large data tasks like the Chinese-English NIST task. The implementation of the described data structure as well as the phrase-match algorithm for confusion networks is available as open source software in the MOSES toolkit1. Not only standard phrase-based systems can benefit from this data structure. It should be rather straightforward to apply this data structure as well as the phrase-match algorithm to the hierarchical approach of (Chiang, 2005). As the number of rules in this approach is typically larger than the number of phrases in a standard phrase-based system, the gains should be even larger. The language model is another model with high memory requirements. It would be interesting to investigate if the described techniques and data structures are applicable for reducing the memory requirements of language models. Some aspects of the phrase-match algorithm are similar to the composition of finite-state automata. An efficient implementation of on-demand loading (not only on-demand computation) for a 1http://www.statmt.org/moses </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. 43rd Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 263– 270, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada, May/June.</location>
<contexts>
<context position="4821" citStr="Koehn et al., 2003" startWordPosition="774" endWordPosition="777">esented in Sec. 6 followed by the conclusions in Sec. 7. 2 Related Work (Callison-Burch et al., 2005) and (Zhang and Vogel, 2005) presented data structures for a compact representation of the word-aligned bilingual data, such that on-the-fly extraction of long phrases is possible. The motivation in (Callison-Burch et al., 2005) is that there are some long source phrases in the test data that also occur in the training data. However, the more interesting question is if these long phrases really help to improve the translation quality. We have investigated this and our results are in line with (Koehn et al., 2003) showing that the translation quality does not improve if we utilize phrases beyond a certain length. Furthermore, the suffix array data structure of (Callison-Burch et al., 2005) requires a fair amount of memory, about 2 GB in their example, whereas our implementation will use only a tiny amount of memory, e.g. less than 20 MB for the large Chinese-English NIST task. 3 Efficient Phrase-table Representation In this section, we will describe the proposed representation of the phrase-table. A prefix tree, also called trie, is an ordered tree data structure used to store an associative array wher</context>
<context position="21481" citStr="Koehn et al., 2003" startWordPosition="3875" endWordPosition="3878">istinct avg. tgt len src phrases src-tgt pairs candidates 1 221505 17 456 415 78.8 2 5 000 041 39 436 617 7.9 3 20 649 699 58 503 904 2.8 4 31383 549 58 436 271 1.9 5 32 679145 51255 866 1.6 total 89 933 939 225 089 073 2.5 function of the maximum source phrase length. We observe a large improvement when going beyond length 1, but this flattens out very fast. Using phrases of lengths larger than 4 or 5 does not result in further improvement. Note that the minor differences in the evaluation results for length 4 and beyond are merely statistical noise. Even a length limit of 3, as proposed by (Koehn et al., 2003), would result in almost optimal translation quality. In the following experiments on this task, we will use a limit of 5 for the source phrase length. In Tab. 3, we present statistics about the extracted phrase pairs for the Chinese–English NIST task as a function of the source phrase length, in this case for length 1-5. The phrases are not limited to a specific test set. We show the number of distinct source phrases, the number of distinct source-target phrase pairs and the average number of target phrases (or translation candidates) per source phrase. In the experiments, we limit the number</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasebased translation. In Proc. Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL), pages 127–133, Edmonton, Canada, May/June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
<author>A Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer, Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="24505" citStr="Mangu et al., 2000" startWordPosition="4386" endWordPosition="4389">ected within the TC-Star 497 Table 4: EPPS task: corpus statistics. Train Spanish English Sentence pairs 1.2 M Running words 31 M 30 M Vocabulary size 140 K 94 K Test confusion networks Full Pruned Sentences 1071 Avg. length 23.6 Avg. / max. depth 2.7 / 136 1.3 / 11 Avg. number of paths 1075 264 K project. The training corpus statistics are presented in Tab. 4. The phrase-tables for this task were kindly provided by ITC-IRST. We evaluate the phrase-match algorithm in the context of confusion network (CN) decoding (Bertoldi and Federico, 2005), which is one approach to speech translation. CNs (Mangu et al., 2000) are interesting for MT because the reordering can be done similar to single best input. For more details on CN decoding, please refer to (Bertoldi et al., 2007). Note that the phrase-match algorithm is not limited to CNs, but can work on arbitrary word graphs. Statistics of the CNs are also presented in Tab. 4. We distinguish between the full CNs and pruned CNs. The pruning parameters were chosen such that the resulting CNs are similar in size to the largest ones in (Bertoldi and Federico, 2005). The average depth of the full CNs, i.e. the average number of alternatives per position, is about</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus in speech recognition: Word error minimization and other applications of confusion networks. Computer, Speech and Language, 14(4):373–400, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mathias</author>
<author>W Byrne</author>
</authors>
<title>Statistical phrase-based speech translation.</title>
<date>2006</date>
<booktitle>In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>561--564</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="3318" citStr="Mathias and Byrne, 2006" startWordPosition="530" endWordPosition="533">ow, e.g. less than 20 MB for the Chin.-Eng. NIST task. Another advantage of the on-demand loading is that we are able to translate new source sentences without filtering. A potential problem is that this on-demand loading might be too slow. To overcome this, we use a binary format which is a memory map of the internal representation used during decoding. Additionally, we load coherent chunks of the tree structure instead of individual phrases, i.e. we have only few disk access operations. In our experiments, the on-demand loading is not slower than the traditional approach. As pointed out in (Mathias and Byrne, 2006), one problem in speech translation is that we have to match the phrases of our phrase-table against a word graph representing the alternative ASR tran492 Proceedings of NAACL HLT 2007, pages 492–499, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics scriptions. We will present a phrase matching algorithm that effectively solves this combinatorial problem exploiting the prefix tree data structure of the phrase-table. This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality. The remaining</context>
<context position="9637" citStr="Mathias and Byrne, 2006" startWordPosition="1684" endWordPosition="1688">� � � � � � � node k are numbered with m,..., Mk, i.e. an edge in the prefix tree is identified by a pair (k, m). The source word labeling the outgoing edge of tree node kis denoted as and the successor node of this edge is denoted as s m E 11, ..., K}. Due to the tree structure, the successor nodes of a tree node k are all 1, ..., mth fTk,m Tk distinct: � G� fj,N.i = #? m = sTk,m sTk,m0 m0 (1) Figure 2: Illustration for graph G: node j with successor nodes sGj,1, ..., sGj,n..., sGj,Nj and corresponding edge labels fGj,1, ..., fGj,n, ..., fG j,Nj. native ASR transcriptions. As pointed out in (Mathias and Byrne, 2006), one problem in speech translation is that we have to match the phrases of our phrase-table against the input word graph. This results in a combinatorial problem as the number of phrases in a word graph increases exponentially with the phrase length. node j is denoted as an fGj,n d the successor node of this edge is denoted as sGj,n E {1, ..., J}. This notation is illustrated in Fig. 2. We use a similar notation for the prefix tree T with nodes 1, ..., k, ..., K. The outgoing edges of a tree Let denote the root node of the prefix tree and let denote the prefix that leads to tree node k. Furt </context>
</contexts>
<marker>Mathias, Byrne, 2006</marker>
<rawString>L. Mathias and W. Byrne. 2006. Statistical phrase-based speech translation. In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), volume 1, pages 561–564, Toulouse, France, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>R Haeb-Umbach</author>
<author>B H Tran</author>
<author>M Oerder</author>
</authors>
<title>Improvements in beam search for 10000-word continuous speech recognition.</title>
<date>1992</date>
<booktitle>In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>9--12</pages>
<location>San Francisco, CA,</location>
<contexts>
<context position="6002" citStr="Ney et al., 1992" startWordPosition="973" endWordPosition="976">d to store an associative array where the keys are symbol sequences. In the case of phrase-based MT, the keys are source phrases, i.e. sequences of source words and the associated values are the possible translations of these source phrases. In a prefix tree, all descendants of any node have a common prefix, namely the source phrase associated with that node. The root node is associated with the empty phrase. The prefix tree data structure is quite common in automatic speech translation. There, the lexicon, i.e. the mapping of phoneme sequences to words, is usually organized as a prefix tree (Ney et al., 1992). We convert the list of source phrases into a prefix tree and, thus, exploit that many of them share the same prefix. This is illustrated in Fig. 1 (left). Within each node of the tree, we store a sorted array of possible successor words along with pointers to the corresponding successor nodes. Additionally, we store a pointer to the possible translations. One property of the tree structure is that we can efficiently access the successor words of a given prefix. This will be a key point to achieve an efficient phrase matching algorithm in Sec. 4. When looking for a specific successor word, we</context>
</contexts>
<marker>Ney, Haeb-Umbach, Tran, Oerder, 1992</marker>
<rawString>H. Ney, R. Haeb-Umbach, B. H. Tran, and M. Oerder. 1992. Improvements in beam search for 10000-word continuous speech recognition. In Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), volume 1, pages 9–12, San Francisco, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="19713" citStr="Och, 2003" startWordPosition="3550" endWordPosition="3551"> phrase-table is reduced to less than 20 MB using ondemand loading. This makes the MT system usable on devices with limited hardware resources. 6 Experimental Results 6.1 Translation System For the experiments, we use a state-of-the-art phrase-based statistical machine translation system as described in (Zens and Ney, 2004). We use a log-linear combination of several models: a fourgram language model, phrase-based and word-based translation models, word, phrase and distortion penalty and a lexicalized distortion model. The model scaling factors are optimized using minimum error rate training (Och, 2003). 6.2 Empirical Analysis for a Large Data Task In this section, we present an empirical analysis of the described data structure for the large data track of the Chinese-English NIST task. The corpus statistics are shown in Tab. 1. The translation quality is measured using two accuracy measures: the BLEU and the NIST score. Additionally, we use the two error rates: the word error rate (WER) and the position-independent word error rate (PER). These evaluation criteria are computed with respect to four reference translations. In Tab. 2, we present the translation quality as a 496 Table 2: NIST ta</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL),</booktitle>
<pages>257--264</pages>
<location>Boston, MA,</location>
<contexts>
<context position="19428" citStr="Zens and Ney, 2004" startWordPosition="3507" endWordPosition="3510"> we can avoid the filtering step and directly translate the source sentence. An additional advantage is that we load only small parts of the full phrase-table into memory. This reduces the memory requirements significantly, e.g. for the Chinese– English NIST task, the memory requirement of the phrase-table is reduced to less than 20 MB using ondemand loading. This makes the MT system usable on devices with limited hardware resources. 6 Experimental Results 6.1 Translation System For the experiments, we use a state-of-the-art phrase-based statistical machine translation system as described in (Zens and Ney, 2004). We use a log-linear combination of several models: a fourgram language model, phrase-based and word-based translation models, word, phrase and distortion penalty and a lexicalized distortion model. The model scaling factors are optimized using minimum error rate training (Och, 2003). 6.2 Empirical Analysis for a Large Data Task In this section, we present an empirical analysis of the described data structure for the large data track of the Chinese-English NIST task. The corpus statistics are shown in Tab. 1. The translation quality is measured using two accuracy measures: the BLEU and the NI</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in phrase-based statistical machine translation. In Proc. Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL), pages 257–264, Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>An efficient phrase-to-phrase alignment model for arbitrarily long phrases and large corpora.</title>
<date>2005</date>
<booktitle>In Proc. 10th Annual Conf. of the European Assoc. for Machine Translation (EAMT),</booktitle>
<pages>294--301</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="4331" citStr="Zhang and Vogel, 2005" startWordPosition="691" endWordPosition="694">ing the prefix tree data structure of the phrase-table. This algorithm enables the use of significantly larger input word graphs in a more efficient way resulting in improved translation quality. The remaining part is structured as follows: we will first discuss related work in Sec. 2. Then, in Sec. 3, we will describe the phrase-table representation. Afterwards, we will present applications in speech translation and online MT in Sec. 4 and 5, respectively. Experimental results will be presented in Sec. 6 followed by the conclusions in Sec. 7. 2 Related Work (Callison-Burch et al., 2005) and (Zhang and Vogel, 2005) presented data structures for a compact representation of the word-aligned bilingual data, such that on-the-fly extraction of long phrases is possible. The motivation in (Callison-Burch et al., 2005) is that there are some long source phrases in the test data that also occur in the training data. However, the more interesting question is if these long phrases really help to improve the translation quality. We have investigated this and our results are in line with (Koehn et al., 2003) showing that the translation quality does not improve if we utilize phrases beyond a certain length. Furtherm</context>
</contexts>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Y. Zhang and S. Vogel. 2005. An efficient phrase-to-phrase alignment model for arbitrarily long phrases and large corpora. In Proc. 10th Annual Conf. of the European Assoc. for Machine Translation (EAMT), pages 294–301, Budapest, Hungary, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>