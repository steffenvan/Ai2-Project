<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000892">
<title confidence="0.988844">
Training Parsers on Incompatible Treebanks
</title>
<author confidence="0.979585">
Richard Johansson
</author>
<affiliation confidence="0.922115">
Spr˚akbanken, Department of Swedish, University of Gothenburg
</affiliation>
<address confidence="0.920543">
Box 200, SE-40530 Gothenburg, Sweden
</address>
<email confidence="0.997628">
richard.johansson@gu.se
</email>
<sectionHeader confidence="0.99386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999802863636364">
We consider the problem of training a sta-
tistical parser in the situation when there are
multiple treebanks available, and these tree-
banks are annotated according to different lin-
guistic conventions. To address this problem,
we present two simple adaptation methods:
the first method is based on the idea of using
a shared feature representation when parsing
multiple treebanks, and the second method on
guided parsing where the output of one parser
provides features for a second one.
To evaluate and analyze the adaptation meth-
ods, we train parsers on treebank pairs in four
languages: German, Swedish, Italian, and En-
glish. We see significant improvements for
all eight treebanks when training on the full
training sets. However, the clearest benefits
are seen when we consider smaller training
sets. Our experiments were carried out with
unlabeled dependency parsers, but the meth-
ods can easily be generalized to other feature-
based parsers.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994111914285715">
When developing a data-driven syntactic parser, we
need to fit the parameters of its statistical model on
a collection of syntactically annotated sentences – a
treebank. Generally speaking, a larger collection of
examples in the training treebank will give a higher
quality of the resulting parser, but the cost in time
and effort of annotating training sentences is fairly
high. Most existing treebanks are in the range of a
few thousand sentences.
However, there is an abundance of theoretical
models of syntax and there is no consensus on how
treebanks should be annotated. For some languages,
there exist multiple treebanks annotated according
to different syntactic theories. Apart from German,
Swedish, and Italian, which will be considered in
this paper, there are important examples among the
world’s major languages, such as Arabic and Chi-
nese.
To exemplify how syntactic annotation conven-
tions may differ in even such a simple case as un-
labeled dependency annotation, consider the Italian
sentence fragment la sospensione o l’interruzione
(’the suspension or the interruption’) in Figure 1. As
we will see in detail in §3.1.3, there are two Ital-
ian treebanks: the ISST and TUT. If annotating as
in the ISST treebank (drawn above the sentence)
determiners (la, l’) are annotated as dependents of
the following nouns (sospensione, interruzione); in
TUT (drawn below the sentence), we have the re-
verse situation. There are also differences in how
coordinate structures are represented: in ISST, the
two conjuncts are directly conjoined and the con-
junction attached to the first of them, while in TUT
the conjunction acts as a link between the conjuncts.
la sospensione o l’ interruzione
</bodyText>
<figureCaption confidence="0.999169">
Figure 1: Differences in dependency annotation styles.
</figureCaption>
<bodyText confidence="0.999665666666667">
Given the high cost of treebank annotation and the
importance of a proper amount of data for parser de-
velopment, this situation is frustrating. How could
we then make use of multiple treebanks when train-
ing a parser? A naive way would be simply to con-
catenate them, but as we will see this results in a
parser that performs badly on all the treebanks.
In this paper, we investigate two simple adapta-
tion methods to bridge the gap between differing
</bodyText>
<page confidence="0.965584">
127
</page>
<note confidence="0.4717545">
Proceedings of NAACL-HLT 2013, pages 127–137,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999869055555556">
syntactic annotation styles, allowing us to use more
data for parser training. The first approach treats
the problem of parsing with multiple syntactic an-
notation styles as a multiview learning problem and
addresses it by using feature representation that is
partly shared between the views. In the second one
we use a parser trained on one treebank to guide a
new parser trained on another treebank. We evaluate
these methods as well as their combination on four
languages: German, Swedish, Italian, and English.
In all four languages, we see a similar picture: the
shared features approach is generally better when
one of the treebanks is very small, while the guided
parsing approach is better when the treebanks are
more similar in size. However, for most training
set sizes the combination of the the two methods
achieves a higher performance than either of them
individually.
</bodyText>
<sectionHeader confidence="0.9150225" genericHeader="method">
2 Methods for Training Parsers on
Multiple Treebanks
</sectionHeader>
<bodyText confidence="0.9996015">
We now describe the two adaptation methods to
leverage multiple treebanks for parser training. For
clarity of presentation, we assume that there are
two treebanks, although we can easily generalize to
more. We use a common graph-based parsing tech-
nique (Carreras, 2007); the approaches described
here could be used in transition-based parsing as
well.
In a graph-based parser, for a given sentence x
the task of finding the top-scoring parse y� is stated
as an optimization problem of maximizing a linear
objective function:
</bodyText>
<equation confidence="0.978167">
y� = arg max
y
</equation>
<bodyText confidence="0.9996674">
Here w is a weight vector produced by some learn-
ing algorithm and f(x, y) a feature representation
that maps the sentence x with a parse tree y to
a high-dimensional vector; the adaptation methods
presented in this work is implemented as modifica-
tions of the feature representation function f. Since
the search space is too large to be enumerated, the
maximization must be handled carefully, and how
this is done determines the expressivity of the fea-
ture representation f. In the parser by Carreras
(2007) the maximization is carried out by a dynamic
programming procedure relying on crucial indepen-
dence assumptions to break down the search space
into tractable parts. The factorization used in this
approach allows f to express features extracted not
only from single edges, as McDonald et al. (2005),
but also from sibling and grandchild edges.
To understand the machine learning problem of
training parsers on incompatible treebanks, we com-
pare it to the related problem of domain adapta-
tion: training a system for a target domain, using
a large collection of training data from a source do-
main combined with a small labeled or large unla-
beled set from the target domain. Some algorithms
for domain adaptation rely on the assumption that
the differences between source and target distribu-
tions P3 and Pt can be explained in terms of a co-
variate shift: P3(y|x) = Pt(y|x) for all x, y, but
P3(x) =� Pt(x) for some x. In our case, we have the
reverse situation: the input distribution is at least in
theory unchanged between the two treebanks, while
the input–output relation (i.e. the treebank annota-
tion style) is different. However, domain adaptation
and cross-treebank training can be seen as instances
of the more general problem of multitask learning
(Caruana, 1997). Indeed, one of the simplest and
most well-known approaches to domain adaptation
(Daum´e III, 2007), which will also be considered in
this paper, should more correctly be seen as a trick
to handle multitask learning with any machine learn-
ing algorithm. On the other hand, there is no point
in trying to use domain adaptation methods assum-
ing a covariate shift, e.g. instance weighting, or any
method in which the target data is unlabeled (Blitzer
et al., 2007; Ben-David et al., 2010).
</bodyText>
<subsectionHeader confidence="0.999151">
2.1 Sharing Feature Representations
</subsectionHeader>
<bodyText confidence="0.99995325">
Our first adaptation method relies on the intuition
that some properties of two treebanks are shared,
while others are unique to each of them. For in-
stance, as we have seen in Figure 1 the two Ital-
ian treebanks annotate coordination differently; on
the other hand, these treebanks also annotate sev-
eral other linguistic phenomena in the same way.
This observation can then be used to devise a model
where we train two parsers at the same time and use
a feature representation that is partly shared between
the two models, allowing the machine learning algo-
rithm to automatically determine which properties
</bodyText>
<equation confidence="0.741444">
w&apos; f(x,y).
</equation>
<page confidence="0.986718">
128
</page>
<bodyText confidence="0.999985666666667">
of the two datasets are common and which are dif-
ferent. The idea of using features that are shared be-
tween the source and target training sets is a slight
generalization of a well-known method for super-
vised domain adaptation (Daum´e III, 2007).
In practice, this is implemented as follows. As-
sume that originally a sentence x with a parse tree y
was represented as f1(x, y) if it came from the first
treebank, and f2(x, y) if from the second treebank.
We then add a shared feature representation fs to f1
and f2, and embed them into a single feature space.
The resulting feature vectors then become
</bodyText>
<equation confidence="0.966429666666667">
f1(x, y) ® 02 ® fs(x, y) (1)
for a sentence from the first treebank, and
01 ® f2(x, y) ® fs(x, y) (2)
</equation>
<bodyText confidence="0.999890333333333">
for the second treebank. Here, 01 means an all-zero
vector with the dimensionality of the feature space
of f1, and ® is vector concatenation. Using this new
representation, the two datasets are combined and a
single model trained. The hope is then that the learn-
ing algorithm will store the information about the re-
spective particularities in the weights for f1 and f2,
and about the commonalities in the weights for fs.
The result of this process is a symmetric parser that
can handle both treebank formats: when we parse
a sentence at test time, we just use the representa-
tion (1) if we want an output according to the first
treebank and (2) for the second treebank.
In this work, f1, f2, and fs are identical: all of
them correspond to the feature set described by Car-
reras (2007). However, it is certainly imaginable
that fs could consist of specially tailored features
that make generalization easier. In particular, using
a generalized fs would allow us to use this approach
in more complex cases than considered here, for in-
stance if the dependencies would be labeled with
two different sets of grammatical function labels, or
if one of the treebanks would use constituents rather
than dependencies.
</bodyText>
<subsectionHeader confidence="0.999826">
2.2 Using One Parser to Guide Another
</subsectionHeader>
<bodyText confidence="0.990273272727273">
The second method is inspired by work in parser
combination, an idea that has been applied success-
fully several times and relies on the fact that dif-
ferent parsing methods have different strengths and
weaknesses (McDonald and Nivre, 2007), so that
combining them may result in a better overall pars-
ing accuracy. There are several ways to combine
parsers; one of the simplest and most successful
methods of parsing combination uses one parser as
a guide for a second parser. This is normally im-
plemented as a pipeline where the second parser ex-
tracts features based on the output of the first parser.
Nivre and McDonald (2008) used this approach
for combining a graph-based and a transition-based
parser and achieved excellent results on test sets for
several languages, and similar ideas were proposed
by Martins et al. (2008).
We added guide features to the parser feature rep-
resentation. However, the features by Nivre and
McDonald (2008) are slightly too simple since they
only describe whether two words are directly con-
nected or not. That makes sense if the two parsers
are trying to predict the same type of representation,
but will not help us if there are systematic annota-
tion differences between the two treebanks, for in-
stance in whether to annotate a function word or a
lexical word as the head. Instead, following work
in semantic role labeling and similar areas, we use a
generalized notion of syntactic relationship that we
encode by determining a path between two nodes
in a syntactic tree. We defined the function Path(x,
y) as a representation describing the steps required
to traverse the parse tree from x to y, first the steps
up from x to the common ancestor a and then down
from a to y. Since we are working with unlabeled
trees, the path can be represented as just two inte-
gers; to generalize to labeled dependency parsing,
we could have used a full path representation as
commonly used in dependency-based semantic role
labeling (Johansson and Nugues, 2008).
We added the following path-based feature tem-
plates, assuming we have a potential head h with
dependent d, a sibling dependent s and grandchild
(dependent-of-dependent) g:
</bodyText>
<listItem confidence="0.999953">
• POS(h)+POS(d)+Path(h, d)
• POS(h)+POS(s)+Path(h, s)
• POS(h)+POS(d)+POS(s)+Path(h, s)
• POS(h)+POS(g)+Path(h, g)
• POS(h)+POS(d)+POS(g)+Path(h, g)
</listItem>
<bodyText confidence="0.992284">
To exemplify, consider again the example la
sospensione o l’interruzione shown in Figure 1. As-
</bodyText>
<page confidence="0.993246">
129
</page>
<bodyText confidence="0.999993863636364">
sume that we are parsing according to the ISST rep-
resentation (drawn above the sentence) and we con-
sider adding an edge with sospensione as head and
la as dependent, and another parser following the
TUT representation (below the sentence) has cre-
ated an edge in the opposite direction. The first
feature template above would then result in a fea-
ture NOUN+DET+(1,0), where (1,0) represents the
path relationship between the two words in the TUT
tree (one step up, no step down). Similarly, when
the ISST parser adds the coordination edge between
sospensione and interruzione, it can make use of
the information that these two nouns are indirectly
connected in the output by the TUT parser; this is
represented as a path (1,3). This is an example of
a situation where we have a systematic correspon-
dence where a single edge in one representation cor-
responds to several edges in the other.
Like the multiview approach described above, this
method is trivially adaptable to more complex situ-
ations such as labeled dependency parsers with dif-
fering label sets, or dependency/constituent parsing.
</bodyText>
<subsectionHeader confidence="0.984384">
2.3 Combining Methods
</subsectionHeader>
<bodyText confidence="0.999922142857143">
The two adaptation methods are orthogonal and can
easily be combined. When trying to improve the per-
formance of a parser trained on the primary treebank
T1 by leveraging a supporting treebank T2, we then
use T2 in two different ways: first by training a guide
parser, and secondly by concatenating it to T1 using
a shared feature representation.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999971333333333">
We carried out experiments to evaluate the cross-
framework adaptation methods. The evaluations
were carried out using the official CoNLL-X eval-
uation script using the default parameters. Since our
parsers do not predict edge labels, we report unla-
beled attachment scores in all tables and plots.
</bodyText>
<subsectionHeader confidence="0.999695">
3.1 Treebanks Used in the Experiments
</subsectionHeader>
<bodyText confidence="0.999905555555556">
In our experiments, we used four languages: Ger-
man, Swedish, Italian, and English. For each lan-
guage, we had two treebanks. Our approaches cur-
rently require that the treebanks use the same tok-
enization conventions, so for Italian and Swedish we
automatically retokenized the treebanks. We also
made sure that the two treebanks for one language
used the same part-of-speech tag sets, by applying
an automatic tagger when necessary.
</bodyText>
<subsectionHeader confidence="0.903297">
3.1.1 German: Tiger and T¨uBa-D/Z
</subsectionHeader>
<bodyText confidence="0.999983791666667">
For German, there are two treebanks available:
Tiger (Brants et al., 2002) and T¨uBa-D/Z (Telljo-
hann et al., 2004). These treebanks are constituent
treebanks, but dependency versions are available:
T¨uBa-D/Z (version 7.0) includes the dependency
version in the distribution, while for Tiger we used
the version from CoNLL-X (Buchholz and Marsi,
2006). The constituent annotation styles in the two
treebanks are radically different: Tiger uses a very
flat structure with a minimal amount of intermediate
nodes, while T¨uBa-D/Z uses a more elaborate struc-
ture including topological field information. How-
ever, the dependency versions are actually quite sim-
ilar, at least with respect to attachment. The most
common systematic difference we observed is in the
annotation of coordination.
Both treebanks are large: for Tiger, the training
set was 31,243 sentences and the test set 7,973 sen-
tences, and for T¨uBa-D/Z 40,000 and 11,428 sen-
tences respectively. We did not use the Tiger test set
from the CoNLL-X shared task since it is very small.
We applied the TreeTagger POS tagger (Schmid,
1994) to both treebanks, using the pre-trained Ger-
man model.
</bodyText>
<subsectionHeader confidence="0.854475">
3.1.2 Swedish: Talbanken05 and Syntag
</subsectionHeader>
<bodyText confidence="0.999988388888889">
As previously noted by Nivre (2002) inter alia,
Swedish has a venerable tradition in treebanking:
there are not only one but two treebanks which must
be counted among the earliest efforts of that kind.
The oldest one is the Talbanken or MAMBA tree-
bank (Einarsson, 1976), which has later been repro-
cessed for modern use (Nilsson et al., 2005). The
original annotation is a function-tagged constituent
syntax without phrase labels, but the reprocessed re-
lease includes a version converted to dependency
syntax. The dependency treebank was used in the
CoNLL-X Shared Task (Buchholz and Marsi, 2006),
and we used that version version in this work.
The second treebank is called Syntag (Jirborg,
1986). Similar to Talbanken, its representation uses
function-tagged constituents but no phrase labels.
We developed a conversion to dependency trees,
which was straightforward since many constituents
</bodyText>
<page confidence="0.98195">
130
</page>
<bodyText confidence="0.999913">
have explicitly defined heads (Johansson, 2013).
The two treebank annotation styles have signifi-
cant differences. Most prominently, the Syntag an-
notation is fairly semantically oriented in its treat-
ment of function words such as prepositions and
subordinating conjunctions: in Talbanken, a prepo-
sition is the head of a prepositional phrase, while
in Syntag the head is the prepositional complement.
There are also some domain differences: Talbanken
consists of student essays and public information,
while Syntag consists of news text.
To make the two treebanks compatible on the to-
ken level, we retokenized Syntag – which handles
punctuation in an idiosyncratic way – and applied a
POS tagger trained on the Stockholm–Ume˚a Corpus
(Gustafson-Capkov´a and Hartmann, 2006) to both
treebanks. For Talbanken, we used 7,362 sentences
for training and set aside a new test set of 3,680 sen-
tences since the CoNLL-X test set is too small for
serious experimental purposes – only 389 sentences.
For Syntag, we split the treebank into 3,524 sen-
tences for training and 1,763 sentences for testing.
</bodyText>
<subsectionHeader confidence="0.606809">
3.1.3 Italian: ISST and TUT
</subsectionHeader>
<bodyText confidence="0.999597727272727">
There are two Italian treebanks. The first is the
Italian Syntactic–Semantic Treebank or ISST (Mon-
temagni et al., 2003). Here, we used the version that
was prepared (Montemagni and Simi, 2007) for the
CoNLL-2007 Shared Task (Nivre et al., 2007).
The TUT treebank1 is a more recent effort. This
treebank is available in multiple constituent and de-
pendency formats, and we have used the CoNLL-
formatted dependency version in this work. The
representation used in TUT is inspired by the Word
Grammar theory (Hudson, 1984) and tends to be
more surface-oriented than that of ISST. For in-
stance, as pointed out above in the discussion of
Figure 1, TUT differs from ISST in its treatment of
determiner–noun constructions and coordination. It
has been noted (Bosco and Lavelli, 2010; Bosco et
al., 2010) that the TUT representation is easier to
parse than the ISST representation.
We simplified the tokenization of both treebanks.
In ISST, we split multiwords into separate tokens
and reattached clitics to nonfinite verb forms. For in-
stance, a single token a causa di was converted into
</bodyText>
<footnote confidence="0.945591">
1http://www.di.unito.it/-tutreeb/
</footnote>
<bodyText confidence="0.999413230769231">
three tokens a, causa, di, and the three tokens trovar-
se-lo into a single token trovarselo. In TUT, we
applied the same conversions and also recomposed
preposition–article and multiple-clitic contractions
that had been split by the annotators, e.g. della,
glielo etc.2 After changing the tokenization, we ap-
plied the TreeTagger POS tagger (Schmid, 1994) to
both treebanks, using the pre-trained Italian model
with the Baroni tagset3.
After preprocessing the data, we created training
and test sets. For ISST, the training set was 2,239
and the test set 1,120 sentences, while for TUT the
training set was 1,906 and the test set 954 sentences.
</bodyText>
<listItem confidence="0.3753475">
3.1.4 English: Two Different Conversions of
the Penn Treebank
</listItem>
<bodyText confidence="0.999941666666667">
For English, there is no significant dependency
treebank so we followed most previous work in us-
ing dependency trees automatically derived from
constituent trees in the large Penn Treebank WSJ
corpus (Marcus et al., 1993). Due to the fact
that there is a highly parametrizable constituent-
to-dependency conversion tool available (Johansson
and Nugues, 2007), we could create two dependency
treebanks with very different annotation styles.
The first training set was created from sections
02–12 of the WSJ corpus. By default, the conversion
tool outputs a treebank using the annotation style
of the CoNLL-2008 Shared Task (Surdeanu et al.,
2008); however we wanted to create a more surface-
oriented style for this treebank, so we turned on op-
tions to make wh-words heads of relative clauses,
and possessive markers heads of noun phrases. This
corpus had 20,706 sentences, and will be referred to
as WSJ Part 1 in the experimental section.
The second training treebank was built from sec-
tions 13–22. For this treebank, we inverted the
value of most options in order to get a more seman-
tically oriented treebank where content words are
connected directly. In this treebank, we also used
“Prague-style” annotation of coordination: the con-
juncts are annotated as dependents of the conjunc-
tion. This set contained 20,826 sentences, and will
</bodyText>
<footnote confidence="0.8458942">
2It should be noted that these conversions also make sense
from a practical NLP point of view, since a number of contrac-
tions are homonymic with other words.
3http://sslmit.unibo.it/-baroni/
collocazioni/itwac.tagset.txt
</footnote>
<page confidence="0.995216">
131
</page>
<bodyText confidence="0.998487733333333">
be called WSJ Part 2.
We finally applied both conversion methods to
sections 24 and 23 to create development and test
sets. The development set contained 1,346 and the
test set 2,416 sentences. We did not change the tok-
enization or part-of-speech tags of the WSJ corpora.
Here, we should note that we have a slightly more
synthetic and controlled experimental setting than
for Swedish and German: the parsers are evaluated
on the same test set, so we know that there is no
difference in test set difficulty. We also know a pri-
ori that performance differences are not due to any
significant differences in genre, since all texts come
from the same source (the Wall Street Journal) and
tend to focus on business-oriented news.
</bodyText>
<subsectionHeader confidence="0.996926">
3.2 Baseline Parsing Performance
</subsectionHeader>
<bodyText confidence="0.9998079">
As a starting point, we trained parsers on all tree-
banks. In addition, we created a parser using a naive
adaptation method by combining the training sets for
each language, and training parsers on those three
sets. We then applied all three parsers for every lan-
guage on both test sets for that language. The re-
sults for German, Swedish, Italian, and English are
presented in Table 1.
Every parser performed well on the test set anno-
tated in the same annotation style as its training set.
As has been observed previously, surface-oriented
styles are easier to parse than semantically oriented
styles: The Talbanken and WSJ Part 1 parsers all
achieve much higher performance on their respec-
tive test sets than the Syntag and WSJ Part 2 parsers.
The better performance of the Talbanken parser is
also partly explainable by the fact that its training
set is more than twice as large as the Syntag training
set. Similarly for German, we see slightly higher
performance for T¨uBa-D/Z than for Tiger.
However, as can be expected every parser per-
formed very poorly when applied to the test set us-
ing the annotation style it was not trained on. For
Swedish and English, the accuracy figures are in the
range of 50-60, while the figure are a bit less poor
for German since the two treebanks are more simi-
lar. We also see, again unsurprisingly, that the naive
combination baseline performs poorly in all situa-
tions: we just get a “worst-of-both-worlds” parser
that performs badly on both test sets.
</bodyText>
<table confidence="0.9994833125">
GERMAN Acc. on Tiger Acc. on TBDZ
Tiger 87.8 72.0
T¨uBa-D/Z 71.8 89.4
Tiger+TBDZ 77.7 87.7
SWEDISH Acc. on ST Acc. on TB
Syntag 81.4 52.6
Talbanken 50.3 88.2
Syntag+Talbanken 61.8 82.7
ITALIAN Acc. on ISST Acc. on TUT
ISST 81.1 57.4
TUT 55.9 84.0
ISST+TUT 73.9 71.6
ENGLISH Acc. on WSJ 1 Acc. on WSJ 2
WSJ part 1 92.6 57.4
WSJ part 2 57.4 89.5
WSJ parts 1+2 75.3 72.1
</table>
<tableCaption confidence="0.99977">
Table 1: Baseline performance figures.
</tableCaption>
<subsectionHeader confidence="0.982507">
3.3 Evaluation on the Full Training Sets
</subsectionHeader>
<bodyText confidence="0.999984642857143">
We trained new parsers using the shared features and
guided parsing adaptation methods described in §2.
Additionally, we trained parsers using both methods
at the same time; we refer to these parsers as com-
bined. Including the baseline parsers, this gave us
24 parsers to evaluate on their respective test sets.
The results for German are given in Table 2. Here,
we see that all three adaptation methods give statis-
tically significant4 improvements over the baseline
when parsing the Tiger treebank. In particular, the
combined method gives a strong 0.7-point improve-
ment, a 6% error reduction. For T¨uBa-D/Z, the im-
provements are smaller, although still significant ex-
cept for the guided parsing method.
</bodyText>
<table confidence="0.9706348">
Method Acc. on Tiger Acc. on T¨uBa-D/Z
Baseline 87.8 89.4
Shared 88.1 89.6
Guided 88.4 89.5
Combined 88.5 89.6
</table>
<tableCaption confidence="0.997410333333333">
Table 2: Performance figures for the German adapted
parsers. Results that are significantly different from the
baseline performances are written in boldface.
</tableCaption>
<footnote confidence="0.9663085">
4At the 95% level. The significance levels of differences
were computed using permutation tests.
</footnote>
<page confidence="0.981689">
132
</page>
<note confidence="0.636499">
Method Acc. on ST Acc. on TB
</note>
<table confidence="0.99246475">
Baseline 81.4 88.2
Shared 81.3 88.3
Guided 82.5 88.4
Combined 82.5 88.5
</table>
<tableCaption confidence="0.999905">
Table 3: Performance of the Swedish adapted parsers.
</tableCaption>
<bodyText confidence="0.9998192">
For Swedish, we have a similar story: we see
stronger improvements in the weak parser. Since
the Talbanken treebank is twice as large as the Syn-
tag treebank and has a surface-oriented representa-
tion that is easier to parse, this parser is useful as
a guide for the Syntag parser: the improvements of
the guided and combined Syntag parsers are statis-
tically significant. However, it is harder to improve
the Talbanken parser, for which the baseline is much
stronger. 3 shows the results for the Swedish parsers.
</bodyText>
<table confidence="0.9591534">
Method Acc. on ISST Acc. on TUT
Baseline 81.1 84.0
Shared 81.5 84.4
Guided 81.7 84.3
Combined 81.8 84.7
</table>
<tableCaption confidence="0.999654">
Table 4: Performance of the Italian adapted parsers.
</tableCaption>
<bodyText confidence="0.999901923076923">
When we turn to the English corpora, the adapta-
tion methods again gave us a number of very large
improvements. The results are shown in Table 5.
The shared features and combined methods gave sta-
tistically significant improvements for the WSJ Part
1 parser, and the guided parsing method an improve-
ment that is nearly significant. However the most
dramatic change is the 1.2-point improvement of the
WSJ Part 2 parser, given by the guided parsing and
combined methods. It is possible that this result
partly can be explained by the fact that this exper-
iment is a bit cleaner: in particular, as outlined in
§3.1.4, there are no domain differences.
</bodyText>
<table confidence="0.9142904">
Method Acc. on WSJ 1 Acc. on WSJ 2
Baseline 92.6 89.5
Shared 92.8 89.5
Guided 92.8 90.7
Combined 92.9 90.7
</table>
<tableCaption confidence="0.999433">
Table 5: Performance of the English adapted parsers.
</tableCaption>
<bodyText confidence="0.997733789473685">
For WSJ Part 2, we analyzed the differences
between the baseline and the best adapted parser.
While there were improvements for all POS tags, the
most notable one was in the attachment of conjunc-
tions, where we got an increase from 69% to 75%
in attachment accuracy, an 18% relative error reduc-
tion. Here we saw a very clear benefit of guided
parsing: since this treebank uses “Prague-style” co-
ordination annotation (i.e. the conjunction governs
the conjuncts), it is hard for the parser to handle va-
lencies and selectional preferences when there is a
conjunction involved. It has been noted (Nilsson et
al., 2007) that this style of annotating coordination
is hard to parse. Since the WSJ Part 1 parser uses
a coordination style that is easier to parse, the WSJ
Part 2 parser can rely on its judgment.
Although conclusions must be very tentative since
we are testing on just four languages, we can make
a few general observations.
</bodyText>
<listItem confidence="0.985907315789474">
• The largest improvements (absolute and rela-
tive) all happen in treebanks that are harder to
parse. In particular, Syntag and WSJ Part 2 are
harder to parse due to their representation, and
to some extent this may be true for Tiger as well
– its learning curve rises more slowly than for
T¨uBa-D/Z. Of course, in some cases (in partic-
ular Syntag, but also Tiger) this may partly be
explained by the training set being smaller, but
not for WSJ Part 2. In these cases, the guided
parsing method seems to be more effective.
• The languages where the shared features
method gives significant improvement for both
treebanks are German and Italian, where we do
not have the situation that one treebank is much
larger or much easier to parse.
• The combination of the two methods gave sig-
nificant improvements in all eight cases, and
had the highest performance in six cases.
</listItem>
<subsectionHeader confidence="0.998563">
3.4 The Effect of the Training Set Size
</subsectionHeader>
<bodyText confidence="0.9999615">
In order to better understand the differences between
the adaptation methods, we analyzed the impact of
training set size on the improvement given by the
respective methods. Let us refer to the training tree-
bank annotated according to the same style as the
test set as the primary treebank, and the other one
as the supporting treebank. We carried out the ex-
periments in this section by varying the number of
</bodyText>
<page confidence="0.99885">
133
</page>
<figureCaption confidence="0.993832">
Figure 2: Error reduction by training set size, German.
</figureCaption>
<figure confidence="0.9697665">
101 102 103 104
Training set size (sentences)
</figure>
<figureCaption confidence="0.984508">
Figure 3: Error reduction by training set size, Swedish.
</figureCaption>
<figure confidence="0.999400596153846">
Tiger error reduction
Shared
Guided
Combined
30
20
10
0
101 102 103 104 105
Training set size (sentences)
Error reduction (percent) 50
40
30
20
10
0
101 102 103 104 105
Training set size (sentences)
Error reduction (percent)
50
40
T Ba-D/Z error reduction
Shared
Guided
Combined
Syntag error reduction
Shared
Guided
Combined
14
12
10
8
6
4
2
0
101 102 103 104
Training set size (sentences)
Talbanken error reduction
Shared
Guided
Combined
Error reduction (percent)
18
16
Error reduction (percent) 25
20
15
10
5
0
</figure>
<bodyText confidence="0.991732125">
training sentences in the primary treebank and keep-
ing the size of the supporting treebank constant.
In order to highlight the differences between the
three adaptation methods, we show error reduction
plots in Figures 2, 3, 4, and 5 for German, Swedish,
Italian, and English respectively. For each training
set size on the x axis, the plot shows the reduction
in relative error with respect to the baseline.
We note that every single one of the 24 adapted
parsers learns faster than the corresponding baseline
parser. While we saw a number of significant im-
provements in §3.3 when using the full training sets,
the relative improvements are much stronger when
the training sets are small- and medium-sized.
These plots illustrate the different properties of
the two methods. Using a shared feature represen-
tation tends to be very effective when the primary
treebank is small: the error reductions are over 40
percent for German and over 25 percent for English.
Guided parsing works best for mid-sized sets, and
the relative effectiveness of both methods decreases
as the size of the primary treebank increases. Again,
we see that guided parsing is less effective if the
guide uses an annotation style that is hard to parse.
</bodyText>
<figureCaption confidence="0.99839">
Figure 4: Error reduction by training set size, Italian.
</figureCaption>
<figure confidence="0.999113766666667">
ISST error reduction
Shared
Guided
Combined
14
12
10
8
6
4
2
0
100 101 102 103 104
Training set size (sentences)
TUT error reduction
0
100 101 102 103 104
Training set size (sentences)
Error reduction (percent)
18
16
Error reduction (percent) 30
25
20
15
10
5
Shared
Guided
Combined
</figure>
<page confidence="0.996125">
134
</page>
<bodyText confidence="0.99947575">
In particular, for Swedish the Syntag parser never
gives a very large improvement when guiding the
Talbanken parser, and this is also true of both Italian
parsers. To a smaller extent, this also holds for En-
glish and German: the WSJ Part 2 and Tiger parsers
are less useful as guides than their counterparts.
The combination method generally performs very
well: in all eight experiments, it outperforms the
other two for almost every training set size. Its per-
formance is very close to that of the guided parsing
method for larger training sets, when the effect of
the shared features method is less pronounced.
</bodyText>
<figureCaption confidence="0.98303">
Figure 5: Error reduction by training set size, English.
</figureCaption>
<sectionHeader confidence="0.998699" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999976787234042">
We have considered the problem of training a de-
pendency parser on incompatible treebanks, and we
studied two very simple methods for addressing this
problem, the shared features and guided parsing
methods. These methods allow us to use more than
one treebank when training dependency parsers. We
evaluated the methods on eight treebanks in four
languages, and had statistically significant improve-
ments in all eight cases. In particular, for English
we saw a strong 1.2-point absolute improvement (an
11% relative error reduction) in the performance of a
semantically oriented parser when trained on the full
training set. For German, we also had very strong
results for the Tiger treebank: a 6% error reduction.
For Swedish, the parser trained on the small Syntag
treebank got a boost from a guide parser trained on
the larger Talbanken. In general, it seems to be eas-
ier to improve parsers that use representations that
are harder to parse.
For all eight treebanks, both methods achieved
large improvements for small training set sizes,
while the effect gradually diminished as the training
set size increased. The shared features method was
the most effective for very small training sets, while
guided parsing surpassed it when training sets got
larger. The combination of the two methods was also
effective, in most cases outperforming both methods
on their own. In particular, when using the full train-
ing sets, this was the only method that had statisti-
cally significant improvements for all treebanks.
While this work used an unlabeled graph-based
dependency parser, our methods generalize naturally
to other parsing approaches, including transition-
based dependency parsing. Labeled parsing with
incompatible label sets is easy to implement in the
shared features framework by removing the label in-
formation from the shared feature representation fs,
and similar modifications of fs could be carried out
to handle more complex situations such as combined
constituent and dependency parsing. Furthermore,
the paths used by the feature extractor in the guided
parser can be extended without much effort as well.
The models presented here are very simple, and in
future work we would like to explore more com-
plex approaches such as quasi-synchronous gram-
mars (Smith and Eisner, 2009; Li et al., 2012) or au-
tomatic treebank transformation (Niu et al., 2009).
</bodyText>
<sectionHeader confidence="0.996515" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999908285714286">
I am grateful to the anonymous reviewers, whose
feedback has helped to clarify the description of the
methods. This research was supported by University
of Gothenburg through its support of the Centre for
Language Technology and Spr˚akbanken. It has been
partly funded by the Swedish Research Council un-
der grant number 2012-5738.
</bodyText>
<figure confidence="0.994570838709677">
WSJ part 1 error reduction
Shared
Guided
Combined
30
25
20
15
10
5
0
101 102 103 104 105
Training set size (sentences)
Error reduction (percent) 40
35
30
25
20
15
10
5
0
101 102 103 104 105
Training set size (sentences)
Error reduction (percent)
40
35
WSJ part 2 error reduction
Shared
Guided
Combined
</figure>
<page confidence="0.991727">
135
</page>
<sectionHeader confidence="0.987715" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833932692307">
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 2010(79):151–175.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440–447,
Prague, Czech Republic.
Cristina Bosco and Alberto Lavelli. 2010. Annota-
tion schema oriented validation for dependency pars-
ing evaluation. In Proceedings of the Ninth Workshop
on Treebanks and Linguistic Theories (TLT9), Tartu,
Estonia.
Cristina Bosco, Simonetta Montemagni, Alessandro
Mazzei, Vincenzo Lombardo, Felice Dell’Orletta,
Alessandro Lenci, Leonardo Lesmo, Giuseppe Attardi,
Maria Simi, Alberto Lavelli, Johan Hall, Jens Nils-
son, and Joakim Nivre. 2010. Comparing the influ-
ence of different treebank annotations on dependency
parsing. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), pages 1794–1801, Valletta, Malta.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theory, pages 24–41, Sozopol, Bul-
garia.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning (CoNLL-X), pages
149–164, New York City, United States.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings the
CoNLL Shared Task, pages 957–961, Prague, Czech
Republic.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41–75.
Hal Daum´e III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256–
263, Prague, Czech Republic.
Jan Einarsson. 1976. Talbankens skrift-
spr˚akskonkordans. Department of Scandinavian
Languages, Lund University.
Sofia Gustafson-Capkov´a and Britt Hartmann. 2006.
Manual of the Stockholm Ume˚a Corpus version 2.0.
Stockholm University.
Richard Hudson. 1984. Word Grammar. Blackwell.
Jerker J¨arborg. 1986. Manual f¨or syntaggning. De-
partment of Linguistic Computation, University of
Gothenburg.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for En-
glish. In NODALIDA 2007 Conference Proceedings,
pages 105–112, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 393–400, Manchester, United Kingdom.
Richard Johansson. 2013. Bridging the gap between
two Swedish treebanks. Northern European Journal
of Language Technology. Submitted.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Ex-
ploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 675–684,
Jeju Island, Korea.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 157–
166, Honolulu, United States.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 122–131, Prague, Czech Re-
public.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
pages 91–98, Ann Arbor, United States.
Simonetta Montemagni and Maria Simi. 2007. The Ital-
ian dependency annotated corpus developed for the
CoNLL-2007 shared task. Technical report, ILC-
CNR.
Simonetta Montemagni, Francesco Barsotti, Marco Bat-
tista, Nicoletta Calzolari, Ornella Corazzari, Alessan-
dro Lenci, Antonio Zampolli, Francesca Fanciulli,
Maria Massetani, Remo Raffaelli, Roberto Basili,
Maria Teresa Pazienza, Dario Saracino, Fabio Zan-
zotto, Nadia Mana, Fabio Pianesi, and Rodolfo Del-
monte. 2003. Building the Italian Syntactic–Semantic
</reference>
<page confidence="0.986269">
136
</page>
<reference confidence="0.999784490566038">
Treebank. In Anne Abeill´e, editor, Building and Using
Syntactically Annotated Corpora. Kluwer, Dordrecht.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of NODAL-
IDA Special Session on Treebanks.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive depen-
dency parsing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 968–975, Prague, Czech Republic.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46–54, Suntec, Singapore.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950–958,
Columbus, United States.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915–932,
Prague, Czech Republic.
Joakim Nivre. 2002. What kinds of trees grow
in Swedish soil? A comparison of four annota-
tion schemes for Swedish. In Proceedings of the
First Workshop on Treebanks and Linguistic Theories
(TLT2002), Sozopol, Bulgaria.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, United Kingdom.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 822–831, Suntec, Singapore.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings of
the Twelfth Conference on Natural Language Learn-
ing, pages 159–177, Manchester, United Kingdom.
Heike Telljohann, Erhard Hinrichs, and Sandra Kbler.
2004. The T¨uba-D/Z treebank: Annotating German
with a context-free backbone. In In Proceedings of
the Fourth International Conference on Language Re-
sources and Evaluation (LREC 2004), pages 2229–
2235.
</reference>
<page confidence="0.997827">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907734">
<title confidence="0.999967">Training Parsers on Incompatible Treebanks</title>
<author confidence="0.996559">Richard</author>
<affiliation confidence="0.991729">Spr˚akbanken, Department of Swedish, University of</affiliation>
<address confidence="0.954634">Box 200, SE-40530 Gothenburg,</address>
<email confidence="0.98819">richard.johansson@gu.se</email>
<abstract confidence="0.998373434782608">We consider the problem of training a statistical parser in the situation when there are multiple treebanks available, and these treebanks are annotated according to different linguistic conventions. To address this problem, we present two simple adaptation methods: the first method is based on the idea of using a shared feature representation when parsing multiple treebanks, and the second method on guided parsing where the output of one parser provides features for a second one. To evaluate and analyze the adaptation methods, we train parsers on treebank pairs in four languages: German, Swedish, Italian, and English. We see significant improvements for all eight treebanks when training on the full training sets. However, the clearest benefits are seen when we consider smaller training sets. Our experiments were carried out with unlabeled dependency parsers, but the methods can easily be generalized to other featurebased parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<volume>2010</volume>
<issue>79</issue>
<contexts>
<context position="7217" citStr="Ben-David et al., 2010" startWordPosition="1162" endWordPosition="1165">, domain adaptation and cross-treebank training can be seen as instances of the more general problem of multitask learning (Caruana, 1997). Indeed, one of the simplest and most well-known approaches to domain adaptation (Daum´e III, 2007), which will also be considered in this paper, should more correctly be seen as a trick to handle multitask learning with any machine learning algorithm. On the other hand, there is no point in trying to use domain adaptation methods assuming a covariate shift, e.g. instance weighting, or any method in which the target data is unlabeled (Blitzer et al., 2007; Ben-David et al., 2010). 2.1 Sharing Feature Representations Our first adaptation method relies on the intuition that some properties of two treebanks are shared, while others are unique to each of them. For instance, as we have seen in Figure 1 the two Italian treebanks annotate coordination differently; on the other hand, these treebanks also annotate several other linguistic phenomena in the same way. This observation can then be used to devise a model where we train two parsers at the same time and use a feature representation that is partly shared between the two models, allowing the machine learning algorithm </context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 2010(79):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7192" citStr="Blitzer et al., 2007" startWordPosition="1158" endWordPosition="1161"> is different. However, domain adaptation and cross-treebank training can be seen as instances of the more general problem of multitask learning (Caruana, 1997). Indeed, one of the simplest and most well-known approaches to domain adaptation (Daum´e III, 2007), which will also be considered in this paper, should more correctly be seen as a trick to handle multitask learning with any machine learning algorithm. On the other hand, there is no point in trying to use domain adaptation methods assuming a covariate shift, e.g. instance weighting, or any method in which the target data is unlabeled (Blitzer et al., 2007; Ben-David et al., 2010). 2.1 Sharing Feature Representations Our first adaptation method relies on the intuition that some properties of two treebanks are shared, while others are unique to each of them. For instance, as we have seen in Figure 1 the two Italian treebanks annotate coordination differently; on the other hand, these treebanks also annotate several other linguistic phenomena in the same way. This observation can then be used to devise a model where we train two parsers at the same time and use a feature representation that is partly shared between the two models, allowing the ma</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Bosco</author>
<author>Alberto Lavelli</author>
</authors>
<title>Annotation schema oriented validation for dependency parsing evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Ninth Workshop on Treebanks and Linguistic Theories (TLT9),</booktitle>
<location>Tartu, Estonia.</location>
<contexts>
<context position="18468" citStr="Bosco and Lavelli, 2010" startWordPosition="3009" endWordPosition="3012">t was prepared (Montemagni and Simi, 2007) for the CoNLL-2007 Shared Task (Nivre et al., 2007). The TUT treebank1 is a more recent effort. This treebank is available in multiple constituent and dependency formats, and we have used the CoNLLformatted dependency version in this work. The representation used in TUT is inspired by the Word Grammar theory (Hudson, 1984) and tends to be more surface-oriented than that of ISST. For instance, as pointed out above in the discussion of Figure 1, TUT differs from ISST in its treatment of determiner–noun constructions and coordination. It has been noted (Bosco and Lavelli, 2010; Bosco et al., 2010) that the TUT representation is easier to parse than the ISST representation. We simplified the tokenization of both treebanks. In ISST, we split multiwords into separate tokens and reattached clitics to nonfinite verb forms. For instance, a single token a causa di was converted into 1http://www.di.unito.it/-tutreeb/ three tokens a, causa, di, and the three tokens trovarse-lo into a single token trovarselo. In TUT, we applied the same conversions and also recomposed preposition–article and multiple-clitic contractions that had been split by the annotators, e.g. della, glie</context>
</contexts>
<marker>Bosco, Lavelli, 2010</marker>
<rawString>Cristina Bosco and Alberto Lavelli. 2010. Annotation schema oriented validation for dependency parsing evaluation. In Proceedings of the Ninth Workshop on Treebanks and Linguistic Theories (TLT9), Tartu, Estonia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Cristina Bosco</author>
<author>Simonetta Montemagni</author>
<author>Alessandro Mazzei</author>
<author>Vincenzo Lombardo</author>
<author>Felice Dell’Orletta</author>
<author>Alessandro Lenci</author>
<author>Leonardo Lesmo</author>
<author>Giuseppe Attardi</author>
<author>Maria Simi</author>
<author>Alberto Lavelli</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
</authors>
<title>Comparing the influence of different treebank annotations on dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<pages>1794--1801</pages>
<location>Valletta,</location>
<marker>Bosco, Montemagni, Mazzei, Lombardo, Dell’Orletta, Lenci, Lesmo, Attardi, Simi, Lavelli, Hall, Nilsson, Nivre, 2010</marker>
<rawString>Cristina Bosco, Simonetta Montemagni, Alessandro Mazzei, Vincenzo Lombardo, Felice Dell’Orletta, Alessandro Lenci, Leonardo Lesmo, Giuseppe Attardi, Maria Simi, Alberto Lavelli, Johan Hall, Jens Nilsson, and Joakim Nivre. 2010. Comparing the influence of different treebank annotations on dependency parsing. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), pages 1794–1801, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theory,</booktitle>
<pages>24--41</pages>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="14571" citStr="Brants et al., 2002" startWordPosition="2393" endWordPosition="2396">attachment scores in all tables and plots. 3.1 Treebanks Used in the Experiments In our experiments, we used four languages: German, Swedish, Italian, and English. For each language, we had two treebanks. Our approaches currently require that the treebanks use the same tokenization conventions, so for Italian and Swedish we automatically retokenized the treebanks. We also made sure that the two treebanks for one language used the same part-of-speech tag sets, by applying an automatic tagger when necessary. 3.1.1 German: Tiger and T¨uBa-D/Z For German, there are two treebanks available: Tiger (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2004). These treebanks are constituent treebanks, but dependency versions are available: T¨uBa-D/Z (version 7.0) includes the dependency version in the distribution, while for Tiger we used the version from CoNLL-X (Buchholz and Marsi, 2006). The constituent annotation styles in the two treebanks are radically different: Tiger uses a very flat structure with a minimal amount of intermediate nodes, while T¨uBa-D/Z uses a more elaborate structure including topological field information. However, the dependency versions are actually quite similar, at least with </context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theory, pages 24–41, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>149--164</pages>
<location>New York City, United States.</location>
<contexts>
<context position="14847" citStr="Buchholz and Marsi, 2006" startWordPosition="2433" endWordPosition="2436">okenization conventions, so for Italian and Swedish we automatically retokenized the treebanks. We also made sure that the two treebanks for one language used the same part-of-speech tag sets, by applying an automatic tagger when necessary. 3.1.1 German: Tiger and T¨uBa-D/Z For German, there are two treebanks available: Tiger (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2004). These treebanks are constituent treebanks, but dependency versions are available: T¨uBa-D/Z (version 7.0) includes the dependency version in the distribution, while for Tiger we used the version from CoNLL-X (Buchholz and Marsi, 2006). The constituent annotation styles in the two treebanks are radically different: Tiger uses a very flat structure with a minimal amount of intermediate nodes, while T¨uBa-D/Z uses a more elaborate structure including topological field information. However, the dependency versions are actually quite similar, at least with respect to attachment. The most common systematic difference we observed is in the annotation of coordination. Both treebanks are large: for Tiger, the training set was 31,243 sentences and the test set 7,973 sentences, and for T¨uBa-D/Z 40,000 and 11,428 sentences respective</context>
<context position="16278" citStr="Buchholz and Marsi, 2006" startWordPosition="2662" endWordPosition="2665">edish: Talbanken05 and Syntag As previously noted by Nivre (2002) inter alia, Swedish has a venerable tradition in treebanking: there are not only one but two treebanks which must be counted among the earliest efforts of that kind. The oldest one is the Talbanken or MAMBA treebank (Einarsson, 1976), which has later been reprocessed for modern use (Nilsson et al., 2005). The original annotation is a function-tagged constituent syntax without phrase labels, but the reprocessed release includes a version converted to dependency syntax. The dependency treebank was used in the CoNLL-X Shared Task (Buchholz and Marsi, 2006), and we used that version version in this work. The second treebank is called Syntag (Jirborg, 1986). Similar to Talbanken, its representation uses function-tagged constituents but no phrase labels. We developed a conversion to dependency trees, which was straightforward since many constituents 130 have explicitly defined heads (Johansson, 2013). The two treebank annotation styles have significant differences. Most prominently, the Syntag annotation is fairly semantically oriented in its treatment of function words such as prepositions and subordinating conjunctions: in Talbanken, a prepositi</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings the CoNLL Shared Task,</booktitle>
<pages>957--961</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4687" citStr="Carreras, 2007" startWordPosition="740" endWordPosition="741">ch is generally better when one of the treebanks is very small, while the guided parsing approach is better when the treebanks are more similar in size. However, for most training set sizes the combination of the the two methods achieves a higher performance than either of them individually. 2 Methods for Training Parsers on Multiple Treebanks We now describe the two adaptation methods to leverage multiple treebanks for parser training. For clarity of presentation, we assume that there are two treebanks, although we can easily generalize to more. We use a common graph-based parsing technique (Carreras, 2007); the approaches described here could be used in transition-based parsing as well. In a graph-based parser, for a given sentence x the task of finding the top-scoring parse y� is stated as an optimization problem of maximizing a linear objective function: y� = arg max y Here w is a weight vector produced by some learning algorithm and f(x, y) a feature representation that maps the sentence x with a parse tree y to a high-dimensional vector; the adaptation methods presented in this work is implemented as modifications of the feature representation function f. Since the search space is too large</context>
<context position="9361" citStr="Carreras (2007)" startWordPosition="1544" endWordPosition="1546">ion, the two datasets are combined and a single model trained. The hope is then that the learning algorithm will store the information about the respective particularities in the weights for f1 and f2, and about the commonalities in the weights for fs. The result of this process is a symmetric parser that can handle both treebank formats: when we parse a sentence at test time, we just use the representation (1) if we want an output according to the first treebank and (2) for the second treebank. In this work, f1, f2, and fs are identical: all of them correspond to the feature set described by Carreras (2007). However, it is certainly imaginable that fs could consist of specially tailored features that make generalization easier. In particular, using a generalized fs would allow us to use this approach in more complex cases than considered here, for instance if the dependencies would be labeled with two different sets of grammatical function labels, or if one of the treebanks would use constituents rather than dependencies. 2.2 Using One Parser to Guide Another The second method is inspired by work in parser combination, an idea that has been applied successfully several times and relies on the fa</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings the CoNLL Shared Task, pages 957–961, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning,</booktitle>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="6732" citStr="Caruana, 1997" startWordPosition="1082" endWordPosition="1083">e target domain. Some algorithms for domain adaptation rely on the assumption that the differences between source and target distributions P3 and Pt can be explained in terms of a covariate shift: P3(y|x) = Pt(y|x) for all x, y, but P3(x) =� Pt(x) for some x. In our case, we have the reverse situation: the input distribution is at least in theory unchanged between the two treebanks, while the input–output relation (i.e. the treebank annotation style) is different. However, domain adaptation and cross-treebank training can be seen as instances of the more general problem of multitask learning (Caruana, 1997). Indeed, one of the simplest and most well-known approaches to domain adaptation (Daum´e III, 2007), which will also be considered in this paper, should more correctly be seen as a trick to handle multitask learning with any machine learning algorithm. On the other hand, there is no point in trying to use domain adaptation methods assuming a covariate shift, e.g. instance weighting, or any method in which the target data is unlabeled (Blitzer et al., 2007; Ben-David et al., 2010). 2.1 Sharing Feature Representations Our first adaptation method relies on the intuition that some properties of t</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. Machine Learning, 28(1):41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256– 263, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Einarsson</author>
</authors>
<title>Talbankens skriftspr˚akskonkordans.</title>
<date>1976</date>
<institution>Department of Scandinavian Languages, Lund University.</institution>
<contexts>
<context position="15952" citStr="Einarsson, 1976" startWordPosition="2614" endWordPosition="2615">was 31,243 sentences and the test set 7,973 sentences, and for T¨uBa-D/Z 40,000 and 11,428 sentences respectively. We did not use the Tiger test set from the CoNLL-X shared task since it is very small. We applied the TreeTagger POS tagger (Schmid, 1994) to both treebanks, using the pre-trained German model. 3.1.2 Swedish: Talbanken05 and Syntag As previously noted by Nivre (2002) inter alia, Swedish has a venerable tradition in treebanking: there are not only one but two treebanks which must be counted among the earliest efforts of that kind. The oldest one is the Talbanken or MAMBA treebank (Einarsson, 1976), which has later been reprocessed for modern use (Nilsson et al., 2005). The original annotation is a function-tagged constituent syntax without phrase labels, but the reprocessed release includes a version converted to dependency syntax. The dependency treebank was used in the CoNLL-X Shared Task (Buchholz and Marsi, 2006), and we used that version version in this work. The second treebank is called Syntag (Jirborg, 1986). Similar to Talbanken, its representation uses function-tagged constituents but no phrase labels. We developed a conversion to dependency trees, which was straightforward s</context>
</contexts>
<marker>Einarsson, 1976</marker>
<rawString>Jan Einarsson. 1976. Talbankens skriftspr˚akskonkordans. Department of Scandinavian Languages, Lund University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sofia Gustafson-Capkov´a</author>
<author>Britt Hartmann</author>
</authors>
<title>Manual of the Stockholm Ume˚a Corpus version 2.0.</title>
<date>2006</date>
<institution>Stockholm University.</institution>
<marker>Gustafson-Capkov´a, Hartmann, 2006</marker>
<rawString>Sofia Gustafson-Capkov´a and Britt Hartmann. 2006. Manual of the Stockholm Ume˚a Corpus version 2.0. Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<date>1984</date>
<publisher>Word Grammar. Blackwell.</publisher>
<contexts>
<context position="18212" citStr="Hudson, 1984" startWordPosition="2968" endWordPosition="2969"> into 3,524 sentences for training and 1,763 sentences for testing. 3.1.3 Italian: ISST and TUT There are two Italian treebanks. The first is the Italian Syntactic–Semantic Treebank or ISST (Montemagni et al., 2003). Here, we used the version that was prepared (Montemagni and Simi, 2007) for the CoNLL-2007 Shared Task (Nivre et al., 2007). The TUT treebank1 is a more recent effort. This treebank is available in multiple constituent and dependency formats, and we have used the CoNLLformatted dependency version in this work. The representation used in TUT is inspired by the Word Grammar theory (Hudson, 1984) and tends to be more surface-oriented than that of ISST. For instance, as pointed out above in the discussion of Figure 1, TUT differs from ISST in its treatment of determiner–noun constructions and coordination. It has been noted (Bosco and Lavelli, 2010; Bosco et al., 2010) that the TUT representation is easier to parse than the ISST representation. We simplified the tokenization of both treebanks. In ISST, we split multiwords into separate tokens and reattached clitics to nonfinite verb forms. For instance, a single token a causa di was converted into 1http://www.di.unito.it/-tutreeb/ thre</context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>Richard Hudson. 1984. Word Grammar. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerker J¨arborg</author>
</authors>
<title>Manual f¨or syntaggning.</title>
<date>1986</date>
<institution>Department of Linguistic Computation, University of Gothenburg.</institution>
<marker>J¨arborg, 1986</marker>
<rawString>Jerker J¨arborg. 1986. Manual f¨or syntaggning. Department of Linguistic Computation, University of Gothenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In NODALIDA 2007 Conference Proceedings,</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="19869" citStr="Johansson and Nugues, 2007" startWordPosition="3226" endWordPosition="3229">After preprocessing the data, we created training and test sets. For ISST, the training set was 2,239 and the test set 1,120 sentences, while for TUT the training set was 1,906 and the test set 954 sentences. 3.1.4 English: Two Different Conversions of the Penn Treebank For English, there is no significant dependency treebank so we followed most previous work in using dependency trees automatically derived from constituent trees in the large Penn Treebank WSJ corpus (Marcus et al., 1993). Due to the fact that there is a highly parametrizable constituentto-dependency conversion tool available (Johansson and Nugues, 2007), we could create two dependency treebanks with very different annotation styles. The first training set was created from sections 02–12 of the WSJ corpus. By default, the conversion tool outputs a treebank using the annotation style of the CoNLL-2008 Shared Task (Surdeanu et al., 2008); however we wanted to create a more surfaceoriented style for this treebank, so we turned on options to make wh-words heads of relative clauses, and possessive markers heads of noun phrases. This corpus had 20,706 sentences, and will be referred to as WSJ Part 1 in the experimental section. The second training </context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In NODALIDA 2007 Conference Proceedings, pages 105–112, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>The effect of syntactic representation on semantic role labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>393--400</pages>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="11805" citStr="Johansson and Nugues, 2008" startWordPosition="1953" endWordPosition="1956">ng and similar areas, we use a generalized notion of syntactic relationship that we encode by determining a path between two nodes in a syntactic tree. We defined the function Path(x, y) as a representation describing the steps required to traverse the parse tree from x to y, first the steps up from x to the common ancestor a and then down from a to y. Since we are working with unlabeled trees, the path can be represented as just two integers; to generalize to labeled dependency parsing, we could have used a full path representation as commonly used in dependency-based semantic role labeling (Johansson and Nugues, 2008). We added the following path-based feature templates, assuming we have a potential head h with dependent d, a sibling dependent s and grandchild (dependent-of-dependent) g: • POS(h)+POS(d)+Path(h, d) • POS(h)+POS(s)+Path(h, s) • POS(h)+POS(d)+POS(s)+Path(h, s) • POS(h)+POS(g)+Path(h, g) • POS(h)+POS(d)+POS(g)+Path(h, g) To exemplify, consider again the example la sospensione o l’interruzione shown in Figure 1. As129 sume that we are parsing according to the ISST representation (drawn above the sentence) and we consider adding an edge with sospensione as head and la as dependent, and another p</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. The effect of syntactic representation on semantic role labeling. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 393–400, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Bridging the gap between two Swedish treebanks.</title>
<date>2013</date>
<journal>Northern European Journal of Language Technology. Submitted.</journal>
<contexts>
<context position="16626" citStr="Johansson, 2013" startWordPosition="2713" endWordPosition="2714"> et al., 2005). The original annotation is a function-tagged constituent syntax without phrase labels, but the reprocessed release includes a version converted to dependency syntax. The dependency treebank was used in the CoNLL-X Shared Task (Buchholz and Marsi, 2006), and we used that version version in this work. The second treebank is called Syntag (Jirborg, 1986). Similar to Talbanken, its representation uses function-tagged constituents but no phrase labels. We developed a conversion to dependency trees, which was straightforward since many constituents 130 have explicitly defined heads (Johansson, 2013). The two treebank annotation styles have significant differences. Most prominently, the Syntag annotation is fairly semantically oriented in its treatment of function words such as prepositions and subordinating conjunctions: in Talbanken, a preposition is the head of a prepositional phrase, while in Syntag the head is the prepositional complement. There are also some domain differences: Talbanken consists of student essays and public information, while Syntag consists of news text. To make the two treebanks compatible on the token level, we retokenized Syntag – which handles punctuation in a</context>
</contexts>
<marker>Johansson, 2013</marker>
<rawString>Richard Johansson. 2013. Bridging the gap between two Swedish treebanks. Northern European Journal of Language Technology. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
</authors>
<title>Exploiting multiple treebanks for parsing with quasisynchronous grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>675--684</pages>
<location>Jeju Island,</location>
<contexts>
<context position="33951" citStr="Li et al., 2012" startWordPosition="5595" endWordPosition="5598">d parsing with incompatible label sets is easy to implement in the shared features framework by removing the label information from the shared feature representation fs, and similar modifications of fs could be carried out to handle more complex situations such as combined constituent and dependency parsing. Furthermore, the paths used by the feature extractor in the guided parser can be extended without much effort as well. The models presented here are very simple, and in future work we would like to explore more complex approaches such as quasi-synchronous grammars (Smith and Eisner, 2009; Li et al., 2012) or automatic treebank transformation (Niu et al., 2009). Acknowledgements I am grateful to the anonymous reviewers, whose feedback has helped to clarify the description of the methods. This research was supported by University of Gothenburg through its support of the Centre for Language Technology and Spr˚akbanken. It has been partly funded by the Swedish Research Council under grant number 2012-5738. WSJ part 1 error reduction Shared Guided Combined 30 25 20 15 10 5 0 101 102 103 104 105 Training set size (sentences) Error reduction (percent) 40 35 30 25 20 15 10 5 0 101 102 103 104 105 Trai</context>
</contexts>
<marker>Li, Liu, Che, 2012</marker>
<rawString>Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Exploiting multiple treebanks for parsing with quasisynchronous grammars. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675–684, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="19734" citStr="Marcus et al., 1993" startWordPosition="3207" endWordPosition="3210">pplied the TreeTagger POS tagger (Schmid, 1994) to both treebanks, using the pre-trained Italian model with the Baroni tagset3. After preprocessing the data, we created training and test sets. For ISST, the training set was 2,239 and the test set 1,120 sentences, while for TUT the training set was 1,906 and the test set 954 sentences. 3.1.4 English: Two Different Conversions of the Penn Treebank For English, there is no significant dependency treebank so we followed most previous work in using dependency trees automatically derived from constituent trees in the large Penn Treebank WSJ corpus (Marcus et al., 1993). Due to the fact that there is a highly parametrizable constituentto-dependency conversion tool available (Johansson and Nugues, 2007), we could create two dependency treebanks with very different annotation styles. The first training set was created from sections 02–12 of the WSJ corpus. By default, the conversion tool outputs a treebank using the annotation style of the CoNLL-2008 Shared Task (Surdeanu et al., 2008); however we wanted to create a more surfaceoriented style for this treebank, so we turned on options to make wh-words heads of relative clauses, and possessive markers heads of </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>157--166</pages>
<location>Honolulu, United States.</location>
<contexts>
<context position="10649" citStr="Martins et al. (2008)" startWordPosition="1754" endWordPosition="1757">nesses (McDonald and Nivre, 2007), so that combining them may result in a better overall parsing accuracy. There are several ways to combine parsers; one of the simplest and most successful methods of parsing combination uses one parser as a guide for a second parser. This is normally implemented as a pipeline where the second parser extracts features based on the output of the first parser. Nivre and McDonald (2008) used this approach for combining a graph-based and a transition-based parser and achieved excellent results on test sets for several languages, and similar ideas were proposed by Martins et al. (2008). We added guide features to the parser feature representation. However, the features by Nivre and McDonald (2008) are slightly too simple since they only describe whether two words are directly connected or not. That makes sense if the two parsers are trying to predict the same type of representation, but will not help us if there are systematic annotation differences between the two treebanks, for instance in whether to annotate a function word or a lexical word as the head. Instead, following work in semantic role labeling and similar areas, we use a generalized notion of syntactic relation</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157– 166, Honolulu, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>122--131</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10061" citStr="McDonald and Nivre, 2007" startWordPosition="1655" endWordPosition="1658">ilored features that make generalization easier. In particular, using a generalized fs would allow us to use this approach in more complex cases than considered here, for instance if the dependencies would be labeled with two different sets of grammatical function labels, or if one of the treebanks would use constituents rather than dependencies. 2.2 Using One Parser to Guide Another The second method is inspired by work in parser combination, an idea that has been applied successfully several times and relies on the fact that different parsing methods have different strengths and weaknesses (McDonald and Nivre, 2007), so that combining them may result in a better overall parsing accuracy. There are several ways to combine parsers; one of the simplest and most successful methods of parsing combination uses one parser as a guide for a second parser. This is normally implemented as a pipeline where the second parser extracts features based on the output of the first parser. Nivre and McDonald (2008) used this approach for combining a graph-based and a transition-based parser and achieved excellent results on test sets for several languages, and similar ideas were proposed by Martins et al. (2008). We added g</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 122–131, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, United States.</location>
<contexts>
<context position="5761" citStr="McDonald et al. (2005)" startWordPosition="917" endWordPosition="920">ptation methods presented in this work is implemented as modifications of the feature representation function f. Since the search space is too large to be enumerated, the maximization must be handled carefully, and how this is done determines the expressivity of the feature representation f. In the parser by Carreras (2007) the maximization is carried out by a dynamic programming procedure relying on crucial independence assumptions to break down the search space into tractable parts. The factorization used in this approach allows f to express features extracted not only from single edges, as McDonald et al. (2005), but also from sibling and grandchild edges. To understand the machine learning problem of training parsers on incompatible treebanks, we compare it to the related problem of domain adaptation: training a system for a target domain, using a large collection of training data from a source domain combined with a small labeled or large unlabeled set from the target domain. Some algorithms for domain adaptation rely on the assumption that the differences between source and target distributions P3 and Pt can be explained in terms of a covariate shift: P3(y|x) = Pt(y|x) for all x, y, but P3(x) =� P</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simonetta Montemagni</author>
<author>Maria Simi</author>
</authors>
<title>The Italian dependency annotated corpus developed for the CoNLL-2007 shared task.</title>
<date>2007</date>
<tech>Technical report, ILCCNR.</tech>
<contexts>
<context position="17887" citStr="Montemagni and Simi, 2007" startWordPosition="2912" endWordPosition="2915"> POS tagger trained on the Stockholm–Ume˚a Corpus (Gustafson-Capkov´a and Hartmann, 2006) to both treebanks. For Talbanken, we used 7,362 sentences for training and set aside a new test set of 3,680 sentences since the CoNLL-X test set is too small for serious experimental purposes – only 389 sentences. For Syntag, we split the treebank into 3,524 sentences for training and 1,763 sentences for testing. 3.1.3 Italian: ISST and TUT There are two Italian treebanks. The first is the Italian Syntactic–Semantic Treebank or ISST (Montemagni et al., 2003). Here, we used the version that was prepared (Montemagni and Simi, 2007) for the CoNLL-2007 Shared Task (Nivre et al., 2007). The TUT treebank1 is a more recent effort. This treebank is available in multiple constituent and dependency formats, and we have used the CoNLLformatted dependency version in this work. The representation used in TUT is inspired by the Word Grammar theory (Hudson, 1984) and tends to be more surface-oriented than that of ISST. For instance, as pointed out above in the discussion of Figure 1, TUT differs from ISST in its treatment of determiner–noun constructions and coordination. It has been noted (Bosco and Lavelli, 2010; Bosco et al., 201</context>
</contexts>
<marker>Montemagni, Simi, 2007</marker>
<rawString>Simonetta Montemagni and Maria Simi. 2007. The Italian dependency annotated corpus developed for the CoNLL-2007 shared task. Technical report, ILCCNR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Simonetta Montemagni</author>
<author>Francesco Barsotti</author>
<author>Marco Battista</author>
</authors>
<title>Nicoletta Calzolari, Ornella Corazzari, Alessandro Lenci, Antonio Zampolli, Francesca Fanciulli,</title>
<booktitle>Pianesi, and Rodolfo Delmonte. 2003. Building the Italian Syntactic–Semantic Treebank. In Anne Abeill´e, editor, Building and Using Syntactically Annotated Corpora.</booktitle>
<publisher>Kluwer,</publisher>
<location>Maria Massetani, Remo Raffaelli, Roberto Basili, Maria Teresa Pazienza, Dario Saracino, Fabio Zanzotto, Nadia Mana, Fabio</location>
<marker>Montemagni, Barsotti, Battista, </marker>
<rawString>Simonetta Montemagni, Francesco Barsotti, Marco Battista, Nicoletta Calzolari, Ornella Corazzari, Alessandro Lenci, Antonio Zampolli, Francesca Fanciulli, Maria Massetani, Remo Raffaelli, Roberto Basili, Maria Teresa Pazienza, Dario Saracino, Fabio Zanzotto, Nadia Mana, Fabio Pianesi, and Rodolfo Delmonte. 2003. Building the Italian Syntactic–Semantic Treebank. In Anne Abeill´e, editor, Building and Using Syntactically Annotated Corpora. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity.</title>
<date>2005</date>
<booktitle>In Proceedings of NODALIDA Special Session on Treebanks.</booktitle>
<contexts>
<context position="16024" citStr="Nilsson et al., 2005" startWordPosition="2625" endWordPosition="2628">a-D/Z 40,000 and 11,428 sentences respectively. We did not use the Tiger test set from the CoNLL-X shared task since it is very small. We applied the TreeTagger POS tagger (Schmid, 1994) to both treebanks, using the pre-trained German model. 3.1.2 Swedish: Talbanken05 and Syntag As previously noted by Nivre (2002) inter alia, Swedish has a venerable tradition in treebanking: there are not only one but two treebanks which must be counted among the earliest efforts of that kind. The oldest one is the Talbanken or MAMBA treebank (Einarsson, 1976), which has later been reprocessed for modern use (Nilsson et al., 2005). The original annotation is a function-tagged constituent syntax without phrase labels, but the reprocessed release includes a version converted to dependency syntax. The dependency treebank was used in the CoNLL-X Shared Task (Buchholz and Marsi, 2006), and we used that version version in this work. The second treebank is called Syntag (Jirborg, 1986). Similar to Talbanken, its representation uses function-tagged constituents but no phrase labels. We developed a conversion to dependency trees, which was straightforward since many constituents 130 have explicitly defined heads (Johansson, 201</context>
</contexts>
<marker>Nilsson, Hall, Nivre, 2005</marker>
<rawString>Jens Nilsson, Johan Hall, and Joakim Nivre. 2005. MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity. In Proceedings of NODALIDA Special Session on Treebanks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
</authors>
<title>Generalizing tree transformations for inductive dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>968--975</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="27083" citStr="Nilsson et al., 2007" startWordPosition="4441" endWordPosition="4444">rsers. For WSJ Part 2, we analyzed the differences between the baseline and the best adapted parser. While there were improvements for all POS tags, the most notable one was in the attachment of conjunctions, where we got an increase from 69% to 75% in attachment accuracy, an 18% relative error reduction. Here we saw a very clear benefit of guided parsing: since this treebank uses “Prague-style” coordination annotation (i.e. the conjunction governs the conjuncts), it is hard for the parser to handle valencies and selectional preferences when there is a conjunction involved. It has been noted (Nilsson et al., 2007) that this style of annotating coordination is hard to parse. Since the WSJ Part 1 parser uses a coordination style that is easier to parse, the WSJ Part 2 parser can rely on its judgment. Although conclusions must be very tentative since we are testing on just four languages, we can make a few general observations. • The largest improvements (absolute and relative) all happen in treebanks that are harder to parse. In particular, Syntag and WSJ Part 2 are harder to parse due to their representation, and to some extent this may be true for Tiger as well – its learning curve rises more slowly th</context>
</contexts>
<marker>Nilsson, Nivre, Hall, 2007</marker>
<rawString>Jens Nilsson, Joakim Nivre, and Johan Hall. 2007. Generalizing tree transformations for inductive dependency parsing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 968–975, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
</authors>
<title>Exploiting heterogeneous treebanks for parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>46--54</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="34007" citStr="Niu et al., 2009" startWordPosition="5604" endWordPosition="5607">ment in the shared features framework by removing the label information from the shared feature representation fs, and similar modifications of fs could be carried out to handle more complex situations such as combined constituent and dependency parsing. Furthermore, the paths used by the feature extractor in the guided parser can be extended without much effort as well. The models presented here are very simple, and in future work we would like to explore more complex approaches such as quasi-synchronous grammars (Smith and Eisner, 2009; Li et al., 2012) or automatic treebank transformation (Niu et al., 2009). Acknowledgements I am grateful to the anonymous reviewers, whose feedback has helped to clarify the description of the methods. This research was supported by University of Gothenburg through its support of the Centre for Language Technology and Spr˚akbanken. It has been partly funded by the Swedish Research Council under grant number 2012-5738. WSJ part 1 error reduction Shared Guided Combined 30 25 20 15 10 5 0 101 102 103 104 105 Training set size (sentences) Error reduction (percent) 40 35 30 25 20 15 10 5 0 101 102 103 104 105 Training set size (sentences) Error reduction (percent) 40 3</context>
</contexts>
<marker>Niu, Wang, Wu, 2009</marker>
<rawString>Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Exploiting heterogeneous treebanks for parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 46–54, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, United States.</location>
<contexts>
<context position="10448" citStr="Nivre and McDonald (2008)" startWordPosition="1723" endWordPosition="1726">The second method is inspired by work in parser combination, an idea that has been applied successfully several times and relies on the fact that different parsing methods have different strengths and weaknesses (McDonald and Nivre, 2007), so that combining them may result in a better overall parsing accuracy. There are several ways to combine parsers; one of the simplest and most successful methods of parsing combination uses one parser as a guide for a second parser. This is normally implemented as a pipeline where the second parser extracts features based on the output of the first parser. Nivre and McDonald (2008) used this approach for combining a graph-based and a transition-based parser and achieved excellent results on test sets for several languages, and similar ideas were proposed by Martins et al. (2008). We added guide features to the parser feature representation. However, the features by Nivre and McDonald (2008) are slightly too simple since they only describe whether two words are directly connected or not. That makes sense if the two parsers are trying to predict the same type of representation, but will not help us if there are systematic annotation differences between the two treebanks, </context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>What kinds of trees grow in Swedish soil? A comparison of four annotation schemes for Swedish.</title>
<date>2002</date>
<booktitle>In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT2002), Sozopol,</booktitle>
<contexts>
<context position="15718" citStr="Nivre (2002)" startWordPosition="2574" endWordPosition="2575">he dependency versions are actually quite similar, at least with respect to attachment. The most common systematic difference we observed is in the annotation of coordination. Both treebanks are large: for Tiger, the training set was 31,243 sentences and the test set 7,973 sentences, and for T¨uBa-D/Z 40,000 and 11,428 sentences respectively. We did not use the Tiger test set from the CoNLL-X shared task since it is very small. We applied the TreeTagger POS tagger (Schmid, 1994) to both treebanks, using the pre-trained German model. 3.1.2 Swedish: Talbanken05 and Syntag As previously noted by Nivre (2002) inter alia, Swedish has a venerable tradition in treebanking: there are not only one but two treebanks which must be counted among the earliest efforts of that kind. The oldest one is the Talbanken or MAMBA treebank (Einarsson, 1976), which has later been reprocessed for modern use (Nilsson et al., 2005). The original annotation is a function-tagged constituent syntax without phrase labels, but the reprocessed release includes a version converted to dependency syntax. The dependency treebank was used in the CoNLL-X Shared Task (Buchholz and Marsi, 2006), and we used that version version in th</context>
</contexts>
<marker>Nivre, 2002</marker>
<rawString>Joakim Nivre. 2002. What kinds of trees grow in Swedish soil? A comparison of four annotation schemes for Swedish. In Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT2002), Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, United Kingdom.</location>
<contexts>
<context position="15589" citStr="Schmid, 1994" startWordPosition="2554" endWordPosition="2555"> amount of intermediate nodes, while T¨uBa-D/Z uses a more elaborate structure including topological field information. However, the dependency versions are actually quite similar, at least with respect to attachment. The most common systematic difference we observed is in the annotation of coordination. Both treebanks are large: for Tiger, the training set was 31,243 sentences and the test set 7,973 sentences, and for T¨uBa-D/Z 40,000 and 11,428 sentences respectively. We did not use the Tiger test set from the CoNLL-X shared task since it is very small. We applied the TreeTagger POS tagger (Schmid, 1994) to both treebanks, using the pre-trained German model. 3.1.2 Swedish: Talbanken05 and Syntag As previously noted by Nivre (2002) inter alia, Swedish has a venerable tradition in treebanking: there are not only one but two treebanks which must be counted among the earliest efforts of that kind. The oldest one is the Talbanken or MAMBA treebank (Einarsson, 1976), which has later been reprocessed for modern use (Nilsson et al., 2005). The original annotation is a function-tagged constituent syntax without phrase labels, but the reprocessed release includes a version converted to dependency synta</context>
<context position="19161" citStr="Schmid, 1994" startWordPosition="3116" endWordPosition="3117">ST representation. We simplified the tokenization of both treebanks. In ISST, we split multiwords into separate tokens and reattached clitics to nonfinite verb forms. For instance, a single token a causa di was converted into 1http://www.di.unito.it/-tutreeb/ three tokens a, causa, di, and the three tokens trovarse-lo into a single token trovarselo. In TUT, we applied the same conversions and also recomposed preposition–article and multiple-clitic contractions that had been split by the annotators, e.g. della, glielo etc.2 After changing the tokenization, we applied the TreeTagger POS tagger (Schmid, 1994) to both treebanks, using the pre-trained Italian model with the Baroni tagset3. After preprocessing the data, we created training and test sets. For ISST, the training set was 2,239 and the test set 1,120 sentences, while for TUT the training set was 1,906 and the test set 954 sentences. 3.1.4 English: Two Different Conversions of the Penn Treebank For English, there is no significant dependency treebank so we followed most previous work in using dependency trees automatically derived from constituent trees in the large Penn Treebank WSJ corpus (Marcus et al., 1993). Due to the fact that ther</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>822--831</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="33933" citStr="Smith and Eisner, 2009" startWordPosition="5591" endWordPosition="5594">pendency parsing. Labeled parsing with incompatible label sets is easy to implement in the shared features framework by removing the label information from the shared feature representation fs, and similar modifications of fs could be carried out to handle more complex situations such as combined constituent and dependency parsing. Furthermore, the paths used by the feature extractor in the guided parser can be extended without much effort as well. The models presented here are very simple, and in future work we would like to explore more complex approaches such as quasi-synchronous grammars (Smith and Eisner, 2009; Li et al., 2012) or automatic treebank transformation (Niu et al., 2009). Acknowledgements I am grateful to the anonymous reviewers, whose feedback has helped to clarify the description of the methods. This research was supported by University of Gothenburg through its support of the Centre for Language Technology and Spr˚akbanken. It has been partly funded by the Swedish Research Council under grant number 2012-5738. WSJ part 1 error reduction Shared Guided Combined 30 25 20 15 10 5 0 101 102 103 104 105 Training set size (sentences) Error reduction (percent) 40 35 30 25 20 15 10 5 0 101 10</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A. Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 822–831, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning,</booktitle>
<pages>159--177</pages>
<location>Manchester, United Kingdom.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages 159–177, Manchester, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Telljohann</author>
<author>Erhard Hinrichs</author>
<author>Sandra Kbler</author>
</authors>
<title>The T¨uba-D/Z treebank: Annotating German with a context-free backbone. In</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<pages>2229--2235</pages>
<contexts>
<context position="14611" citStr="Telljohann et al., 2004" startWordPosition="2399" endWordPosition="2403">plots. 3.1 Treebanks Used in the Experiments In our experiments, we used four languages: German, Swedish, Italian, and English. For each language, we had two treebanks. Our approaches currently require that the treebanks use the same tokenization conventions, so for Italian and Swedish we automatically retokenized the treebanks. We also made sure that the two treebanks for one language used the same part-of-speech tag sets, by applying an automatic tagger when necessary. 3.1.1 German: Tiger and T¨uBa-D/Z For German, there are two treebanks available: Tiger (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2004). These treebanks are constituent treebanks, but dependency versions are available: T¨uBa-D/Z (version 7.0) includes the dependency version in the distribution, while for Tiger we used the version from CoNLL-X (Buchholz and Marsi, 2006). The constituent annotation styles in the two treebanks are radically different: Tiger uses a very flat structure with a minimal amount of intermediate nodes, while T¨uBa-D/Z uses a more elaborate structure including topological field information. However, the dependency versions are actually quite similar, at least with respect to attachment. The most common s</context>
</contexts>
<marker>Telljohann, Hinrichs, Kbler, 2004</marker>
<rawString>Heike Telljohann, Erhard Hinrichs, and Sandra Kbler. 2004. The T¨uba-D/Z treebank: Annotating German with a context-free backbone. In In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004), pages 2229– 2235.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>