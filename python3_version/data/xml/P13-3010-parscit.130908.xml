<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011090">
<title confidence="0.971681">
Topic Modeling Based Classification of Clinical Reports
</title>
<author confidence="0.992909">
Efsun Sarioglu
</author>
<affiliation confidence="0.846219333333333">
Computer Science Department
The George Washington University
Washington, DC, USA
</affiliation>
<email confidence="0.995283">
efsun@gwu.edu
</email>
<author confidence="0.935567">
Kabir Yadav
</author>
<affiliation confidence="0.794404333333333">
Emergency Medicine Department
The George Washington University
Washington, DC, USA
</affiliation>
<email confidence="0.99365">
kyadav@gwu.edu
</email>
<author confidence="0.984204">
Hyeong-Ah Choi
</author>
<affiliation confidence="0.841972">
Computer Science Department
The George Washington University
Washington, DC, USA
</affiliation>
<email confidence="0.996181">
hchoi@gwu.edu
</email>
<sectionHeader confidence="0.993865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898387096774">
Electronic health records (EHRs) contain
important clinical information about pa-
tients. Some of these data are in the form
of free text and require preprocessing to be
able to used in automated systems. Effi-
cient and effective use of this data could
be vital to the speed and quality of health
care. As a case study, we analyzed clas-
sification of CT imaging reports into bi-
nary categories. In addition to regular
text classification, we utilized topic mod-
eling of the entire dataset in various ways.
Topic modeling of the corpora provides in-
terpretable themes that exist in these re-
ports. Representing reports according to
their topic distributions is more compact
than bag-of-words representation and can
be processed faster than raw text in sub-
sequent automated processes. A binary
topic model was also built as an unsuper-
vised classification approach with the as-
sumption that each topic corresponds to a
class. And, finally an aggregate topic clas-
sifier was built where reports are classified
based on a single discriminative topic that
is determined from the training dataset.
Our proposed topic based classifier system
is shown to be competitive with existing
text classification techniques and provides
a more efficient and interpretable repre-
sentation.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948928571429">
Large amounts of medical data are now stored as
electronic health records (EHRs). Some of these
data are in the form of free text and they need to
be processed and coded for better utilization in au-
tomatic or semi-automatic systems. One possible
utilization is to support clinical decision-making,
such as recommending the need for a certain med-
ical test while avoiding intrusive tests or medical
costs. This type of automated analysis of patient
reports can help medical professionals make clin-
ical decisions much faster with more confidence
by providing predicted outcomes. In this study,
we developed several topic modeling based classi-
fication systems for clinical reports.
Topic modeling is an unsupervised technique
that can automatically identify themes from a
given set of documents and find topic distribu-
tions of each document. Representing reports ac-
cording to their topic distributions is more com-
pact and can be processed faster than raw text in
subsequent automated processing. It has previ-
ously been shown that the biomedical concepts
can be well represented as noun phrases (Huang
et al., 2005) and nouns, compared to other parts
of speech, tend to specialize into topics (Griffiths
et al., 2004). Therefore, topic model output of pa-
tient reports could contain very useful clinical in-
formation.
</bodyText>
<sectionHeader confidence="0.971095" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999962636363636">
This study utilized prospective patient data pre-
viously collected for a traumatic orbital fracture
project (Yadav et al., 2012). Staff radiologists dic-
tated each CT report and the outcome of acute or-
bital fracture was extracted by a trained data ab-
stractor. Among the 3,705 reports, 3,242 had neg-
ative outcome while 463 had positive. A random
subset of 507 CT reports were double-coded, and
inter-rater analysis revealed excellent agreement
between the data abstractor and study physician,
with Cohen’s kappa of 0.97.
</bodyText>
<subsectionHeader confidence="0.996001">
2.1 Bag-of-Words (BoW) Representation
</subsectionHeader>
<bodyText confidence="0.994362">
Text data need to be converted to a suitable format
for automated processing. One way of doing this
is bag-of-words (BoW) representation where each
document becomes a vector of its words/tokens.
</bodyText>
<page confidence="0.993702">
67
</page>
<note confidence="0.514403">
Proceedings of the ACL Student Research Workshop, pages 67–73,
</note>
<page confidence="0.363175">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.9976165">
The entries in this matrix could be binary stating
the existence or absence of a word in a document
or it could be weighted such as number of times a
word exists in a document.
</bodyText>
<subsectionHeader confidence="0.998082">
2.2 Topic Modeling
</subsectionHeader>
<bodyText confidence="0.999955857142857">
Topic modeling is an unsupervised learning al-
gorithm that can automatically discover themes
of a document collection. Several techniques
can be used for this purpose such as Latent Se-
mantic Analysis (LSA) (Deerwester et al., 1990),
Probabilistic Latent Semantic Analysis (PLSA)
(Hofmann, 1999), and Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003). LSA is a way
of representing hidden semantic structure of a
term-document matrix where rows are documents
and columns are words/tokens (Deerwester et al.,
1990) based on Singular Value Decomposition
(SVD). One of the problems of LSA is that each
word is treated as having the same meaning due to
the word being represented as a single point; there-
fore in this representation, polysemes of words
cannot be differentiated. Also, the final output of
LSA, which consists of axes in Euclidean space, is
not interpretable or descriptive (Hofmann, 2001).
PLSA is considered probabilistic version of
LSA where an unobserved class variable zk ∈
{z1, ..., zK} is associated with each occurrence of
a word in a particular document (Hofmann, 1999).
These classes/topics are then inferred from the in-
put text collection. PLSA solves the polysemy
problem; however it is not considered a fully gen-
erative model of documents and it is known to be
overfitting (Blei et al., 2003). The number of pa-
rameters grows linearly with the number of docu-
ments.
LDA, first defined by (Blei et al., 2003), de-
fines topic as a distribution over a fixed vocabu-
lary, where each document can exhibit them with
different proportions. For each document, LDA
generates the words in a two-step process:
</bodyText>
<listItem confidence="0.973572857142857">
1. Randomly choose a distribution over topics.
2. For each word in the document:
(a) Randomly choose a topic from the dis-
tribution over topics.
(b) Randomly choose a word from the cor-
responding distribution over the vocab-
ulary.
</listItem>
<bodyText confidence="0.995345">
The probability of generating the word wj from
document di can be calculated as below:
</bodyText>
<equation confidence="0.999464">
K
P(wj|di; θ, φ) = E P(wj|zk; φz)P(zk|di; θd)
k=1
</equation>
<bodyText confidence="0.999996916666667">
where θ is sampled from a Dirichlet distribution
for each document di and φ is sampled from a
Dirichlet distribution for each topic zk. Either
sampling methods such as Gibbs Sampling (Grif-
fiths and Steyvers, 2004) or optimization methods
such as variational Bayes approximation (Asun-
cion et al., 2009) can be used to train a topic
model based on LDA. LDA performs better than
PLSA for small datasets since it avoids overfitting
and it supports polysemy (Blei et al., 2003). It is
also considered a fully generative system for doc-
uments in contrast to PLSA.
</bodyText>
<subsectionHeader confidence="0.994966">
2.3 Text Classification
</subsectionHeader>
<bodyText confidence="0.9999709">
Text classification is a supervised learning algo-
rithm where documents’ categories are learned
from pre-labeled set of documents. Support vec-
tor machines (SVM) is a popular classification al-
gorithm that attempts to find a decision bound-
ary between classes that is the farthest from any
point in the training dataset. Given labeled train-
ing data (xt, yt), t = 1, ..., N where xt ∈ RM and
yt ∈ {1, −1}, SVM tries to find a separating hy-
perplane with the maximum margin (Platt, 1998).
</bodyText>
<subsectionHeader confidence="0.621795">
2.3.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999985666666667">
Once the classifier is built, its performance is eval-
uated on training dataset. Its effectiveness is then
measured in the remaining unseen documents in
the testing set. To evaluate the classification per-
formance, precision, recall, and F-score measures
are typically used (Manning et al., 2008).
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999985916666667">
For text classification, topic modeling techniques
have been utilized in various ways. In (Zhang
et al., 2008), it is used as a keyword selection
mechanism by selecting the top words from topics
based on their entropy. In our study, we removed
the most frequent and infrequent words to have a
manageable vocabulary size but we did not utilize
topic model output for this purpose. (Sarioglu et
al., 2012) and (Sriurai, 2011) compare BoW rep-
resentation to topic model representation for clas-
sification using varying and fixed number of top-
ics respectively. This is similar to our topic vec-
</bodyText>
<page confidence="0.996703">
68
</page>
<bodyText confidence="0.99994175">
tor classification results with SVM, however (Sri-
urai, 2011) uses a fixed number of topics, whereas
we evaluated different number of topics since typ-
ically this is not known in advance. In (Baner-
jee, 2008), topics are used as additional features to
BoW features for the purpose of classification. In
our approaches, we used topic vector representa-
tion as an alternative to BoW and not additional.
This way, we can achieve great dimension reduc-
tion. Finally, (Chen et al., 2011) developed a re-
sampling approach based on topic modeling when
the class distributions are not balanced. In this
study, resampling approaches are also utilized to
compare skewed dataset results to datasets with
equal class distributions; however, we used ran-
domized resampling approaches for this purpose.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.997005666666667">
Figure 1 shows the three approaches of using topic
model of clinical reports to classify them and they
are explained below.
</bodyText>
<subsectionHeader confidence="0.989137">
4.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999866818181818">
During preprocessing, all protected health infor-
mation were removed to meet Institutional Re-
view Board requirements. Medical record num-
bers from each report were replaced by observa-
tion numbers, which are sequence numbers that
are automatically assigned to each report. Fre-
quent words were also removed from the vocabu-
lary to prevent it from getting too big. In addition,
these frequent words typically do not add much in-
formation; most of them were stop words such as
is, am, are, the, of, at, and.
</bodyText>
<subsectionHeader confidence="0.970602">
4.2 Topic Modeling
</subsectionHeader>
<bodyText confidence="0.999860857142857">
LDA was chosen to generate the topic models of
clinical reports due to its being a generative prob-
abilistic system for documents and its robustness
to overfitting. Stanford Topic Modeling Toolbox
(TMT) 1 was used to conduct the experiments
which is an open source software that provides
ways to train and infer topic models for text data.
</bodyText>
<subsectionHeader confidence="0.995676">
4.3 Topic Vectors
</subsectionHeader>
<bodyText confidence="0.99971275">
Topic modeling of reports produces a topic distri-
bution for each report which can be used to repre-
sent them as topic vectors. This is an alternative
representation to BoW where terms are replaced
</bodyText>
<footnote confidence="0.995383">
1http://nlp.stanford.edu/software/tmt/
tmt-0.4/
</footnote>
<bodyText confidence="0.999704">
with topics and entries for each report show the
probability of a specific topic for that report. This
representation is more compact than BoW as the
vocabulary for a text collection usually has thou-
sands of entries whereas a topic model is typically
built with a maximum of hundreds of topics.
</bodyText>
<subsectionHeader confidence="0.989481">
4.4 Supervised Classification
</subsectionHeader>
<bodyText confidence="0.999953928571429">
SVM was chosen as the classification algorithm as
it was shown that it performs well in text classifi-
cation tasks (Joachims, 1998; Yang and Liu, 1999)
and it is robust to overfitting (Sebastiani, 2002).
Weka was used to conduct classification which is a
collection of machine learning algorithms for data
mining tasks written in Java (Hall et al., 2009). It
uses attribute relationship file format (ARFF) to
store data in which each line represents a doc-
ument followed by its assigned class. Accord-
ingly, the raw text of the reports and topic vectors
are compiled into individual files with their cor-
responding outcomes in ARFF and then classified
with SVM.
</bodyText>
<subsectionHeader confidence="0.995334">
4.5 Aggregate Topic Classifier (ATC)
</subsectionHeader>
<bodyText confidence="0.999990705882353">
With this approach, a representative topic vector
for each class was composed by averaging their
corresponding topic distributions in the training
dataset. A discriminative topic was then chosen so
that the difference between positive and negative
representative vectors is maximum. The reports in
the test datasets were then classified by analyzing
the values of this topic and a threshold was cho-
sen to determine the predicted class. This thresh-
old could be chosen automatically based on class
distributions if the dataset is skewed or cross vali-
dation methods can be applied to pick a threshold
that gives the best classification performance in a
validation dataset. This approach is called Aggre-
gate Topic Classifier (ATC) since training labels
were utilized in an aggregate fashion using an av-
erage function and not individually.
</bodyText>
<subsectionHeader confidence="0.995177">
4.6 Binary Topic Classification (BTC)
</subsectionHeader>
<bodyText confidence="0.999950111111111">
Topic modeling of the data with two topics was
also analyzed as an unsupervised classification
technique. In this approach, binary topics were
assumed to correspond to the binary classes. After
topic model was learned, the topic with the higher
probability was assigned as the predicted class for
each document. If the dataset is skewed, which
topic corresponds to which class was found out by
checking predicted class proportions. For datasets
</bodyText>
<page confidence="0.99355">
69
</page>
<figure confidence="0.89831">
Raw Text
</figure>
<figureCaption confidence="0.999986">
Figure 1: System overview
</figureCaption>
<bodyText confidence="0.998649666666667">
with equal class distributions, each of the possi-
ble assignments were checked and the one with
the better classification performance was chosen.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999963846153846">
Classification results using ATC and SVM are
shown in Figures 2, 3, and 4 for precision, recall,
and f-score respectively. They are each divided
into five sections to show the result of using dif-
ferent training/testing proportions. These training
and test datasets were randomized and stratified to
make sure each subset is a good representation of
the original dataset. For ATC, we evaluated differ-
ent quantile points: 75, 80, 82, 85, 87 as threshold
and picked the one that gives the best classifica-
tion performance. These were chosen as candi-
dates based on the positive class ratio of original
dataset of 12%. Best classification performance
was achieved with 15 topics for ATC and 100 top-
ics for SVM. For smaller number of topics, ATC
performed better than SVM. As number of topics
increased, it got harder to find a very discrimina-
tive single topic and therefore ATC’s performance
got worse whereas SVM’s performance got better
as it got more information with more number of
topics. However, using topic vectors to represent
reports still provided great dimension reduction as
raw text of the reports had 1,296 terms and made
the subsequent classification with SVM faster. Fi-
nally, different training and test set proportions did
not have much effect on both of ATC’s and SVM’s
performance. This could be considered a good
outcome as using only 25% of data for training
would be sufficient to build an accurate classifier.
We analyzed the performance of classification
using binary topics with three datasets: original,
undersampled, and oversampled. In the under-
sampled dataset, excess amount of negative cases
were removed and the resulting dataset consisted
of 463 documents for each class. For oversampled
dataset, positive cases were oversampled while
keeping the total number of documents the same.
This approach produced a dataset consisting of
1,895 positive and 1,810 negative cases. With
the original dataset, we could see the performance
on a highly skewed real dataset and with the re-
sampled datasets, we could see the performance
on data with equal class distributions. Classifica-
tion results using this approach are summarized
in Table 2. As a baseline, a trivial rejector/zero
rule classifier was used. This classifier simply
predicted the majority class. Balanced datasets
performed better compared to skewed original
dataset using this approach. This is also due to
the fact that skewed dataset had a higher baseline
compared to the undersampled and oversampled
datasets. In Table 3, the best performance of each
</bodyText>
<figureCaption confidence="0.999969">
Figure 2: Precision
Figure 3: Recall
</figureCaption>
<bodyText confidence="0.9972615">
technique for the original dataset is summarized.
Although BTC performed better than baseline for
</bodyText>
<figure confidence="0.998540083333333">
Prepr*cess
Patient
Rep*rts
T*p1c
M*del1ng
T*p1c Vect*rs
Aggregate
T*p1c Class1fier
B1nary T*p1c
Class1ficati*n
Superv1sed
Class1ficati*n
</figure>
<page confidence="0.989862">
70
</page>
<tableCaption confidence="0.99994">
Table 1: Classification performance using ATC and SVM
</tableCaption>
<table confidence="0.9998585625">
K Dimension Train-Test (%) ATC SVM
Reduction (%)
Precision Recall F-score Precision Recall F-score
5 99.61 75 - 25 92.15 89.96 90.11 93.19 93.52 93.28
66 - 34 92.40 91.26 91.37 92.50 92.85 92.62
50 - 50 93.24 92.37 92.44 92.48 92.76 92.59
34 - 66 93.50 92.43 92.50 92.80 92.92 92.86
25 - 75 93.03 92.84 92.87 92.93 93.06 92.99
10 99.23 75 - 25 95.65 95.03 95.23 95.01 95.14 95.05
66 - 34 95.38 95.23 95.30 94.58 94.76 94.64
50 - 50 95.38 95.29 95.33 94.98 95.14 95.03
34 - 66 95.61 95.13 95.26 95.11 95.26 95.16
25 - 75 95.53 95.07 95.20 94.81 95.00 94.85
15 98.84 75 - 25 95.61 95.14 95.18 95.48 95.57 95.51
66 - 34 95.26 95.23 95.24 95.31 95.39 95.34
50 - 50 95.49 95.35 95.41 95.46 95.57 95.49
34 - 66 96.07 96.03 96.05 95.58 95.71 95.61
25 - 75 95.47 95.43 95.45 95.42 95.57 95.45
20 98.46 75 - 25 95.45 95.36 95.40 95.62 95.68 95.65
66 - 34 90.89 90.62 90.75 95.83 95.87 95.85
50 - 50 93.59 93.35 93.40 95.79 95.90 95.82
34 - 66 96.07 95.95 95.97 95.77 95.87 95.80
25 - 75 95.40 95.28 95.30 96.00 96.11 96.02
25 98.07 75 - 25 95.85 95.36 95.44 95.89 96.00 95.92
66 - 34 93.37 93.16 93.26 95.92 96.03 95.95
50 - 50 94.10 94.00 94.05 95.65 95.79 95.68
34 - 66 93.38 93.17 93.20 95.52 95.66 95.55
25 - 75 94.79 94.56 94.59 95.92 96.04 95.94
30 97.69 75 - 25 93.12 92.98 93.04 96.23 96.33 96.26
66 - 34 94.21 93.64 93.73 95.93 96.03 95.96
50 - 50 94.95 94.86 94.90 95.94 96.06 95.95
34 - 66 94.05 93.95 94.00 95.85 95.95 95.88
25 - 75 94.86 94.71 94.73 95.92 96.04 95.94
50 96.14 75 - 25 93.75 93.63 93.69 95.53 95.68 95.54
66 - 34 92.44 92.21 92.32 95.82 95.95 95.84
50 - 50 94.32 94.21 94.26 96.12 96.22 96.15
34 - 66 91.78 91.70 91.74 96.02 96.11 96.04
25 - 75 93.26 93.20 93.22 96.19 96.29 96.18
75 94.21 75 - 25 91.21 91.04 91.12 96.35 96.44 96.30
66 - 34 91.51 91.26 91.37 96.10 96.19 96.01
50 - 50 93.57 93.46 93.51 96.07 96.17 96.00
34 - 66 89.43 89.33 89.38 95.91 96.03 95.89
25 - 75 91.54 91.47 91.50 95.38 95.54 95.34
100 92.28 75 - 25 91.63 91.47 91.55 96.59 96.65 96.61
66 - 34 91.82 91.57 91.69 96.62 96.66 96.64
50 - 50 92.51 92.37 92.44 96.30 96.38 96.32
34 - 66 91.21 91.12 91.17 96.16 96.24 96.19
25 - 75 91.26 91.18 91.22 96.05 96.15 96.08
</table>
<page confidence="0.992316">
71
</page>
<figureCaption confidence="0.911438">
Figure 4: F-Score
</figureCaption>
<tableCaption confidence="0.965783">
Table 2: Binary Topic Classification Results
</tableCaption>
<table confidence="0.999647428571429">
Dataset Algorithm Precision Recall F-score
Original Baseline 76.6 87.5 81.7
BTC 88.6 73.4 77.7
Undersampled Baseline 49.6 49.7 47.6
BTC 84.4 84.2 84.2
Oversampled Baseline 26.2 51.1 34.6
BTC 83.4 82.5 82.5
</table>
<bodyText confidence="0.9989455">
datasets with equal class distribution, for the orig-
inal skewed dataset, it got worse results than the
baseline. ATC, on the other hand, got compara-
ble results with SVM using both topic vectors and
raw text. In addition, ATC used fewer number of
topics than SVM for its best performance.
</bodyText>
<tableCaption confidence="0.992964">
Table 3: Overall classification performance
</tableCaption>
<table confidence="0.9998515">
Algorithm Precision Recall F-score
Baseline 76.6 87.5 81.7
BTC 88.6 73.4 77.7
ATC 96.1 96.0 96.1
Topic vectors 96.6 96.7 96.6
Raw Text 96.4 96.3 96.3
</table>
<sectionHeader confidence="0.998741" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999975444444444">
In this study, topic modeling of clinical reports are
utilized in different ways with the end goal of clas-
sification. Firstly, bag-of-words representation is
replaced with topic vectors which provide good
dimensionality reduction and still get compara-
ble classification performance. In aggregate topic
classifier, representative topic vectors for positive
and negative classes are composed and used as
a guide to classify the reports in the test dataset.
This approach was competitive with classification
with SVM using raw text and topic vectors. In
addition, it required few topics to get the best per-
formance. And finally, in the unsupervised setting,
binary topic models are built for each dataset with
the assumption that each topic corresponds to a
class. For datasets with equal class distribution,
this approach showed improvement over baseline
approaches.
</bodyText>
<sectionHeader confidence="0.989252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996442">
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee-Whye Teh. 2009. On smoothing and inference
for topic models. In UAI.
Somnath Banerjee. 2008. Improving text classification
accuracy using topic modeling over an additional
corpus. In Proceedings of the 31st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 867–868.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3:993–1022.
Enhong Chen, Yanggang Lin, Hui Xiong, Qiming Luo,
and Haiping Ma. 2011. Exploiting probabilistic
topic models to improve text categorization under
class imbalance. Inf. Process. Manage., 47(2):202–
214.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228–5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2004. Integrating Topics
and Syntax. In NIPS, pages 537–544.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDDExplor. Newsl., 11(1):10–18.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In UAI.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Mach. Learn.,
42(1-2):177–196.
Yang Huang, Henry J Lowe, Dan Klein, and Russell J
Cucina. 2005. Improved identification of noun
phrases in clinical radiology reports using a high-
performance statistical natural language parser aug-
mented with the UMLS specialist lexicon. J Am
Med Inform Assoc, 12(3):275–285.
Thorsten Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with Many Rel-
evant Features. In Proceedings of the 10th European
Conference on Machine Learning, pages 137–142.
</reference>
<page confidence="0.970807">
72
</page>
<reference confidence="0.998478117647059">
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
John C. Platt. 1998. Sequential Minimal Optimiza-
tion: A Fast Algorithm for Training Support Vector
Machines.
Efsun Sarioglu, Kabir Yadav, and Hyeong-Ah Choi.
2012. Clinical Report Classification Using Natu-
ral Language Processing and Topic Modeling. 11th
International Conference on Machine Learning and
Applications (ICMLA), pages 204–209.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34(1):1–47.
Wongkot Sriurai. 2011. Improving Text Categoriza-
tion by Using a Topic Model. Advanced Computing:
An International Journal (ACIJ), 2(6).
Kabir Yadav, Ethan Cowan, Jason S Haukoos,
Zachary Ashwell, Vincent Nguyen, Paul Gennis,
and Stephen P Wall. 2012. Derivation of a clinical
risk score for traumatic orbital fracture. J Trauma
Acute Care Surg, 73(5):1313–1318.
Yiming Yang and Xin Liu. 1999. A re-examination
of text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 42–49.
Zhiwei Zhang, Xuan-Hieu Phan, and Susumu
Horiguchi. 2008. An Efficient Feature Selection
Using Hidden Topic in Text Categorization. In Pro-
ceedings of the 22nd International Conference on
Advanced Information Networking and Applications
- Workshops, AINAW ’08, pages 1223–1228.
</reference>
<page confidence="0.999279">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.104001">
<title confidence="0.999114">Topic Modeling Based Classification of Clinical Reports</title>
<author confidence="0.99122">Efsun Sarioglu</author>
<affiliation confidence="0.913662">Computer Science The George Washington</affiliation>
<address confidence="0.625164">Washington, DC,</address>
<email confidence="0.999424">efsun@gwu.edu</email>
<author confidence="0.933583">Kabir Yadav Emergency Medicine</author>
<affiliation confidence="0.631385">The George Washington</affiliation>
<address confidence="0.453015">Washington, DC,</address>
<email confidence="0.999342">kyadav@gwu.edu</email>
<author confidence="0.980776">Hyeong-Ah Choi</author>
<affiliation confidence="0.9185015">Computer Science The George Washington</affiliation>
<address confidence="0.605548">Washington, DC,</address>
<email confidence="0.999343">hchoi@gwu.edu</email>
<abstract confidence="0.99523075">Electronic health records (EHRs) contain important clinical information about patients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Max Welling</author>
<author>Padhraic Smyth</author>
<author>Yee-Whye Teh</author>
</authors>
<title>On smoothing and inference for topic models.</title>
<date>2009</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="6375" citStr="Asuncion et al., 2009" startWordPosition="1009" endWordPosition="1013">s. 2. For each word in the document: (a) Randomly choose a topic from the distribution over topics. (b) Randomly choose a word from the corresponding distribution over the vocabulary. The probability of generating the word wj from document di can be calculated as below: K P(wj|di; θ, φ) = E P(wj|zk; φz)P(zk|di; θd) k=1 where θ is sampled from a Dirichlet distribution for each document di and φ is sampled from a Dirichlet distribution for each topic zk. Either sampling methods such as Gibbs Sampling (Griffiths and Steyvers, 2004) or optimization methods such as variational Bayes approximation (Asuncion et al., 2009) can be used to train a topic model based on LDA. LDA performs better than PLSA for small datasets since it avoids overfitting and it supports polysemy (Blei et al., 2003). It is also considered a fully generative system for documents in contrast to PLSA. 2.3 Text Classification Text classification is a supervised learning algorithm where documents’ categories are learned from pre-labeled set of documents. Support vector machines (SVM) is a popular classification algorithm that attempts to find a decision boundary between classes that is the farthest from any point in the training dataset. Giv</context>
</contexts>
<marker>Asuncion, Welling, Smyth, Teh, 2009</marker>
<rawString>Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee-Whye Teh. 2009. On smoothing and inference for topic models. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somnath Banerjee</author>
</authors>
<title>Improving text classification accuracy using topic modeling over an additional corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>867--868</pages>
<contexts>
<context position="8260" citStr="Banerjee, 2008" startWordPosition="1329" endWordPosition="1331">ds from topics based on their entropy. In our study, we removed the most frequent and infrequent words to have a manageable vocabulary size but we did not utilize topic model output for this purpose. (Sarioglu et al., 2012) and (Sriurai, 2011) compare BoW representation to topic model representation for classification using varying and fixed number of topics respectively. This is similar to our topic vec68 tor classification results with SVM, however (Sriurai, 2011) uses a fixed number of topics, whereas we evaluated different number of topics since typically this is not known in advance. In (Banerjee, 2008), topics are used as additional features to BoW features for the purpose of classification. In our approaches, we used topic vector representation as an alternative to BoW and not additional. This way, we can achieve great dimension reduction. Finally, (Chen et al., 2011) developed a resampling approach based on topic modeling when the class distributions are not balanced. In this study, resampling approaches are also utilized to compare skewed dataset results to datasets with equal class distributions; however, we used randomized resampling approaches for this purpose. 4 Experiments Figure 1 </context>
</contexts>
<marker>Banerjee, 2008</marker>
<rawString>Somnath Banerjee. 2008. Improving text classification accuracy using topic modeling over an additional corpus. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 867–868.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="4442" citStr="Blei et al., 2003" startWordPosition="686" endWordPosition="689">ia, August 4-9 2013. c�2013 Association for Computational Linguistics The entries in this matrix could be binary stating the existence or absence of a word in a document or it could be weighted such as number of times a word exists in a document. 2.2 Topic Modeling Topic modeling is an unsupervised learning algorithm that can automatically discover themes of a document collection. Several techniques can be used for this purpose such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LSA is a way of representing hidden semantic structure of a term-document matrix where rows are documents and columns are words/tokens (Deerwester et al., 1990) based on Singular Value Decomposition (SVD). One of the problems of LSA is that each word is treated as having the same meaning due to the word being represented as a single point; therefore in this representation, polysemes of words cannot be differentiated. Also, the final output of LSA, which consists of axes in Euclidean space, is not interpretable or descriptive (Hofmann, 2001). PLSA is considered probabilistic version of LSA wh</context>
<context position="6546" citStr="Blei et al., 2003" startWordPosition="1041" endWordPosition="1044">abulary. The probability of generating the word wj from document di can be calculated as below: K P(wj|di; θ, φ) = E P(wj|zk; φz)P(zk|di; θd) k=1 where θ is sampled from a Dirichlet distribution for each document di and φ is sampled from a Dirichlet distribution for each topic zk. Either sampling methods such as Gibbs Sampling (Griffiths and Steyvers, 2004) or optimization methods such as variational Bayes approximation (Asuncion et al., 2009) can be used to train a topic model based on LDA. LDA performs better than PLSA for small datasets since it avoids overfitting and it supports polysemy (Blei et al., 2003). It is also considered a fully generative system for documents in contrast to PLSA. 2.3 Text Classification Text classification is a supervised learning algorithm where documents’ categories are learned from pre-labeled set of documents. Support vector machines (SVM) is a popular classification algorithm that attempts to find a decision boundary between classes that is the farthest from any point in the training dataset. Given labeled training data (xt, yt), t = 1, ..., N where xt ∈ RM and yt ∈ {1, −1}, SVM tries to find a separating hyperplane with the maximum margin (Platt, 1998). 2.3.1 Eva</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enhong Chen</author>
<author>Yanggang Lin</author>
<author>Hui Xiong</author>
<author>Qiming Luo</author>
<author>Haiping Ma</author>
</authors>
<title>Exploiting probabilistic topic models to improve text categorization under class imbalance.</title>
<date>2011</date>
<journal>Inf. Process. Manage.,</journal>
<volume>47</volume>
<issue>2</issue>
<pages>214</pages>
<contexts>
<context position="8532" citStr="Chen et al., 2011" startWordPosition="1373" endWordPosition="1376">n to topic model representation for classification using varying and fixed number of topics respectively. This is similar to our topic vec68 tor classification results with SVM, however (Sriurai, 2011) uses a fixed number of topics, whereas we evaluated different number of topics since typically this is not known in advance. In (Banerjee, 2008), topics are used as additional features to BoW features for the purpose of classification. In our approaches, we used topic vector representation as an alternative to BoW and not additional. This way, we can achieve great dimension reduction. Finally, (Chen et al., 2011) developed a resampling approach based on topic modeling when the class distributions are not balanced. In this study, resampling approaches are also utilized to compare skewed dataset results to datasets with equal class distributions; however, we used randomized resampling approaches for this purpose. 4 Experiments Figure 1 shows the three approaches of using topic model of clinical reports to classify them and they are explained below. 4.1 Preprocessing During preprocessing, all protected health information were removed to meet Institutional Review Board requirements. Medical record numbers</context>
</contexts>
<marker>Chen, Lin, Xiong, Luo, Ma, 2011</marker>
<rawString>Enhong Chen, Yanggang Lin, Hui Xiong, Qiming Luo, and Haiping Ma. 2011. Exploiting probabilistic topic models to improve text categorization under class imbalance. Inf. Process. Manage., 47(2):202– 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="4320" citStr="Deerwester et al., 1990" startWordPosition="669" endWordPosition="672">h document becomes a vector of its words/tokens. 67 Proceedings of the ACL Student Research Workshop, pages 67–73, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics The entries in this matrix could be binary stating the existence or absence of a word in a document or it could be weighted such as number of times a word exists in a document. 2.2 Topic Modeling Topic modeling is an unsupervised learning algorithm that can automatically discover themes of a document collection. Several techniques can be used for this purpose such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LSA is a way of representing hidden semantic structure of a term-document matrix where rows are documents and columns are words/tokens (Deerwester et al., 1990) based on Singular Value Decomposition (SVD). One of the problems of LSA is that each word is treated as having the same meaning due to the word being represented as a single point; therefore in this representation, polysemes of words cannot be differentiated. Also, the final output of LSA, which consists of axes i</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="6287" citStr="Griffiths and Steyvers, 2004" startWordPosition="996" endWordPosition="1000">nt, LDA generates the words in a two-step process: 1. Randomly choose a distribution over topics. 2. For each word in the document: (a) Randomly choose a topic from the distribution over topics. (b) Randomly choose a word from the corresponding distribution over the vocabulary. The probability of generating the word wj from document di can be calculated as below: K P(wj|di; θ, φ) = E P(wj|zk; φz)P(zk|di; θd) k=1 where θ is sampled from a Dirichlet distribution for each document di and φ is sampled from a Dirichlet distribution for each topic zk. Either sampling methods such as Gibbs Sampling (Griffiths and Steyvers, 2004) or optimization methods such as variational Bayes approximation (Asuncion et al., 2009) can be used to train a topic model based on LDA. LDA performs better than PLSA for small datasets since it avoids overfitting and it supports polysemy (Blei et al., 2003). It is also considered a fully generative system for documents in contrast to PLSA. 2.3 Text Classification Text classification is a supervised learning algorithm where documents’ categories are learned from pre-labeled set of documents. Support vector machines (SVM) is a popular classification algorithm that attempts to find a decision b</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101(suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating Topics and Syntax. In</title>
<date>2004</date>
<booktitle>NIPS,</booktitle>
<pages>537--544</pages>
<contexts>
<context position="2882" citStr="Griffiths et al., 2004" startWordPosition="441" endWordPosition="444">, we developed several topic modeling based classification systems for clinical reports. Topic modeling is an unsupervised technique that can automatically identify themes from a given set of documents and find topic distributions of each document. Representing reports according to their topic distributions is more compact and can be processed faster than raw text in subsequent automated processing. It has previously been shown that the biomedical concepts can be well represented as noun phrases (Huang et al., 2005) and nouns, compared to other parts of speech, tend to specialize into topics (Griffiths et al., 2004). Therefore, topic model output of patient reports could contain very useful clinical information. 2 Background This study utilized prospective patient data previously collected for a traumatic orbital fracture project (Yadav et al., 2012). Staff radiologists dictated each CT report and the outcome of acute orbital fracture was extracted by a trained data abstractor. Among the 3,705 reports, 3,242 had negative outcome while 463 had positive. A random subset of 507 CT reports were double-coded, and inter-rater analysis revealed excellent agreement between the data abstractor and study physician</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2004</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2004. Integrating Topics and Syntax. In NIPS, pages 537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="10791" citStr="Hall et al., 2009" startWordPosition="1739" endWordPosition="1742"> of a specific topic for that report. This representation is more compact than BoW as the vocabulary for a text collection usually has thousands of entries whereas a topic model is typically built with a maximum of hundreds of topics. 4.4 Supervised Classification SVM was chosen as the classification algorithm as it was shown that it performs well in text classification tasks (Joachims, 1998; Yang and Liu, 1999) and it is robust to overfitting (Sebastiani, 2002). Weka was used to conduct classification which is a collection of machine learning algorithms for data mining tasks written in Java (Hall et al., 2009). It uses attribute relationship file format (ARFF) to store data in which each line represents a document followed by its assigned class. Accordingly, the raw text of the reports and topic vectors are compiled into individual files with their corresponding outcomes in ARFF and then classified with SVM. 4.5 Aggregate Topic Classifier (ATC) With this approach, a representative topic vector for each class was composed by averaging their corresponding topic distributions in the training dataset. A discriminative topic was then chosen so that the difference between positive and negative representa</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="4383" citStr="Hofmann, 1999" startWordPosition="678" endWordPosition="679">L Student Research Workshop, pages 67–73, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics The entries in this matrix could be binary stating the existence or absence of a word in a document or it could be weighted such as number of times a word exists in a document. 2.2 Topic Modeling Topic modeling is an unsupervised learning algorithm that can automatically discover themes of a document collection. Several techniques can be used for this purpose such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LSA is a way of representing hidden semantic structure of a term-document matrix where rows are documents and columns are words/tokens (Deerwester et al., 1990) based on Singular Value Decomposition (SVD). One of the problems of LSA is that each word is treated as having the same meaning due to the word being represented as a single point; therefore in this representation, polysemes of words cannot be differentiated. Also, the final output of LSA, which consists of axes in Euclidean space, is not interpretable or descriptive (Hofmann</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<pages>42--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="4990" citStr="Hofmann, 2001" startWordPosition="777" endWordPosition="778">, 1999), and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LSA is a way of representing hidden semantic structure of a term-document matrix where rows are documents and columns are words/tokens (Deerwester et al., 1990) based on Singular Value Decomposition (SVD). One of the problems of LSA is that each word is treated as having the same meaning due to the word being represented as a single point; therefore in this representation, polysemes of words cannot be differentiated. Also, the final output of LSA, which consists of axes in Euclidean space, is not interpretable or descriptive (Hofmann, 2001). PLSA is considered probabilistic version of LSA where an unobserved class variable zk ∈ {z1, ..., zK} is associated with each occurrence of a word in a particular document (Hofmann, 1999). These classes/topics are then inferred from the input text collection. PLSA solves the polysemy problem; however it is not considered a fully generative model of documents and it is known to be overfitting (Blei et al., 2003). The number of parameters grows linearly with the number of documents. LDA, first defined by (Blei et al., 2003), defines topic as a distribution over a fixed vocabulary, where each d</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Mach. Learn., 42(1-2):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Huang</author>
<author>Henry J Lowe</author>
<author>Dan Klein</author>
<author>Russell J Cucina</author>
</authors>
<title>Improved identification of noun phrases in clinical radiology reports using a highperformance statistical natural language parser augmented with the UMLS specialist lexicon.</title>
<date>2005</date>
<journal>J Am Med Inform Assoc,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2780" citStr="Huang et al., 2005" startWordPosition="424" endWordPosition="427">clinical decisions much faster with more confidence by providing predicted outcomes. In this study, we developed several topic modeling based classification systems for clinical reports. Topic modeling is an unsupervised technique that can automatically identify themes from a given set of documents and find topic distributions of each document. Representing reports according to their topic distributions is more compact and can be processed faster than raw text in subsequent automated processing. It has previously been shown that the biomedical concepts can be well represented as noun phrases (Huang et al., 2005) and nouns, compared to other parts of speech, tend to specialize into topics (Griffiths et al., 2004). Therefore, topic model output of patient reports could contain very useful clinical information. 2 Background This study utilized prospective patient data previously collected for a traumatic orbital fracture project (Yadav et al., 2012). Staff radiologists dictated each CT report and the outcome of acute orbital fracture was extracted by a trained data abstractor. Among the 3,705 reports, 3,242 had negative outcome while 463 had positive. A random subset of 507 CT reports were double-coded,</context>
</contexts>
<marker>Huang, Lowe, Klein, Cucina, 2005</marker>
<rawString>Yang Huang, Henry J Lowe, Dan Klein, and Russell J Cucina. 2005. Improved identification of noun phrases in clinical radiology reports using a highperformance statistical natural language parser augmented with the UMLS specialist lexicon. J Am Med Inform Assoc, 12(3):275–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="10567" citStr="Joachims, 1998" startWordPosition="1704" endWordPosition="1705"> used to represent them as topic vectors. This is an alternative representation to BoW where terms are replaced 1http://nlp.stanford.edu/software/tmt/ tmt-0.4/ with topics and entries for each report show the probability of a specific topic for that report. This representation is more compact than BoW as the vocabulary for a text collection usually has thousands of entries whereas a topic model is typically built with a maximum of hundreds of topics. 4.4 Supervised Classification SVM was chosen as the classification algorithm as it was shown that it performs well in text classification tasks (Joachims, 1998; Yang and Liu, 1999) and it is robust to overfitting (Sebastiani, 2002). Weka was used to conduct classification which is a collection of machine learning algorithms for data mining tasks written in Java (Hall et al., 2009). It uses attribute relationship file format (ARFF) to store data in which each line represents a document followed by its assigned class. Accordingly, the raw text of the reports and topic vectors are compiled into individual files with their corresponding outcomes in ARFF and then classified with SVM. 4.5 Aggregate Topic Classifier (ATC) With this approach, a representati</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proceedings of the 10th European Conference on Machine Learning, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines.</title>
<date>1998</date>
<contexts>
<context position="7135" citStr="Platt, 1998" startWordPosition="1146" endWordPosition="1147">emy (Blei et al., 2003). It is also considered a fully generative system for documents in contrast to PLSA. 2.3 Text Classification Text classification is a supervised learning algorithm where documents’ categories are learned from pre-labeled set of documents. Support vector machines (SVM) is a popular classification algorithm that attempts to find a decision boundary between classes that is the farthest from any point in the training dataset. Given labeled training data (xt, yt), t = 1, ..., N where xt ∈ RM and yt ∈ {1, −1}, SVM tries to find a separating hyperplane with the maximum margin (Platt, 1998). 2.3.1 Evaluation Once the classifier is built, its performance is evaluated on training dataset. Its effectiveness is then measured in the remaining unseen documents in the testing set. To evaluate the classification performance, precision, recall, and F-score measures are typically used (Manning et al., 2008). 3 Related Work For text classification, topic modeling techniques have been utilized in various ways. In (Zhang et al., 2008), it is used as a keyword selection mechanism by selecting the top words from topics based on their entropy. In our study, we removed the most frequent and infr</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>John C. Platt. 1998. Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efsun Sarioglu</author>
<author>Kabir Yadav</author>
<author>Hyeong-Ah Choi</author>
</authors>
<title>Clinical Report Classification Using Natural Language Processing and Topic Modeling.</title>
<date>2012</date>
<booktitle>11th International Conference on Machine Learning and Applications (ICMLA),</booktitle>
<pages>204--209</pages>
<contexts>
<context position="7868" citStr="Sarioglu et al., 2012" startWordPosition="1262" endWordPosition="1265">eness is then measured in the remaining unseen documents in the testing set. To evaluate the classification performance, precision, recall, and F-score measures are typically used (Manning et al., 2008). 3 Related Work For text classification, topic modeling techniques have been utilized in various ways. In (Zhang et al., 2008), it is used as a keyword selection mechanism by selecting the top words from topics based on their entropy. In our study, we removed the most frequent and infrequent words to have a manageable vocabulary size but we did not utilize topic model output for this purpose. (Sarioglu et al., 2012) and (Sriurai, 2011) compare BoW representation to topic model representation for classification using varying and fixed number of topics respectively. This is similar to our topic vec68 tor classification results with SVM, however (Sriurai, 2011) uses a fixed number of topics, whereas we evaluated different number of topics since typically this is not known in advance. In (Banerjee, 2008), topics are used as additional features to BoW features for the purpose of classification. In our approaches, we used topic vector representation as an alternative to BoW and not additional. This way, we can</context>
</contexts>
<marker>Sarioglu, Yadav, Choi, 2012</marker>
<rawString>Efsun Sarioglu, Kabir Yadav, and Hyeong-Ah Choi. 2012. Clinical Report Classification Using Natural Language Processing and Topic Modeling. 11th International Conference on Machine Learning and Applications (ICMLA), pages 204–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Comput. Surv.,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="10639" citStr="Sebastiani, 2002" startWordPosition="1716" endWordPosition="1717">esentation to BoW where terms are replaced 1http://nlp.stanford.edu/software/tmt/ tmt-0.4/ with topics and entries for each report show the probability of a specific topic for that report. This representation is more compact than BoW as the vocabulary for a text collection usually has thousands of entries whereas a topic model is typically built with a maximum of hundreds of topics. 4.4 Supervised Classification SVM was chosen as the classification algorithm as it was shown that it performs well in text classification tasks (Joachims, 1998; Yang and Liu, 1999) and it is robust to overfitting (Sebastiani, 2002). Weka was used to conduct classification which is a collection of machine learning algorithms for data mining tasks written in Java (Hall et al., 2009). It uses attribute relationship file format (ARFF) to store data in which each line represents a document followed by its assigned class. Accordingly, the raw text of the reports and topic vectors are compiled into individual files with their corresponding outcomes in ARFF and then classified with SVM. 4.5 Aggregate Topic Classifier (ATC) With this approach, a representative topic vector for each class was composed by averaging their correspon</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Comput. Surv., 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wongkot Sriurai</author>
</authors>
<title>Improving Text Categorization by Using a Topic Model.</title>
<date>2011</date>
<journal>Advanced Computing: An International Journal (ACIJ),</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="7888" citStr="Sriurai, 2011" startWordPosition="1267" endWordPosition="1268">e remaining unseen documents in the testing set. To evaluate the classification performance, precision, recall, and F-score measures are typically used (Manning et al., 2008). 3 Related Work For text classification, topic modeling techniques have been utilized in various ways. In (Zhang et al., 2008), it is used as a keyword selection mechanism by selecting the top words from topics based on their entropy. In our study, we removed the most frequent and infrequent words to have a manageable vocabulary size but we did not utilize topic model output for this purpose. (Sarioglu et al., 2012) and (Sriurai, 2011) compare BoW representation to topic model representation for classification using varying and fixed number of topics respectively. This is similar to our topic vec68 tor classification results with SVM, however (Sriurai, 2011) uses a fixed number of topics, whereas we evaluated different number of topics since typically this is not known in advance. In (Banerjee, 2008), topics are used as additional features to BoW features for the purpose of classification. In our approaches, we used topic vector representation as an alternative to BoW and not additional. This way, we can achieve great dimen</context>
</contexts>
<marker>Sriurai, 2011</marker>
<rawString>Wongkot Sriurai. 2011. Improving Text Categorization by Using a Topic Model. Advanced Computing: An International Journal (ACIJ), 2(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kabir Yadav</author>
<author>Ethan Cowan</author>
<author>Jason S Haukoos</author>
<author>Zachary Ashwell</author>
<author>Vincent Nguyen</author>
<author>Paul Gennis</author>
<author>Stephen P Wall</author>
</authors>
<title>Derivation of a clinical risk score for traumatic orbital fracture.</title>
<date>2012</date>
<journal>J Trauma Acute Care Surg,</journal>
<volume>73</volume>
<issue>5</issue>
<contexts>
<context position="3121" citStr="Yadav et al., 2012" startWordPosition="477" endWordPosition="480">ment. Representing reports according to their topic distributions is more compact and can be processed faster than raw text in subsequent automated processing. It has previously been shown that the biomedical concepts can be well represented as noun phrases (Huang et al., 2005) and nouns, compared to other parts of speech, tend to specialize into topics (Griffiths et al., 2004). Therefore, topic model output of patient reports could contain very useful clinical information. 2 Background This study utilized prospective patient data previously collected for a traumatic orbital fracture project (Yadav et al., 2012). Staff radiologists dictated each CT report and the outcome of acute orbital fracture was extracted by a trained data abstractor. Among the 3,705 reports, 3,242 had negative outcome while 463 had positive. A random subset of 507 CT reports were double-coded, and inter-rater analysis revealed excellent agreement between the data abstractor and study physician, with Cohen’s kappa of 0.97. 2.1 Bag-of-Words (BoW) Representation Text data need to be converted to a suitable format for automated processing. One way of doing this is bag-of-words (BoW) representation where each document becomes a vect</context>
</contexts>
<marker>Yadav, Cowan, Haukoos, Ashwell, Nguyen, Gennis, Wall, 2012</marker>
<rawString>Kabir Yadav, Ethan Cowan, Jason S Haukoos, Zachary Ashwell, Vincent Nguyen, Paul Gennis, and Stephen P Wall. 2012. Derivation of a clinical risk score for traumatic orbital fracture. J Trauma Acute Care Surg, 73(5):1313–1318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="10588" citStr="Yang and Liu, 1999" startWordPosition="1706" endWordPosition="1709">nt them as topic vectors. This is an alternative representation to BoW where terms are replaced 1http://nlp.stanford.edu/software/tmt/ tmt-0.4/ with topics and entries for each report show the probability of a specific topic for that report. This representation is more compact than BoW as the vocabulary for a text collection usually has thousands of entries whereas a topic model is typically built with a maximum of hundreds of topics. 4.4 Supervised Classification SVM was chosen as the classification algorithm as it was shown that it performs well in text classification tasks (Joachims, 1998; Yang and Liu, 1999) and it is robust to overfitting (Sebastiani, 2002). Weka was used to conduct classification which is a collection of machine learning algorithms for data mining tasks written in Java (Hall et al., 2009). It uses attribute relationship file format (ARFF) to store data in which each line represents a document followed by its assigned class. Accordingly, the raw text of the reports and topic vectors are compiled into individual files with their corresponding outcomes in ARFF and then classified with SVM. 4.5 Aggregate Topic Classifier (ATC) With this approach, a representative topic vector for e</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiwei Zhang</author>
<author>Xuan-Hieu Phan</author>
<author>Susumu Horiguchi</author>
</authors>
<title>An Efficient Feature Selection Using Hidden Topic in Text Categorization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Advanced Information Networking and Applications - Workshops, AINAW ’08,</booktitle>
<pages>1223--1228</pages>
<contexts>
<context position="7575" citStr="Zhang et al., 2008" startWordPosition="1211" endWordPosition="1214">aining dataset. Given labeled training data (xt, yt), t = 1, ..., N where xt ∈ RM and yt ∈ {1, −1}, SVM tries to find a separating hyperplane with the maximum margin (Platt, 1998). 2.3.1 Evaluation Once the classifier is built, its performance is evaluated on training dataset. Its effectiveness is then measured in the remaining unseen documents in the testing set. To evaluate the classification performance, precision, recall, and F-score measures are typically used (Manning et al., 2008). 3 Related Work For text classification, topic modeling techniques have been utilized in various ways. In (Zhang et al., 2008), it is used as a keyword selection mechanism by selecting the top words from topics based on their entropy. In our study, we removed the most frequent and infrequent words to have a manageable vocabulary size but we did not utilize topic model output for this purpose. (Sarioglu et al., 2012) and (Sriurai, 2011) compare BoW representation to topic model representation for classification using varying and fixed number of topics respectively. This is similar to our topic vec68 tor classification results with SVM, however (Sriurai, 2011) uses a fixed number of topics, whereas we evaluated differe</context>
</contexts>
<marker>Zhang, Phan, Horiguchi, 2008</marker>
<rawString>Zhiwei Zhang, Xuan-Hieu Phan, and Susumu Horiguchi. 2008. An Efficient Feature Selection Using Hidden Topic in Text Categorization. In Proceedings of the 22nd International Conference on Advanced Information Networking and Applications - Workshops, AINAW ’08, pages 1223–1228.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>