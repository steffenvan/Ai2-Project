<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003214">
<title confidence="0.985005">
Learning for Semantic Parsing with Statistical Machine Translation
</title>
<author confidence="0.912372">
Yuk Wah Wong and Raymond J. Mooney
</author>
<affiliation confidence="0.944415666666667">
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.684438">
Austin, TX 78712-0233, USA
</address>
<email confidence="0.999412">
{ywwong,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.994815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925722222222">
We present a novel statistical approach to
semantic parsing, WASP, for construct-
ing a complete, formal meaning represen-
tation of a sentence. A semantic parser
is learned given a set of sentences anno-
tated with their correct meaning represen-
tations. The main innovation of WASP
is its use of state-of-the-art statistical ma-
chine translation techniques. A word
alignment model is used for lexical acqui-
sition, and the parsing model itself can be
seen as a syntax-based translation model.
We show that WASP performs favorably
in terms of both accuracy and coverage
compared to existing learning methods re-
quiring similar amount of supervision, and
shows better robustness to variations in
task complexity and word order.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.880642294117647">
Recent work on natural language understanding has
mainly focused on shallow semantic analysis, such
as semantic role labeling and word-sense disam-
biguation. This paper considers a more ambi-
tious task of semantic parsing, which is the con-
struction of a complete, formal, symbolic, mean-
ing representation (MR) of a sentence. Seman-
tic parsing has found its way in practical applica-
tions such as natural-language (NL) interfaces to
databases (Androutsopoulos et al., 1995) and ad-
vice taking (Kuhlmann et al., 2004). Figure 1 shows
a sample MR written in a meaning-representation
language (MRL) called CLANG, which is used for
((bowner our {4})
(do our {6} (pos (left (half our)))))
If our player 4 has the ball, then our player 6 should
stay in the left side of our half.
</bodyText>
<figureCaption confidence="0.999705">
Figure 1: A meaning representation in CLANG
</figureCaption>
<bodyText confidence="0.999610655172414">
encoding coach advice given to simulated soccer-
playing agents (Kuhlmann et al., 2004).
Prior research in semantic parsing has mainly fo-
cused on relatively simple domains such as ATIS
(Air Travel Information Service) (Miller et al., 1996;
Papineni et al., 1997; Macherey et al., 2001), in
which a typcial MR is only a single semantic frame.
Learning methods have been devised that can gen-
erate MRs with a complex, nested structure (cf.
Figure 1). However, these methods are mostly
based on deterministic parsing (Zelle and Mooney,
1996; Kate et al., 2005), which lack the robustness
that characterizes recent advances in statistical NLP.
Other learning methods involve the use of fully-
annotated augmented parse trees (Ge and Mooney,
2005) or prior knowledge of the NL syntax (Zettle-
moyer and Collins, 2005) in training, and hence re-
quire extensive human efforts when porting to a new
domain or language.
In this paper, we present a novel statistical ap-
proach to semantic parsing which can handle MRs
with a nested structure, based on previous work on
semantic parsing using transformation rules (Kate et
al., 2005). The algorithm learns a semantic parser
given a set of NL sentences annotated with their
correct MRs. It requires no prior knowledge of
the NL syntax, although it assumes that an unam-
biguous, context-free grammar (CFG) of the target
MRL is available. The main innovation of this al-
</bodyText>
<page confidence="0.986198">
439
</page>
<note confidence="0.915717">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 439–446,
New York, June 2006. c�2006 Association for Computational Linguistics
answer(count(city(loc 2(countryid(usa)))))
How many cities are there in the US?
</note>
<figureCaption confidence="0.999215">
Figure 2: A meaning representation in GEOQUERY
</figureCaption>
<bodyText confidence="0.99981803125">
shows a sample query in this language. Note that
both domains involve the use of MRs with a com-
plex, nested structure.
gorithm is its integration with state-of-the-art statis-
tical machine translation techniques. More specif-
ically, a statistical word alignment model (Brown
et al., 1993) is used to acquire a bilingual lexi-
con consisting of NL substrings coupled with their
translations in the target MRL. Complete MRs are
then formed by combining these NL substrings and
their translations under a parsing framework called
the synchronous CFG (Aho and Ullman, 1972),
which forms the basis of most existing statisti-
cal syntax-based translation models (Yamada and
Knight, 2001; Chiang, 2005). Our algorithm is
called WASP, short for Word Alignment-based Se-
mantic Parsing. In initial evaluation on several
real-world data sets, we show that WASP performs
favorably in terms of both accuracy and coverage
compared to existing learning methods requiring the
same amount of supervision, and shows better ro-
bustness to variations in task complexity and word
order.
Section 2 provides a brief overview of the do-
mains being considered. In Section 3, we present
the semantic parsing model of WASP. Section 4 out-
lines the algorithm for acquiring a bilingual lexicon
through the use of word alignments. Section 5 de-
scribes a probabilistic model for semantic parsing.
Finally, we report on experiments that show the ro-
bustness of WASP in Section 6, followed by the con-
clusion in Section 7.
</bodyText>
<sectionHeader confidence="0.942911" genericHeader="method">
2 Application Domains
</sectionHeader>
<bodyText confidence="0.999071416666667">
In this paper, we consider two domains. The first do-
main is ROBOCUP. ROBOCUP (www.robocup.org)
is an AI research initiative using robotic soccer as its
primary domain. In the ROBOCUP Coach Competi-
tion, teams of agents compete on a simulated soccer
field and receive coach advice written in a formal
language called CLANG (Chen et al., 2003). Fig-
ure 1 shows a sample MR in CLANG.
The second domain is GEOQUERY, where a func-
tional, variable-free query language is used for
querying a small database on U.S. geography (Zelle
and Mooney, 1996; Kate et al., 2005). Figure 2
</bodyText>
<sectionHeader confidence="0.955759" genericHeader="method">
3 The Semantic Parsing Model
</sectionHeader>
<bodyText confidence="0.999280257142857">
To describe the semantic parsing model of WASP,
it is best to start with an example. Consider the
task of translating the sentence in Figure 1 into its
MR in CLANG. To achieve this task, we may first
analyze the syntactic structure of the sentence us-
ing a semantic grammar (Allen, 1995), whose non-
terminals are the ones in the CLANG grammar. The
meaning of the sentence is then obtained by com-
bining the meanings of its sub-parts according to
the semantic parse. Figure 3(a) shows a possible
partial semantic parse of the sample sentence based
on CLANG non-terminals (UNUM stands for uni-
form number). Figure 3(b) shows the corresponding
CLANG parse from which the MR is constructed.
This process can be formalized as an instance of
synchronous parsing (Aho and Ullman, 1972), orig-
inally developed as a theory of compilers in which
syntax analysis and code generation are combined
into a single phase. Synchronous parsing has seen a
surge of interest recently in the machine translation
community as a way of formalizing syntax-based
translation models (Melamed, 2004; Chiang, 2005).
According to this theory, a semantic parser defines a
translation, a set of pairs of strings in which each
pair is an NL sentence coupled with its MR. To
finitely specify a potentially infinite translation, we
use a synchronous context-free grammar (SCFG) for
generating the pairs in a translation. Analogous to
an ordinary CFG, each SCFG rule consists of a sin-
gle non-terminal on the left-hand side (LHS). The
right-hand side (RHS) of an SCFG rule is a pair of
strings, (α, Q), where the non-terminals in 0 are a
permutation of the non-terminals in α. Below are
some SCFG rules that can be used for generating the
parse trees in Figure 3:
</bodyText>
<table confidence="0.842196833333333">
RULE (if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 ))
CONDITION (TEAM 1 player UNUM 2 has the ball ,
(bowner TEAM 1 {UNUM 2 }))
TEAM (our , our)
UNUM (4 , 4)
</table>
<page confidence="0.994551">
440
</page>
<figure confidence="0.9931510625">
RULE RULE
...
...)
If CONDITION
( CONDITION
})
player UNUM
4
has the ball
(bowner TEAM
our
{ UNUM
4
TEAM
our
(a) English (b) CLANG
</figure>
<figureCaption confidence="0.999898">
Figure 3: Partial parse trees for the CLANG statement and its English gloss shown in Figure 1
</figureCaption>
<bodyText confidence="0.956354534883721">
Each SCFG rule X → (α, Q) is a combination of a
production of the NL semantic grammar, X → α,
and a production of the MRL grammar, X → Q.
Each rule corresponds to a transformation rule in
Kate et al. (2005). Following their terminology,
we call the string α a pattern, and the string Q a
template. Non-terminals are indexed to show their
association between a pattern and a template. All
derivations start with a pair of associated start sym-
bols, (51 , 51 ). Each step of a derivation involves
the rewriting of a pair of associated non-terminals
in both of the NL and MRL streams. Below is a
derivation that would generate the sample sentence
and its MR simultaneously: (Note that RULE is the
start symbol for CLANG)
(RULE 1 , RULE 1 )
⇒ (if CONDITION 1 , DIRECTIVE 2 . ,
(CONDITION 1 DIRECTIVE 2 ))
⇒ (if TEAM 1 player UNUM 2 has the ball, DIR 3 . ,
((bowner TEAM 1 {UNUM 2 }) DIR 3 ))
⇒ (if our player UNUM 1 has the ball, DIR 2 . ,
((bowner our {UNUM 1 }) DIR 2 ))
⇒ (if our player 4 has the ball, DIRECTIVE 1 . ,
((bowner our {4}) DIRECTIVE 1 ))
⇒ ...
⇒(if our player 4 has the ball, then our player 6
should stay in the left side of our half. ,
((bowner our {4})
(do our {6} (pos (left (half our))))))
Here the MR string is said to be a translation of the
NL string. Given an input sentence, e, the task of
semantic parsing is to find a derivation that yields
(e, f),
be multiple derivations that yield e (and thus mul-
tiple possible translations of e), a mechanism must
be devised for discriminating the correct derivation
from the incorrect ones.
The semantic parsing model of WASP thus con-
sists of an SCFG, G, and a probabilistic model, pa-
rameterized by A, that takes a possible derivation, d,
and returns its likelihood of being correct given an
input sentence, e. The output translation, f*, for a
sentence, e, is defined as:
</bodyText>
<equation confidence="0.9986175">
f* = marg max
(d∈D(G|e) �Prλ(d|e) (1)
</equation>
<bodyText confidence="0.999906608695652">
where m(d) is the MR string that a derivation d
yields, and D(G|e) is the set of all possible deriva-
tions of G that yield e. In other words, the output
MR is the yield of the most probable derivation that
yields e in the NL stream.
The learning task is to induce a set of SCFG rules,
which we call a lexicon, and a probabilistic model
for derivations. A lexicon defines the set of deriva-
tions that are possible, so the induction of a proba-
bilistic model first requires a lexicon. Therefore, the
learning task can be separated into two sub-tasks:
(1) the induction of a lexicon, followed by (2) the
induction of a probabilistic model. Both sub-tasks
require a training set, {(ei, fi)}, where each training
example (ei, fi) is an NL sentence, ei, paired with
its correct MR, fi. Lexical induction also requires
an unambiguous CFG of the MRL. Since there is no
lexicon to begin with, it is not possible to include
correct derivations in the training data. This is un-
like most recent work on syntactic parsing based on
gold-standard treebanks. Therefore, the induction of
a probabilistic model for derivations is done in an
unsupervised manner.
</bodyText>
<sectionHeader confidence="0.986795" genericHeader="method">
4 Lexical Acquisition
</sectionHeader>
<bodyText confidence="0.989658666666667">
In this section, we focus on lexical learning, which
is done by finding optimal word alignments between
so that f is a translation of e. Since there may
</bodyText>
<page confidence="0.987608">
441
</page>
<table confidence="0.794873857142857">
If RULE —* (CONDITION DIRECTIVE)
our CONDITION —* (bowner TEAM {UNUM})
player TEAM —* our
4 UNUM —* 4
has
the
ball
</table>
<figureCaption confidence="0.990476">
Figure 4: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1
</figureCaption>
<bodyText confidence="0.999931302631579">
NL sentences and their MRs in the training set. By
defining a mapping of words from one language to
another, word alignments define a bilingual lexicon.
Using word alignments to induce a lexicon is not a
new idea (Och and Ney, 2003). Indeed, attempts
have been made to directly apply machine transla-
tion systems to the problem of semantic parsing (Pa-
pineni et al., 1997; Macherey et al., 2001). However,
these systems make no use of the MRL grammar,
thus allocating probability mass to MR translations
that are not even syntactically well-formed. Here we
present a lexical induction algorithm that guarantees
syntactic well-formedness of MR translations by us-
ing the MRL grammar.
The basic idea is to train a statistical word align-
ment model on the training set, and then form a
lexicon by extracting transformation rules from the
K = 10 most probable word alignments between
the training sentences and their MRs. While NL
words could be directly aligned with MR tokens,
this is a bad approach for two reasons. First, not all
MR tokens carry specific meanings. For example, in
CLANG, parentheses and braces are delimiters that
are semantically vacuous. Such tokens are not sup-
posed to be aligned with any words, and inclusion of
these tokens in the training data is likely to confuse
the word alignment model. Second, MR tokens may
exhibit polysemy. For instance, the CLANG pred-
icate pt has three meanings based on the types of
arguments it is given: it specifies the xy-coordinates
(e.g. (pt 0 0)), the current position of the ball (i.e.
(pt ball)), or the current position of a player (e.g.
(pt our 4)). Judging from the pt token alone, the
word alignment model would not be able to identify
its exact meaning.
A simple, principled way to avoid these diffi-
culties is to represent an MR using a sequence of
productions used to generate it. Specifically, the
sequence corresponds to the top-down, left-most
derivation of an MR. Figure 4 shows a partial word
alignment between the sample sentence and the lin-
earized parse of its MR. Here the second produc-
tion, CONDITION —* (bowner TEAM {UNUM}), is
the one that rewrites the CONDITION non-terminal
in the first production, RULE —* (CONDITION DI-
RECTIVE), and so on. Note that the structure of a
parse tree is preserved through linearization, and for
each MR there is a unique linearized parse, since the
MRL grammar is unambiguous. Such alignments
can be obtained through the use of any off-the-shelf
word alignment model. In this work, we use the
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 5 (Brown et al., 1993).
Assuming that each NL word is linked to at most
one MRL production, transformation rules are ex-
tracted in a bottom-up manner. The process starts
with productions whose RHS is all terminals, e.g.
TEAM —* our and UNUM —* 4. For each of these
productions, X —* Q, a rule X —* (α, Q) is ex-
tracted such that α consists of the words to which
the production is linked, e.g. TEAM —* (our, our),
UNUM —* (4, 4). Then we consider productions
whose RHS contains non-terminals, i.e. predicates
with arguments. In this case, an extracted pattern
consists of the words to which the production is
linked, as well as non-terminals showing where the
arguments are realized. For example, for the bowner
predicate, the extracted rule would be CONDITION
—* (TEAM 1 player UNUM 2 has (1) ball, (bowner
TEAM 1 {UNUM 2 })), where (1) denotes a word
gap of size 1, due to the unaligned word the that
comes between has and ball. A word gap, (g), can
be seen as a non-terminal that expands to at most
g words in the NL stream, which allows for some
flexibility in pattern matching. Rule extraction thus
proceeds backward from the end of a linearized MR
</bodyText>
<page confidence="0.986588">
442
</page>
<table confidence="0.987612">
our REGION → (left REGION)
left REGION → (penalty-area TEAM)
penalty TEAM → our
area
</table>
<figureCaption confidence="0.972886">
Figure 5: A word alignment from which no rules can be extracted for the penalty-area predicate
</figureCaption>
<bodyText confidence="0.999543673469388">
parse (so that a predicate is processed only after its
arguments have all been processed), until rules are
extracted for all productions.
There are two cases where the above algorithm
would not extract any rules for a production r. First
is when no descendants of r in the MR parse are
linked to any words. Second is when there is a
link from a word w, covered by the pattern for r,
to a production r′ outside the sub-parse rooted at
r. Rule extraction is forbidden in this case be-
cause it would destroy the link between w and r′.
The first case arises when a component of an MR
is not realized, e.g. assumed in context. The sec-
ond case arises when a predicate and its arguments
are not realized close enough. Figure 5 shows an
example of this, where no rules can be extracted
for the penalty-area predicate. Both cases can be
solved by merging nodes in the MR parse tree, com-
bining several productions into one. For example,
since no rules can be extracted for penalty-area,
it is combined with its parent to form REGION →
(left (penalty-area TEAM)), for which the pat-
tern TEAM left penalty area is extracted.
The above algorithm is effective only when words
linked to an MR predicate and its arguments stay
close to each other, a property that we call phrasal
coherence. Any links that destroy this property
would lead to excessive node merging, a major cause
of overfitting. Since building a model that strictly
observes phrasal coherence often requires rules that
model the reordering of tree nodes, our goal is to
bootstrap the learning process by using a simpler,
word-based alignment model that produces a gen-
erally coherent alignment, and then remove links
that would cause excessive node merging before rule
extraction takes place. Given an alignment, a, we
count the number of links that would prevent a rule
from being extracted for each production in the MR
parse. Then the total sum for all productions is ob-
tained, denoted by v(a). A greedy procedure is em-
ployed that repeatedly removes a link a E a that
would maximize v(a) — v(a\{a}) &gt; 0, until v(a)
cannot be further reduced. A link w H r is never
removed if the translation probability, Pr(r|w), is
greater than a certain threshold (0.9). To replenish
the removed links, links from the most probable re-
verse alignment, a� (obtained by treating the source
language as target, and vice versa), are added to a, as
long as a remains n-to-1, and v(a) is not increased.
</bodyText>
<sectionHeader confidence="0.989434" genericHeader="method">
5 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9997924">
Once a lexicon is acquired, the next task is to learn a
probabilistic model for the semantic parser. We pro-
pose a maximum-entropy model that defines a con-
ditional probability distribution over derivations (d)
given the observed NL string (e):
</bodyText>
<equation confidence="0.9991925">
1 1:
Prλ(d|e) = Zλ(e) exp Aifi(d) (2)
</equation>
<bodyText confidence="0.999884545454546">
where fi is a feature function, and Zλ(e) is a nor-
malizing factor. For each rule r in the lexicon there
is a feature function that returns the number of times
r is used in a derivation. Also for each word w there
is a feature function that returns the number of times
w is generated from word gaps. Generation of un-
seen words is modeled using an extra feature whose
value is the total number of words generated from
word gaps. The number of features is quite modest
(less than 3,000 in our experiments). A similar fea-
ture set is used by Zettlemoyer and Collins (2005).
Decoding of the model can be done in cubic time
with respect to sentence length using the Viterbi al-
gorithm. An Earley chart is used for keeping track
of all derivations that are consistent with the in-
put (Stolcke, 1995). The maximum conditional like-
lihood criterion is used for estimating the model pa-
rameters, Ai. A Gaussian prior (a2 = 1) is used for
regularizing the model (Chen and Rosenfeld, 1999).
Since gold-standard derivations are not available in
the training data, correct derivations must be treated
as hidden variables. Here we use a version of im-
</bodyText>
<equation confidence="0.67208">
i
</equation>
<page confidence="0.993605">
443
</page>
<bodyText confidence="0.999802894736842">
proved iterative scaling (IIS) coupled with EM (Rie-
zler et al., 2000) for finding an optimal set of param-
eters.1 Unlike the fully-supervised case, the condi-
tional likelihood is not concave with respect to A,
so the estimation algorithm is sensitive to initial pa-
rameters. To assume as little as possible, A is initial-
ized to 0. The estimation algorithm requires statis-
tics that depend on all possible derivations for a sen-
tence or a sentence-MR pair. While it is not fea-
sible to enumerate all derivations, a variant of the
Inside-Outside algorithm can be used for efficiently
collecting the required statistics (Miyao and Tsujii,
2002). Following Zettlemoyer and Collins (2005),
only rules that are used in the best parses for the
training set are retained in the final lexicon. All
other rules are discarded. This heuristic, commonly
known as Iiterbi approximation, is used to improve
accuracy, assuming that rules used in the best parses
are the most accurate.
</bodyText>
<sectionHeader confidence="0.999353" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999880375">
We evaluated WASP in the ROBOCUP and GEO-
QUERY domains (see Section 2). To build a cor-
pus for ROBOCUP, 300 pieces of coach advice were
randomly selected from the log files of the 2003
ROBOCUP Coach Competition, which were manu-
ally translated into English (Kuhlmann et al., 2004).
The average sentence length is 22.52. To build a
corpus for GEOQUERY, 880 English questions were
gathered from various sources, which were manu-
ally translated into the functional GEOQUERY lan-
guage (Tang and Mooney, 2001). The average sen-
tence length is 7.48, much shorter than ROBOCUP.
250 of the queries were also translated into Spanish,
Japanese and Turkish, resulting in a smaller, multi-
lingual data set.
For each domain, there was a minimal set of ini-
tial rules representing knowledge needed for trans-
lating basic domain entities. These rules were al-
ways included in a lexicon. For example, in GEO-
QUERY, the initial rules were: NUM —* (x, x), for
all x ER; CITY —* (c, cityid(’c’, )), for all
city names c (e.g. new york); and similar rules for
other types of names (e.g. rivers). Name transla-
tions were provided for the multilingual data set (e.g.
</bodyText>
<footnote confidence="0.923781">
1We also implemented limited-memory BFGS (Nocedal,
1980). Preliminary experiments showed that it typically reduces
training time by more than half with similar accuracy.
</footnote>
<bodyText confidence="0.986906541666667">
CITY —* (nyuu yooku, cityid(’new york’, )) for
Japanese).
Standard 10-fold cross validation was used in our
experiments. A semantic parser was learned from
the training set. Then the learned parser was used
to translate the test sentences into MRs. Translation
failed when there were constructs that the parser did
not cover. We counted the number of sentences that
were translated into an MR, and the number of trans-
lations that were correct. For ROBOCUP, a trans-
lation was correct if it exactly matched the correct
MR. For GEOQUERY, a translation was correct if it
retrieved the same answer as the correct query. Us-
ing these counts, we measured the performance of
the parser in terms of precision (percentage of trans-
lations that were correct) and recall (percentage of
test sentences that were correctly translated). For
ROBOCUP, it took 47 minutes to learn a parser us-
ing IIS. For GEOQUERY, it took 83 minutes.
Figure 6 shows the performance of WASP com-
pared to four other algorithms: SILT (Kate et al.,
2005), COCKTAIL (Tang and Mooney, 2001), SCIS-
SOR (Ge and Mooney, 2005) and Zettlemoyer and
Collins (2005). Experimental results clearly show
the advantage of extra supervision in SCISSOR and
Zettlemoyer and Collins’s parser (see Section 1).
However, WASP performs quite favorably compared
to SILT and COCKTAIL, which use the same train-
ing data. In particular, COCKTAIL, a determinis-
tic shift-reduce parser based on inductive logic pro-
gramming, fails to scale up to the ROBOCUP do-
main where sentences are much longer, and crashes
on larger training sets due to memory overflow.
WASP also outperforms SILT in terms of recall,
where lexical learning is done by a local bottom-up
search, which is much less effective than the word-
alignment-based algorithm in WASP.
Figure 7 shows the performance of WASP on
the multilingual GEOQUERY data set. The lan-
guages being considered differ in terms of word or-
der: Subject-Verb-Object for English and Spanish,
and Subject-Object-Verb for Japanese and Turkish.
WASP’s performance is consistent across these lan-
guages despite some slight differences, most proba-
bly due to factors other than word order (e.g. lower
recall for Turkish due to a much larger vocabulary).
Details can be found in a longer version of this pa-
per (Wong, 2005).
</bodyText>
<page confidence="0.997889">
444
</page>
<figure confidence="0.999808254545454">
0 50 100 150 200 250 300
Number of training examples
0 50 100 150 200 250 300
Number of training examples
WASP
SILT
COCKTAIL
SCISSOR
WASP
SILT
COCKTAIL
SCISSOR
Precision (%) 100
80
60
40
20
0
Recall (%) 100
80
60
40
20
0
(a) Precision for ROBOCUP (b) Recall for ROBOCUP
Recall (%)
100
80
60
40
20
0
WASP
SILT
COCKTAIL
SCISSOR
Zettlemoyer et al. (2005)
Precision (%)
100
80
60
40
20
0
WASP
SILT
COCKTAIL
SCISSOR
Zettlemoyer et al. (2005)
0 100 200 300 400 500 600 700 800
Number of training examples
(c) Precision for GEOQUERY
0 100 200 300 400 500 600 700 800
Number of training examples
(d) Recall for GEOQUERY
</figure>
<figureCaption confidence="0.9999985">
Figure 6: Precision and recall learning curves comparing various semantic parsers
Figure 7: Precision and recall learning curves comparing various natural languages
</figureCaption>
<figure confidence="0.998782285714286">
0 50 100 150 200 250
Number of training examples
(a) Precision for GEOQUERY
0 50 100 150 200 250
Number of training examples
(b) Recall for GEOQUERY
Recall (%)
100
80
60
40
20
0
English
Spanish
Japanese
Turkish
Precision (%)
100
80
60
40
20
0
English
Spanish
Japanese
Turkish
</figure>
<sectionHeader confidence="0.984371" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999978">
We have presented a novel statistical approach to
semantic parsing in which a word-based alignment
model is used for lexical learning, and the parsing
model itself can be seen as a syntax-based trans-
lation model. Our method is like many phrase-
based translation models, which require a simpler,
word-based alignment model for the acquisition of a
phrasal lexicon (Och and Ney, 2003). It is also sim-
ilar to the hierarchical phrase-based model of Chi-
ang (2005), in which hierarchical phrase pairs, es-
sentially SCFG rules, are learned through the use of
a simpler, phrase-based alignment model. Our work
shows that ideas from compiler theory (SCFG) and
machine translation (word alignment models) can be
successfully applied to semantic parsing, a closely-
related task whose goal is to translate a natural lan-
guage into a formal language.
Lexical learning requires word alignments that are
phrasally coherent. We presented a simple greedy
algorithm for removing links that destroy phrasal co-
herence. Although it is shown to be quite effective in
the current domains, it is preferable to have a more
principled way of promoting phrasal coherence. The
problem is that, by treating MRL productions as
atomic units, current word-based alignment models
have no knowledge about the tree structure hidden
in a linearized MR parse. In the future, we would
like to develop a word-based alignment model that
</bodyText>
<page confidence="0.997475">
445
</page>
<bodyText confidence="0.980689">
is aware of the MRL syntax, so that better lexicons
can be learned.
I. D. Melamed. 2004. Statistical machine translation
by parsing. In Proc. of ACL-04, pages 653–660,
Barcelona, Spain.
</bodyText>
<sectionHeader confidence="0.997197" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.923345">
This research was supported by Defense Advanced
Research Projects Agency under grant HR0011-04-
1-0007.
</bodyText>
<sectionHeader confidence="0.993162" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902345679013">
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice Hall, Engle-
wood Cliffs, NJ.
J. F. Allen. 1995. Natural Language Understanding (2nd
Ed.). Benjamin/Cummings, Menlo Park, CA.
I. Androutsopoulos, G. D. Ritchie, and P. Thanisch.
1995. Natural language interfaces to databases: An
introduction. Journal of Natural Language Engineer-
ing, 1(1):29–81.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–312, June.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models. Technical re-
port, Carnegie Mellon University, Pittsburgh, PA.
M. Chen et al. 2003. Users manual: RoboCup soc-
cer server manual for soccer server version 7.07 and
later. Available at http://sourceforge.net/
projects/sserver/.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ofACL-05,
pages 263–270, Ann Arbor, MI, June.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of CoNLL-05, pages 9–16, Ann Arbor, MI, July.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learn-
ing to transform natural to formal languages. In Proc.
of AAAI-05, pages 1062–1068, Pittsburgh, PA, July.
G. Kuhlmann, P. Stone, R. J. Mooney, and J. W. Shavlik.
2004. Guiding a reinforcement learner with natural
language advice: Initial results in RoboCup soccer. In
Proc. of the AAAI-04 Workshop on Supervisory Con-
trol of Learning and Adaptive Systems, San Jose, CA,
July.
K. Macherey, F. J. Och, and H. Ney. 2001. Natural lan-
guage understanding using statistical machine transla-
tion. In Proc. of EuroSpeech-01, pages 2205–2208,
Aalborg, Denmark.
S. Miller, D. Stallard, R. Bobrow, and R. Schwartz. 1996.
A fully statistical approach to natural language inter-
faces. In Proc. of ACL-96, pages 55–61, Santa Cruz,
CA.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estima-
tion for feature forests. In Proc. of HLT-02, San Diego,
CA, March.
J. Nocedal. 1980. Updating quasi-Newton matrices
with limited storage. Mathematics of Computation,
35(151):773–782, July.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In Proc. of
EuroSpeech-97, pages 1435–1438, Rhodes, Greece.
S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000.
Lexicalized stochastic modeling of constraint-based
grammars using log-linear measures and EM training.
In Proc. of ACL-00, pages 480–487, Hong Kong.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165–201.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In Proc. of ECML-01, pages 466–
477, Freiburg, Germany.
Y. W. Wong. 2005. Learning for semantic parsing us-
ing statistical machine translation techniques. Techni-
cal Report UT-AI-05-323, Artificial Intelligence Lab,
University of Texas at Austin, Austin, TX, October.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of ACL-01, pages
523–530, Toulouse, France.
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic programming.
In Proc. of AAAI-96, pages 1050–1055, Portland, OR,
August.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Proc.
of UAI-05, Edinburgh, Scotland, July.
</reference>
<page confidence="0.999113">
446
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801348">
<title confidence="0.999983">Learning for Semantic Parsing with Statistical Machine Translation</title>
<author confidence="0.99985">Yuk Wah Wong</author>
<author confidence="0.99985">J Raymond</author>
<affiliation confidence="0.999232">Department of Computer The University of Texas at 1 University Station</affiliation>
<address confidence="0.838508">Austin, TX 78712-0233,</address>
<abstract confidence="0.997618368421052">We present a novel statistical approach to parsing, for constructing a complete, formal meaning representation of a sentence. A semantic parser is learned given a set of sentences annotated with their correct meaning represen- The main innovation of is its use of state-of-the-art statistical machine translation techniques. A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="4080" citStr="Aho and Ullman, 1972" startWordPosition="646" endWordPosition="649"> the US? Figure 2: A meaning representation in GEOQUERY shows a sample query in this language. Note that both domains involve the use of MRs with a complex, nested structure. gorithm is its integration with state-of-the-art statistical machine translation techniques. More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL. Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (Aho and Ullman, 1972), which forms the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2005). Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing. In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order. Section 2 provides a brief overview of the domains being considered. In Section 3, we present the semantic </context>
<context position="6386" citStr="Aho and Ullman, 1972" startWordPosition="1033" endWordPosition="1036">R in CLANG. To achieve this task, we may first analyze the syntactic structure of the sentence using a semantic grammar (Allen, 1995), whose nonterminals are the ones in the CLANG grammar. The meaning of the sentence is then obtained by combining the meanings of its sub-parts according to the semantic parse. Figure 3(a) shows a possible partial semantic parse of the sample sentence based on CLANG non-terminals (UNUM stands for uniform number). Figure 3(b) shows the corresponding CLANG parse from which the MR is constructed. This process can be formalized as an instance of synchronous parsing (Aho and Ullman, 1972), originally developed as a theory of compilers in which syntax analysis and code generation are combined into a single phase. Synchronous parsing has seen a surge of interest recently in the machine translation community as a way of formalizing syntax-based translation models (Melamed, 2004; Chiang, 2005). According to this theory, a semantic parser defines a translation, a set of pairs of strings in which each pair is an NL sentence coupled with its MR. To finitely specify a potentially infinite translation, we use a synchronous context-free grammar (SCFG) for generating the pairs in a trans</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Allen</author>
</authors>
<date>1995</date>
<booktitle>Natural Language Understanding (2nd Ed.). Benjamin/Cummings,</booktitle>
<location>Menlo Park, CA.</location>
<contexts>
<context position="5898" citStr="Allen, 1995" startWordPosition="954" endWordPosition="955">advice written in a formal language called CLANG (Chen et al., 2003). Figure 1 shows a sample MR in CLANG. The second domain is GEOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. geography (Zelle and Mooney, 1996; Kate et al., 2005). Figure 2 3 The Semantic Parsing Model To describe the semantic parsing model of WASP, it is best to start with an example. Consider the task of translating the sentence in Figure 1 into its MR in CLANG. To achieve this task, we may first analyze the syntactic structure of the sentence using a semantic grammar (Allen, 1995), whose nonterminals are the ones in the CLANG grammar. The meaning of the sentence is then obtained by combining the meanings of its sub-parts according to the semantic parse. Figure 3(a) shows a possible partial semantic parse of the sample sentence based on CLANG non-terminals (UNUM stands for uniform number). Figure 3(b) shows the corresponding CLANG parse from which the MR is constructed. This process can be formalized as an instance of synchronous parsing (Aho and Ullman, 1972), originally developed as a theory of compilers in which syntax analysis and code generation are combined into a</context>
</contexts>
<marker>Allen, 1995</marker>
<rawString>J. F. Allen. 1995. Natural Language Understanding (2nd Ed.). Benjamin/Cummings, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>G D Ritchie</author>
<author>P Thanisch</author>
</authors>
<title>Natural language interfaces to databases: An introduction.</title>
<date>1995</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1462" citStr="Androutsopoulos et al., 1995" startWordPosition="219" endWordPosition="222">o existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order. 1 Introduction Recent work on natural language understanding has mainly focused on shallow semantic analysis, such as semantic role labeling and word-sense disambiguation. This paper considers a more ambitious task of semantic parsing, which is the construction of a complete, formal, symbolic, meaning representation (MR) of a sentence. Semantic parsing has found its way in practical applications such as natural-language (NL) interfaces to databases (Androutsopoulos et al., 1995) and advice taking (Kuhlmann et al., 2004). Figure 1 shows a sample MR written in a meaning-representation language (MRL) called CLANG, which is used for ((bowner our {4}) (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half. Figure 1: A meaning representation in CLANG encoding coach advice given to simulated soccerplaying agents (Kuhlmann et al., 2004). Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1</context>
</contexts>
<marker>Androutsopoulos, Ritchie, Thanisch, 1995</marker>
<rawString>I. Androutsopoulos, G. D. Ritchie, and P. Thanisch. 1995. Natural language interfaces to databases: An introduction. Journal of Natural Language Engineering, 1(1):29–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3801" citStr="Brown et al., 1993" startWordPosition="601" endWordPosition="604">novation of this al439 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 439–446, New York, June 2006. c�2006 Association for Computational Linguistics answer(count(city(loc 2(countryid(usa))))) How many cities are there in the US? Figure 2: A meaning representation in GEOQUERY shows a sample query in this language. Note that both domains involve the use of MRs with a complex, nested structure. gorithm is its integration with state-of-the-art statistical machine translation techniques. More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL. Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (Aho and Ullman, 1972), which forms the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2005). Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing. In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accurac</context>
<context position="13713" citStr="Brown et al., 1993" startWordPosition="2352" endWordPosition="2355">ord alignment between the sample sentence and the linearized parse of its MR. Here the second production, CONDITION —* (bowner TEAM {UNUM}), is the one that rewrites the CONDITION non-terminal in the first production, RULE —* (CONDITION DIRECTIVE), and so on. Note that the structure of a parse tree is preserved through linearization, and for each MR there is a unique linearized parse, since the MRL grammar is unambiguous. Such alignments can be obtained through the use of any off-the-shelf word alignment model. In this work, we use the GIZA++ implementation (Och and Ney, 2003) of IBM Model 5 (Brown et al., 1993). Assuming that each NL word is linked to at most one MRL production, transformation rules are extracted in a bottom-up manner. The process starts with productions whose RHS is all terminals, e.g. TEAM —* our and UNUM —* 4. For each of these productions, X —* Q, a rule X —* (α, Q) is extracted such that α consists of the words to which the production is linked, e.g. TEAM —* (our, our), UNUM —* (4, 4). Then we consider productions whose RHS contains non-terminals, i.e. predicates with arguments. In this case, an extracted pattern consists of the words to which the production is linked, as well </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–312, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="18734" citStr="Chen and Rosenfeld, 1999" startWordPosition="3249" endWordPosition="3252">xtra feature whose value is the total number of words generated from word gaps. The number of features is quite modest (less than 3,000 in our experiments). A similar feature set is used by Zettlemoyer and Collins (2005). Decoding of the model can be done in cubic time with respect to sentence length using the Viterbi algorithm. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). The maximum conditional likelihood criterion is used for estimating the model parameters, Ai. A Gaussian prior (a2 = 1) is used for regularizing the model (Chen and Rosenfeld, 1999). Since gold-standard derivations are not available in the training data, correct derivations must be treated as hidden variables. Here we use a version of imi 443 proved iterative scaling (IIS) coupled with EM (Riezler et al., 2000) for finding an optimal set of parameters.1 Unlike the fully-supervised case, the conditional likelihood is not concave with respect to A, so the estimation algorithm is sensitive to initial parameters. To assume as little as possible, A is initialized to 0. The estimation algorithm requires statistics that depend on all possible derivations for a sentence or a sen</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. Chen and R. Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical report, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chen</author>
</authors>
<title>Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http://sourceforge.net/ projects/sserver/.</title>
<date>2003</date>
<marker>Chen, 2003</marker>
<rawString>M. Chen et al. 2003. Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http://sourceforge.net/ projects/sserver/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL-05,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="4204" citStr="Chiang, 2005" startWordPosition="666" endWordPosition="667">e of MRs with a complex, nested structure. gorithm is its integration with state-of-the-art statistical machine translation techniques. More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL. Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (Aho and Ullman, 1972), which forms the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2005). Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing. In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order. Section 2 provides a brief overview of the domains being considered. In Section 3, we present the semantic parsing model of WASP. Section 4 outlines the algorithm for acquiring a bilingual lexicon through the use of word alignments</context>
<context position="6693" citStr="Chiang, 2005" startWordPosition="1082" endWordPosition="1083">e 3(a) shows a possible partial semantic parse of the sample sentence based on CLANG non-terminals (UNUM stands for uniform number). Figure 3(b) shows the corresponding CLANG parse from which the MR is constructed. This process can be formalized as an instance of synchronous parsing (Aho and Ullman, 1972), originally developed as a theory of compilers in which syntax analysis and code generation are combined into a single phase. Synchronous parsing has seen a surge of interest recently in the machine translation community as a way of formalizing syntax-based translation models (Melamed, 2004; Chiang, 2005). According to this theory, a semantic parser defines a translation, a set of pairs of strings in which each pair is an NL sentence coupled with its MR. To finitely specify a potentially infinite translation, we use a synchronous context-free grammar (SCFG) for generating the pairs in a translation. Analogous to an ordinary CFG, each SCFG rule consists of a single non-terminal on the left-hand side (LHS). The right-hand side (RHS) of an SCFG rule is a pair of strings, (α, Q), where the non-terminals in 0 are a permutation of the non-terminals in α. Below are some SCFG rules that can be used fo</context>
<context position="24983" citStr="Chiang (2005)" startWordPosition="4305" endWordPosition="4307">ll for GEOQUERY Recall (%) 100 80 60 40 20 0 English Spanish Japanese Turkish Precision (%) 100 80 60 40 20 0 English Spanish Japanese Turkish 7 Conclusion We have presented a novel statistical approach to semantic parsing in which a word-based alignment model is used for lexical learning, and the parsing model itself can be seen as a syntax-based translation model. Our method is like many phrasebased translation models, which require a simpler, word-based alignment model for the acquisition of a phrasal lexicon (Och and Ney, 2003). It is also similar to the hierarchical phrase-based model of Chiang (2005), in which hierarchical phrase pairs, essentially SCFG rules, are learned through the use of a simpler, phrase-based alignment model. Our work shows that ideas from compiler theory (SCFG) and machine translation (word alignment models) can be successfully applied to semantic parsing, a closelyrelated task whose goal is to translate a natural language into a formal language. Lexical learning requires word alignments that are phrasally coherent. We presented a simple greedy algorithm for removing links that destroy phrasal coherence. Although it is shown to be quite effective in the current doma</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ofACL-05, pages 263–270, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL-05,</booktitle>
<pages>9--16</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="2543" citStr="Ge and Mooney, 2005" startWordPosition="400" endWordPosition="403">sing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require extensive human efforts when porting to a new domain or language. In this paper, we present a novel statistical approach to semantic parsing which can handle MRs with a nested structure, based on previous work on semantic parsing using transformation rules (Kate et al., 2005). The algorithm learns a semantic parser given a set of NL sentences annotated with their correct MRs. It requires no prior knowledge of the NL syntax, although it assumes that an unambiguous, context-free grammar (CFG) of t</context>
<context position="22256" citStr="Ge and Mooney, 2005" startWordPosition="3839" endWordPosition="3842"> translation was correct if it exactly matched the correct MR. For GEOQUERY, a translation was correct if it retrieved the same answer as the correct query. Using these counts, we measured the performance of the parser in terms of precision (percentage of translations that were correct) and recall (percentage of test sentences that were correctly translated). For ROBOCUP, it took 47 minutes to learn a parser using IIS. For GEOQUERY, it took 83 minutes. Figure 6 shows the performance of WASP compared to four other algorithms: SILT (Kate et al., 2005), COCKTAIL (Tang and Mooney, 2001), SCISSOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005). Experimental results clearly show the advantage of extra supervision in SCISSOR and Zettlemoyer and Collins’s parser (see Section 1). However, WASP performs quite favorably compared to SILT and COCKTAIL, which use the same training data. In particular, COCKTAIL, a deterministic shift-reduce parser based on inductive logic programming, fails to scale up to the ROBOCUP domain where sentences are much longer, and crashes on larger training sets due to memory overflow. WASP also outperforms SILT in terms of recall, where lexical learning is done by a local bott</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proc. of CoNLL-05, pages 9–16, Ann Arbor, MI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proc. of AAAI-05,</booktitle>
<pages>1062--1068</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="2360" citStr="Kate et al., 2005" startWordPosition="373" endWordPosition="376">t side of our half. Figure 1: A meaning representation in CLANG encoding coach advice given to simulated soccerplaying agents (Kuhlmann et al., 2004). Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require extensive human efforts when porting to a new domain or language. In this paper, we present a novel statistical approach to semantic parsing which can handle MRs with a nested structure, based on previous work on semantic parsing using transformation rules (Kate et al., 2005). The algorithm learns a semantic parser </context>
<context position="5572" citStr="Kate et al., 2005" startWordPosition="893" endWordPosition="896">lowed by the conclusion in Section 7. 2 Application Domains In this paper, we consider two domains. The first domain is ROBOCUP. ROBOCUP (www.robocup.org) is an AI research initiative using robotic soccer as its primary domain. In the ROBOCUP Coach Competition, teams of agents compete on a simulated soccer field and receive coach advice written in a formal language called CLANG (Chen et al., 2003). Figure 1 shows a sample MR in CLANG. The second domain is GEOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. geography (Zelle and Mooney, 1996; Kate et al., 2005). Figure 2 3 The Semantic Parsing Model To describe the semantic parsing model of WASP, it is best to start with an example. Consider the task of translating the sentence in Figure 1 into its MR in CLANG. To achieve this task, we may first analyze the syntactic structure of the sentence using a semantic grammar (Allen, 1995), whose nonterminals are the ones in the CLANG grammar. The meaning of the sentence is then obtained by combining the meanings of its sub-parts according to the semantic parse. Figure 3(a) shows a possible partial semantic parse of the sample sentence based on CLANG non-ter</context>
<context position="7941" citStr="Kate et al. (2005)" startWordPosition="1319" endWordPosition="1322">n Figure 3: RULE (if CONDITION 1 , DIRECTIVE 2 . , (CONDITION 1 DIRECTIVE 2 )) CONDITION (TEAM 1 player UNUM 2 has the ball , (bowner TEAM 1 {UNUM 2 })) TEAM (our , our) UNUM (4 , 4) 440 RULE RULE ... ...) If CONDITION ( CONDITION }) player UNUM 4 has the ball (bowner TEAM our { UNUM 4 TEAM our (a) English (b) CLANG Figure 3: Partial parse trees for the CLANG statement and its English gloss shown in Figure 1 Each SCFG rule X → (α, Q) is a combination of a production of the NL semantic grammar, X → α, and a production of the MRL grammar, X → Q. Each rule corresponds to a transformation rule in Kate et al. (2005). Following their terminology, we call the string α a pattern, and the string Q a template. Non-terminals are indexed to show their association between a pattern and a template. All derivations start with a pair of associated start symbols, (51 , 51 ). Each step of a derivation involves the rewriting of a pair of associated non-terminals in both of the NL and MRL streams. Below is a derivation that would generate the sample sentence and its MR simultaneously: (Note that RULE is the start symbol for CLANG) (RULE 1 , RULE 1 ) ⇒ (if CONDITION 1 , DIRECTIVE 2 . , (CONDITION 1 DIRECTIVE 2 )) ⇒ (if </context>
<context position="22191" citStr="Kate et al., 2005" startWordPosition="3828" endWordPosition="3831">nd the number of translations that were correct. For ROBOCUP, a translation was correct if it exactly matched the correct MR. For GEOQUERY, a translation was correct if it retrieved the same answer as the correct query. Using these counts, we measured the performance of the parser in terms of precision (percentage of translations that were correct) and recall (percentage of test sentences that were correctly translated). For ROBOCUP, it took 47 minutes to learn a parser using IIS. For GEOQUERY, it took 83 minutes. Figure 6 shows the performance of WASP compared to four other algorithms: SILT (Kate et al., 2005), COCKTAIL (Tang and Mooney, 2001), SCISSOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005). Experimental results clearly show the advantage of extra supervision in SCISSOR and Zettlemoyer and Collins’s parser (see Section 1). However, WASP performs quite favorably compared to SILT and COCKTAIL, which use the same training data. In particular, COCKTAIL, a deterministic shift-reduce parser based on inductive logic programming, fails to scale up to the ROBOCUP domain where sentences are much longer, and crashes on larger training sets due to memory overflow. WASP also outperforms SILT i</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Proc. of AAAI-05, pages 1062–1068, Pittsburgh, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kuhlmann</author>
<author>P Stone</author>
<author>R J Mooney</author>
<author>J W Shavlik</author>
</authors>
<title>Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer.</title>
<date>2004</date>
<booktitle>In Proc. of the AAAI-04 Workshop on Supervisory Control of Learning and Adaptive Systems,</booktitle>
<location>San Jose, CA,</location>
<contexts>
<context position="1504" citStr="Kuhlmann et al., 2004" startWordPosition="227" endWordPosition="230">unt of supervision, and shows better robustness to variations in task complexity and word order. 1 Introduction Recent work on natural language understanding has mainly focused on shallow semantic analysis, such as semantic role labeling and word-sense disambiguation. This paper considers a more ambitious task of semantic parsing, which is the construction of a complete, formal, symbolic, meaning representation (MR) of a sentence. Semantic parsing has found its way in practical applications such as natural-language (NL) interfaces to databases (Androutsopoulos et al., 1995) and advice taking (Kuhlmann et al., 2004). Figure 1 shows a sample MR written in a meaning-representation language (MRL) called CLANG, which is used for ((bowner our {4}) (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half. Figure 1: A meaning representation in CLANG encoding coach advice given to simulated soccerplaying agents (Kuhlmann et al., 2004). Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a ty</context>
<context position="20152" citStr="Kuhlmann et al., 2004" startWordPosition="3486" endWordPosition="3489">02). Following Zettlemoyer and Collins (2005), only rules that are used in the best parses for the training set are retained in the final lexicon. All other rules are discarded. This heuristic, commonly known as Iiterbi approximation, is used to improve accuracy, assuming that rules used in the best parses are the most accurate. 6 Experiments We evaluated WASP in the ROBOCUP and GEOQUERY domains (see Section 2). To build a corpus for ROBOCUP, 300 pieces of coach advice were randomly selected from the log files of the 2003 ROBOCUP Coach Competition, which were manually translated into English (Kuhlmann et al., 2004). The average sentence length is 22.52. To build a corpus for GEOQUERY, 880 English questions were gathered from various sources, which were manually translated into the functional GEOQUERY language (Tang and Mooney, 2001). The average sentence length is 7.48, much shorter than ROBOCUP. 250 of the queries were also translated into Spanish, Japanese and Turkish, resulting in a smaller, multilingual data set. For each domain, there was a minimal set of initial rules representing knowledge needed for translating basic domain entities. These rules were always included in a lexicon. For example, in</context>
</contexts>
<marker>Kuhlmann, Stone, Mooney, Shavlik, 2004</marker>
<rawString>G. Kuhlmann, P. Stone, R. J. Mooney, and J. W. Shavlik. 2004. Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer. In Proc. of the AAAI-04 Workshop on Supervisory Control of Learning and Adaptive Systems, San Jose, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Macherey</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Natural language understanding using statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of EuroSpeech-01,</booktitle>
<pages>2205--2208</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="2089" citStr="Macherey et al., 2001" startWordPosition="327" endWordPosition="330">advice taking (Kuhlmann et al., 2004). Figure 1 shows a sample MR written in a meaning-representation language (MRL) called CLANG, which is used for ((bowner our {4}) (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half. Figure 1: A meaning representation in CLANG encoding coach advice given to simulated soccerplaying agents (Kuhlmann et al., 2004). Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require extensive human efforts when porting to a new </context>
<context position="11533" citStr="Macherey et al., 2001" startWordPosition="1982" endWordPosition="1985">ce there may 441 If RULE —* (CONDITION DIRECTIVE) our CONDITION —* (bowner TEAM {UNUM}) player TEAM —* our 4 UNUM —* 4 has the ball Figure 4: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1 NL sentences and their MRs in the training set. By defining a mapping of words from one language to another, word alignments define a bilingual lexicon. Using word alignments to induce a lexicon is not a new idea (Och and Ney, 2003). Indeed, attempts have been made to directly apply machine translation systems to the problem of semantic parsing (Papineni et al., 1997; Macherey et al., 2001). However, these systems make no use of the MRL grammar, thus allocating probability mass to MR translations that are not even syntactically well-formed. Here we present a lexical induction algorithm that guarantees syntactic well-formedness of MR translations by using the MRL grammar. The basic idea is to train a statistical word alignment model on the training set, and then form a lexicon by extracting transformation rules from the K = 10 most probable word alignments between the training sentences and their MRs. While NL words could be directly aligned with MR tokens, this is a bad approach</context>
</contexts>
<marker>Macherey, Och, Ney, 2001</marker>
<rawString>K. Macherey, F. J. Och, and H. Ney. 2001. Natural language understanding using statistical machine translation. In Proc. of EuroSpeech-01, pages 2205–2208, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>D Stallard</author>
<author>R Bobrow</author>
<author>R Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proc. of ACL-96,</booktitle>
<pages>55--61</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="2042" citStr="Miller et al., 1996" startWordPosition="319" endWordPosition="322">atabases (Androutsopoulos et al., 1995) and advice taking (Kuhlmann et al., 2004). Figure 1 shows a sample MR written in a meaning-representation language (MRL) called CLANG, which is used for ((bowner our {4}) (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half. Figure 1: A meaning representation in CLANG encoding coach advice given to simulated soccerplaying agents (Kuhlmann et al., 2004). Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>S. Miller, D. Stallard, R. Bobrow, and R. Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proc. of ACL-96, pages 55–61, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proc. of HLT-02,</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="19533" citStr="Miyao and Tsujii, 2002" startWordPosition="3382" endWordPosition="3385">tive scaling (IIS) coupled with EM (Riezler et al., 2000) for finding an optimal set of parameters.1 Unlike the fully-supervised case, the conditional likelihood is not concave with respect to A, so the estimation algorithm is sensitive to initial parameters. To assume as little as possible, A is initialized to 0. The estimation algorithm requires statistics that depend on all possible derivations for a sentence or a sentence-MR pair. While it is not feasible to enumerate all derivations, a variant of the Inside-Outside algorithm can be used for efficiently collecting the required statistics (Miyao and Tsujii, 2002). Following Zettlemoyer and Collins (2005), only rules that are used in the best parses for the training set are retained in the final lexicon. All other rules are discarded. This heuristic, commonly known as Iiterbi approximation, is used to improve accuracy, assuming that rules used in the best parses are the most accurate. 6 Experiments We evaluated WASP in the ROBOCUP and GEOQUERY domains (see Section 2). To build a corpus for ROBOCUP, 300 pieces of coach advice were randomly selected from the log files of the 2003 ROBOCUP Coach Competition, which were manually translated into English (Kuh</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Y. Miyao and J. Tsujii. 2002. Maximum entropy estimation for feature forests. In Proc. of HLT-02, San Diego, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
</authors>
<title>Updating quasi-Newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<volume>35</volume>
<issue>151</issue>
<contexts>
<context position="21064" citStr="Nocedal, 1980" startWordPosition="3642" endWordPosition="3643"> queries were also translated into Spanish, Japanese and Turkish, resulting in a smaller, multilingual data set. For each domain, there was a minimal set of initial rules representing knowledge needed for translating basic domain entities. These rules were always included in a lexicon. For example, in GEOQUERY, the initial rules were: NUM —* (x, x), for all x ER; CITY —* (c, cityid(’c’, )), for all city names c (e.g. new york); and similar rules for other types of names (e.g. rivers). Name translations were provided for the multilingual data set (e.g. 1We also implemented limited-memory BFGS (Nocedal, 1980). Preliminary experiments showed that it typically reduces training time by more than half with similar accuracy. CITY —* (nyuu yooku, cityid(’new york’, )) for Japanese). Standard 10-fold cross validation was used in our experiments. A semantic parser was learned from the training set. Then the learned parser was used to translate the test sentences into MRs. Translation failed when there were constructs that the parser did not cover. We counted the number of sentences that were translated into an MR, and the number of translations that were correct. For ROBOCUP, a translation was correct if </context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>J. Nocedal. 1980. Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35(151):773–782, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11372" citStr="Och and Ney, 2003" startWordPosition="1955" endWordPosition="1958">ical Acquisition In this section, we focus on lexical learning, which is done by finding optimal word alignments between so that f is a translation of e. Since there may 441 If RULE —* (CONDITION DIRECTIVE) our CONDITION —* (bowner TEAM {UNUM}) player TEAM —* our 4 UNUM —* 4 has the ball Figure 4: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1 NL sentences and their MRs in the training set. By defining a mapping of words from one language to another, word alignments define a bilingual lexicon. Using word alignments to induce a lexicon is not a new idea (Och and Ney, 2003). Indeed, attempts have been made to directly apply machine translation systems to the problem of semantic parsing (Papineni et al., 1997; Macherey et al., 2001). However, these systems make no use of the MRL grammar, thus allocating probability mass to MR translations that are not even syntactically well-formed. Here we present a lexical induction algorithm that guarantees syntactic well-formedness of MR translations by using the MRL grammar. The basic idea is to train a statistical word alignment model on the training set, and then form a lexicon by extracting transformation rules from the K</context>
<context position="13677" citStr="Och and Ney, 2003" startWordPosition="2344" endWordPosition="2347">f an MR. Figure 4 shows a partial word alignment between the sample sentence and the linearized parse of its MR. Here the second production, CONDITION —* (bowner TEAM {UNUM}), is the one that rewrites the CONDITION non-terminal in the first production, RULE —* (CONDITION DIRECTIVE), and so on. Note that the structure of a parse tree is preserved through linearization, and for each MR there is a unique linearized parse, since the MRL grammar is unambiguous. Such alignments can be obtained through the use of any off-the-shelf word alignment model. In this work, we use the GIZA++ implementation (Och and Ney, 2003) of IBM Model 5 (Brown et al., 1993). Assuming that each NL word is linked to at most one MRL production, transformation rules are extracted in a bottom-up manner. The process starts with productions whose RHS is all terminals, e.g. TEAM —* our and UNUM —* 4. For each of these productions, X —* Q, a rule X —* (α, Q) is extracted such that α consists of the words to which the production is linked, e.g. TEAM —* (our, our), UNUM —* (4, 4). Then we consider productions whose RHS contains non-terminals, i.e. predicates with arguments. In this case, an extracted pattern consists of the words to whic</context>
<context position="24907" citStr="Och and Ney, 2003" startWordPosition="4290" endWordPosition="4293"> Precision for GEOQUERY 0 50 100 150 200 250 Number of training examples (b) Recall for GEOQUERY Recall (%) 100 80 60 40 20 0 English Spanish Japanese Turkish Precision (%) 100 80 60 40 20 0 English Spanish Japanese Turkish 7 Conclusion We have presented a novel statistical approach to semantic parsing in which a word-based alignment model is used for lexical learning, and the parsing model itself can be seen as a syntax-based translation model. Our method is like many phrasebased translation models, which require a simpler, word-based alignment model for the acquisition of a phrasal lexicon (Och and Ney, 2003). It is also similar to the hierarchical phrase-based model of Chiang (2005), in which hierarchical phrase pairs, essentially SCFG rules, are learned through the use of a simpler, phrase-based alignment model. Our work shows that ideas from compiler theory (SCFG) and machine translation (word alignment models) can be successfully applied to semantic parsing, a closelyrelated task whose goal is to translate a natural language into a formal language. Lexical learning requires word alignments that are phrasally coherent. We presented a simple greedy algorithm for removing links that destroy phras</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In Proc. of EuroSpeech-97,</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="2065" citStr="Papineni et al., 1997" startWordPosition="323" endWordPosition="326">ulos et al., 1995) and advice taking (Kuhlmann et al., 2004). Figure 1 shows a sample MR written in a meaning-representation language (MRL) called CLANG, which is used for ((bowner our {4}) (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half. Figure 1: A meaning representation in CLANG encoding coach advice given to simulated soccerplaying agents (Kuhlmann et al., 2004). Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require extensive human effort</context>
<context position="11509" citStr="Papineni et al., 1997" startWordPosition="1977" endWordPosition="1981">a translation of e. Since there may 441 If RULE —* (CONDITION DIRECTIVE) our CONDITION —* (bowner TEAM {UNUM}) player TEAM —* our 4 UNUM —* 4 has the ball Figure 4: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1 NL sentences and their MRs in the training set. By defining a mapping of words from one language to another, word alignments define a bilingual lexicon. Using word alignments to induce a lexicon is not a new idea (Och and Ney, 2003). Indeed, attempts have been made to directly apply machine translation systems to the problem of semantic parsing (Papineni et al., 1997; Macherey et al., 2001). However, these systems make no use of the MRL grammar, thus allocating probability mass to MR translations that are not even syntactically well-formed. Here we present a lexical induction algorithm that guarantees syntactic well-formedness of MR translations by using the MRL grammar. The basic idea is to train a statistical word alignment model on the training set, and then form a lexicon by extracting transformation rules from the K = 10 most probable word alignments between the training sentences and their MRs. While NL words could be directly aligned with MR tokens</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>K. A. Papineni, S. Roukos, and R. T. Ward. 1997. Feature-based language understanding. In Proc. of EuroSpeech-97, pages 1435–1438, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>D Prescher</author>
<author>J Kuhn</author>
<author>M Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proc. of ACL-00,</booktitle>
<pages>480--487</pages>
<location>Hong Kong.</location>
<contexts>
<context position="18967" citStr="Riezler et al., 2000" startWordPosition="3287" endWordPosition="3291"> model can be done in cubic time with respect to sentence length using the Viterbi algorithm. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). The maximum conditional likelihood criterion is used for estimating the model parameters, Ai. A Gaussian prior (a2 = 1) is used for regularizing the model (Chen and Rosenfeld, 1999). Since gold-standard derivations are not available in the training data, correct derivations must be treated as hidden variables. Here we use a version of imi 443 proved iterative scaling (IIS) coupled with EM (Riezler et al., 2000) for finding an optimal set of parameters.1 Unlike the fully-supervised case, the conditional likelihood is not concave with respect to A, so the estimation algorithm is sensitive to initial parameters. To assume as little as possible, A is initialized to 0. The estimation algorithm requires statistics that depend on all possible derivations for a sentence or a sentence-MR pair. While it is not feasible to enumerate all derivations, a variant of the Inside-Outside algorithm can be used for efficiently collecting the required statistics (Miyao and Tsujii, 2002). Following Zettlemoyer and Collin</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>S. Riezler, D. Prescher, J. Kuhn, and M. Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proc. of ACL-00, pages 480–487, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="18551" citStr="Stolcke, 1995" startWordPosition="3220" endWordPosition="3221">erivation. Also for each word w there is a feature function that returns the number of times w is generated from word gaps. Generation of unseen words is modeled using an extra feature whose value is the total number of words generated from word gaps. The number of features is quite modest (less than 3,000 in our experiments). A similar feature set is used by Zettlemoyer and Collins (2005). Decoding of the model can be done in cubic time with respect to sentence length using the Viterbi algorithm. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). The maximum conditional likelihood criterion is used for estimating the model parameters, Ai. A Gaussian prior (a2 = 1) is used for regularizing the model (Chen and Rosenfeld, 1999). Since gold-standard derivations are not available in the training data, correct derivations must be treated as hidden variables. Here we use a version of imi 443 proved iterative scaling (IIS) coupled with EM (Riezler et al., 2000) for finding an optimal set of parameters.1 Unlike the fully-supervised case, the conditional likelihood is not concave with respect to A, so the estimation algorithm is sensitive to i</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proc. of ECML-01,</booktitle>
<pages>466--477</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="20374" citStr="Tang and Mooney, 2001" startWordPosition="3521" endWordPosition="3524">proximation, is used to improve accuracy, assuming that rules used in the best parses are the most accurate. 6 Experiments We evaluated WASP in the ROBOCUP and GEOQUERY domains (see Section 2). To build a corpus for ROBOCUP, 300 pieces of coach advice were randomly selected from the log files of the 2003 ROBOCUP Coach Competition, which were manually translated into English (Kuhlmann et al., 2004). The average sentence length is 22.52. To build a corpus for GEOQUERY, 880 English questions were gathered from various sources, which were manually translated into the functional GEOQUERY language (Tang and Mooney, 2001). The average sentence length is 7.48, much shorter than ROBOCUP. 250 of the queries were also translated into Spanish, Japanese and Turkish, resulting in a smaller, multilingual data set. For each domain, there was a minimal set of initial rules representing knowledge needed for translating basic domain entities. These rules were always included in a lexicon. For example, in GEOQUERY, the initial rules were: NUM —* (x, x), for all x ER; CITY —* (c, cityid(’c’, )), for all city names c (e.g. new york); and similar rules for other types of names (e.g. rivers). Name translations were provided fo</context>
<context position="22225" citStr="Tang and Mooney, 2001" startWordPosition="3833" endWordPosition="3836">that were correct. For ROBOCUP, a translation was correct if it exactly matched the correct MR. For GEOQUERY, a translation was correct if it retrieved the same answer as the correct query. Using these counts, we measured the performance of the parser in terms of precision (percentage of translations that were correct) and recall (percentage of test sentences that were correctly translated). For ROBOCUP, it took 47 minutes to learn a parser using IIS. For GEOQUERY, it took 83 minutes. Figure 6 shows the performance of WASP compared to four other algorithms: SILT (Kate et al., 2005), COCKTAIL (Tang and Mooney, 2001), SCISSOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005). Experimental results clearly show the advantage of extra supervision in SCISSOR and Zettlemoyer and Collins’s parser (see Section 1). However, WASP performs quite favorably compared to SILT and COCKTAIL, which use the same training data. In particular, COCKTAIL, a deterministic shift-reduce parser based on inductive logic programming, fails to scale up to the ROBOCUP domain where sentences are much longer, and crashes on larger training sets due to memory overflow. WASP also outperforms SILT in terms of recall, where lexical l</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proc. of ECML-01, pages 466– 477, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
</authors>
<title>Learning for semantic parsing using statistical machine translation techniques.</title>
<date>2005</date>
<tech>Technical Report UT-AI-05-323,</tech>
<institution>Artificial Intelligence Lab, University of Texas at Austin,</institution>
<location>Austin, TX,</location>
<contexts>
<context position="23454" citStr="Wong, 2005" startWordPosition="4033" endWordPosition="4034"> a local bottom-up search, which is much less effective than the wordalignment-based algorithm in WASP. Figure 7 shows the performance of WASP on the multilingual GEOQUERY data set. The languages being considered differ in terms of word order: Subject-Verb-Object for English and Spanish, and Subject-Object-Verb for Japanese and Turkish. WASP’s performance is consistent across these languages despite some slight differences, most probably due to factors other than word order (e.g. lower recall for Turkish due to a much larger vocabulary). Details can be found in a longer version of this paper (Wong, 2005). 444 0 50 100 150 200 250 300 Number of training examples 0 50 100 150 200 250 300 Number of training examples WASP SILT COCKTAIL SCISSOR WASP SILT COCKTAIL SCISSOR Precision (%) 100 80 60 40 20 0 Recall (%) 100 80 60 40 20 0 (a) Precision for ROBOCUP (b) Recall for ROBOCUP Recall (%) 100 80 60 40 20 0 WASP SILT COCKTAIL SCISSOR Zettlemoyer et al. (2005) Precision (%) 100 80 60 40 20 0 WASP SILT COCKTAIL SCISSOR Zettlemoyer et al. (2005) 0 100 200 300 400 500 600 700 800 Number of training examples (c) Precision for GEOQUERY 0 100 200 300 400 500 600 700 800 Number of training examples (d) Re</context>
</contexts>
<marker>Wong, 2005</marker>
<rawString>Y. W. Wong. 2005. Learning for semantic parsing using statistical machine translation techniques. Technical Report UT-AI-05-323, Artificial Intelligence Lab, University of Texas at Austin, Austin, TX, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of ACL-01,</booktitle>
<pages>523--530</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="4189" citStr="Yamada and Knight, 2001" startWordPosition="662" endWordPosition="665">th domains involve the use of MRs with a complex, nested structure. gorithm is its integration with state-of-the-art statistical machine translation techniques. More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL. Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (Aho and Ullman, 1972), which forms the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2005). Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing. In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order. Section 2 provides a brief overview of the domains being considered. In Section 3, we present the semantic parsing model of WASP. Section 4 outlines the algorithm for acquiring a bilingual lexicon through the use of </context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proc. of ACL-01, pages 523–530, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proc. of AAAI-96,</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR,</location>
<contexts>
<context position="2340" citStr="Zelle and Mooney, 1996" startWordPosition="369" endWordPosition="372">6 should stay in the left side of our half. Figure 1: A meaning representation in CLANG encoding coach advice given to simulated soccerplaying agents (Kuhlmann et al., 2004). Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require extensive human efforts when porting to a new domain or language. In this paper, we present a novel statistical approach to semantic parsing which can handle MRs with a nested structure, based on previous work on semantic parsing using transformation rules (Kate et al., 2005). The algorithm learn</context>
<context position="5552" citStr="Zelle and Mooney, 1996" startWordPosition="889" endWordPosition="892">f WASP in Section 6, followed by the conclusion in Section 7. 2 Application Domains In this paper, we consider two domains. The first domain is ROBOCUP. ROBOCUP (www.robocup.org) is an AI research initiative using robotic soccer as its primary domain. In the ROBOCUP Coach Competition, teams of agents compete on a simulated soccer field and receive coach advice written in a formal language called CLANG (Chen et al., 2003). Figure 1 shows a sample MR in CLANG. The second domain is GEOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. geography (Zelle and Mooney, 1996; Kate et al., 2005). Figure 2 3 The Semantic Parsing Model To describe the semantic parsing model of WASP, it is best to start with an example. Consider the task of translating the sentence in Figure 1 into its MR in CLANG. To achieve this task, we may first analyze the syntactic structure of the sentence using a semantic grammar (Allen, 1995), whose nonterminals are the ones in the CLANG grammar. The meaning of the sentence is then obtained by combining the meanings of its sub-parts according to the semantic parse. Figure 3(a) shows a possible partial semantic parse of the sample sentence ba</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>J. M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proc. of AAAI-96, pages 1050–1055, Portland, OR, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proc. of UAI-05,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2611" citStr="Zettlemoyer and Collins, 2005" startWordPosition="411" endWordPosition="415">as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require extensive human efforts when porting to a new domain or language. In this paper, we present a novel statistical approach to semantic parsing which can handle MRs with a nested structure, based on previous work on semantic parsing using transformation rules (Kate et al., 2005). The algorithm learns a semantic parser given a set of NL sentences annotated with their correct MRs. It requires no prior knowledge of the NL syntax, although it assumes that an unambiguous, context-free grammar (CFG) of the target MRL is available. The main innovation of this al439 Procee</context>
<context position="18329" citStr="Zettlemoyer and Collins (2005)" startWordPosition="3178" endWordPosition="3181"> observed NL string (e): 1 1: Prλ(d|e) = Zλ(e) exp Aifi(d) (2) where fi is a feature function, and Zλ(e) is a normalizing factor. For each rule r in the lexicon there is a feature function that returns the number of times r is used in a derivation. Also for each word w there is a feature function that returns the number of times w is generated from word gaps. Generation of unseen words is modeled using an extra feature whose value is the total number of words generated from word gaps. The number of features is quite modest (less than 3,000 in our experiments). A similar feature set is used by Zettlemoyer and Collins (2005). Decoding of the model can be done in cubic time with respect to sentence length using the Viterbi algorithm. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). The maximum conditional likelihood criterion is used for estimating the model parameters, Ai. A Gaussian prior (a2 = 1) is used for regularizing the model (Chen and Rosenfeld, 1999). Since gold-standard derivations are not available in the training data, correct derivations must be treated as hidden variables. Here we use a version of imi 443 proved iterative scaling (IIS) </context>
<context position="19575" citStr="Zettlemoyer and Collins (2005)" startWordPosition="3387" endWordPosition="3390">(Riezler et al., 2000) for finding an optimal set of parameters.1 Unlike the fully-supervised case, the conditional likelihood is not concave with respect to A, so the estimation algorithm is sensitive to initial parameters. To assume as little as possible, A is initialized to 0. The estimation algorithm requires statistics that depend on all possible derivations for a sentence or a sentence-MR pair. While it is not feasible to enumerate all derivations, a variant of the Inside-Outside algorithm can be used for efficiently collecting the required statistics (Miyao and Tsujii, 2002). Following Zettlemoyer and Collins (2005), only rules that are used in the best parses for the training set are retained in the final lexicon. All other rules are discarded. This heuristic, commonly known as Iiterbi approximation, is used to improve accuracy, assuming that rules used in the best parses are the most accurate. 6 Experiments We evaluated WASP in the ROBOCUP and GEOQUERY domains (see Section 2). To build a corpus for ROBOCUP, 300 pieces of coach advice were randomly selected from the log files of the 2003 ROBOCUP Coach Competition, which were manually translated into English (Kuhlmann et al., 2004). The average sentence </context>
<context position="22291" citStr="Zettlemoyer and Collins (2005)" startWordPosition="3844" endWordPosition="3847">if it exactly matched the correct MR. For GEOQUERY, a translation was correct if it retrieved the same answer as the correct query. Using these counts, we measured the performance of the parser in terms of precision (percentage of translations that were correct) and recall (percentage of test sentences that were correctly translated). For ROBOCUP, it took 47 minutes to learn a parser using IIS. For GEOQUERY, it took 83 minutes. Figure 6 shows the performance of WASP compared to four other algorithms: SILT (Kate et al., 2005), COCKTAIL (Tang and Mooney, 2001), SCISSOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005). Experimental results clearly show the advantage of extra supervision in SCISSOR and Zettlemoyer and Collins’s parser (see Section 1). However, WASP performs quite favorably compared to SILT and COCKTAIL, which use the same training data. In particular, COCKTAIL, a deterministic shift-reduce parser based on inductive logic programming, fails to scale up to the ROBOCUP domain where sentences are much longer, and crashes on larger training sets due to memory overflow. WASP also outperforms SILT in terms of recall, where lexical learning is done by a local bottom-up search, which is much less ef</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proc. of UAI-05, Edinburgh, Scotland, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>