<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.049303">
<title confidence="0.865136">
Inductive Dependency Parsing
</title>
<figure confidence="0.751728375">
Joakim Nivre
(V¨axj¨o University)
Dordrecht: Springer (Text, speech, and language technology series, edited by Nancy
Ide and Jean V´eronis, volume 34), 2006, xi+216 pp; hardbound, ISBN 1-4020-4888-2,
$119.00, €89.95
Reviewed by
Christer Samuelsson
Columbia University
</figure>
<bodyText confidence="0.997367125">
Philologists assure us that it’s worth learning ancient Greek just to read Homer. For
any linguist, it’s definitely worth learning French, just to read Lucien Tesni`ere’s Ele-
ments de Syntaxe Structurale (Tesni`ere 1959). For any serious dependency parsing stu-
dent or professional, it would have been worth learning Swedish, just to read Joakim
Nivre’s Inductive Dependency Parsing if Nivre had not done the world the immense
favor of writing his book in English, rather than in his native tongue of glory and
heroes.
The book demonstrates Nivre’s impressive ability to explain dependency grammar
and dependency parsing clearly and succinctly to a wide audience. His overviews of
dependency grammars (Section 3.1) and dependency parsing (Section 3.2) should be
required reading in any course on the topics. He concisely sums up each relevant
approach, often in a single sentence, and those wishing to learn more can rely on his
extensive collection of references.
The remainder of Chapter 3 sets the board for the proposed novel MALT
parser—the book’s main innovation—by formalizing the dependency parsing task and
specifying a particular type of deterministic shift-reduce parsing algorithm. Here, the
set of parser actions are appropriate for dependency parsing, rather than for con-
stituent parsing, and it is assumed that there is a one-to-one correspondence between
dependency analyses and valid, complete sequences of parser actions. Nivre proves
that the parsing algorithm correctly performs the formalized dependency parsing task,
producing an acyclic, single-headed, projective dependency graph.
Whereas the short Chapter 1 consists of the obligatory introduction, motivation, and
book overview, it is interesting to discuss Nivre’s ambitious four criteria for a successful
dependency parser, stated in Chapter 2:
</bodyText>
<listItem confidence="0.99765825">
1. Robustness: The parser must assign at least one dependency analysis to
each sentence in any given text.
2. Disambiguation: The parser must assign at most one dependency analysis
to each sentence in any given text.
3. Accuracy: The parser must assign the correct dependency analysis to each
sentence in any given text.
4. Efficiency: The parser must process all given sentences in all texts in time
and space linear in the size of each sentence.
</listItem>
<bodyText confidence="0.949795538461539">
Computational Linguistics Volume 33, Number 2
Nivre draws the inevitable conclusion from the first two criteria:
Existence and Uniqueness: The parser must assign exactly one analysis to
each sentence in any given text.
In Chapter 3, Nivre achieves the Accuracy criterion by presupposing the existence
of an Oracle that always provides the correct parser action. In later chapters, he reverts
to more traditional measures of Accuracy, while leaving the other three criteria intact.
The Efficiency criterion is achieved by the deterministic and greedy (non-ambiguity-
packing) nature of the shift-reduce parser. One could argue that even at a word level,
deterministic, non-packing parsing is not always possible, but Nivre sticks to his guns.
In Chapter 4, Nivre replaces the omniscient Oracle of Chapter 3 in two logical steps.
In the first step, the parser instead chooses the locally most advantageous parser action
a* according to some evaluation function g(a):
</bodyText>
<equation confidence="0.9637515">
a* = argmax g(a)
a
</equation>
<bodyText confidence="0.972087214285714">
This preserves the determinism and greediness of the parser, at the cost of potentially
losing the globally best action or the gold-standard action. The latter is the parser action
ultimately resulting in the reference dependency analysis of the training or testing data.
In the second step, an approximation ˆg(a) of this unknown evaluation function g(a)
is learned from training data, using some machine-learning scheme. Thus the parser
instead chooses the parser action ˆa*:
ˆa* = arg max ˆg(a)
a
which may very well deviate from the locally best parser action a*, as well as from the
globally best and the gold-standard parser actions. Nivre relates this learning scheme
to maximizing likelihood estimates and conditional likelihood estimates, as well as to
discriminative learning techniques. He also takes on the chore of relating the gold-
standard parser action to the dependency analyses of the reference data.
The logical progression and the clarity with which this is done is one of the many
strengths of this book. Many followers take the book this far as a point of departure for
alternative explorations into inductive dependency parsing.
Nivre next investigates feature selection for creating the function ˆg(a), then moves
on to his choice of machine-learning framework—memory-based learning (MBL)
(Daelemans and van den Bosch 2005)—ultimately leading to the proposed inductive
shift-reduce parser: the MALT parser.
In Chapter 5, Nivre evaluates his MALT parser on the mandatory Penn Treebank
parsing task. This is arguably a waste of time as: the Penn Treebank is a constituency
treebank; its underlying linguistic theory is quite poor; its annotation consistency is
even worse; and the rules for retrieving corresponding dependency analyses are, at best,
inspired hacks.
Many researchers in the field have used Nivre’s freely available MALT parser,1 but
have replaced MBL with some other machine-learning technique, such as conditional
random fields. About half of the entrants of the dependency-parsing shared task of the
</bodyText>
<footnote confidence="0.927548">
1 http://w3.msi.vxu.se/∼nivre/research/MaltParser.html.
</footnote>
<page confidence="0.98686">
268
</page>
<subsectionHeader confidence="0.920249">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9984805">
2006 Conference on Computational Natural Language Learning (CoNLL X) (Marquez
and Klein 2006) used this approach. That alone demonstrates the importance of this
book.
The 2007 CoNLL conference will most likely exhibit non-greedy, ambiguity-packing
versions of Nivre’s inductive shift-reduce parsing scheme, in the vein of Masaru
Tomita’s generalization of (natural language) LR parsing (Tomita 1987).
Get Nivre’s book; read it; and enjoy it! The excellent and thorough reference list
alone is worth it, constituting a good ten percent of the book.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.997903947368421">
Tesni`ere, Lucien. 1959. Elements de
Syntaxe Structurale. Editions
Klincksieck, Paris.
Daelemans, Walter and Antal van den
Bosch. 2005. Memory-Based Language
Processing, Cambridge University Press,
Cambridge.
Marquez, Lluis and Dan Klein, editors. 2006.
Proceedings of CoNLL-X: Tenth Conference on
Computational Natural Language Learning,
Brooklyn, New York, Association for
Computational Linguistics.
Tomita, Masaru. 1987. An efficient
augmented context-free parsing algorithm.
Computational Linguistics, 13(1–2): 31–46.
Christer Samuelsson has worked with natural language parsing at SICS, the University of the
Saarland, Bell Labs, Xerox Research Centre Europe, Lehman Brothers, and a few start-ups. He is
currently associated with the Center for Computational Learning Systems, Columbia University.
Samuelsson’s e-mail address is christer.samuelsson@gmail.com.
</reference>
<page confidence="0.998515">
269
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.152936">
<title confidence="0.999492">Inductive Dependency Parsing</title>
<author confidence="0.934492">Joakim Nivre</author>
<affiliation confidence="0.92335">(V¨axj¨o University) Dordrecht: Springer (Text, speech, and language technology series, edited by Nancy</affiliation>
<note confidence="0.610628">Ide and Jean V´eronis, volume 34), 2006, xi+216 pp; hardbound, ISBN 1-4020-4888-2, Reviewed by</note>
<author confidence="0.79182">Christer Samuelsson</author>
<affiliation confidence="0.999525">Columbia University</affiliation>
<author confidence="0.680767">For</author>
<abstract confidence="0.98847325">linguist, it’s definitely worth learning French, just to read Lucien Tesni`ere’s Elede Syntaxe Structurale 1959). For any serious dependency parsing student or professional, it would have been worth learning Swedish, just to read Joakim Dependency Parsing Nivre had not done the world the immense favor of writing his book in English, rather than in his native tongue of glory and heroes. The book demonstrates Nivre’s impressive ability to explain dependency grammar and dependency parsing clearly and succinctly to a wide audience. His overviews of dependency grammars (Section 3.1) and dependency parsing (Section 3.2) should be required reading in any course on the topics. He concisely sums up each relevant approach, often in a single sentence, and those wishing to learn more can rely on his extensive collection of references. The remainder of Chapter 3 sets the board for the proposed novel MALT parser—the book’s main innovation—by formalizing the dependency parsing task and specifying a particular type of deterministic shift-reduce parsing algorithm. Here, the set of parser actions are appropriate for dependency parsing, rather than for constituent parsing, and it is assumed that there is a one-to-one correspondence between dependency analyses and valid, complete sequences of parser actions. Nivre proves that the parsing algorithm correctly performs the formalized dependency parsing task, producing an acyclic, single-headed, projective dependency graph.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>Elements de Syntaxe Structurale. Editions Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Tesni`ere, Lucien. 1959. Elements de Syntaxe Structurale. Editions Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>Memory-Based Language Processing,</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>Daelemans, Walter and Antal van den Bosch. 2005. Memory-Based Language Processing, Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Marquez</author>
<author>Dan Klein</author>
<author>editors</author>
</authors>
<date>2006</date>
<booktitle>Proceedings of CoNLL-X: Tenth Conference on Computational Natural Language Learning,</booktitle>
<publisher>Association for Computational Linguistics.</publisher>
<location>Brooklyn, New York,</location>
<marker>Marquez, Klein, editors, 2006</marker>
<rawString>Marquez, Lluis and Dan Klein, editors. 2006. Proceedings of CoNLL-X: Tenth Conference on Computational Natural Language Learning, Brooklyn, New York, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An efficient augmented context-free parsing algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<issue>1</issue>
<pages>31--46</pages>
<marker>Tomita, 1987</marker>
<rawString>Tomita, Masaru. 1987. An efficient augmented context-free parsing algorithm. Computational Linguistics, 13(1–2): 31–46.</rawString>
</citation>
<citation valid="false">
<title>Christer Samuelsson has worked with natural language parsing at SICS, the University of the Saarland, Bell Labs, Xerox Research Centre Europe, Lehman Brothers, and a few start-ups. He is currently associated with the Center for Computational Learning Systems, Columbia University. Samuelsson’s e-mail address is christer.samuelsson@gmail.com.</title>
<marker></marker>
<rawString>Christer Samuelsson has worked with natural language parsing at SICS, the University of the Saarland, Bell Labs, Xerox Research Centre Europe, Lehman Brothers, and a few start-ups. He is currently associated with the Center for Computational Learning Systems, Columbia University. Samuelsson’s e-mail address is christer.samuelsson@gmail.com.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>