<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.998093">
A Novel Approach to Semantic Indexing Based on Concept
</title>
<author confidence="0.997105">
Bo-Yeong Kang
</author>
<affiliation confidence="0.99364">
Department of Computer Engineering
Kyungpook National University
</affiliation>
<address confidence="0.880076">
1370, Sangyukdong, Pukgu, Daegu, Korea(ROK)
</address>
<email confidence="0.985879">
comeng99@hotmail.com
</email>
<sectionHeader confidence="0.994768" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923846153846">
This paper suggests the efficient indexing
method based on a concept vector space
that is capable of representing the semantic
content of a document. The two informa-
tion measure, namely the information quan-
tity and the information ratio, are defined
to represent the degree of the semantic im-
portance within a document. The proposed
method is expected to compensate the lim-
itations of term frequency based methods
by exploiting related lexical items. Further-
more, with information ratio, this approach
is independent of document length.
</bodyText>
<sectionHeader confidence="0.998125" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979666666667">
To improve the unstable performance of a traditional
keyword-based search, a Web document should in-
clude both an index and index weight that represent
the semantic content of the document. However, most
of the previous works on indexing and the weighting
function, which depend on statistical methods, have
limitations in extracting exact indexes(Moens, 2000).
The objective of this paper is to propose a method that
extracts indexes efficiently and weights them accord-
ing to their semantic importance degree in a document
using concept vector space model.
A document is regarded as a conglomerate con-
cept that comprises by many concepts. Hence, an n-
dimensional concept vector space model is defined in
such a way that a document is recognized as a vec-
tor in n-dimensional concept space. We used lexical
chains for the extraction of concepts. With concept
vectors and text vectors, semantic indexes and their
semantic importance degree are computed. Further-
more, proposed indexing method had an advantage in
being independent of document length because we re-
garded overall text information as a value 1 and repre-
sented each index weight by the semantic information
ratio of overall text information.
</bodyText>
<sectionHeader confidence="0.999495" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.99996134375">
Since index terms are not equally important regard-
ing the content of the text, they have term weights as
an indicator of importance. Many weighting functions
have been proposed and tested. However, most weight
functions depend on the statistical methods or on the
document’s term distribution tendency. Representa-
tive weighting functions include such factors as term
frequency, inverse document frequency, the product of
the term and inverse document frequency, and length
normalization(Moens, 2000).
Term frequency is useful in a long document, but
not in a short document. In addition, term frequency
cannot represent the exact term frequency because it
does not include anaphoras, synonyms, and so on.
Inverse document frequency is inappropriate for a
reference collection that changes frequently because
the weight of an index term needs be recomputed.
A length normalization method is proposed because
term frequency factors are numerous for long docu-
ments, and negligible for short ones, obscuring the
real importance of terms. As this approach also uses
term frequency function, it has the same disadvantage
as term frequency does.
Hence, we made an effort to use methods based
on the linguistic phenomena to enhance the index-
ing performance. Our approach focuses on proposing
concept vector space for extracting and weighting in-
dexes, and we intend to compensate limitations of the
term frequency based methods by employing lexical
chains. Lexical chains are to link related lexical items
in a document, and to represent the lexical cohesion
structure of a document(Morris, 1991).
</bodyText>
<sectionHeader confidence="0.967137" genericHeader="method">
3 Semantic Indexing Based on Concept
</sectionHeader>
<bodyText confidence="0.999954142857143">
Current approaches to index weighting for informa-
tion retrieval are based on the statistic method. We
propose an approach that changes the basic index term
weighting method by considering semantics and con-
cepts of a document. In this approach, the concepts of
a document are understood, and the semantic indexes
and their weights are derived from those concepts.
</bodyText>
<subsectionHeader confidence="0.999528">
3.1 System Overview
</subsectionHeader>
<bodyText confidence="0.999989375">
We have developed a system that performs the index
term weighting semantically based on concept vector
space. A schematic overview of the proposed system
is as follows: A document is regarded as a complex
concept that consists of various concepts; it is recog-
nized as a vector in concept vector space. Then, each
concept was extracted by lexical chains(Morris, 1988
and 1991). Extracted concepts and lexical items were
scored at the time of constructing lexical chains. Each
scored chain was represented as a concept vector in
concept vector space, and the overall text vector was
made up of those concept vectors. The semantic im-
portance of concepts and words was normalized ac-
cording to the overall text vector. Indexes that include
their semantic weight are then extracted.
The proposed system has four main components:
</bodyText>
<listItem confidence="0.999995">
• Lexical chains construction
• Chains and nouns weighting
• Term reweighting based on concept
• Semantic index term extraction
</listItem>
<bodyText confidence="0.999981">
The former two components are based on concept
extraction using lexical chains, and the latter two com-
ponents are related with the index term extraction
based on the concept vector space, which will be ex-
plained in the next section.
</bodyText>
<subsectionHeader confidence="0.990442">
3.2 Lexical Chains and Concept Vector Space
Model
</subsectionHeader>
<bodyText confidence="0.999853">
Lexical chains are employed to link related lexical
items in a document, and to represent the lexical co-
hesion structure in a document(Morris, 1991). In ac-
cordance with the accepted view in linguistic works
that lexical chains provide representation of discourse
structures(Morris, 1988 and 1991), we assume that
</bodyText>
<figureCaption confidence="0.989434">
Figure 1: Lexical chains of a sample text
</figureCaption>
<bodyText confidence="0.990642222222222">
each lexical chain is regarded as a concept that ex-
presses the meaning of a document. Therefore, each
concept was extracted by lexical chains.
For example, Figure 1 shows a sample text com-
posed of five chains. Since we can not deal all the
concept of a document, we discriminate representative
chains from lexical chains. Representative chains are
chains delegated to represent a representative concept
of a document. A concept of the sample text is mainly
composed of representative chains, such as chain 1,
chain 2, and chain 3. Each chain represents each
different representative concept: for example man,
machine and anesthetic.
As seen in Figure 1, a document consists of various
concepts. These concepts represent the semantic con-
tent of a document, and their composition generates a
complex composition. Therefore we suggest the con-
cept space model where a document is represented by
a complex of concepts. In the concept space model,
lexical items are discriminated by the interpretation
of concepts and words that constitute a document.
Definition 1 (Concept Vector Space Model)
Concept space is an n-dimensional space composed
of n-concept axes. Each concept axis represents
one concept, and has a magnitude of CZ. In concept
space, a document T is represented by the sum of
n-dimensional concept vectors, CZ.
</bodyText>
<equation confidence="0.992794333333333">
n
T= CZ (1)
Z=1
</equation>
<bodyText confidence="0.934060333333333">
Although each concept that constitutes the overall
text is different, concept similarity may vary. In this
paper, however, we assume that concepts are mutually
independent without consideration of their similarity.
Figure 2 shows the concept space version of the sam-
ple text.
</bodyText>
<subsectionHeader confidence="0.998729">
3.3 Concept Extraction Using Lexical Chains
</subsectionHeader>
<bodyText confidence="0.997171">
Lexical chains are employed for concept extraction.
Lexical chains are formed using WordNet and asso-
</bodyText>
<figure confidence="0.998574173913043">
machine
device
blood
Dr.
Kenny
anesthetic
anesthetic
rate
C1
0.6
C2
device
Kenny
0.7
anesthetic
Document
1.0
Cy
C2
Text
w1+w2+w3 = a
C1
b
</figure>
<page confidence="0.833696">
2
</page>
<equation confidence="0.599996666666667">
y
=
w4+ws = b
a2 +b
2
a2+b
</equation>
<page confidence="0.550112">
2
</page>
<figure confidence="0.7098026">
x
=
a
2
a2+b
</figure>
<page confidence="0.918001">
2
</page>
<figureCaption confidence="0.999381">
Figure 2: The concept space version of the sample text
</figureCaption>
<bodyText confidence="0.997828111111111">
ciated relations among words. Chains have four re-
lations: synonym, hypernyms, hyponym, meronym.
The definitions on the score of each noun and chain
are written as definition 2 and definition 3.
Definition 2 (Score of Noun) Let NRkNi denotes the
number of relations that noun Ni has with relation k.
SRkNi represents the weight of relation k. Then the
score SNOUN(Ni) of a noun Ni in a lexical chain is
defined as:
</bodyText>
<equation confidence="0.9902825">
k k
(NRNi X SRNi) (2)
</equation>
<bodyText confidence="0.915107">
where k E set of relations.
</bodyText>
<figure confidence="0.4842145">
Definition 3 (Score of Chain) The score
SCHAIN(Chx) of a chain Chx is defined as:
</figure>
<figureCaption confidence="0.997028">
Figure 3: Vector space property
</figureCaption>
<bodyText confidence="0.87505825">
Each concept is composed of words and its weight
wi. In composing the text concept vector, the part
that vector C1 contributes to a text vector is x, and
the part that vector C2 contributes is y. By expanding
the vector space property, the weight of lexical items
and concepts was normalized as in definitions 5 and
definition 6.
Definition 5 (Information Quantity, Ω)
Information quantity is the semantic quantity of
a text, concept or a word in the overall document
information. ΩT, ΩC, ΩW are defined as follows. The
magnitude of concept vector Ci is S CHAIN (Chi)
</bodyText>
<equation confidence="0.949944142857143">
ΩT = C2k (5)
k
X
SNOUN(Ni) =
k
C2
ΩCi =
i
qP (6)
k C2k
SCHAIN(Chx) = Xn SNOUN(Ni) + penalty (3)
i=1
where SNOUN(Ni) is the score of noun Ni, and
N1, ..., Nn E Chx.
</equation>
<bodyText confidence="0.996923666666667">
Representative chains are chains delegated to rep-
resent concepts. If the number of the chains was m,
chain Chx, should satisfy the criterion of the defini-
tion 4.
Definition 4 (Criterion of Representative Chain)
The criterion of representative chain, is defined as:
</bodyText>
<equation confidence="0.918032">
SCHAIN(Chi) (4)
</equation>
<subsectionHeader confidence="0.984902">
3.4 Information Quantity and Information Ratio
</subsectionHeader>
<bodyText confidence="0.9990224">
We describe a method to normalize the semantic im-
portance of each concept and lexical item on the con-
cept vector space. Figure 3 depicts the magnitude of
the text vector derived from concept vectors C1 and
C2. When the magnitude of vector C1 is a and that of
</bodyText>
<equation confidence="0.9942164">
\/
vector C2 is b, the overall text magnitude is a2 + b2.
Wj · Ci
ΩWj = ΩT X ΨWj|T = qP (7)
k C2k
</equation>
<bodyText confidence="0.99145825">
The text information quantity, denoted by ΩT, is the
magnitude generated by the composition of all con-
cepts. ΩCi denotes the concept information quantity.
The concept information quantity was derived by the
same method in which x and y were derived in Fig-
ure 3. ΩWj represents the information quantity of a
word. ΨWj|T is illustrated below.
Definition 6 (Information Ratio, Ψ) Information
ratio is the ratio of the information quantity of a
comparative target to the information quantity of a
text, concept or word. ΨC|T, ΨW|C and ΨW|T are
defined as follows:
</bodyText>
<equation confidence="0.998924923076923">
SNOUN(Wj) |Wj|
ΨWj|Ci = (8)
SCHAIN(Ci) = |Ci|
ΩCi C2 i
ΨCi|T = = P (9)
ΩT k C2 k
SCHAIN(Chx) �! α ·
1
m
Xm
i=1
W� · CZ
XFWj|T = XFWj |Ci × XFC |T = i C (10) C2 k
</equation>
<bodyText confidence="0.810948923076923">
The weight of a word and a chain was given when
forming lexical chains by definitions 2 and 3. XFWj|Ci
denotes the information ratio of a word to the concept
in which it is included. XFCi|T is the information ratio
of a concept to the text. The information ratio of a
word to the overall text is denoted by XFWi|T .
The semantic index and weight are extracted ac-
cording to the numerical value of information quantity
and information ratio. We extracted nouns satisfying
definition 7 as semantic indexes.
Definition 7 (Semantic Index) The semantic index
that represents the content of a document is defined
as follows:
</bodyText>
<equation confidence="0.93896325">
�m (QWi) (11)
1
QWj ≥ Q ·
m i=1
</equation>
<bodyText confidence="0.999625727272727">
Although in both cases information quantity is the
same, the relative importance of each word in a doc-
ument differs according to the document informa-
tion quantity. Therefore, we regard information ra-
tio rather than information quantity as the semantic
weight of indexes. This approach has an advantage
in that we need not consider document length when
indexing because the overall text information has a
value 1 and the weight of the index is provided by the
semantic information ratio to overall text information
value, 1, whether a text is long or not.
</bodyText>
<sectionHeader confidence="0.997216" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999962375">
In this section we discuss a series of experiments con-
ducted on the proposed system. The results achieved
below allow us to claim that the lexical chains and
concept vector space effectively provide us with the
semantically important index terms. The goal of the
experiment is to validate the performance of the pro-
posed system and to show the potential in search per-
formance improvement.
</bodyText>
<subsectionHeader confidence="0.993983">
4.1 Standard TF vs. Semantic Indexing
</subsectionHeader>
<bodyText confidence="0.9998815">
Five texts of Reader’s Digest from Web were selected
and six subjects participated in this study. The texts
were composed of average 11 lines in length(about
five to seventeen lines long), each focused on a
specific topic relevant to exercise, diet, holiday
blues,yoga, and weight control. Most texts are re-
lated to a general topic, exercise. Each subject was
presented with five short texts and asked to find index
</bodyText>
<tableCaption confidence="0.9803195">
Table 1: Manually extracted index terms and rele-
vancy to exercise
</tableCaption>
<table confidence="0.9998355">
Text Index Rel.
Text1 exercise(0.39) back(0.3) 0.64
pain(0.175)
Text2 diet(0.56) exercise(0.31) 0.55
Text3 yoga(0.5) exercise(0.25) 0.45
mind(0.11) health(0.1)
Text4 weight(0.46) control(0.18) 0.26
calorie(0.11) exercise(0.11)
Text5 holiday(0.432) humor(0.23) 0.099
blues(0.15)
</table>
<tableCaption confidence="0.963215">
Table 2: Percent Agreement(PA) to manually ex-
tracted index terms
</tableCaption>
<equation confidence="0.6305915">
T1 T2 T3 T4 T5 Avg.
PA 0.79 1.0 0.88 0.79 0.83 0.858
</equation>
<bodyText confidence="0.999862">
terms and weight each with value from 0 to 1. Other
than that, relevancy to a general topic, exercise, was
rated for each text. The score that was rated by six
subjects is normalized as an average.
The results of manually extracted index terms and
their weights are given in Table 1. The index term
weight and the relevance score are obtained by aver-
aging the individual scores rated by six subjects. Al-
though a specific topic of each text is different, most
texts are related to the exercise topic. The percent
agreement to the selected index terms is shown in Ta-
ble 2(Gale, 1992). The average percent agreement is
about 0.86. This indicates the agreement among sub-
jects to an index term is average 86 percent.
We compared these ideal result with standard term
frequency(standard TF, S-TF) and the proposed se-
mantic weight. Table 3 and Figures 4-6 show the com-
parison results. We omitted a few words in represent-
ing figures and tables, because standard TF method
extracts all words as index terms. From Table 3,
subjects regarded exercise, back, and pain as index
terms in Text 1, and the other words are recognized as
relatively unimportant ones. Even though exercise
was mentioned only three times in Text 1, it had con-
siderable semantic importance in the document; yet its
standard TF weight did not represent this point at all,
because the importance of exercise was the same as
that of muscle, which is also mentioned three times in
a text. The proposed approach, however, was able to
</bodyText>
<figure confidence="0.997193090909091">
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
exercise back pain leg muscle chest way routine program strength
word
</figure>
<figureCaption confidence="0.993166">
Figure 4: Weight comparison of Text1
</figureCaption>
<tableCaption confidence="0.995698">
Table 3: Weight comparison of Text 1
</tableCaption>
<table confidence="0.97077025">
Text 1
Word Subject Weight Standard TF Semantic Weight
exercise 0.39 0.29 0.3748
back 0.3 0.67 0.4060
pain 0.175 0.19 0.1065
chest 0.0 0.19 0.1398
leg 0.0 0.19 0.0506
muscle 0.0 0.29 0.0676
way 0.0 0.19 0.0
routine 0.0 0.19 0.0
program 0.0 0.09 0.0
strength 0.0 0.09 0.0
</table>
<bodyText confidence="0.998092842105263">
differentiate the semantic importance of words. Fig-
ure 4 shows the comparison chart version of Table 3,
which contains three weight lines. As the weight line
is closer to the subject weight line, it is expected to
show better performance. We find from the figure that
the semantic weight line is analogous to the manually
weighted value line than the the standard TF weight
line is.
Figures 5 and 6 show two of four texts(Text2,
Text3, Text4, Text5). Figures on the other texts are
omitted due to space consideration. In Figure 5,
pound is mentioned most frequently in a text, con-
sequently, standard TF rates the weight of pound very
high. Nevertheless, subjects regarded it as unimpor-
tant word. Our approach discriminated its impor-
tance and computed its weight lower than diet and
exerciese. From the results, we see the proposed sys-
tem is more analogous to the user weight line than the
standard TF weight line.
</bodyText>
<tableCaption confidence="0.901778">
Table 4: Weight comparison to the index term
exercise of five texts.
</tableCaption>
<table confidence="0.999074">
Text Subject TF LN S-TF Proposed Rel.
1 0.39 3 0.428 0.29 0.3748 0.64
2 0.31 3 0.75 0.375 0.2401 0.55
3 0.25 1 0.33 0.18 0.1320 0.45
4 0.11 1 0.125 0.11 0 0.26
5 0 1 0.2 0.12 0 0.09
</table>
<subsectionHeader confidence="0.881858">
4.2 Applicability of Search Performance
Improvements
</subsectionHeader>
<bodyText confidence="0.999947235294118">
When semantically indexed texts are probed with a
single query, exercise, the ranking result is expected
to be the same as the order of the relevance score to the
general topic exercise, which was rated by subjects.
Table 4 lists the weight comparison to the index
term exercise of five texts, and the subjects’ rele-
vance rate to the general topic exercise. Subjects’
relevance rate is closely related with the subjects’
weight to the index term, exericise. The expected
ranking result is as following Table 5. TF weight
method hardly discerns the subtle semantic impor-
tance of each texts, for example, Text1 and Text2 have
the same rank. Length normalization(LN) and stan-
dard TF discern each texts but fail to rank correctly.
However, the proposed indexing method provides bet-
ter ranking results than the other TF based indexing
methods.
</bodyText>
<subsectionHeader confidence="0.988555">
4.3 Conclusion
</subsectionHeader>
<bodyText confidence="0.999524666666667">
In this paper, we intended to change the basic indexing
methods by presenting a novel approach using a con-
cept vector space model for extracting and weighting
indexes. Our experiment for semantic indexing sup-
ports the validity of the presented approach, which
is capable of capturing the semantic importance of
</bodyText>
<figure confidence="0.997887555555556">
0.6
0.5
0.4
0.3
0.2
0.1
0
diet pound exercise low-fat week husband weight player gym calorie
word
</figure>
<figureCaption confidence="0.923136">
Figure 5: Weight comparison of Text2
</figureCaption>
<figure confidence="0.9993697">
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
holiday humor blues season cartoon christmas negativity exercise sense
word
</figure>
<figureCaption confidence="0.996981">
Figure 6: Weight comparison of Text5
</figureCaption>
<tableCaption confidence="0.989768">
Table 5: Expected ranking results to the query
</tableCaption>
<table confidence="0.944552181818182">
exercise
Rank Rel. Subject TF LN S-TF Proposed
1 Text1 Text1 Text1 Text2 Text2 Text1
Text2
2 Text2 Text2 Text3 Text1 Text1 Text2
Text4
Text5
3 Text3 Text3 Text3 Text3 Text3
4 Text4 Text4 Text5 Text5 Text4
Text5
5 Text5 Text5 Text4 Text4
</table>
<bodyText confidence="0.999813571428571">
a word within the overall document. Seen from the
experimental results, the proposed method achieves a
level of performance comparable to major weighting
methods. In an experiment, we didn’t compared our
method with inverse document frequency(IDF) yet,
because we will develop more sophisticated weight-
ing method concerning IDF in future work.
</bodyText>
<sectionHeader confidence="0.998477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984638235294118">
R. Barzilay and M. Elhadad, Using lexical chains for text
summarization, Proc. ACL’97 Workshop on Intelligent
Scalable Text Summarization(1997).
M.-F. Moens, Automatic Indexing and Abstracting of Doc-
ument Texts, Kluwer Academic Publishers(2000).
J. Morris, Lexical cohesion, the thesaurus, and the struc-
ture of text, Master’s thesis, Department of Computer
Science, University of Toronto(1988).
J. Morris and G. Hirst, Lexical cohesion computed by the-
saural relations as an indicator of the structure of text,
Computational Linguistics 17(1)(1991) 21-43.
W. Gale, K. Church, and D. Yarowsky, Extimating upper
and lower bounds on the performance of word-sense
disambiguation programs. In Proceedings of the 30th
annual Meeting of the Association for Computational
Linguistics(ACL-92)(1992) 249-256.
Reader’s Digest Web site, http://www.rd.com
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970191">
<title confidence="0.998876">A Novel Approach to Semantic Indexing Based on Concept</title>
<author confidence="0.996824">Bo-Yeong Kang</author>
<affiliation confidence="0.999275">Department of Computer Engineering Kyungpook National University</affiliation>
<address confidence="0.995706">1370, Sangyukdong, Pukgu, Daegu, Korea(ROK)</address>
<email confidence="0.998237">comeng99@hotmail.com</email>
<abstract confidence="0.998566571428571">This paper suggests the efficient indexing method based on a concept vector space that is capable of representing the semantic content of a document. The two information measure, namely the information quantity and the information ratio, are defined to represent the degree of the semantic importance within a document. The proposed method is expected to compensate the limitations of term frequency based methods by exploiting related lexical items. Furthermore, with information ratio, this approach is independent of document length.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization,</title>
<booktitle>Proc. ACL’97 Workshop on Intelligent Scalable Text Summarization(1997).</booktitle>
<marker>Barzilay, Elhadad, </marker>
<rawString>R. Barzilay and M. Elhadad, Using lexical chains for text summarization, Proc. ACL’97 Workshop on Intelligent Scalable Text Summarization(1997).</rawString>
</citation>
<citation valid="false">
<authors>
<author>M-F Moens</author>
</authors>
<title>Automatic Indexing and Abstracting of Document Texts,</title>
<publisher>Kluwer Academic Publishers(2000).</publisher>
<marker>Moens, </marker>
<rawString>M.-F. Moens, Automatic Indexing and Abstracting of Document Texts, Kluwer Academic Publishers(2000).</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Morris</author>
</authors>
<title>Lexical cohesion, the thesaurus, and the structure of text,</title>
<tech>Master’s thesis,</tech>
<institution>Department of Computer Science, University of Toronto(1988).</institution>
<marker>Morris, </marker>
<rawString>J. Morris, Lexical cohesion, the thesaurus, and the structure of text, Master’s thesis, Department of Computer Science, University of Toronto(1988).</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text,</title>
<journal>Computational Linguistics</journal>
<volume>17</volume>
<issue>1</issue>
<pages>21--43</pages>
<marker>Morris, Hirst, </marker>
<rawString>J. Morris and G. Hirst, Lexical cohesion computed by thesaural relations as an indicator of the structure of text, Computational Linguistics 17(1)(1991) 21-43.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Extimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<booktitle>In Proceedings of the 30th annual Meeting of the Association for Computational Linguistics(ACL-92)(1992)</booktitle>
<pages>249--256</pages>
<marker>Gale, Church, Yarowsky, </marker>
<rawString>W. Gale, K. Church, and D. Yarowsky, Extimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th annual Meeting of the Association for Computational Linguistics(ACL-92)(1992) 249-256.</rawString>
</citation>
<citation valid="false">
<title>Reader’s Digest Web site,</title>
<location>http://www.rd.com</location>
<marker></marker>
<rawString>Reader’s Digest Web site, http://www.rd.com</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>