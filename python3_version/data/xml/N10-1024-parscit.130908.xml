<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.8803525">
Cheap, Fast and Good Enough:
Automatic Speech Recognition with Non-Expert Transcription
</title>
<author confidence="0.995709">
Scott Novotney and Chris Callison-Burch
</author>
<affiliation confidence="0.909306">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.995542">
snovotne@bbn.com ccb@jhu.edu
</email>
<sectionHeader confidence="0.979908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999659421052632">
Deploying an automatic speech recogni-
tion system with reasonable performance
requires expensive and time-consuming
in-domain transcription. Previous work
demonstrated that non-professional anno-
tation through Amazon’s Mechanical Turk
can match professional quality. We use
Mechanical Turk to transcribe conversa-
tional speech for as little as one thir-
tieth the cost of professional transcrip-
tion. The higher disagreement of non-
professional transcribers does not have a
significant effect on system performance.
While previous work demonstrated that
redundant transcription can improve data
quality, we found that resources are bet-
ter spent collecting more data. Finally, we
describe a quality control method without
needing professional transcription.
</bodyText>
<sectionHeader confidence="0.997334" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99977775">
Successful speech recognition depends on huge
investments in data collection. Even after train-
ing on 2000+ hours of transcribed conversa-
tional speech, over a billion words of language
modeling text, and hand-crafted pronunciation
dictionaries, state of the art systems still have
an error rate of around 15% for English (Prasad
et al., 2005) Transcribing the large volumes of
data required for Large Vocabulary Continuous
Speech Recognition (LVCSR) of new languages
appears prohibitively expensive. Recent work
has shown that Amazon’s Mechanical Turk&apos; can
</bodyText>
<footnote confidence="0.975699">
1http://www.mturk.com
</footnote>
<bodyText confidence="0.998952735294118">
be used to cheaply create data for other nat-
ural language processing applications (Snow et
al., 2008; Zaidan and Callison-Burch, 2009; Mc-
Graw et al., 2009). In this paper we focus
on reducing the cost of transcribing conversa-
tional telephone speech (CTS) data. Previous
measurements of Mechanical Turk stopped at
agreement/disagreement with professional an-
notation. We take the next logical step and
measure performance on systems trained with
non-professional transcription.
Mechanical Turk is an online labor mar-
ket where workers (or Turkers) perform simple
tasks called Human Intelligence Tasks (HITs)
for small amounts of money – frequently as lit-
tle as $0.01 per HIT. Since HITs can be tasks
that are difficult for computers, but easy for hu-
mans, they are ideal for natural language pro-
cessing tasks (Snow et al., 2008). Mechanical
Turk has even spawned a business that special-
izes in manual speech transcription.2
Automatic speech recognition (ASR) of con-
versational speech is an extremely difficult prob-
lem. Characteristics like rapid speech, pho-
netic reductions and speaking style limit the
value of non-CTS data, necessitating in-domain
transcription. Even a few hours of transcrip-
tion is sufficient to bootstrap with unsupervised
methods like self-training (Lamel et al., 2002).
The speech community has built effective down-
stream solutions for the past twenty years de-
spite imperfect recognition. In topic classifi-
cation, 90% accuracy is possible on conversa-
tional data even with 80%+ word error rate
</bodyText>
<footnote confidence="0.959531">
2http://castingwords.com/
</footnote>
<page confidence="0.925338">
207
</page>
<note confidence="0.7621215">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 207–215,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.982398375">
(WER) (Gillick et al., 1993). Other successful
tasks include information retrieval from speech
(Miller et al., 2007) and spoken dialogue pro-
cessing (Young et al., 2007). Inexpensive tran-
scription would quickly open new languages or
domains (like meeting or lecture data) for auto-
matic speech recognition.
In this paper, we make the following points:
</bodyText>
<listItem confidence="0.99420775">
• Quality control isn’t necessary as a system
built with non-professional transcription is
only 6% worse for 130 the cost of professional
transcription.
• Resources are better spent collecting more
data than improving data quality.
• Transcriber skill can be accurately esti-
mated without gold standard data.
</listItem>
<sectionHeader confidence="0.995851" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999854575">
Research into Mechanical Turk by the NLP com-
munity has largely focused on comparing the
quality of annotations produced by non-expert
Turkers against annotations created by experts.
Snow et al. (2008) conducted a comprehensive
study across a variety of NLP tasks. They
showed that high agreement could be reached
with gold-standard expert annotation for these
tasks through a weighted combination of ten re-
dundant annotations produced by Turkers.
Callison-Burch (2009) showed similar results
for machine translation evaluation, and further
showed that Turkers could accomplish complex
tasks like translating Urdu or creating reading
comprehension tests.
McGraw et al. (2009) used Mechanical Turk
to improve an English isolated word speech rec-
ognizer by having Turkers listen to a word and
select from a list of probable words at a cost of
$20 per hour of transcription.
Marge et al. (2010) collected transcriptions of
verbal instructions to robots with clean speech.
By using five duplicate transcriptions, the aver-
age transcription disagreement with experts was
reduced from 4% to 2%.
Previous efforts at reducing the cost of tran-
scription include the EARS Fisher project (Cieri
et al., 2004), which collected 2000+ hours of En-
glish CTS data – an order of magnitude more
than had previously been transcribed. To speed
transcription and lower costs, Kimball et al.
(2004) created new transcription guidelines and
used automatic segmentation. These improved
the speed of transcription from fifty times real
time to six times real time, and made it cost
effective to transcribe 2000 hours at an aver-
age of $150 per hour. Models trained on the
faster transcripts exhibited almost no degra-
dation in performance, although discrimanitve
training was sensitive to transcription errrors.
</bodyText>
<sectionHeader confidence="0.993503" genericHeader="method">
3 Experiment Description
</sectionHeader>
<subsectionHeader confidence="0.991982">
3.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999875391304348">
We conducted most experiments on a twenty
hour subset of the English Switchboard corpus
(Godfrey et al., 1992) where two strangers con-
verse about an assigned topic. We used two sets
of transcription as our gold standard: high qual-
ity transcription from the LDC and those fol-
lowing the Fisher quick transcription guidelines
(Kimball et al., 2004) provided by a professional
transcription company. All English ASR models
were tested with the carefully transcribed three
hour Dev04 test set from the NIST HUB5 eval-
uation.3 A 75k word lexicon taken from the
EARS Fisher training corpus covers the LDC
training data and has a test OOV rate of 0.18%.
We also conducted experiments in Korean and
collected Hindi and Tamil data from the Call-
friend corpora 4. Participants were given a free
long distance phone call to talk with friends or
family in their native language, although En-
glish frequently appears. Since Callfriend was
originally intended for language identification,
only the 27 hour Korean portion has been tran-
scribed by the LDC.
</bodyText>
<subsectionHeader confidence="0.999759">
3.2 LVCSR System
</subsectionHeader>
<bodyText confidence="0.9998304">
We used Byblos, a state-of-the-art multi-pass
LVCSR system with state-clustered Gaussian
tied-mixture acoustic models and modified
Kneser-Ney smoothed language models (Prasad
et al., 2005). While understanding the system
</bodyText>
<footnote confidence="0.999856">
3http://www.itl.nist.gov/iad/mig/tests/ctr/
1998/current-plan.html
4http://www.ldc.upenn.edu/CallFriend2/
</footnote>
<page confidence="0.99713">
208
</page>
<bodyText confidence="0.999574764705882">
details is not essential for this work, we provide
a brief description for completeness.
Recognition begins with cepstral feature ex-
traction using concatenated frames with cepstral
mean subtraction and HLDA to reduce the fea-
ture dimension space. Vocal track length nor-
malization follows. Decoding then requires three
passes: a fast forward pass with coarse one-
gaussian-per-phone models and bigram LM fol-
lowed by a backward pass with triphone models
and a trigram LM to generate word confusion
lattices. The lattices are rescored with a more
powerful quinphone cross-word acoustic model
and trigram LM to extract the one best out-
put. These three steps are repeated after un-
supervised speaker adaptation with constrained
MLLR. Decoding is around ten times real time.
</bodyText>
<subsectionHeader confidence="0.999253">
3.3 Transcription Task
</subsectionHeader>
<bodyText confidence="0.999941">
Using language-independent speaker activity de-
tection models, we segmented each ten minute
conversation into five second utterances, greatly
simplifying the transcription task (Roy and Roy,
2009). Utterances were assigned in batches of
ten per HIT and played with a simple flash
player with a text box for entry. All non-empty
HITs were approved and we did not award
bonuses except as described in Section 5.1.
</bodyText>
<subsectionHeader confidence="0.996414">
3.4 Measuring Annotation Quality
</subsectionHeader>
<bodyText confidence="0.999974705882353">
The usefullness of the transcribed data is ul-
timately measured by how much it benefits a
speech recognition system. Factors that inflate
disagreement (word error rate) between Turkers
and professionals do not necessarily impact sys-
tem performance. These include typographical
mistakes, transcription inconsistencies (like im-
properly marking hesitations or the many vari-
ations of um) and spelling variations (geez or
jeez are both valid spellings). Additionally, the
gold standard is itself imperfect, with typical
estimates of professional disagreement around
five percent. Therefore, we judge the quality of
Mechanical Turk data by comparing the perfor-
mance of one LVCSR system trained on Turker
annotation and another trained on professional
transcriptions of the same dataset.
</bodyText>
<figure confidence="0.913049">
Average Turker Transcription Productivity for English
Mean Turker Productivity 12xRT
0 10 20 30 40 50 60
Transcription time / Utterance length (xRT)
</figure>
<figureCaption confidence="0.7522484">
Figure 1: Histogram of per-turker transcription rate
for twenty hours of English CTS data. Historical
estimates for high quality transcription are 50xRT.
The 2004 Fisher transcription effort achieved 6xRT
and the average here is 11xRT.
</figureCaption>
<figure confidence="0.2270325">
4 Establishing Best Practices with
English Switchboard
</figure>
<bodyText confidence="0.999032416666667">
As an initial test to see how cheaply conversa-
tional data could be transcribed, we uploaded
one hour of test data from Hub5 Dev04. We
first paid $0.20 per HIT ($0.02 per utterance).
This test finished quickly, and we measured the
average disagreement with professionals at 17%.
Next, we reduced payment to $0.10 per HIT
and disagreement was again 17%. Finally, we
pushed the price down to $0.05 per HIT or $5
per hour of transcription and again disagree-
ment was nearly identical at 18%, although a
few Turkers complained about the low pay.
Using this price, we then paid for the full
twenty hours to be redundantly transcribed
three times. 1089 Turkers participated in the
task at an incoming rate of 10 hours of tran-
scription per day. On average, each Turker tran-
scribed 30 utterance (earning 15 cents) at an
average professional disagreement of 23%. Tran-
scribing one minute of audio required an aver-
age eleven minutes of effort (denoted 11xRT).
63 workers transcribed more than one hundred
utterances and one prolific worker transcribed
1223 utterances.
</bodyText>
<figure confidence="0.98944175">
Fisher QuickTrans Speed
Typical High Quality Speed
Number of Turkers
0 20 40 60 80 100
</figure>
<page confidence="0.971756">
209
</page>
<subsectionHeader confidence="0.9996365">
4.1 Comparing Non-Professional to
Professional Transcription
</subsectionHeader>
<bodyText confidence="0.9998346">
Table 1 details the results of different selection
methods for redundant transcription. For each
method of selection, we build an acoustic and
language model and report WER on the heldout
test set (transcribed at very high accuracy).
We first randomly selected one of the three
transcriptions per utterance (as if the data were
only tanscribed once) and repeated this three
times with little variance. Selecting utterances
randomly by nrker performed similarly. Per-
formance of an LVCSR system trained on the
non-professional transcription degrades by only
2.5% absolute (6% relative) despite a disagree-
ment of 23%. This is without any quality
control besides throwing out empty utterances.
The degradation held constant as we swept the
amount of training data frome one to twenty
hours. Bot the acoustic and language models ex-
hibited the log-linear relationship between WER
and the amount of training data. Independent of
the amount of training data, the acoustic model
degraded by a nearly constant 1.7% and the lan-
guage model by 0.8%.
To evaluate the benefit of multiple transcrip-
tions, we built two oracle systems. The nrker
oracle ranks Turkers by the average error rate of
their transcribed utterances against the profes-
sionals and selects utterances by Turker until the
twenty hours is covered (Section 4.3 discusses a
fair way to rank Turkers). The utterance oracle
selects the best of the three different transcrip-
tions per utterance. The best of the three Turk-
ers per utterance wrote the best transcription
two thirds of the time.
The utterance oracle only recovered half of
the degradation for using non-professional tran-
scription. Cutting the disagreement in half
(from 23% to 13%) reduced the WER gap by
about half (from 2.5% to 1%). Using the stan-
dard system combination algorithm ROVER
(Fiscus, 1997) to combine the three transcrip-
tions per utterance only reduced disagreement
from 23% to 21%. While previous work bene-
fited from combining multiple annotations, this
task shows little benefit.
</bodyText>
<table confidence="0.999804">
Transcription Disagreement ASR WER
with LDC
Random Utterance 23% 42.0%
Random Turker 20% 41.4%
Oracle Utterance 13% 40.9%
Oracle Turker 18% 41.1%
Contractor &lt; 5% 39.6%
LDC - 39.5%
</table>
<tableCaption confidence="0.949418375">
Table 1: Quality of Non-Professional Transcription
on 20 hours of English Switchboard. Even though
disagreement for random selection without quality
control has 23% disagreement with professional tran-
scription, an ASR system trained on the data is only
2.5% worse than using LDC transcriptions. The up-
per bound for quality control (row 3) recovers only
50% of the total loss.
</tableCaption>
<subsectionHeader confidence="0.998643">
4.2 Combining with External Sources
</subsectionHeader>
<bodyText confidence="0.999954166666667">
While in-domain speech transcription is typi-
cally the only effective way to improve the acous-
tic model, out-of-domain transcripts tend to
be useful for language models of conversational
speech. Broadcast News (BN) transcription is
particularly well suited for English Switchboard
data as the topics tend to cover news items
like terrorism or politics. We built a small
one million word language model (to simulate a
resource-poor language) and interpolated it with
varying amounts of LDC or Mechanical Turk
transcriptions. Figure 2 details the results.
</bodyText>
<subsectionHeader confidence="0.997235">
4.3 The Value of Quality Control
</subsectionHeader>
<bodyText confidence="0.999970647058823">
With a fixed transcription budget, should one
even bother with redundant transcription to im-
prove an ASR system? To find out, we tran-
scribed 40 additional hours of Switchboard us-
ing Mechanical Turk. Disagreement to the LDC
transcriptions was 24%, similar to the initial
20 hours. The two percent degradation of test
WER when using Mechanical Turk compared to
LDC held up with 40 and 60 hours of training.
Given a fixed budget of 60 hours of transcrip-
tion, we compared the quality of 20 hours tran-
scribed three times to 60 hours transcribed once.
The best we could hope to recover from the three
redundant transcriptions is the utterance oracle.
Oracle and singly transcribed data had 13% and
24% disagreement with LDC respectively. Sys-
tem performance was 40.9% with 20 hours of
</bodyText>
<page confidence="0.992904">
210
</page>
<figure confidence="0.660921">
Improving the Language Model
10K 20K 40K 80K 160K
Words of Transcription for Training (log scale)
</figure>
<figureCaption confidence="0.933123">
Figure 2: WER with a varied amount of LM training
data and a fixed 16hr acoustic model. MTurk tran-
scription degrades WER by 0.8% absolute across LM
size. When interpolated with 1M words of broadcast
news, this degradation shrinks to 0.6%.
</figureCaption>
<bodyText confidence="0.999972277777778">
the former and 37.6% with 60 hours of the latter.
Even though perfect selection cuts disagreement
in half, three times as much data helps more.
The 2004 Fisher effort averaged a price of $150
per hour of English CTS transcription. The
company CastingWords produces high quality
(Passy, 2008) English transcription for $90 an
hour using Mechanical Turk by a multi-pass pro-
cess to collect and clean Turker-provided tran-
scripts. While we did not use their service, we
assume it is of comparable quality to the pri-
vate contractor used earlier. The price for LDC
transcription is not comparable here since it was
intended for more precise linguistic tasks. Ex-
trapolating from Figure 3, the entire 2000 Fisher
corpus could be transcribed using Mechanical
Turk at the same cost of collecting 60 hours of
professional transcription.
</bodyText>
<sectionHeader confidence="0.958432" genericHeader="method">
5 Collection in Other Languages
</sectionHeader>
<bodyText confidence="0.999726666666667">
To test the feasability of improving low-resource
languages, we attempted to collect transcrip-
tions for Korean, Hindi, Tamil CTS data. We
built an LVCSR system in Korean since it is the
only one with reference LDC transcriptions to
use as a test set.
</bodyText>
<subsectionHeader confidence="0.622383">
Comparing Cost of Reducing WER
</subsectionHeader>
<bodyText confidence="0.329229">
Test WER with 20−60 hours of Switchboard Transcription
</bodyText>
<figure confidence="0.6469745">
100 200 500 1000 2000 5000 10000
Total Cost (Dollars) to Collect Data (log scale)
</figure>
<figureCaption confidence="0.999356727272727">
Figure 3: Historical cost estimates are $150 per hour
of transcription (blue cirlces). The company Casting
Words uses Turkers to transcribe English at $90 per
hour which we estimated to be high quality (green
triangles). Transcription without quality control on
Mechanical Turk (red squares) is drastically cheaper
at $5 per hour. With a fixed budget, it is better
to transcribe more data at lower quality than to im-
prove quality. Contrast the oracle WER for 20 hours
transcribed three times (red diamond) with 60 hours
transcribed once (bottom red square).
</figureCaption>
<subsectionHeader confidence="0.918302">
5.1 Korean
</subsectionHeader>
<bodyText confidence="0.999982736842105">
Korean is spoken by roughly 78 million speak-
ers world wide and is written in Hangul, a pho-
netic orthography, although Chinese characters
frequently appear in written text. Since Korean
has essentially arbitrary spacing (Chong-Woo et
al., 2001), we report Phoneme Error Rate (PER)
instead of WER, which would be unfairly pe-
nalized. Both behave similarly as system per-
formance improves. For comparison, an English
WER of 39.5% has a PER of 34.8%.
We uploaded ten hours of audio to be tran-
scribed once, again segmented into short snip-
pets. Transcription was very slow at first and
we had to pay $0.20 per HIT to attract work-
ers. We posted a separate HIT to refer Korean
transcribers, paying a 25% bonus of the income
earned by referrals. This was quite successful
as two referred Turkers contributed over 80%
of the total transcription (at a cost of $25 per
</bodyText>
<figure confidence="0.992172657142857">
(All decodes with a fixed 16 hour LDC acoustic model)
●
● 0.8% Average Degradation
●
●
●
●
1M word BN LM Initial WER
● ●
● ●
● ●
●
0.6% Average Degradation
●
●
●
●
●
Test WER
38% 40% 42% 44% 46% 48%
●
●
MTurk only
LDC Only
MTurk + 1M BN
LDC + 1M BN
Test WER
30 35 40 45
MTurk − $5/hr
Mturk w/Oracle QC − $15/hr
Casting Words − $90/hr
● Private Contractor − $150/hr
●
●
●
</figure>
<page confidence="0.997555">
211
</page>
<bodyText confidence="0.999812647058824">
hour instead of $20). We collected three hours
of transcriptions after five weeks, paying eight
Turkers $113 at a transcription rate of 10xRT.
Average Turker disagreement to the LDC
reference was 17% (computed at the charac-
ter level). Using these transcripts to train an
LUCSR system instead of those provided by
LDC degraded PER by 0.8% from 51.3% to
52.1%. For comparison, a system trained on the
entire 27 hours of LDC data had 41.2% PER.
Although performance seems poor, it is suf-
ficiently good to bootstrap with acoustic model
self-training (Lamel et al., 2002). The language
model can be improved by finding `conversa-
tional&apos; web text found with n-gram queries ex-
tracted from the three hours of transcripts (Bu-
lyko et al., 2003).
</bodyText>
<subsectionHeader confidence="0.998432">
5.2 Hindi and Tamil
</subsectionHeader>
<bodyText confidence="0.999982272727273">
As a feasability experiment, we collected one
hour of transcription in Hindi and Tamil, pay-
ing $20 per hour of transcription. Hindi and
Tamil transcription finished in eight days, per-
haps due to the high prevalence of Turkers in
India (Ipeirotis, 2008). While we did not have
any professional reference, Hindi speaking col-
leagues viewed some of the data and pointed
out errors in English transliteration, but over-
all quality appeared fine. The true test will be
to build an LUCSR system and report WER.
</bodyText>
<sectionHeader confidence="0.987474" genericHeader="method">
6 Quality Control sans Quality Data
</sectionHeader>
<bodyText confidence="0.999836307692308">
Although we have shown that redundantly tran-
scribing an entire corpus gives little gain, there
is value in some amount of quality control. We
could improve system performance by only re-
jecting Turkers with high disagreement, similar
to confidence selection for active learning or un-
supervised training (Ma and Schwartz, ). But if
we are transcribing a truly new domain, there is
no gold-standard data to use as reference, so we
must estimate disagreement against errorful ref-
erence. In this section we provide a practical use
for quality control without gold standard refer-
ence data.
</bodyText>
<figure confidence="0.43791">
Distribution of Turker Skill
0% 10% 30% 50% 70% 90%
Average Disagreement of Transcribed Utterances by Each Turker
</figure>
<figureCaption confidence="0.9983989">
Figure 4: Each Turker was judged against profes-
sional and non-professional reference and assigned
an overall disagreement. The distribution of Turker
disagreement follows a gamma distribution, with a
tight cluster of average Turkers and a long-tail of bad
Turkers. Estimating with non-professionals (even
though the reference is 23% wrong on average) is
surprisingly well matched to professional estimate.
Turker estimation over-estimated disagreement by
only 2%.
</figureCaption>
<subsectionHeader confidence="0.999733">
6.1 Estimating Turker Skill
</subsectionHeader>
<bodyText confidence="0.999763210526316">
Using the twenty hour English transcriptions
from Section 4, we computed disagreement for
each Turker against the professional transcrip-
tion for all utterances longer than four words.
Note that each utterance was transcribed by
three random turkers, so there is not one set of
utterances which were transcribed by all turk-
ers. Each Turker transcribed a different, par-
tially overlapping, subset of the data.
For a particular Turker, we estimated the dis-
agreement with other Turkers by using the two
other transcripts as reference and taking the
average. Figure 4 shows the density estimate
of Turker disagreement when calculated against
professional and non-professional transcription.
On average, the non-professional estimate was
3% off from the professional disagreement.
Given that non-professional disagreement is
a good estimate of professional disagreement
</bodyText>
<figure confidence="0.9843472">
25%
23%
Normalized Density
Estimated Against Professionals
Estimated Against Other Turkers
</figure>
<page confidence="0.856281">
212
</page>
<subsectionHeader confidence="0.66942">
Quickly Estimating Disagreement
</subsectionHeader>
<bodyText confidence="0.488913">
Number of Utterances to Estimate Non−Professional Disagreement
</bodyText>
<figureCaption confidence="0.752233857142857">
Figure 5: Boxplot of the difference of non-
professional disagreement with a fixed number of ut-
terances to professional disagreement over all utter-
ances. While error is expectedly high with one ut-
terance, 50% of the estimates are within 3% of the
truth after ten utterances and 75% of the estimates
are within 6% after fifteen utterances.
</figureCaption>
<bodyText confidence="0.999954352941176">
over all of a Turker&apos;s utterances, we wondered
how few needed to be redundantly transcribed
by other non-professionals. For each Turker,
we started by randomly selecting one utter-
ance and computed the non-professional dis-
agreement. We compared the estimate to the
true professional disagreement over all of the ut-
terances and repeatedly sample 20 times. Then
we increased the number of utterances used to
estimate non-professional disagreement until all
utterances by that Turker are selected.
Figure 5 shows a boxplot of the differences of
non-professional to professional disagreement on
all utterances. As few as fifteen utterances need
to be redundantly transcribed to accurately es-
timate three out of four Turkers within 5% of
the professional disagreement.
</bodyText>
<subsectionHeader confidence="0.999941">
6.2 Finding the Right I urkers
</subsectionHeader>
<bodyText confidence="0.9999576">
Since we can accurately predict a Turker&apos;s skill
with as few as fifteen utterances on average, we
can rank Turkers by their professional and non-
professional disagremeents. By thresholding on
disagreement, we can either select good turk-
</bodyText>
<table confidence="0.980395430769231">
Rating Turkers: Professional v. Non−Professional
Threshold at Mean Disagreement of 23.17%
Incorrect Reject +++ + + Correct Reject
12.5% + + + 25.46%+
+ + +
+ + + +
+ + + + ++
+ + +
+ + + ++ + +
+ + + + +
++ + + + + + +
+ + + + +
+ + + + + + + +
+ + + + +
+ ++ ++ + +
++ + + + + + + + +
+ + + + + + + + + +
+ + + + + + + + + + +
+ + + ++ + + +
+ + + + +
+ + + + + +
++ +
+ + +
+ + +
+ + + +
+ + +
+
+ + +
+ +
+ +
+
+
+ + + + +
+ + + + +
+
+ + + + + + + Incorrect Accept
+ + + + + 4.5%
Correct Accept + +
+ + + ++
+ + + + +
+ + + +
+ + + +
+ + +
+ + + + +
57.54% + + + +
+ + + + +
+ + + ++
+ + + ++ +
+ +
+ + +
+ +
+ + + + +
+
+ + + +
+ +
+ + +
+
++ + +
+ +
+ ++ + + +
+ +
+ +
+
0.0 0.2 0.4 0.6 0.8 1.0
Turker Disagreement against Professional
</table>
<figureCaption confidence="0.993170117647059">
Figure 6: Each Turker is a point with professional (X
axis) plotted against non-professional (Y axis) dis-
agreement. The non-professional disagreement cor-
relates surprisingly well with professional disagree-
ment even though the transcripts used as reference
are 23% wrong on average. By setting a selection
threshold, the space is divided into four quadrants.
The bottom left are correctly accepted: both non-
professional and professional disagreement are below
the threshold. The top left are incorrectly rejected:
using their transcripts would have helped, but they
don’t hurt system performance, just waste money.
The top right are correctly rejected for having high
disagreement. The bottom right are the troublesome
false positives that are included in training but actu-
ally may hurt performance. Luckily, the ratio of false
negatives to false positives is usually much larger.
</figureCaption>
<bodyText confidence="0.9997136">
ers or equivalently reject bad turkers. We can
view the ranking as a precision/recall problem to
select only the ‘good’ Turkers below the thresh-
old. Figure 6 plots each Turker where the X axis
is the professional disagreement and the Y axis
is the non-professional disagreement. Sweeping
the disagreement threshold from zero to one gen-
erates Figure 7, which reports F-score (the har-
monic mean of precision and recall). This sec-
tion suggests a concrete qualification test by first
transcribing 15-30 utterance multiple times to
create a gold standard. Using the transcription
from the best Turker as reference, approve new
Turkers with a WER less than the average WER
from the initial set.
</bodyText>
<figure confidence="0.9972283">
Maximum
Third Quartile
Median
First Quartile
Minimum
0 5 10 15 20 25 30
Difference from Professional Estimation on All Utterances
0% 5% 10% 15% 20% 25% 30%
0.0 0.2 0.4 0.6 0.8 1.0
Turker Disagreement against other Turkers
</figure>
<page confidence="0.914676">
213
</page>
<figure confidence="0.329014">
Selecting Turkers by Estimated Skill
WER Selection Threshold
</figure>
<figureCaption confidence="0.892398">
Figure 7: It is difficult to find only good Turkers since
</figureCaption>
<bodyText confidence="0.9398656">
the false positives outnumber the few good workers.
However, rejecting bad Turkers becomes very easy
once past the mean error rate of 23%. It is better to
use disagreement estimation to reject poor workers
instead of finding good workers.
</bodyText>
<sectionHeader confidence="0.985554" genericHeader="method">
7 Experience with Mechanical Turk
</sectionHeader>
<bodyText confidence="0.9999936">
We initially expected to invest most of our ef-
fort in managing Turker transcription. But the
vast majority of Turkers completed the effort in
good faith with few complaints about pay. Many
left positive comments5 despite the very difficult
task. Indeed, the author’s own disagreement on
a few dozen English utterances were 17.7% and
26.8% despite an honest effort.
Instead, we spent most of our time normaliz-
ing the transcriptions for English acoustic model
training. Every single misspelling or new word
had to be mapped to a pronunciation in order
to be used in training. We initially discarded
any utterance with an out of vocabulary word,
but after losing half of the data, we used a set
of simple heuristics to produce pronunciations.
Even though there were a few thousand of these
errors, they were all singletons and had little
effect on performance. Turkers sometimes left
comments in the transcription box such as “no
</bodyText>
<footnote confidence="0.697982666666667">
5One Turker left a comment “You don’t grow pick-
les!!&amp;quot; in regards to the misinformed speakers she was
transcribing.
</footnote>
<bodyText confidence="0.999970380952381">
audio” or “man1: man2:&amp;quot;. These errant tran-
scriptions could be detected by force aligning
the transcript with the audio and rejecting any
with low scores (Lamel et al., 2000). Extending
transcription to thousands of hours will require
robust methods to automatically deal with er-
rant transcripts and additionally run the risk of
exhausting the available pool of workers.
Finding Korean transcribers required the
most creativity. We found success in interact-
ing with the transcribers, providing feedback,
encouragement and paying bonuses for referring
other workers. Cultivating workers for a new
language is definitely a ‘hands on&apos; process.
For Hindi and Tamil, Turkers sometimes mis-
interpreted or ignored instructions and trans-
lated into English or transliterated into Roman
characters. Additionally, some linguistic knowl-
edge is required to classify phonemic categories
(like fricative or sonorant) required for acoustic
model training.
</bodyText>
<sectionHeader confidence="0.997581" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999610625">
Unlike previous work which studied the quality
of Mechanical Turk annotations alone, we judge
its value in terms of the real task: improving
system performance. Despite relatively high dis-
agreement with professional transcription, data
collected with Mechanical Turk was nearly as
effective for training speech models. Since this
degradation is so small, redundant annotation to
improve quality is not worth the cost. Resources
are better spent collecting more transcription.
In addition to English, we demonstrated similar
trends in Korean and also collected transcripts
for Hindi and Tamil. Finally, we proposed an
effective procedure to reduce costs by maintain-
ing the quality of the annotator pool without
needing high quality annotation.
</bodyText>
<sectionHeader confidence="0.999315" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998935">
This research was supported by the EuroMa-
trixPlus project funded by the European Com-
mission by the DARPA GALE program under
Contract No. HR0011-06-2-0001, and NSF un-
der grant IIS-0713448 and by BBN Technologies.
The views and findings are the authors’ alone.
</bodyText>
<figure confidence="0.961857">
0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
F−score
</figure>
<page confidence="0.996018">
214
</page>
<sectionHeader confidence="0.987741" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873">
Ivan Bulyko, Mari Ostendorf, and A. Stolcke. 2003.
Getting more mileage from web text sources for
conversational speech language modeling using
class-dependent mixtures. In HLT-NAACL.
Chris Callison-Burch. 2009. Fast, Cheap, and Cre-
ative: Evaluating Translation Quality Using Ama-
zons Mechanical Turk. EMNLP.
Seung-Shik Kang Chong-Woo, Chong woo Woo, and
Kookmin Univerity. 2001. Automatic segmen-
tation of words using syllable bigram statistics.
In 6th Natural Language Processing Pacific Rim
Symposium.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The fisher corpus: a resource for the next
generations of speech-to-text. In LREC.
Jonathan G. Fiscus. 1997. A post-processing sys-
tem to yield reduced word error rates: Recognizer
output voting error reduction (rover).
L. Gillick, J. Baker, J. Bridle, M. Hunt, Y. Ito,
S. Lowe, J. Orloff, B. Peskin, R. Roth, and F. Scat-
tone. 1993. Application of large vocabulary con-
tinuous speech recognition to topic and speaker
identification using telephone speech. In ICASSP.
Jack Godfrey, Edward Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech
corpus for research and development. In ICASSP.
Panos Ipeirotis. 2008. Mechanical turk: The de-
mographics. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.html.
Owen Kimball, Chai-Lin Kao, Teodoro Arvizo, John
Makhoul, and Rukmini Iyer. 2004. Quick tran-
scription and automatic segmentation of the fisher
conversational telephone speech corpus. In RT04
Workshop.
Lori Lamel, Jean luc Gauvain, and Gilles Adda.
2000. Lightly supervised acoustic model training.
In ISCA ITRW ASR2000.
Lori Lamel, Jean luc Gauvain, and Gilles Adda.
2002. Lightly supervised and unsupervised acous-
tic model training. Computer Speech and Lan-
guage, 16(1).
Jeff Ma and Rich Schwartz. Unsupervised versus su-
pervised training of acoustic models. In INTER-
SPEECH.
Matthew Marge, Satanjeev Banerjee, and Alexan-
der Rudnicky. 2010. Using the amazon me-
chanical turk for transcription of spoken language.
ICASSP, March.
Ian McGraw, Alexander Gruenstein, and Andrew
Sutherland. 2009. A self-labeling speech corpus:
Collecting spoken words with an online educa-
tional game. In INTERSPEECH.
D. Miller, M. Kleber, C. Kao, O. Kimball,
T. Colthurst, S.A. Lowe, R.M. Schwartz, and
H. Gish. 2007. Rapid and Accurate Spoken Term
Detection. In INTERSPEECH.
Charles Passy. 2008. Turning audio into words on
the screen. http://online.wsj.com/article/
SB122351860225518093.html.
R. Prasad, S. Matsoukas, CL Kao, J.Z. Ma, DX Xu,
T. Colthurst, O. Kimball, R. Schwartz, J.L. Gau-
vain, L. Lamel, et al. 2005. The 2004 BBN/LIMSI
20xRT English conversational telephone speech
recognition system. In INTERSPEECH.
Brandon Roy and Deb Roy. 2009. Fast transcrip-
tion of unstructured audio recordings. In INTER-
SPEECH.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is
it good?: evaluating non-expert annotations for
natural language tasks. In EMNLP.
Steve. Young, Jost. Schatzmann, Karl. Weilhammer,
and Hui. Ye. 2007. The hidden information state
approach to dialog management. In ICASSP.
Omar F. Zaidan and Chris Callison-Burch. 2009.
Feasibility of human-in-the-loop minimum error
rate training. In EMNLP.
</reference>
<page confidence="0.999146">
215
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.618644">
<title confidence="0.84054">Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription Novotney Center for Language and Speech</title>
<author confidence="0.724254">Johns Hopkins</author>
<email confidence="0.999461">snovotne@bbn.comccb@jhu.edu</email>
<abstract confidence="0.99963875">Deploying an automatic speech recognition system with reasonable performance requires expensive and time-consuming in-domain transcription. Previous work demonstrated that non-professional annotation through Amazon’s Mechanical Turk can match professional quality. We use Mechanical Turk to transcribe conversational speech for as little as one thirtieth the cost of professional transcription. The higher disagreement of nonprofessional transcribers does not have a significant effect on system performance. While previous work demonstrated that redundant transcription can improve data quality, we found that resources are better spent collecting more data. Finally, we describe a quality control method without needing professional transcription.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ivan Bulyko</author>
<author>Mari Ostendorf</author>
<author>A Stolcke</author>
</authors>
<title>Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="18962" citStr="Bulyko et al., 2003" startWordPosition="2995" endWordPosition="2999"> transcription rate of 10xRT. Average Turker disagreement to the LDC reference was 17% (computed at the character level). Using these transcripts to train an LUCSR system instead of those provided by LDC degraded PER by 0.8% from 51.3% to 52.1%. For comparison, a system trained on the entire 27 hours of LDC data had 41.2% PER. Although performance seems poor, it is sufficiently good to bootstrap with acoustic model self-training (Lamel et al., 2002). The language model can be improved by finding `conversational&apos; web text found with n-gram queries extracted from the three hours of transcripts (Bulyko et al., 2003). 5.2 Hindi and Tamil As a feasability experiment, we collected one hour of transcription in Hindi and Tamil, paying $20 per hour of transcription. Hindi and Tamil transcription finished in eight days, perhaps due to the high prevalence of Turkers in India (Ipeirotis, 2008). While we did not have any professional reference, Hindi speaking colleagues viewed some of the data and pointed out errors in English transliteration, but overall quality appeared fine. The true test will be to build an LUCSR system and report WER. 6 Quality Control sans Quality Data Although we have shown that redundantly</context>
</contexts>
<marker>Bulyko, Ostendorf, Stolcke, 2003</marker>
<rawString>Ivan Bulyko, Mari Ostendorf, and A. Stolcke. 2003. Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazons Mechanical Turk.</title>
<date>2009</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="1709" citStr="Callison-Burch, 2009" startWordPosition="234" endWordPosition="235">aining on 2000+ hours of transcribed conversational speech, over a billion words of language modeling text, and hand-crafted pronunciation dictionaries, state of the art systems still have an error rate of around 15% for English (Prasad et al., 2005) Transcribing the large volumes of data required for Large Vocabulary Continuous Speech Recognition (LVCSR) of new languages appears prohibitively expensive. Recent work has shown that Amazon’s Mechanical Turk&apos; can 1http://www.mturk.com be used to cheaply create data for other natural language processing applications (Snow et al., 2008; Zaidan and Callison-Burch, 2009; McGraw et al., 2009). In this paper we focus on reducing the cost of transcribing conversational telephone speech (CTS) data. Previous measurements of Mechanical Turk stopped at agreement/disagreement with professional annotation. We take the next logical step and measure performance on systems trained with non-professional transcription. Mechanical Turk is an online labor market where workers (or Turkers) perform simple tasks called Human Intelligence Tasks (HITs) for small amounts of money – frequently as little as $0.01 per HIT. Since HITs can be tasks that are difficult for computers, bu</context>
<context position="4457" citStr="Callison-Burch (2009)" startWordPosition="651" endWordPosition="652">etter spent collecting more data than improving data quality. • Transcriber skill can be accurately estimated without gold standard data. 2 Related Work Research into Mechanical Turk by the NLP community has largely focused on comparing the quality of annotations produced by non-expert Turkers against annotations created by experts. Snow et al. (2008) conducted a comprehensive study across a variety of NLP tasks. They showed that high agreement could be reached with gold-standard expert annotation for these tasks through a weighted combination of ten redundant annotations produced by Turkers. Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of verbal instructions to robots with clean speech. By using five duplicate transcriptions, the average transcription disagreement with experts was reduc</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazons Mechanical Turk. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seung-Shik Kang Chong-Woo</author>
<author>Chong woo Woo</author>
<author>Kookmin Univerity</author>
</authors>
<title>Automatic segmentation of words using syllable bigram statistics.</title>
<date>2001</date>
<booktitle>In 6th Natural Language Processing Pacific Rim Symposium.</booktitle>
<contexts>
<context position="17242" citStr="Chong-Woo et al., 2001" startWordPosition="2673" endWordPosition="2676">high quality (green triangles). Transcription without quality control on Mechanical Turk (red squares) is drastically cheaper at $5 per hour. With a fixed budget, it is better to transcribe more data at lower quality than to improve quality. Contrast the oracle WER for 20 hours transcribed three times (red diamond) with 60 hours transcribed once (bottom red square). 5.1 Korean Korean is spoken by roughly 78 million speakers world wide and is written in Hangul, a phonetic orthography, although Chinese characters frequently appear in written text. Since Korean has essentially arbitrary spacing (Chong-Woo et al., 2001), we report Phoneme Error Rate (PER) instead of WER, which would be unfairly penalized. Both behave similarly as system performance improves. For comparison, an English WER of 39.5% has a PER of 34.8%. We uploaded ten hours of audio to be transcribed once, again segmented into short snippets. Transcription was very slow at first and we had to pay $0.20 per HIT to attract workers. We posted a separate HIT to refer Korean transcribers, paying a 25% bonus of the income earned by referrals. This was quite successful as two referred Turkers contributed over 80% of the total transcription (at a cost</context>
</contexts>
<marker>Chong-Woo, Woo, Univerity, 2001</marker>
<rawString>Seung-Shik Kang Chong-Woo, Chong woo Woo, and Kookmin Univerity. 2001. Automatic segmentation of words using syllable bigram statistics. In 6th Natural Language Processing Pacific Rim Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>The fisher corpus: a resource for the next generations of speech-to-text.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="5182" citStr="Cieri et al., 2004" startWordPosition="763" endWordPosition="766">sh complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of verbal instructions to robots with clean speech. By using five duplicate transcriptions, the average transcription disagreement with experts was reduced from 4% to 2%. Previous efforts at reducing the cost of transcription include the EARS Fisher project (Cieri et al., 2004), which collected 2000+ hours of English CTS data – an order of magnitude more than had previously been transcribed. To speed transcription and lower costs, Kimball et al. (2004) created new transcription guidelines and used automatic segmentation. These improved the speed of transcription from fifty times real time to six times real time, and made it cost effective to transcribe 2000 hours at an average of $150 per hour. Models trained on the faster transcripts exhibited almost no degradation in performance, although discrimanitve training was sensitive to transcription errrors. 3 Experiment </context>
</contexts>
<marker>Cieri, Miller, Walker, 2004</marker>
<rawString>Christopher Cieri, David Miller, and Kevin Walker. 2004. The fisher corpus: a resource for the next generations of speech-to-text. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover).</title>
<date>1997</date>
<contexts>
<context position="12664" citStr="Fiscus, 1997" startWordPosition="1931" endWordPosition="1932">ances against the professionals and selects utterances by Turker until the twenty hours is covered (Section 4.3 discusses a fair way to rank Turkers). The utterance oracle selects the best of the three different transcriptions per utterance. The best of the three Turkers per utterance wrote the best transcription two thirds of the time. The utterance oracle only recovered half of the degradation for using non-professional transcription. Cutting the disagreement in half (from 23% to 13%) reduced the WER gap by about half (from 2.5% to 1%). Using the standard system combination algorithm ROVER (Fiscus, 1997) to combine the three transcriptions per utterance only reduced disagreement from 23% to 21%. While previous work benefited from combining multiple annotations, this task shows little benefit. Transcription Disagreement ASR WER with LDC Random Utterance 23% 42.0% Random Turker 20% 41.4% Oracle Utterance 13% 40.9% Oracle Turker 18% 41.1% Contractor &lt; 5% 39.6% LDC - 39.5% Table 1: Quality of Non-Professional Transcription on 20 hours of English Switchboard. Even though disagreement for random selection without quality control has 23% disagreement with professional transcription, an ASR system tr</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gillick</author>
<author>J Baker</author>
<author>J Bridle</author>
<author>M Hunt</author>
<author>Y Ito</author>
<author>S Lowe</author>
<author>J Orloff</author>
<author>B Peskin</author>
<author>R Roth</author>
<author>F Scattone</author>
</authors>
<title>Application of large vocabulary continuous speech recognition to topic and speaker identification using telephone speech.</title>
<date>1993</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="3344" citStr="Gillick et al., 1993" startWordPosition="480" endWordPosition="483">ranscription. Even a few hours of transcription is sufficient to bootstrap with unsupervised methods like self-training (Lamel et al., 2002). The speech community has built effective downstream solutions for the past twenty years despite imperfect recognition. In topic classification, 90% accuracy is possible on conversational data even with 80%+ word error rate 2http://castingwords.com/ 207 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 207–215, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics (WER) (Gillick et al., 1993). Other successful tasks include information retrieval from speech (Miller et al., 2007) and spoken dialogue processing (Young et al., 2007). Inexpensive transcription would quickly open new languages or domains (like meeting or lecture data) for automatic speech recognition. In this paper, we make the following points: • Quality control isn’t necessary as a system built with non-professional transcription is only 6% worse for 130 the cost of professional transcription. • Resources are better spent collecting more data than improving data quality. • Transcriber skill can be accurately estimate</context>
</contexts>
<marker>Gillick, Baker, Bridle, Hunt, Ito, Lowe, Orloff, Peskin, Roth, Scattone, 1993</marker>
<rawString>L. Gillick, J. Baker, J. Bridle, M. Hunt, Y. Ito, S. Lowe, J. Orloff, B. Peskin, R. Roth, and F. Scattone. 1993. Application of large vocabulary continuous speech recognition to topic and speaker identification using telephone speech. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Godfrey</author>
<author>Edward Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="5916" citStr="Godfrey et al., 1992" startWordPosition="878" endWordPosition="881">ed. To speed transcription and lower costs, Kimball et al. (2004) created new transcription guidelines and used automatic segmentation. These improved the speed of transcription from fifty times real time to six times real time, and made it cost effective to transcribe 2000 hours at an average of $150 per hour. Models trained on the faster transcripts exhibited almost no degradation in performance, although discrimanitve training was sensitive to transcription errrors. 3 Experiment Description 3.1 Corpora We conducted most experiments on a twenty hour subset of the English Switchboard corpus (Godfrey et al., 1992) where two strangers converse about an assigned topic. We used two sets of transcription as our gold standard: high quality transcription from the LDC and those following the Fisher quick transcription guidelines (Kimball et al., 2004) provided by a professional transcription company. All English ASR models were tested with the carefully transcribed three hour Dev04 test set from the NIST HUB5 evaluation.3 A 75k word lexicon taken from the EARS Fisher training corpus covers the LDC training data and has a test OOV rate of 0.18%. We also conducted experiments in Korean and collected Hindi and T</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>Jack Godfrey, Edward Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panos Ipeirotis</author>
</authors>
<title>Mechanical turk: The demographics. http://behind-the-enemy-lines.</title>
<date>2008</date>
<note>blogspot.com/2010/03/ new-demographics-of-mechanical-turk.html.</note>
<contexts>
<context position="19236" citStr="Ipeirotis, 2008" startWordPosition="3044" endWordPosition="3045"> on the entire 27 hours of LDC data had 41.2% PER. Although performance seems poor, it is sufficiently good to bootstrap with acoustic model self-training (Lamel et al., 2002). The language model can be improved by finding `conversational&apos; web text found with n-gram queries extracted from the three hours of transcripts (Bulyko et al., 2003). 5.2 Hindi and Tamil As a feasability experiment, we collected one hour of transcription in Hindi and Tamil, paying $20 per hour of transcription. Hindi and Tamil transcription finished in eight days, perhaps due to the high prevalence of Turkers in India (Ipeirotis, 2008). While we did not have any professional reference, Hindi speaking colleagues viewed some of the data and pointed out errors in English transliteration, but overall quality appeared fine. The true test will be to build an LUCSR system and report WER. 6 Quality Control sans Quality Data Although we have shown that redundantly transcribing an entire corpus gives little gain, there is value in some amount of quality control. We could improve system performance by only rejecting Turkers with high disagreement, similar to confidence selection for active learning or unsupervised training (Ma and Sch</context>
</contexts>
<marker>Ipeirotis, 2008</marker>
<rawString>Panos Ipeirotis. 2008. Mechanical turk: The demographics. http://behind-the-enemy-lines. blogspot.com/2010/03/ new-demographics-of-mechanical-turk.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Kimball</author>
<author>Chai-Lin Kao</author>
<author>Teodoro Arvizo</author>
<author>John Makhoul</author>
<author>Rukmini Iyer</author>
</authors>
<title>Quick transcription and automatic segmentation of the fisher conversational telephone speech corpus.</title>
<date>2004</date>
<booktitle>In RT04 Workshop.</booktitle>
<contexts>
<context position="5360" citStr="Kimball et al. (2004)" startWordPosition="793" endWordPosition="796">by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of verbal instructions to robots with clean speech. By using five duplicate transcriptions, the average transcription disagreement with experts was reduced from 4% to 2%. Previous efforts at reducing the cost of transcription include the EARS Fisher project (Cieri et al., 2004), which collected 2000+ hours of English CTS data – an order of magnitude more than had previously been transcribed. To speed transcription and lower costs, Kimball et al. (2004) created new transcription guidelines and used automatic segmentation. These improved the speed of transcription from fifty times real time to six times real time, and made it cost effective to transcribe 2000 hours at an average of $150 per hour. Models trained on the faster transcripts exhibited almost no degradation in performance, although discrimanitve training was sensitive to transcription errrors. 3 Experiment Description 3.1 Corpora We conducted most experiments on a twenty hour subset of the English Switchboard corpus (Godfrey et al., 1992) where two strangers converse about an assig</context>
</contexts>
<marker>Kimball, Kao, Arvizo, Makhoul, Iyer, 2004</marker>
<rawString>Owen Kimball, Chai-Lin Kao, Teodoro Arvizo, John Makhoul, and Rukmini Iyer. 2004. Quick transcription and automatic segmentation of the fisher conversational telephone speech corpus. In RT04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Lamel</author>
<author>Jean luc Gauvain</author>
<author>Gilles Adda</author>
</authors>
<title>Lightly supervised acoustic model training.</title>
<date>2000</date>
<booktitle>In ISCA ITRW ASR2000.</booktitle>
<contexts>
<context position="27276" citStr="Lamel et al., 2000" startWordPosition="4462" endWordPosition="4465"> any utterance with an out of vocabulary word, but after losing half of the data, we used a set of simple heuristics to produce pronunciations. Even though there were a few thousand of these errors, they were all singletons and had little effect on performance. Turkers sometimes left comments in the transcription box such as “no 5One Turker left a comment “You don’t grow pickles!!&amp;quot; in regards to the misinformed speakers she was transcribing. audio” or “man1: man2:&amp;quot;. These errant transcriptions could be detected by force aligning the transcript with the audio and rejecting any with low scores (Lamel et al., 2000). Extending transcription to thousands of hours will require robust methods to automatically deal with errant transcripts and additionally run the risk of exhausting the available pool of workers. Finding Korean transcribers required the most creativity. We found success in interacting with the transcribers, providing feedback, encouragement and paying bonuses for referring other workers. Cultivating workers for a new language is definitely a ‘hands on&apos; process. For Hindi and Tamil, Turkers sometimes misinterpreted or ignored instructions and translated into English or transliterated into Roma</context>
</contexts>
<marker>Lamel, Gauvain, Adda, 2000</marker>
<rawString>Lori Lamel, Jean luc Gauvain, and Gilles Adda. 2000. Lightly supervised acoustic model training. In ISCA ITRW ASR2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori Lamel</author>
<author>Jean luc Gauvain</author>
<author>Gilles Adda</author>
</authors>
<title>Lightly supervised and unsupervised acoustic model training.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2863" citStr="Lamel et al., 2002" startWordPosition="410" endWordPosition="413">HIT. Since HITs can be tasks that are difficult for computers, but easy for humans, they are ideal for natural language processing tasks (Snow et al., 2008). Mechanical Turk has even spawned a business that specializes in manual speech transcription.2 Automatic speech recognition (ASR) of conversational speech is an extremely difficult problem. Characteristics like rapid speech, phonetic reductions and speaking style limit the value of non-CTS data, necessitating in-domain transcription. Even a few hours of transcription is sufficient to bootstrap with unsupervised methods like self-training (Lamel et al., 2002). The speech community has built effective downstream solutions for the past twenty years despite imperfect recognition. In topic classification, 90% accuracy is possible on conversational data even with 80%+ word error rate 2http://castingwords.com/ 207 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 207–215, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics (WER) (Gillick et al., 1993). Other successful tasks include information retrieval from speech (Miller et al., 2007) and spoken dialogue processing</context>
<context position="18795" citStr="Lamel et al., 2002" startWordPosition="2967" endWordPosition="2970">ds − $90/hr ● Private Contractor − $150/hr ● ● ● 211 hour instead of $20). We collected three hours of transcriptions after five weeks, paying eight Turkers $113 at a transcription rate of 10xRT. Average Turker disagreement to the LDC reference was 17% (computed at the character level). Using these transcripts to train an LUCSR system instead of those provided by LDC degraded PER by 0.8% from 51.3% to 52.1%. For comparison, a system trained on the entire 27 hours of LDC data had 41.2% PER. Although performance seems poor, it is sufficiently good to bootstrap with acoustic model self-training (Lamel et al., 2002). The language model can be improved by finding `conversational&apos; web text found with n-gram queries extracted from the three hours of transcripts (Bulyko et al., 2003). 5.2 Hindi and Tamil As a feasability experiment, we collected one hour of transcription in Hindi and Tamil, paying $20 per hour of transcription. Hindi and Tamil transcription finished in eight days, perhaps due to the high prevalence of Turkers in India (Ipeirotis, 2008). While we did not have any professional reference, Hindi speaking colleagues viewed some of the data and pointed out errors in English transliteration, but ov</context>
</contexts>
<marker>Lamel, Gauvain, Adda, 2002</marker>
<rawString>Lori Lamel, Jean luc Gauvain, and Gilles Adda. 2002. Lightly supervised and unsupervised acoustic model training. Computer Speech and Language, 16(1).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jeff Ma</author>
<author>Rich Schwartz</author>
</authors>
<title>Unsupervised versus supervised training of acoustic models.</title>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Ma, Schwartz, </marker>
<rawString>Jeff Ma and Rich Schwartz. Unsupervised versus supervised training of acoustic models. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Satanjeev Banerjee</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Using the amazon mechanical turk for transcription of spoken language.</title>
<date>2010</date>
<publisher>ICASSP,</publisher>
<contexts>
<context position="4878" citStr="Marge et al. (2010)" startWordPosition="717" endWordPosition="720">howed that high agreement could be reached with gold-standard expert annotation for these tasks through a weighted combination of ten redundant annotations produced by Turkers. Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of verbal instructions to robots with clean speech. By using five duplicate transcriptions, the average transcription disagreement with experts was reduced from 4% to 2%. Previous efforts at reducing the cost of transcription include the EARS Fisher project (Cieri et al., 2004), which collected 2000+ hours of English CTS data – an order of magnitude more than had previously been transcribed. To speed transcription and lower costs, Kimball et al. (2004) created new transcription guidelines and used automatic segmentation. These improved the speed of transcription from </context>
</contexts>
<marker>Marge, Banerjee, Rudnicky, 2010</marker>
<rawString>Matthew Marge, Satanjeev Banerjee, and Alexander Rudnicky. 2010. Using the amazon mechanical turk for transcription of spoken language. ICASSP, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian McGraw</author>
<author>Alexander Gruenstein</author>
<author>Andrew Sutherland</author>
</authors>
<title>A self-labeling speech corpus: Collecting spoken words with an online educational game.</title>
<date>2009</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="1731" citStr="McGraw et al., 2009" startWordPosition="236" endWordPosition="240">of transcribed conversational speech, over a billion words of language modeling text, and hand-crafted pronunciation dictionaries, state of the art systems still have an error rate of around 15% for English (Prasad et al., 2005) Transcribing the large volumes of data required for Large Vocabulary Continuous Speech Recognition (LVCSR) of new languages appears prohibitively expensive. Recent work has shown that Amazon’s Mechanical Turk&apos; can 1http://www.mturk.com be used to cheaply create data for other natural language processing applications (Snow et al., 2008; Zaidan and Callison-Burch, 2009; McGraw et al., 2009). In this paper we focus on reducing the cost of transcribing conversational telephone speech (CTS) data. Previous measurements of Mechanical Turk stopped at agreement/disagreement with professional annotation. We take the next logical step and measure performance on systems trained with non-professional transcription. Mechanical Turk is an online labor market where workers (or Turkers) perform simple tasks called Human Intelligence Tasks (HITs) for small amounts of money – frequently as little as $0.01 per HIT. Since HITs can be tasks that are difficult for computers, but easy for humans, the</context>
<context position="4663" citStr="McGraw et al. (2009)" startWordPosition="677" endWordPosition="680"> largely focused on comparing the quality of annotations produced by non-expert Turkers against annotations created by experts. Snow et al. (2008) conducted a comprehensive study across a variety of NLP tasks. They showed that high agreement could be reached with gold-standard expert annotation for these tasks through a weighted combination of ten redundant annotations produced by Turkers. Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. McGraw et al. (2009) used Mechanical Turk to improve an English isolated word speech recognizer by having Turkers listen to a word and select from a list of probable words at a cost of $20 per hour of transcription. Marge et al. (2010) collected transcriptions of verbal instructions to robots with clean speech. By using five duplicate transcriptions, the average transcription disagreement with experts was reduced from 4% to 2%. Previous efforts at reducing the cost of transcription include the EARS Fisher project (Cieri et al., 2004), which collected 2000+ hours of English CTS data – an order of magnitude more th</context>
</contexts>
<marker>McGraw, Gruenstein, Sutherland, 2009</marker>
<rawString>Ian McGraw, Alexander Gruenstein, and Andrew Sutherland. 2009. A self-labeling speech corpus: Collecting spoken words with an online educational game. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Miller</author>
<author>M Kleber</author>
<author>C Kao</author>
<author>O Kimball</author>
<author>T Colthurst</author>
<author>S A Lowe</author>
<author>R M Schwartz</author>
<author>H Gish</author>
</authors>
<title>Rapid and Accurate Spoken Term Detection.</title>
<date>2007</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="3432" citStr="Miller et al., 2007" startWordPosition="492" endWordPosition="495">sed methods like self-training (Lamel et al., 2002). The speech community has built effective downstream solutions for the past twenty years despite imperfect recognition. In topic classification, 90% accuracy is possible on conversational data even with 80%+ word error rate 2http://castingwords.com/ 207 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 207–215, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics (WER) (Gillick et al., 1993). Other successful tasks include information retrieval from speech (Miller et al., 2007) and spoken dialogue processing (Young et al., 2007). Inexpensive transcription would quickly open new languages or domains (like meeting or lecture data) for automatic speech recognition. In this paper, we make the following points: • Quality control isn’t necessary as a system built with non-professional transcription is only 6% worse for 130 the cost of professional transcription. • Resources are better spent collecting more data than improving data quality. • Transcriber skill can be accurately estimated without gold standard data. 2 Related Work Research into Mechanical Turk by the NLP co</context>
</contexts>
<marker>Miller, Kleber, Kao, Kimball, Colthurst, Lowe, Schwartz, Gish, 2007</marker>
<rawString>D. Miller, M. Kleber, C. Kao, O. Kimball, T. Colthurst, S.A. Lowe, R.M. Schwartz, and H. Gish. 2007. Rapid and Accurate Spoken Term Detection. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Passy</author>
</authors>
<title>Turning audio into words on the screen.</title>
<date>2008</date>
<note>http://online.wsj.com/article/ SB122351860225518093.html.</note>
<contexts>
<context position="15445" citStr="Passy, 2008" startWordPosition="2382" endWordPosition="2383">nguage Model 10K 20K 40K 80K 160K Words of Transcription for Training (log scale) Figure 2: WER with a varied amount of LM training data and a fixed 16hr acoustic model. MTurk transcription degrades WER by 0.8% absolute across LM size. When interpolated with 1M words of broadcast news, this degradation shrinks to 0.6%. the former and 37.6% with 60 hours of the latter. Even though perfect selection cuts disagreement in half, three times as much data helps more. The 2004 Fisher effort averaged a price of $150 per hour of English CTS transcription. The company CastingWords produces high quality (Passy, 2008) English transcription for $90 an hour using Mechanical Turk by a multi-pass process to collect and clean Turker-provided transcripts. While we did not use their service, we assume it is of comparable quality to the private contractor used earlier. The price for LDC transcription is not comparable here since it was intended for more precise linguistic tasks. Extrapolating from Figure 3, the entire 2000 Fisher corpus could be transcribed using Mechanical Turk at the same cost of collecting 60 hours of professional transcription. 5 Collection in Other Languages To test the feasability of improvi</context>
</contexts>
<marker>Passy, 2008</marker>
<rawString>Charles Passy. 2008. Turning audio into words on the screen. http://online.wsj.com/article/ SB122351860225518093.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>S Matsoukas</author>
<author>CL Kao</author>
<author>J Z Ma</author>
<author>DX Xu</author>
<author>T Colthurst</author>
<author>O Kimball</author>
<author>R Schwartz</author>
<author>J L Gauvain</author>
<author>L Lamel</author>
</authors>
<title>BBN/LIMSI 20xRT English conversational telephone speech recognition system.</title>
<date>2005</date>
<booktitle>In INTERSPEECH.</booktitle>
<publisher>The</publisher>
<contexts>
<context position="1339" citStr="Prasad et al., 2005" startWordPosition="181" endWordPosition="184">on system performance. While previous work demonstrated that redundant transcription can improve data quality, we found that resources are better spent collecting more data. Finally, we describe a quality control method without needing professional transcription. 1 Introduction Successful speech recognition depends on huge investments in data collection. Even after training on 2000+ hours of transcribed conversational speech, over a billion words of language modeling text, and hand-crafted pronunciation dictionaries, state of the art systems still have an error rate of around 15% for English (Prasad et al., 2005) Transcribing the large volumes of data required for Large Vocabulary Continuous Speech Recognition (LVCSR) of new languages appears prohibitively expensive. Recent work has shown that Amazon’s Mechanical Turk&apos; can 1http://www.mturk.com be used to cheaply create data for other natural language processing applications (Snow et al., 2008; Zaidan and Callison-Burch, 2009; McGraw et al., 2009). In this paper we focus on reducing the cost of transcribing conversational telephone speech (CTS) data. Previous measurements of Mechanical Turk stopped at agreement/disagreement with professional annotatio</context>
<context position="7047" citStr="Prasad et al., 2005" startWordPosition="1058" endWordPosition="1061">test OOV rate of 0.18%. We also conducted experiments in Korean and collected Hindi and Tamil data from the Callfriend corpora 4. Participants were given a free long distance phone call to talk with friends or family in their native language, although English frequently appears. Since Callfriend was originally intended for language identification, only the 27 hour Korean portion has been transcribed by the LDC. 3.2 LVCSR System We used Byblos, a state-of-the-art multi-pass LVCSR system with state-clustered Gaussian tied-mixture acoustic models and modified Kneser-Ney smoothed language models (Prasad et al., 2005). While understanding the system 3http://www.itl.nist.gov/iad/mig/tests/ctr/ 1998/current-plan.html 4http://www.ldc.upenn.edu/CallFriend2/ 208 details is not essential for this work, we provide a brief description for completeness. Recognition begins with cepstral feature extraction using concatenated frames with cepstral mean subtraction and HLDA to reduce the feature dimension space. Vocal track length normalization follows. Decoding then requires three passes: a fast forward pass with coarse onegaussian-per-phone models and bigram LM followed by a backward pass with triphone models and a tr</context>
</contexts>
<marker>Prasad, Matsoukas, Kao, Ma, Xu, Colthurst, Kimball, Schwartz, Gauvain, Lamel, 2005</marker>
<rawString>R. Prasad, S. Matsoukas, CL Kao, J.Z. Ma, DX Xu, T. Colthurst, O. Kimball, R. Schwartz, J.L. Gauvain, L. Lamel, et al. 2005. The 2004 BBN/LIMSI 20xRT English conversational telephone speech recognition system. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brandon Roy</author>
<author>Deb Roy</author>
</authors>
<title>Fast transcription of unstructured audio recordings.</title>
<date>2009</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="8173" citStr="Roy and Roy, 2009" startWordPosition="1215" endWordPosition="1218">ian-per-phone models and bigram LM followed by a backward pass with triphone models and a trigram LM to generate word confusion lattices. The lattices are rescored with a more powerful quinphone cross-word acoustic model and trigram LM to extract the one best output. These three steps are repeated after unsupervised speaker adaptation with constrained MLLR. Decoding is around ten times real time. 3.3 Transcription Task Using language-independent speaker activity detection models, we segmented each ten minute conversation into five second utterances, greatly simplifying the transcription task (Roy and Roy, 2009). Utterances were assigned in batches of ten per HIT and played with a simple flash player with a text box for entry. All non-empty HITs were approved and we did not award bonuses except as described in Section 5.1. 3.4 Measuring Annotation Quality The usefullness of the transcribed data is ultimately measured by how much it benefits a speech recognition system. Factors that inflate disagreement (word error rate) between Turkers and professionals do not necessarily impact system performance. These include typographical mistakes, transcription inconsistencies (like improperly marking hesitation</context>
</contexts>
<marker>Roy, Roy, 2009</marker>
<rawString>Brandon Roy and Deb Roy. 2009. Fast transcription of unstructured audio recordings. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann Young</author>
<author>Karl Weilhammer</author>
<author>Hui Ye</author>
</authors>
<title>The hidden information state approach to dialog management.</title>
<date>2007</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="3484" citStr="Young et al., 2007" startWordPosition="501" endWordPosition="504">The speech community has built effective downstream solutions for the past twenty years despite imperfect recognition. In topic classification, 90% accuracy is possible on conversational data even with 80%+ word error rate 2http://castingwords.com/ 207 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 207–215, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics (WER) (Gillick et al., 1993). Other successful tasks include information retrieval from speech (Miller et al., 2007) and spoken dialogue processing (Young et al., 2007). Inexpensive transcription would quickly open new languages or domains (like meeting or lecture data) for automatic speech recognition. In this paper, we make the following points: • Quality control isn’t necessary as a system built with non-professional transcription is only 6% worse for 130 the cost of professional transcription. • Resources are better spent collecting more data than improving data quality. • Transcriber skill can be accurately estimated without gold standard data. 2 Related Work Research into Mechanical Turk by the NLP community has largely focused on comparing the quality</context>
</contexts>
<marker>Young, Weilhammer, Ye, 2007</marker>
<rawString>Steve. Young, Jost. Schatzmann, Karl. Weilhammer, and Hui. Ye. 2007. The hidden information state approach to dialog management. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Feasibility of human-in-the-loop minimum error rate training.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1709" citStr="Zaidan and Callison-Burch, 2009" startWordPosition="232" endWordPosition="235">en after training on 2000+ hours of transcribed conversational speech, over a billion words of language modeling text, and hand-crafted pronunciation dictionaries, state of the art systems still have an error rate of around 15% for English (Prasad et al., 2005) Transcribing the large volumes of data required for Large Vocabulary Continuous Speech Recognition (LVCSR) of new languages appears prohibitively expensive. Recent work has shown that Amazon’s Mechanical Turk&apos; can 1http://www.mturk.com be used to cheaply create data for other natural language processing applications (Snow et al., 2008; Zaidan and Callison-Burch, 2009; McGraw et al., 2009). In this paper we focus on reducing the cost of transcribing conversational telephone speech (CTS) data. Previous measurements of Mechanical Turk stopped at agreement/disagreement with professional annotation. We take the next logical step and measure performance on systems trained with non-professional transcription. Mechanical Turk is an online labor market where workers (or Turkers) perform simple tasks called Human Intelligence Tasks (HITs) for small amounts of money – frequently as little as $0.01 per HIT. Since HITs can be tasks that are difficult for computers, bu</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2009</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2009. Feasibility of human-in-the-loop minimum error rate training. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>