<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<note confidence="0.8304115">
The Impact of Spelling Errors on Patent Search
Benno Stein and Dennis Hoppe and Tim Gollub
</note>
<address confidence="0.719502">
Bauhaus-Universität Weimar
99421 Weimar, Germany
</address>
<email confidence="0.690604">
&lt;first name&gt;.&lt;last name&gt;@uni-weimar.de
</email>
<sectionHeader confidence="0.995663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999667961538462">
The search in patent databases is a risky
business compared to the search in other
domains. A single document that is relevant
but overlooked during a patent search can
turn into an expensive proposition. While
recent research engages in specialized mod-
els and algorithms to improve the effective-
ness of patent retrieval, we bring another
aspect into focus: the detection and ex-
ploitation of patent inconsistencies. In par-
ticular, we analyze spelling errors in the as-
signee field of patents granted by the United
States Patent &amp; Trademark Office. We in-
troduce technology in order to improve re-
trieval effectiveness despite the presence of
typographical ambiguities. In this regard,
we (1) quantify spelling errors in terms of
edit distance and phonological dissimilarity
and (2) render error detection as a learn-
ing problem that combines word dissimi-
larities with patent meta-features. For the
task of finding all patents of a company,
our approach improves recall from 96.7%
(when using a state-of-the-art patent search
engine) to 99.5%, while precision is com-
promised by only 3.7%.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965914893617">
Patent search forms the heart of most retrieval
tasks in the intellectual property domain—cf. Ta-
ble 1, which provides an overview of various user
groups along with their typical (•) and related (o)
tasks. The due diligence task, for example, is
concerned with legal issues that arise while inves-
tigating another company. Part of an investiga-
tion is a patent portfolio comparison between one
or more competitors (Lupu et al., 2011). Within
all tasks recall is preferred over precision, a fact
which distinguishes patent search from general
web search. This retrieval constraint has produced
a variety of sophisticated approaches tailored to
the patent domain: citation analysis (Magdy and
Jones, 2010), the learning of section-specific re-
trieval models (Lopez and Romary, 2010), and au-
tomated query generation (Xue and Croft, 2009).
Each approach improves retrieval performance,
but what keeps them from attaining maximum ef-
fectiveness in terms of recall are the inconsisten-
cies found in patents: incomplete citation sets, in-
correctly assigned classification codes, and, not
least, spelling errors.
Our paper deals with spelling errors in an oblig-
atory and important field of each patent, namely,
the patent assignee name. Bibliographic fields are
widely used among professional patent searchers
in order to constrain keyword-based search ses-
sions (Joho et al., 2010). The assignee name is
particularly helpful for patentability searches and
portfolio analyses since it determines the com-
pany holding the patent. Patent experts address
these search tasks by formulating queries contain-
ing the company name in question, in the hope of
finding all patents owned by that company. A for-
mal and more precise description of this relevant
search task is as follows: Given a query q which
specifies a company, and a set D of patents, de-
termine the set DQ C D comprised of all patents
held by the respective company.
For this purpose, all assignee names in the
patents in D should be analyzed. Let A denote
the set of all assignee names in D, and let a ∼ q
denote the fact that an assignee name a E A refers
to company q. Then in the portfolio search task,
all patents filed under a are relevant. The retrieval
of DQ can thus be rendered as a query expansion
</bodyText>
<page confidence="0.949054">
570
</page>
<note confidence="0.998348">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 570–579,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<tableCaption confidence="0.997771">
Table 1: User groups and patent-search-related retrieval tasks in the patent domain (Hunt et al., 2007).
</tableCaption>
<figure confidence="0.415428875">
User group
Analyst Attorney Manager Inventor Investor Researcher
Patentability • o • o
State of the art o •
Patent search task Infringement •
Opposition • •
Due diligence • •
Portfolio • o • •
</figure>
<bodyText confidence="0.985350166666666">
task, where q is expanded by the disjunction of
assignee names Aq with Aq = {a E A  |a ∼ q}.
While the trivial expansion of q by the entire
set A ensures maximum recall but entails an un-
acceptable precision, the expansion of q by the
empty set yields a reasonable baseline. The latter
approach is implemented in patent search engines
such as PatBase1 or FreePatentsOnline,2 which
return all patents where the company name q oc-
curs as a substring of the assignee name a. This
baseline is simple but reasonable; due to trade-
mark law, a company name q must be a unique
identifier (i.e. a key), and an assignee name a that
contains q can be considered as relevant. It should
be noted in this regard that |q |&lt; |a |holds for
most elements in Aq, since the assignee names
often contain company suffixes such as “Ltd”
or “Inc”.
Our hypothesis is that due to misspelled as-
signee names a substantial fraction of relevant
patents cannot be found by the baseline ap-
proach. In this regard, the types of spelling er-
rors in assignee names given in Table 2 should
be considered.
Table 2: Types of spelling errors with increasing
problem complexity according to Stein and Curatolo
(2006). The first row refers to lexical errors, whereas
the last two rows refer to phonological errors. For each
type, an example is given, where a misspelled com-
pany name is followed by the correctly spelled variant.
</bodyText>
<subsectionHeader confidence="0.907806">
Spelling error type Example
</subsectionHeader>
<bodyText confidence="0.595056375">
Permutations or dropped letters Whirpool Corporation
→ Whirlpool Corporation
Misremembering spelling details Whetherford International
→ Weatherford International
Spelling out the pronunciation Emulecks Corporation
→ Emulex Corporation
In order to raise the recall for portfolio search
without significantly impairing precision, an ap-
</bodyText>
<footnote confidence="0.999614">
1www.patbase.com
2www.freepatentsonline.com
</footnote>
<bodyText confidence="0.9996284">
proach more sophisticated than the standard re-
trieval approach, which is the expansion of q by
the empty set, is needed. Such an approach must
strive for an expansion of q by a subset of Aq,
whereby this subset should be as large as possible.
</bodyText>
<subsectionHeader confidence="0.995311">
1.1 Contributions
</subsectionHeader>
<bodyText confidence="0.999941090909091">
The paper provides a new solution to the problem
outlined. This solution employs machine learn-
ing on orthographic features, as well as on patent
meta features, to reliably detect spelling errors. It
consists of two steps: (1) the computation of AQ ,
the set of assignee names that are in a certain edit
distance neighborhood to q; and (2) the filtering of
A�q , yielding the set AQ, which contains those as-
signee names from Aq that are classified as mis-
spellings of q. The power of our approach can be
seen from Table 3, which also shows a key result
of our research; a retrieval system that exploits
our classifier will miss only 0.5% of the relevant
patents, while retrieval precision is compromised
by only 3.7%.
Another contribution relates to a new, manu-
ally-labeled corpus comprising spelling errors in
the assignee field of patents (cf. Section 3). In
this regard, we consider the over 2 million patents
granted by the USPTO between 2001 and 2010.
Last, we analyze indications of deliberately in-
serted spelling errors (cf. Section 4).
</bodyText>
<tableCaption confidence="0.9962885">
Table 3: Mean average Precision, Recall, and F-
Measure (0 = 2) for different expansion sets for q in
a portfolio search task, which is conducted on our test
corpus (cf. Section 3).
</tableCaption>
<table confidence="0.984262">
Expansion set for q Precision Recall F2
0 (baseline) 0.993 0.967 0.968
A9 (machine learning) 0.956 0.995 0.980
A (trivial) 0.001 1.0 0.005
A9 (edit distance) 0.274 1.0 0.672
</table>
<page confidence="0.994576">
571
</page>
<subsectionHeader confidence="0.787071">
1.2 Causes for Inconsistencies in Patents
</subsectionHeader>
<bodyText confidence="0.999866861111111">
We identify the following six factors for inconsis-
tencies in the bibliographic fields of patents, in
particular for assignee names: (1) Misspellings
are introduced due to the lack of knowledge, the
lack of attention, and due to spelling disabili-
ties. Intellevate Inc. (2006) reports that 98%
of a sample of patents taken from the USPTO
database contain errors, most which are spelling
errors. (2) Spelling errors are only removed by the
USPTO upon request (U.S. Patent &amp; Trademark
Office, 2010). (3) Spelling variations of inventor
names are permitted by the USPTO. The Manual
of Patent Examining Procedure (MPEP) states in
paragraph 605.04(b) that “if the applicant’s full
name is ’John Paul Doe,’ either ’John P. Doe’ or
’J. Paul Doe’ is acceptable.” Thus, it is valid to in-
troduce many different variations: with and with-
out initials, with and without a middle name, or
with and without suffixes. This convention ap-
plies to assignee names, too. (4) Companies of-
ten have branches in different countries, where
each branch has its own company suffix, e.g.,
“Limited” (United States), “GmbH” (Germany),
or “Kabushiki Kaisha” (Japan). Moreover, the
usage of punctuation varies along company suf-
fix abbreviations: “L.L.C.” in contrast to “LLC”,
for example. (5) Indexing errors emerge from
OCR processing patent applications, because sim-
ilar looking letters such as “e” versus “c” or “l”
versus “I” are likely to be misinterpreted. (6) With
the advent of electronic patent application filing,
the number of patent reexamination steps was re-
duced. As a consequence, the chance of unde-
tected spelling errors increases (Adams, 2010).
All of the mentioned factors add to a highly in-
consistent USPTO corpus.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999489147540984">
Information within a corpus can only be retrieved
effectively if the data is both accurate and unique
(Müller and Freytag, 2003). In order to yield data
that is accurate and unique, approaches to data
cleansing can be utilized to identify and remove
inconsistencies. Müller and Freytag (2003) clas-
sify inconsistencies, where duplicates of entities
in a corpus are part of a semantic anomaly. These
duplicates exist in a database if two or more dif-
ferent tuples refer to the same entity. With respect
to the bibliographic fields of patents, the assignee
names “Howlett-Packard” and “Hewett-Packard”
are distinct but refer to the same company. These
kinds of near-duplicates impede the identification
of duplicates (Naumann and Herschel, 2010).
Near-duplicate Detection The problem of
identifying near-duplicates is also known as
record linkage, or name matching; it is sub-
ject of active research (Elmagarmid et al., 2007).
With respect to text documents, slightly modi-
fied passages in these documents can be identi-
fied using fingerprints (Potthast and Stein, 2008).
On the other hand, for data fields which con-
tain natural language such as the assignee name
field, string similarity metrics (Cohen et al.,
2003) as well as spelling correction technol-
ogy are exploited (Damerau, 1964; Monge and
Elkan, 1997). String similarity metrics com-
pute a numeric value to capture the similarity
of two strings. Spelling correction algorithms,
by contrast, capture the likelihood for a given
word being a misspelling of another word. In
our analysis, the similarity metric SoftTfIdf is
applied, which performs best in name matching
tasks (Cohen et al., 2003), as well as the complete
range of spelling correction algorithms shown in
Figure 1: Soundex, which relies on similarity
hashing (Knuth, 1997), the Levenshtein distance,
which gives the minimum number of edits needed
to transform a word into another word (Leven-
shtein, 1966), and SmartSpell, a phonetic pro-
duction approach that computes the likelihood
of a misspelling (Stein and Curatolo, 2006). In
order to combine the strength of multiple met-
rics within a near-duplicate detection task, sev-
eral authors resort to machine learning (Bilenko
and Mooney, 2002; Cohen et al., 2003). Christen
(2006) concludes that it is important to exploit all
kinds of knowledge about the type of data in ques-
tion, and that inconsistencies are domain-specific.
Hence, an effective near-duplicate detection ap-
proach should employ domain-specific heuristics
and algorithms (Müller and Freytag, 2003). Fol-
lowing this argumentation, we augment various
word similarity assessments with patent-specific
meta-features.
Patent Search Commercial patent search en-
gines, such as PatBase and FreePatentsOnline,
handle near-duplicates in assignee names as fol-
lows. For queries which contain a company name
followed by a wildcard operator, PatBase suggests
</bodyText>
<page confidence="0.977257">
572
</page>
<figure confidence="0.99883825">
Collision-based
Neighborhood-based
Trigram-based
Edit-distance-based
Rule-based
Heuristic search
Hidden Markov
models
</figure>
<figureCaption confidence="0.997485">
Figure 1: Classification of spelling correction methods
according to Stein and Curatolo (2006).
</figureCaption>
<bodyText confidence="0.998031593406593">
a set of additional companies (near-duplicates),
which can be considered alongside the company
name in question. These suggestions are solely
retrieved based on a trailing wildcard query. Each
additional company name can then be marked in-
dividually by a user to expand the original query.
In case the entire set of suggestions is consid-
ered, this strategy conforms to the expansion of
a query by the empty set, which equals a rea-
sonable baseline approach. This query expansion
strategy, however, has the following drawbacks:
(1) The strategy captures only inconsistencies that
succeed the given company name in the origi-
nal query. Thus, near-duplicates which contain
spelling errors in the company name itself are not
found. Even if PatBase would support left trailing
wildcards, then only the full combination of wild-
card expressions would cover all possible cases of
misspellings. (2) Given an acronym of a company
such as IBM, it is infeasible to expand the ab-
breviation to “International Business Machines”
without considering domain knowledge.
Query Expansion Methods for Patent Search
To date, various studies have investigated query
expansion techniques in the patent domain that
focus on prior-art search and invalidity search
(Magdy and Jones, 2011). Since we are dealing
with queries that comprise only a company name,
existing methods cannot be applied. Instead, the
near-duplicate task in question is more related to a
text reuse detection task discussed by Hagen and
Stein (2011); given a document, passages which
also appear identical or slightly modified in other
documents, have to be retrieved by using standard
keyword-based search engines. Their approach is
guided by the user-over-ranking hypothesis intro-
duced by Stein and Hagen (2011). It states that
“the best retrieval performance can be achieved
with queries returning about as many results as
can be considered at user site.” If we make use
of their terminology, then we can distinguish the
query expansion sets (cf. Table 3) into two cate-
gories: (1) The trivial as well as the edit distance
expansion sets are underspecific, i.e., users cannot
cope with the large amount of irrelevant patents
returned; the precision is close to zero. (2) The
baseline approach, by contrast, is overspecific;
it returns too few documents, i.e., the achieved
recall is not optimal. As a consequence, these
query expansion sets are not suitable for portfolio
search. Our approach, on the other hand, excels
in both precision and recall.
Query Spelling Correction Queries which are
submitted to standard web search engines differ
from queries which are posed to patent search en-
gines with respect to both length and language
diversity. Hence, research in the field of web
search is concerned with suggesting reasonable
alternatives to misspelled queries rather than cor-
recting single words (Li et al., 2011). Since stan-
dard spelling correction dictionaries (e.g. ASpell)
are not able to capture the rich language used in
web queries, large-scale knowledge sources such
as Wikipedia (Li et al., 2011), query logs (Chen
et al., 2007), and large n-gram corpora (Brants et
al., 2007) are employed. It should be noted that
the set of correctly written assignee names is un-
known for the USPTO patent corpus.
Moreover, spelling errors are modeled on the
basis of language models (Li et al., 2011). Okuno
(2011) proposes a generative model to encounter
spelling errors, where the original query is ex-
panded based on alternatives produced by a small
edit distance to the original query. This strategy
correlates to the trivial query expansion set (cf.
Section 1). Unlike using a small edit distance, we
allow a reasonable high edit distance to maximize
the recall.
Trademark Search The trademark search is
about identifying registered trademarks which are
similar to a new trademark application. Sim-
ilarities between trademarks are assessed based
on figurative and verbal criteria. In the former
case, the focus is on image-based retrieval tech-
niques. Trademarks are considered verbally simi-
lar for a variety of reasons, such as pronunciation,
spelling, and conceptual closeness, e.g., swapping
letters or using numbers for words. The verbal
similarity of trademarks, on the other hand, can
be determined by using techniques comparable
to near-duplicate detection: phonological parsing,
</bodyText>
<figure confidence="0.9654385">
Single word
spelling
correction
Near similarity
hashing
Phonetic production
approach
Editing
</figure>
<page confidence="0.994723">
573
</page>
<bodyText confidence="0.978888">
fuzzy search, and edit distance computation (Fall
and Giraud-Carrier, 2005).
</bodyText>
<sectionHeader confidence="0.937682" genericHeader="method">
3 Detection of Spelling Errors
</sectionHeader>
<bodyText confidence="0.999986384615384">
This section presents our machine learning ap-
proach to expand a company query q; the classi-
fier c delivers the set AQ = {a E A  |c(q, a) = 1},
an approximation of the ideal set of relevant as-
signee names AQ. As a classification technol-
ogy a support vector machine with linear kernel
is used, which receives each pair (q, a) as a six-
dimensional feature vector. For training and test
purposes we identified misspellings for 100 dif-
ferent company names. A detailed description of
the constructed test corpus and a report on the
classifiers performance is given in the remainder
of this section.
</bodyText>
<subsectionHeader confidence="0.994642">
3.1 Feature Set
</subsectionHeader>
<bodyText confidence="0.9998358">
The feature set comprises six features, three of
them being orthographic similarity metrics, which
are computed for every pair (q, a). Each metric
compares a given company name q with the first
|q |words of the assignee name a:
</bodyText>
<listItem confidence="0.997430466666666">
1. SoftTfIdf. The SoftTfIdf metric is consid-
ered, since the metric is suitable for the com-
parison of names (Cohen et al., 2003). The
metric incorporates the Jaro-Winkler met-
ric (Winkler, 1999) with a distance threshold
of 0.9. The frequency values for the similar-
ity computation are trained on A.
2. Soundex. The Soundex spelling correction
algorithm captures phonetic errors. Since the
algorithm computes hash values for both q
and a, the feature is 1 if these hash values
are equal, 0 otherwise.
3. Levenshtein distance. The Levenshtein dis-
tance for (q, a) is normalized by the charac-
ter length of q.
</listItem>
<bodyText confidence="0.9996528">
To obtain further evidence for a misspelling
in an assignee name, meta information about the
patents in D, to which the assignee name refers
to, is exploited. In this regard, the following three
features are derived:
</bodyText>
<listItem confidence="0.859773666666667">
1. Assignee Name Frequency. The number
of patents filed under an assignee name a:
FF,,q(a) = Req(a, D). We assume that the
</listItem>
<bodyText confidence="0.903100823529412">
probability of a misspelling to occur multi-
ple times is low, and thus an assignee name
with a misspelled company name has a low
frequency.
2. IPC Overlap. The IPC codes of a patent
specify the technological areas it applies
to. We assume that patents filed under the
same company name are likely to share the
same set of IPC codes, regardless whether
the company name is misspelled or not.
Hence, if we determine the IPC codes of
patents which contain q in the assignee
name, IPC(q), and the IPC codes of patents
filed under assignee name a, IPC(a), then
the intersection size of the two sets serves as
an indicator for a misspelled company name
in a:
</bodyText>
<equation confidence="0.996067">
IPC(q) fl IPC(a)
FIPC (q, a) =
IPC(q) U IPC(a)
</equation>
<listItem confidence="0.99150775">
3. Company Suffix Match. The suffix match
relies on the company suffixes Suffixes(q)
that occur in the assignee names of A con-
taining q. Similar to the IPC overlap fea-
ture, we argue that if the company suffix
of a exists in the set Suffixes(q), a mis-
spelling in a is likely: FS,,4ff,,;e3(q, a) = 1
iffSuffixes(a) E Suffixes(q).
</listItem>
<subsectionHeader confidence="0.998261">
3.2 Webis Patent Retrieval Assignee Corpus
</subsectionHeader>
<bodyText confidence="0.9998957">
A key contribution of our work is a new cor-
pus called Webis Patent Retrieval Assignee Cor-
pus 2012 (Webis-PRA-12). We compiled the cor-
pus in order to assess the impact of misspelled
companies on patent retrieval and the effective-
ness of our classifier to detect them.3 The corpus
is built on the basis of 2132 825 patents D granted
by the USPTO between 2001 and 2010; the patent
corpus is provided publicly by the USPTO in
XML format. Each patent contains bibliographic
fields as well as textual information such as the
abstract and the claims section. Since we are in-
terested in the assignee name a associated with
each patent d E D, we parse each patent and ex-
tract the assignee name. This yields the set A of
202 846 different assignee names. Each assignee
name refers to a set of patents, which size varies
from 1 to 37 202 (the number of patents filed
under “International Business Machines Corpo-
ration”). It should be noted that for a portfolio
</bodyText>
<footnote confidence="0.989962">
3The Webis-PRA-12 corpus is freely available via
www.webis.de/research/corpora
</footnote>
<page confidence="0.998357">
574
</page>
<tableCaption confidence="0.947074">
Table 4: Statistics of spelling errors for the 100 companies in the Webis-PRA-12 corpus. Considered are the
number of words and the number of letters in the company names, as well as the number of different company
suffixes that are used together with a company name (denoted as variants of q)
</tableCaption>
<table confidence="0.64741725">
Total Num. of words in q Num. of letters in q Num. of variants of q
1 2 3-4 2-10 11-15 16-35 1-5 6-15 16-96
Number of companies in Q 100 36 53 11 30 35 35 45 32 23
Avg. num. of misspellings in A 3.79 2.13 3.75 9.36 1.16 2.94 6.88 0.91 3.81 9.39
</table>
<bodyText confidence="0.999964">
search task the number of patents which refer to
an assignee name matters for the computation of
precision and recall. If we, however, isolate the
task of detecting misspelled company names, then
it is also reasonable to weight each assignee name
equally and independently from the number of
patents it refers to. Both scenarios are addressed
in the experiments.
Given A, the corpus construction task is to map
each assignee name a E A to the company name
q it refers to. This gives for each company name
q the set of relevant assignee names Aq. For our
corpus, we do not construct Aq for all company
names but take a selection of 100 company names
from the 2011 Fortune 500 ranking as our set of
company names Q. Since the Fortune 500 rank-
ing contains only large companies, the test cor-
pus may appear to be biased towards these com-
panies. However, rather than the company size the
structural properties of a company name are de-
terminative; our sample includes short, medium,
and long company names, as well as company
names with few, medium, and many different
company suffixes. Table 4 shows the distribution
of company names in Q along these criteria in the
first row.
For each company name q E Q, we ap-
ply a semi-automated procedure to derive the
set of relevant assignee names Aq. In a first
step, all assignee names in A which do not re-
fer to the company name q are filtered auto-
matically. From a preliminary evaluation we
concluded that the Levenshtein distance d(q, a)
with a relative threshold of jqj/2 is a reasonable
choice for this filtering step. The resulting sets
= {a E A j d(q, a) G jqj/2) contain, in total
over Q, 14 189 assignee names. These assignee
names are annotated by human assessors within a
second step to derive the final set Aq for each q E
Q. Altogether we identify 1 538 assignee names
that refer to the 100 companies in Q. With respect
to our classification task, the assignee names in
each Aq are positive examples; the remaining as-
signee names A+q \ Aq form the set of negative
examples (12 651 in total).
During the manual assessment, names of as-
signees which include the correct company name
q were distinguished from misspelled ones. The
latter holds true for 379 of the 1 538 assignee
names. These names are not retrievable by the
baseline system, and thus form the main target for
our classifier. The second row of Table 4 reports
on the distribution of the 379 misspelled assignee
names. As expectable, the longer the company
name, the more spelling errors occur. Compa-
nies which file patents under many different as-
signee names are likelier to have patents with mis-
spellings in the company name.
</bodyText>
<subsectionHeader confidence="0.998017">
3.3 Classifier Performance
</subsectionHeader>
<bodyText confidence="0.999942333333333">
For the evaluation with the Webis-PRA-12 cor-
pus, we train a support vector machine,4 which
considers the six outlined features, and compare
it to the other expansion techniques. For the train-
ing phase, we use 2/3 of the positive examples
to form a balanced training set of 1 025 posi-
tive and 1 025 negative examples. After 10-fold
cross validation, the achieved classification accu-
racy is 95.97%.
For a comparison of the expansion techniques
on the test set, which contains the examples not
considered in the training phase, two tasks are
distinguished: finding near duplicates in assignee
names (cf. Table 5, Columns 3–5), and finding all
patents of a company (cf. Table 5, Columns 6–8).
The latter refers to the actual task of portfo-
lio search. It can be observed that the perfor-
mance improvements on both tasks are pretty sim-
ilar. The baseline expansion 0 yields a recall
of 0.83 in the first task. The difference of 0.17
to a perfect recall can be addressed by consid-
ering query expansion techniques. If the triv-
ial expansion A is applied to the task the max-
imum recall can be achieved, which, however,
</bodyText>
<footnote confidence="0.8494845">
4We use the implementation of the WEKA toolkit with default
parameters.
</footnote>
<figure confidence="0.652935">
A+
q
</figure>
<page confidence="0.997243">
575
</page>
<tableCaption confidence="0.860096">
Table 5: The search results (macro-averaged) for two retrieval tasks and various expansion techniques. Besides
Precision and Recall, the F-Measure with 0 = 2 is stated.
</tableCaption>
<table confidence="0.9997028">
Misspelling detection Task: assignee names P Task: patents F2
P R F2 R
Baseline (0) .975 .829 .838 .993 .967 .968
Trivial (A) .000 1.0 .001 .001 1.0 .005
Edit distance (Aq) .274 1.0 .499 .412 1.0 .672
SVM (Levenshtein) .752 .981 .853 .851 .991 .911
SVM (SoftTfIdf) .702 .980 .796 .826 .993 .886
SVM (Soundex) .433 .931 .624 .629 .984 .759
SVM (orthographic features) .856 .975 .922 .942 .990 .967
SVM (Aq, all features) .884 .975 .938 .956 .995 .980
</table>
<bodyText confidence="0.959265294117647">
is bought with precision close to zero. Using
the edit distance expansion AQ yields a precision
of 0.274 while keeping the recall at maximum. Fi-
nally, the machine learning expansion AQ leads
to a dramatic improvement (cf. Table 5, bottom
lines), whereas the exploitation of patent meta-
features significantly outperforms the exclusive
use of orthography-related features; the increase
in recall which is achieved by AQ is statistically
significant (matched pair t-test) for both tasks (as-
signee names task: t = −7.6856, df = 99,
p = 0.00; patents task: t = −2.1113, df = 99,
p = 0.037). Note that when being applied as a
single feature none of the spelling metrics (Lev-
enshtein, SoftTfIdf, Soundex) is able to achieve
a recall close to 1 without significantly impairing
the precision.
</bodyText>
<sectionHeader confidence="0.878939" genericHeader="method">
4 Distribution of Spelling Errors
</sectionHeader>
<bodyText confidence="0.999951333333333">
Encouraged by the promising retrieval results
achieved on the Webis-PRA-12 corpus, we ex-
tend the analysis of spelling errors in patents to
the entire USPTO corpus of granted patents be-
tween 2001 and 2010. The analysis focuses on
the following two research questions:
</bodyText>
<listItem confidence="0.9098055">
1. Are spelling errors an increasing issue in
patents? According to Adams (2010), the
amount of spelling errors should have been
increased in the last years due to the elec-
tronic patent filing process (cf. Section 1.2).
We address this hypothesis by analyzing the
distribution of spelling errors in company
names that occur in patents granted between
2001 and 2010.
2. Are misspellings introduced deliberately in
patents? We address this question by analyz-
ing the patents with respect to the eight tech-
</listItem>
<bodyText confidence="0.913110552631579">
nological areas based on the International
Patent Classification scheme IPC: A (Hu-
man necessities), B (Performing operations;
transporting), C (Chemistry; metallurgy),
D (Textiles; paper), E (Fixed constructions),
F (Mechanical engineering; lighting; heat-
ing; weapons; blasting), G (Physics), and
H (Electricity). If spelling errors are in-
troduced accidentally, then we expect them
to be uniformly distributed across all ar-
eas. A biased distribution, on the other
hand, indicates that errors might be in-
serted deliberately.
In the following, we compile a second corpus
on the basis of the entire set A of assignee names.
In order to yield a uniform distribution of the com-
panies across years, technological areas and coun-
tries, a set of 120 assignee names is extracted for
each dimension. After the removal of duplicates,
we revised these assignee names manually in or-
der to check (and correct) their spelling. Finally,
trailing business suffixes are removed, which re-
sults in a set of 3 110 company names. For each
company name q, we generate the set AQ as de-
scribed in Section 3.
The results of our analysis are shown in Table 6.
Table 6(a) refers to the first research question and
shows that the amount of misspellings in compa-
nies decreased over the years from 6.67% in 2001
to 4.74% in 2010 (cf. Row 3). These results let us
reject the hypothesis of Adams (2010). Neverthe-
less, the analysis provides evidence that spelling
errors are still an issue. For example, the company
identified with most spelling errors are “Konin-
klijke Philips Electronics” with 45 misspellings
in 2008, and “Centre National de la Recherche
Scientifique” with 28 misspellings in 2009. The
results are consistent with our findings with re-
</bodyText>
<page confidence="0.99571">
576
</page>
<table confidence="0.5639515">
Measure
Measure
</table>
<tableCaption confidence="0.76517375">
Table 6: Distribution of spelling errors for 3 110 company identifiers in the USPTO patents. The mean of spelling
errors per company identifier and the standard deviation u refer to companies with misspellings. The last row in
each table shows the number of patents that are additionally found if the original query q is expanded by A9.
(a) Distribution of spelling errors between the years 2001 and 2010.
</tableCaption>
<table confidence="0.957749380952381">
2002 2004 Year 2008 2010
2006
2001 2003 2005 2007 2009
Number of companies 1028 1 066 1 115 1 151 1 219 1 261 1 274 1210 1224 1 268
Number of companies with misspellings 67 63 53 65 65 60 65 64 53 60
Companies with misspellings (%) 6.52 5.91 4.75 5.65 5.33 4.76 5.1 5.29 4.33 4.73
Mean 2.78 2.35 2.23 2.28 2.18 2.48 2.23 3.0 2.64 2.8
Standard deviation u 4.62 3.3 3.63 3.13 2.8 3.55 2.87 6.37 4.71 4.6
Maximum misspellings per company 24 12 16 12 10 18 12 45 28 22
Additional number of patents 7.1 7.21 7.43 7.68 7.91 8.48 7.83 8.84 8.92 8.92
(b) Distribution of spelling errors based on the IPC scheme.
B D IPC code H
F
A C E G
Number of companies 954 1 231 811 277 412 771 1 232 949
Number of companies with misspellings 59 70 51 7 10 33 83 63
Companies with misspellings (%) 6.18 5.69 6.29 2.53 2.43 4.28 6.74 6.64
Mean 3.0 2.49 3.57 1.86 2.8 1.88 3.29 4.05
Standard deviation u 5.28 3.65 7.03 1.99 4.22 2.31 5.72 7.13
Maximum misspellings per company 32 14 40 3 12 6 24 35
Additional number of patents 9.25 9.67 11.12 4.71 4.6 4.79 8.92 12.84
</table>
<bodyText confidence="0.999927052631579">
spect to the Fortune 500 sample (cf. Table 4),
where company names that are longer and pre-
sumably more difficult to write contain more
spelling errors.
In contrast to the uniform distribution of mis-
spellings over the years, the situation with re-
gard to the technological areas is different (cf. Ta-
ble 6(b)). Most companies are associated with
the IPC sections G and B, which both refer to
technical domains (cf. Table 6(b), Row 1). The
percentage of misspellings in these sections in-
creased compared to the spelling errors grouped
by year. A significant difference can be seen for
the sections D and E. Here, the number of as-
signed companies drops below 450 and the per-
centage of misspellings decreases significantly
from about 6% to 2.5%. These findings might
support the hypothesis that spelling errors are in-
serted deliberately in technical domains.
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999987692307692">
While researchers in the patent domain concen-
trate on retrieval models and algorithms to im-
prove the search performance, the original aspect
of our paper is that it points to a different (and or-
thogonal) research avenue: the analysis of patent
inconsistencies. With the analysis of spelling er-
rors in assignee names we made a first yet consid-
erable contribution in this respect; searches with
assignee constraints become a more sensible op-
eration. We showed how a special treatment of
spelling errors can significantly raise the effec-
tiveness of patent search. The identification of
this untapped potential, but also the utilization of
machine learning to combine patent features with
typography, form our main contributions.
Our current research broadens the application
of a patent spelling analysis. In order to iden-
tify errors that are introduced deliberately we
investigate different types of misspellings (edit
distance versus phonological). Finally, we con-
sider the analysis of acquisition histories of com-
panies as promising research direction: since
acquired companies often own granted patents,
these patents should be considered while search-
ing for the company in question in order to further
increase the recall.
</bodyText>
<sectionHeader confidence="0.997783" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.967461">
This work is supported in part by the German Sci-
ence Foundation under grants STE1019/2-1 and
FU205/22-1.
</bodyText>
<page confidence="0.995085">
577
</page>
<sectionHeader confidence="0.996257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99962172815534">
Stephen Adams. 2010. The Text, the Full Text and
nothing but the Text: Part 1 – Standards for creating
Textual Information in Patent Documents and Gen-
eral Search Implications. World Patent Information,
32(1):22–29, March.
Mikhail Bilenko and Raymond J. Mooney. 2002.
Learning to Combine Trained Distance Metrics
for Duplicate Detection in Databases. Technical
Report AI 02-296, Artificial Intelligence Labora-
tory, University of Austin, Texas, USA, Austin,
TX, February.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language
Models in Machine Translation. In EMNLP-CoNLL
’07: Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 858–867. ACL, June.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing Query Spelling Correction Using Web Search
Results. In EMNLP-CoNLL ’07: Proceedings of
the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 181–189. ACL,
June.
Peter Christen. 2006. A Comparison of Personal
Name Matching: Techniques and Practical Is-
sues. In ICDM ’06: Workshops Proceedings of
the sixth IEEE International Conference on Data
Mining, pages 290–294. IEEE Computer Society,
December.
William W. Cohen, Pradeep Ravikumar, and Stephen
E. Fienberg. 2003. A Comparison of String
Distance Metrics for Name-Matching Tasks. In
Subbarao Kambhampati and Craig A. Knoblock,
editors, IIWeb ’03: Proceedings of the IJCAI
workshop on Information Integration on the Web,
pages 73–78, August.
Fred J. Damerau. 1964. A Technique for Computer
Detection and Correction of Spelling Errors. Com-
munications of the ACM, 7(3):171–176.
Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and
Vassilios S. Verykios. 2007. Duplicate Record De-
tection: A Survey. IEEE Trans. Knowl. Data Eng.,
19(1):1–16.
Caspas J. Fall and Christophe Giraud-Carrier. 2005.
Searching Trademark Databases for Verbal Similar-
ities. World Patent Information, 27(2):135–143.
Matthias Hagen and Benno Stein. 2011. Candidate
Document Retrieval for Web-Scale Text Reuse De-
tection. In 18th International Symposium on String
Processing and Information Retrieval (SPIRE 11),
volume 7024 of Lecture Notes in Computer Science,
pages 356–367. Springer.
David Hunt, Long Nguyen, and Matthew Rodgers, ed-
itors. 2007. Patent Searching: Tools &amp; Techniques.
Wiley.
Intellevate Inc. 2006. Patent Quality, a blog en-
try. http://www.patenthawk.com/blog/
2006/01/patent_quality.html, January.
Hideo Joho, Leif A. Azzopardi, and Wim Vander-
bauwhede. 2010. A Survey of Patent Users: An
Analysis of Tasks, Behavior, Search Functionality
and System Requirements. In IIix ’10: Proceed-
ing of the third symposium on Information Inter-
action in Context, pages 13–24, New York, NY,
USA. ACM.
Donald E. Knuth. 1997. The Art of Computer Pro-
gramming, Volume I: Fundamental Algorithms, 3rd
Edition. Addison-Wesley.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions and reversals.
Soviet Physics Doklady, 10(8):707–710. Original
in Doklady Akademii Nauk SSSR 163(4): 845-848.
Yanen Li, Huizhong Duan, and ChengXiang Zhai.
2011. CloudSpeller: Spelling Correction for Search
Queries by Using a Unified Hidden Markov Model
with Web-scale Resources. In Spelling Alteration
for Web Search Workshop, pages 10–14, July.
Patrice Lopez and Laurent Romary. 2010. Experi-
ments with Citation Mining and Key-Term Extrac-
tion for Prior Art Search. In Martin Braschler,
Donna Harman, and Emanuele Pianta, editors,
CLEF 2010 LABs and Workshops, Notebook Pa-
pers, September.
Mihai Lupu, Katja Mayer, John Tait, and Anthony J.
Trippe, editors. 2011. Current Challenges in Patent
Information Retrieval, volume 29 of The Informa-
tion Retrieval Series. Springer.
Walid Magdy and Gareth J. F. Jones. 2010. Ap-
plying the KISS Principle for the CLEF-IP 2010
Prior Art Candidate Patent Search Task. In Martin
Braschler, Donna Harman, and Emanuele Pianta,
editors, CLEF 2010 LABs and Workshops, Note-
book Papers, September.
Walid Magdy and Gareth J.F. Jones. 2011. A Study
on Query Expansion Methods for Patent Retrieval.
In PAIR ’11: Proceedings of the 4th workshop on
Patent information retrieval, AAAI Workshop on
Plan, Activity, and Intent Recognition, pages 19–
24, New York, NY, USA. ACM.
Alvaro E. Monge and Charles Elkan. 1997. An Ef-
ficient Domain-Independent Algorithm for Detect-
</reference>
<page confidence="0.976392">
578
</page>
<reference confidence="0.999689196078431">
ing Approximately Duplicate Database Records.
In DMKD ’09: Proceedings of the 2nd workshop
on Research Issues on Data Mining and Knowl-
edge Discovery, pages 23–29, New York, NY,
USA. ACM.
Heiko Müller and Johann-C. Freytag. 2003. Prob-
lems, Methods and Challenges in Comprehensive
Data Cleansing. Technical Report HUB-IB-164,
Humboldt-Universität zu Berlin, Institut für Infor-
matik, Germany.
Felix Naumann and Melanie Herschel. 2010. An In-
troduction to Duplicate Detection. Synthesis Lec-
tures on Data Management. Morgan &amp; Claypool
Publishers.
Yoh Okuno. 2011. Spell Generation based on Edit
Distance. In Spelling Alteration for Web Search
Workshop, pages 25–26, July.
Martin Potthast and Benno Stein. 2008. New Is-
sues in Near-duplicate Detection. In Christine
Preisach, Hans Burkhardt, Lars Schmidt-Thieme,
and Reinhold Decker, editors, Data Analysis, Ma-
chine Learning and Applications. Selected papers
from the 31th Annual Conference of the German
Classification Society (GfKl 07), Studies in Classi-
fication, Data Analysis, and Knowledge Organiza-
tion, pages 601–609, Berlin Heidelberg New York.
Springer.
Benno Stein and Daniel Curatolo. 2006. Phonetic
Spelling and Heuristic Search. In Gerhard Brewka,
Silvia Coradeschi, Anna Perini, and Paolo Traverso,
editors, 17th European Conference on Artificial In-
telligence (ECAI 06), pages 829–830, Amsterdam,
Berlin, August. IOS Press.
Benno Stein and Matthias Hagen. 2011. Introducing
the User-over-Ranking Hypothesis. In Advances in
Information Retrieval. 33rd European Conference
on IR Resarch (ECIR 11), volume 6611 of Lecture
Notes in Computer Science, pages 503–509, Berlin
Heidelberg New York, April. Springer.
U.S. Patent &amp; Trademark Office. 2010. Manual of
Patent Examining Procedure (MPEP), Eighth Edi-
tion, July.
William W. Winkler. 1999. The State of Record Link-
age and Current Research Problems. Technical re-
port, Statistical Research Division, U.S. Bureau of
the Census.
Xiaobing Xue and Bruce W. Croft. 2009. Automatic
Query Generation for Patent Search. In CIKM
’09: Proceeding of the eighteenth ACM conference
on Information and Knowledge Management, pages
2037–2040, New York, NY, USA. ACM.
</reference>
<page confidence="0.998579">
579
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.259486">
<title confidence="0.998929">The Impact of Spelling Errors on Patent Search</title>
<author confidence="0.933437">Stein Hoppe Gollub</author>
<affiliation confidence="0.659177">Bauhaus-Universität</affiliation>
<address confidence="0.999282">99421 Weimar, Germany</address>
<email confidence="0.952811"><firstname>.<lastname>@uni-weimar.de</email>
<abstract confidence="0.970784666666667">The search in patent databases is a risky business compared to the search in other domains. A single document that is relevant but overlooked during a patent search can turn into an expensive proposition. While recent research engages in specialized models and algorithms to improve the effectiveness of patent retrieval, we bring another aspect into focus: the detection and exploitation of patent inconsistencies. In particular, we analyze spelling errors in the assignee field of patents granted by the United States Patent &amp; Trademark Office. We introduce technology in order to improve retrieval effectiveness despite the presence of typographical ambiguities. In this regard, we (1) quantify spelling errors in terms of edit distance and phonological dissimilarity and (2) render error detection as a learning problem that combines word dissimilarities with patent meta-features. For the task of finding all patents of a company, our approach improves recall from 96.7% (when using a state-of-the-art patent search engine) to 99.5%, while precision is compromised by only 3.7%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Adams</author>
</authors>
<title>The Text, the Full Text and nothing but the Text: Part 1 – Standards for creating Textual Information in Patent Documents and General Search Implications. World Patent Information,</title>
<date>2010</date>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="9166" citStr="Adams, 2010" startWordPosition="1501" endWordPosition="1502">h branch has its own company suffix, e.g., “Limited” (United States), “GmbH” (Germany), or “Kabushiki Kaisha” (Japan). Moreover, the usage of punctuation varies along company suffix abbreviations: “L.L.C.” in contrast to “LLC”, for example. (5) Indexing errors emerge from OCR processing patent applications, because similar looking letters such as “e” versus “c” or “l” versus “I” are likely to be misinterpreted. (6) With the advent of electronic patent application filing, the number of patent reexamination steps was reduced. As a consequence, the chance of undetected spelling errors increases (Adams, 2010). All of the mentioned factors add to a highly inconsistent USPTO corpus. 2 Related Work Information within a corpus can only be retrieved effectively if the data is both accurate and unique (Müller and Freytag, 2003). In order to yield data that is accurate and unique, approaches to data cleansing can be utilized to identify and remove inconsistencies. Müller and Freytag (2003) classify inconsistencies, where duplicates of entities in a corpus are part of a semantic anomaly. These duplicates exist in a database if two or more different tuples refer to the same entity. With respect to the bibl</context>
<context position="26941" citStr="Adams (2010)" startWordPosition="4454" endWordPosition="4455"> = −2.1113, df = 99, p = 0.037). Note that when being applied as a single feature none of the spelling metrics (Levenshtein, SoftTfIdf, Soundex) is able to achieve a recall close to 1 without significantly impairing the precision. 4 Distribution of Spelling Errors Encouraged by the promising retrieval results achieved on the Webis-PRA-12 corpus, we extend the analysis of spelling errors in patents to the entire USPTO corpus of granted patents between 2001 and 2010. The analysis focuses on the following two research questions: 1. Are spelling errors an increasing issue in patents? According to Adams (2010), the amount of spelling errors should have been increased in the last years due to the electronic patent filing process (cf. Section 1.2). We address this hypothesis by analyzing the distribution of spelling errors in company names that occur in patents granted between 2001 and 2010. 2. Are misspellings introduced deliberately in patents? We address this question by analyzing the patents with respect to the eight technological areas based on the International Patent Classification scheme IPC: A (Human necessities), B (Performing operations; transporting), C (Chemistry; metallurgy), D (Textile</context>
<context position="28731" citStr="Adams (2010)" startWordPosition="4751" endWordPosition="4752">is extracted for each dimension. After the removal of duplicates, we revised these assignee names manually in order to check (and correct) their spelling. Finally, trailing business suffixes are removed, which results in a set of 3 110 company names. For each company name q, we generate the set AQ as described in Section 3. The results of our analysis are shown in Table 6. Table 6(a) refers to the first research question and shows that the amount of misspellings in companies decreased over the years from 6.67% in 2001 to 4.74% in 2010 (cf. Row 3). These results let us reject the hypothesis of Adams (2010). Nevertheless, the analysis provides evidence that spelling errors are still an issue. For example, the company identified with most spelling errors are “Koninklijke Philips Electronics” with 45 misspellings in 2008, and “Centre National de la Recherche Scientifique” with 28 misspellings in 2009. The results are consistent with our findings with re576 Measure Measure Table 6: Distribution of spelling errors for 3 110 company identifiers in the USPTO patents. The mean of spelling errors per company identifier and the standard deviation u refer to companies with misspellings. The last row in ea</context>
</contexts>
<marker>Adams, 2010</marker>
<rawString>Stephen Adams. 2010. The Text, the Full Text and nothing but the Text: Part 1 – Standards for creating Textual Information in Patent Documents and General Search Implications. World Patent Information, 32(1):22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to Combine Trained Distance Metrics for Duplicate Detection in Databases.</title>
<date>2002</date>
<tech>Technical Report AI 02-296,</tech>
<institution>Artificial Intelligence Laboratory, University of Austin,</institution>
<location>Texas, USA, Austin, TX,</location>
<contexts>
<context position="11458" citStr="Bilenko and Mooney, 2002" startWordPosition="1860" endWordPosition="1863">d, which performs best in name matching tasks (Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein distance, which gives the minimum number of edits needed to transform a word into another word (Levenshtein, 1966), and SmartSpell, a phonetic production approach that computes the likelihood of a misspelling (Stein and Curatolo, 2006). In order to combine the strength of multiple metrics within a near-duplicate detection task, several authors resort to machine learning (Bilenko and Mooney, 2002; Cohen et al., 2003). Christen (2006) concludes that it is important to exploit all kinds of knowledge about the type of data in question, and that inconsistencies are domain-specific. Hence, an effective near-duplicate detection approach should employ domain-specific heuristics and algorithms (Müller and Freytag, 2003). Following this argumentation, we augment various word similarity assessments with patent-specific meta-features. Patent Search Commercial patent search engines, such as PatBase and FreePatentsOnline, handle near-duplicates in assignee names as follows. For queries which conta</context>
</contexts>
<marker>Bilenko, Mooney, 2002</marker>
<rawString>Mikhail Bilenko and Raymond J. Mooney. 2002. Learning to Combine Trained Distance Metrics for Duplicate Detection in Databases. Technical Report AI 02-296, Artificial Intelligence Laboratory, University of Austin, Texas, USA, Austin, TX, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large Language Models in Machine Translation.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL ’07: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>858--867</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="15480" citStr="Brants et al., 2007" startWordPosition="2475" endWordPosition="2478">tion Queries which are submitted to standard web search engines differ from queries which are posed to patent search engines with respect to both length and language diversity. Hence, research in the field of web search is concerned with suggesting reasonable alternatives to misspelled queries rather than correcting single words (Li et al., 2011). Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al., 2011), query logs (Chen et al., 2007), and large n-gram corpora (Brants et al., 2007) are employed. It should be noted that the set of correctly written assignee names is unknown for the USPTO patent corpus. Moreover, spelling errors are modeled on the basis of language models (Li et al., 2011). Okuno (2011) proposes a generative model to encounter spelling errors, where the original query is expanded based on alternatives produced by a small edit distance to the original query. This strategy correlates to the trivial query expansion set (cf. Section 1). Unlike using a small edit distance, we allow a reasonable high edit distance to maximize the recall. Trademark Search The tr</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large Language Models in Machine Translation. In EMNLP-CoNLL ’07: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 858–867. ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Chen</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Improving Query Spelling Correction Using Web Search Results.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL ’07: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>181--189</pages>
<publisher>ACL,</publisher>
<contexts>
<context position="15432" citStr="Chen et al., 2007" startWordPosition="2467" endWordPosition="2470">th precision and recall. Query Spelling Correction Queries which are submitted to standard web search engines differ from queries which are posed to patent search engines with respect to both length and language diversity. Hence, research in the field of web search is concerned with suggesting reasonable alternatives to misspelled queries rather than correcting single words (Li et al., 2011). Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al., 2011), query logs (Chen et al., 2007), and large n-gram corpora (Brants et al., 2007) are employed. It should be noted that the set of correctly written assignee names is unknown for the USPTO patent corpus. Moreover, spelling errors are modeled on the basis of language models (Li et al., 2011). Okuno (2011) proposes a generative model to encounter spelling errors, where the original query is expanded based on alternatives produced by a small edit distance to the original query. This strategy correlates to the trivial query expansion set (cf. Section 1). Unlike using a small edit distance, we allow a reasonable high edit distance</context>
</contexts>
<marker>Chen, Li, Zhou, 2007</marker>
<rawString>Qing Chen, Mu Li, and Ming Zhou. 2007. Improving Query Spelling Correction Using Web Search Results. In EMNLP-CoNLL ’07: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 181–189. ACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Christen</author>
</authors>
<title>A Comparison of Personal Name Matching: Techniques and Practical Issues.</title>
<date>2006</date>
<booktitle>In ICDM ’06: Workshops Proceedings of the sixth IEEE International Conference on Data Mining,</booktitle>
<pages>290--294</pages>
<publisher>IEEE Computer Society,</publisher>
<contexts>
<context position="11496" citStr="Christen (2006)" startWordPosition="1868" endWordPosition="1869">Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein distance, which gives the minimum number of edits needed to transform a word into another word (Levenshtein, 1966), and SmartSpell, a phonetic production approach that computes the likelihood of a misspelling (Stein and Curatolo, 2006). In order to combine the strength of multiple metrics within a near-duplicate detection task, several authors resort to machine learning (Bilenko and Mooney, 2002; Cohen et al., 2003). Christen (2006) concludes that it is important to exploit all kinds of knowledge about the type of data in question, and that inconsistencies are domain-specific. Hence, an effective near-duplicate detection approach should employ domain-specific heuristics and algorithms (Müller and Freytag, 2003). Following this argumentation, we augment various word similarity assessments with patent-specific meta-features. Patent Search Commercial patent search engines, such as PatBase and FreePatentsOnline, handle near-duplicates in assignee names as follows. For queries which contain a company name followed by a wildca</context>
</contexts>
<marker>Christen, 2006</marker>
<rawString>Peter Christen. 2006. A Comparison of Personal Name Matching: Techniques and Practical Issues. In ICDM ’06: Workshops Proceedings of the sixth IEEE International Conference on Data Mining, pages 290–294. IEEE Computer Society, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Pradeep Ravikumar</author>
<author>Stephen E Fienberg</author>
</authors>
<title>A Comparison of String Distance Metrics for Name-Matching Tasks.</title>
<date>2003</date>
<booktitle>In Subbarao Kambhampati and</booktitle>
<pages>73--78</pages>
<editor>Craig A. Knoblock, editors,</editor>
<contexts>
<context position="10465" citStr="Cohen et al., 2003" startWordPosition="1705" endWordPosition="1708">ckard” are distinct but refer to the same company. These kinds of near-duplicates impede the identification of duplicates (Naumann and Herschel, 2010). Near-duplicate Detection The problem of identifying near-duplicates is also known as record linkage, or name matching; it is subject of active research (Elmagarmid et al., 2007). With respect to text documents, slightly modified passages in these documents can be identified using fingerprints (Potthast and Stein, 2008). On the other hand, for data fields which contain natural language such as the assignee name field, string similarity metrics (Cohen et al., 2003) as well as spelling correction technology are exploited (Damerau, 1964; Monge and Elkan, 1997). String similarity metrics compute a numeric value to capture the similarity of two strings. Spelling correction algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word. In our analysis, the similarity metric SoftTfIdf is applied, which performs best in name matching tasks (Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein dista</context>
<context position="17846" citStr="Cohen et al., 2003" startWordPosition="2857" endWordPosition="2860">feature vector. For training and test purposes we identified misspellings for 100 different company names. A detailed description of the constructed test corpus and a report on the classifiers performance is given in the remainder of this section. 3.1 Feature Set The feature set comprises six features, three of them being orthographic similarity metrics, which are computed for every pair (q, a). Each metric compares a given company name q with the first |q |words of the assignee name a: 1. SoftTfIdf. The SoftTfIdf metric is considered, since the metric is suitable for the comparison of names (Cohen et al., 2003). The metric incorporates the Jaro-Winkler metric (Winkler, 1999) with a distance threshold of 0.9. The frequency values for the similarity computation are trained on A. 2. Soundex. The Soundex spelling correction algorithm captures phonetic errors. Since the algorithm computes hash values for both q and a, the feature is 1 if these hash values are equal, 0 otherwise. 3. Levenshtein distance. The Levenshtein distance for (q, a) is normalized by the character length of q. To obtain further evidence for a misspelling in an assignee name, meta information about the patents in D, to which the assi</context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>William W. Cohen, Pradeep Ravikumar, and Stephen E. Fienberg. 2003. A Comparison of String Distance Metrics for Name-Matching Tasks. In Subbarao Kambhampati and Craig A. Knoblock, editors, IIWeb ’03: Proceedings of the IJCAI workshop on Information Integration on the Web, pages 73–78, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>A Technique for Computer Detection and Correction of Spelling Errors.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="10536" citStr="Damerau, 1964" startWordPosition="1718" endWordPosition="1719">ates impede the identification of duplicates (Naumann and Herschel, 2010). Near-duplicate Detection The problem of identifying near-duplicates is also known as record linkage, or name matching; it is subject of active research (Elmagarmid et al., 2007). With respect to text documents, slightly modified passages in these documents can be identified using fingerprints (Potthast and Stein, 2008). On the other hand, for data fields which contain natural language such as the assignee name field, string similarity metrics (Cohen et al., 2003) as well as spelling correction technology are exploited (Damerau, 1964; Monge and Elkan, 1997). String similarity metrics compute a numeric value to capture the similarity of two strings. Spelling correction algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word. In our analysis, the similarity metric SoftTfIdf is applied, which performs best in name matching tasks (Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein distance, which gives the minimum number of edits needed to transform a word</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Fred J. Damerau. 1964. A Technique for Computer Detection and Correction of Spelling Errors. Communications of the ACM, 7(3):171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed K Elmagarmid</author>
<author>Panagiotis G Ipeirotis</author>
<author>Vassilios S Verykios</author>
</authors>
<title>Duplicate Record Detection: A Survey.</title>
<date>2007</date>
<journal>IEEE Trans. Knowl. Data Eng.,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="10175" citStr="Elmagarmid et al., 2007" startWordPosition="1658" endWordPosition="1661">ssify inconsistencies, where duplicates of entities in a corpus are part of a semantic anomaly. These duplicates exist in a database if two or more different tuples refer to the same entity. With respect to the bibliographic fields of patents, the assignee names “Howlett-Packard” and “Hewett-Packard” are distinct but refer to the same company. These kinds of near-duplicates impede the identification of duplicates (Naumann and Herschel, 2010). Near-duplicate Detection The problem of identifying near-duplicates is also known as record linkage, or name matching; it is subject of active research (Elmagarmid et al., 2007). With respect to text documents, slightly modified passages in these documents can be identified using fingerprints (Potthast and Stein, 2008). On the other hand, for data fields which contain natural language such as the assignee name field, string similarity metrics (Cohen et al., 2003) as well as spelling correction technology are exploited (Damerau, 1964; Monge and Elkan, 1997). String similarity metrics compute a numeric value to capture the similarity of two strings. Spelling correction algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word.</context>
</contexts>
<marker>Elmagarmid, Ipeirotis, Verykios, 2007</marker>
<rawString>Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and Vassilios S. Verykios. 2007. Duplicate Record Detection: A Survey. IEEE Trans. Knowl. Data Eng., 19(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caspas J Fall</author>
<author>Christophe Giraud-Carrier</author>
</authors>
<title>Searching Trademark Databases for Verbal Similarities. World Patent Information,</title>
<date>2005</date>
<contexts>
<context position="16851" citStr="Fall and Giraud-Carrier, 2005" startWordPosition="2682" endWordPosition="2685">re assessed based on figurative and verbal criteria. In the former case, the focus is on image-based retrieval techniques. Trademarks are considered verbally similar for a variety of reasons, such as pronunciation, spelling, and conceptual closeness, e.g., swapping letters or using numbers for words. The verbal similarity of trademarks, on the other hand, can be determined by using techniques comparable to near-duplicate detection: phonological parsing, Single word spelling correction Near similarity hashing Phonetic production approach Editing 573 fuzzy search, and edit distance computation (Fall and Giraud-Carrier, 2005). 3 Detection of Spelling Errors This section presents our machine learning approach to expand a company query q; the classifier c delivers the set AQ = {a E A |c(q, a) = 1}, an approximation of the ideal set of relevant assignee names AQ. As a classification technology a support vector machine with linear kernel is used, which receives each pair (q, a) as a sixdimensional feature vector. For training and test purposes we identified misspellings for 100 different company names. A detailed description of the constructed test corpus and a report on the classifiers performance is given in the rem</context>
</contexts>
<marker>Fall, Giraud-Carrier, 2005</marker>
<rawString>Caspas J. Fall and Christophe Giraud-Carrier. 2005. Searching Trademark Databases for Verbal Similarities. World Patent Information, 27(2):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hagen</author>
<author>Benno Stein</author>
</authors>
<title>Candidate Document Retrieval for Web-Scale Text Reuse Detection.</title>
<date>2011</date>
<booktitle>In 18th International Symposium on String Processing and Information Retrieval (SPIRE 11),</booktitle>
<volume>7024</volume>
<pages>356--367</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13836" citStr="Hagen and Stein (2011)" startWordPosition="2212" endWordPosition="2215">isspellings. (2) Given an acronym of a company such as IBM, it is infeasible to expand the abbreviation to “International Business Machines” without considering domain knowledge. Query Expansion Methods for Patent Search To date, various studies have investigated query expansion techniques in the patent domain that focus on prior-art search and invalidity search (Magdy and Jones, 2011). Since we are dealing with queries that comprise only a company name, existing methods cannot be applied. Instead, the near-duplicate task in question is more related to a text reuse detection task discussed by Hagen and Stein (2011); given a document, passages which also appear identical or slightly modified in other documents, have to be retrieved by using standard keyword-based search engines. Their approach is guided by the user-over-ranking hypothesis introduced by Stein and Hagen (2011). It states that “the best retrieval performance can be achieved with queries returning about as many results as can be considered at user site.” If we make use of their terminology, then we can distinguish the query expansion sets (cf. Table 3) into two categories: (1) The trivial as well as the edit distance expansion sets are under</context>
</contexts>
<marker>Hagen, Stein, 2011</marker>
<rawString>Matthias Hagen and Benno Stein. 2011. Candidate Document Retrieval for Web-Scale Text Reuse Detection. In 18th International Symposium on String Processing and Information Retrieval (SPIRE 11), volume 7024 of Lecture Notes in Computer Science, pages 356–367. Springer.</rawString>
</citation>
<citation valid="true">
<date>2007</date>
<booktitle>Patent Searching: Tools &amp; Techniques.</booktitle>
<editor>David Hunt, Long Nguyen, and Matthew Rodgers, editors.</editor>
<publisher>Wiley.</publisher>
<marker>2007</marker>
<rawString>David Hunt, Long Nguyen, and Matthew Rodgers, editors. 2007. Patent Searching: Tools &amp; Techniques. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Intellevate Inc</author>
</authors>
<title>Patent Quality, a blog entry. http://www.patenthawk.com/blog/</title>
<date>2006</date>
<marker>Inc, 2006</marker>
<rawString>Intellevate Inc. 2006. Patent Quality, a blog entry. http://www.patenthawk.com/blog/ 2006/01/patent_quality.html, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideo Joho</author>
<author>Leif A Azzopardi</author>
<author>Wim Vanderbauwhede</author>
</authors>
<title>A Survey of Patent Users: An Analysis of Tasks, Behavior, Search Functionality and System Requirements.</title>
<date>2010</date>
<booktitle>In IIix ’10: Proceeding of the third symposium on Information Interaction in Context,</booktitle>
<pages>13--24</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2653" citStr="Joho et al., 2010" startWordPosition="406" endWordPosition="409">els (Lopez and Romary, 2010), and automated query generation (Xue and Croft, 2009). Each approach improves retrieval performance, but what keeps them from attaining maximum effectiveness in terms of recall are the inconsistencies found in patents: incomplete citation sets, incorrectly assigned classification codes, and, not least, spelling errors. Our paper deals with spelling errors in an obligatory and important field of each patent, namely, the patent assignee name. Bibliographic fields are widely used among professional patent searchers in order to constrain keyword-based search sessions (Joho et al., 2010). The assignee name is particularly helpful for patentability searches and portfolio analyses since it determines the company holding the patent. Patent experts address these search tasks by formulating queries containing the company name in question, in the hope of finding all patents owned by that company. A formal and more precise description of this relevant search task is as follows: Given a query q which specifies a company, and a set D of patents, determine the set DQ C D comprised of all patents held by the respective company. For this purpose, all assignee names in the patents in D sh</context>
</contexts>
<marker>Joho, Azzopardi, Vanderbauwhede, 2010</marker>
<rawString>Hideo Joho, Leif A. Azzopardi, and Wim Vanderbauwhede. 2010. A Survey of Patent Users: An Analysis of Tasks, Behavior, Search Functionality and System Requirements. In IIix ’10: Proceeding of the third symposium on Information Interaction in Context, pages 13–24, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>The Art of Computer Programming, Volume I: Fundamental Algorithms, 3rd Edition.</title>
<date>1997</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="11042" citStr="Knuth, 1997" startWordPosition="1797" endWordPosition="1798">larity metrics (Cohen et al., 2003) as well as spelling correction technology are exploited (Damerau, 1964; Monge and Elkan, 1997). String similarity metrics compute a numeric value to capture the similarity of two strings. Spelling correction algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word. In our analysis, the similarity metric SoftTfIdf is applied, which performs best in name matching tasks (Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein distance, which gives the minimum number of edits needed to transform a word into another word (Levenshtein, 1966), and SmartSpell, a phonetic production approach that computes the likelihood of a misspelling (Stein and Curatolo, 2006). In order to combine the strength of multiple metrics within a near-duplicate detection task, several authors resort to machine learning (Bilenko and Mooney, 2002; Cohen et al., 2003). Christen (2006) concludes that it is important to exploit all kinds of knowledge about the type of data in question, and that inconsistencies are domain-specific</context>
</contexts>
<marker>Knuth, 1997</marker>
<rawString>Donald E. Knuth. 1997. The Art of Computer Programming, Volume I: Fundamental Algorithms, 3rd Edition. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<booktitle>Original in Doklady Akademii Nauk SSSR</booktitle>
<volume>10</volume>
<issue>8</issue>
<pages>845--848</pages>
<contexts>
<context position="11174" citStr="Levenshtein, 1966" startWordPosition="1817" endWordPosition="1819">997). String similarity metrics compute a numeric value to capture the similarity of two strings. Spelling correction algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word. In our analysis, the similarity metric SoftTfIdf is applied, which performs best in name matching tasks (Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein distance, which gives the minimum number of edits needed to transform a word into another word (Levenshtein, 1966), and SmartSpell, a phonetic production approach that computes the likelihood of a misspelling (Stein and Curatolo, 2006). In order to combine the strength of multiple metrics within a near-duplicate detection task, several authors resort to machine learning (Bilenko and Mooney, 2002; Cohen et al., 2003). Christen (2006) concludes that it is important to exploit all kinds of knowledge about the type of data in question, and that inconsistencies are domain-specific. Hence, an effective near-duplicate detection approach should employ domain-specific heuristics and algorithms (Müller and Freytag,</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710. Original in Doklady Akademii Nauk SSSR 163(4): 845-848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanen Li</author>
<author>Huizhong Duan</author>
<author>ChengXiang Zhai</author>
</authors>
<title>CloudSpeller: Spelling Correction for Search Queries by Using a Unified Hidden Markov Model with Web-scale Resources. In Spelling Alteration for Web Search Workshop,</title>
<date>2011</date>
<pages>10--14</pages>
<contexts>
<context position="15208" citStr="Li et al., 2011" startWordPosition="2431" endWordPosition="2434">st, is overspecific; it returns too few documents, i.e., the achieved recall is not optimal. As a consequence, these query expansion sets are not suitable for portfolio search. Our approach, on the other hand, excels in both precision and recall. Query Spelling Correction Queries which are submitted to standard web search engines differ from queries which are posed to patent search engines with respect to both length and language diversity. Hence, research in the field of web search is concerned with suggesting reasonable alternatives to misspelled queries rather than correcting single words (Li et al., 2011). Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al., 2011), query logs (Chen et al., 2007), and large n-gram corpora (Brants et al., 2007) are employed. It should be noted that the set of correctly written assignee names is unknown for the USPTO patent corpus. Moreover, spelling errors are modeled on the basis of language models (Li et al., 2011). Okuno (2011) proposes a generative model to encounter spelling errors, where the original query is expanded based on</context>
</contexts>
<marker>Li, Duan, Zhai, 2011</marker>
<rawString>Yanen Li, Huizhong Duan, and ChengXiang Zhai. 2011. CloudSpeller: Spelling Correction for Search Queries by Using a Unified Hidden Markov Model with Web-scale Resources. In Spelling Alteration for Web Search Workshop, pages 10–14, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrice Lopez</author>
<author>Laurent Romary</author>
</authors>
<title>Experiments with Citation Mining and Key-Term Extraction for Prior Art Search. In</title>
<date>2010</date>
<booktitle>CLEF 2010 LABs and Workshops, Notebook Papers,</booktitle>
<editor>Martin Braschler, Donna Harman, and Emanuele Pianta, editors,</editor>
<contexts>
<context position="2063" citStr="Lopez and Romary, 2010" startWordPosition="317" endWordPosition="320">along with their typical (•) and related (o) tasks. The due diligence task, for example, is concerned with legal issues that arise while investigating another company. Part of an investigation is a patent portfolio comparison between one or more competitors (Lupu et al., 2011). Within all tasks recall is preferred over precision, a fact which distinguishes patent search from general web search. This retrieval constraint has produced a variety of sophisticated approaches tailored to the patent domain: citation analysis (Magdy and Jones, 2010), the learning of section-specific retrieval models (Lopez and Romary, 2010), and automated query generation (Xue and Croft, 2009). Each approach improves retrieval performance, but what keeps them from attaining maximum effectiveness in terms of recall are the inconsistencies found in patents: incomplete citation sets, incorrectly assigned classification codes, and, not least, spelling errors. Our paper deals with spelling errors in an obligatory and important field of each patent, namely, the patent assignee name. Bibliographic fields are widely used among professional patent searchers in order to constrain keyword-based search sessions (Joho et al., 2010). The assi</context>
</contexts>
<marker>Lopez, Romary, 2010</marker>
<rawString>Patrice Lopez and Laurent Romary. 2010. Experiments with Citation Mining and Key-Term Extraction for Prior Art Search. In Martin Braschler, Donna Harman, and Emanuele Pianta, editors, CLEF 2010 LABs and Workshops, Notebook Papers, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Lupu</author>
<author>Katja Mayer</author>
<author>John Tait</author>
<author>J Anthony</author>
</authors>
<title>of The Information Retrieval Series.</title>
<date>2011</date>
<booktitle>Current Challenges in Patent Information Retrieval,</booktitle>
<volume>29</volume>
<editor>Trippe, editors.</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1717" citStr="Lupu et al., 2011" startWordPosition="267" endWordPosition="270">ll patents of a company, our approach improves recall from 96.7% (when using a state-of-the-art patent search engine) to 99.5%, while precision is compromised by only 3.7%. 1 Introduction Patent search forms the heart of most retrieval tasks in the intellectual property domain—cf. Table 1, which provides an overview of various user groups along with their typical (•) and related (o) tasks. The due diligence task, for example, is concerned with legal issues that arise while investigating another company. Part of an investigation is a patent portfolio comparison between one or more competitors (Lupu et al., 2011). Within all tasks recall is preferred over precision, a fact which distinguishes patent search from general web search. This retrieval constraint has produced a variety of sophisticated approaches tailored to the patent domain: citation analysis (Magdy and Jones, 2010), the learning of section-specific retrieval models (Lopez and Romary, 2010), and automated query generation (Xue and Croft, 2009). Each approach improves retrieval performance, but what keeps them from attaining maximum effectiveness in terms of recall are the inconsistencies found in patents: incomplete citation sets, incorrec</context>
</contexts>
<marker>Lupu, Mayer, Tait, Anthony, 2011</marker>
<rawString>Mihai Lupu, Katja Mayer, John Tait, and Anthony J. Trippe, editors. 2011. Current Challenges in Patent Information Retrieval, volume 29 of The Information Retrieval Series. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid Magdy</author>
<author>Gareth J F Jones</author>
</authors>
<title>Applying the KISS Principle for the CLEF-IP 2010 Prior Art Candidate Patent Search Task. In</title>
<date>2010</date>
<booktitle>CLEF 2010 LABs and Workshops, Notebook Papers,</booktitle>
<editor>Martin Braschler, Donna Harman, and Emanuele Pianta, editors,</editor>
<contexts>
<context position="1987" citStr="Magdy and Jones, 2010" startWordPosition="306" endWordPosition="309">erty domain—cf. Table 1, which provides an overview of various user groups along with their typical (•) and related (o) tasks. The due diligence task, for example, is concerned with legal issues that arise while investigating another company. Part of an investigation is a patent portfolio comparison between one or more competitors (Lupu et al., 2011). Within all tasks recall is preferred over precision, a fact which distinguishes patent search from general web search. This retrieval constraint has produced a variety of sophisticated approaches tailored to the patent domain: citation analysis (Magdy and Jones, 2010), the learning of section-specific retrieval models (Lopez and Romary, 2010), and automated query generation (Xue and Croft, 2009). Each approach improves retrieval performance, but what keeps them from attaining maximum effectiveness in terms of recall are the inconsistencies found in patents: incomplete citation sets, incorrectly assigned classification codes, and, not least, spelling errors. Our paper deals with spelling errors in an obligatory and important field of each patent, namely, the patent assignee name. Bibliographic fields are widely used among professional patent searchers in or</context>
</contexts>
<marker>Magdy, Jones, 2010</marker>
<rawString>Walid Magdy and Gareth J. F. Jones. 2010. Applying the KISS Principle for the CLEF-IP 2010 Prior Art Candidate Patent Search Task. In Martin Braschler, Donna Harman, and Emanuele Pianta, editors, CLEF 2010 LABs and Workshops, Notebook Papers, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walid Magdy</author>
<author>Gareth J F Jones</author>
</authors>
<title>A Study on Query Expansion Methods for Patent Retrieval.</title>
<date>2011</date>
<booktitle>In PAIR ’11: Proceedings of the 4th workshop on Patent information retrieval, AAAI Workshop on Plan, Activity, and Intent Recognition,</booktitle>
<pages>19--24</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13602" citStr="Magdy and Jones, 2011" startWordPosition="2174" endWordPosition="2177"> near-duplicates which contain spelling errors in the company name itself are not found. Even if PatBase would support left trailing wildcards, then only the full combination of wildcard expressions would cover all possible cases of misspellings. (2) Given an acronym of a company such as IBM, it is infeasible to expand the abbreviation to “International Business Machines” without considering domain knowledge. Query Expansion Methods for Patent Search To date, various studies have investigated query expansion techniques in the patent domain that focus on prior-art search and invalidity search (Magdy and Jones, 2011). Since we are dealing with queries that comprise only a company name, existing methods cannot be applied. Instead, the near-duplicate task in question is more related to a text reuse detection task discussed by Hagen and Stein (2011); given a document, passages which also appear identical or slightly modified in other documents, have to be retrieved by using standard keyword-based search engines. Their approach is guided by the user-over-ranking hypothesis introduced by Stein and Hagen (2011). It states that “the best retrieval performance can be achieved with queries returning about as many </context>
</contexts>
<marker>Magdy, Jones, 2011</marker>
<rawString>Walid Magdy and Gareth J.F. Jones. 2011. A Study on Query Expansion Methods for Patent Retrieval. In PAIR ’11: Proceedings of the 4th workshop on Patent information retrieval, AAAI Workshop on Plan, Activity, and Intent Recognition, pages 19– 24, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro E Monge</author>
<author>Charles Elkan</author>
</authors>
<title>An Efficient Domain-Independent Algorithm for Detecting Approximately Duplicate Database Records.</title>
<date>1997</date>
<booktitle>In DMKD ’09: Proceedings of the 2nd workshop on Research Issues on Data Mining and Knowledge Discovery,</booktitle>
<pages>23--29</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10560" citStr="Monge and Elkan, 1997" startWordPosition="1720" endWordPosition="1723"> identification of duplicates (Naumann and Herschel, 2010). Near-duplicate Detection The problem of identifying near-duplicates is also known as record linkage, or name matching; it is subject of active research (Elmagarmid et al., 2007). With respect to text documents, slightly modified passages in these documents can be identified using fingerprints (Potthast and Stein, 2008). On the other hand, for data fields which contain natural language such as the assignee name field, string similarity metrics (Cohen et al., 2003) as well as spelling correction technology are exploited (Damerau, 1964; Monge and Elkan, 1997). String similarity metrics compute a numeric value to capture the similarity of two strings. Spelling correction algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word. In our analysis, the similarity metric SoftTfIdf is applied, which performs best in name matching tasks (Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein distance, which gives the minimum number of edits needed to transform a word into another word (Leve</context>
</contexts>
<marker>Monge, Elkan, 1997</marker>
<rawString>Alvaro E. Monge and Charles Elkan. 1997. An Efficient Domain-Independent Algorithm for Detecting Approximately Duplicate Database Records. In DMKD ’09: Proceedings of the 2nd workshop on Research Issues on Data Mining and Knowledge Discovery, pages 23–29, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heiko Müller</author>
<author>Johann-C Freytag</author>
</authors>
<title>Problems, Methods and Challenges in Comprehensive Data Cleansing.</title>
<date>2003</date>
<tech>Technical Report HUB-IB-164, Humboldt-Universität zu</tech>
<institution>Berlin, Institut für Informatik,</institution>
<contexts>
<context position="9383" citStr="Müller and Freytag, 2003" startWordPosition="1536" endWordPosition="1539">” in contrast to “LLC”, for example. (5) Indexing errors emerge from OCR processing patent applications, because similar looking letters such as “e” versus “c” or “l” versus “I” are likely to be misinterpreted. (6) With the advent of electronic patent application filing, the number of patent reexamination steps was reduced. As a consequence, the chance of undetected spelling errors increases (Adams, 2010). All of the mentioned factors add to a highly inconsistent USPTO corpus. 2 Related Work Information within a corpus can only be retrieved effectively if the data is both accurate and unique (Müller and Freytag, 2003). In order to yield data that is accurate and unique, approaches to data cleansing can be utilized to identify and remove inconsistencies. Müller and Freytag (2003) classify inconsistencies, where duplicates of entities in a corpus are part of a semantic anomaly. These duplicates exist in a database if two or more different tuples refer to the same entity. With respect to the bibliographic fields of patents, the assignee names “Howlett-Packard” and “Hewett-Packard” are distinct but refer to the same company. These kinds of near-duplicates impede the identification of duplicates (Naumann and He</context>
<context position="11780" citStr="Müller and Freytag, 2003" startWordPosition="1907" endWordPosition="1910">(Levenshtein, 1966), and SmartSpell, a phonetic production approach that computes the likelihood of a misspelling (Stein and Curatolo, 2006). In order to combine the strength of multiple metrics within a near-duplicate detection task, several authors resort to machine learning (Bilenko and Mooney, 2002; Cohen et al., 2003). Christen (2006) concludes that it is important to exploit all kinds of knowledge about the type of data in question, and that inconsistencies are domain-specific. Hence, an effective near-duplicate detection approach should employ domain-specific heuristics and algorithms (Müller and Freytag, 2003). Following this argumentation, we augment various word similarity assessments with patent-specific meta-features. Patent Search Commercial patent search engines, such as PatBase and FreePatentsOnline, handle near-duplicates in assignee names as follows. For queries which contain a company name followed by a wildcard operator, PatBase suggests 572 Collision-based Neighborhood-based Trigram-based Edit-distance-based Rule-based Heuristic search Hidden Markov models Figure 1: Classification of spelling correction methods according to Stein and Curatolo (2006). a set of additional companies (near-</context>
</contexts>
<marker>Müller, Freytag, 2003</marker>
<rawString>Heiko Müller and Johann-C. Freytag. 2003. Problems, Methods and Challenges in Comprehensive Data Cleansing. Technical Report HUB-IB-164, Humboldt-Universität zu Berlin, Institut für Informatik, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Naumann</author>
<author>Melanie Herschel</author>
</authors>
<title>An Introduction to Duplicate Detection. Synthesis Lectures on Data Management.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="9996" citStr="Naumann and Herschel, 2010" startWordPosition="1631" endWordPosition="1634">Freytag, 2003). In order to yield data that is accurate and unique, approaches to data cleansing can be utilized to identify and remove inconsistencies. Müller and Freytag (2003) classify inconsistencies, where duplicates of entities in a corpus are part of a semantic anomaly. These duplicates exist in a database if two or more different tuples refer to the same entity. With respect to the bibliographic fields of patents, the assignee names “Howlett-Packard” and “Hewett-Packard” are distinct but refer to the same company. These kinds of near-duplicates impede the identification of duplicates (Naumann and Herschel, 2010). Near-duplicate Detection The problem of identifying near-duplicates is also known as record linkage, or name matching; it is subject of active research (Elmagarmid et al., 2007). With respect to text documents, slightly modified passages in these documents can be identified using fingerprints (Potthast and Stein, 2008). On the other hand, for data fields which contain natural language such as the assignee name field, string similarity metrics (Cohen et al., 2003) as well as spelling correction technology are exploited (Damerau, 1964; Monge and Elkan, 1997). String similarity metrics compute </context>
</contexts>
<marker>Naumann, Herschel, 2010</marker>
<rawString>Felix Naumann and Melanie Herschel. 2010. An Introduction to Duplicate Detection. Synthesis Lectures on Data Management. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoh Okuno</author>
</authors>
<title>Spell Generation based on Edit Distance. In Spelling Alteration for Web Search Workshop,</title>
<date>2011</date>
<pages>25--26</pages>
<contexts>
<context position="15704" citStr="Okuno (2011)" startWordPosition="2516" endWordPosition="2517"> with suggesting reasonable alternatives to misspelled queries rather than correcting single words (Li et al., 2011). Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al., 2011), query logs (Chen et al., 2007), and large n-gram corpora (Brants et al., 2007) are employed. It should be noted that the set of correctly written assignee names is unknown for the USPTO patent corpus. Moreover, spelling errors are modeled on the basis of language models (Li et al., 2011). Okuno (2011) proposes a generative model to encounter spelling errors, where the original query is expanded based on alternatives produced by a small edit distance to the original query. This strategy correlates to the trivial query expansion set (cf. Section 1). Unlike using a small edit distance, we allow a reasonable high edit distance to maximize the recall. Trademark Search The trademark search is about identifying registered trademarks which are similar to a new trademark application. Similarities between trademarks are assessed based on figurative and verbal criteria. In the former case, the focus </context>
</contexts>
<marker>Okuno, 2011</marker>
<rawString>Yoh Okuno. 2011. Spell Generation based on Edit Distance. In Spelling Alteration for Web Search Workshop, pages 25–26, July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Martin Potthast</author>
<author>Benno Stein</author>
</authors>
<title>New Issues in Near-duplicate Detection.</title>
<date>2008</date>
<booktitle>Data Analysis, Machine Learning and Applications. Selected papers from the 31th Annual Conference of the German Classification Society (GfKl 07), Studies in Classification, Data Analysis, and Knowledge Organization,</booktitle>
<pages>601--609</pages>
<editor>In Christine Preisach, Hans Burkhardt, Lars Schmidt-Thieme, and Reinhold Decker, editors,</editor>
<publisher>Springer.</publisher>
<location>Berlin Heidelberg New York.</location>
<contexts>
<context position="10318" citStr="Potthast and Stein, 2008" startWordPosition="1680" endWordPosition="1683"> more different tuples refer to the same entity. With respect to the bibliographic fields of patents, the assignee names “Howlett-Packard” and “Hewett-Packard” are distinct but refer to the same company. These kinds of near-duplicates impede the identification of duplicates (Naumann and Herschel, 2010). Near-duplicate Detection The problem of identifying near-duplicates is also known as record linkage, or name matching; it is subject of active research (Elmagarmid et al., 2007). With respect to text documents, slightly modified passages in these documents can be identified using fingerprints (Potthast and Stein, 2008). On the other hand, for data fields which contain natural language such as the assignee name field, string similarity metrics (Cohen et al., 2003) as well as spelling correction technology are exploited (Damerau, 1964; Monge and Elkan, 1997). String similarity metrics compute a numeric value to capture the similarity of two strings. Spelling correction algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word. In our analysis, the similarity metric SoftTfIdf is applied, which performs best in name matching tasks (Cohen et al., 2003), as well as the c</context>
</contexts>
<marker>Potthast, Stein, 2008</marker>
<rawString>Martin Potthast and Benno Stein. 2008. New Issues in Near-duplicate Detection. In Christine Preisach, Hans Burkhardt, Lars Schmidt-Thieme, and Reinhold Decker, editors, Data Analysis, Machine Learning and Applications. Selected papers from the 31th Annual Conference of the German Classification Society (GfKl 07), Studies in Classification, Data Analysis, and Knowledge Organization, pages 601–609, Berlin Heidelberg New York. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benno Stein</author>
<author>Daniel Curatolo</author>
</authors>
<title>Phonetic Spelling and Heuristic Search.</title>
<date>2006</date>
<booktitle>17th European Conference on Artificial Intelligence (ECAI 06),</booktitle>
<pages>829--830</pages>
<editor>In Gerhard Brewka, Silvia Coradeschi, Anna Perini, and Paolo Traverso, editors,</editor>
<publisher>IOS Press.</publisher>
<location>Amsterdam, Berlin,</location>
<contexts>
<context position="5218" citStr="Stein and Curatolo (2006)" startWordPosition="861" endWordPosition="864"> q must be a unique identifier (i.e. a key), and an assignee name a that contains q can be considered as relevant. It should be noted in this regard that |q |&lt; |a |holds for most elements in Aq, since the assignee names often contain company suffixes such as “Ltd” or “Inc”. Our hypothesis is that due to misspelled assignee names a substantial fraction of relevant patents cannot be found by the baseline approach. In this regard, the types of spelling errors in assignee names given in Table 2 should be considered. Table 2: Types of spelling errors with increasing problem complexity according to Stein and Curatolo (2006). The first row refers to lexical errors, whereas the last two rows refer to phonological errors. For each type, an example is given, where a misspelled company name is followed by the correctly spelled variant. Spelling error type Example Permutations or dropped letters Whirpool Corporation → Whirlpool Corporation Misremembering spelling details Whetherford International → Weatherford International Spelling out the pronunciation Emulecks Corporation → Emulex Corporation In order to raise the recall for portfolio search without significantly impairing precision, an ap1www.patbase.com 2www.free</context>
<context position="11295" citStr="Stein and Curatolo, 2006" startWordPosition="1834" endWordPosition="1837">ion algorithms, by contrast, capture the likelihood for a given word being a misspelling of another word. In our analysis, the similarity metric SoftTfIdf is applied, which performs best in name matching tasks (Cohen et al., 2003), as well as the complete range of spelling correction algorithms shown in Figure 1: Soundex, which relies on similarity hashing (Knuth, 1997), the Levenshtein distance, which gives the minimum number of edits needed to transform a word into another word (Levenshtein, 1966), and SmartSpell, a phonetic production approach that computes the likelihood of a misspelling (Stein and Curatolo, 2006). In order to combine the strength of multiple metrics within a near-duplicate detection task, several authors resort to machine learning (Bilenko and Mooney, 2002; Cohen et al., 2003). Christen (2006) concludes that it is important to exploit all kinds of knowledge about the type of data in question, and that inconsistencies are domain-specific. Hence, an effective near-duplicate detection approach should employ domain-specific heuristics and algorithms (Müller and Freytag, 2003). Following this argumentation, we augment various word similarity assessments with patent-specific meta-features. </context>
</contexts>
<marker>Stein, Curatolo, 2006</marker>
<rawString>Benno Stein and Daniel Curatolo. 2006. Phonetic Spelling and Heuristic Search. In Gerhard Brewka, Silvia Coradeschi, Anna Perini, and Paolo Traverso, editors, 17th European Conference on Artificial Intelligence (ECAI 06), pages 829–830, Amsterdam, Berlin, August. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benno Stein</author>
<author>Matthias Hagen</author>
</authors>
<title>Introducing the User-over-Ranking Hypothesis.</title>
<date>2011</date>
<booktitle>In Advances in Information Retrieval. 33rd European Conference on IR Resarch (ECIR</booktitle>
<volume>11</volume>
<pages>503--509</pages>
<publisher>Springer.</publisher>
<location>Berlin Heidelberg New York,</location>
<contexts>
<context position="14100" citStr="Stein and Hagen (2011)" startWordPosition="2251" endWordPosition="2254">query expansion techniques in the patent domain that focus on prior-art search and invalidity search (Magdy and Jones, 2011). Since we are dealing with queries that comprise only a company name, existing methods cannot be applied. Instead, the near-duplicate task in question is more related to a text reuse detection task discussed by Hagen and Stein (2011); given a document, passages which also appear identical or slightly modified in other documents, have to be retrieved by using standard keyword-based search engines. Their approach is guided by the user-over-ranking hypothesis introduced by Stein and Hagen (2011). It states that “the best retrieval performance can be achieved with queries returning about as many results as can be considered at user site.” If we make use of their terminology, then we can distinguish the query expansion sets (cf. Table 3) into two categories: (1) The trivial as well as the edit distance expansion sets are underspecific, i.e., users cannot cope with the large amount of irrelevant patents returned; the precision is close to zero. (2) The baseline approach, by contrast, is overspecific; it returns too few documents, i.e., the achieved recall is not optimal. As a consequenc</context>
</contexts>
<marker>Stein, Hagen, 2011</marker>
<rawString>Benno Stein and Matthias Hagen. 2011. Introducing the User-over-Ranking Hypothesis. In Advances in Information Retrieval. 33rd European Conference on IR Resarch (ECIR 11), volume 6611 of Lecture Notes in Computer Science, pages 503–509, Berlin Heidelberg New York, April. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U S Patent</author>
<author>Trademark Office</author>
</authors>
<date>2010</date>
<booktitle>Manual of Patent Examining Procedure (MPEP), Eighth Edition,</booktitle>
<marker>Patent, Office, 2010</marker>
<rawString>U.S. Patent &amp; Trademark Office. 2010. Manual of Patent Examining Procedure (MPEP), Eighth Edition, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Winkler</author>
</authors>
<title>The State of Record Linkage and Current Research Problems.</title>
<date>1999</date>
<journal>Statistical Research Division, U.S. Bureau of the Census.</journal>
<tech>Technical report,</tech>
<contexts>
<context position="17911" citStr="Winkler, 1999" startWordPosition="2868" endWordPosition="2869">ngs for 100 different company names. A detailed description of the constructed test corpus and a report on the classifiers performance is given in the remainder of this section. 3.1 Feature Set The feature set comprises six features, three of them being orthographic similarity metrics, which are computed for every pair (q, a). Each metric compares a given company name q with the first |q |words of the assignee name a: 1. SoftTfIdf. The SoftTfIdf metric is considered, since the metric is suitable for the comparison of names (Cohen et al., 2003). The metric incorporates the Jaro-Winkler metric (Winkler, 1999) with a distance threshold of 0.9. The frequency values for the similarity computation are trained on A. 2. Soundex. The Soundex spelling correction algorithm captures phonetic errors. Since the algorithm computes hash values for both q and a, the feature is 1 if these hash values are equal, 0 otherwise. 3. Levenshtein distance. The Levenshtein distance for (q, a) is normalized by the character length of q. To obtain further evidence for a misspelling in an assignee name, meta information about the patents in D, to which the assignee name refers to, is exploited. In this regard, the following </context>
</contexts>
<marker>Winkler, 1999</marker>
<rawString>William W. Winkler. 1999. The State of Record Linkage and Current Research Problems. Technical report, Statistical Research Division, U.S. Bureau of the Census.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>Bruce W Croft</author>
</authors>
<title>Automatic Query Generation for Patent Search.</title>
<date>2009</date>
<booktitle>In CIKM ’09: Proceeding of the eighteenth ACM conference on Information and Knowledge Management,</booktitle>
<pages>2037--2040</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2117" citStr="Xue and Croft, 2009" startWordPosition="326" endWordPosition="329">ue diligence task, for example, is concerned with legal issues that arise while investigating another company. Part of an investigation is a patent portfolio comparison between one or more competitors (Lupu et al., 2011). Within all tasks recall is preferred over precision, a fact which distinguishes patent search from general web search. This retrieval constraint has produced a variety of sophisticated approaches tailored to the patent domain: citation analysis (Magdy and Jones, 2010), the learning of section-specific retrieval models (Lopez and Romary, 2010), and automated query generation (Xue and Croft, 2009). Each approach improves retrieval performance, but what keeps them from attaining maximum effectiveness in terms of recall are the inconsistencies found in patents: incomplete citation sets, incorrectly assigned classification codes, and, not least, spelling errors. Our paper deals with spelling errors in an obligatory and important field of each patent, namely, the patent assignee name. Bibliographic fields are widely used among professional patent searchers in order to constrain keyword-based search sessions (Joho et al., 2010). The assignee name is particularly helpful for patentability se</context>
</contexts>
<marker>Xue, Croft, 2009</marker>
<rawString>Xiaobing Xue and Bruce W. Croft. 2009. Automatic Query Generation for Patent Search. In CIKM ’09: Proceeding of the eighteenth ACM conference on Information and Knowledge Management, pages 2037–2040, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>